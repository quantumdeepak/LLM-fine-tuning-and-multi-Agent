{
  "summary": {
    "perplexity": {
      "base_model": 14.105068492889405,
      "finetuned_model": 10.100432109832763
    },
    "bleu": {
      "base_model": {
        "bleu_1": 0.335046918004788,
        "bleu_2": 0.05851677976383881,
        "bleu_3": 0.015793742230523843,
        "bleu_4": 0.008602415026833631
      },
      "finetuned_model": {
        "bleu_1": 0.35012717290256,
        "bleu_2": 0.07936160796520217,
        "bleu_3": 0.025923503849213686,
        "bleu_4": 0.005163561076604555
      }
    },
    "rouge": {
      "base_model": {
        "rouge_1": {
          "f1": 0.34267216884715457,
          "precision": 0.37930009419258137,
          "recall": 0.31413012323256645
        },
        "rouge_2": {
          "f1": 0.07391877996283815,
          "precision": 0.08195640595540168,
          "recall": 0.06770444017559951
        },
        "rouge_l": {
          "f1": 0.20577332118484803,
          "precision": 0.22735998783226882,
          "recall": 0.18890005178812017
        }
      },
      "finetuned_model": {
        "rouge_1": {
          "f1": 0.36165391587862644,
          "precision": 0.39675152890928067,
          "recall": 0.3331168988458867
        },
        "rouge_2": {
          "f1": 0.08944653489355515,
          "precision": 0.09847284713618444,
          "recall": 0.08216272457791589
        },
        "rouge_l": {
          "f1": 0.21764240929591497,
          "precision": 0.23872805754439613,
          "recall": 0.20051709614857943
        }
      }
    },
    "win_counts": {
      "perplexity": {
        "base_wins": 0,
        "finetuned_wins": 10,
        "ties": 0
      },
      "bleu": {
        "base_wins": 2,
        "finetuned_wins": 8,
        "ties": 0
      },
      "rouge": {
        "base_wins": 4,
        "finetuned_wins": 6,
        "ties": 0
      }
    },
    "improvements": {
      "perplexity": 28.391470662304435,
      "bleu": {
        "bleu_1": 4.5009382529394095,
        "bleu_2": 35.621967383524215,
        "bleu_3": 64.1378178194685,
        "bleu_4": -39.97544805153218
      },
      "rouge": {
        "rouge_1": {
          "f1": 5.5393313951733525,
          "precision": 4.600957127060115,
          "recall": 6.044239061805405
        },
        "rouge_2": {
          "f1": 21.006508682263718,
          "precision": 20.152715322546634,
          "recall": 21.355001776570486
        },
        "rouge_l": {
          "f1": 5.768040308979036,
          "precision": 5.000030929150964,
          "recall": 6.149836514332739
        }
      }
    }
  },
  "samples": [
    {
      "title": "",
      "prompt": "Sparse Output Coding for Large-Scale Visual Recognition Bin Zhao Eric P. Xing School of Computer Science, Carnegie Mellon University {binzhao,epxing}@cs.cmu.edu Abstract Many vision tasks require a multi-class classi er to dis- criminate multiple categories, on the order of hundreds or thousands. In this paper, we propose sparse output coding, a",
      "target": "principled way for large-scale multi-class classi cation, by turning high-cardinality multi-class categorization into a bit-by-bit decoding problem. Speci cally, sparse output coding is composed of two steps: ef cient coding matrix learning with scalability to thousands of classes, and prob- abilistic decoding. Empirical results on object recognition and scene classi cation demonstrate the effectiveness of our proposed approach. 1. Introduction A recent trend in visual recognition is the rapid increase of concept space. For example, the SUN [26] database for scene recognition has 899 categories, and ImageNet [10] spans a total number of 21841 image classes. Moreover, the number of",
      "base_model_continuation": ", a novel coding scheme for the multi-class problem. It encodes a multi-class image by a sparse code, whose elements are sparse in the output space. The code is generated by a two-stage generative process: first, an output code is generated by a generative model, and second, a sparse code is generated from the output code by a sparse coding model. Our sparse coding model is based on the sparse coding algorithm of Elad and Aharon, which is a powerful tool for sparse coding",
      "finetuned_model_continuation": ", a novel framework for representing multi-class classi ers. Our framework combines the sparse representation and coding approaches in a single framework, and provides a new representation of a classi er with a sparse representation and a codebook. Our framework has the ability to scale up to very large classi er sizes, and is able to achieve state-of-the-art accuracy on several large-scale visual classi cation tasks. 1. Introduction The goal of visual recognition is to nd the most likely class for an input image",
      "base_perplexity": 10.240824699401855,
      "finetuned_perplexity": 7.688392162322998,
      "base_bleu": {
        "bleu_1": 0.32142857142857145,
        "bleu_2": 0.0,
        "bleu_3": 0.0,
        "bleu_4": 0.0
      },
      "finetuned_bleu": {
        "bleu_1": 0.40476190476190477,
        "bleu_2": 0.060240963855421686,
        "bleu_3": 0.012195121951219513,
        "bleu_4": 0.0
      },
      "base_rouge": {
        "rouge_1": {
          "precision": 0.36046511627906974,
          "recall": 0.29245283018867924,
          "f1": 0.32291666666666663
        },
        "rouge_2": {
          "precision": 0.047058823529411764,
          "recall": 0.0380952380952381,
          "f1": 0.042105263157894736
        },
        "rouge_l": {
          "precision": 0.23255813953488372,
          "recall": 0.18867924528301888,
          "f1": 0.20833333333333334
        }
      },
      "finetuned_rouge": {
        "rouge_1": {
          "precision": 0.4431818181818182,
          "recall": 0.36792452830188677,
          "f1": 0.4020618556701031
        },
        "rouge_2": {
          "precision": 0.08045977011494253,
          "recall": 0.06666666666666667,
          "f1": 0.07291666666666666
        },
        "rouge_l": {
          "precision": 0.23863636363636365,
          "recall": 0.19811320754716982,
          "f1": 0.21649484536082475
        }
      }
    },
    {
      "title": "",
      "prompt": "Fits Like a Glove: Rapid and Reliable Hand Shape Personalization David Joseph Tan1,2 Thomas Cashman1 Jonathan Taylor1 Andrew Fitzgibbon1 Daniel Tarlow1 Sameh Khamis1 Shahram Izadi1 Jamie Shotton1 1Microsoft Research 2Technische Universit at M unchen Abstract We present a fast, practical method for personalizing a hand shape basis to an individual",
      "target": "user s detailed hand shape using only a small set of depth images. To achieve this, we minimize an energy based on a sum of render-and-compare cost functions called the golden energy. However, this energy is only piecewise continuous, due to pixels crossing occlu- sion boundaries, and is therefore not obviously amenable to ef cient gradient-based optimization. A key insight is that the energy is the combination of a smooth low-frequency function with a high-frequency, low-amplitude, piecewise- continuous function. A central nite difference approxima- tion with a suitable step size can therefore jump over the dis- continuities to obtain a",
      "base_model_continuation": "an individual hand using a single 3D scan. Our method takes as input a single 3D scan of a hand and a set of training scans of the same hand. We then learn a linear transformation to map from the training scans to a personalized shape basis. The learned transformation is then applied to the input scan to produce a personalized hand shape. Our method is robust to occlusions, allows for arbitrary re-positioning of the hand in the scan, and can be applied to arbitrary hands. Our method",
      "finetuned_model_continuation": "an individual s hand. The method is based on a novel technique for the automatic and rapid alignment of hand models to a user s hand shape. It is capable of producing a personalized hand shape basis in less than one minute. The method is highly scalable and can be applied to a large number of users and hand shapes. We demonstrate the effectiveness of our method by personalizing a hand shape basis to a variety of different hand shapes and for different poses and camera angles. We also show that our method is effective for",
      "base_perplexity": 23.039583206176758,
      "finetuned_perplexity": 16.337291717529297,
      "base_bleu": {
        "bleu_1": 0.3181818181818182,
        "bleu_2": 0.022988505747126436,
        "bleu_3": 0.0,
        "bleu_4": 0.0
      },
      "finetuned_bleu": {
        "bleu_1": 0.3229166666666667,
        "bleu_2": 0.042105263157894736,
        "bleu_3": 0.010638297872340425,
        "bleu_4": 0.0
      },
      "base_rouge": {
        "rouge_1": {
          "precision": 0.3258426966292135,
          "recall": 0.27358490566037735,
          "f1": 0.2974358974358975
        },
        "rouge_2": {
          "precision": 0.03409090909090909,
          "recall": 0.02857142857142857,
          "f1": 0.031088082901554404
        },
        "rouge_l": {
          "precision": 0.19101123595505617,
          "recall": 0.16037735849056603,
          "f1": 0.17435897435897435
        }
      },
      "finetuned_rouge": {
        "rouge_1": {
          "precision": 0.34375,
          "recall": 0.3113207547169811,
          "f1": 0.3267326732673267
        },
        "rouge_2": {
          "precision": 0.042105263157894736,
          "recall": 0.0380952380952381,
          "f1": 0.04
        },
        "rouge_l": {
          "precision": 0.17708333333333334,
          "recall": 0.16037735849056603,
          "f1": 0.16831683168316833
        }
      }
    },
    {
      "title": "",
      "prompt": "Supplementary Material: Object Scene Flow for Autonomous Vehicles Moritz Menze Leibniz Universit at Hannover menze@ipi.uni-hannover.de Andreas Geiger MPI T ubingen andreas.geiger@tue.mpg.de Abstract This supplementary document provides additional descriptions, visualizations and experiments. We start by visualizing the 3D CAD models used for generating our scene ow ground truth. Next, we provide",
      "target": "a detailed description of the initial- ization procedure for object hypotheses in our model. For reproducibility, we also detail all (estimated) model parameters together with plots illustrating the sensitivity of our model with respect to the choice of parameters. We further demon- strate that the loss in performance is small when limiting the runtime of our method to two minutes per scene on a single core. We also provide additional qualitative results of our method on the sphere sequence and quantitatively compare our method to state-of-the-art stereo, optical ow and scene ow approaches on the well-established KITTI stereo and optical",
      "base_model_continuation": "we provide a comparison between our scene flow estimates and a state of the art optical flow algorithm. We also present a comparison of our scene flow estimates with the ground truth scene flow estimated by the computer vision group of ETH Zurich. We conclude with a comparison of our scene flow estimates to the scene flow estimated by the computer vision group of ETH Zurich.",
      "finetuned_model_continuation": "we provide a more detailed description of our evaluation protocol and show the results of the object detection, semantic segmentation and scene ow prediction. Finally, we show additional visualizations of the scene ow predictions. 1. CAD Models The CAD models used for the generation of the ground truth scene ow are provided as 3D models in the scene ow training data. We use the CAD models for visualizing the scene ow ground truth. We use the 3D models in the format provided by the Stanford 3D Widgets library",
      "base_perplexity": 20.80345916748047,
      "finetuned_perplexity": 12.770057678222656,
      "base_bleu": {
        "bleu_1": 0.4090909090909091,
        "bleu_2": 0.07692307692307693,
        "bleu_3": 0.0,
        "bleu_4": 0.0
      },
      "finetuned_bleu": {
        "bleu_1": 0.36363636363636365,
        "bleu_2": 0.08045977011494253,
        "bleu_3": 0.023255813953488372,
        "bleu_4": 0.0
      },
      "base_rouge": {
        "rouge_1": {
          "precision": 0.45454545454545453,
          "recall": 0.28846153846153844,
          "f1": 0.3529411764705882
        },
        "rouge_2": {
          "precision": 0.1076923076923077,
          "recall": 0.06796116504854369,
          "f1": 0.08333333333333333
        },
        "rouge_l": {
          "precision": 0.2727272727272727,
          "recall": 0.17307692307692307,
          "f1": 0.21176470588235294
        }
      },
      "finetuned_rouge": {
        "rouge_1": {
          "precision": 0.4090909090909091,
          "recall": 0.34615384615384615,
          "f1": 0.37500000000000006
        },
        "rouge_2": {
          "precision": 0.10344827586206896,
          "recall": 0.08737864077669903,
          "f1": 0.09473684210526315
        },
        "rouge_l": {
          "precision": 0.25,
          "recall": 0.21153846153846154,
          "f1": 0.22916666666666666
        }
      }
    },
    {
      "title": "",
      "prompt": "Non-Rigid Structure from Motion with Diffusion Maps Prior Lili Tao Bogdan J. Matuszewski Applied Digital Signal and Image Processing Research Centre University of Central Lancashire, UK {lltao,bmatuszewski1}@uclan.ac.uk Abstract In this paper, a novel approach based on a non-linear manifold learning technique is proposed to recover 3D non- rigid structures from",
      "target": "2D image sequences captured by a sin- gle camera. Most of the existing approaches assume that 3D shapes can be accurately modelled in a linear subspace. These techniques perform well when the deformations are relatively small or simple, but fail when more complex de- formations need to be recovered. The non-linear deforma- tions are often observed in highly exible objects for which the use of the linear model is impractical. A speci c type of shape variations might be governed by only a small number of parameters, therefore can be well- represented in a low dimensional manifold. We learn a",
      "base_model_continuation": "structures from a single sequence of images. The main idea of the proposed method is to combine the non-linear manifold learning technique with the diffusion map prior. The proposed approach is based on a diffusion map prior, which is a non-linear representation of the image structure. The diffusion map prior is used to represent the structure of the input image. The proposed method consists of three stages. The first stage is to detect and track the 3D rigid objects in the input sequence. The second stage is to compute",
      "finetuned_model_continuation": "structures from a sequence of images. The proposed ap- proach relies on the notion of non-rigid structure from motion, which is formulated as a non-linear diffusion map problem. The non-linear diffusion map prior is used to model the non-rigid structure from the image sequence. The non-rigid structure is recovered by solving the non- linear diffusion map problem using a hybrid particle swarm optimization (PSO) algorithm. The proposed non-linear diffusion map prior is tested on both synthetic and real data.",
      "base_perplexity": 13.508871078491211,
      "finetuned_perplexity": 10.370699882507324,
      "base_bleu": {
        "bleu_1": 0.2247191011235955,
        "bleu_2": 0.03409090909090909,
        "bleu_3": 0.0,
        "bleu_4": 0.0
      },
      "finetuned_bleu": {
        "bleu_1": 0.22784810126582278,
        "bleu_2": 0.01282051282051282,
        "bleu_3": 0.0,
        "bleu_4": 0.0
      },
      "base_rouge": {
        "rouge_1": {
          "precision": 0.31868131868131866,
          "recall": 0.2871287128712871,
          "f1": 0.3020833333333333
        },
        "rouge_2": {
          "precision": 0.044444444444444446,
          "recall": 0.04,
          "f1": 0.042105263157894736
        },
        "rouge_l": {
          "precision": 0.17582417582417584,
          "recall": 0.15841584158415842,
          "f1": 0.16666666666666669
        }
      },
      "finetuned_rouge": {
        "rouge_1": {
          "precision": 0.27058823529411763,
          "recall": 0.22772277227722773,
          "f1": 0.24731182795698925
        },
        "rouge_2": {
          "precision": 0.03571428571428571,
          "recall": 0.03,
          "f1": 0.03260869565217391
        },
        "rouge_l": {
          "precision": 0.17647058823529413,
          "recall": 0.1485148514851485,
          "f1": 0.16129032258064516
        }
      }
    },
    {
      "title": "",
      "prompt": "Pairwise Geometric Matching for Large-scale Object Retrieval Xinchao Li, Martha Larson, Alan Hanjalic Multimedia Computing Group, Delft University of Technology Delft, The Netherlands {x.li-3,m.a.larson,a.hanjalic}@tudelft.nl Abstract Spatial veri cation is a key step in boosting the perfor- mance of object-based image retrieval. It serves to elimi- nate unreliable correspondences between salient",
      "target": "points in a given pair of images, and is typically performed by analyz- ing the consistency of spatial transformations between the image regions involved in individual correspondences. In this paper, we consider the pairwise geometric relations be- tween correspondences and propose a strategy to incorpo- rate these relations at signi cantly reduced computational cost, which makes it suitable for large-scale object retrieval. In addition, we combine the information on geometric rela- tions from both the individual correspondences and pairs of correspondences to further improve the veri cation ac- curacy. Experimental results on three reference datasets show that the proposed approach",
      "base_model_continuation": "between salient object regions in the query and object instances in the database. Most existing spatial veri cation methods use a single metric (e.g., euclidean distance) and are only able to deal with a limited number of matching candidates. We propose a pairwise geometric matching method for large-scale object retrieval. The key idea is to use the geometric relationship between object regions to reduce the search space. The proposed method can handle up to 50,000 candidate matches and is able to retrieve 9",
      "finetuned_model_continuation": "between salient visual fea- tures. While many algorithms have been proposed to solve the problem, most of them are tailored for small-scale data sets. In this paper, we propose a novel pairwise geometric matching method to solve the problem for large-scale data sets. The method takes into account the spatial relationships between the features and provides a fast and ef cient solution. The method is tested on the NUS-WIDE and the MSR-Popular-100K image datasets, where it outperforms existing",
      "base_perplexity": 11.281835556030273,
      "finetuned_perplexity": 7.891571044921875,
      "base_bleu": {
        "bleu_1": 0.3855421686746988,
        "bleu_2": 0.0975609756097561,
        "bleu_3": 0.024691358024691357,
        "bleu_4": 0.0125
      },
      "finetuned_bleu": {
        "bleu_1": 0.3974358974358974,
        "bleu_2": 0.09090909090909091,
        "bleu_3": 0.02631578947368421,
        "bleu_4": 0.013333333333333334
      },
      "base_rouge": {
        "rouge_1": {
          "precision": 0.3953488372093023,
          "recall": 0.33663366336633666,
          "f1": 0.36363636363636365
        },
        "rouge_2": {
          "precision": 0.10588235294117647,
          "recall": 0.09,
          "f1": 0.0972972972972973
        },
        "rouge_l": {
          "precision": 0.22093023255813954,
          "recall": 0.18811881188118812,
          "f1": 0.20320855614973263
        }
      },
      "finetuned_rouge": {
        "rouge_1": {
          "precision": 0.39759036144578314,
          "recall": 0.32673267326732675,
          "f1": 0.3586956521739131
        },
        "rouge_2": {
          "precision": 0.0975609756097561,
          "recall": 0.08,
          "f1": 0.08791208791208793
        },
        "rouge_l": {
          "precision": 0.21686746987951808,
          "recall": 0.1782178217821782,
          "f1": 0.1956521739130435
        }
      }
    },
    {
      "title": "",
      "prompt": "Modeling Actions through State Changes Alireza Fathi and James M. Rehg College of Computing Georgia Institute of Technology {afathi3,rehg}@gatech.edu Abstract In this paper we present a model of action based on the change in the state of the environment. Many actions in- volve similar dynamics and hand-object relationships, but differ",
      "target": "in their purpose and meaning. The key to differentiat- ing these actions is the ability to identify how they change the state of objects and materials in the environment. We propose a weakly supervised method for learning the ob- ject and material states that are necessary for recognizing daily actions. Once these state detectors are learned, we can apply them to input videos and pool their outputs to de- tect actions. We further demonstrate that our method can be used to segment discrete actions from a continuous video of an activity. Our results outperform state-of-the-art action recognition and activity segmentation",
      "base_model_continuation": "but differ- ing in the state changes of the environment. This paper addresses the problem of learning such actions from data. We present a model of action based on the change in the state of the environment. It models actions as a sequence of state changes that occur over time. This model is able to learn actions with varying dynamics and object relationships by using data from a single environment. The model is based on the state of the environment, the state of the hand, and the object. We use this model to",
      "finetuned_model_continuation": "but differ in their state changes. We propose a model that encodes the state of the environment through the state of a deformable object. The state of the object is encoded as a set of state variables that capture the location, orientation and scale of the object. We show that these state variables can be used to represent the state of the environment, and can also be used to predict the future state of the environment. We show that the model can be used to predict the future state of the environment, and to",
      "base_perplexity": 11.647538185119629,
      "finetuned_perplexity": 9.053670883178711,
      "base_bleu": {
        "bleu_1": 0.3118279569892473,
        "bleu_2": 0.06521739130434782,
        "bleu_3": 0.01098901098901099,
        "bleu_4": 0.0
      },
      "finetuned_bleu": {
        "bleu_1": 0.35106382978723405,
        "bleu_2": 0.11827956989247312,
        "bleu_3": 0.05434782608695652,
        "bleu_4": 0.01098901098901099
      },
      "base_rouge": {
        "rouge_1": {
          "precision": 0.3978494623655914,
          "recall": 0.3592233009708738,
          "f1": 0.3775510204081633
        },
        "rouge_2": {
          "precision": 0.08695652173913043,
          "recall": 0.0784313725490196,
          "f1": 0.08247422680412371
        },
        "rouge_l": {
          "precision": 0.24731182795698925,
          "recall": 0.22330097087378642,
          "f1": 0.23469387755102042
        }
      },
      "finetuned_rouge": {
        "rouge_1": {
          "precision": 0.4148936170212766,
          "recall": 0.3786407766990291,
          "f1": 0.39593908629441626
        },
        "rouge_2": {
          "precision": 0.13978494623655913,
          "recall": 0.12745098039215685,
          "f1": 0.13333333333333333
        },
        "rouge_l": {
          "precision": 0.30851063829787234,
          "recall": 0.2815533980582524,
          "f1": 0.29441624365482233
        }
      }
    },
    {
      "title": "",
      "prompt": "Probabilistic Graphlet Cut: Exploiting Spatial Structure Cue for Weakly Supervised Image Segmentation Luming Zhang , Mingli Song , Zicheng Liu , Xiao Liu , Jiajun Bu and Chun Chen College of Computer Science, Zhejiang University, Hang Zhou Microsoft Research, Redmond {zglumg, brooksong, ender liux, bjj, chenc}@zju.edu.cn zliu@microsoft.com Abstract Weakly supervised",
      "target": "image segmentation is a challenging problem in computer vision eld. In this paper, we present a new weakly supervised image segmentation algorithm by learning the distribution of spatially structured superpixel sets from image-level labels. Speci cally, we rst extrac- t graphlets from each image where a graphlet is a small- sized graph consisting of superpixels as its nodes and it en- capsulates the spatial structure of those superpixels. Then, a manifold embedding algorithm is proposed to transfor- m graphlets of different sizes into equal-length feature vec- tors. Thereafter, we use GMM to learn the distribution of the post-embedding graphlets. Finally,",
      "base_model_continuation": "ly supervised image segmentation is a challenging problem that requires a large amount of supervision. We present a new approach to weakly supervised image segmentation by leveraging spatial structure cues. Our approach is based on a probabilistic graphlet cut model. Specifically, we model the segmentation of an image as a probabilistic graphlet cut problem where each pixel is treated as a vertex in the graph and a set of graphlets are selected to describe the local structures. We derive a variational objective that minimizes the Kullback-Leibler divergence between the",
      "finetuned_model_continuation": "ly supervised image segmentation (WSIS) is a challenging problem because of the lack of ground truth labels. This paper presents a probabilistic graphlet cut (PGC) framework for WSIS that exploits spatial structure cues to enhance the labeling quality of weakly labeled images. A novel graphlet-based spatial structure cue is proposed to characterize the spatial structure in weakly labeled images. It is built upon the graphlet structure in the graphlet-based texture descriptor, which is a promising feature for WSIS. Based on the",
      "base_perplexity": 14.750701904296875,
      "finetuned_perplexity": 11.370599746704102,
      "base_bleu": {
        "bleu_1": 0.47191011235955055,
        "bleu_2": 0.13636363636363635,
        "bleu_3": 0.09195402298850575,
        "bleu_4": 0.05813953488372093
      },
      "finetuned_bleu": {
        "bleu_1": 0.3950617283950617,
        "bleu_2": 0.1375,
        "bleu_3": 0.06329113924050633,
        "bleu_4": 0.01282051282051282
      },
      "base_rouge": {
        "rouge_1": {
          "precision": 0.5,
          "recall": 0.4368932038834951,
          "f1": 0.466321243523316
        },
        "rouge_2": {
          "precision": 0.1348314606741573,
          "recall": 0.11764705882352941,
          "f1": 0.1256544502617801
        },
        "rouge_l": {
          "precision": 0.3333333333333333,
          "recall": 0.2912621359223301,
          "f1": 0.310880829015544
        }
      },
      "finetuned_rouge": {
        "rouge_1": {
          "precision": 0.4939759036144578,
          "recall": 0.39805825242718446,
          "f1": 0.44086021505376344
        },
        "rouge_2": {
          "precision": 0.17073170731707318,
          "recall": 0.13725490196078433,
          "f1": 0.15217391304347827
        },
        "rouge_l": {
          "precision": 0.2891566265060241,
          "recall": 0.23300970873786409,
          "f1": 0.2580645161290323
        }
      }
    },
    {
      "title": "",
      "prompt": "A Minimum Error Vanishing Point Detection Approach for Uncalibrated Monocular Images of Man-made Environments Yiliang Xu Sangmin Oh Anthony Hoogs Kitware Inc. 28 Corporate Drive, Clifton Park, NY 12065 {yiliang.xu,sangmin.oh,anthony.hoogs}@kitware.com Abstract We present a novel vanishing point detection algorithm for uncalibrated monocular images of man-made environ- ments. We advance the",
      "target": "state-of-the-art by a new model of measurement error in the line segment extraction and min- imizing its impact on the vanishing point estimation. Our contribution is twofold: 1) Beyond existing hand-crafted models, we formally derive a novel consistency measure, which captures the stochastic nature of the correlation be- tween line segments and vanishing points due to the mea- surement error, and use this new consistency measure to improve the line segment clustering. 2) We propose a novel minimum error vanishing point estimation approach by op- timally weighing the contribution of each line segment pair in the cluster towards the vanishing",
      "base_model_continuation": "advance the state of the art by exploiting the spatial information of the image to robustify the detection and improve the robustness of the detection to image distortion. We also exploit the spatial information to robustify the detection and improve the robustness of the detection to image distortion. The algorithm is robust to illumination changes, and is not affected by the presence of occlusions or cluttered background. The algorithm is also robust to image distortion and to the presence of scale changes in the man-made environment. We evaluate the algorithm on both",
      "finetuned_model_continuation": "advance the state-of-the-art by leveraging the concept of minimum error vanishing points to detect the vanishing points from the images. Our method is based on the well-known feature point selection method, i.e., the minimum error selection method, which selects the feature points that have the smallest error between the corresponding feature points in the original image and the corresponding feature points in the transformed image. We show that the concept of minimum error is useful in our method. Our method is not limited to man-",
      "base_perplexity": 10.105752944946289,
      "finetuned_perplexity": 7.467902660369873,
      "base_bleu": {
        "bleu_1": 0.2717391304347826,
        "bleu_2": 0.04395604395604396,
        "bleu_3": 0.0,
        "bleu_4": 0.0
      },
      "finetuned_bleu": {
        "bleu_1": 0.35294117647058826,
        "bleu_2": 0.09523809523809523,
        "bleu_3": 0.012048192771084338,
        "bleu_4": 0.0
      },
      "base_rouge": {
        "rouge_1": {
          "precision": 0.3118279569892473,
          "recall": 0.27884615384615385,
          "f1": 0.29441624365482233
        },
        "rouge_2": {
          "precision": 0.08695652173913043,
          "recall": 0.07766990291262135,
          "f1": 0.08205128205128204
        },
        "rouge_l": {
          "precision": 0.23655913978494625,
          "recall": 0.21153846153846154,
          "f1": 0.22335025380710657
        }
      },
      "finetuned_rouge": {
        "rouge_1": {
          "precision": 0.4,
          "recall": 0.34615384615384615,
          "f1": 0.3711340206185567
        },
        "rouge_2": {
          "precision": 0.1348314606741573,
          "recall": 0.11650485436893204,
          "f1": 0.125
        },
        "rouge_l": {
          "precision": 0.25555555555555554,
          "recall": 0.22115384615384615,
          "f1": 0.23711340206185566
        }
      }
    },
    {
      "title": "",
      "prompt": "GRASP Recurring Patterns from a Single View Jingchen Liu1 Yanxi Liu1,2 1 Computer Science and Engineering, 2 Electrical Engineering The Pennsylvania State University University Park, PA 16802, USA {jingchen, yanxi}@cse.psu.edu Abstract We propose a novel unsupervised method for discovering recurring patterns from a single view. A key contribution of our",
      "target": "approach is the formulation and validation of a joint as- signment optimization problem where multiple visual words and object instances of a potential recurring pattern are considered simultaneously. The optimization is achieved by a greedy randomized adaptive search procedure (GRASP) with moves speci cally designed for fast convergence. We have quanti ed systematically the performance of our ap- proach under stressed conditions of the input (missing fea- tures, geometric distortions). We demonstrate that our pro- posed algorithm outperforms state of the art methods for recurring pattern discovery on a diverse set of 400+ real world and synthesized test images. 1.",
      "base_model_continuation": "of our method is to combine the view-based information with the pattern-based information to achieve a better pattern discovery. Our method discovers recurring patterns from a single view by analyzing the view-based information and pattern-based information together. We use a new measure called the view-based information to measure the view-based information of a pattern. The view-based information measures the view-based information of a pattern as a function of the view-based information of the view. We use a new measure called",
      "finetuned_model_continuation": "of our work is the development of a new graph-based pattern discovery algorithm which allows for the discovery of recurring patterns from a single view. The algorithm builds on the graph-based pattern discovery method proposed by [1]. The key difference is that our method is able to handle arbitrary patterns, such as regular and irregular, while the method in [1] can only handle regular patterns. The algorithm also enables us to deal with a variety of different patterns. We conduct extensive experiments on both synthetic and real images.",
      "base_perplexity": 12.870098114013672,
      "finetuned_perplexity": 10.145354270935059,
      "base_bleu": {
        "bleu_1": 0.3125,
        "bleu_2": 0.06329113924050633,
        "bleu_3": 0.0,
        "bleu_4": 0.0
      },
      "finetuned_bleu": {
        "bleu_1": 0.3522727272727273,
        "bleu_2": 0.05747126436781609,
        "bleu_3": 0.0,
        "bleu_4": 0.0
      },
      "base_rouge": {
        "rouge_1": {
          "precision": 0.3258426966292135,
          "recall": 0.29,
          "f1": 0.3068783068783069
        },
        "rouge_2": {
          "precision": 0.07954545454545454,
          "recall": 0.0707070707070707,
          "f1": 0.0748663101604278
        },
        "rouge_l": {
          "precision": 0.16853932584269662,
          "recall": 0.15,
          "f1": 0.15873015873015872
        }
      },
      "finetuned_rouge": {
        "rouge_1": {
          "precision": 0.37777777777777777,
          "recall": 0.34,
          "f1": 0.3578947368421052
        },
        "rouge_2": {
          "precision": 0.06741573033707865,
          "recall": 0.06060606060606061,
          "f1": 0.06382978723404256
        },
        "rouge_l": {
          "precision": 0.2111111111111111,
          "recall": 0.19,
          "f1": 0.2
        }
      }
    },
    {
      "title": "",
      "prompt": "Fine-grained Categorization and Dataset Bootstrapping using Deep Metric Learning with Humans in the Loop Yin Cui1,2 Feng Zhou3 Yuanqing Lin3 Serge Belongie1,2 1Department of Computer Science, Cornell University 2Cornell Tech 3NEC Labs America 1,2{ycui, sjb}@cs.cornell.edu 3{feng, ylin}@nec-labs.com Abstract Existing ne-grained visual categorization methods of- ten suffer from three challenges: lack",
      "target": "of training data, large number of ne-grained categories, and high intra- class vs. low inter-class variance. In this work we pro- pose a generic iterative framework for ne-grained catego- rization and dataset bootstrapping that handles these three challenges. Using deep metric learning with humans in the loop, we learn a low dimensional feature embedding with anchor points on manifolds for each category. These an- chor points capture intra-class variances and remain dis- criminative between classes. In each round, images with high con dence scores from our model are sent to humans for labeling. By comparing with exemplar images, labelers mark",
      "base_model_continuation": ": lack of human annotation, lack of large-scale dataset, and lack of strong evaluation metric. To address these challenges, we propose a human-in-the-loop approach that leverages human-annotated training samples to bootstrap the large-scale dataset for fine-grained categorization. The bootstrapped dataset is further augmented with human-annotated samples, and fine-grained categorization is achieved by deep metric learning with the augmented dataset. We evaluate our method on the recently released CUB-200-",
      "finetuned_model_continuation": ": lack of datasets, lack of training data, and lack of supervision. To address these challenges, we propose a new dataset, the Fine-grained Image Categorization Benchmark (F- GICB), which contains 20,000 images from 3500 ca- tegories. We introduce a novel metric learning framework that is designed to handle the above challenges. The proposed method consists of two steps: (1) we train a deep metric learning model with humans in the loop (",
      "base_perplexity": 12.802020072937012,
      "finetuned_perplexity": 7.908781051635742,
      "base_bleu": {
        "bleu_1": 0.3235294117647059,
        "bleu_2": 0.04477611940298507,
        "bleu_3": 0.030303030303030304,
        "bleu_4": 0.015384615384615385
      },
      "finetuned_bleu": {
        "bleu_1": 0.3333333333333333,
        "bleu_2": 0.09859154929577464,
        "bleu_3": 0.05714285714285714,
        "bleu_4": 0.014492753623188406
      },
      "base_rouge": {
        "rouge_1": {
          "precision": 0.4025974025974026,
          "recall": 0.2980769230769231,
          "f1": 0.34254143646408836
        },
        "rouge_2": {
          "precision": 0.09210526315789473,
          "recall": 0.06796116504854369,
          "f1": 0.0782122905027933
        },
        "rouge_l": {
          "precision": 0.19480519480519481,
          "recall": 0.14423076923076922,
          "f1": 0.16574585635359115
        }
      },
      "finetuned_rouge": {
        "rouge_1": {
          "precision": 0.4166666666666667,
          "recall": 0.28846153846153844,
          "f1": 0.3409090909090909
        },
        "rouge_2": {
          "precision": 0.11267605633802817,
          "recall": 0.07766990291262135,
          "f1": 0.09195402298850575
        },
        "rouge_l": {
          "precision": 0.2638888888888889,
          "recall": 0.18269230769230768,
          "f1": 0.2159090909090909
        }
      }
    }
  ]
}