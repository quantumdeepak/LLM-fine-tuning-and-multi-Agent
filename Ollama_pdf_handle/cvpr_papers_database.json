[
  {
    "title": "Learning a Sequential Search for Landmarks\n---AUTHORs---\nSaurabh Singh\nDerek Hoiem\nDavid Forsyth",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Singh_Learning_a_Sequential_2015_CVPR_paper.pdf",
    "id": "Singh_Learning_a_Sequential_2015_CVPR_paper",
    "abstract": "We propose a general method to find landmarks in images of objects using both appearance and spatial context. This method is applied without changes to two problems: parsing human body layouts, and finding landmarks in images of birds. Our method learns a sequential search for localizing landmarks, iteratively detecting new landmarks given the appearance and contextual information from the already detected ones. The choice of landmark to be added is opportunistic and depends on the image; for example, in one image a head-shoulder group might be expanded to a head-shoulder-hip group but in a different image to a head-shoulder-elbow group. The choice of initial landmark is similarly image dependent. Groups are scored using a learned function, which is used to expand them greedily. Our scoring function is learned from data labelled with landmarks but without any labeling of a detection order. Our method represents a novel spatial model for the kinematics of groups of landmarks, and displays strong performance on two different model problems.\n\n---TOPIC---\nLandmark Detection\nSequential Search\nSpatial Context\nImage-Dependent Order\nKinematics",
    "topics": [],
    "references": [
      {
        "citation": "[M. Andriluka, S. Roth, and B. Schiele. Pictorial structures revisited: People detection and articulated pose estimation. In CVPR, 2009.]"
      },
      {
        "citation": "[A. G. Barto. Reinforcement learning: An introduction. MIT press, 1998.]"
      },
      {
        "citation": "[P. F. Felzenszwalb and D. P. Huttenlocher. Pictorial structures for object recognition. IJCV, 61(1):55–79, January 2005.]"
      },
      {
        "citation": "[R. Fergus, P. Perona, and A. Zisserman. Object Class Recognition by Unsupervised Scale-Invariant Learning. CVPR, 2003.]"
      },
      {
        "citation": "[P. Doll´ar, Z. Tu, P. Perona, and S. Belongie. Integral channel features. In BMVC, 2009.]"
      },
      {
        "citation": "[M. Eichner and V. Ferrari. Appearance sharing for collective human pose estimation. In ACCV, 2012.]"
      },
      {
        "citation": "[L. Fei-Fei, R. Fergus, and P. Perona. One-shot learning of object categories. 28(4):594–611, 2006.]"
      },
      {
        "citation": "[P. F. Felzenszwalb, R. B. Girshick, and D. McAllester. Cascade object detection with deformable part models. In CVPR, 2010.]"
      },
      {
        "citation": "[R. Fergus, P. Perona, and A. Zisserman. A sparse object category model for efﬁcient learning and exhaustive recognition. In CVPR, 2005.]"
      },
      {
        "citation": "[Y. Wang and G. Mori. Multiple tree models for occlusion and spatial constraints in human pose estimation. In ECCV, 2008.]"
      }
    ],
    "author_details": [
      {
        "name": "Saurabh Singh",
        "affiliation": "University of Illinois, Urbana-Champaign",
        "email": "ss1@illinois.edu"
      },
      {
        "name": "Derek Hoiem",
        "affiliation": "University of Illinois, Urbana-Champaign",
        "email": "dhoiem@illinois.edu"
      },
      {
        "name": "David Forsyth",
        "email": "daf@illinois.edu"
      }
    ]
  },
  {
    "title": "DeepEdge: A Multi-Scale Bifurcated Deep Network for Top-Down Contour Detection\n---AUTHOR---\nGedas Bertasius\nJianbo Shi\nLorenzo Torresani",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Bertasius_DeepEdge_A_Multi-Scale_2015_CVPR_paper.pdf",
    "id": "Bertasius_DeepEdge_A_Multi-Scale_2015_CVPR_paper",
    "abstract": "Contour detection has traditionally relied on low-level features for object detection. This paper challenges this approach, proposing a novel method that exploits object-related features as high-level cues for contour detection. The authors introduce DeepEdge, a multi-scale deep network that reuses features from a pre-trained object classification network (KNet) and incorporates a bifurcated sub-network to predict contour likelihood and the fraction of human labelers agreeing on contour presence. The method achieves state-of-the-art results in contour detection without feature engineering.\n\n---TOPIC---\nContour Detection\nDeep Learning\nMulti-Scale Networks\nObject Recognition\nTop-Down Approach",
    "topics": [],
    "references": [
      {
        "citation": "[Arbeláez, P., Maire, M., Fowlkes, C., & Malik, J. (2011). Contour detection and hierarchical image segmentation. *IEEE Transactions on Pattern Analysis and Machine Intelligence*, *33*(5), 898–916.]"
      },
      {
        "citation": "[Lim, J., Zitnick, C. L., & Dollár, P. (2013). Sketch tokens: A learned mid-level representation for contour and object detection. *CVPR*.]"
      },
      {
        "citation": "[Long, J., Shelhamer, E., & Darrell, T. (2014). Fully convolutional networks for semantic segmentation. *CoRR*, abs/1411.4038.]"
      },
      {
        "citation": "[Malik, J., Belongie, S., Leung, T., & Shi, J. (2001). Contour and texture analysis for image segmentation. *International Journal of Computer Vision*, *43*(1), 22–27.]"
      },
      {
        "citation": "[Girshick, R., Donahue, J., Darrell, T., & Malik, J. (2014). Rich feature hierarchies for accurate object detection and semantic segmentation. *CVPR*.]"
      },
      {
        "citation": "[Hariharan, B., Arbeláez, P., Girshick, R., & Malik, J. (2014). Hypercolumns for object segmentation and fine-grained localization. *CoRR*, abs/1411.5752.]"
      },
      {
        "citation": "[Iandola, F. N., Moskewicz, M. W., Karayev, S., Girshick, R. B., Darrell, T., & Keutzer, K. (2014). Densenet: Implementing efficient convnet descriptor pyramids. *CoRR*, abs/1404.1869.]"
      },
      {
        "citation": "[Isola, P., Zoran, D., Krishnan, D., & Adelson, E. H. (2014). Crisp boundary detection using pointwise mutual information. *ECCV*.]"
      },
      {
        "citation": "[Jia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick, R., Guadarrama, S., & Darrell, T. (2014). Caffe: Convolutional architecture for fast feature embedding. *arXiv preprint arXiv:1408.5093*.]"
      },
      {
        "citation": "[Ren, X., Fowlkes, C., & Malik, J. (2013). Scale-invariant contour completion using condition random fields. *Technical report*.]"
      }
    ],
    "author_details": [
      {
        "name": "Gedas Bertasius",
        "affiliation": "University of Pennsylvania",
        "email": "gberta@seas.upenn.edu"
      },
      {
        "name": "Jianbo Shi",
        "affiliation": "University of Pennsylvania",
        "email": "jshi@seas.upenn.edu"
      },
      {
        "name": "Lorenzo Torresani",
        "affiliation": "Dartmouth College",
        "email": "lt@dartmouth.edu"
      }
    ]
  },
  {
    "title": "Discriminant Analysis on Riemannian Manifold of Gaussian Distributions for Face Recognition with Image Sets\n---AUTHOR---\nWen Wang\nRuiping Wang\nZhiwu Huang\nShiguan Shan\nXilin Chen",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Wang_Discriminant_Analysis_on_2015_CVPR_paper.pdf",
    "id": "Wang_Discriminant_Analysis_on_2015_CVPR_paper",
    "abstract": "To address the problem of face recognition with image sets, this paper proposes a novel method called Discriminant Analysis on Riemannian manifold of Gaussian distributions (DARG). The method aims to capture the underlying data distribution in each set and facilitate more robust classification by representing image sets as Gaussian Mixture Models (GMMs). Recognizing that Gaussian distributions lie on a specific Riemannian manifold, the paper derives provably positive definite probabilistic kernels to encode this geometry and utilizes a weighted Kernel Discriminant Analysis to discriminate Gaussian components from different subjects, incorporating their prior probabilities. The proposed method is evaluated on four challenging face recognition databases and demonstrates superior performance compared to state-of-the-art approaches.\n\n---TOPICCS---\nFace Recognition\nGaussian Mixture Models (GMM)\nRiemannian Manifold\nDiscriminant Analysis\nKernel Methods",
    "topics": [],
    "references": [
      {
        "citation": "[Arandi, O., Shakhnarovich, G., Fisher, J., Cipolla, R., & Darrell, T. (2005). Face recognition with image sets using manifold density divergence. In *IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)*.]"
      },
      {
        "citation": "[Amar, S., & Nagaoka, H. (2000). *Methods of Information Geometry*. Translations of Mathematical Monographs. Oxford University Press.]"
      },
      {
        "citation": "[Chan, A. B., Vasconcelos, N., & Moreno, P. J. (2004). A family of probabilistic kernels based on information divergence. University of California, San Diego, CA, Tech. Rep. SVCL-TR-2004-1.]"
      },
      {
        "citation": "[Cevikalp, H., & Triggs, B. (2010). Face recognition based on image sets. In *IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)*.]"
      },
      {
        "citation": "[Cui, Z., Shan, S., Zhang, H., Lao, S., & Chen, X. (2012). Image sets alignment for video-based face recognition. *IEEE Computer Society on Computer Vision and Pattern Recognition (CVPR)*.]"
      },
      {
        "citation": "[Hamm, J., & Lee, D. D. (2008). Grassmann discriminant analysis: a unifying view on subspace-based learning. In *Proceedings of the 25th international conference on Machine learning (ICML)*.]"
      },
      {
        "citation": "[Hu, Y., Mian, A. S., & Owens, R. (2011). Sparse approximated nearest points for image set classification. In *IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)*.]"
      },
      {
        "citation": "[Harandi, M. T., Salzmann, M., Jayasumana, S., Hartley, R., & Li, H. (n.d.). Expanding the family of grasmannian kernels: An.]"
      },
      {
        "citation": "[Jayasumana, S., Hartley, R., Salzmann, M., Li, H., & Harandi, M. T. (2013). Kernel methods on the riemannian manifold of symmetric positive deﬁnite matrices. In *IEEE Computer Society on Computer Vision and Pattern Recognition (CVPR)*.]"
      },
      {
        "citation": "[Kim, M., Kumar, S., Pavlovic, V., & Rowley, H. (2008). Face tracking and recognition with visual constraints in real-world videos. In *IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)*.]"
      }
    ],
    "author_details": [
      {
        "name": "Wen Wang",
        "affiliation": "Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing, 100190, China",
        "email": "wen.wang@vipl.ict.ac.cn"
      },
      {
        "name": "Ruiping Wang",
        "affiliation": "Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing, 100190, China",
        "email": "wangruiping@ict.ac.cn"
      },
      {
        "name": "Zhiwu Huang",
        "affiliation": "Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing, 100190, China",
        "email": "zhiwu.huang@vipl.ict.ac.cn"
      },
      {
        "name": "Shiguan Shan",
        "affiliation": "Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing, 100190, China",
        "email": "sgshan@ict.ac.cn"
      },
      {
        "name": "Xilin Chen",
        "affiliation": "Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing, 100190, China",
        "email": "xlchen@ict.ac.cn"
      }
    ]
  },
  {
    "title": "Super-resolution Person Re-identiﬁcation with Semi-coupled Low-rank Discriminant Dictionary Learning\n---AUTHOR---\nXiao-Yuan Jing\nXiaoke Zhu\nFei Wu\nXinge You\nQinglong Liu\nDong Yue\nRuimin Hu\nBaowen Xu",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Jing_Super-Resolution_Person_Re-Identification_2015_CVPR_paper.pdf",
    "id": "Jing_Super-Resolution_Person_Re-Identification_2015_CVPR_paper",
    "abstract": "Person re-identiﬁcation has been widely studied due to its importance in surveillance and forensics applications. In practice, gallery images are high-resolution (HR) while probe images are usually low-resolution (LR) in the identification scenarios with large variation of illumination, weather or quality of cameras. This paper addresses super-resolution (S-R) person re-identiﬁcation, which has not been well studied. The authors propose a semi-coupled low-rank discriminant dictionary learning (SLD2L) approach for SR person re-identiﬁcation. The approach learns a pair of HR and LR dictionaries and a mapping from the features of HR gallery images and LR probe images, aiming to convert LR probe image features into discriminating HR features. A discriminant term ensures converted features are close to same-person HR gallery features and far from different-person HR gallery features. Low-rank regularization is applied to characterize the intrinsic feature space of HR and LR images. Experimental results on public datasets demonstrate the effectiveness of SLD2L.",
    "topics": [
      "Super-resolution person re-identification",
      "Low-rank discriminant dictionary learning",
      "Semi-coupled dictionaries",
      "Feature representation learning",
      "Person re-identification"
    ],
    "references": [
      {
        "citation": "[Bak, S., Corvee, E., Brémond, F., & Thonnat, M. (2010). Person re-identiﬁcation using haar-based and dcd-based signature. *Advanced Video and Signal Based Surveillance (AVSS), IEEE Conference on*, 1–8.]"
      },
      {
        "citation": "[Bedagkar-Gala, A., & Shah, S. K. (2014). A survey of approaches and trends in person re-identiﬁcation. *Image and Vision Computing*, *32*(4), 270–286.]"
      },
      {
        "citation": "[Liu, X., Song, M., Tao, D., Zhou, X., Chen, C., & Bu, J. (2014). Semi-supervised coupled dictionary learning for person re-identiﬁcation. *CVPR, IEEE Conference on*, 3550–3557.]"
      },
      {
        "citation": "[Ma, L., Wang, C., Xiao, B., & Zhou, W. (2012). Sparse representation for face recognition based on discriminative low-rank dictionary learning. *CVPR, IEEE Conference on*, 2586–2593.]"
      },
      {
        "citation": "[Gray, D., Brennan, S., & Tao, H. (2007). Evaluating appearance models for recognition, reacquisition, and tracking. *Performance Evaluation of Tracking and Surveillance, IEEE workshop on*.]"
      },
      {
        "citation": "[Gray, D., & Tao, H. (2008). Viewpoint invariant pedestrian recognition with an ensemble of localized features. *ECCV*, 262–275.]"
      },
      {
        "citation": "[Ma, L., Yang, X., & Tao, D. (2014). Person re-identiﬁcation over camera networks using multi-task distance metric learning. *Image Processing, IEEE Transactions on*, *23*(8), 3656–3670.]"
      },
      {
        "citation": "[Yang, J., Wright, J., Huang, T. S., & Ma, Y. (2010). Image super-resolution via sparse representation. *Image Processing, IEEE Transactions on*, *19*(11), 2861–2873.]"
      },
      {
        "citation": "[Hirzer, M., Beleznai, C., Roth, P. M., & Bischof, H. (2011). Person re-identiﬁcation by descriptive and discriminative classification. *Image Analysis*, 91–102.]"
      },
      {
        "citation": "[Zheng, W.-S., Gong, S., & Xiang, T. (2013). Reidentiﬁcation by relative distance comparison. *Pattern Analysis and Machine Intelligence, IEEE Transactions on*, *35*(3), 653–668.]"
      }
    ],
    "author_details": [
      {
        "name": "Xiao-Yuan Jing",
        "affiliation": "State Key Laboratory of Software Engineering, School of Computer, Wuhan University, China",
        "email": "Not available"
      },
      {
        "name": "Xiaoke Zhu",
        "affiliation": "State Key Laboratory of Software Engineering, School of Computer, Wuhan University, China",
        "email": "Not available"
      },
      {
        "name": "Fei Wu",
        "affiliation": "State Key Laboratory of Software Engineering, School of Computer, Wuhan University, China",
        "email": "Not available"
      },
      {
        "name": "Xinge You",
        "affiliation": "School of Electronic Information and Communications, Huazhong University of Science and Technology, China",
        "email": "Not available"
      },
      {
        "name": "Qinglong Liu",
        "affiliation": "State Key Laboratory of Software Engineering, School of Computer, Wuhan University, China",
        "email": "Not available"
      },
      {
        "name": "Dong Yue",
        "affiliation": "College of Automation, Nanjing University of Posts and Telecommunications, China",
        "email": "Not available"
      },
      {
        "name": "Ruimin Hu",
        "affiliation": "National Engineering Research Center for Multimedia Software, School of Computer, Wuhan University, China",
        "email": "Not available"
      },
      {
        "name": "Baowen Xu",
        "affiliation": "State Key Laboratory of Software Engineering, School of Computer, Wuhan University, China",
        "email": "Not available"
      }
    ]
  },
  {
    "title": "From Image-level to Pixel-level Labeling with Convolutional Networks\n---AUTHOR---\nPedro O. Pinheiro\nRonan Collobert",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Pinheiro_From_Image-Level_to_2015_CVPR_paper.pdf",
    "id": "Pinheiro_From_Image-Level_to_2015_CVPR_paper",
    "abstract": "We are interested in inferring object segmentation by leveraging only object class information, and by considering only minimal priors on the object segmentation task. This problem could be viewed as a kind of weakly supervised segmentation task, and naturally fits the Multiple Instance Learning (MIL) framework: every training image is known to have (or not) at least one pixel corresponding to the image class label, and the segmentation task can be rewritten as inferring the pixels belonging to the class of the object (given one image, and its object class). We propose a Convolutional Neural Network-based model, which is constrained during training to put more weight on pixels which are important for classifying the image. We show that at test time, the model has learned to discriminate the right pixels well enough, such that it performs very well on an existing segmentation benchmark, by adding only few smoothing priors. Our system is trained using a subset of the Imaginet dataset and the segmentation experiments are performed on the challenging Pascal VOC dataset (with no fine-tuning of the model on Pascal VOC). Our model beats the state of the art results in weakly supervised object segmentation task by a large margin. We also compare the performance of our model with state of the art fully-supervised segmentation approaches.",
    "topics": [
      "Convolutional Neural Networks (CNNs)",
      "Weakly Supervised Segmentation",
      "Multiple Instance Learning (MIL)",
      "Image-level Training",
      "Object Segmentation"
    ],
    "references": [
      {
        "citation": "[Arbeláez, P., Pont-Tuset, J., Barron, J., Marques, F., & Malik, J. (2009). Multiscale combinatorial grouping. In *IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*.]"
      },
      {
        "citation": "[Boyd, S., & Vandenberghe, L. (2004). *Convex optimization*. Cambridge University Press.]"
      },
      {
        "citation": "[Bridle, J. S. (1990). Probabilistic interpretation of feedforward classification network outputs, with relationships to statistical pattern recognition. *Neurocomputing: Algorithms, Architectures and Applications*.]"
      },
      {
        "citation": "[Farabet, C., Couprié, C., Najman, L., & LeCun, Y. (2013). Learning hierarchical features for scene labeling. *IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)*.]"
      },
      {
        "citation": "[Felzenszwalb, P. F., & Huttenlocher, D. P. (2004). Efficient graph-based image segmentation. *International Journal of Computer Vision (IJCV)*.]"
      },
      {
        "citation": "[Girshick, R., Donahue, J., Darrell, T., & Malik, J. (2014). Rich feature hierarchies for accurate object detection and semantic segmentation. In *IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*.]"
      },
      {
        "citation": "[Hariharan, B., Arbeláez, P., Girshick, R., & Malik, J. (2014). Simultaneous detection and segmentation. In *Proceedings of the European Conference on Computer Vision (ECCV)*.]"
      },
      {
        "citation": "[Krizhevsky, A., Sutskever, I., & Hinton, G. (2012). ImageNet classification with deep convolutional neural networks. In *Advances in Neural Information Processing Systems (NIPS)*.]"
      },
      {
        "citation": "[LeCun, Y., Bottou, L., Bengio, Y., & Haffner, P. (1998). Gradient-based learning applied to document recognition. *Proceedings of the IEEE*.]"
      },
      {
        "citation": "[Maron, O., & Lozano-Pérez, T. (1998). A framework for multiple instance learning. In *Advances in Neural Information Processing Systems (NIPS)*.]"
      }
    ],
    "author_details": [
      {
        "name": "Pedro O. Pinheiro",
        "affiliation": "Idiap Research Institute, Martigny, Switzerland",
        "email": "pedro@opinheiro.com"
      },
      {
        "name": "Ronan Collobert",
        "affiliation": "Facebook AI Research, Menlo Park, CA, USA",
        "email": "ronan@collobert.com"
      }
    ]
  },
  {
    "title": "Deep Sparse Representation for Robust Image Registration\n---AUTHOR---\nYeqing Li\nChen Chen\nFei Yang\nJunzhou Huang",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Li_Deep_Sparse_Representation_2015_CVPR_paper.pdf",
    "id": "Li_Deep_Sparse_Representation_2015_CVPR_paper",
    "abstract": "This paper proposes a novel similarity measure for image registration based on the concept of deep sparse representation. The method aims to achieve subpixel-level accuracy and robustness to severe intensity distortions commonly found in medical, remotely sensed, and natural images. The proposed similarity measure sparsifies images in the gradient and frequency domains, separating sparse error tensors. Two efficient algorithms are developed for batch and pair registration, validated on challenging datasets, and shown to outperform traditional and state-of-the-art methods in terms of robustness, accuracy, and efficiency. The method addresses limitations of existing approaches that struggle with spatially-varying intensity distortions.\n\n---TOPICCS---\nImage Registration\nSparse Representation\nSimilarity Measures\nIntensity Distortions\nDeep Learning (implied by \"deep sparse representation\")",
    "topics": [],
    "references": [
      {
        "citation": "[Peng, Y., Ganesh, A., Wright, J., Xu, W., & Ma, Y. (2012). RASL: Robust alignment by sparse and low-rank decomposition for linearly correlated images. *IEEE Transactions on Pattern Analysis and Machine Intelligence*, *34*(11), 2233–2246.]"
      },
      {
        "citation": "[Sotiras, A., Davatzikos, C., & P Aragios, N. (2013). Deformable medical image registration: A survey. *IEEE Transactions on Medical Imaging*, *32*(7), 1153–1190.]"
      },
      {
        "citation": "[Candès, E. J., Li, X., Ma, Y., & Wright, J. (2011). Robust principal component analysis?. *Journal of the ACM*, *58*(3), 11.]"
      },
      {
        "citation": "[Cohen, B., & Dinstein, I. (2002). New maximum likelihood motion estimation schemes for noisy ultrasound images. *Pattern Recognition*, *35*(2), 455–463.]"
      },
      {
        "citation": "[Thomas, C., Ranchin, T., Wald, L., & Chanussot, J. (2008). Synthesis of multispectral images to high spatial resolution: A critical review of fusion methods based on remote sensing physics. *IEEE Transactions on Geoscience and Remote Sensing*, *46*(5), 1301–1312.]"
      },
      {
        "citation": "[Zheng, Y., Daniel, E., Hunter III, A. A., Xiao, R., Gao, J., Li, H., Maguire, M. G., Brainard, D. H., & Gee, J. C. (2014). Landmark matching based retinal image alignment by enforcing sparsity in correspondence matrix. *Medical image analysis*, *18*(6), 903–913.]"
      },
      {
        "citation": "[Tzimiropouulos, G., Argyriou, V., Zafeiriou, S., & Stathaki, T. (2010). Robust FFT-based scale-invariant image registration with image gradients. *IEEE Transactions on Pattern Analysis and Machine Intelligence*, *32*(10), 1899–1906.]"
      },
      {
        "citation": "[Viola, P., & Wells III, W. M. (1997). Alignment by maximization of mutual information. *International Journal of Computer Vision*, *24*(2), 137–154.]"
      },
      {
        "citation": "[Gross, R., Matthews, I., Cohn, J., Kanade, T., & Baker, S. (2010). Multi-pie. *Image and Vision Computing*, *28*(5), 807–813.]"
      },
      {
        "citation": "[Zitova, B., & Flusser, J. (2003). Image registration methods: a survey. *Image and vision computing*, *21*(11), 977–1000.]"
      },
      {
        "citation": "---"
      }
    ],
    "author_details": [
      {
        "name": "Yeqing Li",
        "affiliation": "University of Texas at Arlington",
        "email": "*Email address not available in the provided text*"
      },
      {
        "name": "Chen Chen",
        "affiliation": "University of Texas at Arlington",
        "email": "*Email address not available in the provided text*"
      },
      {
        "name": "Fei Yang",
        "affiliation": "Facebook Inc.",
        "email": "*Email address not available in the provided text*"
      },
      {
        "name": "Junzhou Huang",
        "affiliation": "University of Texas at Arlington",
        "email": "*Email address not available in the provided text*"
      }
    ]
  },
  {
    "title": "Learning Deep Representations for Ground-to-Aerial Geolocalization\n---AUTHORISTS---\nTsung-Yi Lin\nYin Cui\nSerge Belongie\nJames Hays",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Lin_Learning_Deep_Representations_2015_CVPR_paper.pdf",
    "id": "Lin_Learning_Deep_Representations_2015_CVPR_paper",
    "abstract": "The recent availability of geo-tagged images and geospatial data has spurred image-based geolocalization algorithms. Most approaches predict location by matching query images to ground-level images with known locations. However, ground-level imagery is often unavailable. This paper addresses this limitation by localizing ground-level query images by matching them to a reference database of aerial imagery. The authors introduce Where-CNN, a deep learning approach inspired by face verification, and create a dataset of 78K aligned cross-view image pairs. The method learns a feature representation where matching views are close and mismatched views are far apart, demonstrating significant improvements over traditional and existing deep features and generalizing to novel locations.\n\n---TOPICICS---\nGeolocalization\nAerial Imagery\nDeep Learning\nCross-View Matching\nFeature Representation",
    "topics": [],
    "references": [
      {
        "citation": "[Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). Imagenet classification with deep convolutional neural networks. In NIPS.]"
      },
      {
        "citation": "[Anguelov, D., Dulong, C., Filip, D., Frueh, C., Lafon, S., Lyon, R., Ogale, A., Vincent, L., & Weaver, J. (2010). Google street view: Capturing the world at street level. Computer.]"
      },
      {
        "citation": "[Lowe, D. G. (2004). Distinctive image features from scale-invariant keypoints. IJCV.]"
      },
      {
        "citation": "[Taigman, Y., Yang, M., Ranzato, M., & Wolf, L. (2014). Deepface: Closing the gap to human-level performance in face verification. In CVPR.]"
      },
      {
        "citation": "[Chopra, S., Hadsell, R., & LeCun, Y. (2005). Learning a similarity metric discriminatively, with application to face verification. In CVPR.]"
      },
      {
        "citation": "[Lin, T.-Y., Belongie, S., & Hays, J. (2013). Cross-view image geolocalization. In CVPR.]"
      },
      {
        "citation": "[Bansal, M., & Daniilidis, K. (2014). Geometric urban geo-localization. In CVPR.]"
      },
      {
        "citation": "[van der Maaten, L., & Hinton, G. (2008). Visualizing high-dimensional data using t-sne. In JMLR.]"
      },
      {
        "citation": "[Xiao, J., Hays, J., Ehinger, K. A., Oliva, A., & Torralba, A. (2010). Sun database: Large-scale scene recognition from abbey to zoo. In CVPR.]"
      },
      {
        "citation": "[Felzenszwalb, P. F., Girshick, R. B., McAllester, D., & Ramanan, D. (2010). Object detection with discriminatively trained part-based models. PAMI.]"
      }
    ],
    "author_details": [
      {
        "name": "Tsung-Yi Lin",
        "affiliation": "Cornell Tech",
        "email": "tl483@cornell.edu"
      },
      {
        "name": "Yin Cui",
        "affiliation": "Cornell Tech",
        "email": "yc984@cornell.edu"
      },
      {
        "name": "Serge Belongie",
        "affiliation": "Cornell Tech",
        "email": "sjb344@cornell.edu"
      },
      {
        "name": "James Hays",
        "affiliation": "Brown University",
        "email": "hays@cs.brown.edu"
      }
    ]
  },
  {
    "title": "Rotating Your Face Using Multi-task Deep Neural Network\n---AUTHOR---\nJunho Yim\nHeechul Jung\nByungIn Yoo\nChangkyu Choi\nDusik Park\nJunmo Kim",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Yim_Rotating_Your_Face_2015_CVPR_paper.pdf",
    "id": "Yim_Rotating_Your_Face_2015_CVPR_paper",
    "abstract": "Face recognition under viewpoint and illumination changes is a difficult problem. This paper proposes a new deep architecture based on a novel type of multitask learning, which can achieve superior performance in rotating to a target-pose face image from an arbitrary pose and illumination image while preserving identity. The target pose can be controlled by the user’s intention. This novel type of multi-task model significantly improves identity preservation over the single task model. By using all the synthesized controlled pose images, called Controlled Pose Image (CPI), for the pose-illumination- invariant feature and voting among the multiple face recognition results, the proposed method outperforms state-of-the-art algorithms by more than 4∼6% on the MultiPIE dataset.\n\n---TOPICICS---\nDeep Neural Networks\nMultitask Learning\nFace Recognition\nPose Estimation\nImage Rotation",
    "topics": [],
    "references": [],
    "author_details": [
      {
        "name": "Junho Yim",
        "affiliation": "School of Electrical Engineering, KAIST",
        "email": "junho.yim@kaist.ac.kr"
      },
      {
        "name": "Heechul Jung",
        "affiliation": "School of Electrical Engineering, KAIST",
        "email": "heechul@kaist.ac.kr"
      },
      {
        "name": "ByungIn Yoo",
        "affiliation": "School of Electrical Engineering, KAIST; Samsung Advanced Institute of Technology",
        "email": "byungin.yoo@samsung.com"
      },
      {
        "name": "Changkyu Choi",
        "affiliation": "Samsung Advanced Institute of Technology",
        "email": "changkyu choi@samsung.com"
      },
      {
        "name": "Dusik Park",
        "affiliation": "Samsung Advanced Institute of Technology",
        "email": "dusikpark@samsung.com"
      },
      {
        "name": "Junmo Kim",
        "affiliation": "School of Electrical Engineering, KAIST",
        "email": "junmo.kim@kaist.ac.kr"
      }
    ]
  },
  {
    "title": "CIDEr: Consensus-based Image Description Evaluation\n---AUTHOR---\nRamakrishna Vedantam\nC. Lawrence Zitnick\nDevi Parikh",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Vedantam_CIDEr_Consensus-Based_Image_2015_CVPR_paper.pdf",
    "id": "Vedantam_CIDEr_Consensus-Based_Image_2015_CVPR_paper",
    "abstract": "Automatically describing an image with a sentence is a long-standing challenge in computer vision and natural language processing. Due to recent progress in object detection, attribute classification, action recognition, etc., there is renewed interest in this area. However, evaluating the quality of descriptions has proven to be challenging. We propose a novel paradigm for evaluating image descriptions that uses human consensus. This paradigm consists of three main parts: a new triplet-based method of collecting human annotations to measure consensus, a new automated metric that captures consensus, and two new datasets: PASCAL-50S and ABSTRACT-50S that contain 50 sentences describing each image. Our simple metric captures human judgment of consensus better than existing metrics across sentences generated by various sources. We also evaluate five state-of-the-art image description approaches using this new protocol and provide a benchmark for future comparisons. A version of CIDEr named CIDEr-D is available as a part of MS COCO evaluation server to enable systematic evaluation and benchmarking.",
    "topics": [
      "Image Description Evaluation",
      "Human Consensus",
      "Automated Metrics",
      "CIDEr (and CIDEr-D)",
      "Natural Language Processing & Computer Vision"
    ],
    "references": [],
    "author_details": [
      {
        "name": "Ramakrishna Vedantam",
        "affiliation": "Virginia Tech",
        "email": "vrama91@vt.edu"
      },
      {
        "name": "C. Lawrence Zitnick",
        "affiliation": "Microsoft Research",
        "email": "larryz@microsoft.com"
      },
      {
        "name": "Devi Parikh",
        "affiliation": "Virginia Tech",
        "email": "parikh@vt.edu"
      }
    ]
  },
  {
    "title": "Beyond Gaussian Pyramid: Multi-skip Feature Stacking for Action Recognition\n---AUTHOR---\nZhenzhong Lan\n---AUTHOR---\nMing Lin\n---AUTHOR---\nXuanchong Li\n---AUTHOR---\nAlexander G. Hauptmann\n---AUTHOR---\nBhiksha Raj",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Lan_Beyond_Gaussian_Pyramid_2015_CVPR_supplemental.pdf",
    "id": "Lan_Beyond_Gaussian_Pyramid_2015_CVPR_supplemental",
    "abstract": "This is the supplementary material for the paper entitled ”Beyond Gaussian Pyramid: Multi-skip Feature Stacking for Action Recognition”. The material gives the proof of theorem 1 and 2.\n\n---TOPICHS---\nAction Recognition\nMatrix Bernstein's Inequality\nCondition Number\nFeature Stacking\nGaussian Pyramid",
    "topics": [],
    "references": [],
    "author_details": [
      {
        "name": "Zhenzhong Lan",
        "affiliation": "School of Computer Science, Carnegie Mellon University",
        "email": "lanzzh@cs.cmu.edu"
      },
      {
        "name": "Ming Lin",
        "affiliation": "School of Computer Science, Carnegie Mellon University",
        "email": "minglin@cs.cmu.edu"
      },
      {
        "name": "Xuanchong Li",
        "affiliation": "School of Computer Science, Carnegie Mellon University",
        "email": "xcli@cs.cmu.edu"
      },
      {
        "name": "Alexander G. Hauptmann",
        "affiliation": "School of Computer Science, Carnegie Mellon University",
        "email": "alex@cs.cmu.edu"
      },
      {
        "name": "Bhiksha Raj",
        "affiliation": "School of Computer Science, Carnegie Mellon University",
        "email": "bhiksha@cs.cmu.edu"
      }
    ]
  },
  {
    "title": "Local High-order Regularization on Data Manifolds\n---AUTHOR---\nKwang In Kim\nJames Tompkin\nHanspeter Pfister\nChristian Theobalt",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Kim_Local_High-Order_Regularization_2015_CVPR_paper.pdf",
    "id": "Kim_Local_High-Order_Regularization_2015_CVPR_paper",
    "abstract": "The common graph Laplacian regularizer is well-established but suffers from degeneracy in high-dimensional manifolds. Iterated graph Laplacian addresses this but incurs high computational complexity. This paper proposes a new, globally high-order regularizer that avoids degeneracy while maintaining sparsity for efficient computation in semi-supervised learning. The approach builds a local first-order approximation of the manifold as a surrogate geometry and constructs the high-order regularizer based on local derivative evaluations. Experiments on human body shape and pose analysis demonstrate the effectiveness and efficiency of the method.",
    "topics": [
      "Graph Laplacian Regularization",
      "Semi-Supervised Learning",
      "Manifold Approximation",
      "High-Order Derivatives",
      "Reproducing Kernel Hilbert Space (RKHS)"
    ],
    "references": [
      {
        "citation": "[J.-Y. Audibert and A. B. Tsybakov, Fast learning rates for plug-in classifiers, The Annals of Statistics, 2007]"
      },
      {
        "citation": "[M. Belkin and P. Niyogi, Laplacian eigenmaps for dimensionality reduction and data representation, Neural Computation, 2003]"
      },
      {
        "citation": "[M. Belkin and P. Niyogi, Towards a theoretical foundation for Laplacian-based manifold methods, Journal of Computer and System Sciences, 2005]"
      },
      {
        "citation": "[D. L. Donoho and C. Grimes, Hessian eigenmaps: locally linear embedding techniques for high-dimensional data, Proc. of the National Academy of Sciences, 2003]"
      },
      {
        "citation": "[O. Chapelle, B. Sch¨olkopf, and A. Zien, Semi-Supervised Learning, MIT Press, 2006]"
      },
      {
        "citation": "[U. Luxburg, A tutorial on spectral clustering, Statistics and Computing, 2007]"
      },
      {
        "citation": "[J. Shi and J. Malik, Normalized cuts and image segmentation, IEEE TPAMI, 2000]"
      },
      {
        "citation": "[R. M. Dudley, Real Analysis and Probability, Cambridge University Press, 2002]"
      },
      {
        "citation": "[M. Hein, J.-Y. Audibert, and U. von Luxburg, From graphs to manifolds - weak and strong pointwise consistency of graph Laplacians, Proc. COLT, 2005]"
      },
      {
        "citation": "[K. I. Kim, F. Steinke, and M. Hein, Semi-supervised regression using Hessian energy with an application to semi-supervised dimensionality reduction, NIPS, 2010]"
      }
    ],
    "author_details": [
      {
        "name": "Kwang In Kim",
        "affiliation": "Lancaster University",
        "email": "*Not available in the provided text*"
      },
      {
        "name": "James Tompkin",
        "affiliation": "Harvard SEAS",
        "email": "*Not available in the provided text*"
      },
      {
        "name": "Hanspeter Pfister",
        "affiliation": "Harvard SEAS",
        "email": "*Not available in the provided text*"
      },
      {
        "name": "Christian Theobalt",
        "affiliation": "MPI for Informatics",
        "email": "*Not available in the provided text*"
      }
    ]
  },
  {
    "title": "Just Noticeable Defocus Blur Detection and Estimation\n---AUTHOR---\nJianping Shi\nLi Xu\nJiaya Jia",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Shi_Just_Noticeable_Defocus_2015_CVPR_paper.pdf",
    "id": "Shi_Just_Noticeable_Defocus_2015_CVPR_paper",
    "abstract": "We tackle a fundamental problem to detect and estimate just noticeable blur (JNB) caused by defocus that spans a small number of pixels in images. This type of blur is common during photo taking. Although it is not strong, the slight edge blurriness contains informative clues related to depth. We found existing blur descriptors based on local information cannot distinguish this type of small blur reliably from unblurred structures. We propose a simple yet effective blur feature via sparse representation and image decomposition. It directly establishes correspondence between sparse edge representation and blur strength estimation. Extensive experiments manifest the generality and robustness of this feature.\n\n---TOPICCS---\nJust Noticeable Blur (JNB)\nSparse Representation\nImage Decomposition\nDepth Estimation\nBlur Detection and Estimation",
    "topics": [],
    "references": [],
    "author_details": [
      {
        "name": "Jianping Shi",
        "affiliation": "The Chinese University of Hong Kong",
        "email": "jpshi@cse.cuhk.edu.hk"
      },
      {
        "name": "Li Xu",
        "affiliation": "Image & Visual Computing Lab, Lenovo R&T",
        "email": "xulihk@lenovo.com"
      },
      {
        "name": "Jiaya Jia",
        "affiliation": "The Chinese University of Hong Kong",
        "email": "leojia@cse.cuhk.edu.hk"
      }
    ]
  },
  {
    "title": "Filtered Channel Features for Pedestrian Detection\n---AUTHOR---\nBernt Schiele\nShanshan Zhang\nRodrigo Benenson",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Zhang_Filtered_Feature_Channels_2015_CVPR_supplemental.pdf",
    "id": "Zhang_Filtered_Feature_Channels_2015_CVPR_supplemental",
    "abstract": "This supplementary material provides additional qualitative results for the pedestrian detection models presented in the main paper. Figures 1 and 2 illustrate the spatial distribution of learned filters in the Checkerboards4x3 and RandomFilters models, comparing them to a weaker model (Roerei). The analysis reveals that while filtered channels don't alter the areas of the pedestrian deemed informative, they enable the extraction of more discriminative information. Specific channels (U for face, L and gradient magnitude for the body) exhibit characteristic usage patterns. Furthermore, the filter usage distribution is observed to be similar across different filter bank families. The figures also show the frequency of usage of each filter as a feature for decision tree split nodes.\n\n---TOPIICS---\nPedestrian detection\nFiltered channel features\nDecision trees\nFilter bank families\nSpatial feature distribution",
    "topics": [],
    "references": [
      {
        "citation": "[Benenson, R., Mathias, M., Tuytelaars, T., & Van Gool, L. (2013). Seeking the strongest rigid detector. In CVPR.] - This paper is directly referenced and visually compared to in the text, making it highly relevant."
      },
      {
        "citation": "[Roerei, et al.] - Mentioned in relation to filter usage and comparison, suggesting it's a key related work. (Full citation details not provided in the excerpt.)"
      },
      {
        "citation": "[ACF] - Mentioned in relation to filter usage, indicating a connection to the methodology. (Full citation details not provided in the excerpt.)"
      },
      {
        "citation": "[Squares]ChnFtrs - Mentioned in relation to filter usage, indicating a connection to the methodology. (Full citation details not provided in the excerpt.)"
      }
    ],
    "author_details": [
      {
        "name": "Bernt Schiele",
        "affiliation": "Max Planck Institute for Informatics",
        "email": "firstname.lastname@mpi-inf.mpg.de"
      },
      {
        "name": "Shanshan Zhang",
        "affiliation": "Max Planck Institute for Informatics",
        "email": "firstname.lastname@mpi-inf.mpg.de"
      },
      {
        "name": "Rodrigo Benenson",
        "affiliation": "Max Planck Institute for Informatics",
        "email": "firstname.lastname@mpi-inf.mpg.de"
      }
    ]
  },
  {
    "title": "A Metric Parametrization for Trifocal Tensprs with Non-Colinear Pinholes\n---AUTHOR---\nSpyridon Leonardos\n---AUTHOR---\nRoberto Tron\n---AUTHOR---\nKostas Daniilidis",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Leonardos_A_Metric_Parametrization_2015_CVPR_paper.pdf",
    "id": "Leonardos_A_Metric_Parametrization_2015_CVPR_paper",
    "abstract": "This paper introduces a novel parametrization of the trifocal tensor for calibrated cameras with non-colinear pinholes, based on a quotient Riemannian manifold. This parametrization is almost symmetric, utilizing a preferred camera only for translations. The authors demonstrate how this parametrization can be incorporated into state-of-the-art optimization techniques on manifolds, allowing for refinement of tensor estimates from image data. Furthermore, the Riemannian structure provides a notion of distance between trifocal tensors, which is shown to produce meaningful results in a Structure from Motion problem. The work investigates a new formulation of the trifocal tensor and provides a meaningful way to measure distances between them.",
    "topics": [
      "Trifocal Tensor",
      "Riemannian Manifolds",
      "Structure from Motion",
      "Geometric Computer Vision",
      "Camera Calibration"
    ],
    "references": [
      {
        "citation": "[Absil, P.-A., Mahony, R., and Sepulchre, R. Optimization Algorithms on Matrix Manifolds. Princeton University Press, 2008. 1, 5, 7]"
      },
      {
        "citation": "[Bertsekas, D. P. Nonlinear Programming. Athena Scientiﬁc, Belmont, MA, 1999. 6]"
      },
      {
        "citation": "[Boumal, N., Mishra, B., Absil, P.-A., and Sepulchre, R. Manopt, a Matlab toolbox for optimization on manifolds. Journal of Machine Learning Research, 15:1455–1459, 2014. 7]"
      },
      {
        "citation": "[Hartley, R. I. Lines and points in three views and the trifocal tensor. Int. J. Comput. Vision, 22(2):125–140, Mar. 1997. 1]"
      },
      {
        "citation": "[Hartley, R. I. and Zisserma, A. Multiple View Geometry in Computer Vision. Cambridge University Press, 8004. 2, 6]"
      },
      {
        "citation": "[Kendall, D. G. Shape Manifolds, Procustean Metrics, and Complex Projective Spaces. Bulletin of the London Mathematical Society, 16:81–121, 84. 2]"
      },
      {
        "citation": "[Hartley, R. I. Projective reconstruction from line correspondences. In In Proc. IEEE Conf. on Computer Vision and Pattern Recognition, pages 903–907, 94. 1]"
      },
      {
        "citation": "[Papadopoulo, T. and Faugeras, O. A new characterization of the trifocal tensor. In European Conference on Computer Vision, pages 109–123, 98. 1]"
      },
      {
        "citation": "[Torr, P. and Zisserma, A. Robust parameterization and computation of the trifocal tensor. Image and Vision Computing, 15:591–605, 97. 1]"
      },
      {
        "citation": "[Weng, J., Huang, T. S., and Ahuja, N. Motion and structure from line correspondences; closed-form solution, uniqueness, and optimization. IEEE Trans. Pattern Anal. Mach. Intell., 14(3):318–336, 92. 1, 3, 7, 8]"
      }
    ],
    "author_details": [
      {
        "name": "Spyridon Leonardos",
        "affiliation": "GRASP Laboratory, University of Pennsylvania",
        "email": "spyridon@seas.upenn.edu"
      },
      {
        "name": "Roberto Tron",
        "affiliation": "GRASP Laboratory, University of Pennsylvania",
        "email": "tron@seas.upenn.edu"
      },
      {
        "name": "Kostas Daniilidis",
        "affiliation": "GRASP Laboratory, University of Pennsylvania",
        "email": "kostas@cis.upenn.edu"
      }
    ]
  },
  {
    "title": "Fast 2D Border Ownership Assignment\n---AUTHOR---\nCornelia Fermüller\nChing L. Teo\nYiannis Aloimonos",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Teo_Fast_2D_Border_2015_CVPR_paper.pdf",
    "id": "Teo_Fast_2D_Border_2015_CVPR_paper",
    "abstract": "A method for efficient border ownership assignment in 2D images is proposed. Leveraging recent advances using Structured Random Forests (SRF) for boundary detection, a novel border ownership structure is introduced that detects both boundaries and border ownership simultaneously. The method utilizes features that predict ownership cues from 2D images, including shape (using HoG-like descriptors), spectral properties of boundary patches (using PCA on orthonormal bases), and semi-global grouping cues indicative of perceived depth. Experimental results on the Berkeley Segmentation Dataset (BSDS) and the NYU Depth V2 dataset demonstrate superior performance compared to current state-of-the-art multi-stage approaches.\n\n---TOPIC---\nBorder Ownership Assignment\n---TOPI---\nStructured Random Forests (SRF)\n---TOPI---\nImage Segmentation\n---TOPI---\nComputer Vision\n---TOPI---\nFeature Extraction (HoG, PCA)",
    "topics": [],
    "references": [
      {
        "citation": "[Alexe, B., Deselaers, T., & Ferrari, V. (2012). Measuring the object-ness of image windows. PAMI, 34(11), 34(11):2189–2202.]"
      },
      {
        "citation": "[Arbeláez, P., Maire, M., Fowlkes, C., & Malik, J. (2011). Contour detection and hierarchical image segmentation. PAMI, 33(5), 33(5):898–916.]"
      },
      {
        "citation": "[Cheng, M.-M., Zhang, Z., Lin, W.-Y., & Torr, P. (2014). Bing: Binarized normed gradients for objectness estimation at 300fps. CVPR, pages 3286–3293.]"
      },
      {
        "citation": "[Dalal, N., & Triggs, B. (2005). Histograms of oriented gradients for human detection. CVPR, pages 3286–3293.]"
      },
      {
        "citation": "[Dollár, P., & Zitnick, C. L. (2015). Fast edge detection using structured forests. PAMI, 2015.]"
      },
      {
        "citation": "[Dollár, P., Appel, R., Belongie, S., & Perona, P. (2014). Fast feature pyramids for object detection. PAMI, 36(8), 36(8):1532–1545.]"
      },
      {
        "citation": "[Endres, I., & Hoiem, D. (2014). Category-independent object proposals with diverse ranking. PAMI, 36(2), 36(2):222–234.]"
      },
      {
        "citation": "[Geurts, P., Ernst, D., & Wehenkel, L. (2006). Extremely randomized trees. Machine learning, 63(1), 63(1):3–42.]"
      },
      {
        "citation": "[Gupta, S., Arbeláez, P., & Malik, J. (2013). Perceptual organization and recognition of indoor scenes from rgb-d images. CVPR, pages 564–571.]"
      },
      {
        "citation": "[Ho, T. K. (1995). Random decision forests. ICDAR, pages 278–282.]"
      }
    ],
    "author_details": [
      {
        "name": "Cornelia Fermüller",
        "affiliation": "Computer Vision Lab, University of Maryland, College Park, MD 20742, USA",
        "email": "fer@umiacs.umd.edu"
      },
      {
        "name": "Ching L. Teo",
        "affiliation": "Computer Vision Lab, University of Maryland, College Park, MD 20742, USA",
        "email": "cteo@cs.umd.edu"
      },
      {
        "name": "Yiannis Aloimonos",
        "affiliation": "Computer Vision Lab, University of Maryland, College Park, MD 20742, USA",
        "email": "yiannis@cs.umd.edu"
      }
    ]
  },
  {
    "title": "Heat Diffusion Over Weighted Manifolds: A New Descriptor for Textured 3D Non-Rigid Shapes\n---AUTHOR---\nMostafa Abdelrahman\nAly Farag\nDavid Swanson\nMoumen T. El-Melegy",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Abdelrahman_Heat_Diffusion_Over_2015_CVPR_paper.pdf",
    "id": "Abdelrahman_Heat_Diffusion_Over_2015_CVPR_paper",
    "abstract": "Recently, the ability to acquire both 3D shape and color information has increased, necessitating shape descriptors that incorporate photometric features. Most existing descriptors focus solely on geometric or topological properties. This paper proposes a new approach for modeling textured 3D non-rigid models based on Weighted Heat Kernel Signature (W-HKS). The approach includes photometric information as a weight over the shape manifold, proposes a novel formulation for heat diffusion over weighted manifolds, and introduces a new discretization method using finite element approximation. The resulting weighted heat kernel signature encodes both photometric and geometric information and incorporates a method for scale invariance. Experimental results on benchmark datasets confirm the approach's high performance in textured shape retrieval and its ability to handle challenges where pure geometric or photometric methods fail.\n\n---TOPICCS---\nWeighted Heat Kernel Signature (W-HKS)\nTextured 3D Shape Retrieval\nNon-Rigid Shape Modeling\nHeat Diffusion on Manifolds\nPhotometric Shape Descriptors",
    "topics": [],
    "references": [],
    "author_details": [
      {
        "name": "Mostafa Abdelrahman",
        "affiliation": "Electrical Engineering Department, Assiut University, Assiut 71516, Egypt",
        "email": "mostafa.abdelrahman@aun.edu.eg"
      },
      {
        "name": "Aly Farag",
        "affiliation": "CVIP Lab, University of Louisville, Louisville, KY 40292, USA",
        "email": "aly.farag@louisville.edu"
      },
      {
        "name": "David Swanson",
        "affiliation": "Department of Mathematics, University of Louisville, Louisville, KY 40292, USA",
        "email": "david.swanson@louisville.edu"
      },
      {
        "name": "Moumen T. El-Melegy",
        "affiliation": "Electrical Engineering Department, Assiut University, Assiut 71516, Egypt",
        "email": "moumen@aun.edu.eg"
      }
    ]
  },
  {
    "title": "Matching-CNN Meets KNN: Quasi-Parametric Human Parsing\n---AUTHOR---\nSi Liu\nXiaodan Liang\nLuoqi Liu\nXiaohui Shen\nJianchao Yang\nChangsheng Xu\nLiang Lin\nXiaochun Cao\nShuicheng Yan",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Liu_Matching-CNN_Meets_KNN_2015_CVPR_paper.pdf",
    "id": "Liu_Matching-CNN_Meets_KNN_2015_CVPR_paper",
    "abstract": "This work introduces a new solution for human parsing, combining the benefits of both parametric and non-parametric methodologies. The proposed quasi-parametric model leverages a classic K Nearest Neighbor (KNN)-based framework and a novel Matching Convolutional Neural Network (M-CNN). The M-CNN predicts matching confidence and displacements of the best-matched region in the testing image for a particular semantic region in a KNN image. The model retrieves KNN images, matches semantic regions using M-CNN, fuses matched regions, and refines the result with superpixel smoothing. Evaluations on a large dataset demonstrate significant performance gains over state-of-the-arts.\n\n---TOPIC---\nHuman parsing\nKNN (K Nearest Neighbors)\nM-CNN (Matching Convolutional Neural Network)\nQuasi-parametric methods\nSuperpixel smoothing",
    "topics": [],
    "references": [],
    "author_details": [
      {
        "name": "Si Liu",
        "affiliation": "SKLOIS, IIE, Chinese Academy of Sciences",
        "email": "liusi@iie.ac.cn"
      },
      {
        "name": "Xiaodan Liang",
        "affiliation": "National University of Singapore",
        "email": "xdliang328@gmail.com"
      },
      {
        "name": "Luoqi Liu",
        "affiliation": "National University of Singapore",
        "email": "N/A"
      },
      {
        "name": "Xiaohui Shen",
        "affiliation": "Adobe Research",
        "email": "N/A"
      },
      {
        "name": "Jianchao Yang",
        "affiliation": "Adobe Research",
        "email": "N/A"
      },
      {
        "name": "Changsheng Xu",
        "affiliation": "IA, Chinese Academy of Sciences",
        "email": "N/A"
      },
      {
        "name": "Liang Lin",
        "affiliation": "Sun Yat-sen University",
        "email": "N/A"
      },
      {
        "name": "Xiaochun Cao",
        "affiliation": "SKLOIS, IIE, Chinese Academy of Sciences",
        "email": "N/A"
      },
      {
        "name": "Shuicheng Yan",
        "affiliation": "National University of Singapore",
        "email": "N/A"
      }
    ]
  },
  {
    "title": "Combination Features and Models for Human Detection\n---AUTHOR---\nYunsheng Jiang\n---AUTHOR---\nJinwen Ma",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Jiang_Combination_Features_and_2015_CVPR_paper.pdf",
    "id": "Jiang_Combination_Features_and_2015_CVPR_paper",
    "abstract": "This paper addresses the challenges of human detection by proposing effective combination models with complementary features. Existing features and models often exhibit biases that limit their performance across diverse human body appearances. To overcome this, the authors combine complementary features and models using effective organization and fusion methods. Specifically, they introduce HOG-III features (combining HOG, color, and bar-shape features) and a weighted-NMS fusion algorithm. Experiments on the PASCAL VOC datasets demonstrate that these approaches significantly improve detection performance and maintain computational efficiency when used with various detection models.\n\n---TOPICCS---\nHuman Detection\nFeature Combination\nModel Fusion\nHOG-III Features\nWeighted-NMS",
    "topics": [],
    "references": [
      {
        "citation": "[Belongie, S., Malik, J., & Puzicha, J. (2001). Matching shapes. In *IEEE Int’l Conf. on Computer Vision (ICCV)*, volume 1, pages 454–461.]"
      },
      {
        "citation": "[Hubel, D. H. (1995). *Eye, brain, and vision*. Scientific American Library/Scientific American Books.]"
      },
      {
        "citation": "[Ioffe, S., & Forsyth, D. (2001). Mixtures of trees for object recogni-tion. In *IEEE Conf. on Computer Vision and Pattern Recogni-tion (CVPR)*, volume 1, pages 180–185.]"
      },
      {
        "citation": "[Dalal, N. (2006). *Finding people in images and videos*. PhD thesis, Institut National Polytechnique de Grenoble-INPG.]"
      },
      {
        "citation": "[Felzenszwalb, P. F., McAllester, D., & Ramanan, D. (2008). A discrimi-natively trained, multiscale, deformable part model. In *IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)*, pages 1–8.]"
      },
      {
        "citation": "[Girshick, R. B., Donahue, J., Darrell, T., & Malik, J. (2014). Rich feature hierarchies for accurate object detection and semantic segmentation. In *IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)*.]"
      },
      {
        "citation": "[Viola, P., & Jones, M. J. (2004). Robust real-time face detection. *International Journal of Computer Vision*, *57*(2), 137–154.]"
      },
      {
        "citation": "[Felzenszwalb, P. F., & McAllester, D. (2011). Object detection grammars. In *IEEE Int’l Conf. on Computer Vision (ICCV) Workshops*, page 691.]"
      },
      {
        "citation": "[Felzenszwalb, P. F., Girshick, R. B., & McAllester, D. A. (2010). Object detection with grammar models. In *Advances in Neural Information Processing Systems*, pages 442–450.]"
      },
      {
        "citation": "[Gkioxari, G., Hariharan, B., Girshick, R., & Malik, J. (2014). Using k-poselets for detecting people and localizing their keypoints. In *IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)*.]"
      }
    ],
    "author_details": [
      {
        "name": "Yunsheng Jiang",
        "affiliation": "Department of Information Science, School of Mathematical Sciences and LMAM, Peking University",
        "email": "Not available"
      },
      {
        "name": "Jinwen Ma",
        "affiliation": "Department of Information Science, School of Mathematical Sciences and LMAM, Peking University",
        "email": "jwma@math.pku.edu.cn"
      }
    ]
  },
  {
    "title": "Effective Learning-Based Illuminant Estimation Using Simple Features\n---AUTHOR---\nDongliang Cheng\nBrian Price\nScott Cohen\nMichael S. Brown",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Cheng_Effective_Learning-Based_Illuminant_2015_CVPR_paper.pdf",
    "id": "Cheng_Effective_Learning-Based_Illuminant_2015_CVPR_paper",
    "abstract": "Illumination estimation is the process of determining the chromaticity of the illumination in an imaged scene in order to remove undesirable color casts through white-balancing. While computational color constancy is a well-studied topic in computer vision, it remains challenging due to the ill-posed nature of the problem. In this paper, we present a learning-based method based on four simple color features and show how to use this with an ensemble of regression trees to estimate the illumination. We demonstrate that our approach is not only faster than existing learning-based methods in terms of both evaluation and training time, but also gives the best results reported to date on modern color constancy data sets.\n\n---TOPICCS---\nIllumination Estimation\nColor Constancy\nLearning-Based Methods\nRegression Trees\nImage Processing",
    "topics": [],
    "references": [
      {
        "citation": "[Forsyth, D. A. A novel algorithm for color constancy. IJCV, 5(1):5–35, 1990.]"
      },
      {
        "citation": "[Banić, N., and S. Lončarić. Color dog: Guiding the global illumination estimation to better accuracy. In International Conference on Computer Vision Theory and Applications, 2015.]"
      },
      {
        "citation": "[Funt, B., and W. Xiong. Estimating illumination chromaticity via support vector regression. In Color and Imaging Conference, 2004.]"
      },
      {
        "citation": "[Gao, S., W. Han, K. Yang, C. Li, and Y. Li. Efﬁcient color constancy with local surface reﬂectance statistics. In ECCV, 2014.]"
      },
      {
        "citation": "[Barnard, K., L. Martin, A. Coath, and B. Funt. A comparison of computational color constancy algorithms. ii. experiments with image data. TIP, 11(9):985–996, 2002.]"
      },
      {
        "citation": "[Barnard, K., L. Martin, B. Funt, and A. Coath. A data set for color research. Color Research & Application, 27(3):147–151, 2002.]"
      },
      {
        "citation": "[Gehler, P. V., C. Rother, A. Blake, T. Minka, and T. Sharp. Bayesian color constancy revisited. In CVPR, 2008.]"
      },
      {
        "citation": "[Bianco, S., G. Ciocca, C. Cusano, and R. Schettini. Improving color constancy using indoor - outdoor image classification. TIP, 17(12):2381–2392, 2008.]"
      },
      {
        "citation": "[Bianco, S., G. Ciocca, C. Cusano, and R. Schettini. Automatic color constancy algorithm selection and combination. Pattern Recognition, 43(3):695–705, 2010.]"
      },
      {
        "citation": "[Botev, Z., J. Grotowski, D. Kroese, et al. Kernel density estimation via diffusion. The Annals of Statistics, 38(5):2916–2957, 2010.]"
      }
    ],
    "author_details": [
      {
        "name": "Dongliang Cheng",
        "affiliation": "National University of Singapore",
        "email": "dcheng@comp.nus.edu.sg"
      },
      {
        "name": "Brian Price",
        "affiliation": "Adobe Research",
        "email": "bprice@adobe.com"
      },
      {
        "name": "Scott Cohen",
        "affiliation": "Adobe Research",
        "email": "scohen@adobe.com"
      },
      {
        "name": "Michael S. Brown",
        "affiliation": "National University of Singapore",
        "email": "brown@comp.nus.edu.sg"
      }
    ]
  },
  {
    "title": "Robust Regression on Image Manifolds for Ordered Label Denoising\n---AUTHOR---\nHui Wu\n---AUTHOR---\nRichard Souvenir",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Wu_Robust_Regression_on_2015_CVPR_supplemental.pdf",
    "id": "Wu_Robust_Regression_on_2015_CVPR_supplemental",
    "abstract": "Due to space constraints, the results in the submission were condensed by showing small images or excluding results from poorly-performing competing approaches. These detailed results are from the same experiments in the paper. The paper focuses on robust regression on image manifolds for ordered label denoising. Figures 1-4 illustrate results for the Statue and Face Pose datasets, demonstrating the performance of various methods (RANSC, K-NN, RBFN, SVR, KSPCA, H3R) in recovering ordered labels under corruption.",
    "topics": [
      "Robust Regression",
      "Image Manifolds",
      "Ordered Label Denoising",
      "Statue Data Set",
      "Face Pose Estimation"
    ],
    "references": [],
    "author_details": [
      {
        "name": "Hui Wu",
        "affiliation": "University of North Carolina at Charlotte",
        "email": "hwu13@uncc.edu"
      },
      {
        "name": "Richard Souvenir",
        "affiliation": "University of North Carolina at Charlotte",
        "email": "souvenir@uncc.edu"
      }
    ]
  },
  {
    "title": "A Convex Optimization Approach to Robust Fundamental Matrix Estimation\n\n---AUTHOR---\nY. Cheng\nJ. A. Lopez\nO. Camps\nM. Sznaier",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Cheng_A_Convex_Optimization_2015_CVPR_paper.pdf",
    "id": "Cheng_A_Convex_Optimization_2015_CVPR_paper",
    "abstract": "This paper addresses the problem of estimating the fundamental matrix from corrupted point correspondences. A general nonconvex framework is proposed that explicitly takes into account the rank-2 constraint on the fundamental matrix and the presence of noise and outliers. The main result demonstrates that this nonconvex problem can be solved by solving a sequence of convex semi-deﬁnite programs, achieved by combining polynomial optimization tools and rank minimization techniques. The algorithm is readily extensible to handle partially labeled correspondences and to leverage co-occurrence information, if available. Experimental results consistently show the method's effectiveness, even with a high percentage of outliers.",
    "topics": [
      "Fundamental Matrix Estimation",
      "Robust Optimization",
      "Convex Semi-Definite Programming",
      "Rank-Constrained Optimization",
      "Outlier Rejection"
    ],
    "references": [
      {
        "citation": "[Hartley, R., & Zisserman, A. (2003). *Multiple view geometry in computer vision*. Cambridge university press.] - This appears to be a foundational text on the topic."
      },
      {
        "citation": "[Mohan, K., & Fazel, M. (2012). *Iterative reweighted algorithms for matrix rank minimization*. *J. of Machine Learning Research*, *13*, 3441–3473.] - Relevant for rank minimization techniques."
      },
      {
        "citation": "[Lasserre, J. B. (2001). *Global optimization with polynomials and the problem of moments*. *SIAM Journal on Optimization*, *11*(3), 796–817.] - Important for polynomial optimization methods."
      },
      {
        "citation": "[Bugarin, F., Bartoli, A., Henrion, D., Lasserre, J.-B., Orteu, J.-J., & Sentenac, T. (2014). *Rank-constrained fundamental matrix estimation by polynomial global optimization versus the eight-point algorithm*. arXiv preprint arXiv:1403.4806.] - A recent paper comparing different estimation methods."
      },
      {
        "citation": "[Torr, P. H., & Murray, D. W. (1997). *The development and comparison of robust methods for estimating the fundamental matrix*. *International journal of computer vision*, *24*(3), 271–300.] - A comparative study of fundamental matrix estimation methods."
      },
      {
        "citation": "[Zheng, Y., Sugimoto, S., & Okutomi, M. (2013). *A practical rank-constrained eight-point algorithm for fundamental matrix estimation*. In *Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on* (pp. 1546–1553). IEEE.] - A practical algorithm for fundamental matrix estimation."
      },
      {
        "citation": "[Torr, P. H., & Zisserman, A. (2000). *Mlesac: A new robust estimator with application to estimating image geometry*. *Computer Vision and Image Understanding*, *78*(1), 138–156.] - Introduces a robust estimator."
      },
      {
        "citation": "[Lasserre, J. B. (2006). *Convergent sdp-relaxations in polynomial optimization with sparsity*. *SIAM Journal on Optimization*, *17*(3), 422–443.] - Discusses SDP relaxations."
      },
      {
        "citation": "[Fischler, M. A., & Bolles, R. C. (1981). *Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography*. *Communications of the ACM*, *24*(6), 281–395.] - Introduces RANSAC."
      },
      {
        "citation": "[Sugaya, Y., & Kanatani, K. (2007). *High accuracy computation of rank-constrained fundamental matrix*. In *BMVC* (pp. 1–10).] - Focuses on high-accuracy computation."
      }
    ],
    "author_details": [
      {
        "name": "Y. Cheng",
        "affiliation": "Northeaster University, Boston, MA 02115",
        "email": "cheng.yong@husky.neu.edu"
      },
      {
        "name": "J. A. Lopez",
        "affiliation": "Northeaster University, Boston, MA 02115",
        "email": "lopez.jo@husky.neu.edu"
      },
      {
        "name": "O. Camps",
        "affiliation": "Northeaster University, Boston, MA 02115",
        "email": "camps@coe.neu.edu"
      },
      {
        "name": "M. Sznaier",
        "affiliation": "Northeaster University, Boston, MA 02115",
        "email": "msznaier@coe.neu.edu"
      }
    ]
  },
  {
    "title": "Learning a Convolutional Neural Network for Non-uniform Motion Blur Removal\n---AUTHOR---\nJian Sun\nWenfei Cao\nZongben Xu\nJean Ponce",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Sun_Learning_a_Convolutional_2015_CVPR_paper.pdf",
    "id": "Sun_Learning_a_Convolutional_2015_CVPR_paper",
    "abstract": "This paper addresses the problem of estimating and removing non-uniform motion blur from a single blurry image. The authors propose a deep learning approach to predicting the probabilistic distribution of motion blur at the patch level using a convolutional neural network (CNN). They extend the candidate set of motion kernels predicted by the CNN using image rotations. A Markov random field model is then used to infer a dense non-uniform motion blur field, enforcing motion smoothness. Finally, motion blur is removed by a non-uniform deblurring model using patch-level image prior. Experimental evaluations demonstrate the effectiveness of the approach in handling complex non-uniform motion blur.\n\n---TOPICCS---\nConvolutional Neural Networks (CNNs)\nNon-uniform Motion Blur\nImage Deblurring\nMarkov Random Fields (MRFs)\nPatch-based Image Processing",
    "topics": [],
    "references": [],
    "author_details": [
      {
        "name": "Jian Sun",
        "affiliation": "Xi’an Jiaotong University",
        "email": "*Not available in the provided text*"
      },
      {
        "name": "Wenfei Cao",
        "affiliation": "Xi’an Jiaotong University",
        "email": "*Not available in the provided text*"
      },
      {
        "name": "Zongben Xu",
        "affiliation": "Xi’an Jiaotong University",
        "email": "*Not available in the provided text*"
      },
      {
        "name": "Jean Ponce",
        "affiliation": "École Normale Supérieure / PSL Research University",
        "email": "*Not available in the provided text*"
      }
    ]
  },
  {
    "title": "Scalable Object Detection by Filter Compression with Regularized Sparse Coding\n---AUTHOR---\nTing-Hsuan Chao\nYen-Liang Lin\nYin-Hsi Kuo\nWinston H. Hsu",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Chao_Scalable_Object_Detection_2015_CVPR_paper.pdf",
    "id": "Chao_Scalable_Object_Detection_2015_CVPR_paper",
    "abstract": "Object detection systems require a large number of classes for practical applications, but this often leads to long detection times due to extensive convolution operations. Existing methods employing sparse coding to reduce computational complexity often compromise accuracy when a large speedup is required, particularly when using a small codebook to reconstruct filters. This paper addresses this issue by proposing Regularized Sparse Coding, which focuses on reconstructing filter functionality rather than appearance. This approach minimizes score map error, leading to a significant speedup (16x on ILSVIRC 2013) with minimal accuracy loss (0.04 mAP drop) compared to the original Deformable Part Model. The method also demonstrates applicability for parallel computing on GPUs.\n\n---TOPIC---\nObject Detection\n---TOPI---\nSparse Coding\n---TOPI---\nFilter Compression\n---TOPI---\nRegularization\n---TOPI---\nComputational Efficiency",
    "topics": [],
    "references": [],
    "author_details": [
      {
        "name": "Ting-Hsuan Chao",
        "affiliation": "National Taiwan University",
        "email": "[Not available in text]"
      },
      {
        "name": "Yen-Liang Lin",
        "affiliation": "National Taiwan University",
        "email": "[Not available in text]"
      },
      {
        "name": "Yin-Hsi Kuo",
        "affiliation": "National Taiwan University",
        "email": "[Not available in text]"
      },
      {
        "name": "Winston H. Hsu",
        "affiliation": "National Taiwan University",
        "email": "[Not available in text]"
      }
    ]
  },
  {
    "title": "New Insights into Laplacian Similarity Search\n---AUTHOR---\nXiao-Ming Wu\nZhenguo Li\nShih-Fu Chang",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Wu_New_Insights_Into_2015_CVPR_supplemental.pdf",
    "id": "Wu_New_Insights_Into_2015_CVPR_supplemental",
    "abstract": "Abstract not found",
    "topics": [],
    "references": [
      {
        "citation": "[U. von Luxburg, A. Radl, and M. Hein, Hitting and commute times in large random neighborhood graphs, Journal of Machine Learning Research, 2014]"
      },
      {
        "citation": "[X.-M. Wu, Z. Li, and S.-F. Chang, Analyzing the harmonic structure in graph-based learning, NIPS, 2013]"
      },
      {
        "citation": "[X.-M. Wu, Z. Li, and S.-F. Chang, New insights into laplacian similarity search, CVPR, 2015]"
      }
    ],
    "author_details": [
      {
        "name": "Xiao-Ming Wu",
        "affiliation": "Department of Electrical Engineering, Columbia University",
        "email": "xmwu@ee.columbia.edu"
      },
      {
        "name": "Zhenguo Li",
        "affiliation": "Huawei Noah’s Ark Lab, Hong Kong",
        "email": "li.zhenguo@huawei.com"
      },
      {
        "name": "Shih-Fu Chang",
        "affiliation": "Department of Electrical Engineering, Columbia University",
        "email": "sfchang@ee.columbia.edu"
      }
    ]
  },
  {
    "title": "Region-based Temporally Consistent Video Post-processing\n---AUTHOR---\nXuan Dong\nBoyan Bonev\nYu Zhu\nAlan L. Yuille",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Dong_Region-Based_Temporally_Consistent_2015_CVPR_paper.pdf",
    "id": "Dong_Region-Based_Temporally_Consistent_2015_CVPR_paper",
    "abstract": "This paper addresses the problem of temporally consistent video post-processing when original input and enhancement videos are available. The goal is to maintain both temporal consistency (consistent enhancement of the same objects across frames) and fidelity (similarity to the original enhancement). The authors observe that many image/video enhancement algorithms enforce a spatially consistent prior, meaning pixels with the same RGB values receive similar enhancement values within a local region. They segment each frame into regions and temporally-spatially adjust the enhancement of regions across different frames, considering fidelity, temporal consistency, and spatial consistency. Experimental results demonstrate that the proposed method achieves high fidelity and temporal consistency. The approach is particularly useful when the original enhancement algorithms are unknown or inaccessible.\n\n---TOPICCS---\nVideo Post-processing\nTemporal Consistency\nSpatial Consistency\nRegion-Based Segmentation\nImage Enhancement Algorithms",
    "topics": [],
    "references": [
      {
        "citation": "[R. Achanta, A. Shaji, K. Smith, A. Lucchi, P. Fua, and S. Susstrunk. Slic superpixels compared to state-of-the-art superpixel methods. IEEE Transactions on Pattern Analysis and Machine Intelligence, 34(11):2274–2282, 2012.]"
      },
      {
        "citation": "[S. Bae, S. Paris, and F. Durand. Two-scale tone management for photographic look. ACM Trans. on Graph., 25(3):637–645, 2006.]"
      },
      {
        "citation": "[N. Bonneel, K. Sunkavalli, S. Paris, and H. Pﬁster. Example-based video color grading. ACM Trans. on Graph., 32(4):1–11, 2013.]"
      },
      {
        "citation": "[Y. Chang, S. Saito, and M. Nakajima. Example-based color transformation of image and video using basic color categories. IEEE Transactions on Image Processing, 16(2):329–336, 2007.]"
      },
      {
        "citation": "[Z. Farbman and D. Lischinski. Tonal stabilization of video. ACM Trans. on Graph., 30(4):1–9, 2011.]"
      },
      {
        "citation": "[P. F. Felzenszwalb and D. P. Huttenlocher. Efﬁcient belief propagation for early vision. CVPR, 16(2):261–268, 2004.]"
      },
      {
        "citation": "[M. Grundmann, C. McClanahan, S. Kang, and I. Essa. Post-processing approach for radiometric self-calibration of video. Int. Conf. Computational Photography.]"
      },
      {
        "citation": "[Y. Hacohen, E. Shechtman, D. Goldman, and D. Lischinsky. Non-rigid dense correspondence with applications for image enhancement. ACM Trans. Graph., 30.]"
      },
      {
        "citation": "[N. K. Kalantari, E. Shechtman, C. Barnes, S. Darabi, D. B. Goldman, and P. Sen. Patch-based high dynamic range video. ACM Trans. on Graph., 32(6):1–8, 2013.]"
      },
      {
        "citation": "[S. B. Kang, M. Uyttendaele, S. Winder, and R. Szeliski. High dynamic range video. ACM Trans. on Graph., 22(3):319–325, 2003.]"
      }
    ],
    "author_details": [
      {
        "name": "Xuan Dong",
        "affiliation": "Tsinghua University",
        "email": "dongx10@mails.tsinghua.edu.cn"
      },
      {
        "name": "Boyan Bonev",
        "affiliation": "UC Los Angeles",
        "email": "bonev@ucla.edu"
      },
      {
        "name": "Yu Zhu",
        "affiliation": "Northwestern Polytechnical University",
        "email": "zhuyu1986@mail.nwpu.edu.cn"
      },
      {
        "name": "Alan L. Yuille",
        "affiliation": "UC Los Angeles",
        "email": "yuille@stat.ucla.edu"
      }
    ]
  },
  {
    "title": "Large-Scale Damage Detection Using Satellite Imagery\n---AUTHOR---\nLionel Gueguen\nRaffay Hamid",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Gueguen_Large-Scale_Damage_Detection_2015_CVPR_paper.pdf",
    "id": "Gueguen_Large-Scale_Damage_Detection_2015_CVPR_paper",
    "abstract": "Satellite imagery is a valuable resource for assessing damages in distressed areas, but manual inspection is impractical due to the vast amount of data. This paper presents a semi-supervised learning framework for large-scale damage detection in satellite imagery. The framework is evaluated using over 88 million images collected from 4,665 KM2 across 12 locations worldwide. A novel use of hierarchical shape features within a bag-of-visual words setting is introduced for accurate and efficient damage detection. The study analyzes the impact of practical factors like sun angle, sensor resolution, and registration differences, comparing the proposed representation to five alternatives. A user study demonstrates a ten-fold reduction in human annotation time with minimal loss in detection accuracy compared to manual inspection.",
    "topics": [
      "Semi-supervised learning",
      "Satellite imagery analysis",
      "Damage detection",
      "Hierarchical shape features",
      "Large-scale data processing"
    ],
    "references": [
      {
        "citation": "[Xia, G.-S., Delon, J., & Gousseau, Y. (2010). Shape-based invariant texture indexing. *International Journal of Computer Vision*, *88*(3), 382–403.]"
      },
      {
        "citation": "[Markou, M., & Singh, S. (2003). Novelty detection - a review: statistical approaches. *Signal Processing*, *83*(12), 2003.]"
      },
      {
        "citation": "[Blanchard, G., Lee, G., & Scott, C. (2010). Semi-supervised novelty detection. *Journal of Machine Learning Research*, *11*, 2973–3009.]"
      },
      {
        "citation": "[Bruzzone, L., & Prieto, D. (2000). Automatic analysis of the difference image for unsupervised change detection. *IEEE Transactions on Geoscience and Remote Sensing*, *38*(3), 1171–1182.]"
      },
      {
        "citation": "[Geraud, T., Carlinet, E., Crozet, S., & Najman, L. (2013). A quasi-linear algorithm to compute the tree of shapes of nD images. *International Symposium on Mathematical Morphology*, 2013.]"
      },
      {
        "citation": "[Monasse, P., & Guichard, F. (2000). Fast computation of a contrast-invariant image representation. *IEEE Transactions on Image Processing*, *9*(5), 860–872.]"
      },
      {
        "citation": "[Nielsen, A. (2007). The regularized iteratively reweighted mad method for change detection in multi- and hyperspectral data. *IEEE Transactions on Image Processing*, 2007.]"
      },
      {
        "citation": "[Vaduva, C., Costachioiu, T., Patrascu, C., Gavat, I., Lazarescu, V., & Datcu, M. (2013). A latent analysis of earth surface dynamic evolution using change map time series. *IEEE Transactions on Geoscience and Remote Sensing*, *51*(4), 2105–2118.]"
      },
      {
        "citation": "[Camps-Vallès, G., Gomez-Chova, L., Munoz-Mari, J., Rojo-Alvarez, J., & Martinez-Ramón, M. (2008). Kernel-based framework for multitemporal and multisource remote sensing data classification and change detection. *IEEE Transactions on Geoscience and Remote Sensing*, *46*(6), 5, 1822–1835.]"
      },
      {
        "citation": "[Wang, J., Yang, J., Yu, K., Lv, F., Huang, T., & Gong, Y. (2010). Locality-constrained linear coding for image classification. *IEEE Conference on Computer Vision and Pattern Recognition*, 3360–3367.]"
      }
    ],
    "author_details": [
      {
        "name": "Lionel Gueguen",
        "affiliation": "DigitalGlobe Inc.",
        "email": "lgueguen@digitalglobe.com"
      },
      {
        "name": "Raffay Hamid",
        "affiliation": "DigitalGlobe Inc.",
        "email": "mhamid@digitalglobe.com"
      }
    ]
  },
  {
    "title": "Fusing Subcategory Probabilities for Texture Classiﬁcation\n---AUTHOR---\nYang Song\nWeidong Cai\nQing Li\nFan Zhang\nDavid Dagan Feng\nHeng Huang",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Song_Fusing_Subcategory_Probabilities_2015_CVPR_paper.pdf",
    "id": "Song_Fusing_Subcategory_Probabilities_2015_CVPR_paper",
    "abstract": "Texture classification remains challenging due to high intra-class variation and low inter-class distinction. This paper proposes a sub-categorization model for texture classification. By clustering each class into subcategories, classification probabilities at the subcategory-level are computed based on between-subcategory distinctiveness and within-subcategory representativeness. These subcategory probabilities are then fused based on their contribution levels and cluster qualities. This fused probability is added to the multiclass classification probability to obtain the final class label. The method was applied to texture classification on three challenging datasets – KTH-TIPS2, FMD and DTD – and demonstrated excellent performance compared to state-of-the-art approaches.\n\n---TOPICCS---\nTexture Classification\nSubcategorization\nFeature Fusion\nClustering\nComputer Vision",
    "topics": [],
    "references": [],
    "author_details": [
      {
        "name": "Yang Song",
        "affiliation": "BMIT Research Group, School of IT, University of Sydney, Australia",
        "email": "[Email not available in the provided text]"
      },
      {
        "name": "Weidong Cai",
        "affiliation": "BMIT Research Group, School of IT, University of Sydney, Australia",
        "email": "[Email not available in the provided text]"
      },
      {
        "name": "Qing Li",
        "affiliation": "BMIT Research Group, School of IT, University of Sydney, Australia",
        "email": "[Email not available in the provided text]"
      },
      {
        "name": "Fan Zhang",
        "affiliation": "BMIT Research Group, School of IT, University of Sydney, Australia",
        "email": "[Email not available in the provided text]"
      },
      {
        "name": "David Dagan Feng",
        "affiliation": "BMIT Research Group, School of IT, University of Sydney, Australia",
        "email": "[Email not available in the provided text]"
      },
      {
        "name": "Heng Huang",
        "affiliation": "Department of Computer Science and Engineering, University of Texas, Arlington, USA",
        "email": "[Email not available in the provided text]"
      }
    ]
  },
  {
    "title": "Beyond Frontal Faces: Improving Person Recognition Using Multiple Cues\n---AUTHOR---\nNing Zhang\nManohar Paluri\nYaniv Taigman\nRob Fergus\nLubomir Bourdev",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Zhang_Beyond_Frontal_Faces_2015_CVPR_paper.pdf",
    "id": "Zhang_Beyond_Frontal_Faces_2015_CVPR_paper",
    "abstract": "We explore the task of recognizing peoples’ identities in photo albums in an unconstrained setting. To facilitate this, we introduce the new People In Photo Albums (PIPA) dataset, consisting of over 60000 instances of ∼2000 individuals collected from public Flickr photo albums. With only about half of the person images containing a frontal face, the recognition task is very challenging due to the large variations in pose, clothing, camera viewpoint, image resolution and illumination. We propose the Pose Invariant PErson Recognition (PIPER) method, which accumulates the cues of poselet-level person recognizers trained by deep convolutional networks to discount for the pose variations, combined with a face recognizer and a global recognizer. Experiments on three different settings confirm that in our unconstrained setup PIPER significantly improves on the performance of DeepFace, which is one of the best face recognizers as measured on the LFW dataset.",
    "topics": [
      "Person recognition",
      "Unconstrained settings",
      "Pose Invariant Recognition",
      "PIPA dataset",
      "Deep convolutional networks"
    ],
    "references": [],
    "author_details": [
      {
        "name": "Ning Zhang",
        "affiliation": "UC Berkeley",
        "email": "nzhang@eecs.berkeley.edu"
      },
      {
        "name": "Manohar Paluri",
        "affiliation": "Facebook AI Research",
        "email": "mano@fb.com"
      },
      {
        "name": "Yaniv Taigman",
        "affiliation": "Facebook AI Research",
        "email": "yaniv@fb.com"
      },
      {
        "name": "Rob Fergus",
        "affiliation": "Facebook AI Research",
        "email": "robfergus@fb.com"
      },
      {
        "name": "Lubomir Bourdev",
        "affiliation": "Facebook AI Research",
        "email": "lubomir@fb.com"
      }
    ]
  },
  {
    "title": "Learning Descriptors for Object Recognition and 3D Pose Estimation\n---AUTHOR---\nPaul Wohlhart\n---AUTHOR---\nVincent Lepetit",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Wohlhart_Learning_Descriptors_for_2015_CVPR_paper.pdf",
    "id": "Wohlhart_Learning_Descriptors_for_2015_CVPR_paper",
    "abstract": "Abstract not found",
    "topics": [],
    "references": [],
    "author_details": [
      {
        "name": "Paul Wohlhart",
        "affiliation": "Institute for Computer Vision and Graphics, Graz University of Technology, Austria",
        "email": "{wohlhart}@icg.tugraz.at"
      },
      {
        "name": "Vincent Lepetit",
        "affiliation": "Institute for Computer Vision and Graphics, Graz University of Technology, Austria",
        "email": "{lepetit}@icg.tugraz.at"
      }
    ]
  },
  {
    "title": "Beyond Short Snippets: Deep Networks for Video Classiﬁcation\n---AUTHOR---\nJoe Yue-Hei Ng\nMatthew Hausknecht\nSudheendra Vijayanarasimhan\nOriol Vinyals\nRajat Monga\nGeorge Toderici",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Ng_Beyond_Short_Snippets_2015_CVPR_paper.pdf",
    "id": "Ng_Beyond_Short_Snippets_2015_CVPR_paper",
    "abstract": "Convolutional neural networks (CNNs) have been extensively applied for image recognition problems giving state-of-the-art results on recognition, detection, segmentation and retrieval. In this work we propose and evaluate several deep neural network architectures to combine image information across a video over longer time periods than previously attempted. We propose two methods capable of handling full length videos. The first method explores various convolutional temporal feature pooling architectures, examining the various design choices which need to be made when adapting a CNN for this task. The second proposed method explicitly models the video as an ordered sequence of frames. For this purpose we employ a recurrent neural network that uses Long Short-Term Memory (LSTM) cells which are connected to the output of the underlying CNN. Our best networks exhibit significant performance improvements over previously published results on the Sports 1 million dataset (73.1% vs. 60.9%) and the UCF-101 datasets with (88.6% vs. 88.0%) and without additional optical ﬂow information (82.6% vs. 73.0%).\n\n---TOPIC---\nVideo Classification\nDeep Neural Networks\nConvolutional Neural Networks (CNNs)\nRecurrent Neural Networks (RNNs) / LSTMs\nTemporal Feature Pooling",
    "topics": [],
    "references": [],
    "author_details": [
      {
        "name": "Joe Yue-Hei Ng",
        "affiliation": "University of Maryland, College Park",
        "email": "yhng@umiacs.umd.edu"
      },
      {
        "name": "Matthew Hausknecht",
        "affiliation": "University of Texas at Austin",
        "email": "mhauskn@cs.utexas.edu"
      },
      {
        "name": "Sudheendra Vijayanarasimhan",
        "affiliation": "Google, Inc.",
        "email": "svnaras@google.com"
      },
      {
        "name": "Oriol Vinyals",
        "affiliation": "Google, Inc.",
        "email": "vinyals@google.com"
      },
      {
        "name": "Rajat Monga",
        "affiliation": "Google, Inc.",
        "email": "rajatmonga@google.com"
      },
      {
        "name": "George Toderici",
        "affiliation": "Google, Inc.",
        "email": "gtoderici@google.com"
      }
    ]
  },
  {
    "title": "On the Appearance of Translucnt Edges: Supplementary Material\n---AUTHOR---\nIoannis Gkioulekas\nBruce Walter\nEdward H. Adelson\nTodd Zickler\nKavita Bala",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Gkioulekas_On_the_Appearance_2015_CVPR_supplemental.pdf",
    "id": "Gkioulekas_On_the_Appearance_2015_CVPR_supplemental",
    "abstract": "In the supplementary material, we provide additional results and discussion of several parts of the main paper.",
    "topics": [
      "Edge Radiance Profiles",
      "Non-Idealities (Camera & Illumination)",
      "Surface Roughness/Microfacets",
      "Profile Fitting",
      "Bevel Effects"
    ],
    "references": [
      {
        "citation": "[Walter, B., Marschner, S. R., Li, H., & Torrance, K. E. Microfacet models for refraction through rough surfaces. EGSR, 2007.] - This reference is directly cited and visually referenced within the paper (Figures 10 and 11), indicating its core relevance to the presented work on refraction and edge radiance."
      }
    ],
    "author_details": [
      {
        "name": "Ioannis Gkioulekas",
        "affiliation": "Harvard SEAS",
        "email": "igkio@seas.harvard.edu"
      },
      {
        "name": "Bruce Walter",
        "affiliation": "Cornell University",
        "email": "bruce.walter@cornell.edu"
      },
      {
        "name": "Edward H. Adelson",
        "affiliation": "Massachusetts Institute of Technology",
        "email": "adelson@cail.mit.edu"
      },
      {
        "name": "Todd Zickler",
        "affiliation": "Harvard SEAS",
        "email": "zickler@seas.harvard.edu"
      },
      {
        "name": "Kavita Bala",
        "affiliation": "Cornell University",
        "email": "kb@cs.cornell.edu"
      }
    ]
  },
  {
    "title": "Shape-Tailored Local Descriptors and their Application to Segmentation and Tracking\n---AUTHOR---\nNaeemullah Khan\n---AUTHOR---\nMarei Algarni\n---AUTHOR---\nAnthony Yezzi\n---AUTHOR---\nGanesh Sundaramoorthi",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Khan_Shape-Tailored_Local_Descriptors_2015_CVPR_paper.pdf",
    "id": "Khan_Shape-Tailored_Local_Descriptors_2015_CVPR_paper",
    "abstract": "We propose new dense descriptors for texture segmentation. Given a region of arbitrary shape in an image, these descriptors are formed from shape-dependent scale spaces of oriented gradients. These scale spaces are deﬁned by Poisson-like partial differential equations. A key property of our new descriptors is that they do not aggregate image data across the boundary of the region, in contrast to existing descriptors based on aggregation of oriented gradients. As an example, we show how the descriptor can be incorporated in a Mumford-Shah energy for texture segmentation. We test our method on several challenging datasets for texture segmentation and textured object tracking. Experiments indicate that our descriptors lead to more accurate segmentation than non-shape dependent descriptors and the state-of-the-art in texture segmentation.\n\n---TOPICCS---\nTexture Segmentation\nLocal Descriptors\nPartial Differential Equations (PDE)\nShape-Tailored Descriptors (STLD)\nObject Tracking",
    "topics": [],
    "references": [],
    "author_details": [
      {
        "name": "Naeemullah Khan",
        "affiliation": "King Abdullah University of Science & Technology (KAUST), Saudi Arabia",
        "email": "naeemullah.khan@kust.edu.sa"
      },
      {
        "name": "Marei Algarni",
        "affiliation": "King Abdullah University of Science & Technology (KAUST), Saudi Arabia",
        "email": "marei.algarni@kust.edu.sa"
      },
      {
        "name": "Anthony Yezzi",
        "affiliation": "School of Electrical & Computer Engineering, Georgia Institute of Technology, USA",
        "email": "ayezzi@ece.gatech.edu"
      },
      {
        "name": "Ganesh Sundaramoorthi",
        "affiliation": "King Abdullah University of Science & Technology (KAUST), Saudi Arabia",
        "email": "ganesh.sundaramoorthi@kust.edu.sa"
      }
    ]
  },
  {
    "title": "How Do We Use Our Hands? Discovering a Diverse Set of Common Grasps\n---AUTHOR---\nDe-An Huang\nMinghuang Ma\nWei-Chiu Ma\nKris M. Kitani",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Huang_How_Do_We_2015_CVPR_paper.pdf",
    "id": "Huang_How_Do_We_2015_CVPR_paper",
    "abstract": "Our aim is to show how state-of-the-art computer vision techniques can be used to advance prehensile analysis (i.e., understanding the functionality of human hands). Prehensile analysis is a broad field of multi-disciplinary interest, where researchers painstakingly manually analyze hours of hand-object interaction videos to understand the mechanics of hand manipulation. In this work, we present promising empirical results indicating that wearable cameras and unsupervised clustering techniques can be used to automatically discover common modes of human hand use. In particular, we use a first-person point-of-view camera to record common manipulation tasks and leverage its strengths for reliably observing human hand use. To learn a diverse set of hand-object interactions, we propose a fast online clustering algorithm based on the Determinantal Point Process (DPP). Furthermore, we develop a hierarchical extension to the DPP clustering algorithm and show that it can be used to discover appearance-based grasp taxonomies. Using a purely data-driven approach, our proposed algorithm is able to obtain hand grasp taxonomies that roughly correspond to the classic Cutkosky grasp taxonomy. We validate our approach on over 10 hours of first-person point-of-view videos in both choreographed and real-life scenarios.",
    "topics": [
      "Prehensile Analysis",
      "Computer Vision",
      "Hand-Object Interaction",
      "Determinantal Point Process (DPP)",
      "Grasp Taxonomy"
    ],
    "references": [
      {
        "citation": "[N. Ailon, R. Jaiswal, and C. Monteleoni. Streaming k-means approximation. In NIPS, 2009.]"
      },
      {
        "citation": "[W. Barbakh and C. Fyfe. Online clustering algorithms. International Journal of Neural Systems, 18(03):185–194, 2008.]"
      },
      {
        "citation": "[A. Fathi, J. Hodgins, and J. Rehg. Social interactions: A first-person perspective. In CVPR, 2012.]"
      },
      {
        "citation": "[R. Filipovych and E. Ribeiro. Recognizing primitive interactions by exploring actor-object states. In CVPR, 2008.]"
      },
      {
        "citation": "[J. Case-Smith and C. Pehoski. Development of hand skills in children. American Occupational Therapy Association, 1992.]"
      },
      {
        "citation": "[L. Cheng and K. M. Kitani. Pixel-level hand detection in ego-centric videos. In CVPR, 2013.]"
      },
      {
        "citation": "[M. Cutkosky. On grasp choice, grasp models, and design of hands for manufacturing tasks. Trans. on Robotics and Automation, 5(3):269–279, 1989.]"
      },
      {
        "citation": "[V. Delaitre, J. Sivic, and I. Laptev. Learning person-object interactions for action recognition in still images. In NIPS, 2011.]"
      },
      {
        "citation": "[H. N. Djidjev, G. E. Pantzious, and C. D. Zaroliagis. Computing shortest paths and distances in planar graphs. In Automata, Languages and Programming, pages 27–338. Springer, 1991.]"
      },
      {
        "citation": "[C. Desai, D. Ramanan, and C. Fowlkes. Discriminaitve models for static human-object interactions. In CVPR Workshops, 2010.]"
      }
    ],
    "author_details": [
      {
        "name": "De-An Huang",
        "affiliation": "Carnegie Mellon University",
        "email": "deanh@andrew.cmu.edu"
      },
      {
        "name": "Minghuang Ma",
        "affiliation": "Carnegie Mellon University",
        "email": "minghuam@andrew.cmu.edu"
      },
      {
        "name": "Wei-Chiu Ma",
        "affiliation": "Carnegie Mellon University",
        "email": "weichium@andrew.cmu.edu"
      },
      {
        "name": "Kris M. Kitani",
        "affiliation": "Carnegie Mellon University",
        "email": "kkitani@cs.cmu.edu"
      }
    ]
  },
  {
    "title": "Becoming the Expert - Interactive Multi-Class Machine Teaching\n---AUTHOR---\nEdward Johns\nOisin Mac Aodha\nGabriel J. Brostow",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Johns_Becoming_the_Expert_2015_CVPR_paper.pdf",
    "id": "Johns_Becoming_the_Expert_2015_CVPR_paper",
    "abstract": "This paper proposes an Interactive Machine Teaching algorithm that enables a computer to teach challenging visual concepts to a human. The algorithm adaptively chooses labeled images from a teaching set to show the student as they learn, based on a probabilistic model of the student’s ability and progress informed by their responses. Results with real human participants across varied datasets demonstrate that this teaching strategy produces better ‘experts’ compared to traditional methods. The work addresses the challenge of designing effective teaching sets for annotators with varying degrees of expertise.\n\n---TOPICICS---\nInteractive Machine Teaching\nHuman Learning\nAdaptive Algorithms\nVisual Classification\nExpertise Modeling",
    "topics": [],
    "references": [
      {
        "citation": "[Bruner, J. S. The Process of Education. Harvard University Press, 1960.] - Provides foundational concepts related to teaching and learning, relevant to the paper's exploration of automated teaching methods."
      },
      {
        "citation": "[Bengio, Y., Louradour, R., Collobert, R., & Weston, J. Curriculum learning. In ICML, 2009.] - Introduces the concept of curriculum learning, a technique that aligns with the paper's focus on teaching strategies."
      },
      {
        "citation": "[Love, B. C. Categorization. In Oxford Handbook of Cognitive Neuroscience, pages 342–358. 2013.] - Provides a cognitive neuroscience perspective on categorization, a core element of the tasks being taught."
      },
      {
        "citation": "[Roy, N., & McCallum, A. Toward optimal active learning through sampling estimation of error reduction. In ICML, 2001.] - A foundational paper on active learning, a key technique explored in the paper."
      },
      {
        "citation": "[Settles, B. Active Learning. Morgan & Claypool, 2012.] - Provides a comprehensive overview of active learning techniques."
      },
      {
        "citation": "[Zhu, X. Machine teaching: An inverse problem to machine learning and an approach toward optimal education. AAAI Conference on Artificial Intelligence (Senior Member Track), 2015.] - Introduces the concept of machine teaching, directly relevant to the paper's approach."
      },
      {
        "citation": "[Love, B. C., & Patil, K. R. Optimal teaching for limited-capacity human learners. In NIPS, 2014.] - Addresses the challenge of teaching learners with limited cognitive capacity, a consideration in the paper's design."
      },
      {
        "citation": "[Balbach, F. J., & Zeugmann, T. Recent developments in algorithmic teaching. In Language and Automata Theory and Applications. 2009.] - Provides background on algorithmic teaching methods."
      },
      {
        "citation": "[Basu, S., & Christensen, J. Teaching classification boundaries to humans. In AAAI, 2013.] - Directly related to the paper's focus on teaching classification tasks."
      },
      {
        "citation": "[Gigu`ere, G., & Love, B. C. Limits in decision making arise from limits in memory retrieval. PNAS, 110(19):7613–7618, 2013.] - Explores the cognitive limitations that influence learning, informing the design of effective teaching strategies."
      },
      {
        "citation": "---"
      }
    ],
    "author_details": [
      {
        "name": "Edward Johns",
        "affiliation": "University College London",
        "email": "[Not available in the provided text]"
      },
      {
        "name": "Oisin Mac Aodha",
        "affiliation": "University College London",
        "email": "[Not available in the provided text]"
      },
      {
        "name": "Gabriel J. Brostow",
        "affiliation": "University College London",
        "email": "[Not available in the provided text]"
      }
    ]
  },
  {
    "title": "Efﬁcient Globally Optimal Consensus Maximisation with Tree Search\n---AUTHORISTS---\nTat-Jun Chin\nPulak Purkait\nAnders Eriksson\nDavid Suter",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Chin_Efficient_Globally_Optimal_2015_CVPR_paper.pdf",
    "id": "Chin_Efficient_Globally_Optimal_2015_CVPR_paper",
    "abstract": "Maximum consensus is a widely used criterion for robust estimation in computer vision. While popular, optimizing this criterion is typically done using randomized sample-and-test techniques that don't guarantee optimality. Existing globally optimal algorithms are too slow to compete with randomized methods. This paper proposes a highly efficient algorithm for global maximization of consensus, framing consensus maximization as a tree search problem using LP-type methods. The algorithm utilizes A* search and efficient heuristic and support set updating routines to rapidly find globally optimal results, achieving orders of magnitude faster performance compared to previous exact methods.\n\n---TOPICAS---\nMaximum Consensus\nA* Search\nRobust Estimation\nTree Search\nLP-type Methods",
    "topics": [],
    "references": [
      {
        "citation": "[N. Amenta, M. Bern, and D. Eppstein. Optimal point placement for mesh smoothing. In SODA, 1997.] - Appears relevant due to the focus on mesh smoothing and optimization."
      },
      {
        "citation": "[B. Chazelle and J. Matouˇsek. On linear-time deterministic algorithms for optimization problems in ﬁxed dimensions. In Symp. Discrete Algorithms, 1993.] - Deals with optimization algorithms, a core theme."
      },
      {
        "citation": "[D. Eppstein. Quasiconvex programming. Combinatorial and Computational Geometry, 25, 2005.] - Directly related to a key concept mentioned in the paper."
      },
      {
        "citation": "[M. A. Fischler and R. C. Bolles. Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography. Comm. of the ACM, 24(6):381–395, 1981.] - Introduces the RANSAC algorithm, a fundamental technique for outlier rejection."
      },
      {
        "citation": "[R. Hartley and A. Zisserman. Multiple view geometry in computer vision. Cambridge University Press, 2nd edition, 2004.] - A standard reference for multiple view geometry, a central topic."
      },
      {
        "citation": "[H. Li. A practical algorithm for l∞ triangulation with outliers. In CVPR, 2007.] - Addresses a specific triangulation method with outlier handling."
      },
      {
        "citation": "[J. Matouˇsek. On geometric optimization with few violated constraints. Discrete and computational geometry, 14(1):365–384, 1995.] - Discusses geometric optimization and constraints, relevant to the problem."
      },
      {
        "citation": "[C. Olsson, O. Enqvist, and F. Kahl. A polynomial-time bound for matching and registration with outliers. In CVPR, 2008.] - Focuses on outlier handling in matching and registration."
      },
      {
        "citation": "[C. Olsson, A. Eriksson, and F. Kahl. Efﬁcient optimization for l∞-problems using pseudoconvexity. In ICCV, 2007.] - Addresses optimization techniques specifically for l∞-norm problems."
      }
    ],
    "author_details": [
      {
        "name": "Tat-Jun Chin",
        "affiliation": "School of Computer Science, The University of Adelaide",
        "email": "Not available"
      },
      {
        "name": "Pulak Purkait",
        "affiliation": "School of Computer Science, The University of Adelaide",
        "email": "Not available"
      },
      {
        "name": "Anders Eriksson",
        "affiliation": "School of Electrical Engineering and Computer Science, Queensland University of Technology",
        "email": "Not available"
      },
      {
        "name": "David Suter",
        "affiliation": "School of Computer Science, The University of Adelaide",
        "email": "Not available"
      }
    ]
  },
  {
    "title": "Small-Variance Nonparametric Clustering on the Hypsphere\n---AUTHOR---\nJulian Straub\nTrevor Campbell\nJonathan P. How\nJohn W. Fisher III",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Straub_Small-Variance_Nonparametric_Clustering_2015_CVPR_paper.pdf",
    "id": "Straub_Small-Variance_Nonparametric_Clustering_2015_CVPR_paper",
    "abstract": "Structural regularities in man-made environments reflect in the distribution of their surface normals. Based on the small-variance limit of Bayesian nonparametric von-Mises-Fisher (vMF) mixture distributions, we propose two new flexible and efficient k-means-like clustering algorithms for directional data such as surface normals. The first, DP-vMF-means, is a batch clustering algorithm derived from the Dirichlet process (DP) vMF mixture. Recognizing the sequential nature of data collection, we extend this algorithm to DDP-vMF-means, which infers temporally evolving cluster structure from streaming data. Both algorithms naturally respect the geometry of directional data, which lies on the unit sphere. We demonstrate their performance on synthetic directional data and real 3D surface normals from RGB-D sensors. The algorithms generalize to high dimensional directional data such as protein backbone configurations and semantic word vectors.",
    "topics": [
      "Surface Normals",
      "Bayesian Nonparametric Clustering",
      "von-Mises-Fisher Distributions",
      "Streaming Data Analysis",
      "Spherical Geometry"
    ],
    "references": [
      {
        "citation": "[Abramowitz, M. and Stegun, I., editors. Handbook of Mathematical Functions. Dover Books on Mathematics. Dover Publications, 1965.] - This is a foundational reference providing mathematical background."
      },
      {
        "citation": "[Neal, R. Markov chain sampling methods for Dirichlet process mixture models. Journal of Computational and Graphical Statistics, 9(2):249–265, 2000.] - A key reference on sampling methods for DPMMs."
      },
      {
        "citation": "[Jiang, K., Kulis, B., and Jordan, M. Small-variance asymptotics for exponential family Dirichlet process mixture models. In NIPS, 2012.] - Provides theoretical analysis of DPMMs."
      },
      {
        "citation": "[Blei, D. M., Ng, A. Y., and Jordan, M. I. Latent Dirichlet allocation. JMLR, 3:993–1022, 2003.] - A seminal work on topic modeling, relevant to spherical topic models and related concepts."
      },
      {
        "citation": "[Kulis, B. and Jordan, M. I. Revisiting k-means: New algorithms via Bayesian nonparametrics. In ICML, 2012.] - Explores connections between k-means and Bayesian nonparametrics."
      },
      {
        "citation": "[Mardia, K. V. and Jupp, P. E. Directional statistics, volume 494. John Wiley & Sons, 2009.] - Provides a comprehensive treatment of directional statistics."
      },
      {
        "citation": "[Ferguson, T. A Bayesian analysis of some nonparametric problems. The Annals of Statistics, 1973.] - A foundational paper on Bayesian nonparametric methods."
      },
      {
        "citation": "[Blackwell, D. and MacQueen, J. B. Ferguson distributions via p´olya urn schemes. The Annals of Statistics, 1973.] - Introduces Ferguson distributions, crucial for DPMMs."
      },
      {
        "citation": "[Straub, J., Chang, J., Freifeld, O., and Fisher III, J. W. A Dirichlet process mixture model for spherical data. In AISTATS, 2015.] - Focuses on DPMMs specifically for spherical data."
      },
      {
        "citation": "[Teh, Y. W. Dirichlet processes. In Encyclopedia of Machine Learning. Springer, 2010.] - Provides a broad overview of Dirichlet processes."
      },
      {
        "citation": "---"
      }
    ],
    "author_details": [
      {
        "name": "Julian Straub",
        "affiliation": "CSAIL and LIDS, Massachusetts Institute of Technology",
        "email": "jstraub@csaill.mit.edu"
      },
      {
        "name": "Trevor Campbell",
        "affiliation": "CSAIL and LIDS, Massachusetts Institute of Technology",
        "email": "tdjc@.mit.edu"
      },
      {
        "name": "Jonathan P. How",
        "affiliation": "CSAIL and LIDS, Massachusetts Institute of Technology",
        "email": "jhow@.mit.edu"
      },
      {
        "name": "John W. Fisher III",
        "affiliation": "CSAIL and LIDS, Massachusetts Institute of Technology",
        "email": "fisher@csaill.mit.edu"
      }
    ]
  },
  {
    "title": "Co-saliency Detection via Looking Deep and Wide\n---AUTHORs---\nDingwen Zhang\nJunwei Han\nChao Li\nJingdong Wang",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Zhang_Co-Saliency_Detection_via_2015_CVPR_paper.pdf",
    "id": "Zhang_Co-Saliency_Detection_via_2015_CVPR_paper",
    "abstract": "With the goal of effectively identifying common and salient objects in a group of relevant images, co-saliency detection has become essential for many applications such as video foreground extraction, surveillance, image retrieval, and image annotation. In this paper, we propose a unified co-saliency detection framework by introducing two novel insights: 1) looking deep to transfer higher-level representations by using the convolutional neural network with additional adaptive layers could better reflect the properties of the co-salient objects, especially their consistency among the image group; 2) looking wide to take advantage of the visually similar neighbors beyond a certain image group could effectively suppress the influence of the common background regions when formulating the intra-group consistency. In the proposed framework, the wide and deep information are explored for the object proposal windows extracted in each image, and the co-saliency scores are calculated by integrating the intra-image contrast and intra-group consistency via a principled Bayesian formulation. Finally the window-level co-saliency scores are converted to the superpixel-level co-saliency maps through a foreground region agreement strategy. Comprehensive experiments on two benchmark datasets have demonstrated the consistent performance gain of the proposed approach.\n\n---TOPIPS---\nCo-saliency detection\nConvolutional Neural Networks (CNNs)\nImage Group Consistency\nBayesian Formulation\nVisual Attention",
    "topics": [],
    "references": [
      {
        "citation": "[Batra, D., Kowdle, A., Parikh, D., Jie, L., and Chen, T. iCoseg: Interactive co-segmentation with intelligent scribble guidance. CVPR, 2010.]"
      },
      {
        "citation": "[Li, H., Meng, F., and Ngan, K. N. Co-Salient Object Detection From Multiple Images. IEEE Trans. Multimedia, 15(8): 1896-1909, 2013.]"
      },
      {
        "citation": "[Shen, X., and Wu, Y. A unified approach to salient object detection via low rank matrix recovery. CVPR, 2012.]"
      },
      {
        "citation": "[Xie, Y., Lu, H., and Yang, M.-H. Bayesian saliency via low and mid level cues. IEEE Trans. Image Process., 22(5): 1689-1698, 2013.]"
      },
      {
        "citation": "[Jiang, H., Wang, J., Yuan, Z., Wu, Y., Zheng, N., and Li, S. Salient object detection: A discriminative regional feature integration approach. CVPR, 2013.]"
      },
      {
        "citation": "[Han, J., He, S., Qian, X., Wang, D., Guo, L., and Liu, T. An object-oriented visual saliency detection framework based on sparse coding representations. IEEE Trans. Circuits Syst. Video Technol., 23(12):2009 -2021, 2013.]"
      },
      {
        "citation": "[Rubinstein, M., Joulin, A., Kopf, J., and Liu, C. Unsupervised joint object discovery and segmentation in internet images. CVPR, 2013.]"
      },
      {
        "citation": "[Jiang, H., Wang, J., Yuan, Z., Liu, T., Zheng, N., and Li, S. Automatic salient object segmentation based on context and shape prior. BMVC, 2011.]"
      },
      {
        "citation": "[Cao, X., Tao, Z., Zhang, B., Fu, H., and Feng, W. Self-Adaptively Weighted Co-Saliency Detection via Rank Constraint. IEEE Trans. Image Process., 23(9):4175-4186, 2014.]"
      },
      {
        "citation": "[Han, J., Zhang, D., Wen, S., Guo, L., Liu, T., and Li, X. Two-Stage Learning to Predict Human Eye Fixations via SDAEs. IEEE Trans. on Cybernetics, 2015.]"
      }
    ],
    "author_details": [
      {
        "name": "Dingwen Zhang",
        "affiliation": "Northwestern Polytechnical University, P.R. China",
        "email": "zhangdingwen2006yyy@gmail.com"
      },
      {
        "name": "Junwei Han",
        "affiliation": "Northwestern Polytechnical University, P.R. China",
        "email": "junweihan2010@gmail.com"
      },
      {
        "name": "Chao Li",
        "affiliation": "Northwestern Polytechnical University, P.R. China",
        "email": "lllcho1314@gmail.com"
      },
      {
        "name": "Jingdong Wang",
        "affiliation": "Microsoft Research, P.R. China",
        "email": "jingdw@microsoft.com"
      }
    ]
  },
  {
    "title": "Cross-scene Crowd Counting via Deep Convolutional Neural Networks\n---AUTHOR---\nCong Zhang\nHongsheng Li\nXiaogang Wang\nXiaokang Yang",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Zhang_Cross-Scene_Crowd_Counting_2015_CVPR_paper.pdf",
    "id": "Zhang_Cross-Scene_Crowd_Counting_2015_CVPR_paper",
    "abstract": "Cross-scene crowd counting is a challenging task where no laborious data annotation is required for counting people in new target surveillance crowd scenes unseen in the training set. The performance of most existing crowd counting methods drops significantly when they are applied to an unseen scene. To address this problem, we propose a deep convolutional neural network (CNN) for crowd counting, and it is trained alternatively with two related learning objectives, crowd density and crowd count. This proposed switchable learning approach is able to obtain better local optimum for both objectives. To handle an unseen target crowd scene, we present a data-driven method to fine-tune the trained CNN model for the target scene. A new dataset including 108 crowd scenes with nearly 200,000 head annotations is introduced to better evaluate the accuracy of cross-scene crowd counting methods. Extensive experiments on the proposed and another two existing datasets demonstrate the effectiveness and reliability of our approach.\n\n---TOPIC---\nCrowd Counting\nDeep Convolutional Neural Networks (CNNs)\nCross-Scene Adaptation\nDensity Estimation\nDataset Creation",
    "topics": [],
    "references": [
      {
        "citation": "[Chen, K., Gong, S., Xiang, T., Mary, Q., & Loy, C. C. (2013). Cumulative attribute space for age and crowd density estimation. *CVPR*.] - This paper introduces a cumulative attribute space, directly relevant to crowd density estimation, a core topic."
      },
      {
        "citation": "[Lempitsky, V., & Zisserman, A. (2010). Learning to count objects in images. *NIPS*.] - This is a foundational work on learning to count objects, a key problem addressed by the paper."
      },
      {
        "citation": "[Chen, K., Loy, C. C., Gong, S., & Xiang, T. (2012). Feature mining for localised crowd counting. *BMVC*.] - Focuses on feature mining specifically for localized crowd counting, a crucial aspect of the research."
      },
      {
        "citation": "[An, S., Liu, W., & Venkatesh, S. (2007). Face recognition using kernel ridge regression. *CVPR*.] - While seemingly unrelated, it demonstrates the use of kernel ridge regression, a technique potentially applicable to other vision tasks."
      },
      {
        "citation": "[Kai, K., & Xiaogang, W. (2014). Fully convolutional neural networks for crowd segmentation. *arXiv preprint arXiv:1411.4464*.] - Introduces a fully convolutional network for crowd segmentation, a significant advancement in the field."
      },
      {
        "citation": "[Kong, D., Gray, D., & Tao, H. (2006). A viewpoint invariant approach for crowd counting. *ICPR*.] - Addresses the challenge of viewpoint invariance in crowd counting, a practical consideration."
      },
      {
        "citation": "[Jing, S., Kai, K., Chen, L., Chang, & Xiaogang, W. (2015). Deeply learned attributes for crowd scene understanding. *CVPR*.] - Explores the use of deep learning for attribute extraction in crowd scenes, contributing to a more comprehensive understanding."
      },
      {
        "citation": "[Fiaschi, L., Nair, R., Koethe, U., & Hamprecht, F. A. (2012). Learning to count with regression forest and structured labels. *ICPR*.] - Investigates a specific machine learning approach (regression forest) for counting, offering an alternative to neural networks."
      },
      {
        "citation": "[Loy, C. C., Gong, S., & Xiang, T. (2013). From semi-supervised to transfer counting of crowds. *ICCV*.] - Explores techniques for improving crowd counting using semi-supervised and transfer learning approaches."
      },
      {
        "citation": "[Lowe, D. G. (2004). Distinctive image features from scale-invariant keypoints. *IJCV*.] - Introduces SIFT features, a common building block for many computer vision tasks, including crowd analysis."
      }
    ],
    "author_details": [
      {
        "name": "Cong Zhang",
        "affiliation": "Institute of Image Communication and Network Engineering, Shanghai Jiao Tong University",
        "email": "zhangcong0929@gmail.com"
      },
      {
        "name": "Hongsheng Li",
        "affiliation": "Department of Electronic Engineering, The Chinese University of Hong Kong and School of Electronic Engineering, University of Electronic Science and Technology of China",
        "email": "lihongsheng@gmail.com"
      },
      {
        "name": "Xiaogang Wang",
        "affiliation": "Department of Electronic Engineering, The Chinese University of Hong Kong",
        "email": "xgwang@ee.cuhk.edu.hk"
      },
      {
        "name": "Xiaokang Yang",
        "affiliation": "Institute of Image Communication and Network Engineering, Shanghai Jiao Tong University",
        "email": "xkyang@sjtu.edu.cn"
      }
    ]
  },
  {
    "title": "Deep Semantic Ranking Based Hashing for Multi-Label Image Retrieval\n---AUTHOR---\nFang Zhao\nYongzhen Huang\nLiang Wang\nTieniu Tan",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Zhao_Deep_Semantic_Ranking_2015_CVPR_paper.pdf",
    "id": "Zhao_Deep_Semantic_Ranking_2015_CVPR_paper",
    "abstract": "With the rapid growth of web images, hashing has received increasing interests in large scale image retrieval. However, most existing hashing methods are designed to handle simple binary similarity and fail to explore the complex multilevel semantic structure of images associated with multiple labels. This paper proposes a deep semantic ranking based method for learning hash functions that preserve multilevel semantic similarity between multi-label images. The approach incorporates a deep convolutional neural network into hash functions to jointly learn feature representations and mappings from them to hash codes, avoiding the limitation of semantic representation power of hand-crafted features. A ranking list that encodes the multilevel similarity information is employed to guide the learning of such deep hash functions. Experimental results show the superiority of the proposed approach over several state-of-the-art hashing methods.\n\n---TOPICICS---\nDeep Convolutional Neural Networks (CNN)\nMulti-label Image Retrieval\nHashing Functions\nSemantic Ranking\nBinary Codes",
    "topics": [],
    "references": [
      {
        "citation": "[Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). ImageNet classification with deep convolutional neural networks. In *Advances in Neural Information Processing Systems* (NIPS).]"
      },
      {
        "citation": "[Gong, Y., Jia, Y., Leung, T., Toshev, A., & Ioffe, S. (2013). Deep convolutional ranking for multilabel image annotation. *CoRR*, abs/1312.4894.]"
      },
      {
        "citation": "[Gong, Y., Lazebnik, S., Gordo, A., & Perronnin, F. (2012). Iterative quantization: a procustean approach to learning binary codes for large-scale image retrieval. *IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)*.]"
      },
      {
        "citation": "[Norouzi, M., Fleet, D. J., & Salakhutdinov, R. (2012). Hamming distance metric learning. In *Advances in Neural Information Processing Systems* (NIPS).]"
      },
      {
        "citation": "[Wang, J., Kumar, S., & Chang, S.-F. (2010). Semi-supervised hashing for scalable image retrieval. In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition* (CVPR).]"
      },
      {
        "citation": "[Torralba, A., Fergus, R., & Weiss, Y. (2008). Small codes and large image databases for recognition. In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition* (CVPR).]"
      },
      {
        "citation": "[Gong, Y., Jia, Y., Leung, T., Toshev, A., & Ioffe, S. (2013). Deep convolutional ranking for multilabel image annotation. *CoRR*, abs/1312.4894.]"
      },
      {
        "citation": "[Krizhevsky, A. (2014). One weird trick for parallellizing convolutional neural networks. *CoRR*, abs/1404.5997.]"
      },
      {
        "citation": "[Norouzi, M., Fleet, D. J., & Salakhutdinov, R. (2011). Minimal loss hashing for compact binary codes. In *Proceedings of the International Conference on Machine Learning* (ICML).]"
      },
      {
        "citation": "[Lin, G., Shen, C., & Xin, J. (2014). Optimizing ranking measures for compact binary code learning. In *Proceedings of the European Conference on Computer Vision* (ECCV).]"
      }
    ],
    "author_details": [
      {
        "name": "Fang Zhao",
        "affiliation": "Center for Research on Intelligent Perception and Computing, Institute of Automation, Chinese Academy of Sciences",
        "email": "fang.zhao@nlpr.ia.ac.cn"
      },
      {
        "name": "Yongzhen Huang",
        "affiliation": "Center for Research on Intelligent Perception and Computing, Institute of Automation, Chinese Academy of Sciences",
        "email": "yzhuang@nlpr.ia.ac.cn"
      },
      {
        "name": "Liang Wang",
        "affiliation": "Center for Research on Intelligent Perception and Computing, Institute of Automation, Chinese Academy of Sciences",
        "email": "wangliang@nlpr.ia.ac.cn"
      },
      {
        "name": "Tieniu Tan",
        "affiliation": "Center for Research on Intelligent Perception and Computing, Institute of Automation, Chinese Academy of Sciences",
        "email": "tnt@nlpr.ia.ac.cn"
      }
    ]
  },
  {
    "title": "Adaptive Eye-Camera Calibration for Head-Worn Devices\n---AUTHOR---\nDavid Perra\n---AUTHOR---\nRohit Kumar Gupta\n---AUTHOR---\nJan-Micheal Frahm",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Perra_Adaptive_Eye-Camera_Calibration_2015_CVPR_paper.pdf",
    "id": "Perra_Adaptive_Eye-Camera_Calibration_2015_CVPR_paper",
    "abstract": "We present a novel, continuous, locally optimal calibration scheme for use with head-worn devices. Current calibration schemes solve for a globally optimal model of the eye-device transformation by performing calibration on a per-user or once-per-use basis. However, these calibration schemes are impractical for real-world applications because they do not account for changes in calibration during the time of use. Our calibration scheme allows a head-worn device to calculate a locally optimal eye-device transformation on demand by computing an optimal model from a local window of previous frames. By leveraging naturally occurring interest regions within the user’s environment, our system can calibrate itself without the user’s active participation. Experimental results demonstrate that our proposed calibration scheme outperforms the existing state of the art systems while being significantly less restrictive to the user and the environment.\n\n---TOPICICS---\nEye-camera calibration\nHead-worn devices\nGaze tracking\nPoint of Regard (PoR) estimation\nAdaptive calibration",
    "topics": [],
    "references": [
      {
        "citation": "[F. Alnajar, T. Gevers, R. Valenti, and S. Ghebreab, Calibration-free gaze estimation using human gaze patterns, 15th IEEE International Conference on Computer Vision, 2013.]"
      },
      {
        "citation": "[J. Chen and Q. Ji, Probabilistic gaze estimation without active personal calibration, Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on, 2011.]"
      },
      {
        "citation": "[F. Corno, L. Farinetti, and I. Signorile, A cost-effective solution for eye-gaze assistive technology, IEEE International Conference on Multimedia and Expo, 2002.]"
      },
      {
        "citation": "[E. Guestrin and E. Eizenman, General theory of remote gaze estimation using the pupil center and corneal reflections, IEEE Transactions on Biomedical Engineering, 53(6):1124–1133, June 2006.]"
      },
      {
        "citation": "[D. Hansen and Q. Ji, In the eye of the beholder: A survey of models for eyes and gaze, IEEE Transactions on Pattern Analysis and Machine Intelligence, 32(3):478–500, March 2010.]"
      },
      {
        "citation": "[J. Harel, C. Koch, and P. Perona, Graph-based visual saliency, Advances in Neural Information Processing Systems 19, 2007.]"
      },
      {
        "citation": "[X. Hou, J. Harel, and C. Koch, Image signature: Highlighting sparse salient regions, IEEE Transactions on Pattern Analysis and Machine Intelligence, 34(1):194–201, 2012.]"
      },
      {
        "citation": "[U. Lahiri, Z. Warren, and N. Sarkar, Design of a gaze-sensitive virtual social interactive system for children with autism, Neural Systems and Rehabilitation Engineering, IEEE Transactions on, 19(4):443–452, Aug 2011.]"
      },
      {
        "citation": "[R. Kumar, A. Ilie, J.-M. Frahm, and M. Pollefeys, Simple calibration of non-overlapping cameras with a mirror, Computer Vision and Pattern Recognition, 2008.]"
      },
      {
        "citation": "[U. Lahiri, Z. Warren, and N. Sarkar, Dynamic gaze measurement with adaptive response technology in virtual reality based social communication for autism, Virtual Rehabilitation (ICVR), 2011.]"
      }
    ],
    "author_details": [
      {
        "name": "David Perra",
        "affiliation": "Google Inc.",
        "email": "perra@google.com"
      },
      {
        "name": "Rohit Kumar Gupta",
        "affiliation": "The University of North Carolina at Chapel Hill.",
        "email": "rkgupta@cs.unc.edu"
      },
      {
        "name": "Jan-Micheal Frahm",
        "affiliation": "The University of North Carolina at Chapel Hill.",
        "email": "jmf@cs.unc.edu"
      }
    ]
  },
  {
    "title": "Direct Structure Estimation for 3D Reconstruction",
    "authors": [
      "Nianuan Jiang",
      "Wen-Yan Lin",
      "Minh N. Do",
      "Jiangbo Lu"
    ],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Jiang_Direct_Structure_Estimation_2015_CVPR_paper.pdf",
    "id": "Jiang_Direct_Structure_Estimation_2015_CVPR_paper",
    "abstract": "Most conventional structure-from-motion (SFM) techniques require camera pose estimation before computing any scene structure. In this work we show that when combined with single/multiple homography estimation, the general Euclidean rigidity constraint provides a simple formulation for scene structure recovery without explicit camera pose computation. This direct structure estimation (DSE) opens a new way to design a SFM system that reverses the order of structure and motion estimation. We show that this alternative approach works well for recovering scene structure and camera poses from sideway motion given planar or general man-made scenes.\n\n---TOPIC---\nStructure from Motion (SFM)\nHomography Estimation\nEuclidean Rigidity\nDirect Structure Estimation (DSE)\nCamera Pose Estimation",
    "topics": [],
    "references": [
      {
        "citation": "[D. Nistér, An efficient solution to the five-point relative pose problem, IEEE Trans. PAMI, 2004]"
      },
      {
        "citation": "[D. G. Aliaga, J. Zhang, and M. Boutin, Simplifying the reconstruction of 3d models using parameter elimination, ICCV, 2007]"
      },
      {
        "citation": "[K. S. Arun, T. S. Huang, and S. D. Blostein, Least-squares fitting of two 3-d point sets, IEEE Trans. PAMI, 1987]"
      },
      {
        "citation": "[D. Crandall, A. Owens, N. Snavely, and D. Huttenlocher, Discrete-continuous optimization for large-scale structure from motion, CVPR, 2011]"
      },
      {
        "citation": "[D. W. Eggert, A. Lorusso, and R. B. Fisher, Estimating 3-d rigid body transformations: a comparison of four major algorithms, Machine Vision and Applications, 1997]"
      },
      {
        "citation": "[M. A. Fischler and R. C. Bolles, Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography, Communications of the ACM, 1981]"
      },
      {
        "citation": "[N. M. Grzywacz and E. C. Hildreth, Incremental rigidity scheme for recovering structure from motion: Position-based versus velocity-based formulations, J. Opt. Soc. Am. A, 1987]"
      },
      {
        "citation": "[R. Hartley, In defense of the eight-point algorithm, IEEE Trans. PAMI, 1997]"
      },
      {
        "citation": "[H. Isack and Y. Boykov, Energy-based geometric multi-model fitting, IJCV, 2012]"
      },
      {
        "citation": "[N. Jiang, Z. Cui, and P. Tan, A global linear method for camera pose registration, ICCV, 2013]"
      }
    ],
    "author_details": [
      {
        "name": "Nianuan Jiang",
        "affiliation": "Advanced Digital Sciences Center, Singapore",
        "email": "Not available"
      },
      {
        "name": "Wen-Yan Lin",
        "affiliation": "Advanced Digital Sciences Center, Singapore",
        "email": "Not available"
      },
      {
        "name": "Minh N. Do",
        "affiliation": "University of Illinois at Urbana-Champaign",
        "email": "Not available"
      },
      {
        "name": "Jiangbo Lu",
        "affiliation": "Advanced Digital Sciences Center, Singapore",
        "email": "Not available"
      }
    ]
  },
  {
    "title": "Discriminative Shape from Shading in Uncalibrated Illumination – Supplementary Material –\n---AUTHOR---\nStephan R. Richter\nStefan Roth",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Richter_Discriminative_Shape_From_2015_CVPR_supplemental.pdf",
    "id": "Richter_Discriminative_Shape_From_2015_CVPR_supplemental",
    "abstract": "This supplementary material provides additional results from a quantitative comparison of a novel shape from shading method to other approaches, focusing on laboratory and natural illumination conditions. The method combines local and global context within a learning framework, demonstrating improved reconstructions compared to approaches relying on smooth local context. Results on real images under both laboratory and natural illumination are presented, highlighting the method's ability to adapt to unknown reflectance maps and reconstruct fine detail without strong smoothness priors. A new ground truth dataset of real objects is also introduced.\n\n---TOPIC---\nShape from Shading\n---TOPI---\nIllumination Estimation\n---TOPI---\nLocal and Global Context\n---TOPI---\nMachine Learning\n---TOPI---\nSurface Reconstruction",
    "topics": [],
    "references": [
      {
        "citation": "[Barron, J. T., & Malik, J. (2012). Color constancy, intrinsic images, and shape estimation. In ECCV.] - Appears to be a foundational reference for color constancy and intrinsic images, relevant to the paper's topic."
      },
      {
        "citation": "[Johnson, M. K., & Adelson, E. H. (2011). Shape estimation in natural illumination. In CVPR.] - Another key reference related to shape estimation under varying illumination conditions."
      },
      {
        "citation": "[Xiong, Y., Chakrabarti, A., Basri, R., Gortler, S. J., Jacobs, D. W., & Zickler, T. (2014). From shading to local shape. *IEEE Transactions on Pattern Analysis and Machine Intelligence*, *37*(1), 67–79.] - Directly cited and used for input images and local context comparisons (Figure 2), indicating its importance to the methodology."
      },
      {
        "citation": "[Dosch Design. (n.d.). Comic_Characters_V2. http://www.doschdesign.com/products/3d/Comic_Characters_V2.html.] - While seemingly less critical, it's included in the references, suggesting it was used for data or evaluation."
      },
      {
        "citation": "[IEEE. (2015). *IEEE Transactions on Pattern Analysis and Machine Intelligence*.] - The publication venue for reference [4], included as part of the citation."
      },
      {
        "citation": "[CVPR. (Year not specified).] - The publication venue for reference [3], included as part of the citation."
      },
      {
        "citation": "[ECCV. (Year not specified).] - The publication venue for reference [1], included as part of the citation."
      },
      {
        "citation": "[Gortler, S. J. (Year not specified).] - Appears as part of the author list for reference [4]."
      },
      {
        "citation": "[Jacobs, D. W. (Year not specified).] - Appears as part of the author list for reference [4]."
      },
      {
        "citation": "[Basri, R. (Year not specified).] - Appears as part of the author list for reference [4]."
      }
    ],
    "author_details": [
      {
        "name": "Stephan R. Richter",
        "affiliation": "Department of Computer Science, TU Darmstadt",
        "email": "[Not available in the provided text]"
      },
      {
        "name": "Stefan Roth",
        "affiliation": "Department of Computer Science, TU Darmstadt",
        "email": "[Not available in the provided text]"
      }
    ]
  },
  {
    "title": "Superpixel Meshes for Fast Edge-Preserving Surface Reconstruction\n---AUTHOR---\nAndr´as B´odis-Szomor´u\nHayko Riemenschneider\nLuc Van Gool",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Bodis-Szomoru_Superpixel_Meshes_for_2015_CVPR_supplemental.pdf",
    "id": "Bodis-Szomoru_Superpixel_Meshes_for_2015_CVPR_supplemental",
    "abstract": "Multi-View-Stereo (MVS) methods aim for the highest detail possible, however, such detail is often not required. In this work, we propose a novel surface reconstruction method based on image edges, superpixels and second-order smoothness constraints, producing meshes comparable to classic MVS surfaces in quality but orders of magnitudes faster. Our method performs per-view dense depth optimization directly over sparse 3D Ground Control Points (GCPs), hence, removing the need for view pairing, image rectiﬁcation, and stereo depth estimation, and allowing for full per-image parallelization. We use Structure-from-Motion (SfM) points as GCPs, but the method is not speciﬁc to these, e.g. LiDAR or RGB-D can also be used. The resulting meshes are compact and inherently edge-aligned with image gradients, enabling good-quality lightweight per-face ﬂat renderings. Our experiments demonstrate on a variety of 3D datasets the superiority in speed and competitive surface quality.\n\n---TOPIX---\nSurface Reconstruction\nSuperpixels\nStructure-from-Motion (SfM)\nMesh Generation\nEdge-Preserving Methods",
    "topics": [],
    "references": [],
    "author_details": [
      {
        "name": "Andr´as B´odis-Szomor´u",
        "affiliation": "ETH Zurich, Computer Vision Lab",
        "email": "bodis@vision.ee.ethz.ch"
      },
      {
        "name": "Hayko Riemenschneider",
        "affiliation": "ETH Zurich, Computer Vision Lab",
        "email": "hayko@vision.ee.ethz.ch"
      },
      {
        "name": "Luc Van Gool",
        "affiliation": "ETH Zurich, Computer Vision Lab; PSI-VISICS, KU Leuven",
        "email": "vangool@vision.ee.ethz.ch"
      }
    ]
  },
  {
    "title": "Ô·¹¸¬ Ú·»´¼ º®±³ Ó·½®±ó¾¿-»´·²» ×³¿¹» Ð¿·®\n---AUTHOR---\nDidyk et.al",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Zhang_Light_Field_From_2015_CVPR_paper.pdf",
    "id": "Zhang_Light_Field_From_2015_CVPR_paper",
    "abstract": "This paper introduces a novel approach to depth estimation from sparse views. The core idea involves synthesizing intermediate views using a combination of techniques, including iterative refinement and light field synthesis. The method leverages existing work on depth from Didyk et al. and incorporates synthesized right views to improve accuracy. The paper details the process of creating these intermediate views, addressing challenges related to disparity refinement and incorporating synthesized light fields. The approach aims to enhance depth perception from limited viewpoints, demonstrating improvements over existing methods.",
    "topics": [
      "Depth Estimation",
      "View Synthesis",
      "Light Field Reconstruction",
      "Disparity Refinement",
      "Iterative View Generation"
    ],
    "references": [],
    "author_details": [
      {
        "name": "Didyk",
        "affiliation": "Not specified in the provided text.",
        "email": "Not available in the provided text."
      }
    ]
  },
  {
    "title": "Taking a Deeper Look at Pedestrians\n---AUTHOR---\nJan Hosang\nMohamed Omran\nRodrigo Benenson\nBernt Schiele",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Hosang_Taking_a_Deeper_2015_CVPR_supplemental.pdf",
    "id": "Hosang_Taking_a_Deeper_2015_CVPR_supplemental",
    "abstract": "This paper investigates the impact of various parameters on the performance of CifarNet and AlexNet for pedestrian detection. Through grid searches, the authors explore the effects of filter size, layer width, pooling layers, learning rate policies, regularization, and SVM parameters. They also analyze the distribution of pedestrian heights in different datasets (Caltech and KITTI) and observe differences in transferability between datasets. The findings highlight the sensitivity of neural network training to parameter choices and the importance of careful tuning for optimal results.",
    "topics": [
      "Pedestrian Detection",
      "Neural Network Training",
      "Parameter Optimization",
      "Dataset Analysis",
      "Transfer Learning"
    ],
    "references": [
      {
        "citation": "[Benenson, R., Omran, M., Hosang, J., and Schiele, B. Ten years of pedestrian detection, what have we learned? In ECCV, CVRSUAD workshop, 2014.] - This appears to be a comprehensive review of pedestrian detection techniques, making it a key foundational reference."
      },
      {
        "citation": "[Hosang, J., Benenson, R., and Schiele, B. How good are detection proposals, really? In BMVC, 2014.] - This reference likely addresses the quality of detection proposals, a crucial aspect of pedestrian detection systems."
      }
    ],
    "author_details": [
      {
        "name": "Jan Hosang",
        "affiliation": "Max Planck Institute for Informatics",
        "email": "jan.hosang@mpi-inf.mpg.de"
      },
      {
        "name": "Mohamed Omran",
        "affiliation": "Max Planck Institute for Informatics",
        "email": "mohamed.omran@mpi-inf.mpg.de"
      },
      {
        "name": "Rodrigo Benenson",
        "affiliation": "Max Planck Institute for Informatics",
        "email": "rodrigo.benenson@mpi-inf.mpg.de"
      },
      {
        "name": "Bernt Schiele",
        "affiliation": "Max Planck Institute for Informatics",
        "email": "bernt.schiele@mpi-inf.mpg.de"
      }
    ]
  },
  {
    "title": "Depth Image Enhancement Using Local Tangent Plane Approximations\n---AUTHOR---\nYoshimitsu Aoki\nKiyoshi Matsuo",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Matsuo_Depth_Image_Enhancement_2015_CVPR_paper.pdf",
    "id": "Matsuo_Depth_Image_Enhancement_2015_CVPR_paper",
    "abstract": "This paper describes a depth image enhancement method for consumer RGB-D cameras. Most existing methods use the pixel-coordinates of the aligned color image, which is not suitable for handling local geometries because the image plane generally has no relationship to the measured surfaces. To improve enhancement accuracy, we use local tangent planes as local coordinates for the measured surfaces. Our method is composed of two steps, a calculation of the local tangents and surface reconstruction. Accurate depth image enhancement is achieved by using the local geometries approximated by the local tangents. The method demonstrates a high completion rate and achieves the lowest errors in noisy cases when compared with existing techniques.",
    "topics": [
      "Depth Image Enhancement",
      "Local Tangent Planes",
      "RGB-D Cameras",
      "Surface Reconstruction",
      "Noise Reduction"
    ],
    "references": [
      {
        "citation": "[Asus xtion pro live. http://www.asus.com/Commercial_3D_Sensor/Xtion_PRO_LIVE/]"
      },
      {
        "citation": "[M. Kiechle, S. Hawe, and M. Kleinsteuber. A joint intensity and depth co-sparse analysis model for depth map super-resolution. In Computer Vision (ICCV), 2013 IEEE International Conference on, pages 1545–1552, Dec 2013.]"
      },
      {
        "citation": "[C. Kim, H. Son, and C. Kim. Automated construction progress measurement using a 4d building information model and 3d data. Automation in Construction, 31(0):75 – 82, 2013.]"
      },
      {
        "citation": "[J. Kim, J. Lee, S.-R. Han, D. Kim, J. Min, and C. Kim. A high quality depth map upsampling method robust to misalignment of depth and color boundaries. Journal of Signal Processing Systems, 75(1):23–37, 2014.]"
      },
      {
        "citation": "[J. Kopf, M. F. Cohen, D. Lischinski, and M. Uyttendaele. Joint bilateral upsampling. ACM Trans. Graph., 26(3), July 2007.]"
      },
      {
        "citation": "[Y. Li, T. Xue, L. Sun, and J. Liu. Joint example-based depth map super-resolution. In Multimedia and Expo (ICME), 2012 IEEE International Conference on, pages 152–157, July 2012.]"
      },
      {
        "citation": "[M.-Y. Liu, O. Tuzel, and Y. Taguchi. Joint geodesic up-sampling of depth images. In Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on, pages 169–176, June 2013.]"
      },
      {
        "citation": "[S. Lu, X. Ren, and F. Liu. Depth enhancement via low-rank matrix completion. In Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on, pages 3390–3397, June 2014.]"
      },
      {
        "citation": "[D. Scharstein and C. Pal. Learning conditional random fields for stereo. In Computer Vision and Pattern Recognition, 2007. CVPR ’07. IEEE Conference on, pages 1 –8, June 2007.]"
      },
      {
        "citation": "[J. Papon, T. Kulvicius, E. Aksoy, and F. Worgotter. Point cloud video object segmentation using a persistent super-voxel world-model. In Intelligent Robots and Systems (IROS), 2013 IEEE/RSJ International Conference on, pages 3712–3718, Nov 2013.]"
      }
    ],
    "author_details": [
      {
        "name": "Yoshimitsu Aoki",
        "affiliation": "Keio University",
        "email": "aoki@elec.keio.ac.jp"
      },
      {
        "name": "Kiyoshi Matsuo",
        "affiliation": "Hokuyo Automatic Co., LTD.",
        "email": "k-matsuo@hokuyo-aut.co.jp"
      }
    ]
  },
  {
    "title": "Material Recognition in the Wild with the Materials in Context Database\n---AUTHOR---\nSean Bell\nPaul Upchurch\nNoah Snavely\nKavita Bala",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Bell_Material_Recognition_in_2015_CVPR_paper.pdf",
    "id": "Bell_Material_Recognition_in_2015_CVPR_paper",
    "abstract": "Recognizing materials in real-world images is a challenging task due to rich surface texture, geometry, lighting conditions, and clutter. This paper introduces the Materials in Context Database (MINC), a new, large-scale, open dataset of materials in the wild, which is an order of magnitude larger than previous material databases and more diverse. Using MINC, the authors train convolutional neural networks (CNNs) for material classification from patches and simultaneous material recognition and segmentation in full images. They achieve 85.2% mean class accuracy for patch-based classification and 73.1% mean class accuracy for full image segmentation. The results demonstrate the importance of large, well-sampled datasets for real-world material recognition and segmentation.",
    "topics": [
      "Material Recognition",
      "Deep Learning (CNNs)",
      "Dataset Creation (MINC)",
      "Image Segmentation",
      "Computer Vision"
    ],
    "references": [
      {
        "citation": "[G. Patterson, C. Xu, H. Su, and J. Hays, The SUN Attribute Database: Beyond Categories for Deeper Scene Understanding, IJCV, 108(1-2):59–81, 2014.]"
      },
      {
        "citation": "[S. Bell, P. Upchurch, N. Snavely, and K. Bala, OpenSurfaces: A richly annotated catalog of surface appearance, ACM Trans. on Graphics (SIGGRAPH), 32(4), 2013.]"
      },
      {
        "citation": "[X. Qi, R. Xiao, J. Guo, and L. Zhang, Pairwise rotation invariant co-occurrence local binary pattern, In ECCV, pages 158–171. Springer, 2012.]"
      },
      {
        "citation": "[B. Caputo, E. Hayman, and P. Mallikarjuna, Class-speciﬁc material categorisation, In ICCV, pages 1597–1604, 2005.]"
      },
      {
        "citation": "[O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei, ImageNet Large Scale Visual Recognition Challenge, 2014.]"
      },
      {
        "citation": "[M. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed, and A. Vedaldi, Describing textures in the wild, In CVPR, pages 3606–3613. IEEE, 2014.]"
      },
      {
        "citation": "[K. J. Dana, B. Van Ginneken, S. K. Nayar, and J. J. Koen-derink, Reﬂectance and texture of real-world surfaces, ACM Transactions on Graphics (TOG), 18(1):1–34, 1999.]"
      },
      {
        "citation": "[B. C. Russell, A. Torralba, K. P. Murphy, and W. T. Freeman, LabelMe: A database and web-based tool for image annotation, IJCV, 77(1-3):157–173, May 2008.]"
      },
      {
        "citation": "[M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman, The Pascal Visual Object Classes (VOC) Challenge, IJCV, 88(2):303–338, June 2010.]"
      },
      {
        "citation": "[G. Schwartz and K. Nishino, Visual material traits: Recognizing per-pixel material context, In Proceedings of the International Conference on Computer Vision Workshops (ICCVW), pages 883–890. IEEE, 2013.]"
      }
    ],
    "author_details": [
      {
        "name": "Sean Bell",
        "affiliation": "Department of Computer Science, Cornell University",
        "email": "sbell@cs.cornell.edu"
      },
      {
        "name": "Paul Upchurch",
        "affiliation": "Department of Computer Science, Cornell University",
        "email": "paulu@cs.cornell.edu"
      },
      {
        "name": "Noah Snavely",
        "affiliation": "Department of Computer Science, Cornell University",
        "email": "snavely@cs.cornell.edu"
      },
      {
        "name": "Kavita Bala",
        "affiliation": "Department of Computer Science, Cornell University",
        "email": "kb@cs.cornell.edu"
      }
    ]
  },
  {
    "title": "Scene Labeling with LSTM Recurrent Neural Networks\n---AUTHOR---\nWonmin Byeon\nThomas M. Breuel\nFederico Raue\nMarcus Liwicki",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Byeon_Scene_Labeling_With_2015_CVPR_paper.pdf",
    "id": "Byeon_Scene_Labeling_With_2015_CVPR_paper",
    "abstract": "Accurate scene labeling is an important step towards image understanding. This paper addresses the problem of pixel-level segmentation and classification of scene images with an entirely learning-based approach using Long Short Term Memory (LSTM) recurrent neural networks. We investigate two-dimensional (2D) LSTM networks for natural scene images taking into account the complex spatial dependencies of labels. Our approach, which has a much lower computational complexity than prior methods, achieved state-of-the-art performance over the Stanford Background and the SIFT Flow datasets. The networks efficiently capture local and global contextual information over raw RGB values and adapt well for complex scene images.\n\n---TOPIC---\nScene Labeling\nLSTM Recurrent Neural Networks\nImage Segmentation\nSpatial Dependencies\nDeep Learning",
    "topics": [],
    "references": [],
    "author_details": [
      {
        "name": "Wonmin Byeon",
        "affiliation": "German Research Center for Artiﬁcial Intelligence (DFKI), Germany",
        "email": "wonmin.byeon@dfki.de"
      },
      {
        "name": "Thomas M. Breuel",
        "affiliation": "University of Kaiserslautern, Germany",
        "email": "tmb@cs.uni-kl.de"
      },
      {
        "name": "Federico Raue",
        "affiliation": "German Research Center for Artiﬁcial Intelligence (DFKI), Germany",
        "email": "federico.raue@dfki.de"
      },
      {
        "name": "Marcus Liwicki",
        "affiliation": "University of Kaiserslautern, Germany",
        "email": "liwicki@cs.uni-kl.de"
      }
    ]
  },
  {
    "title": "Iteratively Reweighted Graph Cut for Multi-label MRFs with Non-convex Priors\n---AUTHOR---\nThalaiyasingam Ajanthan\nRichard Hartley\nMathieu Salzmann\nHongdong Li",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Ajanthan_Iteratively_Reweighted_Graph_2015_CVPR_paper.pdf",
    "id": "Ajanthan_Iteratively_Reweighted_Graph_2015_CVPR_paper",
    "abstract": "This paper introduces an algorithm to minimize the energy of multi-label Markov Random Fields (MRFs) with non-convex edge priors. The algorithm iteratively approximates the original energy with a weighted surrogate energy that is easier to minimize, guaranteeing that the original energy decreases at each iteration. The approach leverages the multi-label graph cut algorithm and is inspired by the Iteratively Reweighted Least Squares (IRLS) algorithm. The effectiveness of the algorithm is demonstrated on stereo correspondence estimation and image inpainting problems, consistently outperforming state-of-the-art graph-cut-based algorithms and yielding lower energy values than TRW-S.",
    "topics": [
      "Multi-label Markov Random Fields (MRFs)",
      "Non-convex Priors",
      "Iteratively Reweighted Algorithms",
      "Graph Cut Optimization",
      "Image Processing (Stereo/Inpainting)"
    ],
    "references": [
      {
        "citation": "[Ishikawa, H. Exact optimization for Markov random fields with convex priors. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 25(10):1333–1336, 2003.] - Cited multiple times, foundational work on MRF optimization."
      },
      {
        "citation": "[Boykov, Y., Kolmogorov, V., & Zabih, R. Fast approximate energy minimization via graph cuts. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 23(11):1222–1239, 2001.] - A seminal paper on graph cut methods for energy minimization."
      },
      {
        "citation": "[Kolmogorov, V. Convergent tree-reweighted message passing for energy minimization. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 27(9):833–843, 1999.] - Important work on reweighted message passing."
      },
      {
        "citation": "[Hartley, R., & Zisserma, A. Multiple view geometry in computer vision. Cambridge university press, 2003.] - A key reference for understanding geometric relationships in computer vision."
      },
      {
        "citation": "[Szeliski, R., Zabih, R., Scharstein, D., Vekler, O., Kolmogorov, V., Agarwala, A., Tappen, M., & Rother, C. A comparative study of energy minimization methods for Markov random fields with smoothness-based priors. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 30(6):1068–1080, 2008.] - Provides a comparative analysis of different energy minimization techniques."
      },
      {
        "citation": "[Boykov, Y., & Kolmogorov, V. An experimental comparison of min-cut/max-ﬂow algorithms for energy minimization in vision. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 26(9):1124–1137, 2004.] - Provides a comparison of different min-cut/max-flow algorithms."
      },
      {
        "citation": "[Pock, T., Schoenemann, T., Graber, G., Bischof, H., & Cre-mers, D. A convex formulation of continuous multi-label problems. In Computer Vision–ECCV 2008, pages 677–690. Springer, 2008.] - Introduces a convex formulation for multi-label problems."
      },
      {
        "citation": "[Kappes, J. H., Andres, B., Hamprecht, F. A., Schnorr, C., Nowozin, S., Batra, D., Kim, S., Kausler, B. X., Lellmann, J., Komodakis, N., et al. A comparative study of modern inference techniques for discrete energy minimization problems. In Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on, pages 1328–1335. IEEE, 2013.] - Provides a comparative study of modern inference techniques."
      },
      {
        "citation": "[Scharstein, D., & Szeliski, R. A taxonomy and evaluation of dense two-frame stereo correspondence algorithms. International journal of computer vision, 47(1-3):7–42, 2002.] - Important for understanding stereo correspondence algorithms."
      },
      {
        "citation": "[Vekler, O. Multi-label moves for mrfs with truncated convex priors. International journal of computer vision, 98(1):1–14, 2012.] - Addresses multi-label problems with truncated convex priors."
      }
    ],
    "author_details": [
      {
        "name": "Thalaiyasingam Ajanthan",
        "affiliation": "Australian National University & NICTA",
        "email": "[Email not available in the provided text]"
      },
      {
        "name": "Richard Hartley",
        "affiliation": "Australian National University & NICTA",
        "email": "[Email not available in the provided text]"
      },
      {
        "name": "Mathieu Salzmann",
        "affiliation": "Australian National University & NICTA",
        "email": "[Email not available in the provided text]"
      },
      {
        "name": "Hongdong Li",
        "affiliation": "Australian National University & NICTA",
        "email": "[Email not available in the provided text]"
      }
    ]
  },
  {
    "title": "Saliency Detection by Multi-Context Deep Learning\n---AUTHORs---\nRui Zhao\nWanli Ouyang\nHongsheng Li\nXiaogang Wang",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Zhao_Saliency_Detection_by_2015_CVPR_paper.pdf",
    "id": "Zhao_Saliency_Detection_by_2015_CVPR_paper",
    "abstract": "Image salience detection aims to highlight visually salient regions or objects in an image. Conventional approaches often struggle when salient objects appear in low-contrast backgrounds with confusing visual appearances. This paper proposes a multi-context deep learning framework for salient object detection, employing deep Convolutional Neural Networks to model salience while considering both global and local context. A task-specific pre-training scheme is designed to improve performance. The approach is evaluated on five public datasets and demonstrates significant improvements over state-of-the-art methods.\n\n---TOPIC---\nSalient Object Detection\nDeep Convolutional Neural Networks (CNNs)\nMulti-Context Modeling\nImage Processing\nComputer Vision",
    "topics": [],
    "references": [
      {
        "citation": "[R. Achanta, S. Hemami, F. Estrada, and S. Susstrunk, Frequency-tuned salient region detection, CVPR, 2009]"
      },
      {
        "citation": "[G. E. Hinton, Training products of experts by minimizing contrasive divergence, Neural computation, 14(8):1771–1800, 2002]"
      },
      {
        "citation": "[X. Hou and L. Zhang, Saliency detection: A spectral residual approach, CVPR, 2007]"
      },
      {
        "citation": "[Y. Jia and M. Han, Category-independent object-level saliency detection, ICCV, 2013]"
      },
      {
        "citation": "[R. Girshick, J. Donahue, T. Darrell, and J. Malik, Rich feature hierarchies for accurate object detection and semantic segmentation, arXiv preprint arXiv:1311.2524, 2013]"
      },
      {
        "citation": "[J. Harel, C. Koch, and P. Perona, Graph-based visual saliency, NIPS, 2006]"
      },
      {
        "citation": "[A. Borji, Boosting bottom-up and top-down visual features for saliency estimation, CVPR, 2012]"
      },
      {
        "citation": "[M.-M. Cheng, N. J. Mitra, X. Huang, P. H. S. Torr, and S.-M. Hu, Global contrast based salient region detection, IEEE Trans. on PAMI, 2014]"
      },
      {
        "citation": "[M.-M. Cheng, J. Warrell, W.-Y. Lin, S. Zheng, V. Vineet, and N. Crook, Efﬁcient salient region detection with soft image abstraction, ICCV, 2013]"
      },
      {
        "citation": "[R. Mairon and O. Ben-Shahar, A closer look at context: From coxels to the contextual emergence of object saliency, ECCV, 2014]"
      }
    ],
    "author_details": [
      {
        "name": "Rui Zhao",
        "affiliation": "Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences",
        "email": "rzhao@ee.cuhk.edu.hk"
      },
      {
        "name": "Wanli Ouyang",
        "affiliation": "Department of Electronic Engineering, The Chinese University of Hong Kong",
        "email": "wlouyang@ee.cuhk.edu.hk"
      },
      {
        "name": "Hongsheng Li",
        "affiliation": "Department of Electronic Engineering, The Chinese University of Hong Kong",
        "email": "hsli@ee.cuhk.edu.hk"
      },
      {
        "name": "Xiaogang Wang",
        "affiliation": "Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences",
        "email": "xgwang@ee.cuhk.edu.hk"
      }
    ]
  },
  {
    "title": "Subspace Clustering by Mixture of Gaussian Regression\n---AUTHORISTS---\nBaohua Li\nYing Zhang\nZhouchen Lin\nHuchuan Lu",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Li_Subspace_Clustering_by_2015_CVPR_paper.pdf",
    "id": "Li_Subspace_Clustering_by_2015_CVPR_paper",
    "abstract": "Subspace clustering aims to find a multi-subspace representation that best fits sample points drawn from a high-dimensional space. Existing methods often rely on different norms to model noise, which can be restrictive. This paper proposes Mixture of Gaussian Regression (MoG Regression) for subspace clustering, which models noise as a Mixture of Gaussians (MoG). This approach provides a more effective way to model a broader range of noise distributions, leading to a better affinity matrix and improved clustering performance. Experimental results demonstrate that MoG Regression significantly outperforms state-of-the-art subspace clustering methods.",
    "topics": [
      "Subspace Clustering",
      "Mixture of Gaussian Regression (MoG)",
      "Noise Modeling",
      "Spectral Clustering",
      "Affinity Matrix Construction"
    ],
    "references": [
      {
        "citation": "[Bradley, P. S., & Mangasarian, O. L. K-plane clustering. Journal of Global Optimization, 16(1):23–32, 2000.] - This paper introduces K-plane clustering, a relevant technique for segmentation."
      },
      {
        "citation": "[Shi, J., & Malik, J. Normalized cuts and image segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 22(8):888–905, 2000.] - Normalized Cuts is a foundational work in graph-based image segmentation."
      },
      {
        "citation": "[Vidal, R., & Hartley, R. Motion segmentation with missing data using powerfactorization and GPCA. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, volume 2, pages II–310. IEEE, 2004.] - This paper utilizes power factorization and GPCA for motion segmentation, demonstrating a specific approach."
      },
      {
        "citation": "[Vidal, R., Ma, Y., & Sastry, S. S. Generalized principal component analysis (GPCA). IEEE Transactions on Pattern Analysis and Machine Intelligence, 27(12):1945–1959, 2005.] - GPCA is a core technique used in several of the referenced papers, making it a crucial reference."
      },
      {
        "citation": "[Von Luxburg, U. A tutorial on spectral clustering. Statistics and computing, 17(4):395–416, 2007.] - Spectral clustering is a widely used technique, and this tutorial provides a good overview."
      },
      {
        "citation": "[Wright, J., Yang, A. Y., Ganesh, A., Sastry, S. S., & Ma, Y. Robust face recognition via sparse representation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 31(2):210–227, 2009.] - Sparse representation is a related technique that likely informs the approaches taken in the paper."
      },
      {
        "citation": "[Xu, L., & Jordan, M. I. On convergence properties of the EM algorithm for gaussian mixtures. Neural Computation, 8(1):129–151, 1996.] - This paper addresses the convergence of the EM algorithm, which is often used in segmentation."
      },
      {
        "citation": "[Yan, J., & Pollefeys, M. A general framework for motion segmentation: Independent, articulated, rigid, non-rigid, degenerate and non-degenerate. In Proceedings of European Conference on Computer Vision, pages 294–307. 2006.] - This paper provides a broad framework for motion segmentation, covering various types of motion."
      },
      {
        "citation": "[Yang, A. Y., Wright, J., Ma, Y., & Sastry, S. S. Unsupervised segmentation of natural images via lossy data compression. Computer Vision and Image Understanding, 110(2):212–225, 2008.] - This paper explores unsupervised image segmentation using lossy data compression, a specific methodology."
      },
      {
        "citation": "[Zhao, Q., Meng, D., Xu, Z., Zuo, W., & Zhang, L. Robust principal component analysis with complex noise. In Proceedings of International Conference on Machine Learning, 2014.] - This paper addresses robust PCA, a relevant consideration for dealing with noisy data in segmentation."
      }
    ],
    "author_details": [
      {
        "name": "Baohua Li",
        "affiliation": "Dalian University of Technology",
        "email": "[Email not available in the provided text]"
      },
      {
        "name": "Ying Zhang",
        "affiliation": "Dalian University of Technology",
        "email": "[Email not available in the provided text]"
      },
      {
        "name": "Zhouchen Lin",
        "affiliation": "Key Laboratory of Machine Perception (MOE), School of EECS, Peking University",
        "email": "[Email not available in the provided text]"
      },
      {
        "name": "Huchuan Lu",
        "affiliation": "Dalian University of Technology",
        "email": "[Email not available in the provided text]"
      }
    ]
  },
  {
    "title": "A Convolutional Neural Network Cascade for Face Detection\n---AUTHOR---\nHaoxiang Li\nZhe Lin\nXiaohui Shen\nJonathan Brandt\nGang Hua",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Li_A_Convolutional_Neural_2015_CVPR_paper.pdf",
    "id": "Li_A_Convolutional_Neural_2015_CVPR_paper",
    "abstract": "Face detection faces the challenges of large visual variations and a large search space. While advanced models are needed to address visual variations, they are often computationally expensive. This paper proposes a CNN cascade architecture to balance these conflicting demands. The cascade quickly rejects background regions in low-resolution stages and carefully evaluates challenging candidates in high-resolution stages. A CNN-based calibration stage is introduced to improve localization effectiveness and accelerate the cascade. The proposed method achieves state-of-the-art detection performance and runs at 14 FPS on a single CPU core and 100 FPS using a GPU.\n\n---TOPIC---\nFace Detection\nConvolutional Neural Networks (CNNs)\nCascade Architecture\nBounding Box Calibration\nReal-time Performance",
    "topics": [],
    "references": [
      {
        "citation": "[Viola, P. A., & Jones, M. J. (2001). Rapid object detection using a boosted cascade of simple features. *In Proc. IEEE Conference on Computer Vision and Pattern Recognition*. 1, 2]"
      },
      {
        "citation": "[LeCun, Y., & Bengio, Y. (1995). Convolutional networks for images, speech, and time series. *The handbook of brain theory and neural networks*. 1, 2]"
      },
      {
        "citation": "[Rowley, H. A., Baluja, S., & Kanade, T. (1996). Neural network-based face detection. *In Computer Vision and Pattern Recognition*. 2]"
      },
      {
        "citation": "[Felzenszwalb, P. F., Girshick, R. B., McAllester, D., & Ramanan, D. (2010). Object detection with discriminatively trained part-based models. *IEEE Transactions on Pattern Analysis and Machine Intelligence*. 2, 6]"
      },
      {
        "citation": "[Jain, V., & Learned-Miller, E. (2010). Fddb: A benchmark for face detection in unconstrained settings. *Technical Report UM-CS-2010-009, University of Massachusetts, Amherst*. 2, 6, 7]"
      },
      {
        "citation": "[Girshick, R., Donahue, J., Darrell, T., & Malik, J. (2013). Rich feature hierarchies for accurate object detection and semantic segmentation. *arXiv preprint arXiv:1311.2524*. 2]"
      },
      {
        "citation": "[Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). ImageNet classification with deep convolutional neural networks. *In Advances in neural information processing systems*. 3]"
      },
      {
        "citation": "[Jia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick, R., Guadarrama, S., & Darrell, T. (2014). Caffe: Convolutional architecture for fast feature embedding. *arXiv preprint arXiv:1408.5093*. 2]"
      },
      {
        "citation": "[Vaillant, R., Monrocq, C., & Le Cun, Y. (1994). Original approach for the localisation of objects in images. *IEE Proceedings-Vision, Image and Signal Processing*. 2]"
      },
      {
        "citation": "[Yang, B., Yan, J., Lei, Z., & Li, S. Z. (2014). Aggregate channel features for multi-view face detection. *arXiv preprint arXiv:1407.4023*. 2, 8]"
      }
    ],
    "author_details": [
      {
        "name": "Haoxiang Li",
        "affiliation": "Stevens Institute of Technology",
        "email": "hli18@steverns.edu"
      },
      {
        "name": "Zhe Lin",
        "affiliation": "Adobe Research",
        "email": "zlin@adobe.com"
      },
      {
        "name": "Xiaohui Shen",
        "affiliation": "Adobe Research",
        "email": "xshen@adobe.com"
      },
      {
        "name": "Jonathan Brandt",
        "affiliation": "Adobe Research",
        "email": "jbrandt@adobe.com"
      },
      {
        "name": "Gang Hua",
        "affiliation": "Stevens Institute of Technology",
        "email": "ghua@steverns.edu"
      }
    ]
  },
  {
    "title": "Salient Object Detection via Bootstrap Learning\n---AUTHORISTS---\nNa Tong\nHuchuan Lu\nXiang Ruan\nMing-Hsuan Yang",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Tong_Salient_Object_Detection_2015_CVPR_paper.pdf",
    "id": "Tong_Salient_Object_Detection_2015_CVPR_paper",
    "abstract": "We propose a bootstrap learning algorithm for salient object detection in which both weak and strong models are exploited. First, a weak saliency map is constructed based on image priors to generate training samples for a strong model. Second, a strong classiﬁer based on samples directly from an input image is learned to detect salient pixels. Results from multiscale saliency maps are integrated to further improve the detection performance. Extensive experiments on six benchmark datasets demonstrate that the proposed bootstrap learning algorithm performs favorably against the state-of-the-art saliency detection methods. Furthermore, we show that the proposed bootstrap learning approach can be easily applied to other bottom-up saliency models for signiﬁcant improvement.\n\n---TOPIC---\nSalient Object Detection\nBootstrap Learning\nComputer Vision\nWeak and Strong Models\nMultiscale Analysis",
    "topics": [],
    "references": [
      {
        "citation": "[Li, Y., Hou, X., Koch, C., Rehg, J., & Yuille, A. (2014). The secrets of salient object segmentation. In CVPR.]"
      },
      {
        "citation": "[Achanta, R., Hemami, S., Estrada, F., & S¨usstrunk, S. (2009). Frequency-tuned salient region detection. In CVPR.]"
      },
      {
        "citation": "[Liu, T., Sun, J., Zheng, N.-N., Tang, X., & Shum, H.-Y. (2007). Learning to detect a salient object. In CVPR.]"
      },
      {
        "citation": "[Movahedi, V., & Elder, J. H. (2010). Design and perceptual validation of performance measures for salient object segmentation. In POCV.]"
      },
      {
        "citation": "[Achanta, R., Shaji, A., Smith, K., Lucchi, A., Fua, P., & S¨usstrunk, S. (2010). Slic superpixels. EPFL.]"
      },
      {
        "citation": "[Ojala, T., Pietikainen, M., & Maenpaa, T. (2002). Multiresolution gray-scale and rotation invariant texture classification with local binary patterns. PAMI, 24(7), 971–987.]"
      },
      {
        "citation": "[Bach, F. R., Lanckriet, G. R., & Jordan, M. I. (2004). Multiple kernel learning, conic duality, and the SMO algorithm. In ICML.]"
      },
      {
        "citation": "[Perazzi, F., Kr¨ahenb¨uhl, P., Pritch, Y., & Hornung, A. (2012). Saliency filters: Contrast based filtering for salient region detection. In CVPR.]"
      },
      {
        "citation": "[Borji, A., Sihite, D. N., & Itti, L. (2012). Salient object detection: A benchmark. In ECCV.]"
      },
      {
        "citation": "[Rahtu, E., Kannala, J., Salo, M., & Heikkil¨a, J. (2010). Segmenting salient objects from images and videos. In ECCV.]"
      }
    ],
    "author_details": [
      {
        "name": "Na Tong",
        "affiliation": "Dalian University of Technology",
        "email": "Not available"
      },
      {
        "name": "Huchuan Lu",
        "affiliation": "Dalian University of Technology",
        "email": "Not available"
      },
      {
        "name": "Xiang Ruan",
        "affiliation": "OMRON Corporation",
        "email": "Not available"
      },
      {
        "name": "Ming-Hsuan Yang",
        "affiliation": "University of California at Merced",
        "email": "Not available"
      }
    ]
  },
  {
    "title": "Convolutional Neural Networks at Constrained Time Cost\n---AUTHOR---\nKaiming He\nJian Sun",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/He_Convolutional_Neural_Networks_2015_CVPR_paper.pdf",
    "id": "He_Convolutional_Neural_Networks_2015_CVPR_paper",
    "abstract": "Recent convolutional neural networks (CNNs) have significantly improved image recognition accuracy, but they are increasingly complex and time-consuming. This paper investigates the accuracy of CNN architectures under constrained time cost during both training and testing stages. The authors progressively modify a baseline model while preserving its time complexity, using a \"layer replacement\" strategy. They present an architecture that achieves competitive accuracy on the ImageNet dataset (11.8% top-5 error) while being 20% faster than AlexNet. The study also considers the time constraints during offline training, highlighting the need to understand which factors contribute most to accuracy improvements within a limited time budget.",
    "topics": [
      "Convolutional Neural Networks (CNNs)",
      "Time Complexity / Constrained Time Cost",
      "Architecture Optimization",
      "ImageNet Dataset",
      "Layer Replacement"
    ],
    "references": [
      {
        "citation": "[Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., et al. Imagenet large scale visual recognition challenge. arXiv:1409.0575, 2014.] - This paper describes the ImageNet Large Scale Visual Recognition Challenge, a crucial benchmark for image classification."
      },
      {
        "citation": "[Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet: A large-scale hierarchical image database. In CVPR, 2009.] - Introduces the ImageNet dataset, foundational for many computer vision tasks."
      },
      {
        "citation": "[Simonyan, K., and Zisserman, A. Very deep convolutional networks for large-scale image recognition. arXiv:1409.1556, 2014.] - Explores the use of very deep convolutional networks, a significant development in the field."
      },
      {
        "citation": "[Ciresan, D., Meier, U., and Schmidhuber, J. Multi-column deep neural networks for image classification. In CVPR, 2012.] - Presents multi-column deep neural networks, an early architecture demonstrating the power of deep learning for image classification."
      },
      {
        "citation": "[Girshick, R., Donahue, J., Darrell, T., and Malik, J. Rich feature hierarchies for accurate object detection and semantic segmentation. In CVPR, 2014.] - Introduces rich feature hierarchies for object detection and semantic segmentation."
      },
      {
        "citation": "[Zeiler, M. D., and Fergus, R. Visualizing and understanding convolutional neural networks. In ECCV, 2014.] - Focuses on visualizing and understanding convolutional neural networks, crucial for interpretability."
      },
      {
        "citation": "[Eigen, D., Rolfe, J., Fergus, R., and LeCun, Y. Understanding deep architectures using a recursive convolutional network. arXiv:1312.1847, 2013.] - Explores understanding deep architectures using recursive convolutional networks."
      },
      {
        "citation": "[Chatfield, K., Simonyan, K., Vedaldi, A., and Zisserman, A. Return of the devil in the details: Delving deep into convo-lutional nets. In BMVC, 2014.] - Investigates details within convolutional networks."
      },
      {
        "citation": "[Sermanet, P., Eigen, D., Zhang, X., Mathieu, M., Fergus, R., and LeCun, Y. Overfeat: Integrated recognition, localization and detection using convolutional networks. 2014.] - Presents Overfeat, an integrated recognition, localization, and detection system."
      },
      {
        "citation": "[Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V., and Rabinovich, A. Going deeper with convolutions. arXiv:1409.4842, 2014.] - Explores going deeper with convolutions."
      }
    ],
    "author_details": [
      {
        "name": "Kaiming He",
        "affiliation": "Microsoft Research",
        "email": "kahe@microsoft.com"
      },
      {
        "name": "Jian Sun",
        "affiliation": "Microsoft Research",
        "email": "jiansun@microsoft.com"
      }
    ]
  },
  {
    "title": "Reweighted Laplace Prior Based Hyperspectral Compressive Sensing for Unknown Sparsity: Supplementary Material\n---AUTHOR---\nLei Zhang\nWei Wei\nYanning Zhang\nChunna Tian\nFei Li",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Zhang_Reweighted_Laplace_Prior_2015_CVPR_supplemental.pdf",
    "id": "Zhang_Reweighted_Laplace_Prior_2015_CVPR_supplemental",
    "abstract": "This supplementary material details the derivation and optimization procedures for a hyperspectral compressive sensing method utilizing a reweighted Laplace prior. The paper focuses on addressing the challenge of unknown sparsity in hyperspectral data. It provides detailed derivations of the reweighted Laplace prior and the optimization procedure, including sparsity learning over γ (sparse signal recovery) and noise estimation over λ. The derivations involve matrix algebra manipulations and the use of conjugate functions to transform non-convex optimization problems into convex ones. Experimental results are also presented to validate the proposed approach.",
    "topics": [
      "Hyperspectral Compressive Sensing",
      "Reweighted Laplace Prior",
      "Optimization Procedure",
      "Sparsity Learning",
      "Noise Estimation"
    ],
    "references": [],
    "author_details": []
  },
  {
    "title": "QRSTUTRVWXTXTYXTZT[VRV\\][Y^R_ZR[TZZT[V\\R^X]^T\\[\n\n---AUTHOR---\nuvw\nx+\nyz",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Fang_Collaborative_Feature_Learning_2015_CVPR_paper.pdf",
    "id": "Fang_Collaborative_Feature_Learning_2015_CVPR_paper",
    "abstract": "Unfortunately, the provided text appears to be corrupted or heavily encoded, making it impossible to extract a coherent abstract. The content consists primarily of non-standard characters and seemingly random sequences, preventing any meaningful interpretation of the paper's purpose, methods, or findings. A proper abstract requires readable text describing the research.",
    "topics": [
      "Due to the unreadable nature of the text, it is impossible to accurately determine the research topics. However, based on the limited recognizable elements and the structure of the provided content, potential (but speculative) topics might include:",
      "Data Encoding/Decoding",
      "Character Set Analysis",
      "Text Corruption/Error Correction",
      "Pattern Recognition (likely related to the encoding)",
      "Information Retrieval (attempting to extract meaning from corrupted data)"
    ],
    "references": [],
    "author_details": [
      {
        "name": "uvw",
        "affiliation": "Not available",
        "email": "Not available"
      },
      {
        "name": "x+",
        "affiliation": "Not available",
        "email": "Not available"
      },
      {
        "name": "yz",
        "affiliation": "Not available",
        "email": "Not available"
      }
    ]
  },
  {
    "title": "On the Relationship between Visual Attributes and Convolutional Networks\n---AUTHOR---\nVictor Escorcia\nJuan Carlos Niebles\nBernard Ghanem",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Escorcia_On_the_Relationship_2015_CVPR_paper.pdf",
    "id": "Escorcia_On_the_Relationship_2015_CVPR_paper",
    "abstract": "Through extensive experiments, we characterize the relationship between abstract concepts (specifically objects in images) learned by popular convolutional networks (conv-nets) and established mid-level representations used in computer vision (specifically semantic visual attributes). We focus on attributes due to their impact on several applications, such as object description, retrieval and mining, and active (and zero-shot) learning. Among the findings we uncover, we show empirical evidence of the existence of Attribute Centric Nodes (ACNs) within a conv-net, which is trained to recognize objects (not attributes) in images. These special conv-net nodes (1) collectively encode information pertinent to visual attribute representation and discrimination, (2) are unevenly and sparsely distribution across all layers of the conv-net, and (3) play an important role in conv-net based object recognition.\n\n---TOPICICS---\nConvolutional Networks (Conv-Nets)\nVisual Attributes\nAttribute Centric Nodes (ACNs)\nObject Recognition\nMid-Level Representations",
    "topics": [],
    "references": [],
    "author_details": [
      {
        "name": "Victor Escorcia",
        "affiliation": "King Abdullah University of Science and Technology (KAUST), Saudi Arabia",
        "email": "[Email not available in the provided text]"
      },
      {
        "name": "Juan Carlos Niebles",
        "affiliation": "Universidad del Norte, Colombia",
        "email": "[Email not available in the provided text]"
      },
      {
        "name": "Bernard Ghanem",
        "affiliation": "King Abdullah University of Science and Technology (KAUST), Saudi Arabia",
        "email": "[Email not available in the provided text]"
      }
    ]
  },
  {
    "title": "Zero-Shot Object Recognition by Semantic Manifold Distance\n---AUTHOR---\nZhenyong Fu\nTao Xiang\nElyor Kodirov\nShaogang Gong",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Fu_Zero-Shot_Object_Recognition_2015_CVPR_paper.pdf",
    "id": "Fu_Zero-Shot_Object_Recognition_2015_CVPR_paper",
    "abstract": "Zero-shot learning (ZSL) aims to recognise objects without seeing any visual examples by learning knowledge transfer between seen and unseen object classes. Existing works measure similarity in a semantic embedding space using conventional distance metrics that do not consider the rich intrinsic structure of the semantic categories. This paper proposes to model the semantic manifold in an embedding space using a semantic class label graph and introduces a novel absorbing Markov chain process (AMP) to compute a semantic manifold distance. The proposed model improves upon and unifies existing ZSL algorithms, demonstrating significant performance gains on ImageNet and AwA datasets.\n\n---TOPIC---\nZero-shot learning (ZSL)\nSemantic manifold\nAbsorbing Markov chain process (AMP)\nDistance metric learning\nKnowledge transfer",
    "topics": [],
    "references": [
      {
        "citation": "[Akata, Z., Perronnin, F., Harchaoui, Z., & Schmid, C. (2013). Label-embedding for attribute-based classification. In *Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on*, pages 819–826. IEEE.]"
      },
      {
        "citation": "[Bart, E., & Ullman, S. (2005). Single-example learning of novel classes using representation by similarity. In *BMVC*, volume 1, page 2.]"
      },
      {
        "citation": "[Chapelle, O., Weston, J., & Schölkopf, B. (2002). Cluster kernels for semi-supervised learning. In *Advances in neural information processing systems*, pages 585–592.]"
      },
      {
        "citation": "[Deng, J., Ding, N., Jia, Y., Frome, A., Murphy, K., Bengio, S., Li, Y., Neven, H., & Adam, H. (2014). Large-scale object classification using label relation graphs. In *Computer Vision–ECCV 2014*, pages 48–64. Springer.]"
      },
      {
        "citation": "[Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., & Fei-Fei, L. (2009). ImageNet: A large-scale hierarchical image database. In *Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on*, pages 248–255. IEEE.]"
      },
      {
        "citation": "[Frome, A., Corrado, G. S., Shlens, J., Bengio, S., Dean, J., Mikolov, T., et al. (2013). Devise: A deep visual-semantic embedding model. In *Advances in Neural Information Processing Systems*, pages 2121–2129.]"
      },
      {
        "citation": "[Fu, Y., Hospedales, T. M., Xiang, T., Fu, Z., & Gong, S. (2014). Transductive multi-view embedding for zero-shot recognition and annotation. In *ECCV*, pages 1–14.]"
      },
      {
        "citation": "[Jayaraman, D., & Grauman, K. (2014). Zero shot recognition with unreliable attributes. arXiv preprint arXiv:1409.4327.]"
      },
      {
        "citation": "[Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). ImageNet classification with deep convolutional neural networks. In *NIPS*, volume 1, page 4.]"
      },
      {
        "citation": "[Mikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.]"
      }
    ],
    "author_details": [
      {
        "name": "Zhenyong Fu",
        "affiliation": "Queen Mary, University of London",
        "email": "z.fu@qmul.ac.uk"
      },
      {
        "name": "Tao Xiang",
        "affiliation": "Queen Mary, University of London",
        "email": "t.xiang@qmul.ac.uk"
      },
      {
        "name": "Elyor Kodirov",
        "affiliation": "Queen Mary, University of London",
        "email": "e.kodirov@qmul.ac.uk"
      },
      {
        "name": "Shaogang Gong",
        "affiliation": "Queen Mary, University of London",
        "email": "s.gong@qmul.ac.uk"
      }
    ]
  },
  {
    "title": "Learning to rank in person re-identiﬁcation with metric ensembles\n---AUTHOR---\nSakrapee Paisitkriangkrai\nChunhua Shen\nAnton van den Hengel",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Paisitkriangkrai_Learning_to_Rank_2015_CVPR_paper.pdf",
    "id": "Paisitkriangkrai_Learning_to_Rank_2015_CVPR_paper",
    "abstract": "The task of person re-identification (re-id) aims to match pedestrian images observed from multiple cameras. Despite advances, it remains challenging due to variations in appearance, pose, illumination, and background clutter. Existing approaches often rely on predefined weights for combining multiple visual features, which are not adaptable to different datasets. This paper introduces two principled approaches to learn these weights, optimizing relative distance using triplet information and maximizing the average rank-k recognition rate. The proposed framework, built on multiple visual features, outperforms state-of-the-art methods on benchmark datasets, improving rank-1 recognition rates significantly. The ensemble-based approaches are flexible and can be combined with linear and non-linear metrics.\n\n---TOPIC---\nPerson Re-Identification\nMetric Learning\nEnsemble Methods\nStructured Learning\nVisual Feature Combination",
    "topics": [],
    "references": [
      {
        "citation": "[Chopra, S., Hadsell, R., and LeCun, Y. Learning a similarity metric discriminatively, with application to face veriﬁcation. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 2005.] - This paper introduces a method for learning a similarity metric, a foundational concept in many re-identification approaches."
      },
      {
        "citation": "[Gong, S., Crisitan, M., Yan, S., and Loy, C. C. Person Re-Identiﬁcation. Springer, 2014.] - This is a comprehensive overview of the field, providing context and a collection of techniques."
      },
      {
        "citation": "[Joachims, T. A support vector method for multivariate performance measures. In Proc. Int. Conf. Mach. Learn., 2005.] - This paper presents a support vector method relevant to performance measures, which is useful for evaluating re-identification systems."
      },
      {
        "citation": "[Krizhevsky, A., Sutskever, I., and Hinton, G. E. Imaginet classiﬁcation with deep convolutional neural networks. In Proc. Adv. Neural Inf. Process. Syst., 2012.] - This paper introduces AlexNet, a pivotal moment in deep learning and a precursor to many subsequent re-identification models."
      },
      {
        "citation": "[Roth, P. M., Hirzer, M., K¨ostinger, M., Beleznai, C., and Bischof, H. Mahalanobis distance learning for person re-identiﬁcation. In Person Re-Identiﬁcation, pages 247–267. Springer, 2014.] - This paper focuses on a specific metric learning technique (Mahalanobis distance) commonly used in re-identification."
      },
      {
        "citation": "[Weinberger, K. Q., and Saul, L. K. Fast solvers and efficient implementations for distance metric learning. In Proc. Int. Conf. Mach. Learn., 2008.] - This paper addresses the computational challenges of metric learning, a critical aspect for scalability."
      },
      {
        "citation": "[Felzenszwalb, P., Girshick, R., McAllester, D., and Ramanan, D. Object detection with discriminatively trained part based models. IEEE Trans. Pattern Anal. Mach. Intell., 32(9):1627–1645, 2010.] - This paper introduces a part-based model for object detection, relevant to pedestrian detection which is often a precursor to re-identification."
      },
      {
        "citation": "[Lowe, D. G. Distinctive image features from scale-invariant keypoints. Int. J. Comp. Vis., 60(2):91–110, 2004.] - This paper introduces SIFT features, a common building block for many image representation techniques used in re-identification."
      },
      {
        "citation": "[Wang, X., Doretto, G., Sebastian, T., Rittscher, J., and Tu, P. Shape and appearance context modeling. In Proc. IEEE Int. Conf. Comp. Vis., 2007.] - This paper explores combining shape and appearance information, a common strategy for robust re-identification."
      },
      {
        "citation": "[Shawe-Taylor, J., and Cristianini, N. Kernel Methods for Pattern Analysis. Cambridge University Press, 2004.] - Provides a foundational understanding of kernel methods, which are often used in metric learning and other re-identification techniques."
      }
    ],
    "author_details": [
      {
        "name": "Sakrapee Paisitkriangkrai",
        "affiliation": "The University of Adelaide, Australia; and Australian Centre for Robotic Vision",
        "email": "*Email not available in the provided text*"
      },
      {
        "name": "Chunhua Shen",
        "affiliation": "The University of Adelaide, Australia; and Australian Centre for Robotic Vision",
        "email": "*Email not available in the provided text*"
      },
      {
        "name": "Anton van den Hengel",
        "affiliation": "The University of Adelaide, Australia; and Australian Centre for Robotic Vision",
        "email": "*Email not available in the provided text*"
      }
    ]
  },
  {
    "title": "Metric imitation by manifold transfer for efﬁcient vision applications\n---AUTHORs---\nDengxin Dai\nTill Kroeger\nRadu Timofte\nLuc Van Gool",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Dai_Metric_Imitation_by_2015_CVPR_supplemental.pdf",
    "id": "Dai_Metric_Imitation_by_2015_CVPR_supplemental",
    "abstract": "This paper introduces Metric Imitation (MI), a novel approach for transferring manifold structure from a source domain to a target domain without requiring labels. MI aims to improve the performance of vision applications by leveraging features learned in a source domain to enhance feature representations in a target domain. The paper demonstrates the effectiveness of MI on three tasks: instance-based object retrieval, image clustering, and category-based image retrieval, using GIST and PHOG features as target features and SIFT-llc, object-bank (OB), and CNN features as source features. Experimental results on several datasets show that MI consistently yields better performance compared to using the original target features.\n\n---TOPICICS---\nMetric Imitation (MI)\nManifold Transfer\nImage Clustering\nObject Retrieval\nFeature Representation",
    "topics": [],
    "references": [
      {
        "citation": "Bosch, A., Zisserman, A., & Muoz, X. (2007). Image classification using random forests and ferns. *ICCV*."
      },
      {
        "citation": "Chatfield, K., Simonyan, K., Vedaldi, A., & Zisserman, A. (2014). Return of the devil in the details: Delving deep into convolutional nets. *BMVC*."
      },
      {
        "citation": "Dai, D., Prasad, M., Leistner, C., & Gool, L. V. (2012). Ensemble partitioning for unsupervised image categorization. *ECCV*."
      },
      {
        "citation": "Dai, D., Wu, T., & Zhu, S. C. (2010). Discovering scene categories by information projection and cluster sampling."
      },
      {
        "citation": "Dana, K. J., van Ginneken, B., Nayar, S. K., & Koenderink, J. J. (1999). Reflectance and texture of real-world surfaces. *ACM Trans. Graph.*, *18*(1), 1–34."
      },
      {
        "citation": "Fei-Fei, L., Fergus, R., & Perona, P. (2004). Learning generative visual models from few training examples: an incremental Bayesian approach tested on 101 object categories. *Workshop on Generative-Model Based Vision*."
      },
      {
        "citation": "Jegou, H., Douze, M., & Schmid, C. (2008). Hamming embedding and weak geometric consistency for large scale image search. *ECCV*."
      },
      {
        "citation": "Lazebnik, S., Schmid, C., & Ponce, J. (2006). Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories. *CVPR*."
      },
      {
        "citation": "Li, L.-J., & Fei-Fei, L. (2007). What, where and who? classifying event by scene and object recognition."
      },
      {
        "citation": "Li, L.-J., Su, H., Xing, E. P., & Li, F.-F. (2010). Object bank: A high-level image representation for scene classification & semantic feature sparsiﬁcation. *NIPS*."
      },
      {
        "citation": "Nist´er, D., & Stew´enius, H. (2006). Scalable recognition with a vocabulary tree. *CVPR*."
      },
      {
        "citation": "Oliva, A., & Torralba, A. (2001). Modeling the shape of the scene: A holistic representation of the spatial envelope. *IJCV*, *42*(3), 145–175."
      },
      {
        "citation": "Tuytelaars, T., Lampert, C. H., Blaschko, M. B., & Buntine, W. (2009). Unsupervised object discovery: A comparison. *IJCV*."
      },
      {
        "citation": "Wang, J., Yang, J., Yu, K., Lv, F., Huang, T., & Gong, Y. (2010). Locality-constrained linear coding for image classification. *CVPR*."
      }
    ],
    "author_details": [
      {
        "name": "Dengxin Dai",
        "affiliation": "Computer Vision Lab, ETH Zurich",
        "email": "dai@vision.ee.ethz.ch"
      },
      {
        "name": "Till Kroeger",
        "affiliation": "Computer Vision Lab, ETH Zurich",
        "email": "kroegert@vision.ee.ethz.ch"
      },
      {
        "name": "Radu Timofte",
        "affiliation": "Computer Vision Lab, ETH Zurich",
        "email": "timofter@vision.ee.ethz.ch"
      },
      {
        "name": "Luc Van Gool",
        "affiliation": "Computer Vision Lab, ETH Zurich; VISICS, ESAT/PSI, KU Leuven",
        "email": "vangool@vision.ee.ethz.ch"
      }
    ]
  },
  {
    "title": "Privacy Preseving Optics for Miniature Vision Sensors\n---AUTHOR---\nFrancesco Pittaluga\n---AUTHOR---\nSanjeev J. Koppal",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Pittaluga_Privacy_Preserving_Optics_2015_CVPR_supplemental.pdf",
    "id": "Pittaluga_Privacy_Preserving_Optics_2015_CVPR_supplemental",
    "abstract": "This paper presents supplementary material for privacy-preserving optics designed for miniature vision sensors. The work investigates the impact of defocusing optics on the performance of face recognition algorithms and derives the angular support for FLIR One thermal sensors and Kinect time-of-flight sensors when fitted with these privacy-preserving optics. Experiments demonstrate the effect of blur on face recognition rates and provide geometric derivations for determining the angular support of the sensors.\n\n---TOPSICS---\nPrivacy-preserving optics\nFace recognition\nAngular support derivation\nFLIR One thermal sensor\nKinect time-of-flight sensor",
    "topics": [],
    "references": [
      {
        "citation": "[Bolme, D. S., Beveridge, J. R., Teixeira, M., & Draper, B. A. The csu face identiﬁcation evaluation system: Its purpose, fea-tures and structure. 2003.] - This appears to be a foundational paper describing a face identification evaluation system, likely relevant to the paper's context."
      },
      {
        "citation": "[Newton, E., Sweeney, L., & Malin, B. Preerving privacy by de-identifying facial images. CMU Technical Report CMU-CS-03-119, 2003.] - Given the mention of privacy, this reference on de-identifying facial images is highly important."
      },
      {
        "citation": "[Phillips, P. J., Moon, H., Rizvi, S. A., & Raus, P. J. The feret evaluation methodology for face-recognition algorithms. 2000.] - The Feret methodology is a standard in face recognition evaluation, making this a key reference."
      }
    ],
    "author_details": [
      {
        "name": "Francesco Pittaluga",
        "affiliation": "University of Florida, Electrical and Computer Engineering Dept.",
        "email": "f.pittaluga@ufl.edu"
      },
      {
        "name": "Sanjeev J. Koppal",
        "affiliation": "University of Florida, Electrical and Computer Engineering Dept.",
        "email": "sjkoppal@ece.ufl.edu"
      }
    ]
  },
  {
    "title": "Leveraging Stereo Matching with Learning-based Conﬁdence Measures\n---AUTHOR---\nMin-Gyu Park\n---AUTHOR---\nKuk-Jin Yoon",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Park_Leveraging_Stereo_Matching_2015_CVPR_paper.pdf",
    "id": "Park_Leveraging_Stereo_Matching_2015_CVPR_paper",
    "abstract": "Abstract not found",
    "topics": [],
    "references": [
      {
        "citation": "[Breiman, L. (2001). Random forests. *Machine Learning*, 5-32.] - This is a seminal work on Random Forests, a widely used machine learning algorithm, likely relevant if the paper uses such methods."
      },
      {
        "citation": "[Egnal, G., Mintz, M., & Wildes, R. P. (2004). A stereo confidence metric using single view imagery with comparison to five alternative approaches. *Image and Vision Computing*, 22(12), 943-957.] - This paper introduces a stereo confidence metric, a key aspect of stereo vision and likely a foundational reference."
      },
      {
        "citation": "[Hirschmüller, H. (2008). Stereo processing by semiglobal matching and mutual information. *PAMI*, 30(2), 328-341.] - This paper presents a significant approach to stereo processing using semiglobal matching, a common technique."
      },
      {
        "citation": "[Spyropoulos, N. K. A., & Mordohai, N. (2014). Learning to detect ground control points for improving the accuracy of stereo matching. *CVPR*, 2014.] - This paper addresses a specific challenge in stereo matching (ground control points) and is likely relevant if the paper deals with accuracy improvements."
      },
      {
        "citation": "[Geiger, A., Lenz, P., Stiller, C., & Urbas, J. (2013). Vision meets robotics: The KITTI dataset. *International Journal of Robotics Research (IJRR)*, 2013.] - The KITTI dataset is a standard benchmark for vision and robotics, so this reference is important if the paper uses or compares against it."
      },
      {
        "citation": "[Hu, X., & Mordohai, N. (2012). A quantitative evaluation of confidence measures for stereo vision. *PAMI*, 34(11), 2121-2133.] - This paper provides a quantitative evaluation of confidence measures, a crucial aspect of stereo vision."
      },
      {
        "citation": "[Kostkova, J. (2003). Stratified dense matching for stereopsis in complex scenes. *BMVC*, 339-348.] - This paper addresses dense matching in complex scenes, a common challenge in stereo vision."
      },
      {
        "citation": "[Manduchi, R., & Tomasi, C. (1999). Distinctiveness maps for image matching. *ICIAP*, 26-31.] - Distinctiveness maps are a technique for image matching, and this reference is likely relevant if the paper uses such methods."
      },
      {
        "citation": "[Haeusler, R., Nair, R., & Kondermann, D. (2013). Ensemble learning for confidence measures in stereo vision. *CVPR*, 305-312.] - This paper explores ensemble learning for confidence measures, a relevant technique for improving stereo vision."
      },
      {
        "citation": "[Lew, M. S., Huang, T. S., & Wong, K. (1994). Learning and feature selection in stereo matching. *PAMI*, 16(9), 869-881.] - This paper explores learning and feature selection in stereo matching, a foundational work in the field."
      }
    ],
    "author_details": [
      {
        "name": "Min-Gyu Park",
        "affiliation": "GIST",
        "email": "mpark@gist.ac.kr"
      },
      {
        "name": "Kuk-Jin Yoon",
        "affiliation": "GIST",
        "email": "kjyoon@gist.ac.kr"
      }
    ]
  },
  {
    "title": "Viewpoints and Keypoints\n---AUTHORs---\nShubham Tulsiani\nJitendra Malik",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Tulsiani_Viewpoints_and_Keypoints_2015_CVPR_paper.pdf",
    "id": "Tulsiani_Viewpoints_and_Keypoints_2015_CVPR_paper",
    "abstract": "We characterize the problem of pose estimation for rigid objects in terms of determining viewpoint to explain coarse pose and keypoint prediction to capture the finer details. We address both these tasks in two different settings - the constrained setting with known bounding boxes and the more challenging detection setting where the aim is to simultaneously detect and correctly estimate pose of objects. We present Convolutional Neural Network based architectures for these and demonstrate that leveraging viewpoint estimates can substantially improve local appearance based keypoint predictions. In addition to achieving significant improvements over state-of-the-art in the above tasks, we analyze the error modes and effect of object characteristics on performance to guide future efforts towards this goal.\n\n---TOPIC---\nPose Estimation\nKeypoint Prediction\nViewpoint Prediction\nConvolutional Neural Networks (CNNs)\nObject Detection",
    "topics": [],
    "references": [],
    "author_details": [
      {
        "name": "Shubham Tulsiani",
        "affiliation": "University of California, Berkeley",
        "email": "shubhtuls@eecs.berkeley.edu"
      },
      {
        "name": "Jitendra Malik",
        "affiliation": "University of California, Berkeley",
        "email": "malik@eecs.berkeley.edu"
      }
    ]
  },
  {
    "title": "Unknown Title",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Song_Joint_Multi-Feature_Spatial_2015_CVPR_paper.pdf",
    "id": "Song_Joint_Multi-Feature_Spatial_2015_CVPR_paper",
    "abstract": "Abstract not found",
    "topics": [],
    "references": [],
    "author_details": []
  },
  {
    "title": "A Linear Least-Squares Solution to Elastic Shape-from-Template\n---AUTHOR---\nAbed Malti\nAdrien Bartoli\nRichard Hartley",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Malti_A_Linear_Least-Squares_2015_CVPR_paper.pdf",
    "id": "Malti_A_Linear_Least-Squares_2015_CVPR_paper",
    "abstract": "This paper addresses the limitations of existing Shape-from-Template (SfT) methods, which often struggle to balance accuracy, speed, and robustness when dealing with elastic deformations. Current approaches, including analytical solutions, non-linear optimization, and Kalman filtering, suffer from drawbacks such as computational cost, dependence on initialization, and error accumulation. The proposed solution utilizes mechanical constraints within a linear least-squares estimation framework, employing finite element methods (FEM) to represent the surface and deformation. This formulation allows for accurate and efficient reconstruction of 3D deformed surfaces, offering a novel fully linear least-squares SfT method that models elastic deformations.\n\n---TOPICICS---\nShape-from-Template (SfT)\nElastic Deformations\nFinite Element Methods (FEM)\nLinear Least-Squares Estimation\nMechanical Constraints",
    "topics": [],
    "references": [
      {
        "citation": "[Agudo, A., Calvo, B., and Montiel, J. FEM models to code non-rigid EKF monocular SLAM. IEEE Workshop on Dynamic Shape Capture and Analysis of ICCV, 2011.] - Cited frequently (1, 2, 6), likely foundational to the work."
      },
      {
        "citation": "[Agudo, A., Calvo, B., and Montiel, J. Finite element based sequential bayesian non-rigid structure from motion. CVPR, 2012.] - Also frequently cited (1, 2), suggesting a key related work."
      },
      {
        "citation": "[Bartoli, A., G´erard, Y., Chadebecq, F., and Collins, T. On template-based reconstruction from a single view: Analytical solutions and proofs of well-posedness for developable, isometric and conformal surfaces. CVPR, 2012.] - Cited frequently (1, 2, 6), indicating relevance to the methodology."
      },
      {
        "citation": "[Salzmann, M., and Urtasun, R. Beyond feature points: Structured prediction for monocular non-rigid 3D reconstruction. ECCV, 2012.] - Cited once, likely a relevant approach."
      },
      {
        "citation": "[Moreno-Noguer, F., and Porta, J. Probabilistic simultaneous pose and non-rigid shape recovery. CVPR, 2011.] - Cited once, likely a related approach."
      },
      {
        "citation": "[Chaskalovic, J. Finite Elements Methods for Engineering Sciences. Springer Verlag, 2008.] - Cited for background knowledge (5, 6)."
      },
      {
        "citation": "[Salzmann, M., and Fua, P. Reconstructing sharply folding surfaces: A convex formulation. CVPR, 2009.] - Cited once, likely a relevant approach."
      },
      {
        "citation": "[Lowe, D. G. Distinctive image features from scale-invariant keypoints. International Journal of Computer Vision, 60(2):91–110, 2004.] - Cited once, likely for feature extraction."
      },
      {
        "citation": "[Salzmann, M., and Fua, P. Linear local models for monocular reconstruction of deformable surfaces. IEEE Transactions on Pattern Analysis and Machine Intelligence, pages 931–944, 2011.] - Cited once, likely a relevant approach."
      },
      {
        "citation": "[Matthews, I., and Baker, S. Active appearance models revisited. International Journal of Computer Vision, 60(2):135–164, November 2004.] - Cited once, likely for related modeling techniques."
      }
    ],
    "author_details": [
      {
        "name": "Abed Malti",
        "affiliation": "Fluminance/INRIA, Rennes, France",
        "email": "[Email not available in provided text]"
      },
      {
        "name": "Adrien Bartoli",
        "affiliation": "ALCoV/ISIT, UMR 6284 CNRS/Universit´e d’Auvergne, Clermont-Ferrand, France",
        "email": "[Email not available in provided text]"
      },
      {
        "name": "Richard Hartley",
        "affiliation": "Australian National University and NICTA, Canberra, Australia",
        "email": "[Email not available in provided text]"
      }
    ]
  },
  {
    "title": "Riemannian Coding and Dictionary Learning: Kernels to the Rescue\n---AUTHOR---\nMehrtash Harandi\nMathieu Salzmann",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Harandi_Riemannian_Coding_and_2015_CVPR_paper.pdf",
    "id": "Harandi_Riemannian_Coding_and_2015_CVPR_paper",
    "abstract": "This paper addresses the problem of coding and dictionary learning on Riemannian manifolds. Many powerful image and video descriptors, such as covariance descriptors, normalized histograms, linear subspaces, and 2D shape outlines, are known to lie on Riemannian manifolds. Existing solutions either are dedicated to specific manifolds or rely on optimization problems that are difficult to solve, especially when it comes to dictionary learning. To overcome these limitations, the authors propose to utilize kernels to perform coding and dictionary learning on Riemannian manifolds. They introduce a general Riemannian coding framework with its kernel-based counterpart, allowing for generalization beyond sparse coding, efficient solutions to coding schemes, learning of kernel parameters, and simpler dictionary learning. The effectiveness of the approach is demonstrated on various non-flat manifolds and applied to Euclidean spaces.\n\n---TOPIC---\nRiemannian Manifolds\n---TOPIC---\nKernel Methods\n---TOPIC---\nDictionary Learning\n---TOPIC---\nCoding Theory\n---TOPIC---\nReproducing Kernel Hilbert Space (RKHS)",
    "topics": [],
    "references": [],
    "author_details": [
      {
        "name": "Mehrtash Harandi",
        "affiliation": "Australian National University & NICTA",
        "email": "mehrtash.harandi@nicta.com.au"
      },
      {
        "name": "Mathieu Salzmann",
        "affiliation": "Australian National University & NICTA",
        "email": "mathieu.salzmann@nicta.com.au"
      }
    ]
  },
  {
    "title": "Improving Object Detection with Deep Convolutional Networks via Bayesian Optimization and Structured Prediction\n---AUTHOR---\nYuting Zhang\nKihyuk Sohn\nRuben Villegas\nGang Pan\nHonglak Lee",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Zhang_Improving_Object_Detection_2015_CVPR_paper.pdf",
    "id": "Zhang_Improving_Object_Detection_2015_CVPR_paper",
    "abstract": "Object detection systems based on the deep convolutional neural network (CNN) have recently made groundbreaking advances on several object detection benchmarks. While the features learned by these high-capacity neural networks are discriminative for categorization, inaccurate localization is still a major source of error for detection. Building upon high-capacity CNN architectures, we address the localization problem by 1) using a search algorithm based on Bayesian optimization that sequentially proposes candidate regions for an object bounding box, and 2) training the CNN with a structured loss that explicitly penalizes the localization inaccuracy. In experiments, we demonstrate that each of the proposed methods improves the detection performance over the baseline method on PASPAL VOC 2007 and 2012 datasets. Furthermore, two methods are complementary and significantly outperform the previous state-of-the-art when combined.",
    "topics": [
      "Deep Convolutional Neural Networks (CNNs)",
      "Object Detection",
      "Bayesian Optimization",
      "Structured Prediction (Structured SVM)",
      "Localization Accuracy"
    ],
    "references": [
      {
        "citation": "[Hinton, G. E., & Salakhutdinov, R. R. (2006). Reducing the dimensionality of data with neural networks. *Science*, *313*(5786), 504–507.]"
      },
      {
        "citation": "[Ahonen, T., Hadid, A., & Pietikinen, M. (2004). Face recognition with local binary patterns. In *ECCV*.]"
      },
      {
        "citation": "[Bengio, Y., Lamblin, P., Popovici, D., & Larochelle, H. (2007). Greedy layer-wise training of deep networks. In *NIPS*.]"
      },
      {
        "citation": "[Bengio, Y., Courville, A., & Vincent, P. (2013). Representation learning: A review and new perspectives. *IEEE Transactions on Pattern Analysis and Machine Intelligence*, *35*(8), 1798–1828.]"
      },
      {
        "citation": "[Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). ImageNet classification with deep convolutional neural networks. In *NIPS*.]"
      },
      {
        "citation": "[Girshick, R., Donahue, J., Darrell, T., & Malik, J. (2014). Rich feature hierarchies for accurate object detection and semantic segmentation. In *CVPR*.]"
      },
      {
        "citation": "[Everingham, M., Van Gool, L., Williams, C. K. I., Winn, J., & Zisserma, A. (2007). The PASUAL Visual Object Classes Challenge 2007 (VOC2007) Results.]"
      },
      {
        "citation": "[Deng, J., Dong, W., Socher, R., Li, L. J., Li, K., & Fei-Fei, L. (2009). ImageNet: A large-scale hierarchical image database. In *CVPR*.]"
      },
      {
        "citation": "[Donahue, J., Jia, Y., Vinyals, O., Hoffman, J., Zhang, N., Tzeng, E., & Darrell, T. (2013). DeCAF: A deep convolutional activation feature for generic visual recognition. *CoRR*, abs/1310.1531.]"
      },
      {
        "citation": "[Erhan, D., Szegedy, C., Toshev, A., & Anguelov, D. (2014). Scalable object detection using deep neural networks. In *CVPR*.]"
      }
    ],
    "author_details": [
      {
        "name": "Yuting Zhang",
        "affiliation": "Department of Computer Science, Zhejiang University",
        "email": "zyt@zju.edu.cn"
      },
      {
        "name": "Kihyuk Sohn",
        "affiliation": "Department of Electrical Engineering and Computer Science, University of Michigan",
        "email": "yutingzh@umich.edu"
      },
      {
        "name": "Ruben Villegas",
        "affiliation": "Department of Electrical Engineering and Computer Science, University of Michigan",
        "email": "rubville@umich.edu"
      },
      {
        "name": "Gang Pan",
        "affiliation": "Department of Computer Science, Zhejiang University",
        "email": "gpan@zju.edu.cn"
      },
      {
        "name": "Honglak Lee",
        "affiliation": "Department of Electrical Engineering and Computer Science, University of Michigan",
        "email": "honglak@umich.edu"
      }
    ]
  },
  {
    "title": "A Geodesic-Preserving Method for Image Warping\n---AUTHORs---\nDongping Li\nKaiming He\nJian Sun\nKun Zhou",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Li_A_Geodesic-Preserving_Method_2015_CVPR_supplemental.pdf",
    "id": "Li_A_Geodesic-Preserving_Method_2015_CVPR_supplemental",
    "abstract": "This paper introduces a novel geodesic-preserving method for image warping. The approach aims to maintain the shape and boundary of the image while minimizing distortions caused by the warping process. The core of the method lies in preserving geodesic distances, which ensures local smoothness and prevents the creation of unwanted artifacts. The authors formulate an energy function that incorporates shape, boundary, and geodesic preservation terms, and solve it using the Gauss-Newton method. The effectiveness of the method is demonstrated through experimental results, showcasing its ability to produce high-quality warped images.\n\n---TOPSICS---\nImage Warping\nGeodesic Preservation\nShape and Boundary Preservation\nEnergy Minimization\nGauss-Newton Method",
    "topics": [],
    "references": [
      {
        "citation": "[Zhang, G., Cheng, M., Hu, S., & Martin, R. (2009). A shape-preserving approach to image resizing. Computer Graphics Forum.] - This paper is directly referenced and appears crucial for the methodology described, specifically regarding shape preservation."
      },
      {
        "citation": "[IEEE (2015). 978-1-4673-6964-0/15/$31.00 ©2015 IEEE] - This is the publisher and copyright information, indicating the paper's origin and likely context."
      },
      {
        "citation": "[Reference related to Eqn. (1): Rotation matrix Rθ,φ] - While not explicitly listed, the understanding of rotation matrices is fundamental to the described method."
      },
      {
        "citation": "[Reference related to Eqn. (7): Shape-preserving term ES(V)] - This equation is central to the shape-preserving approach, making the referenced paper essential."
      },
      {
        "citation": "[Reference related to Eqn. (4): Local smoothness preservation EC(V)] - This equation describes a method for preserving local smoothness, a key aspect of the work."
      },
      {
        "citation": "[Reference related to Eqn. (5): Combined energy function E(V)] - This equation represents the overall energy function being minimized, combining various terms."
      },
      {
        "citation": "[Reference related to Gauss-Newton method] - The Gauss-Newton method is used to solve the energy function, so a reference on this optimization technique would be relevant."
      },
      {
        "citation": "[Reference related to orthogonal matrices] - The definition of Q relies on orthogonal matrices, so a reference on this mathematical concept would be useful."
      },
      {
        "citation": "[Reference related to quad representation] - The use of \"quads\" is central to the shape representation, so a reference explaining this concept would be helpful."
      },
      {
        "citation": "[Reference related to geodesic preservation] - The method replaces a geodesic-preserving term, so a reference on geodesic preservation would be relevant for context."
      }
    ],
    "author_details": [
      {
        "name": "Dongping Li",
        "affiliation": "Zhejiang University",
        "email": "[Email not available]"
      },
      {
        "name": "Kaiming He",
        "affiliation": "Microsoft Research",
        "email": "[Email not available]"
      },
      {
        "name": "Jian Sun",
        "affiliation": "Microsoft Research",
        "email": "[Email not available]"
      },
      {
        "name": "Kun Zhou",
        "affiliation": "Zhejiang University",
        "email": "[Email not available]"
      }
    ]
  },
  {
    "title": "Simultaneous Pose and Non-Rigid Shape with Particle Dynamics\n---AUTHOR---\nAntonio Agudo\nFrancesc Moreno-Noguer",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Agudo_Simultaneous_Pose_and_2015_CVPR_paper.pdf",
    "id": "Agudo_Simultaneous_Pose_and_2015_CVPR_paper",
    "abstract": "In this paper, we propose a sequential solution to simultaneously estimate camera pose and non-rigid 3D shape from a monocular video. In contrast to most existing approaches that rely on global representations of the shape, we model the object at a local level, as an ensemble of particles, each ruled by the linear equation of the Newton’s second law of motion. This dynamic model is incorporated into a bundle adjustment framework, in combination with simple regularization components that ensure temporal and spatial consistency of the estimated shape and camera poses. The resulting approach is both efficient and robust to several artifacts such as noisy and missing data or sudden camera motions, while it does not require any training data at all. Validation is done in a variety of real video sequences, including articulated and non-rigid motion, both for continuous and discontinuous shapes. Our system is shown to perform comparable to competing batch, computationally expensive, methods and shows remarkable improvement with respect to the sequential ones.",
    "topics": [
      "Non-Rigid Structure from Motion (NRSfM)",
      "Particle Dynamics",
      "Bundle Adjustment",
      "Camera Pose Estimation",
      "Monocular Video Analysis"
    ],
    "references": [],
    "author_details": [
      {
        "name": "Antonio Agudo",
        "affiliation": "Instituto de Invesigaci´on en Ingenier´ıa de Arag´on (I3A), Universidad de Zaragoza, Spain",
        "email": "[Email not available in the provided text]"
      },
      {
        "name": "Francesc Moreno-Noguer",
        "affiliation": "Institut de Rob`otica i Inform`atica Industrial (CSI-UPC), Barcelona, Spain",
        "email": "[Email not available in the provided text]"
      }
    ]
  },
  {
    "title": "Traditional Saliency Reloaded: A Good Old Model in New Shape\n---AUTHOR---\nSimone Frintrop\n---AUTHOR---\nThomas Werner\n---AUTHOR---\nGermán M. García",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Frintrop_Traditional_Saliency_Reloaded_2015_CVPR_paper.pdf",
    "id": "Frintrop_Traditional_Saliency_Reloaded_2015_CVPR_paper",
    "abstract": "This paper investigates the continued relevance of the seminal, biologically-inspired salience model by Itti et al. [21] in the context of current state-of-the-art methods for salient object segmentation. The authors demonstrate that with key adaptations, particularly concerning the scale-space structure (introducing a twin pyramid for flexible center-surround ratio), the original model remains competitive. They present a new system, VOCUS2, characterized by its elegant structure, speed, and pixel-level salience computation. Furthermore, they integrate the system into an object proposal generation framework to produce segment-based salience maps, achieving state-of-the-art performance on benchmark datasets. The work highlights the importance of revisiting foundational approaches and adapting them for modern applications.\n\n---TOPICCS---\nSaliency Models\nItti Model\nComputer Vision\nObject Segmentation\nScale-space representation",
    "topics": [],
    "references": [
      {
        "citation": "[R. Achanta, S. Hemami, F. Estrada, and S. S¨usstrunk, Frequency-tuned salient region detection. In CVPR, 2009.]"
      },
      {
        "citation": "[S. Frintrop, G. M. Garc´ıa, and A. B. Cremers, A cognitive approach for object discovery. In ICPR, 2014.]"
      },
      {
        "citation": "[D. Gao, S. Han, and N. Vasconcelos, Discriminant saliency, the detection of suspicious coincidences, and applications to visual recognition. TPAMI, 31(6), 2009.]"
      },
      {
        "citation": "[S. Alpert, M. Galun, R. Basri, and A. Brandt, Image segmentation by probabilistic bottom-up aggregation and cue integration. In CVPR, 2007.]"
      },
      {
        "citation": "[A. Borji and L. Itti, State-of-the-art in visual attention modeling. TPAMI, 2010.]"
      },
      {
        "citation": "[L. Itti and P. Baldi, Bayesian surprise attracts human attention. Vision Research, 49(10), 2009.]"
      },
      {
        "citation": "[L. Itti, C. Koch, and E. Niebur, A model of saliency-based visual attention for rapid scene analysis. TPAMI, 20(11), 1998.]"
      },
      {
        "citation": "[M.-M. Cheng, N. J. Mitra, X. Huang, P. H. S. Torr, and S.-M. Hu, Global contrast based salient region detection. TPAMI, 37(3):569–582, 2015.]"
      },
      {
        "citation": "[N. D. B. Bruce and J. K. Tsotzos, Salience, attention, and visual search: An information theoretic approach. J. of Vision, 9(3), 2009.]"
      },
      {
        "citation": "[L. Hurvich and D. Jameson, An opponent-process theory of color vision. Psychological review, 64(6), 1957.]"
      }
    ],
    "author_details": [
      {
        "name": "Simone Frintrop",
        "affiliation": "Rheinische Friedrich-Wilhelms-Universit¨at Bonn",
        "email": "frintrop@iai.uni-bonn.de"
      },
      {
        "name": "Thomas Werner",
        "affiliation": "Rheinische Friedrich-Wilhelms-Universit¨at Bonn",
        "email": "*Not available*"
      },
      {
        "name": "Germán M. García",
        "affiliation": "Rheinische Friedrich-Wilhelms-Universit¨at Bonn",
        "email": "*Not available*"
      }
    ]
  },
  {
    "title": "Dense, Accurate Optical Flow Estimation with Piecewise Parametric Model\n---AUTHOR---\nJiaolong Yang\n---AUTHOR---\nHongdong Li",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Yang_Dense_Accurate_Optical_2015_CVPR_paper.pdf",
    "id": "Yang_Dense_Accurate_Optical_2015_CVPR_paper",
    "abstract": "This paper proposes a simple method for estimating dense and accurate optical ﬂow ﬁeld. It revitalizes an early idea of piecewise parametric ﬂow model. A key innovation is that, we fit a ﬂow ﬁeld piecewise to a variety of parametric models, where the domain of each piece (i.e., each piece’s shape, position and size) is determined adaptively, while at the same time maintaining a global inter-piece ﬂow continuity constraint. We achieve this by a multi-model fitting scheme via energy minimization. Our energy takes into account both the piecewise constant model assumption and the ﬂow ﬁeld continuity constraint, enabling the proposed method to effectively handle both homogeneous motions and complex motions. The experiments on three public optical ﬂow benchmarks (KITTI, MPI Sintel, and Middlebury) show the superiority of our method compared with the state of the art: it achieves top-tier performances on all the three benchmarks.",
    "topics": [
      "Optical flow estimation",
      "Piecewise parametric models",
      "Energy minimization",
      "Homography transformation",
      "Motion segmentation"
    ],
    "references": [
      {
        "citation": "[Baker, S., Scharstein, D., Lewis, J., Roth, S., Black, M. J., & Szeliski, R. (2011). A database and evaluation methodology for optical flow. International Journal of Computer Vision (IJCV), 92(1), 1–31.]"
      },
      {
        "citation": "[Bao, L., Yang, Q., & Jin, H. (2014). Fast edge-preserving patchmatch for large displacement optical flow. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 5534–5541.]"
      },
      {
        "citation": "[Barnes, C., Shechtman, E., Finkelshtein, A., & Goldman, D. (2009). PatchMatch: A randomized correspondence algorithm for structural image editing. ACM Transactions on Graphics (TOG), 28(3), 24.]"
      },
      {
        "citation": "[Bhat, P., Zheng, K. C., Snavel, N., Agarwala, A., Agrawala, M., Cohen, M. F., & Curless, B. (2006). Piecewise image registration in the presence of multiple large motions. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), volume 2, pages 2491–2497.]"
      },
      {
        "citation": "[Birchfield, S., & Tomasi, C. (1999). Multiway cut for stereo and motion with slanted surfaces. In International Conference on Computer Vision (ICCV), volume 1, pages 489–495.]"
      },
      {
        "citation": "[Black, M. J., & Anandan, P. (1996). The robust estimation of multiple motions: Parametric and piecewise-smooth flow fields. Computer Vision and Image Understanding (CVIU), 63(1), 75–104.]"
      },
      {
        "citation": "[Black, M. J., & Jepson, A. D. (1996). Estimating optical flow in segmented images using variable-order parametric models with local deformations. IEEE Transactions on Pattern Analysis and Machine Intelligence (T PAM I), 18(10), 972–986.]"
      },
      {
        "citation": "[Bleyer, M., Rhemann, C., & Rother, C. (2011). PatchMatch Stereo - Stereo matching with slanted support windows. In British Machine Vision Conference (BMVC), pages 14.1–14.11.]"
      },
      {
        "citation": "[Boykov, Y., Vekslér, O., & Zabih, R. (2001). Fast approximate energy minimization via graph cuts. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 23(11), 1222–1239.]"
      },
      {
        "citation": "[Braux-Zin, J., Dupont, R., & Bartoli, A. (2013). A general dense image matching framework combining direct and feature-based costs. In International Conference on Computer Vision (ICCV), pages 185–192.]"
      }
    ],
    "author_details": [
      {
        "name": "Jiaolong Yang",
        "affiliation": "Beijing Lab of Intelligent Information Technology, Beijing Institute of Technology",
        "email": "[Email not available in the provided text]"
      },
      {
        "name": "Hongdong Li",
        "affiliation": "Research School of Engineering, The Australian National University (ANU) and NICTA",
        "email": "[Email not available in the provided text]"
      }
    ]
  },
  {
    "title": "Encoding based Saliency Detection for Videos and Images\n---AUTHOR---\nThomas Mauthner\nHorst Possegger\nGeorg Waltner\nHorst Bischof",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Mauthner_Encoding_Based_Saliency_2015_CVPR_paper.pdf",
    "id": "Mauthner_Encoding_Based_Saliency_2015_CVPR_paper",
    "abstract": "Recent research focuses on predicting human gaze in images and videos, leveraging eye-tracking data and annotations. However, reliance on human gaze or annotations can introduce bias and may not be suitable for general salient object detection. This paper presents a novel, fully unsupervised video saliency detection method to support human activity recognition and weakly supervised training of activity detection algorithms. The approach utilizes an encoding method to approximate joint feature distributions, enabling efficient computation of salience by enforcing the Gestalt principle of figure-ground segregation. The method is evaluated on challenging datasets and demonstrates favorable performance compared to state-of-the-art methods in estimating ground-truth eye-gaze and activity annotations.\n\n---TOPIC---\nVideo Salience Detection\nUnsupervised Learning\nGestalt Principles\nEncoding Methods\nHuman Activity Recognition",
    "topics": [],
    "references": [
      {
        "citation": "[Itti, L., Koch, C., and Niebur, E. A model of salience-based visual attention for rapid scene analysis. PAMI, 1998.]"
      },
      {
        "citation": "[Alexe, B., Deselaers, T., and Ferrari, V. What is an object? In CVPR, 2010.]"
      },
      {
        "citation": "[Liu, T., Sun, J., Zheng, N.-N., Tang, X., and Shum, H.-Y. Learning to Detect A Salient Object. In CVPR, 2007.]"
      },
      {
        "citation": "[Johansson, G. Visual perception of biological motion and a model for its analysis. Perception & Psychophysics, 1973.]"
      },
      {
        "citation": "[Gorelick, L., Blank, M., Shechtman, E., Irani, M., and Basri, R. Actions as Space-Time Shapes. PAMI, 2007.]"
      },
      {
        "citation": "[Borji, A., Sihte, D., and Itti, L. Salient Object Detection: A Benchmark. In ECCV, 2012.]"
      },
      {
        "citation": "[Guo, C., Ma, Q., and Zhang, L. Spatio-temporal Saliency detection using phase spectrum of quaternion fourier transform. In CVPR, 2008.]"
      },
      {
        "citation": "[Judd, T., Ehinger, K., Durand, F., and Torralba, A. Learning to Predict Where Humans Look. In ICCV, 2009.]"
      },
      {
        "citation": "[Harel, J., Koch, C., and Perona, P. Graph-based Visual Saliency. In NIPS, 2006.]"
      },
      {
        "citation": "[Rahtu, E., Kannala, J., Salo, M., and Heikkil¨a, J. Segmenting Salient Objects from Images and Videos. In ECCV, 2010.]"
      }
    ],
    "author_details": [
      {
        "name": "Thomas Mauthner",
        "affiliation": "Institute for Computer Graphics and Vision, Graz University of Technology",
        "email": "mauthner@icg.tugraz.at"
      },
      {
        "name": "Horst Possegger",
        "affiliation": "Institute for Computer Graphics and Vision, Graz University of Technology",
        "email": "possegger@icg.tugraz.at"
      },
      {
        "name": "Georg Waltner",
        "affiliation": "Institute for Computer Graphics and Vision, Graz University of Technology",
        "email": "waltner@icg.tugraz.at"
      },
      {
        "name": "Horst Bischof",
        "affiliation": "Institute for Computer Graphics and Vision, Graz University of Technology",
        "email": "bischof@icg.tugraz.at"
      }
    ]
  },
  {
    "title": "Visual Saliency Based on Multiscale Deep Features\n---AUTHOR---\nGuanbin Li\nYizhou Yu",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Li_Visual_Saliency_Based_2015_CVPR_supplemental.pdf",
    "id": "Li_Visual_Saliency_Based_2015_CVPR_supplemental",
    "abstract": "This paper introduces a novel approach to salient object detection based on multiscale deep features (MDF). The method leverages deep features extracted at multiple scales to capture objects of varying sizes and complexities. The proposed approach is evaluated on the SED dataset and a newly created dataset (HKU-IS), demonstrating superior performance compared to existing methods in terms of precision, recall, F-measure, and mean absolute error. Visual comparisons further confirm the effectiveness of the MDF approach in generating accurate salience maps.\n\n---TOPICs---\nSalient Object Detection\nDeep Learning Features\nMultiscale Analysis\nComputer Vision\nImage Processing",
    "topics": [],
    "references": [
      {
        "citation": "[Achanta, R., Hemami, S., Estrada, F., & Susstrunk, S. (2009). Frequency-tuned salient region detection. *CVPR*.] - Referenced frequently and appears to be a foundational work."
      },
      {
        "citation": "[Cheng, M.-M., Mitra, N. J., Huang, X., Torr, P. H. S., & Hu, S.-M. (2014). Global contrast based salient region detection. *TPAMI*.] - Referenced frequently and appears to be a foundational work."
      },
      {
        "citation": "[Hou, X., & Zhang, L. (2007). Saliency detection: A spectral residual approach. *CVPR*.] - Referenced frequently and appears to be a foundational work."
      },
      {
        "citation": "[Jiang, H., Wang, J., Yuan, Z., Wu, Y., Zheng, N., & Li, S. (2013). Salient object detection: A discriminative regional feature integration approach. *CVPR*.] - Referenced frequently and appears to be a foundational work."
      },
      {
        "citation": "[Perazzi, F., Krahenbuhl, P., Pritch, Y., & Hornung, A. (2012). Saliency filters: Contrast based filtering for salient region detection. *CVPR*.] - Referenced frequently and appears to be a foundational work."
      },
      {
        "citation": "[Wei, Y., Wen, F., Zhu, W., & Sun, J. (2012). Geodesic saliency using background priors. *ECCV*.] - Referenced frequently and appears to be a foundational work."
      },
      {
        "citation": "[Yan, Q., Xu, L., Shi, J., & Jia, J. (2013). Hierarchical saliency detection. *CVPR*.] - Referenced frequently and appears to be a foundational work."
      },
      {
        "citation": "[Yang, C., Zhang, L., Lu, H., Ruan, X., & Yang, M.-H. (2013). Saliency detection via graph-based manifold ranking. *CVPR*.] - Referenced frequently and appears to be a foundational work."
      },
      {
        "citation": "[Zhu, W., Liang, S., Wei, Y., & Sun, J. (2014). Saliency optimization from robust background detection. *CVPR*.] - Referenced frequently and appears to be a foundational work."
      },
      {
        "citation": "[Sun, J., Wei, Y., & Zhu, W. (2014). MDF: A discriminative manifold-based salient object detection. *CVPR*.] - This is the paper itself, so it's implicitly referenced throughout."
      }
    ],
    "author_details": [
      {
        "name": "Guanbin Li",
        "affiliation": "Department of Computer Science, The University of Hong Kong",
        "email": "Not available in the provided text."
      },
      {
        "name": "Yizhou Yu",
        "affiliation": "Department of Computer Science, The University of Hong Kong",
        "email": "Not available in the provided text."
      }
    ]
  },
  {
    "title": "Unsupervised Object Discovery and Localization in the Wild: Part-based Matching with Bottom-up Region Proposals\n---AUTHOR---\nMinsu Cho\nSuha Kwak\nCordelia Schmid\nJean Ponce",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Cho_Unsupervised_Object_Discovery_2015_CVPR_paper.pdf",
    "id": "Cho_Unsupervised_Object_Discovery_2015_CVPR_paper",
    "abstract": "This paper addresses unsupervised discovery and localization of dominant objects from a noisy image collection with multiple object classes. The setting of this problem is fully unsupervised, without even image-level annotations or any assumption of a single dominant class. We tackle the discovery and localization problem using a part-based region matching approach: We use off-the-shelf region proposals to form a set of candidate bounding boxes for objects and object parts. These regions are efficiently matched across images using a probabilistic Hough transform that evaluates the confidence for each candidate correspondence considering both appearance and spatial consistency. Dominant objects are discovered and localized by comparing the scores of candidate regions and selecting those that stand out over other regions containing them. Extensive experimental evaluations on standard benchmarks demonstrate that the proposed approach significantly outperforms the current state of the art in colocalization, and achieves robust object discovery in challenging mixed-class datasets.\n\n---TOPICCS---\nUnsupervised object localization\nPart-based region matching\nProbabilistic Hough transform\nObject discovery\nRegion proposals",
    "topics": [],
    "references": [
      {
        "citation": "[B. Alexe, T. Deselaers, and V. Ferrari. Measuring the object-ness of image windows. TPAMI, 2012.]"
      },
      {
        "citation": "[B. Hariharan, J. Malik, and D. Ramanan. Discriminative decorrelation for clustering and classification. In ECCV, 2012.]"
      },
      {
        "citation": "[D. Ballard. Generalizing the Hough transform to detect arbitrary shapes. Pattern Recognition, 1981.]"
      },
      {
        "citation": "[O. Barinova, V. Lempitsky, and P. Kohli. On detection of multiple object instances using Hough transforms. In CVPR, 2010.]"
      },
      {
        "citation": "[A. Joulin, F. Bach, and J. Ponce. Discriminative clustering for image co-segmentation. In CVPR, 2010.]"
      },
      {
        "citation": "[M. Cho, K. Alahari, and J. Ponce. Learning graphs to match. In ICCV, 2013.]"
      },
      {
        "citation": "[A. Joulin, K. Tang, and L. Fei-Fei. Efficient image and video co-localization with frank-wolfe algorithm. In ECCV, 2014.]"
      },
      {
        "citation": "[J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A large-scale hierarchical image database. In CVPR, 2009.]"
      },
      {
        "citation": "[R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In CVPR, 2014.]"
      },
      {
        "citation": "[P. Felzenszwalb and D. P. Huttenlocher. Pictorial structures for object recognition. IJCV, 2003.]"
      }
    ],
    "author_details": [
      {
        "name": "Minsu Cho",
        "affiliation": "Inria",
        "email": "[Email not available]"
      },
      {
        "name": "Suha Kwak",
        "affiliation": "Inria",
        "email": "[Email not available]"
      },
      {
        "name": "Cordelia Schmid",
        "affiliation": "Inria",
        "email": "[Email not available]"
      },
      {
        "name": "Jean Ponce",
        "affiliation": "École Normale Supérieure / PSL Research University",
        "email": "[Email not available]"
      }
    ]
  },
  {
    "title": "Pose-Conditioned Joint Angle Limits for 3D Human Pose Reconstruction\n---AUTHOR---\nIjaz Akhter\n---AUTHOR---\nMichael J. Black",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Akhter_Pose-Conditioned_Joint_Angle_2015_CVPR_paper.pdf",
    "id": "Akhter_Pose-Conditioned_Joint_Angle_2015_CVPR_paper",
    "abstract": "Estimating 3D human pose from 2D joint locations is central to the analysis of people in images and video. To address the fact that the problem is inherently ill-posed, many methods impose a prior over human poses. Unfortunately, these priors admit invalid poses because they do not model how joint limits vary with pose. Here we make two key contributions. First, we collect a motion capture dataset that explores a wide range of human poses. From this we learn a pose-dependent model of joint limits that forms our prior. Both dataset and prior are available for research purposes. Second, we define a general parametrization of body pose and a new, multi-stage, method to estimate 3D pose from 2D joint locations using an over-complete dictionary of poses. Our method shows good generalization while avoiding impossible poses. We quantitatively compare our method with recent work and show state-of-the-art results on 2D to 3D pose estimation using the CMU mocap dataset. We also show superior results using manual annotations on real images and automatic detections on the Leeds sports pose dataset.",
    "topics": [
      "Pose Estimation",
      "Joint Angle Limits",
      "Motion Capture Data",
      "Prior Models",
      "Human Pose Reconstruction"
    ],
    "references": [
      {
        "citation": "[Andriluka, M., Roth, S., & Schiele, B. (2010). Monocular 3D pose estimation and tracking by detection. In Computer Vision and Pattern Recognition, pages 623–630.]"
      },
      {
        "citation": "[Barr`on, C., & Kakadiaris, I. (2001). Estimating anthropometry and pose from a single uncalibrated image. Computer Vision and Image Understanding, 81(3), 269–284.]"
      },
      {
        "citation": "[BenAbdelkader, C., & Yacoob, Y. (2008). Statistical estimation of human anthropometry from a single uncalibrated image. In Methods, Applications, and Challenges in Computer-assisted Criminal Investigations, Studies in Computational Intelligence.]"
      },
      {
        "citation": "[Bourdev, L., & Malik, J. (2009). Poselets: Body part detectors trained using 3D human pose annotations. In International Conference on Computer Vision, pages 1365–1372.]"
      },
      {
        "citation": "[Chen, J., Nie, S., & Ji, Q. (2013). Data-free prior model for upper body pose estimation and tracking. IEEE Transactions on Image Processing, 22(12), 4627–4639.]"
      },
      {
        "citation": "[Guan, P., Weiss, A., Balan, A., & Black, M. J. (2009). Estimating human shape and pose from a single image. In Int. Conf. on Computer Vision, ICCV, pages 1381–1388.]"
      },
      {
        "citation": "[Grochow, K., Martin, S. L., Hertzmann, A., & Popovi´c, Z. (2004). Style-based inverse kinematics. ACM Transactions on Graphics (TOG), 23(3), 522–531.]"
      },
      {
        "citation": "[Hatz, H. (1997). A three-dimensional multivariate model of passive human joint torques and articular boundaries. Clinical Biomechanics, 12(2), 128–135.]"
      },
      {
        "citation": "[Herda, L., Urtasun, R., & Fua, P. (2005). Hierarchical implicit surface joint limits for human body tracking. Computer Vision and Image Understanding, 99(2), 189–209.]"
      },
      {
        "citation": "[Lin, J., Igarashi, T., Mitani, J., Liao, M., & He, Y. (2012). A sketching interface for sitting pose design in the virtual environment. IEEE Transactions on Visualization and Computer Graphics, 18(11), 1979–1991.]"
      }
    ],
    "author_details": [
      {
        "name": "Ijaz Akhter",
        "affiliation": "Max Planck Institute for Intelligent Systems",
        "email": "ijaz.akhter@tuebingen.mpg.de"
      },
      {
        "name": "Michael J. Black",
        "affiliation": "Max Planck Institute for Intelligent Systems",
        "email": "black@tuebingen.mpg.de"
      }
    ]
  },
  {
    "title": "Attributes and Categories for Generic Instance Search from One Example\n---AUTHOR---\nRan Tao\nArnold W.M. Smeulders\nShih-Fu Chang",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Tao_Attributes_and_Categories_2015_CVPR_paper.pdf",
    "id": "Tao_Attributes_and_Categories_2015_CVPR_paper",
    "abstract": "This paper addresses the challenging problem of generic instance search from a single example. Existing instance search methods, which have been successful with planar objects like buildings and logos, struggle when applied to arbitrary 3D objects such as shoes. To overcome this limitation, the authors propose using automatically learned category-specific attributes to handle the large appearance variations inherent in generic instance search. They demonstrate significant performance improvements on a shoe dataset, and extend their methods to search objects without restricting to a specific category, showing that combining category-level information with category-specific attributes outperforms approaches relying on low-level features. The core challenge is representing the query image robustly to appearance variations while maintaining a rich representation for distinction from similar instances.\n\n---TOPICAS---\nGeneric Instance Search\nCategory-Specific Attributes\nAppearance Variation\nAttribute Representation\n3D Object Recognition",
    "topics": [],
    "references": [
      {
        "citation": "[Krizhevsky, A., Sutskever, I., and Hinton, G. (2012). ImageNet classification with deep convolutional neural networks. In *NIPS*.]"
      },
      {
        "citation": "[Large Scale Visual Recognition Challenge. (2010). http://www.imagenet.org/challenges/LSVRC/2010]"
      },
      {
        "citation": "[Akata, Z., Perronnin, F., Harchaoui, Z., and Schmid, C. (2013). Label-embedding for attribute-based classification. In *CVPR*.]"
      },
      {
        "citation": "[Lampert, C. H., Nickisch, H., and Harmeling, S. (2009). Learning to detect unseen object classes by between-class attribute transfer. In *CVPR*.]"
      },
      {
        "citation": "[Aranandjelovic, R., and Zisserman, A. (2012). Multiple queries for large scale specific object retrieval. In *BMVC*.]"
      },
      {
        "citation": "[Naphade, M., Smith, J. R., Tesic, J., Chang, S.-F., Hsu, W., Kennedy, L., Hauptmann, A., and Curtis, J. (2006). Large-scale concept ontology for multimedia. *IEEE MultiMedia*, *13*(3), 86–91.]"
      },
      {
        "citation": "[Perdoch, M., Chum, O., and Matas, J. (2009). Efficient representation of local geometry for large scale object retrieval. In *CVPR*.]"
      },
      {
        "citation": "[Peronnin, F., Liu, Y., S´anchez, J., and Poirier, H. (2010). Large-scale image retrieval with compressed Fisher vectors. In *CVPR*.]"
      },
      {
        "citation": "[Everingham, M., Van Gool, L., Williams, C. K. I., Winn, J., and Zisseramn, A. (2007). The PASCAL Visual Object Classes Challenge 2007 (VOC2007) Results. http://www.pascal-network.org/challenges/VOC/]"
      },
      {
        "citation": "[Farhad, A., Endres, I., Hoiem, D., and Forsyth, D. (2009). Describing objects by their attributes. In *CVPR*.]"
      }
    ],
    "author_details": [
      {
        "name": "Ran Tao",
        "affiliation": "ISLA, Informatics Institute, University of Amsterdam, The Netherlands",
        "email": "[Email not available in the provided text]"
      },
      {
        "name": "Arnold W.M. Smeulders",
        "affiliation": "ISLA, Informatics Institute, University of Amsterdam, The Netherlands",
        "email": "[Email not available in the provided text]"
      },
      {
        "name": "Shih-Fu Chang",
        "affiliation": "Department of Electrical Engineering, Columbia University, USA",
        "email": "[Email not available in the provided text]"
      }
    ]
  },
  {
    "title": "Hypercolumns for Object Segmentation and Fine-grained Localization\n---AUTHOR---\nPablo Arbeláez\nBharath Hariharan\nRoss Girshick\nJitendra Malik",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Hariharan_Hypercolumns_for_Object_2015_CVPR_paper.pdf",
    "id": "Hariharan_Hypercolumns_for_Object_2015_CVPR_paper",
    "abstract": "Recognition algorithms based on convolutional networks (CNNs) typically use the output of the last layer as a feature representation. However, the information in this layer may be too coarse spatially to allow precise localization. On the contrary, earlier layers may be precise in localization but will not capture semantics. To get the best of both worlds, we deﬁne the hypercolumn at a pixel as the vector of activations of all CNN units above that pixel. Using hypercolumns as pixel descriptors, we show results on three fine-grained localization tasks: simultaneous detection and segmentation, keypoint localization, and part labeling, demonstrating improvements over state-of-the-art results.\n\n---TOPICICS---\nConvolutional Neural Networks (CNNs)\nHypercolumn Representation\nFine-grained Localization\nObject Segmentation\nMulti-scale Feature Integration",
    "topics": [],
    "references": [
      {
        "citation": "[Hariharan, B., Arbeláez, P., Girshick, R., & Malik, J. (2014). Simultaneous detection and segmentation. In ECCV.]"
      },
      {
        "citation": "[Arbeláez, P., Pont-Tuset, J., Barron, J., Marques, F., & Malik, J. (2014). Multiscale combinatorial grouping. In CVPR.]"
      },
      {
        "citation": "[He, K., Zhang, X., Ren, S., & Sun, J. (2014). Spatial pyramid pooling in deep convolutional networks for visual recognition. In ECCV.]"
      },
      {
        "citation": "[Barron, J. T., Arbeláez, P., Keränen, S. V. E., Biggin, M. D., Knowles, D. W., & Malik, J. (2013). Volumetric semantic segmentation using pyramid context features. ICCV.]"
      },
      {
        "citation": "[Hubel, D. H., & Wiesel, T. N. (1962). Receptive fields, binocular interaction and functional architecture in the cat’s visual cortex. The Journal of physiology, 160(1).]"
      },
      {
        "citation": "[Bo, Y., & Fowlkes, C. C. (2011). Shape-based pedestrian parsing. In CVPR.]"
      },
      {
        "citation": "[Ionescu, C., Carreira, J., & Sminchisescu, C. (2014). Iterated second-order label sensitive pooling for 3d human pose estimation. In CVPR.]"
      },
      {
        "citation": "[Jones, D. G., & Malik, J. (1992). Determining three-dimensional shape from orientation and spatial frequency disparities. In ECCV.]"
      },
      {
        "citation": "[Koenderink, J. J., & van Doorn, A. J. (1987). Representation of local geometry in the visual system. Biological cybernetics, 55(6).]"
      },
      {
        "citation": "[Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). ImageNet classification with deep convolutional neural networks. In NIPS.]"
      }
    ],
    "author_details": [
      {
        "name": "Pablo Arbeláez",
        "affiliation": "Universidad de los Andes",
        "email": "pa.arbelaez@uniandes.edu.co"
      },
      {
        "name": "Bharath Hariharan",
        "affiliation": "University of California, Berkeley",
        "email": "bharath2@eecs.berkeley.edu"
      },
      {
        "name": "Ross Girshick",
        "affiliation": "Microsoft Research",
        "email": "rbg@microsoft.com"
      },
      {
        "name": "Jitendra Malik",
        "affiliation": "University of California, Berkeley",
        "email": "malik@eecs.berkeley.edu"
      }
    ]
  },
  {
    "title": "DeepShape: Deep Learned Shape Descriptor for 3D Shape Matching and Retrieval\n---AUTHOR---\nJin Xie\n---AUTHOR---\nYi Fang\n---AUTHOR---\nFan Zhu\n---AUTHOR---\nEdward Wong",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Xie_DeepShape_Deep_Learned_2015_CVPR_paper.pdf",
    "id": "Xie_DeepShape_Deep_Learned_2015_CVPR_paper",
    "abstract": "Complex geometric structural variations of 3D model usually pose great challenges in 3D shape matching and retrieval. In this paper, we propose a high-level shape feature learning scheme to extract features that are insensitive to deformations via a novel discriminative deep auto-encoder. First, a multiscale shape distribution is developed for use as input to the auto-encoder. Then, by imposing the Fisher discrimination criterion on the neurons in the hidden layer, we developed a novel discriminative deep auto-encoder for shape feature learning. Finally, the neurons in the hidden layers from multiple discriminative auto-encoders are concatenated to form a shape descriptor for 3D shape matching and retrieval. The proposed method is evaluated on the representative datasets that contain 3D models with large geometric variations, i.e., Mcgill and SHREC’10 ShapeGoogle datasets. Experimental results on the benchmark datasets demonstrate the effectiveness of the proposed method for 3D shape matching and retrieval.\n\n---TOPICICS---\n3D Shape Matching\nShape Retrieval\nDeep Auto-encoders\nGeometric Feature Learning\nIsometric Transformations",
    "topics": [],
    "references": [
      {
        "citation": "[Agathos, A., Pratikakis, I., Papadakis, P., Perantonis, S. J., & Azariadis, P. N. (2009). Retrieval of 3D articulated objects using a graph-based representation. In Eurographics Workshop on 3D Object Retrieval (pp. 29–36).]"
      },
      {
        "citation": "[Assfalg, J., Bertini, M., Bimbo, A. D., & Pala, P. (2007). Content-based retrieval of 3D objects using spin image signatures. IEEE Transactions on Multimedia, 9(3), 589–599.]"
      },
      {
        "citation": "[Belongie, S., Malik, J., & Puzicha, J. (2000). Shape context: A new descriptor for shape matching and object recognition. Advances in Neural Information Processing Systems, 831–837.]"
      },
      {
        "citation": "[Bengio, Y. (2009). Learning deep architectures for AI. Foundations and Trends in Machine Learning, 2(1), 1–127.]"
      },
      {
        "citation": "[Bronstein, A. M., Bronstein, M. M., Guibas, L. J., & Ovsjanikov, M. (2011). Shape google: Geometric words and expressions for invariant shape retrieval. ACM Transactions on Graphics, 30(1), 1.]"
      },
      {
        "citation": "[Bronstein, A. M., Bronstein, M. M., & Kimmel, R. (2006). Efficient computation of isometry-invariant distances between surfaces. SIAM Journal on Scientific Computing, 28(9), 1812–1836.]"
      },
      {
        "citation": "[Bronstein, A. M., Bronstein, M. M., Kimmel, R., Mahmoudi, M., & Sapire, G. (2010). A Gromov-Hausdorff framework with diffusion geometry for topologically-robust non-rigid shape matching. International Journal of Computer Vision, 89(3), 266–286.]"
      },
      {
        "citation": "[Chen, D.-Y., Tian, X.-P., Shen, Y.-T., & Ouhyoung, M. (2003). On visual similarity based 3D model retrieval. Computer Graphics Forum, 22(3), 223–232.]"
      },
      {
        "citation": "[Chen, X., Golovinskiy, A., & Funkhouser, T. (2009). A benchmark for 3D mesh segmentation. ACM Transactions on Graphics.]"
      },
      {
        "citation": "[De Goes, F., Goldenstein, S., & Velho, L. (2008). A hierarchical segmentation of articulated bodies. Computer Graphics Forum, 27(7), 1349–1356.]"
      }
    ],
    "author_details": [
      {
        "name": "Jin Xie",
        "affiliation": "Department of Electrical and Computer Engineering, New York University Abu Dhabi",
        "email": "jin.xie@nyu.edu"
      },
      {
        "name": "Yi Fang",
        "affiliation": "Department of Electrical and Computer Engineering, New York University Abu Dhabi",
        "email": "yfang@nyu.edu"
      },
      {
        "name": "Fan Zhu",
        "affiliation": "Department of Electrical and Computer Engineering, New York University Abu Dhabi",
        "email": "fan.zhu@nyu.edu"
      },
      {
        "name": "Edward Wong",
        "affiliation": "Polytechnic School of Engineering, New York University",
        "email": "ewong@nyu.edu"
      }
    ]
  },
  {
    "title": "Motion Part Regularization: Improving Action Recognition via Trajectory Group Selection\n---AUTHORs---\nBingbing Ni\nPierre Moulin\nXiaokang Yang\nShuicheng Yan",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Ni_Motion_Part_Regularization_2015_CVPR_paper.pdf",
    "id": "Ni_Motion_Part_Regularization_2015_CVPR_paper",
    "abstract": "Action label: golf \n\nAction label: punch \n\nOriginal Fisher vector \n\ngolf \n\npunch \n\nThe paper introduces a Motion Part Regularization framework to improve action recognition by mining for discriminative groups of dense trajectories that form important motion parts. The framework generates discriminativeness weighted Fisher vector representation, which is more discriminative than the unweighted traditional Fisher vector. The approach involves generating motion part candidates, formulating an objective function that encourages sparse selection of trajectory groups and an action class discriminative term, and utilizing an optimization algorithm with auxiliary variables to learn discriminative weights for each motion part. The proposed method achieves state-of-the-art performance on several action recognition benchmarks.\n\n---TOPICCS---\nAction Recognition\nDense Trajectories\nMotion Part Regularization\nFisher Vector Representation\nSpatio-Temporal Grouping",
    "topics": [],
    "references": [
      {
        "citation": "[C.-C. Chang and C.-J. Lin, LIBSVM: A library for support vector machines, ACM TIST, 2011]"
      },
      {
        "citation": "[H. Wang, A. Kl¨aser, C. Schmid, and L. Cheng-Lin, Action recognition by dense trajectories, CVPR, 2011]"
      },
      {
        "citation": "[P. Dollar, V. Rabaud, G. Cottrell, and S. Belongie, Behavior recognition via sparse spatiotemporal features, VS-PETS, 2005]"
      },
      {
        "citation": "[H. Wang, A. Kl¨aser, C. Schmid, and C.-L. Liu, Dense trajectories and motion boundary descriptors for action recognition, IJCV, 2013]"
      },
      {
        "citation": "[O. Duchenne, I. Laptev, J. Sivic, F. Bach, and J. Ponce, Automatic annotation of human actions in video, ICCV, 2009]"
      },
      {
        "citation": "[H. Wang and C. Schmid, Action recognition with improved trajectories, ICCV, 2013]"
      },
      {
        "citation": "[P. F. Felzenszwalb, R. B. Girshick, D. McAlleser, and D. Ramanan, Object detection with discriminatively trained part based models, IEEE T-PAMI, 2010]"
      },
      {
        "citation": "[J. Wang, Z. Liu, Y. Wu, and J. Yuan, Mining actionlet ensemble for action recognition with depth cameras, CVPR, 2012]"
      },
      {
        "citation": "[M. Jain, H. Jegou, and P. Bouthemy, Better exploiting motion for better action recognition, CVPR, 2013]"
      },
      {
        "citation": "[Y.-G. Jiang, Q. Dai, X. Xue, W. Liu, and C.-W. Ngo, Trajectory-based modeling of human actions with motion reference points, ECCV, 2012]"
      }
    ],
    "author_details": [
      {
        "name": "Bingbing Ni",
        "affiliation": "ADSC Singapore",
        "email": "bingbing.ni@adsc.com.sg"
      },
      {
        "name": "Pierre Moulin",
        "affiliation": "UIUC USA",
        "email": "moulin@ifp.uiuc.edu"
      },
      {
        "name": "Xiaokang Yang",
        "affiliation": "SJTU China",
        "email": "xkyang@sjtu.edu.cn"
      },
      {
        "name": "Shuicheng Yan",
        "affiliation": "NUS Singapore",
        "email": "eleyans@nus.edu.sg"
      }
    ]
  },
  {
    "title": "Supplementary Material : Robust Large Scale Monocular Visual SLAM\n---AUTHOR---\nGuillaume Bourmaud\n---AUTHOR---\nR\\'emi M\\'egret",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Bourmaud_Robust_Large_Scale_2015_CVPR_supplemental.pdf",
    "id": "Bourmaud_Robust_Large_Scale_2015_CVPR_supplemental",
    "abstract": "In this supplementary material, we provide additional results on several challenging video sequences. We present results on KITTI benchmark sequences 13 and 15, compare our approach against state-of-the-art methods, evaluate performance on a \"perfect circle\" sequence, and provide a qualitative assessment of a large-scale apartment reconstruction.\n\n---TOPICCS---\nVisual SLAM\nMonocular Vision\nLarge-Scale Environments\nTrajectory Estimation\nReconstruction",
    "topics": [],
    "references": [
      {
        "citation": "[Lim, H., Lim, J., & Kim, H. J. \"Real-time 6-dof monocular visual SLAM in a large-scale environment.\" ICRA, 2014.]"
      },
      {
        "citation": "[Zhang, J., & Singh, S. \"Visual-lidar odometry and mapping: Low-drift, robust, and fast.\" Submitted to IEEE International Conference on Robotics and Automation(ICRA), 2015.]"
      },
      {
        "citation": "[Engel, J., Schops, T., & Cremers, D. \"LSD-SLAM: Large-scale direct monocular SLAM.\" ECCV, Lecture Notes in Computer Science, pp. 834–849, 2014.]"
      }
    ],
    "author_details": [
      {
        "name": "Guillaume Bourmaud",
        "affiliation": "Univ. Bordeaux, CNRS, IMS, UMR 5218, F-33400 Talence, France",
        "email": "guillaume.bourmaund@ims-bordeaux.fr"
      },
      {
        "name": "R\\'emi M\\'egret",
        "affiliation": "Univ. Bordeaux, CNRS, IMS, UMR 5218, F-33400 Talence, France",
        "email": "remi.megret@ims-bordeaux.fr"
      }
    ]
  },
  {
    "title": "Realtime Monocular Gaze Correction Using Machine Learning\n---AUTHOR---\nDaniil Kononenko\nVictor Lempitsky",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Kononenko_Learning_To_Look_2015_CVPR_paper.pdf",
    "id": "Kononenko_Learning_To_Look_2015_CVPR_paper",
    "abstract": "Most gaze correction solutions rely on additional hardware, limiting their applicability. This paper addresses the challenge of creating a purely monocular gaze correction system using supervised machine learning. The system learns to synthesize images with altered gaze direction from pairs of images, enabling redirection of gaze for previously unseen individuals. The system is computationally efficient, running in real-time on a laptop, and avoids the uncanny valley effect through localized pixel replacement operations around the eyes. The paper demonstrates the system's performance with qualitative and quantitative evaluations.\n\n---TOPICCS---\nMonocular Gaze Correction\nMachine Learning\nReal-time Video Processing\nComputer Vision\nUncanny Valley Effect",
    "topics": [],
    "references": [
      {
        "citation": "[Amit, Y., & Geman, D. (2013). Shape quantization and recognition from single depth images. IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(12), 2821–2840.]"
      },
      {
        "citation": "[Breiman, L. (2001). Random forests. Machine Learning, 45(1), 5–32.]"
      },
      {
        "citation": "[Dollár, P., & Zitnick, C. L. (2013). Structured forests for fast edge detection. IEEE International Conference on Computer Vision (ICCV), 1841–1848.]"
      },
      {
        "citation": "[Fanelli, G., Dantone, M., Gall, J., Fossati, A., & Gool, L. J. V. (2013). Random forests for real time 3d face analysis. International Journal of Computer Vision, 101(3), 1–17.]"
      },
      {
        "citation": "[Giger, D., Bazin, J.-C., Kuster, C., Popa, T., & Gross, M. (2014). Gaze correction with a single webcam. IEEE International Conference on Multimedia & Expo.]"
      },
      {
        "citation": "[Gall, J., & Lempitsky, V. S. (2009). Class-specific hough forests for object detection. Computer Vision and Pattern Recognition (CVPR), 1022–1029.]"
      },
      {
        "citation": "[Jones, A., Lang, M., Fyffe, G., Yu, X., Busch, J., McDowall, I., Bolas, M. T., & Debevec, P. E. (2009). Achieving eye contact in a one-to-many 3D video teleconferencing system. ACM Transactions on Graphics, 28(3).]"
      },
      {
        "citation": "[Kazemi, V., & Sullivan, J. (2014). One milliseccond face alignment with an ensemble of regression trees. Computer Vision and Pattern Recognition (CVPR), 1867–1874.]"
      },
      {
        "citation": "[Kuster, C., Popa, T., Bazin, J.-C., Gotsman, C., & Gross, M. (2012). Gaze correction for home video conferencing. ACM, 174.]"
      },
      {
        "citation": "[Ren, S., Cao, X., Wei, Y., & 0001, J. S. (2014). Face alignment at 3000 fps via regressing local binary features. Computer Vision and Pattern Recognition (CVPR), 1685–1692.]"
      }
    ],
    "author_details": [
      {
        "name": "Daniil Kononenko",
        "affiliation": "Skolkovo Institute of Science and Technology (Skoltech)",
        "email": "daniil.kononenko@skoltech.ru"
      },
      {
        "name": "Victor Lempitsky",
        "affiliation": "Skolkovo Institute of Science and Technology (Skoltech)",
        "email": "lempitsky@skoltech.ru"
      }
    ]
  },
  {
    "title": "Joint Patch and Multi-label Learning for Facial Action Unit Detection\n---AUTHOR---\nKaili Zhao\nWen-Sheng Chu\nFernando De la Torre\nJeffrey F. Cohn\nHonggang Zhang",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Zhao_Joint_Patch_and_2015_CVPR_paper.pdf",
    "id": "Zhao_Joint_Patch_and_2015_CVPR_paper",
    "abstract": "The Facial Action Coding System (FACS) is a comprehensive system for describing facial movements using Action Units (AUs). Manual FACS coding is labor-intensive, motivating research into automatic AU detection. Most existing methods treat AU detection using one-vs-all classifiers and fail to exploit dependencies among AUs and facial features. This paper introduces joint-patch and multi-label learning (JPML) to address these issues. JPML leverages group sparsity by selecting a sparse subset of facial patches while learning a multi-label classifier. Experiments on three diverse datasets (CK+, GFT, and BP4D) demonstrate that JPML produces the highest average F1 scores in comparison with state-of-the-art methods.\n\n---TOPICCS---\nFacial Action Coding System (FACS)\nAction Unit (AU) Detection\nPatch Learning\nMulti-label Learning\nJoint Patch and Multi-label Learning (JPML)",
    "topics": [],
    "references": [
      {
        "citation": "[M. S. Bartlett, G. Littlewort, C. Lainscsek, I. Fasel, and J. Movellan, Machine learning methods for fully automatic recognition of facial expressions and facial actions. Systems, Man and Cybernetics, 2004.] - *Foundational work on machine learning for facial expression recognition.*"
      },
      {
        "citation": "[S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein, Distributed optimization and statistical learning via the alternating direction method of multipliers. Foundations and Trends® in Machine Learning, 3(1):1–122, 2011.] - *Relevant if the paper utilizes optimization techniques.*"
      },
      {
        "citation": "[J. F. Cohn and F. De La Torre, The oxford handbook of affective computing. Automated Face Analysis for Affective Computing, 2014.] - *Provides context and overview of the field.*"
      },
      {
        "citation": "[F. De la Torre, W.-S. Chu, X. Xiong, X. Ding, and J. F. Cohn, Intraface. AFGR, 2015.] - *Likely a key publication from the authors.*"
      },
      {
        "citation": "[X. Ding, W.-S. Chu, F. De la Torre, J. F. Cohn, and Q. Wang, Facial action unit event detection by cascade of tasks. ICCV, 2013.] - *Important work on facial action unit detection.*"
      },
      {
        "citation": "[P. Ekman, W. Friesen, and J. C. Hager, Facial action coding system. A Human Face, 2002.] - *Fundamental reference for facial action coding.*"
      },
      {
        "citation": "[W.-S. Chu, F. De la Torre, and J. F. Cohn, Selective transfer machine for personalized facial action unit detection. CVPR, 2013.] - *Addresses personalization in facial action unit detection.*"
      },
      {
        "citation": "[L. A. Jeni, J. F. Cohn, and F. De La Torre, Facing imbalanced data–recommendations for the use of performance metrics. Affective Computing and Intelligent Interaction, 2013.] - *Important if the paper deals with imbalanced datasets.*"
      },
      {
        "citation": "[Y. Li, J. Chen, Y. Zhao, and Q. Ji, Data-free prior model for facial action unit recognition. IEEE Transactions on Affective Computing, 4(2):127–141, April 2013.] - *Relevant if the paper uses a data-free approach.*"
      },
      {
        "citation": "[G. Littlewort, M. S. Bartlett, I. Fasel, J. Susskind, and J. Movellan, Dynamics of facial expression extracted au-cascadies with bidirectional bootstrapping for action unit detection in spontaneous facial behavior. IEEE Transactions on Affective Computing, 2(2):79–91, 2011.] - *Focuses on dynamic facial expressions and action unit detection.*"
      }
    ],
    "author_details": [
      {
        "name": "Kaili Zhao",
        "affiliation": "School of Comm. and Info. Engineering, Beijing University of Posts and Telecom., Beijing China",
        "email": "[Email not available]"
      },
      {
        "name": "Wen-Sheng Chu",
        "affiliation": "Robotics Institute, Carnegie Mellon University, Pittsburgh, PA 15213",
        "email": "[Email not available]"
      },
      {
        "name": "Fernando De la Torre",
        "affiliation": "Robotics Institute, Carnegie Mellon University, Pittsburgh, PA 15213",
        "email": "[Email not available]"
      },
      {
        "name": "Jeffrey F. Cohn",
        "affiliation": "Robotics Institute, Carnegie Mellon University, Pittsburgh, PA 15213",
        "email": "[Email not available]"
      },
      {
        "name": "Honggang Zhang",
        "affiliation": "School of Comm. and Info. Engineering, Beijing University of Posts and Telecom., Beijing China",
        "email": "[Email not available]"
      }
    ]
  },
  {
    "title": "TVSum: Summarizing Web Videos Using Titles\n---AUTHORNS---\nYale Song\nJordi Vallmitjana\nAmanda Stent\nAlejandro Jaimes",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Song_TVSum_Summarizing_Web_2015_CVPR_paper.pdf",
    "id": "Song_TVSum_Summarizing_Web_2015_CVPR_paper",
    "abstract": "Video summarization is a challenging problem because identifying important video segments requires prior knowledge of the main topic. This paper introduces TVSum, an unsupervised video summarization framework that uses title-based image search results to find visually important shots. The authors observe that video titles are often carefully chosen to be descriptive and use this to guide image search. A novel co-archetypal analysis technique is developed to learn canonical visual concepts shared between video and images, dealing with noise and variance in image search results. A new benchmark dataset, TVSum50, is also introduced. Experimental results demonstrate that TVSum produces superior quality summaries compared to existing approaches.",
    "topics": [
      "Video Summarization",
      "Unsupervised Learning",
      "Title-Based Image Search",
      "Co-Archetypal Analysis",
      "Canonical Visual Concepts"
    ],
    "references": [
      {
        "citation": "[M. Basseville, I. V. Nikiforov, et al. Detection of abrupt changes: theory and application, volume 104. Prentice Hall Englewood Cliffs, 1993.]"
      },
      {
        "citation": "[A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse problems. SIAM Journal on Imaging Sciences, 2(1), 2009.]"
      },
      {
        "citation": "[K. Bleakley and J.-P. Vert. The group fused lasso for multiple change-point detection. arXiv preprint arXiv:1106.4199, 2011.]"
      },
      {
        "citation": "[Y. Chen, J. Mairal, and Z. Harchaoui. Fast and robust archetypal analysis for representation learning. In CVPR, 2014.]"
      },
      {
        "citation": "[S. Fidler, A. Sharma, and R. Urtasun. A sentence is worth a thousand pixels. In CVPR, 2013.]"
      },
      {
        "citation": "[M. Gygli, H. Grabner, H. Riemenschneider, and L. V. Gool. Creating summaries from user videos. In ECCV, 2014.]"
      },
      {
        "citation": "[Y. Jia, J. T. Abbott, J. Austerweil, T. Griffths, and T. Darrell. Visual concept learning: Combining machine vision and bayesian generalization on concept hierarchies. In NIPS, 2013.]"
      },
      {
        "citation": "[Y. J. Lee, J. Ghosh, and K. Grauman. Discovering important people and objects for egocentric video summarization. In CVPR, 2012.]"
      },
      {
        "citation": "[L. Li, K. Zhou, G.-R. Xue, H. Zha, and Y. Yu. Video summarization via transferrable structured learning. In WWW, 2011.]"
      },
      {
        "citation": "[D. Lin, S. Fidler, C. Kong, and R. Urtasun. Visual semantic search: Retrieving videos via complex textual queries. In CVPR, 2014.]"
      }
    ],
    "author_details": [
      {
        "name": "Yale Song",
        "affiliation": "Yahoo Labs",
        "email": "yalesong@yahoo-inc.com"
      },
      {
        "name": "Jordi Vallmitjana",
        "affiliation": "Yahoo Labs",
        "email": "jvallmi@yahoo-inc.com"
      },
      {
        "name": "Amanda Stent",
        "affiliation": "Yahoo Labs",
        "email": "stent@yahoo-inc.com"
      },
      {
        "name": "Alejandro Jaimes",
        "affiliation": "Yahoo Labs",
        "email": "ajaimes@yahoo-inc.com"
      }
    ]
  },
  {
    "title": "The Application of Two-level Attention Models in Deep Convolutional Neural Network for Fine-grained Image Classiﬁcation\n---AUTHOR---\nTianjun Xiao\nYichong Xu\nKuiyuan Yang\nJiaxing Zhang\nYuxin Peng\nZheng Zhang",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Xiao_The_Application_of_2015_CVPR_paper.pdf",
    "id": "Xiao_The_Application_of_2015_CVPR_paper",
    "abstract": "Fine-grained classification is challenging due to subtle differences between categories and variations in pose, scale, or rotation. This paper proposes a method applying visual attention to fine-grained classification using deep neural networks. The pipeline integrates bottom-up attention for candidate patches, object-level top-down attention for relevant patch selection, and part-level top-down attention for localizing discriminative parts. The approach avoids expensive annotations and achieves significant improvements under weak supervision, demonstrating competitive performance against methods relying on additional annotations.\n\n---TOPICCS---\nFine-grained image classification\nVisual attention models\nDeep convolutional neural networks\nWeak supervision\nObject part localization",
    "topics": [],
    "references": [],
    "author_details": [
      {
        "name": "Tianjun Xiao",
        "affiliation": "Institute of Computer Science and Technology, Peking University",
        "email": "xiaotianjun@pku.edu.cn"
      },
      {
        "name": "Yichong Xu",
        "affiliation": "Microsoft Research, Beijing",
        "email": "xycking@163.com"
      },
      {
        "name": "Kuiyuan Yang",
        "affiliation": "Microsoft Research, Beijing",
        "email": "kuyang@microsoft.com"
      },
      {
        "name": "Jiaxing Zhang",
        "affiliation": "Microsoft Research, Beijing",
        "email": "jiazx@microsoft.com"
      },
      {
        "name": "Yuxin Peng",
        "affiliation": "Institute of Computer Science and Technology, Peking University",
        "email": "pengyuxin@pku.edu.cn"
      },
      {
        "name": "Zheng Zhang",
        "affiliation": "New York University Shanghai",
        "email": "zz@nyu.edu"
      }
    ]
  },
  {
    "title": "Geodesic Exponential Kernels: When Curvature and Linearity Conﬂict\n---AUTHOR---\nAasa Feragen\nFrançois Lauze\nSøren Hauberg",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Feragen_Geodesic_Exponential_Kernels_2015_CVPR_paper.pdf",
    "id": "Feragen_Geodesic_Exponential_Kernels_2015_CVPR_paper",
    "abstract": "We consider kernel methods on general geodesic metric spaces and provide both negative and positive results. First we show that the common Gaussian kernel can only be generalized to a positive deﬁnite kernel on a geodesic metric space if the space is ﬂat. As a result, for data on a Riemannian manifold, the geodesic Gaussian kernel is positive deﬁnite for all λ > 0 if and only if the Riemannian manifold is Euclidean. This implies that any attempt to design geodesic Gaussian kernels on curved Riemannian manifolds is futile. However, we show that for spaces with conditionally negative deﬁnite distances the geodesic Laplacian kernel can be generalized while retaining positive deﬁniteness. This implies that geodesic Laplacian kernels can be generalized to some curved spaces, including spheres and hyperbolic spaces. Our theoretical results are veriﬁed empirically.\n\n---TOPICICS---\nGeodesic metric spaces\nKernel methods\nRiemannian manifolds\nGaussian kernels\nLaplacian kernels",
    "topics": [],
    "references": [
      {
        "citation": "[M. Alamgir and U. von Luxburg. Shortest path distance in random k-nearest neighbor graphs. Proceedings of the 29th International Conference on Machine Learning, ICML, 2012.]"
      },
      {
        "citation": "[N. Dalal and B. Triggs. Histograms of oriented gradients for human detection. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 886–893, 2005.]"
      },
      {
        "citation": "[S. Amarí and H. Nagaoka. Methods of information geometry. American Mathematical Society, 2000.]"
      },
      {
        "citation": "[V. Arsigny, P. Fillard, X. Pennec, and N. Ayache. Fast and simple calculus on tensors in the log-Euclidean framework. Medical Image Computing and Computer-Assisted Intervention (MICCAI), 115–122, 2005.]"
      },
      {
        "citation": "[A. Feragen, S. Haugberg, M. Nielsen, and F. Lauze. Means in spaces of tree-like shapes. IEEE International Conference on Computer Vision (ICCV), 736–746, 2011.]"
      },
      {
        "citation": "[A. Feragen, N. Kasenburg, J. Petersen, M. de Bruijne, and K. Borgwardt. Scalable kernels for graphs with continuous attributes. Advances in Neural Information Processing Systems (NIPS), 216–224, 2013.]"
      },
      {
        "citation": "[C. Atkinson and A. F. Mitchell. Rao’s distance measure. Sankhy¯a: The Indian Journal of Statistics, pages 345–365, 1981.]"
      },
      {
        "citation": "[A. V. B. Bekka and P. de la Harpe. Kazhdan’s Property (T). New Mathematical Monographs, 2008.]"
      },
      {
        "citation": "[M. Bridson and A. Haeﬂiger. Metric spaces of non-positive curvature. Springer, 1999.]"
      },
      {
        "citation": "[A. M. Bronstein, M. M. Bronstein, R. Kimmel, M. Mahmoudi, and G. Sapiro. A Gromov-Hausdorff framework with diffusion geometry for topologically-robust non-rigid shape matching. International Journal of Computer Vision, 266–286, 2010.]"
      }
    ],
    "author_details": [
      {
        "name": "Aasa Feragen",
        "affiliation": "DIKU, University of Copenhagen",
        "email": "aasa@diku.dk"
      },
      {
        "name": "François Lauze",
        "affiliation": "DIKU, University of Copenhagen",
        "email": "francois@diku.dk"
      },
      {
        "name": "Søren Hauberg",
        "affiliation": "DTU Compute",
        "email": "sohau@dtu.dk"
      }
    ]
  },
  {
    "title": "ℓ0TV: A New Method for Image Restoration in the Presence of Impulse Noise\n---AUTHORs---\nGanzhao Yuan\nBernard Ghanem",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Yuan_L0TV_A_New_2015_CVPR_paper.pdf",
    "id": "Yuan_L0TV_A_New_2015_CVPR_paper",
    "abstract": "This paper addresses the problem of image restoration in the presence of impulse noise. The authors propose a new method, called ℓ0TV -PADMM, which solves the TV-based restoration problem with ℓ0-norm data fidelity. They reformulate the resulting non-convex non-smooth optimization problem as an equivalent MPEC (Mathematical Program with Equilibrium Constraints) and solve it using a proximal Alternating Direction Method of Multipliers (PADMM). The proposed method finds a desirable solution and is proven to be convergent under mild conditions, outperforming state-of-the-art image restoration methods in experiments.",
    "topics": [
      "Image Restoration",
      "Impulse Noise",
      "Total Variation (TV)",
      "ℓ0-norm data fidelity",
      "PADMM (Proximal Alternating Direction Method of Multipliers)"
    ],
    "references": [],
    "author_details": [
      {
        "name": "Ganzhao Yuan",
        "affiliation": "South China University of Technology (SCUT), P.R. China",
        "email": "yuan Ganzhao@gmail.com"
      },
      {
        "name": "Bernard Ghanem",
        "affiliation": "King Abdullah University of Science and Technology (KAUST), Saudi Arabia",
        "email": "bernard.ghanem@kaust.edu.sa"
      }
    ]
  },
  {
    "title": "An Improved Deep Learning Architecture for Person Re-Identiﬁcation\n---AUTHOR---\nEjaz Ahmed\nMichael Jones\nTim K. Marks",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Ahmed_An_Improved_Deep_2015_CVPR_paper.pdf",
    "id": "Ahmed_An_Improved_Deep_2015_CVPR_paper",
    "abstract": "In this work, we propose a method for simultaneously learning features and a corresponding similarity metric for person re-identiﬁcation. We present a deep convolutional architecture with layers specially designed to address the problem of re-identiﬁcation. Given a pair of images as input, our network outputs a similarity value indicating whether the two input images depict the same person. Novel elements of our architecture include a layer that computes cross-input neighborhood differences, which capture local relationships between the two input images based on mid-level features from each input image. A high-level summary of the outputs of this layer is computed by a layer of patch summary features, which are then spatially integrated in subsequent layers. Our method significantly outperforms the state of the art on both a large data set (CUHK03) and a medium-sized data set (CUHK01), and is resistant to over-fitting. We also demonstrate that by initially training on an unrelated large data set before fine-tuning on a small target data set, our network can achieve results comparable to the state of the art even on a small data set (VIPeR).",
    "topics": [
      "Person Re-Identification",
      "Deep Convolutional Architecture",
      "Similarity Metric Learning",
      "Neighborhood Difference Layer",
      "Metric Learning"
    ],
    "references": [
      {
        "citation": "[Li, W., & Wang, X. (2013). Locally aligned feature transforms across views. In *CVPR*.]"
      },
      {
        "citation": "[Bak, S., Corvee, E., Bremond, F., & Thonnat, M. (2011). Multiple-shot human re-identiﬁcation by mean riemannian covariance grid. In *AVSS*.]"
      },
      {
        "citation": "[Li, W., Zhao, R., & Wang, X. (2012). Human re-identiﬁcation with transferred metric learning. In *ACCV*.]"
      },
      {
        "citation": "[Li, W., Zhao, R., Xiao, T., & Wang, X. (2014). Deepreid: Deep ﬁlter pairing neural network for person re-identiﬁcation. In *CVPR*.]"
      },
      {
        "citation": "[Li, Z., Chang, S., Liang, F., Huang, T., Cao, L., & Smith, J. (2013). Learning locally-adaptive decision functions for person ver-iﬁcation. In *CVPR*.]"
      },
      {
        "citation": "[Bazzani, L., Crisan, M., Perina, A., & Murino, V. (2012). Multiple-shot person re-identiﬁcation by chromatic and epitomic analyses. *Pattern Recognition Letters*, 33(7):898–903.]"
      },
      {
        "citation": "[Bottou, L. (2012). Stochastic gradient tricks. In *Neural Networks, Tricks of the Trade, Reloaded*. Springer.]"
      },
      {
        "citation": "[Davis, J. V., Kulis, B., Jain, P., Sra, S., & Dhillon, I. S. (2007). Information-theoretic metric learning. In *ICML*.]"
      },
      {
        "citation": "[Farenzena, M., Bazzani, L., Perina, A., Murino, V., & Crisan, M. (2010). Person re-identiﬁcation by symmetry-driven ac-cumulation of local features. In *CVPR*.]"
      },
      {
        "citation": "[Felzenszwalb, P. F., Girshick, R. B., McAllester, D., & Ramanan, D. (2010). Object detection with discriminatively trained part-based models. *IEEE Trans. Pattern Anal. Mach. Intell*, 32(9):1627–1645.]"
      }
    ],
    "author_details": [
      {
        "name": "Ejaz Ahmed",
        "affiliation": "University of Maryland",
        "email": "ejaz@umd.edu"
      },
      {
        "name": "Michael Jones",
        "affiliation": "Mitsubishi Electric Research Labs",
        "email": "{mjones}@merl.com"
      },
      {
        "name": "Tim K. Marks",
        "affiliation": "Mitsubishi Electric Research Labs",
        "email": "{tmarks}@merl.com"
      }
    ]
  },
  {
    "title": "BOLD - Binary Online Learned Descriptor For Efﬁcient Image Matching\n---AUTHOR---\nVassileios Balntas\nLilian Tang\nKrystian Mikolajczyk",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Balntas_BOLD_-_Binary_2015_CVPR_paper.pdf",
    "id": "Balntas_BOLD_-_Binary_2015_CVPR_paper",
    "abstract": "This paper introduces BOLD (Binary Online Learned Descriptor), a novel approach for generating binary descriptors optimized for each image patch independently. Inspired by linear discriminant embedding, the method establishes discriminative and uncorrelated binary tests offline and then efficiently builds patch-adapted descriptors online, leading to lower intra-class distances and a more robust descriptor. The descriptor consists of two binary strings, one representing test results and the other indicating a subset of robust tests used for masked Hamming distance calculation. Experiments on three benchmarks demonstrate performance improvements, highlighting the benefits of per-patch optimization over global optimization.\n\n---TOPSICS---\nBinary descriptors\nDiscriminant embedding\nOnline descriptor optimization\nImage matching\nPer-patch adaptation",
    "topics": [],
    "references": [
      {
        "citation": "[D. G. Lowe. Distinctive image features from scale-invariant keypoints. IJCV, 60:91–110, 2004.] - This is a foundational work on SIFT features, frequently referenced."
      },
      {
        "citation": "[K. Mikolajczyk and C. Schmid. A performance evaluation of local descriptors. IEEE TPAMI, 27(10):1615–1630, 2005.] - Provides a benchmark and comparison of various local descriptors."
      },
      {
        "citation": "[H. Bay, T. Tuytelaars, and L. V. Gool. Surf: Speeded up robust features. In ECCV, 2006.] - Introduces the SURF descriptor."
      },
      {
        "citation": "[G. H. M. Brown and S. Winder. Discrimina-tive learning of local image descriptors. IEEE TPAMI, 33(1):43–57, 2010.] - Explores discriminative learning for local descriptors."
      },
      {
        "citation": "[K. Mikolajczyk and C. Schmid. Scale and afﬁne invariant interest point detectors. IJCV, 60:(1), 63–86, 2004.] - Addresses interest point detection."
      },
      {
        "citation": "[S. Hare, A. Saffari, and P. Torr. Struck: Structured output tracking with kernels. In ICCV, 2011.] - Relevant for tracking applications."
      },
      {
        "citation": "[Z. Kalal, K. Mikolajczyk, and J. Matas. Tracking-learning-detection. IEEE TPAMI, 60:91–110, 2004.] - Discusses the integration of tracking, learning, and detection."
      },
      {
        "citation": "[M. Ozuysal, M. Calonder, V. Lepetit, and P. Fua. Fast keypoint recognition using random ferns. IEEE TPAMI, 32(3):448–461, March 2010.] - Introduces a fast keypoint recognition method."
      },
      {
        "citation": "[E. Rublee, V. Rabaud, K. Konolige, and G. Bradski. Orb: An efﬁcient alternative to sift or surf. In ICCV, 2011.] - Presents ORB as an efficient alternative to SIFT and SURF."
      },
      {
        "citation": "[V. L. T. Trzcinski, M. Chris-toudias and P. Fua. Boosting Binary Keypoint Descriptors. In CVPR, 2013.] - Focuses on binary keypoint descriptors."
      }
    ],
    "author_details": [
      {
        "name": "Vassileios Balntas",
        "affiliation": "University of Surrey, UK",
        "email": "v.balntas@surrey.ac.uk"
      },
      {
        "name": "Lilian Tang",
        "affiliation": "University of Surrey, UK",
        "email": "h.tang@surrey.ac.uk"
      },
      {
        "name": "Krystian Mikolajczyk",
        "affiliation": "University of Surrey, UK",
        "email": "k.mikolajczyk@surrey.ac.uk"
      }
    ]
  },
  {
    "title": "Deep Multiple Instance Learning for Image Classiﬁcation and Auto-Annotation\n---AUTHOR---\nJiajun Wu\n---AUTHOR---\nYinan Yu\n---AUTHOR---\nChang Huang\n---AUTHOR---\nKai Yu",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Wu_Deep_Multiple_Instance_2015_CVPR_paper.pdf",
    "id": "Wu_Deep_Multiple_Instance_2015_CVPR_paper",
    "abstract": "Deep learning has achieved tremendous improvements in visual recognition with full supervision. However, research on learning deep representations with weak supervision is still in its early stage. This paper attempts to model deep learning in a weakly supervised learning (multiple instance learning) framework. The authors leverage the observation that object proposals and possible text annotations in images can be regarded as two instance sets, following a dual multiple-instance assumption. They design systems to exploit this MIL property with deep learning strategies and jointly learn the relationship between object and annotation proposals. Experiments demonstrate convincing performance in classification and image annotation, and the framework extracts reasonable region-keyword pairs with little supervision.\n\n---TOPIC---\nDeep Learning\n---TOPI---\nMultiple Instance Learning (MIL)\n---TOPI---\nWeakly Supervised Learning\n---TOPI---\nImage Classification\n---TOPI---\nImage Annotation",
    "topics": [],
    "references": [
      {
        "citation": "[Lee, H., Grosse, R., Ranganath, R., & Ng, A. Y. Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations. In ICML, 2009.]"
      },
      {
        "citation": "[Andrews, S., Tsochantaridis, I., & Hofmann, T. Support vector machines for multiple-instance learning. In NIPS, 2002.]"
      },
      {
        "citation": "[Li, J., & Wang, J. Z. Real-time computerized annotation of pictures. IEEE TPAMI, 30(6):985–1002, 2008.]"
      },
      {
        "citation": "[Barnard, K., Duygulü, P., Forsyth, D., De Freitas, N., Blei, D. M., & Jordan, M. I. Matching words and pictures. JMLR, 3:1107–1135, 2003.]"
      },
      {
        "citation": "[Li, L.-J., & Li, F.-F. What, where and who? classifying events by scene and object recognition. In ICCV, 2007.]"
      },
      {
        "citation": "[Li, L.-J., Su, H., Xing, E. P., & Li, F.-F. Object bank: A high-level image representation for scene classification & semantic feature sparsiﬁcation. In NIPS, 2010.]"
      },
      {
        "citation": "[Chen, Q., Song, Z., Hua, Y., Huang, Z., & Yan, S. Hierarchical matching with side information for image classiﬁcation. In CVPR, 2012.]"
      },
      {
        "citation": "[Li, Q., Wu, J., & Tu, Z. Harvesting mid-level visual concepts from large-scale internet images. In CVPR, 2013.]"
      },
      {
        "citation": "[Cheng, M.-M., Zhang, Z., Lin, W.-Y., & Torr, P. Bing: Binarized normed gradients for objectness estimation at 300fps. In CVPR, 2014.]"
      },
      {
        "citation": "[Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., & Fei-Fei, L. Imaginet: A large-scale hierarchical image database. In CVPR, 2009.]"
      }
    ],
    "author_details": [
      {
        "name": "Jiajun Wu",
        "affiliation": "Massachusetts Institute of Technology",
        "email": "[Not available in the provided text]"
      },
      {
        "name": "Yinan Yu",
        "affiliation": "Institute of Deep Learning, Baidu",
        "email": "[Not available in the provided text]"
      },
      {
        "name": "Chang Huang",
        "affiliation": "Institute of Deep Learning, Baidu",
        "email": "[Not available in the provided text]"
      },
      {
        "name": "Kai Yu",
        "affiliation": "Institute of Deep Learning, Baidu",
        "email": "[Not available in the provided text]"
      }
    ]
  },
  {
    "title": "Weakly Supervised Localization of Novel Objects Using Appearance Transfer\n---AUTHOR---\nMrigank Rochan\nYang Wang",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Rochan_Weakly_Supervised_Localization_2015_CVPR_paper.pdf",
    "id": "Rochan_Weakly_Supervised_Localization_2015_CVPR_paper",
    "abstract": "Although it is difﬁcult to collect training images anno-\ntated with object bounding boxes, it is usually much easier\nto collect weakly labeled data, where labels are only given\nat the image level. For example, many online data (Flickr\nimages, YouTube videos) might come with user-generated\ntags describing the objects present in the images/videos. It\nis also possible to collect weakly labeled images of an ob-\nject class via image search. In this paper, our goal is to\ndevelop techniques to localize the object in weakly labeled\ndata. Given a collection of images labeled with an object\ncategory (e.g. “car”), our method will output the\nbounding box of this object in each image. Our method can\nalso be applied in videos. In this case, we are given one\nsingle video of the novel object. Our method will treat the\nframes of the video as the image collection and localize the\nobject in each frame. We consider the problem of localizing\nunseen objects in weakly labeled image collections. Given\na set of images annotated at the image level, our goal is\nto localize the object in each image. The novelty of our\nproposed work is that, in addition to building object ap-\npearance model from the weakly labeled data, we also make\nuse of existing detectors of some other object classes\n(which we call “familiar objects”). We propose a method\nfor transferring the appearance models of the familiar ob-\njects to the unseen object. Our experimental results on\nboth image and video datasets demonstrate the effectiveness\nof our approach.\n\n---TOPIC---\nWeakly Supervised Localization\nAppearance Transfer\nNovel Object Detection\nImage and Video Analysis\nObject Category Recognition",
    "topics": [],
    "references": [
      {
        "citation": "[C. H. Lampert, H. Nickisch, and S. Harmeling, Learning to detect unseen object classes by between-class attribute transfer, IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2009]"
      },
      {
        "citation": "[B. Alexe, T. Deselaers, and V. Ferrari, What is an object?, IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2010]"
      },
      {
        "citation": "[Y. J. Lee and K. Grauman, Object-graphs for context-aware category discovery, IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2010]"
      },
      {
        "citation": "[R. G. Cinbis, J. Verbeek, and C. Schmid, Multi-fold mil training for weakly supervised object localization, IEEE Conference on Computer Vision and Pattern Recognition, 2014]"
      },
      {
        "citation": "[T. Mikolov, I. Sutskever, K. Chen, G. Corrado, and J. Dean, Distributed representations of words and phrases and their compositionality, Advances in Neural Information Processing Systems, 2013]"
      },
      {
        "citation": "[M. H. Nguyen, L. Torresani, F. de la Torre, and Carsten, Weakly supervised discrimiative localization and classiﬁcaition: a joint learning approach, IEEE International Conference on Computer Vision, 2009]"
      },
      {
        "citation": "[A. Papazoglou and V. Ferrari, Fast object segmentation in unconstrained video, IEEE International Conference on Computer Vision, 2013]"
      },
      {
        "citation": "[A. Prest, C. Leistner, J. Civera, C. Schmid, and V. Ferrari, Learning object class detectors from weakly annotated videos, IEEE Conference on Computer Vision and Pattern Recognition, 2012]"
      },
      {
        "citation": "[M. Rohrbach, M. Stark, and B. Schiele, Evaluating knowledge transfer and zero-shot learning in a large-scale setting, IEEE Conference on Computer Vision and Pattern Recognition, 2011]"
      },
      {
        "citation": "[M. Rohrbach, M. Stark, G. Szarvas, I. Gurevych, and B. Schiele, What helps where - and why? semantic relatedness for knowledge transfer, IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2010]"
      }
    ],
    "author_details": [
      {
        "name": "Mrigank Rochan",
        "affiliation": "Department of Computer Science, University of Manitoba, Canada",
        "email": "mrochan@cs.umanitoba.ca"
      },
      {
        "name": "Yang Wang",
        "affiliation": "Department of Computer Science, University of Manitoba, Canada",
        "email": "ywang@cs.umanitoba.ca"
      }
    ]
  },
  {
    "title": "Towards 3D Object Detection with Bimodal Deep Boltzmann Machines over RGBD Imagery\n---AUTHOR---\nWei Liu\n---AUTHOR---\nRongrong Ji\n---AUTHOR---\nShaozi Li",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Liu_Towards_3D_Object_2015_CVPR_paper.pdf",
    "id": "Liu_Towards_3D_Object_2015_CVPR_paper",
    "abstract": "Detecting objects in 3D scenes like point clouds has become an emerging challenge due to the deficiency of labeling 3D training data. To deploy an accurate detection algorithm typically resorts to investigating both RGB and depth modalities, which have distinct statistics while correlated with each other. This paper proposes a cross-modality deep learning framework based on deep Boltzmann Machines for 3D Scenes object detection. By learning cross-modality features from RGBD data, it is possible to capture their joint information to reinforce detector training in individual modalities. The framework utilizes labeled 2D samples from existing datasets and 3D CAD models to overcome the lack of 3D training data. Experiments on the RMRC dataset demonstrate the effectiveness of the proposed approach.\n\n---TOPIC---\n3D Object Detection\nRGBD Imagery\nDeep Boltzmann Machines\nCross-Modality Feature Learning\nData Deficiency/Labeling Challenges",
    "topics": [],
    "references": [
      {
        "citation": "[H. S. Koppula, A. Anand, T. Joachims, and A. Saxena, Semantic labeling of 3d point clouds for indoor scenes, Advances in Neural Information Processing Systems, 2011]"
      },
      {
        "citation": "[S. Gupta, R. Girshick, R. Arbelez, and J. Malik, Learning rich features from RGB-D images for object detection and segmentation, European Conference on Computer Vision, 2014]"
      },
      {
        "citation": "[Deng J, Berg A, Satheesh S, et al., ImageNet large scale visual recognition competition 2012(ILSVRC2012), 2012]"
      },
      {
        "citation": "[O. Kahler and I. Reid, Efficient 3d scene labeling using field-s of trees, International Conference on Computer Vision, 2013]"
      },
      {
        "citation": "[N. Srivastava and R. Salakhutdinov, Multimodal learning with deep boltzmann machines, Advances in neural information processing systems, 2012]"
      },
      {
        "citation": "[K. Lai, L. Bo, X. Ren, and D. Fox, Detection-based object labeling in 3d scenes, IEEE International Conference on Robotics and Automation, 2012]"
      },
      {
        "citation": "[L. Bo, X. Ren, and D. Fox, Unsupervised feature learning for rgb-d based object recognition, Experimental Robotics, 2013]"
      },
      {
        "citation": "[X. Xiong, D. Munoz, J. A. Bagnell, and M. Hebert, 3-d scene analysis via sequenced predictions over points and regions, IEEE International Conference on Robotics and Automation, 2011]"
      },
      {
        "citation": "[A. Wang, J. Lu, G. Wang, J. Cai, and T.-J. Cham, Multi-modal unsupervised feature learning for rgb-d scene labeling, European Conference on Computer Vision, 2014]"
      },
      {
        "citation": "[S. Song and J. Xiao, Sliding shapes for 3d object detection in rgb-d images, European Conference on Computer Vision, 2014]"
      }
    ],
    "author_details": [
      {
        "name": "Wei Liu",
        "affiliation": "Dep. of Cognitive Science, School of Info. Science and Eng., Xiamen University, China",
        "email": "Not available"
      },
      {
        "name": "Rongrong Ji",
        "affiliation": "Dep. of Cognitive Science, School of Info. Science and Eng., Xiamen University, China",
        "email": "rrji@xmu.edu.cn"
      },
      {
        "name": "Shaozi Li",
        "affiliation": "Dep. of Cognitive Science, School of Info. Science and Eng., Xiamen University, China",
        "email": "szlig@xmu.edu.cn"
      }
    ]
  },
  {
    "title": "Deep Hierarchical Parsing for Semantic Segmentation\n---AUTHOR---\nAbhishek Sharma\nOncel Tuzel\nDavid W. Jacobs",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Sharma_Deep_Hierarchical_Parsing_2015_CVPR_paper.pdf",
    "id": "Sharma_Deep_Hierarchical_Parsing_2015_CVPR_paper",
    "abstract": "This paper proposes improvements to the Recursive Context Propagation Network (RCPN), a deep feed-forward neural network used for semantic segmentation. The authors analyze RCPN and identify \"bypass error paths\" that hinder contextual propagation, leading to reduced performance. They introduce two novel modifications: (1) incorporating the classification loss of internal nodes (pure-nodes) within the random parse trees into the loss function, and (2) utilizing a tree-style Markov Random Field (MRF) on the parse tree nodes to model hierarchical dependencies. These modifications enhance performance, achieving state-of-the-art results on several datasets.\n\n---TOPIC---\nSemantic Segmentation\nRecursive Context Propagation Network (RCPN)\nDeep Neural Networks\nMarkov Random Fields (MRF)\nContextual Propagation",
    "topics": [],
    "references": [
      {
        "citation": "[R. Socher, C. C.-Y. Lin, A. Y. Ng, and C. D. Manning. Parsing natural scenes and natural language with recursive neural networks. ICML, 2011.]"
      },
      {
        "citation": "[D. Erhan, Y. Bengio, A. Courville, P.-A. Manzagol, P. Vincent, and S. Bengio. Why does unsupervised pre-training help deep learning? J. Mach. Learn. Res., 11:625–660, 2010.]"
      },
      {
        "citation": "[C. Farabet, C. Couprie, L. Najman, and Y. LeCun. Learning hierarchical features for scene labeling. IEEE TPAMI, August 2013.]"
      },
      {
        "citation": "[R. Fergus and D. Eigen. Nonparametric image parsing using adaptive neighbor sets. IEEE CVPR, 2012.]"
      },
      {
        "citation": "[A. Torralba, K. Murphy, W. Freeman, and M. Rubin. Context-based vision system for place and object recognition. IEEE CVPR, 2003.]"
      },
      {
        "citation": "[P. H. O. Pinheiro and R. Collobert. Recurrent convolutional neural networks for scene parsing. ICML, 2014.]"
      },
      {
        "citation": "[A. Sharma, O. Tuzel, and M. Y. Liu. Recursive context propagation network for semantic segmentation. NIPS, 2014.]"
      },
      {
        "citation": "[J. Tighe and S. Lazebnik. Finding things: Image parsing with regions and per-exemplar detectors. IEEE CVPR, 2013.]"
      },
      {
        "citation": "[R. Mottaghi, S. Fidler, J. Yao, R. Urtasun, and D. Parikh. Analyzing semantic segmentation using hybrid human-machine crfs. IEEE CVPR, 2013.]"
      },
      {
        "citation": "[J. Tighe and S. Lazebnik. Superparsing. Int. J. Comput. Vision, 101(2):329–349, 2013.]"
      }
    ],
    "author_details": [
      {
        "name": "Abhishek Sharma",
        "affiliation": "Computer Science Department, University of Maryland",
        "email": "bhokaal@cs.umd.edu"
      },
      {
        "name": "Oncel Tuzel",
        "affiliation": "MERL, Cambridge",
        "email": "oncel@merl.com"
      },
      {
        "name": "David W. Jacobs",
        "affiliation": "Computer Science Department, University of Maryland",
        "email": "djacobs@umiacs.umd.edu"
      }
    ]
  },
  {
    "title": "Deep Transfer Metric Learning\n---AUTHOR---\nJunlin Hu\n---AUTHOR---\nJiwen Lu\n---AUTHOR---\nYap-Peng Tan",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Hu_Deep_Transfer_Metric_2015_CVPR_paper.pdf",
    "id": "Hu_Deep_Transfer_Metric_2015_CVPR_paper",
    "abstract": "Conventional metric learning methods typically assume that training and test samples originate from similar scenarios, a condition often unmet in real-world visual recognition applications, especially when dealing with different datasets. This paper proposes a novel deep transfer metric learning (DTML) method to address this challenge by learning hierarchical nonlinear transformations to transfer discriminative knowledge from a labeled source domain to an unlabeled target domain. The DTML method maximizes inter-class variations, minimizes intra-class variations, and minimizes distribution divergence between the source and target domains. A deeply supervised transfer metric learning (DSTML) method is further developed to jointly optimize the outputs of hidden and top layers. Experimental results on cross-dataset face verification and person re-identification validate the effectiveness of the proposed methods.\n\n---TOPIC---\nDeep Transfer Metric Learning (DTML)\nCross-Domain Visual Recognition\nMetric Learning\nDistribution Divergence\nDeep Neural Networks",
    "topics": [],
    "references": [
      {
        "citation": "[Ahonen, T., Member, A., Hadid, A., Pietikanen, M., & Member, S. (2006). Face description with local binary patterns: Application to face recognition. *Pattern Analysis and Machine Intelligence*, *28*(2037–2041).]"
      },
      {
        "citation": "[Ando, R. K., & Zhang, T. (2005). A framework for learning predictive structures from multiple tasks and unlabeled data. *Journal of Machine Learning Research*, *6*(1817–1853).]"
      },
      {
        "citation": "[Bengio, Y. (2009). Learning deep architectures for AI. *Foundations and Trends in Machine Learning*, *2*(1–127).]"
      },
      {
        "citation": "[Cai, X., Wang, C., Xiao, B., Chen, X., & Zhou, J. (2012). Deep nonlinear metric learning with independent subspace analysis for face veriﬁcation. *ACM Multimedia*, *749–752.*]"
      },
      {
        "citation": "[Chen, D., Cao, X., Wang, L., Wen, F., & Sun, J. (2012). Bayesian face revisited: A joint formulation. *European Conference on Computer Vision*, *566–579.*]"
      },
      {
        "citation": "[Duan, L., Tsang, I. W., Xu, D., & Maybank, S. J. (2009). Domain transfer SVM for video concept detection. *Conference on Computer Vision and Pattern Recognition*, *1375–1381.*]"
      },
      {
        "citation": "[Gray, D., & Tao, H. (2008). Viewpoint invariant pedestrian recognition with an ensemble of localized features. *European Conference on Computer Vision*, *262–275.*]"
      },
      {
        "citation": "[Gretton, A., Borgwardt, K. M., Rasch, M. J., Schölkopf, B., & Smolå, A. J. (2006). A kernel method for the two-sample-problem. *Neural Information Processing Systems*, *513–520.*]"
      },
      {
        "citation": "[Hinton, G. E., Osindero, S., & Teh, Y.-W. (2006). A fast learning algorithm for deep belief nets. *Neural Computation*, *18*(7), 1527–1554.]"
      },
      {
        "citation": "[Huang, G. B., Lee, H., & Learned-Miller, E. G. (2012). Learning hierarchical representations for face veriﬁcation with convolutional deep belief networks. *Conference on Computer Vision and Pattern Recognition*, *2518–2525.*]"
      }
    ],
    "author_details": [
      {
        "name": "Junlin Hu",
        "affiliation": "School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore",
        "email": "jhu007@e.ntu.edu.sg"
      },
      {
        "name": "Jiwen Lu",
        "affiliation": "Advanced Digital Sciences Center, Singapore",
        "email": "jiwen.lu@adsc.com.sg"
      },
      {
        "name": "Yap-Peng Tan",
        "affiliation": "School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore",
        "email": "eyptan@ntu.edu.sg"
      }
    ]
  },
  {
    "title": "Learning Lightness from Human Judgement on Relative Reﬂectance\n---AUTHOR---\nTakuya Narihira\nMichael Maire\nStella X. Yu",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Narihira_Learning_Lightness_From_2015_CVPR_paper.pdf",
    "id": "Narihira_Learning_Lightness_From_2015_CVPR_paper",
    "abstract": "We develop a new approach to inferring lightness, the perceived reﬂectance of surfaces, from a single image. Classic methods view this problem from the perspective of intrinsic image decomposition, where an image is separated into reﬂectance and shading components. Rather than reason about reﬂectance and shading together, we learn to directly predict lightness differences between pixels. Large-scale training from human judgement data on relative reﬂectance, and patch representations built using deep networks, provide the foundation for our model. Benchmarked on the Intrinsic Images in the Wild dataset [4], our local lightness model achieves on-par performance with the state-of-the-art global lightness model, which incorporates multiple shading/reﬂectance priors and simultaneous reasoning between pairs of pixels in a dense conditional random field formulation.",
    "topics": [
      "Lightness perception",
      "Intrinsic image decomposition",
      "Deep learning",
      "Human judgment data",
      "Relative reflectance"
    ],
    "references": [
      {
        "citation": "[E. H. Adelson. Lightness perception and lightness illusions. In M. Gazzaniga, editor, The cognitive neurosciences, pages 339–51. MIT Press, Cambridge, MA, 1999.] - This paper provides foundational work on lightness perception, a key concept in intrinsic image decomposition."
      },
      {
        "citation": "[H. G. Barrow and J. M. Tenenbaum. Recovering intrinsic scene characteristics from images. Computer Vision Systems, pages 3–26, 1978.] - A seminal early work on recovering scene characteristics from images, laying groundwork for intrinsic image algorithms."
      },
      {
        "citation": "[E. H. Land and J. J. McCann. Lightness and retinex theory. Journal of Optical Society of America, 61(1):1–11, 1971.] - Introduces the Retinex theory, a crucial concept for understanding how humans perceive brightness and color constancy, relevant to intrinsic image recovery."
      },
      {
        "citation": "[B. Horn. Determining lightness from an image. Computer Graphics and Image Processing, 3:277–99, 1974.] - An early attempt at determining lightness from an image, a core component of intrinsic image decomposition."
      },
      {
        "citation": "[M. Tappen, W. Freeman, and E. Adelson. Recovering intrinsic images from a single image. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2005.] - A significant work directly addressing intrinsic image recovery from a single image."
      },
      {
        "citation": "[R. Grosse, M. K. Johnson, E. H. Adelson, and W. T. Freeman. Ground truth dataset and baseline evaluations for intrinsic image algorithms. In International Conference on Computer Vision, 2009.] - Provides a dataset and baseline evaluations for intrinsic image algorithms, crucial for comparing different approaches."
      },
      {
        "citation": "[Y. Tang, R. Salakhutdinov, and G. Hinton. Deep Lambertian networks. In International Conference on Machine Learning, 2012.] - Explores deep learning approaches for intrinsic image decomposition, specifically using Lambertian reflectance models."
      },
      {
        "citation": "[J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imaginet: A large-scale hierarchical image database. In IEEE Conference on Computer Vision and Pattern Recognition, pages 248–255, 2009.] - Introduces ImageNet, a large dataset that has become a standard for training and evaluating computer vision models, potentially used in intrinsic image algorithms."
      },
      {
        "citation": "[A. Krizhevsky, S.Ilya, and G. E. Hinton. Imaginet classification with deep convolutional neural networks. In Neural Information Processing Systems, 2012.] - Introduces AlexNet, a deep convolutional neural network that achieved breakthrough performance on ImageNet, influencing subsequent research in intrinsic image decomposition."
      },
      {
        "citation": "[Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, and T. Darrell. Caffe: Convolutional architecture for fast feature embedding. In arXiv preprint arXiv:1408.5093, 2014.] - Introduces Caffe, a deep learning framework that has been widely used for computer vision tasks, including intrinsic image decomposition."
      }
    ],
    "author_details": [
      {
        "name": "Takuya Narihira",
        "affiliation": "UC Berkeley / ICSI / Sony Corp.",
        "email": "takuya.narihira@berkeley.edu"
      },
      {
        "name": "Michael Maire",
        "affiliation": "TTI Chicago",
        "email": "mmaire@ttic.edu"
      },
      {
        "name": "Stella X. Yu",
        "affiliation": "UC Berkeley / ICSI",
        "email": "stellayu@berkeley.edu"
      }
    ]
  },
  {
    "title": "Hierarchical-PEP Model for Real-world Face Recognition\n---AUTHOR---\nHaoxiang Li\n---AUTHOR---\nGang Hua",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Li_Hierarchical-PEP_Model_for_2015_CVPR_paper.pdf",
    "id": "Li_Hierarchical-PEP_Model_for_2015_CVPR_paper",
    "abstract": "Pose variation remains a major challenge in real-world face recognition. Inspired by the probabilistic elastic part (PEP) model and deep hierarchical architectures, we propose the Hierarchical-PEP model to address this problem. This model hierarchically decomposes a face image into face parts at different levels of detail to build pose-invariant part-based face representations. The model stacks face part representations at each layer, reduces dimensionality, and aggregates them to build a compact and invariant face representation. The Hierarchical-PEP model exploits fine-grained structures of face parts and is guided by supervised information. Empirical verification on public benchmarks (LFW, YouTube Faces) and a face recognition challenge (PaSC) demonstrates state-of-the-art performance.\n\n---TOPICCS---\nFace Recognition\nPose Variation\nHierarchical Modeling\nPEP (Probabilistic Elastic Part) Model\nInvariant Representations",
    "topics": [],
    "references": [
      {
        "citation": "[Ahonen, T., Hadid, A., & Pietikainen, M. (2004). Face recognition with local binary patterns. *Proceedings of the European Conference on Computer Vision*.]"
      },
      {
        "citation": "[Grauman, K., & Darrell, T. (2005). The pyramid match kernel: Discriminative classification with sets of image features. *Proceedings of the IEEE International Conference on Computer Vision*.]"
      },
      {
        "citation": "[Hu, J., Lu, J., & Tan, Y.-P. (2014). Discriminative deep metric learning for face verification in the wild. *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*.]"
      },
      {
        "citation": "[Belhumeur, P. N., Hespanha, J. P., & Kriegman, D. J. (1997). Eigenfaces vs. Fisherfaces: Recognition using class specific linear projection. *IEEE Transactions on Pattern Analysis and Machine Intelligence*.]"
      },
      {
        "citation": "[Huang, G., Jain, V., & Learned-Miller, E. (2007). Unsupervised joint alignment of complex images. *Proceedings of the IEEE International Conference on Computer Vision*.]"
      },
      {
        "citation": "[Huang, G. B., & Learned-Miller, E. (2007). Labeled faces in the wild: Updates and new reporting procedures.]"
      },
      {
        "citation": "[Hu, J., Lu, J., Yuan, J., & Tan, Y.-P. (2014). Large margin multi-metric learning for face and kinship verification in the wild. *Asian Conference on Computer Vision (ACCV)*.]"
      },
      {
        "citation": "[Cao, X., Wipf, D., Wen, F., Duan, G., & Sun, J. (2013). A practical transfer learning algorithm for face verification. *Proceedings of the IEEE International Conference on Computer Vision*.]"
      },
      {
        "citation": "[Lei, Z., Pietikainen, M., & Li, S. Z. (2014). Learning discriminant face descriptor. *IEEE Transactions on Pattern Analysis and Machine Intelligence*.]"
      },
      {
        "citation": "[Simonyan, K., Parkhi, O. M., Vedaldi, A., & Zisserma, A. (2013). Deep fisher networks for large-scale image classification. *Advances in neural information processing systems*.]"
      }
    ],
    "author_details": [
      {
        "name": "Haoxiang Li",
        "affiliation": "Stevens Institute of Technology",
        "email": "{hli18}@steverns.edu"
      },
      {
        "name": "Gang Hua",
        "affiliation": "Stevens Institute of Technology",
        "email": "{ghua}@steverns.edu"
      }
    ]
  },
  {
    "title": "Bilinear Heterogeneous Information Machine for RGB-D Action Recognition\n---AUTHOR---\nYu Kong\n---AUTHOR---\nYun Fu",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Kong_Bilinear_Heterogeneous_Information_2015_CVPR_paper.pdf",
    "id": "Kong_Bilinear_Heterogeneous_Information_2015_CVPR_paper",
    "abstract": "This paper proposes a novel approach to action recognition from RGB-D cameras, which jointly uses depth features and RGB visual features. Rich heterogeneous RGB and depth data are effectively compressed and projected to a learned shared space to reduce noise and capture useful information for recognition. Knowledge from various sources is shared in the learned space to learn cross-modal features, guiding the discovery of valuable information. The method represents RGB and depth data in a matrix form and formulates the recognition task as a low-rank bilinear model. The rank of the model parameter is minimized to build a low-rank classifier, which improves generalization power. The method is evaluated on two public RGB-D action datasets and achieves state-of-the-art results, also showing promising results when RGB or depth data are missing.\n\n---TOPICCS---\nAction Recognition\nRGB-D Data\nBilinear Modeling\nCross-Modal Features\nLow-Rank Approximation",
    "topics": [],
    "references": [
      {
        "citation": "[A. Argyriou, T. Evgeniou, and M. Pontil. Convex multi-task feature learning. IJCV, 2008.] - This paper likely provides foundational work on feature learning, which is relevant to action recognition."
      },
      {
        "citation": "[L. Bo, K. Lai, X. Ren, and D. Fox. Object recognition with hierarchical kernel descriptors. In CVPR, June 2011.] -  Deals with object recognition, a related task, and uses kernel descriptors, a common technique."
      },
      {
        "citation": "[T.-M.-T. Do and T. Artieres. Large margin training for hid-den markov models with partially observed states. In ICML, 2009.] - Introduces a training method applicable to HMMs, a model often used in action recognition."
      },
      {
        "citation": "[S. Hadﬁeld and R. Bowden. Hollywood 3D: Recognizing actions in 3D natural scenes. In CVPR, 2013.] - Directly addresses action recognition in 3D scenes."
      },
      {
        "citation": "[S. Ji, W. Xu, M. Yang, and K. Yu. 3D convolutional neural networks for human action recognition. PAMI, 2013.] -  Presents a key approach using 3D CNNs, a dominant technique in modern action recognition."
      },
      {
        "citation": "[J. Wang, Z. Liu, Y. Wu, and J. Yuan. Mining actionlet ensemble for action recognition with depth cameras. In CVPR, June 2012.] - Focuses on action recognition using depth cameras, a common modality."
      },
      {
        "citation": "[T. Kobayashi. Low-rank biliner classiﬁcation: Efﬁcient convex optimization and extensions. IJCV, 2014.] - Introduces a low-rank bilinear classification method, potentially useful for modeling complex interactions."
      },
      {
        "citation": "[L. Xia and J. Aggarwal. Spatio-temporal depth cuboid similarity feature for activity recognition using depth camera. In CVPR, 2013.] -  Presents a specific feature representation for activity recognition using depth data."
      },
      {
        "citation": "[O. Oreifej and Z. Liu. HON4D: Histogram of oriented 4D normals for activity recognition from depth sequences. In CVPR, 2013.] - Introduces a specific feature descriptor (HON4D) for action recognition from depth sequences."
      },
      {
        "citation": "[N. Tishby, F. C. Pereira, and W. Bialek. The information bottleneck method. In Proc. of the 37-th Annual Allerton Con-ference on Communication, Control and Computing, pages 368–377, 1999.] - Provides theoretical background on information bottlenecks, a concept that can be applied to feature selection and representation learning."
      }
    ],
    "author_details": [
      {
        "name": "Yu Kong",
        "affiliation": "Northeaster University, Boston, MA, USA",
        "email": "yukong@ece.neu.edu"
      },
      {
        "name": "Yun Fu",
        "affiliation": "Northeaster University, Boston, MA, USA",
        "email": "yunfu@ece.neu.edu"
      }
    ]
  },
  {
    "title": "Absolute Pose for Cameras Under Flat Refracive Interfaces\n---AUTHOR---\nSebastian Haner\n---AUTHOR---\nKalle ˚Astr¨om",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Haner_Absolute_Pose_for_2015_CVPR_paper.pdf",
    "id": "Haner_Absolute_Pose_for_2015_CVPR_paper",
    "abstract": "This paper studies the problem of determining the absolute pose of a perspective camera observing a scene through a known refractive plane, the ﬂat boundary between transparent media with different refractive indices. Efficient minimal solvers are developed for the 2D, known orientation and known rotation axis cases, and near-minimal solvers for the general calibrated and unknown focal length cases. We show that ambiguities in the equations of Snell’s law give rise to a large number of false solutions, increasing the complexity of the problem. Evaluation of the solvers on both synthetic and real data show excellent numerical performance, and the necessity of explicitly modelling refraction to obtain accurate pose estimates.\n\n---TOPICCS---\nAbsolute Pose Estimation\nRefractive Interfaces\nSnell's Law\nCamera Calibration\nStructure-and-Motion",
    "topics": [],
    "references": [
      {
        "citation": "[Agrawal, A., Ramalingam, S., Taguchi, Y., & Chari, V. A theory of multi-layer flat refractive geometry. In Conference on Computer Vision and Pattern Recognition, pages 3346–3353. IEEE, 2012.] - Cited 3 times, likely foundational work on the topic."
      },
      {
        "citation": "[Byr¨od, M., Josephson, K., & ˚Astr¨om, K. Fast and stable polynomial equation solving and its application to computer vision. International Journal of Computer Vision, 84(3):237–256, 2009.] - Cited 4 times, important for the polynomial solving techniques used."
      },
      {
        "citation": "[Cox, D. A., Little, J., & O’Shea, D. Ideals, Varieties, and Algorithms: An Introduction to Computational Algebraic Geometry and Commutative Algebra, 3rd ed. Springer-Verlag New York, Inc., 2007.] - Cited 4 times, likely a key resource for the algebraic geometry aspects."
      },
      {
        "citation": "[˚Astr¨om, K., Kuang, Y., & Ask, E. Exploiting p-fold symmetries for faster polynomial equation solving. In International Conference on Pattern Recognition, pages 3232–3235. IEEE, 2012.] - Cited 2 times, related to polynomial solving optimization."
      },
      {
        "citation": "[Chari, V., & Sturm, P. F. Multi-view geometry of the refractive plane. In British Machine Vision Conference, 2009.] - Cited 1 time, a direct reference to the geometry of refractive planes."
      },
      {
        "citation": "[Fitzgibbon, A. W. Simultaneous linear estimation of multiple view geometry and lens distortion. In Conference on Computer Vision and Pattern Recognition, pages 125–132. IEEE, 2001.] - Cited 2 times, relevant for geometric estimation and lens distortion."
      },
      {
        "citation": "[Kuang, Y., & ˚Astr¨om, K. Numerically stable optimization of polynomial solvers for minimal problems. In A. W. Fitzgibbon, S. Lazebnik, P. Perona, Y. Sato, and C. Schmid, editors, European Conference on Computer Vision, volume 7574 of Lecture Notes in Computer Science, pages 100–113. Springer, 2012.] - Cited 1 time, focuses on optimization of polynomial solvers."
      },
      {
        "citation": "[Nist´er, D. A minimal solution to the generalised 3-point pose problem. In Conference on Computer Vision and Pattern Recognition, pages 560–567, 2004.] - Cited 1 time, relevant for pose estimation."
      },
      {
        "citation": "[Stew´enius, H., Nist´er, D., Oskarsson, M., & ˚Astr¨om, K. Solutions to minimal generalized relative pose problems. In Workshop on Omnidirectional Vision, 2005.] - Cited 1 time, related to pose estimation."
      },
      {
        "citation": "[Kukelova, Z., Bujnak, M., & Pajdla, T. Polynomial eigenvalue solutions to minimal problems in computer vision. IEEE Transactions on Pattern Analysis and Machine Intelligence, 34(7):1381–1393, 2012.] - Cited 1 time, focuses on polynomial solutions to minimal problems."
      }
    ],
    "author_details": [
      {
        "name": "Sebastian Haner",
        "affiliation": "Centre for Mathematical Sciences, Lund University, Sweden",
        "email": "haner@maths.lth.se"
      },
      {
        "name": "Kalle ˚Astr¨om",
        "affiliation": "Centre for Mathematical Sciences, Lund University, Sweden",
        "email": "kalle@maths.lth.se"
      }
    ]
  },
  {
    "title": "R6P - Rolling Shutter Absolute Pose Problem\n---AUTHOR---\nCenek Albl\nZuzana Kukelova\nTomas Pajdla",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Albl_R6P_-_Rolling_2015_CVPR_paper.pdf",
    "id": "Albl_R6P_-_Rolling_2015_CVPR_paper",
    "abstract": "We present a minimal, non-iterative solution to the absolute pose problem for images from rolling shutter cameras. The absolute pose problem is a key problem in computer vision and rolling shutter is present in a vast majority of today’s digital cameras. We propose several rolling shutter camera models and verify their feasibility for a polynomial solver. A solution based on a linearized camera model is chosen and verified in several experiments. We use a linear approximation to the camera orientation, which is meaningful only around the identity rotation. We show that the standard P3P algorithm is able to estimate camera orientation within 6 degrees for camera rotation velocity as high as 30deg/frame. Therefore we can use the standard P3P algorithm to estimate camera orientation and to bring the camera rotation matrix close to the identity. Using this solution, camera position, orientation, translational velocity and angular velocity can be computed using six 2D-to-3D correspondences, with orientation error under half a degree and relative position error under 2%. A significant improvement in terms of the number of inliers in RANSAC is demonstrated.\n\n---TOPIC---\nRolling Shutter Cameras\nAbsolute Pose Problem\nPolynomial Solvers\nLinearized Camera Models\nComputer Vision",
    "topics": [],
    "references": [
      {
        "citation": "[M. A. Fischler and R. C. Bolles. Random sample consensus: A paradigm for model fitting with applications to image analysis and automated cartography. Commun. ACM, 24(6):381–395, June 1981.] - This is a foundational paper on RANSAC, a core technique for robust model fitting, likely relevant to many computer vision problems."
      },
      {
        "citation": "[R. Haralick, D. Lee, K. Ottenburg, and M. Nolle. Analysis and solutions of the three point perspective pose estimation problem. In Computer Vision and Pattern Recognition, 1991. Proceedings CVPR ’91., IEEE Computer Society Conference on, pages 592–598, Jun 1991.] - Addresses a fundamental problem in pose estimation, likely important for understanding the context of the paper."
      },
      {
        "citation": "[J. Hedborg, E. Ringaby, P.-E. Forssen, and M. Felsberg. Structure and motion estimation from rolling shutter video. In Computer Vision Workshops (ICCV Workshops), 2011 IEEE International Conference on, pages 17–23, 2011.] - Directly addresses the problem of structure and motion estimation from rolling shutter video, a key focus given the paper's likely subject."
      },
      {
        "citation": "[J. Hedborg, P.-E. Forss´en, M. Felsberg, and E. Ringaby. Rolling shutter bundle adjustment. In CVPR, pages 1434–1441, 2012.] -  A direct extension of the previous paper, focusing on bundle adjustment for rolling shutter data."
      },
      {
        "citation": "[C. Jia and B. L. Evans. Probablistic 3-d motion estimation for rolling shutter video rectiﬁcation from visual and inertial measurements. In MMSP, pages 203–208. IEEE, 2012.] - Combines rolling shutter video with inertial measurements, a common approach for improved accuracy."
      },
      {
        "citation": "[M. A. Fischler and R. C. Bolles. Random sample consensus: A paradigm for model fitting with applications to image analysis and automated cartography. Commun. ACM, 24(6):381–395, June 1981.] - This is a foundational paper on RANSAC, a core technique for robust model fitting, likely relevant to many computer vision problems."
      },
      {
        "citation": "[G. Klein and D. Murray. Parallel tracking and mapping on a camera phone. In Proceedings of the 2009 8th IEEE International Symposium on Mixed and Augmented Reality, ISMAR ’09, pages 2009.] - Introduces a key approach to visual SLAM, relevant to understanding the broader context of camera pose estimation."
      },
      {
        "citation": "[Z. Kukelova, M. Bujnak, J.Heller, and T. Pajdla. Singly-bordered block-diagonal form for minimal problem solvers. In ACCV’14, 2014.] - Addresses optimization techniques, likely important for efficient computation."
      },
      {
        "citation": "[Z. Kukelova, M. Bujnak, and T. Pajdla. Automatic generator of minimal problem solvers. In D. A. Forsyth, P. H. S. Torr, and A. Zisserman, editors, Computer Vision - ECCV 2008, 10th European Conference on Computer Vision, Proceedings, Part III, volume 5304 of Lecture Notes in Computer Science, pages 302–315, Berlin, Germany, October 2008. Springer.] -  Related to the previous reference, focusing on efficient solvers."
      },
      {
        "citation": "[D. Cox, J. Little, and D. O’Shea. Using Algebraic Geometry. Graduate Texts in Mathematics. Springer, 2005.] - Provides mathematical background that may be relevant to the paper's methods."
      }
    ],
    "author_details": [
      {
        "name": "Cenek Albl",
        "affiliation": "Czech Technical University in Prague, Faculty of Electrical engineering",
        "email": "{alblcene}@cmp.felk.cvut.cz"
      },
      {
        "name": "Zuzana Kukelova",
        "affiliation": "Microsoft Research Ltd",
        "email": "a-zukuke@microsoft.com"
      },
      {
        "name": "Tomas Pajdla",
        "affiliation": "Czech Technical University in Prague, Faculty of Electrical engineering",
        "email": "{alblcene,pajdla}@cmp.felk.cvut.cz"
      }
    ]
  },
  {
    "title": "Fast and Robust Hand Tracking Using Detection-Guided Optimization\n---AUTHOR---\nSrinath Sridhar\nFranziska Mueller\nAntti Oulasvirta\nChristian Theobalt",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Sridhar_Fast_and_Robust_2015_CVPR_paper.pdf",
    "id": "Sridhar_Fast_and_Robust_2015_CVPR_paper",
    "abstract": "This paper presents a novel method for hand tracking with a single depth camera that aims to address challenges such as tracking inaccuracies, incomplete coverage of motions, low framerate, complex camera setups, and high computational requirements. Our algorithm uses a novel detection-guided optimization strategy that increases the robustness and speed of pose estimation. It combines the benefits of model-based generative tracking and discriminative hand pose detection into a unified framework, achieving high efficiency and robust performance while minimizing mutual failures. The approach is extremely fast (50 fps without GPU support) and supports varying static or moving camera-to-scene arrangements.\n\n---TOPICCS---\nHand Tracking\nDepth Cameras\nOptimization\nComputer Vision\nHuman-Computer Interaction",
    "topics": [],
    "references": [
      {
        "citation": "[V. Athitzos and S. Sclaroff, Estimating 3D hand pose from a cluttered image, Proc. of CVPR 2003]"
      },
      {
        "citation": "[A. Baak, M. Muller, G. Bharaj, H.-P. Seidel, and C. Theobalt, A data-driven approach for real-time full body pose reconstruction from a depth camera, Proc. of ICCV 2011]"
      },
      {
        "citation": "[L. Ballan, A. Taneja, J. Gall, L. Van Gool, and M. Pollefeys, Motion capture of hands in action using discriminative salient points, LNCS, 2012]"
      },
      {
        "citation": "[A. Bhattacharyya, On a measure of divergence between two multinomial populations, Sankhya: The Indian Journal of Statistics, 1946]"
      },
      {
        "citation": "[A. Criminisi and J. Shotton, Decision forests for computer vision and medical image analysis, Springer, 2013]"
      },
      {
        "citation": "[S. R. Fanello, C. Keskin, S. Izadi, P. Kohli, D. Kim, D. Sweeney, A. Criminisi, J. Shotton, S. B. Kang, and T. Paek, Learning to be a depth camera for close-range human capture and interaction, ACM TOG, 2014]"
      },
      {
        "citation": "[V. Ganapathi, C. Plagemann, D. Koller, and S. Thrun, Real-time human pose tracking from range data, LNCS, 2012]"
      },
      {
        "citation": "[R. Girshick, J. Shotton, P. Kohli, A. Criminisi, and A. Fitzgibbon, Efﬁcient regression of general-activity human poses from depth images, ICCV, 2011]"
      },
      {
        "citation": "[H. Hamer, K. Schindler, E. Koller-Meier, and L. Van Gool, Tracking a hand manipulating an object, ICCV, 2009]"
      },
      {
        "citation": "[C. Keskin, F. Kirac, Y. Kara, and L. Akarun, Real time hand pose estimation using depth sensors, ICCV Workshops, 2011]"
      }
    ],
    "author_details": [
      {
        "name": "Srinath Sridhar",
        "affiliation": "Max Planck Institute for Informatics",
        "email": "ssridhar@mpi-inf.mpg.de"
      },
      {
        "name": "Franziiska Mueller",
        "affiliation": "Max Planck Institute for Informatics, Saarland University",
        "email": "frmueller@mpi-inf.mpg.de"
      },
      {
        "name": "Antti Oulasvirta",
        "affiliation": "Aalto University",
        "email": "antti.oulasvirta@aalto.fi"
      },
      {
        "name": "Christian Theobalt",
        "affiliation": "Max Planck Institute for Informatics",
        "email": "theobalt@mpi-inf.mpg.de"
      }
    ]
  },
  {
    "title": "Supervised Discrete Hashing\n---AUTHORs---\nFumin Shen\nChunhua Shen\nWei Liu\nHeng Tao Shen",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Shen_Supervised_Discrete_Hashing_2015_CVPR_paper.pdf",
    "id": "Shen_Supervised_Discrete_Hashing_2015_CVPR_paper",
    "abstract": "This paper introduces Supervised Discrete Hashing (SDH), a new supervised hashing framework designed to generate optimal binary hash codes for linear classification. The framework addresses the difficulty of handling discrete constraints in learning to hash, which typically leads to NP-hard optimization problems. By introducing an auxiliary variable and employing a regularization algorithm, the objective is reformulated to be solved efficiently. The approach utilizes cyclic coordinate descent to solve a regularization sub-problem and achieves high-quality discrete solutions efficiently, enabling the handling of massive datasets. Experiments on four large image datasets demonstrate SDH's superiority to state-of-the-art hashing methods in large-scale image retrieval.",
    "topics": [
      "Hashing",
      "Discrete Optimization",
      "Supervised Learning",
      "Image Retrieval",
      "Binary Codes"
    ],
    "references": [
      {
        "citation": "[Belkin, M., & Niyogi, P. (2003). Laplacian eigenmaps for dimensionality reduction and data representation. *Neural Computation, 15*(6), 1373–1396.] - A foundational paper on dimensionality reduction using spectral methods."
      },
      {
        "citation": "[Datar, N., Immorlica, P., Indyk, P., & Mirrokni, V. S. (2004). Locality-sensitive hashing scheme based on p-stable distributions. *Proc. International Symposium on Computational Geometry, 99-106.*] - Introduces a key hashing technique based on p-stable distributions."
      },
      {
        "citation": "[Gong, Y., Kumar, S., Rowley, H. A., & Lazebnik, S. (2013). Learning binary codes for high-dimensional data using bilinear projections. *Proc. CVPR, 31-38.*] - Explores learning binary codes using bilinear projections."
      },
      {
        "citation": "[Weiss, Y., Fergus, R., & Torralba, A. (2008). Spectral hashing. *NIPS 21, 2008.*] - Introduces spectral hashing, a significant contribution to the field."
      },
      {
        "citation": "[Gong, Y., Lazebnik, S., Gordo, A., & Perronnin, F. (2013). Iterative quantization: A procrustean approach to learning binary codes for large-scale image retrieval. *IEEE Transactions on Pattern Analysis and Machine Intelligence, 35*(12), 2916–2929.*] - Presents an iterative quantization approach for learning binary codes."
      },
      {
        "citation": "[Kulis, B., & Darrell, T. (2009). Learning to hash with binary reconstructive embeddings. *NIPS 22, 2009.*] - Introduces a method for learning to hash using binary reconstructive embeddings."
      },
      {
        "citation": "[Liu, W., Wang, J., Kumar, S., & Chang, S.-F. (2011). Hashing with graphs. *Proc. ICML, 2011.*] - Explores hashing techniques using graph structures."
      },
      {
        "citation": "[Wang, J., Kumar, S., & Chang, S.-F. (2012). Semi-supervised hashing for large scale search. *IEEE Transactions on Pattern Analysis and Machine Intelligence, 34*(12), 2393–2406.*] - Addresses hashing in a semi-supervised setting."
      },
      {
        "citation": "[Shen, C., & Hao, Z. (2011). A direct formulation for totally-corrective multi-class boosting. *Proc. CVPR, 2011.*] - While not directly hashing, this paper on boosting is relevant to the broader context of learning and classification."
      },
      {
        "citation": "[Norouzi, M., & Blei, D. M. (2011). Minimal loss hashing for compact binary codes. *Proc. ICML, 2011.*] - Introduces a method for creating compact binary codes with minimal loss."
      }
    ],
    "author_details": [
      {
        "name": "Fumin Shen",
        "affiliation": "University of Electronic Science and Technology of China",
        "email": "[Email not available]"
      },
      {
        "name": "Chunhua Shen",
        "affiliation": "University of Adelaide; and Australian Centre for Robotic Vision",
        "email": "[Email not available]"
      },
      {
        "name": "Wei Liu",
        "affiliation": "IBM Research",
        "email": "[Email not available]"
      },
      {
        "name": "Heng Tao Shen",
        "affiliation": "The University of Queensland",
        "email": "[Email not available]"
      }
    ]
  },
  {
    "title": "A Maximum Entropy Feature Descriptor for Age Invariant Face Recognition\n---AUTHOR---\nDihong Gong\nZhifeng Li\nDacheng Tao\nJianzhuang Liu\nXuelong Li",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Gong_A_Maximum_Entropy_2015_CVPR_paper.pdf",
    "id": "Gong_A_Maximum_Entropy_2015_CVPR_paper",
    "abstract": "In this paper, we propose a new approach to overcome the representation and matching problems in age invariant face recognition. First, a new maximum entropy feature descriptor (MEFD) is developed that encodes the microstructure of facial images into a set of discrete codes in terms of maximum entropy. By densely sampling the encoded face image, sufficient discriminatory and expressive information can be extracted for further analysis. A new matching method is also developed, called identity factor analysis (IFA), to estimate the probability that two faces have the same underlying identity. The effectiveness of the framework is confirmed by extensive experimentation on two face aging datasets, MORPH and FGNET. We also conduct experiments on the famous LFW dataset to demonstrate the excellent generalizability of our new approach.\n\n---TOPIC---\nAge Invariant Face Recognition (AIFR)\nMaximum Entropy Feature Descriptor (MEFD)\nIdentity Factor Analysis (IFA)\nFace Aging Datasets (MORPH, FGNET, LFW)\nMicrostructure Encoding",
    "topics": [],
    "references": [
      {
        "citation": "[Li, Zhifeng, Dahua Lin, and Xiaoou Tang. “Nonparametric Discriminant Analysis for Face Recognition.” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 31, no. 4, 2009.]"
      },
      {
        "citation": "[Wang, Xiaogang, and Xiaoou Tang. “A unified framework for subspace face recognition.” IEEE Trans. Pattern Anal. Mach. Intell., vol. 26, no. 9, pp. 1222–1228, 2004.]"
      },
      {
        "citation": "[Li, Unsang, Zhifeng Li, and Anil K. Jain. “A discriminative model for age invariant face recognition.” IEEE Transactions on Information Forensics and Security, vol. 6, no. 3-2, pp. 1028–1037, 2011.]"
      },
      {
        "citation": "[Park, Unsang, Yiying Tong, and Anil K. Jain. “Age-invariant face recognition.” IEEE Trans. Pattern Anal. Mach. Intell., vol. 32, no. 5, pp. 947–954, 2010.]"
      },
      {
        "citation": "[Belhumeur, Peter N., Jo˜a o P. Hespanha, and David J. Kriegman. “Eigenfaces vs. fisherfaces: Recognition using class specific linear projection.” IEEE Trans. Pattern Anal. Mach. Intell., vol. 19, no. 7, pp. 711–720, 1997.]"
      },
      {
        "citation": "[Gong, D., Z. Li, D. Lin, J. Liu, X. Tang. “Hidden Factor Analysis for Age Invariant Face Recognition,” ICCV 2013.]"
      },
      {
        "citation": "[Huang, G.B., M. Mattar, T. Berg, E. Learned-Miller, and A. Hanson. “Labelled faces in the wild: A database for studying face recognition in unconstrained environments,” Technical Report 07-49, University of Massachusetts, Amherst, October 2007.]"
      },
      {
        "citation": "[Li, Zhifeng, Dahua Lin, and Xiaoou Tang. “Nonparametric Discriminant Analysis for Face Recognition,” IEEE Transactions on Pattern Analysis and Machine Intelligence, Volume 31, Issue 4, 2009.]"
      },
      {
        "citation": "[Wang, Xiaogang, and Xiaoou Tang. “Random sampling LDA for face recognition,” in CVPR, 2004, pp. 259–265.]"
      },
      {
        "citation": "[Klare, Brendan, Zhifeng Li, and Anil K. Jain. “Matching forensic sketches to mug shot photos,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 33, no. 3, pp. 639–646, 2011.]"
      }
    ],
    "author_details": [
      {
        "name": "Dihong Gong",
        "affiliation": "Shenzhen Key Lab of Computer Vision and Pattern Recognition, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, China",
        "email": "dh.gong@siat.ac.cn"
      },
      {
        "name": "Zhifeng Li",
        "affiliation": "Shenzhen Key Lab of Computer Vision and Pattern Recognition, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, China",
        "email": "zhifeng.li@siat.ac.cn"
      },
      {
        "name": "Dacheng Tao",
        "affiliation": "Centre for Quantum Computation & Intelligent Systems, Faculty of Engineering and IT, University of Technology, Sydney, NSW 2007, Australia",
        "email": "dacheng.tao@uts.edu.au"
      },
      {
        "name": "Jianzhuang Liu",
        "affiliation": "Dept. of Information Engineering, the Chinese University of Hong Kong and Media Lab, Huawei Technologies Co. Ltd., China",
        "email": "liu.jianzhuang@huawei.com"
      },
      {
        "name": "Xuelong Li",
        "affiliation": "Xi'an Institute of Optics and Precision Mechanics, Chinese Academy of Sciences",
        "email": "xuelong_li@opt.ac.cn"
      }
    ]
  },
  {
    "title": "How many bits does it take for a stimulus to be salient?\n---AUTHOR---\nSayed Hossein Khatoonabadi\nNuno Vasconcelos\nIvan V. Bajic´\nYuifeng Shan",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Khatoonabadi_How_Many_Bits_2015_CVPR_paper.pdf",
    "id": "Khatoonabadi_How_Many_Bits_2015_CVPR_paper",
    "abstract": "A considerable research effort has recently been devoted to the development of computational models of salience. Early approaches modeled salience as the result of center-surround filters and normalization, while more recent works have tried to identify general computational principles applicable to various salience mechanisms and broader perception. This paper proposes a direct measure of salience based on the number of bits required by an optimal video compressor to encode a video patch, demonstrating its predictive power for eye fixations. The measure is embedded in a Markov random field model to account for global salience effects, achieving state-of-the-art accuracy for fixation prediction at a low computational cost. The research connects salience to probabilistic inference and draws on the view of the brain as a universal compression device.\n\n---TOPICCS---\nVisual Salience\nComputational Models\nVideo Compression\nProbabilistic Inference\nAttention Mechanisms",
    "topics": [],
    "references": [
      {
        "citation": "[Helbing, D. and Molnar, P. Social force model for pedestrian dynamics. Physical review E, 51(5):4282–4286, 1995.]"
      },
      {
        "citation": "[Adelson, E. H. and Bergen, J. R. Spatiotemporal energy models for the perception of motion. J. Opt. Soc. Am, 2(2):284–299, 1985.]"
      },
      {
        "citation": "[Hochberg, Y. and Tamhane, A. C. Multiple comparison procedures. John Wiley & Sons, Inc., 1987.]"
      },
      {
        "citation": "[Hou, X. and Zhang, L. Saliency detection: A spectral residual approach. In Proc. IEEE CVPR’07, pages 8, 2007.]"
      },
      {
        "citation": "[Agarwal, G., Anbu, A., and Sinha, A. A fast algorithm to find the region-of-interest in the compressed MPEG domain. In Proc. IEEE ICME’03, volume 2, pages 133–136, 2003.]"
      },
      {
        "citation": "[Anstis, S. M. and Mackay, D. M. The perception of apparent movement [and discussion]. Philosophical Transactions of the Royal Society of London. B, Biological Sciences, 290(1038):153–168, 1980.]"
      },
      {
        "citation": "[Attneave, F. Informational aspects of visual perception. Psychological Review, 61:183–193, 1954.]"
      },
      {
        "citation": "[Barlow, H. Cerebral cortex as a model builder. In Models of the Visual Cortex, pages 37–46, 1985.]"
      },
      {
        "citation": "[Barlow, H. Redundancy reduction revisited. Network: Computation in Neural Systems, 12:241–253, 2001.]"
      },
      {
        "citation": "[Besag, J. Spatial interaction and the spatial analysis of lattice systems. Journal of the Royal Statistical Society. Series B, 36:192–236, 1974.]"
      }
    ],
    "author_details": [
      {
        "name": "Sayed Hossein Khatoonabadi",
        "affiliation": "Simon Fraser University",
        "email": "skhatoon@sfu.ca"
      },
      {
        "name": "Nuno Vasconcelos",
        "affiliation": "University of California, San Diego",
        "email": "nuno@uscd.edu"
      },
      {
        "name": "Ivan V. Bajic´",
        "affiliation": "Simon Fraser University",
        "email": "ibajic@ensc.sfu.ca"
      },
      {
        "name": "Yuifeng Shan",
        "affiliation": "Cisco Systems",
        "email": "yshan@cisco.com"
      }
    ]
  },
  {
    "title": "Action Recognition with Trajectory-Pooled Deep-Convolutional Descriptors\n---AUTHOR---\nLimin Wang\nYu Qiao\nXiaoou Tang",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Wang_Action_Recognition_With_2015_CVPR_paper.pdf",
    "id": "Wang_Action_Recognition_With_2015_CVPR_paper",
    "abstract": "This paper presents a new video representation, called trajectory-pooled deep-convolutional descriptor (TDD), which combines the advantages of both hand-crafted features and deep-learned features for human action recognition. TDD utilizes deep architectures to learn discriminative convolutional feature maps and employs trajectory-constrained pooling to aggregate these features into effective descriptors. Two normalization methods are designed to enhance robustness. Experimental results on HMD-B51 and UCF101 datasets demonstrate that TDDs outperform previous hand-crafted and deep-learned features, achieving state-of-the-art performance.\n\n---TOPICCS---\nHuman Action Recognition\nDeep Convolutional Descriptors\nTrajectory-Constrained Pooling\nFeature Normalization\nVideo Representation",
    "topics": [],
    "references": [
      {
        "citation": "[Aggarwal, J. K., & Ryoo, M. S. (2011). Human activity analysis: A review. ACM Comput. Surv., 43(3), 16.]"
      },
      {
        "citation": "[Bay, H., Tuytelaars, T., & Van Gool, L. J. (2006). SURF: Speeded up robust features. In ECCV.]"
      },
      {
        "citation": "[Cai, Z., Wang, L., Peng, X., & Qiao, Y. (2014). Multi-view super vector for action recognition. In CVPR.]"
      },
      {
        "citation": "[Chatfield, K., Simonyan, K., Vedaldi, A., & Zisserman, A. (2014). Return of the devil in the details: Delving deep into convolutional nets. In BMVC.]"
      },
      {
        "citation": "[Dalal, N., & Triggs, B. (2005). Histograms of oriented gradients for human detection. In CVPR.]"
      },
      {
        "citation": "[Deng, J., Dong, W., Socher, R., Li, L., Li, K., & Li, F. (2009). ImageNet: A large-scale hierarchical image database. In CVPR.]"
      },
      {
        "citation": "[Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). ImageNet classification with deep convolutional neural networks. In NIPS.]"
      },
      {
        "citation": "[Karpathy, A., Toderici, G., Shetty, S., Leung, T., Sukthankar, R., & Fei-Fei, L. (2014). Large-scale video classification with convolutional neural networks. In CVPR.]"
      },
      {
        "citation": "[Jiang, Y.-G., Liu, J., Roshan Zamir, A., Laptev, I., Piccardi, M., Shah, M., & Sukthankar, R. (2013). THUMOS challenge: Action recognition with a large number of classes.]"
      },
      {
        "citation": "[Kuehne, H., Jhuang, H., Garrote, E., Poggio, T., & Serre, T. (2011). HMDB: A large video database for human motion recognition. In ICCV.]"
      }
    ],
    "author_details": [
      {
        "name": "Limin Wang",
        "affiliation": "Department of Information Engineering, The Chinese University of Hong Kong",
        "email": "07wanglimin@gmail.com"
      },
      {
        "name": "Yu Qiao",
        "affiliation": "Shenzhen key lab of Comp. Vis. & Pat. Rec., Shenzhen Institutes of Advanced Technology, CAS, China",
        "email": "yu.qiao@siat.ac.cn"
      },
      {
        "name": "Xiaoou Tang",
        "affiliation": "Department of Information Engineering, The Chinese University of Hong Kong",
        "email": "xtang@ie.cuhk.edu.hk"
      }
    ]
  },
  {
    "title": "Deeply Learned Attributes for Crowded Scene Understanding\n---AUTHOR---\nJing Shao\nKai Kang\nChen Change Loy\nXiaogang Wang",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Shao_Deeply_Learned_Attributes_2015_CVPR_paper.pdf",
    "id": "Shao_Deeply_Learned_Attributes_2015_CVPR_paper",
    "abstract": "Crowded scene understanding is a fundamental problem in computer vision. In this study, we develop a multi-task deep model to jointly learn and combine appearance and motion features for crowd understanding. We propose crowd motion channels as the input of the deep model and the channel design is inspired by generic properties of crowd systems. To well demonstrate our deep model, we construct a new large-scale WWW Crowd dataset with 10, 000 videos from 8, 257 crowded scenes, and build an attribute set with 94 attributes on WWW. We further measure user study performance on WWW and compare this with the proposed deep models. Extensive experiments show that our deep models display significant performance improvements in cross-scene attribute recognition compared to strong crowd-related feature-based baselines, and the deeply learned features behave a superior performance in multi-task learning.",
    "topics": [
      "Crowded scene understanding",
      "Deep learning models",
      "Crowd motion channels",
      "WWW Crowd dataset",
      "Attribute recognition"
    ],
    "references": [
      {
        "citation": "[Ali, S. and Shah, M. A lagrangian particle dynamics approach for crowd ﬂow segmentation and stability analysis. In CVPR, 2007.]"
      },
      {
        "citation": "[Ali, S. and Shah, M. Floor ﬁelds for tracking in high density crowd scenes. In ECCV, 2008.]"
      },
      {
        "citation": "[Andrade, E. L., Blunsden, S., and Fisher, R. B. Modelling crowd scenes for event detection. In ICPR, 2006.]"
      },
      {
        "citation": "[Chan, A. B., Liang, Z.-S., and Vasconcelos, N. Privacy preserving crowd monitoring: Counting people without people models or tracking. In CVPR, 2008.]"
      },
      {
        "citation": "[Chan, A. B., and Vasconcelos, N. Modeling, clustering, and segmenting video with mixtures of dynamic textures. TPAMI, 2008.]"
      },
      {
        "citation": "[Dalal, N. and Triggs, B. Histograms of oriented gradients for human detection. In CVPR, 2005.]"
      },
      {
        "citation": "[Farhad, A., Endres, I., Hoiem, D., and Forsyth, D. Describing objects by their attributes. In CVPR, 2009.]"
      },
      {
        "citation": "[Fei-Fei, L., Iyer, A., Koch, C., and Perona, P. What do we perceive in a glance of a real-world scene? Journal of vision, 2007.]"
      },
      {
        "citation": "[Hospedales, T., Gong, S., and Xiang, T. A markov clustering topic model for mining behaviour in video. In CVPR, 2009.]"
      },
      {
        "citation": "[Kang, K. and Wang, X. Fully convolutional neural networks for crowd segmentation. arXiv preprint arXiv:1411.4464, 2014.]"
      }
    ],
    "author_details": [
      {
        "name": "Jing Shao",
        "affiliation": "Department of Electronic Engineering, The Chinese University of Hong Kong",
        "email": "jshao@ee.cuhk.edu.hk"
      },
      {
        "name": "Kai Kang",
        "affiliation": "Department of Electronic Engineering, The Chinese University of Hong Kong",
        "email": "kkang@ee.cuhk.edu.hk"
      },
      {
        "name": "Chen Change Loy",
        "affiliation": "Department of Information Engineering, The Chinese University of Hong Kong",
        "email": "ccloy@ie.cuhk.edu.hk"
      },
      {
        "name": "Xiaogang Wang",
        "affiliation": "Department of Electronic Engineering, The Chinese University of Hong Kong",
        "email": "xgwang@ee.cuhk.edu.hk"
      }
    ]
  },
  {
    "title": "Eye tracking assisted extraction of attentionally important objects from videos\n---AUTHOR---\nS. Karthikeyan\nThuyen Ngo\nMiguel Eckstein\nB.S. Manjunath",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Vadivel_Eye_Tracking_Assisted_2015_CVPR_paper.pdf",
    "id": "Vadivel_Eye_Tracking_Assisted_2015_CVPR_paper",
    "abstract": "This paper proposes an algorithm to extract objects that attract visual attention from videos. The algorithm leverages eye tracking data from multiple subjects to identify dominant visual tracks, which then guide a generic object search algorithm. A novel multiple object extraction algorithm is constructed using a spatio-temporal mixed graph and binary linear integer programming, followed by grabcut segmentation to refine object boundaries. The proposed technique outperforms state-of-the-art video segmentation using eye tracking prior and demonstrates favorable object extraction compared to methods without eye tracking data.\n\n---TOPICCS---\nEye Tracking\nObject Extraction\nVideo Segmentation\nVisual Attention\nBinary Linear Integer Programming",
    "topics": [],
    "references": [
      {
        "citation": "[Intriligator, J., & Cavanagh, P. (2001). The spatial resolution of visual attention. *Cognitive psychology, 43*(3), 171–216.]"
      },
      {
        "citation": "[Itti, L., Koch, C., & Niebur, E. (1998). A model of salience-based visual attention for rapid scene analysis. *Pattern Analysis and Machine Intelligence, IEEE Transactions on, 20*(11), 1254–1259.]"
      },
      {
        "citation": "[Judd, T., Ehinger, K., Durand, F., & Torralba, A. (2009). Learning to predict where humans look. *Computer Vision, 2009 IEEE 12th international conference on, pages 2106–2113.]"
      },
      {
        "citation": "[Karthikeyan, S., Delibaltov, D., Gaur, U., Jiang, M., Williams, D., & Manjunath, B. (2012). Uniﬁed probabilistic framework for simultaneous detection and tracking of multiple ob-jects with application to bio-image sequences. *Image Processing (ICIP), 2012 19th IEEE International Conference on, pages 1349–1352.]"
      },
      {
        "citation": "[Borji, A., & Itti, L. (2013). State-of-the-art in visual attention modeling. *Pattern Analysis and Machine Intelligence, IEEE Transactions on, 35*(1), 185–207.]"
      },
      {
        "citation": "[Borji, A., Sihite, D. N., & Itti, L. (2012). Salient object detection: A benchmark. *Computer Vision–ECCV 2012, pages 414–429.]"
      },
      {
        "citation": "[Karthikeyan, S., Jagadeesh, V., & Manjunath, B. (2013). Learning top-down scene context for visual attention modeling in natural images. *ICIP, IEEE, 2013.]"
      },
      {
        "citation": "[Eckstein, M. P. (2011). Visual search: A retrospective. *Journal of Vision, 11*(5), 14.]"
      },
      {
        "citation": "[Ehinger, K. A., Hidalgo-Sotelo, B., Torralba, A., & Oliva, A. (2009). Modelling search for people in 900 scenes: A combined source model of eye guidance. *Visual cognition, 17*(6-7), 945–978.]"
      },
      {
        "citation": "[Felzenszwalb, P., McAlleser, D., & Ramanan, D. (2008). A discriminatively trained, multiscale, deformable part model. *Computer Vision and Pattern Recognition, 2008. CVPR 2008. IEEE Conference on, pages 1–8.]"
      }
    ],
    "author_details": [
      {
        "name": "S. Karthikeyan",
        "affiliation": "University of California Santa Barbara",
        "email": "{karthikeyan, thuyen, manj}@ece.ucsb.edu"
      },
      {
        "name": "Thuyen Ngo",
        "affiliation": "University of California Santa Barbara",
        "email": "{karthikeyan, thuyen, manj}@ece.ucsb.edu"
      },
      {
        "name": "Miguel Eckstein",
        "affiliation": "University of California Santa Barbara",
        "email": "eckstein@psych.ucsb.edu"
      },
      {
        "name": "B.S. Manjunath",
        "affiliation": "University of California Santa Barbara",
        "email": "{karthikeyan, thuyen, manj}@ece.ucsb.edu"
      }
    ]
  },
  {
    "title": "Geo-semantic Segmentation\n---AUTHOR---\nShervin Ardeshir\n---AUTHOR---\nKoﬁ Malcolm Collins-Sibley\n---AUTHOR---\nMubarak Shah",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Ardeshir_Geo-Semantic_Segmentation_2015_CVPR_paper.pdf",
    "id": "Ardeshir_Geo-Semantic_Segmentation_2015_CVPR_paper",
    "abstract": "This paper proposes a method for geo-semantic segmentation that leverages Geographical Information System (GIS) databases to improve image segmentation and simultaneously refine the alignment of GIS projections. The method first segments an image into super-pixels and then projects GIS data (building and street locations) onto the image plane. Recognizing inaccuracies in these projections due to GPS errors and camera parameter inaccuracies, the paper introduces an iterative data fusion approach. This approach evaluates and weights projections based on reliability, fuses them with super-pixel segmentations, and iteratively refines the alignment of projections to the image content using random walks and global transformations. The goal is to create semantically segmented images with geo-references (addresses and geo-locations) for each segment.\n\n---TOPICCS---\nGeo-semantic Segmentation\nGIS Data Integration\nIterative Data Fusion\nSemantic Segmentation\nImage Alignment",
    "topics": [],
    "references": [
      {
        "citation": "[17] P. Zhao, T. Fang, J. Xiao, H. Zhang, Q. Zhao, and L. Quan. Rectilinear parsing of architecture in urban environment. In Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on, pages 342–349. IEEE, 2010."
      },
      {
        "citation": "[10] O. Teboul, L. Simon, P. Koutsourakis, and N. Paragios. Segmentation of building facades using procedural shape priors. In Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on, pages 3105–3112. IEEE, 2010."
      },
      {
        "citation": "[2] G. J. Brostow, J. Shotton, J. Fauqueur, and R. Cipolla. Segmentation and recognition using structure from motion point clouds. In Computer Vision–ECCV 2008, pages 44–57. Springer, 2008."
      },
      {
        "citation": "[3] X. He, R. S. Zemel, and M. Carreira-Perpindn. Multiscale conditional random ﬁelds for image labeling. In Computer vision and pattern recognition, 2004. CVPR 2004. Proceedings of the 2004 IEEE computer society conference on, volume 2, pages II–695. IEEE, 2004."
      },
      {
        "citation": "[7] M.-Y. Liu, O. Tuzel, S. Ramalingam, and R. Chellappa. Entropy rate superpixel segmentation. In Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on, pages 2097–2104. IEEE, 2011."
      },
      {
        "citation": "[8] P. M¨uller, P. Wonka, S. Haegler, A. Ulmer, and L. Van Gool. Procedural modeling of buildings, volume 25. ACM, 2006."
      },
      {
        "citation": "[9] P. Musialski, M. Wimmer, and P. Wonka. Interactive coherence-based fac¸ade modeling. In Computer Graphics Forum, volume 31, pages 661–670. Wiley Online Library, 2012."
      },
      {
        "citation": "[1] S. Ardeshir, A. R. Zamir, A. Torroella, and M. Shah. Gis-assisted object detection and geospatial localization. In European Conference on Computer Vision–ECCV 2014, pages 602-617. Springer."
      },
      {
        "citation": "[5] C. C. Lerma and J. Kosecka. Semantic segmentation of urban environments into object and background categories. Technical report, DTIC Document, 2013."
      },
      {
        "citation": "[4] D. Hoiem, A. Efroos, and M. Hebert. Automatic photo popup., 2005."
      }
    ],
    "author_details": [
      {
        "name": "Shervin Ardeshir",
        "affiliation": "University of Central Florida",
        "email": "ardeshir@cs.ucf.edu"
      },
      {
        "name": "Koﬁ Malcolm Collins-Sibley",
        "affiliation": "Northeastern University",
        "email": "collins-sibley.k@husky.neu.edu"
      },
      {
        "name": "Mubarak Shah",
        "affiliation": "University of Central Florida",
        "email": "shah@crcv.ucf.edu"
      }
    ]
  },
  {
    "title": "Bayesian Inference for Neighborhood Filters with Application in Denoising\n---AUTHOR---\nChao-Tsung Huang",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Huang_Bayesian_Inference_for_2015_CVPR_paper.pdf",
    "id": "Huang_Bayesian_Inference_for_2015_CVPR_paper",
    "abstract": "Range-weighted neighborhood filters are useful for edge-preserving denoising, but their theoretical understanding and parameter estimation have been limited. This paper introduces a unified empirical Bayesian framework to directly infer these filters and estimate their range variance. A neighborhood noise model is proposed to reason the Yaroslavsky, bilateral, and modified non-local means filters. An EM+ algorithm is devised to estimate the range variance via model fitting to empirical distributions. Experimental results on color-image denoising demonstrate the model's effectiveness in fitting noisy images, accurately estimating range variance, and improving image quality through a recursive fitting and filtering scheme. The framework is believed to be extensible to other range-weighted algorithms.\n\n---TOPICCS---\nBayesian Inference\nNeighborhood Filters\nImage Denoising\nRange Variance Estimation\nEmpirical Models",
    "topics": [],
    "references": [
      {
        "citation": "[Milanfar, P. A tour of modern image filtering: New insights and methods, both practical and theoretical. IEEE Signal Processing Magazine, 30(1):106–128, Jan 2013.]"
      },
      {
        "citation": "[Barash, D. and Comaniciu, D. A common framework for non-linear diffusion, adaptive smoothing, bilateral filtering and mean shift. Image and Vision Computing, 22(1):73–81, Jan 2004.]"
      },
      {
        "citation": "[Paris, S., Kornprobst, P., Tumbin, J., and Durand, F. Bilateral filtering: Theory and applications. Foundations and Trends in Computer Graphics and Vision, 4:1–73, 2008.]"
      },
      {
        "citation": "[Buades, A., Coll, B., and Morel, J. M. A review of image denoising algorithms, with a new one. SIAM Journal on Multi-scale Modeling and Simulation, 4(2):490–530, 2005.]"
      },
      {
        "citation": "[Chatterjee, P. and Milanfar, P. Patch-based near-optimal image denoising. IEEE Transactions on Image Processing, 21(4):1635–1649, Apr 2012.]"
      },
      {
        "citation": "[Peng, H. and Rao, H. Bilateral kernel parameter optimization by risk minimization. Proceedings of International Conference on Image Processing, 3293–3296, 2010.]"
      },
      {
        "citation": "[Peng, H., Rao, R., and Dianat, S. A. Multispectral image denoising with optimized vector bilateral filter. IEEE Transactions on Image Processing, 23(1):264–273, Jan 2014.]"
      },
      {
        "citation": "[Portilla, J., Strela, V., Wainwright, M. J., and Simoncelli, E. P. Image denoising using scale mixtures of gaussians in the wavelet domain. IEEE Transactions on Image Processing, 12(11):1338–1351, Nov 2003.]"
      },
      {
        "citation": "[Tomasi, C. and Manduchi, R. Bilateral filtering for gray and color images. Proceedings of International Conference on Computer Vision, 839–846, 1998.]"
      },
      {
        "citation": "[Elad, M. On the origin of the bilateral filter and ways to improve it. IEEE Transactions on Image Processing, 11(10):1141–1151, Oct 2002.]"
      }
    ],
    "author_details": [
      {
        "name": "Chao-Tsung Huang",
        "affiliation": "National Tsing Hua University, Taiwan",
        "email": "chaotsung@ee.nthu.edu.tw"
      }
    ]
  },
  {
    "title": "Deep Networks for Saliency Detection via Local Estimation and Global Search\n---AUTHOR---\nLijun Wang\n---AUTHOR---\nHuchuan Lu\n---AUTHOR---\nXiang Ruan\n---AUTHOR---\nMing-Hsuan Yang",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Wang_Deep_Networks_for_2015_CVPR_paper.pdf",
    "id": "Wang_Deep_Networks_for_2015_CVPR_paper",
    "abstract": "This paper presents a saliency detection algorithm by integrating both local estimation and global search. In the local estimation stage, we detect local saliency by using a deep neural network (DNN-L) which learns local patch features to determine the saliency value of each pixel. The estimated local saliency maps are further refined by exploring the high-level object concepts. In the global search stage, the local saliency map together with global contrast and geometric information are used as global features to describe a set of object candidate regions. Another deep neural network (DNN-G) is trained to predict the saliency score of each object region based on the global features. The final saliency map is generated by a weighted sum of salient object regions. Our method presents two interesting insights. First, local features learned by a supervised scheme can effectively capture local contrast, texture and shape information for saliency detection. Second, the complex relationship between different global saliency cues can be captured by deep networks and exploited principally rather than heuristically. Quantitative and qualitative experiments on several benchmark data sets demonstrate that our algorithm performs favorably against the state-of-the-art methods.",
    "topics": [
      "Saliency Detection",
      "Deep Neural Networks (DNN)",
      "Local Estimation",
      "Global Search",
      "Object Candidate Regions"
    ],
    "references": [
      {
        "citation": "[R. Achanta, S. Hemami, F. Estrada, and S. Susstrunk. Frequency-tuned salient region detection. In CVPR, pages 1597–1604, 2009.] - This paper likely provides foundational work on salient region detection, a core concept in the field."
      },
      {
        "citation": "[J. Carreira and C. Sminchisescu. Constrained parametric min-cuts for automatic object segmentation. In CVPR, pages 3241–3248, 2010.] - Object segmentation is closely related to salient object detection, and this paper presents a method using min-cuts."
      },
      {
        "citation": "[L. Itti, C. Koch, and E. Niebur. A model of saliency-based visual attention for rapid scene analysis. PAMI, 20(11):1254–1259, 1998.] - This is a seminal work establishing the computational model of visual salience."
      },
      {
        "citation": "[H. Jiang, J. Wang, Z. Yuan, Y. Wu, N. Zheng, and S. Li. Salient object detection: A discriminative regional feature integration approach. In CVPR, pages 2083–2090, 2013.] - A more recent paper focusing on salient object detection with a discriminative approach."
      },
      {
        "citation": "[B. Jiang, L. Zhang, H. Lu, C. Yang, and M.-H. Yang. Saliency detection via absorbing markov chain. In ICCV, pages 1665–1672, 2013.] - This paper introduces a Markov Chain approach to saliency detection."
      },
      {
        "citation": "[Q. Yan, L. Xu, J. Shi, and J. Jia. Hierarchical saliency detection. In CVPR, pages 1155–1162, 2013.] - Hierarchical approaches are common in computer vision, and this paper applies it to saliency detection."
      },
      {
        "citation": "[J. R. Uijlings, K. E. van de Sande, T. Gevers, and A. W. Smeulders. Selective search for object recognition. IJCV, 104(2):154–171, 2013.] - Selective search is a widely used technique for generating object proposals, often used as a preprocessing step in salient object detection pipelines."
      },
      {
        "citation": "[M.-M. Cheng, J. Warrell, W.-Y. Lin, S. Zheng, V. Vineet, and N. Crook. Efﬁcient salient region detection with soft image abstraction. In ICCV, pages 2013.] - This paper focuses on efficient computation of salient regions."
      },
      {
        "citation": "[B. Hariharan, P. Arbel´aez, R. Girshick, and J. Malik. Simultaneous detection and segmentation. In ECCV, pages 297–312. 2014.] - Simultaneous detection and segmentation is a related task, and this paper presents a method for it."
      },
      {
        "citation": "[Y. Xie and H. Lu. Visual saliency detection based on bayesian model. In ICIP, pages 645–648, 2011.] - Bayesian models are frequently used in computer vision, and this paper applies it to visual saliency detection."
      }
    ],
    "author_details": [
      {
        "name": "Lijun Wang",
        "affiliation": "Dalian University of Technology",
        "email": "*Not available in the text*"
      },
      {
        "name": "Huchuan Lu",
        "affiliation": "Dalian University of Technology",
        "email": "*Not available in the text*"
      },
      {
        "name": "Xiang Ruan",
        "affiliation": "OMRON Corporation",
        "email": "*Not available in the text*"
      },
      {
        "name": "Ming-Hsuan Yang",
        "affiliation": "University of California at Merced",
        "email": "*Not available in the text*"
      }
    ]
  },
  {
    "title": "Category-Speciﬁc Object Reconstruction from a Single Image\n---AUTHOR---\nAbhishek Kar\n---AUTHOR---\nShubham Tulsiani\n---AUTHOR---\nJo˜ao Carreira\n---AUTHOR---\nJitendra Malik",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Kar_Category-Specific_Object_Reconstruction_2015_CVPR_paper.pdf",
    "id": "Kar_Category-Specific_Object_Reconstruction_2015_CVPR_paper",
    "abstract": "Abstract not found",
    "topics": [],
    "references": [],
    "author_details": [
      {
        "name": "Abhishek Kar",
        "affiliation": "University of California, Berkeley",
        "email": "akar@eecs.berkeley.edu"
      },
      {
        "name": "Shubham Tulsiani",
        "affiliation": "University of California, Berkeley",
        "email": "shubhtuls@eecs.berkeley.edu"
      },
      {
        "name": "Jo˜ao Carreira",
        "affiliation": "University of California, Berkeley",
        "email": "carreira@eecs.berkeley.edu"
      },
      {
        "name": "Jitendra Malik",
        "affiliation": "University of California, Berkeley",
        "email": "malik@eecs.berkeley.edu"
      }
    ]
  },
  {
    "title": "Fusion Moves for Correlation Clustering\n---AUTHOR---\nThorsten Beier\nFred A. Hamprecht\nJörg H. Kappes",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Beier_Fusion_Moves_for_2015_CVPR_paper.pdf",
    "id": "Beier_Fusion_Moves_for_2015_CVPR_paper",
    "abstract": "Abstract not found",
    "topics": [],
    "references": [],
    "author_details": [
      {
        "name": "Thorsten Beier",
        "affiliation": "IWR, University of Heidelberg",
        "email": "thorsten.beier@iwr.uni-heidelberg.de"
      },
      {
        "name": "Fred A. Hamprecht",
        "affiliation": "IWR, University of Heidelberg",
        "email": "fred.hamprecht@iwr.uni-heidelberg.de"
      },
      {
        "name": "Jörg H. Kappes",
        "affiliation": "Math, University of Heidelberg",
        "email": "kappes@math.uni-heidelberg.de"
      }
    ]
  },
  {
    "title": "Laplacian Mixture Model\n---AUTHOR---\n[Authors not explicitly listed in the provided text]",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Klein_Associating_Neural_Word_2015_CVPR_supplemental.pdf",
    "id": "Klein_Associating_Neural_Word_2015_CVPR_supplemental",
    "abstract": "Abstract not found",
    "topics": [],
    "references": [],
    "author_details": []
  },
  {
    "title": "Learning a Non-linear Knowledge Transfer Model for Cross-View Action Recognition\n---AUTHOR---\nHossein Rahmani\nAjmal Mian",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Rahmani_Learning_a_Non-Linear_2015_CVPR_paper.pdf",
    "id": "Rahmani_Learning_a_Non-Linear_2015_CVPR_paper",
    "abstract": "Abstract not found",
    "topics": [],
    "references": [],
    "author_details": [
      {
        "name": "Hossein Rahmani",
        "affiliation": "Computer Science and Software Engineering, The University of Western Australia",
        "email": "hossein@csse.uwa.edu.au"
      },
      {
        "name": "Ajmal Mian",
        "affiliation": "Computer Science and Software Engineering, The University of Western Australia",
        "email": "ajmal.mian@uwa.edu.au"
      }
    ]
  },
  {
    "title": "On the (joint) convexity of Sparse Kernel MTL\n---AUTHOR---\nNot specified in the provided text.",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Ciliberto_Learning_Multiple_Visual_2015_CVPR_supplemental.pdf",
    "id": "Ciliberto_Learning_Multiple_Visual_2015_CVPR_supplemental",
    "abstract": "This paper investigates the convexity of Sparse Kernel Multi-task Learning (SKMTL) models and relates them to Convex Multi-task Cluster Learning. It establishes that the SKMTL problem, when restricted to functions of a specific form, is jointly convex in its optimization variables. Furthermore, it demonstrates how the framework can be used to recover clustered structures of tasks and provides empirical results on a robotics dataset (Sarcos) to illustrate the efficacy of the approach. The paper also explores the implications of the recovered sparse structure in settings beyond computer vision.",
    "topics": [
      "Sparse Kernel Multi-task Learning (SKMTL)",
      "Joint Convexity",
      "Cluster Multi-task Learning",
      "Laplacian Eigenmaps",
      "Robotics (Sarcos dataset)"
    ],
    "references": [],
    "author_details": []
  },
  {
    "title": "Robust Multiple Homography Estimation: An Ill-Solved Problem\n---AUTHOR---\nZygmunt L. Szpak\n---AUTHOR---\nWojciech Chojnacki\n---AUTHOR---\nAnton van den Hengel",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Szpak_Robust_Multiple_Homography_2015_CVPR_paper.pdf",
    "id": "Szpak_Robust_Multiple_Homography_2015_CVPR_paper",
    "abstract": "The paper argues that estimating multiple homographies between two views of a rigid scene, a task often assumed to be solved, is actually an \"ill-solved problem.\" The authors identify a critical oversight in existing methods: the failure to enforce consistency constraints that arise from the rigidity of the scene and the relationships between homographies. They derive new constraints that mutually compatible homographies must satisfy, demonstrate that prevailing methods fail to meet these constraints, and explain the resulting inconsistent epipolar geometries. The paper highlights the need for a new generation of robust multi-structure estimation methods capable of enforcing constraints on homography matrices and critiques existing approaches that rely on incomplete constraint satisfaction.",
    "topics": [
      "Multiple Homography Estimation",
      "Consistency Constraints",
      "Robust Multi-Structure Estimation",
      "Projective Geometry",
      "Epiopolar Geometry"
    ],
    "references": [
      {
        "citation": "[Baker, S., Datta, A., and Kanade, T. Parameterizing homographies. Tech. Rep. CMU-RI-TR-06-11, Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, 2006.]"
      },
      {
        "citation": "[Bartoli, A., and Sturm, P. Constrained structure and motion from multiple uncalibrated views of a piecewise planar scene. Int. J. Computer Vision, 52(1):45–64, 2003.]"
      },
      {
        "citation": "[Bernstein, D. S. Matrix Mathematics: Theory, Facts, and Formulas. Princeton University Press, Princeton, NJ, 2nd edition, 2009.]"
      },
      {
        "citation": "[Chen, P., and Suter, D. Rank constraints for homographies over two views: revisiting the rank four constraint. Int. J. Computer Vision, 81(2):205–225, 2009.]"
      },
      {
        "citation": "[Chojnacki, W., Szpak, Z., Brooks, M. J., and van den Hengel, A. Multiple homography estimation with full consistency constraints. In Proc. Int. Conf. Digital Image Computing: Techniques and Applications, pages 580–585, 2010.]"
      },
      {
        "citation": "[Chojnacki, W., and van den Hengel, A. A dimensionality result for multiple homography matrices. In Proc. 13th Int. Conf. Computer Vision, pages 2104–2109, 2011.]"
      },
      {
        "citation": "[Fouhey, D. F., Scharstein, D., and Briggs, A. J. Multiple plane detection in image pairs using J-linkage. In Proc. 20th Int. Conf. Pattern Recognition, pages 336–339, 2010.]"
      },
      {
        "citation": "[Goldberger, J. Reconstructing camera projection matrices from multiple pairwise overlapping views. Comput. Vis. Image Unders., 97(3):283–296, 2005.]"
      },
      {
        "citation": "[Hartley, R. I., and Zisserma, A. Multiple View Geometry in Computer Vision. Cambridge University Press, Cambridge, 2nd edition, 2004.]"
      },
      {
        "citation": "[Irving, R. S. Integers, Polynomials, and Rings: A Course in Algebra. Springer, New York, 2004.]"
      }
    ],
    "author_details": [
      {
        "name": "Zygmunt L. Szpak",
        "affiliation": "School of Computer Science, The University of Adelaide",
        "email": "zygmunt.szpak@adelaide.edu.au"
      },
      {
        "name": "Wojciech Chojnacki",
        "affiliation": "School of Computer Science, The University of Adelaide",
        "email": "wojciech.chojnacki@adelaide.edu.au"
      },
      {
        "name": "Anton van den Hengel",
        "affiliation": "School of Computer Science, The University of Adelaide",
        "email": "anton.vandenhengel@adelaide.edu.au"
      }
    ]
  },
  {
    "title": "Saturation-preserving Specular Reﬂection Separation\n---AUTHOR---\nYuanliu Liu\nZejian Yuan\nNanning Zheng\nYang Wu",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Liu_Saturation-Preserving_Specular_Reflection_2015_CVPR_paper.pdf",
    "id": "Liu_Saturation-Preserving_Specular_Reflection_2015_CVPR_paper",
    "abstract": "Specular reflection generally decreases the saturation of surface colors, potentially leading to confusion with other colors having the same hue but lower saturation. Traditional methods for specular reflection separation often suffer from this hue-saturation ambiguity, producing oversaturated specular-free images. This paper proposes a two-step approach to address this problem. First, an over-saturated specular-free image is produced through global chromaticity propagation from specular-free pixels to highlighted ones. Then, saturation is recovered based on priors of piecewise constancy of diffuse chromaticity and spatial sparsity/smoothness of specular reflection, achieved by increasing the achromatic component of diffuse chromaticity using linear programming. Experiments demonstrate the method's ability to separate specular reflection while preserving the saturation of underlying surface colors.",
    "topics": [
      "Specular Reflection Separation",
      "Hue-Saturation Ambiguity",
      "Chromaticity Propagation",
      "Diffuse Chromaticity",
      "Linear Programming"
    ],
    "references": [
      {
        "citation": "[Shafer, S. Using color to separate reflection components. Color Res. Appl., 10(4):210–218, 1985.] - This appears to be a foundational work in using color for reflection separation."
      },
      {
        "citation": "[Artusi, A., Banterle, F., & Chetverikov, D. A survey of specular removal methods. Computer Graphics Forum, 30(8):2208–2230, 3011.] - Provides a broad overview of existing techniques, useful for context."
      },
      {
        "citation": "[Bajcsy, R., Lee, S. W., & Leonardis, A. Detection of diffuse and specular interface reflections by color image segmentation. International Journal of Computer Vision, 17(3):249–272, 1996.] - Early work on separating diffuse and specular reflections."
      },
      {
        "citation": "[Gonzalez, R., & Woods, R. Digital Image Processing. Addison-Wesley, 1993.] - A standard reference for image processing fundamentals."
      },
      {
        "citation": "[Land, E. H., & McCann, J. J. Lightness and retinex theory. Journal of the Optical Society of America, 61(1):1–11, 1971.] - Introduces the retinex theory, relevant to color constancy and image understanding."
      },
      {
        "citation": "[Kim, H., Jin, H., Hadap, S., & Kweon, I. Specular reflection separation using dark channel prior. In IEEE Conference on Computer Vision and Pattern Recognition, 2013.] - A more recent work utilizing a specific technique (dark channel prior)."
      },
      {
        "citation": "[Mallick, S. P., Zickler, T., Belhumeur, P. N., & Kriegman, D. J. Beyond lambert: reconstructing specular surfaces using color. In CVPR, 2005.] - Explores reconstructing specular surfaces using color information."
      },
      {
        "citation": "[Lin, S., & Shum, H.-Y. Separation of diffuse and specular reflection in color images. In IEEE Conference on Computer Vision and Pattern Recognition, 2001.] - A key paper on separating diffuse and specular reflections."
      },
      {
        "citation": "[Tan, R. T., Nishino, K., & Ikeuchi, K. Color constancy through inverse intensity chromaticity space. JOSA A, 21(3):321–334, 2004.] - Addresses color constancy, a related problem."
      },
      {
        "citation": "[Mallick, S. P., Zickler, T., Kriegman, D. J., & Belhumeur, P. N. Specularity removal in images and videos: A PDE approach. In ECCV, pages 550–563, 2006.] - Presents a PDE approach to specular removal."
      },
      {
        "citation": "---"
      }
    ],
    "author_details": [
      {
        "name": "Yuanliu Liu",
        "affiliation": "Institute of Artificial Intelligence and Robotics, Xi’an Jiaotong University",
        "email": "liuyuanliu88@gmail.com"
      },
      {
        "name": "Zejian Yuan",
        "affiliation": "Institute of Artificial Intelligence and Robotics, Xi’an Jiaotong University",
        "email": "yuan.ze.jian@mail.xjtu.edu.cn"
      },
      {
        "name": "Nanning Zheng",
        "affiliation": "Institute of Artificial Intelligence and Robotics, Xi’an Jiaotong University",
        "email": "nnzheng@mail.xjtu.edu.cn"
      },
      {
        "name": "Yang Wu",
        "affiliation": "Center for Frontier Science and Technology, Nara Institute of Science and Technology",
        "email": "yangwu@rsc.naist.jp"
      }
    ]
  },
  {
    "title": "Photometric Stereo with Near Point Lighting: A Solution by Mesh Deformation\n---AUTHOR---\nWuyuan Xie\n---AUTHOR---\nChengkai Dai\n---AUTHOR---\nCharlie C. L. Wang",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Xie_Photometric_Stereo_With_2015_CVPR_paper.pdf",
    "id": "Xie_Photometric_Stereo_With_2015_CVPR_paper",
    "abstract": "This paper addresses the problem of photometric stereo (PS) under near point lighting conditions. Unlike conventional PS formulations that assume parallel lighting, near point lighting introduces a nonlinear relationship between local surface normals, camera distance, and light source positions. To solve this, a mesh deformation approach is developed, simultaneously determining facet position and orientation. The method decouples the deformation into local projection and global blending steps, achieving accurate surface shape estimation with few iterations and demonstrating robustness to light source position errors.\n\n---TOPICCS---\nPhotometric Stereo\nMesh Deformation\nNear Point Lighting\nNonlinear Optimization\nSurface Reconstruction",
    "topics": [],
    "references": [
      {
        "citation": "[S. Barsky and M. Petrou, The 4-source photometric stereo technique for three-dimensional surfaces in the presence of highlights and shadows, IEEE Transactions on Pattern Analysis and Machine Intelligence, 2003]"
      },
      {
        "citation": "[D. Nehab, S. Rusinkiewicz, J. Davis, and R. Ramamoorthi, Efficiently combining positions and normals for precise 3d geometry, ACM Trans. Graph., 2005]"
      },
      {
        "citation": "[A. Hertzmann and S.M. Seitz, Example-based photometric stereo: Shape reconstruction with general, varying brdfs, IEEE Transactions on Pattern Analysis and Machine Intelligence, 2005]"
      },
      {
        "citation": "[D.B. Goldman, B. Curless, A. Hertzmann, and S.M. Seitz, Shape and spatially-varying brdfs from photometric stereo, IEEE Transactions on Pattern Analysis and Machine Intelligence, 2010]"
      },
      {
        "citation": "[R.J. Woodham, Photometric method for determining surface orientation from multiple images, Optical engineering, 1980]"
      },
      {
        "citation": "[Fumihiko Sakaue and Jun Sato, A new approach of photometric stereo from linear image representation under close lighting, Computer Vision Workshops (ICCV Workshops), 2011]"
      },
      {
        "citation": "[Wetzler, A., Kimmel, R., Bruckstein, A.M., & Mecca, R., Close-range photometric stereo with point light sources, 3D Vision (3DV), 2014]"
      },
      {
        "citation": "[O. Sorkine and M. Alexa, As-rigid-as-possible surface modeling, Proceedings of the Fifth Eurographics Symposium on Geometry Processing, 2007]"
      },
      {
        "citation": "[S. Bouaziz, M. Deuss, Y. Schwartzburg, T. Weise, and M. Pauly, Shape-up: Shaping discrete geometry with projections, Compter Graphics Forum, 2012]"
      },
      {
        "citation": "[C.C.L. Wang and T. Chen, Thickening freeform surfaces for solid fabrication, Rapid Prototyping Journal, 2013]"
      }
    ],
    "author_details": [
      {
        "name": "Wuyuan Xie",
        "affiliation": "Department of Mechanical and Automation Engineering, The Chinese University of Hong Kong",
        "email": "Not available"
      },
      {
        "name": "Chengkai Dai",
        "affiliation": "Department of Mechanical and Automation Engineering, The Chinese University of Hong Kong",
        "email": "Not available"
      },
      {
        "name": "Charlie C. L. Wang",
        "affiliation": "Department of Mechanical and Automation Engineering, The Chinese University of Hong Kong",
        "email": "cwang@mae.cuhk.edu.hk"
      }
    ]
  },
  {
    "title": "Deep Hashing for Compact Binary Codes Learning\n---AUTHOR---\nVenice Erin Liong\nJiwen Lu\nGang Wang\nPierre Moulin\nJie Zhou",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Liong_Deep_Hashing_for_2015_CVPR_paper.pdf",
    "id": "Liong_Deep_Hashing_for_2015_CVPR_paper",
    "abstract": "In this paper, we propose a new deep hashing (DH) approach to learn compact binary codes for large scale visual search. Unlike existing binary codes learning methods which seek a single linear projection to map each sample into a binary vector, we develop a deep neural network to seek multiple hierarchical non-linear transformations to learn these binary codes, so that the nonlinear relationship of samples can be well exploited. Our model is learned under three constraints at the top layer of the deep network: 1) the loss between the original real-valued feature descriptor and the learned binary vector is minimized, 2) the binary codes distribute evenly on each bit, and 3) different bits are as independent as possible. To further improve the discriminative power of the learned binary codes, we extend DH into supervised DH (SDH) by including one discriminative term into the objective function of DH which simultaneously maximizes the inter-class variations and minimizes the intra-class variations of the learned binary codes. Experimental results show the superiority of the proposed approach over the state-of-the-arts.",
    "topics": [
      "Deep Hashing",
      "Binary Codes Learning",
      "Large-Scale Visual Search",
      "Non-Linear Transformations",
      "Hashing Functions"
    ],
    "references": [
      {
        "citation": "[Andoni, A., & Indyk, P. (2006). Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions. In *FOCS*, pages 459–468.]"
      },
      {
        "citation": "[Bengio, Y. (2009). Learning deep architectures for AI. *Foundations and trends R⃝ in Machine Learning*, *2*(1), 1–127.]"
      },
      {
        "citation": "[Gong, Y., Kumar, S., Verma, V., & Lazebnik, S. (2012). Angular quantization-based binary codes for fast similarity search. In *NIPS*, pages 1196–1204.]"
      },
      {
        "citation": "[Hinton, G. E., & Salakhutdinov, R. R. (2006). Reducing the dimensionality of data with neural networks. *Science*, *313*(5786), 504–507.]"
      },
      {
        "citation": "[Krizhevsky, A., & Hinton, G. (2009). Learning multiple layers of features from tiny images. Technical report, Univ. of Toronto.]"
      },
      {
        "citation": "[Liu, X., He, J., Lang, B., & Chang, S.-F. (2013). Hash bit selection: a unified solution for selection problems in hashing. In *CVPR*, pages 1570–1577.]"
      },
      {
        "citation": "[Norouzi, M., & Blei, D. M. (2011). Minimal loss hashing for compact binary codes. In *ICML*, pages 353–360.]"
      },
      {
        "citation": "[Raginsky, M., & Lazebnik, S. (2009). Locality-sensitive binary codes from shift-invariant kernels. In *NIPS*, pages 1509–1517.]"
      },
      {
        "citation": "[Torralba, A., Fergus, R., & Freeman, W. T. (2008). 80 million tiny images: A large data set for nonparametric object and scene recognition. *PAMI*, *30*(11), 1958–1970.]"
      },
      {
        "citation": "[Wang, J., Kumar, S., & Chang, S.-F. (2010). Semi-supervised hashing for scalable image retrieval. In *CVPR*, pages 3424–3431.]"
      }
    ],
    "author_details": [
      {
        "name": "Venice Erin Liong",
        "affiliation": "Advanced Digital Sciences Center",
        "email": "venice.l@adsc.com.sg"
      },
      {
        "name": "Jiwen Lu",
        "affiliation": "Advanced Digital Sciences Center",
        "email": "jiwen.lu@adsc.com.sg"
      },
      {
        "name": "Gang Wang",
        "affiliation": "School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore",
        "email": "wanggang@ntu.edu.sg"
      },
      {
        "name": "Pierre Moulin",
        "affiliation": "Department of ECE, University of Illinois at Urbana-Champaign, IL USA",
        "email": "moulin@ifp.uiuc.edu"
      },
      {
        "name": "Jie Zhou",
        "affiliation": "Department of Automation, Tsinghua University, Beijing, China",
        "email": "jzhou@tsinghua.edu.cn"
      }
    ]
  },
  {
    "title": "Unsupervised Simultaneous Orthogonal Basis Clustering Feature Selection\n---AUTHOR---\nDongyoon Han\n---AUTHOR---\nJunmo Kim",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Han_Unsupervised_Simultaneous_Orthogonal_2015_CVPR_paper.pdf",
    "id": "Han_Unsupervised_Simultaneous_Orthogonal_2015_CVPR_paper",
    "abstract": "A number of feature selection methods have been proposed and classified as supervised and unsupervised feature selection methods. In this paper, we propose a novel unsupervised feature selection method: Simultaneous Orthogonal basis Clustering Feature Selection (SOCFS). To perform feature selection on unlabeled data effectively, a regularized regression-based formulation with a new type of target matrix is designed. The target matrix captures latent cluster centers of the projected data points by performing orthogonal basis clustering, and then guides the projection matrix to select discriminative features. Experimental results demonstrate the effectiveness of SOCFS achieving the state-of-the-art results with diverse real world datasets.\n\n---TOPIC---\nUnsupervised Feature Selection\n---TOPIC---\nOrthogonal Basis Clustering\n---TOPIC---\nRegularized Regression\n---TOPIC---\nLatent Cluster Centers\n---TOPIC---\nSimultaneous Feature Selection and Clustering",
    "topics": [],
    "references": [
      {
        "citation": "[Belkin, M. and Niyogi, P. Laplacian eigenmaps and spectral techniques for embedding and clustering. In NIPS, volume 14, pages 585–591, 2001.]"
      },
      {
        "citation": "[Lee, D. D. and Seung, H. S. Algorithms for non-negative matrix factorization. In NIPS, pages 556–562, 2001.]"
      },
      {
        "citation": "[Viklands, T. Algorithms for the weighted orthogonal Procustes problem and other least squares problems. PhD thesis, Ume˚a University, 2006.]"
      },
      {
        "citation": "[Nie, F., Huang, H., Cai, X., and Ding, C. H. Efﬁcient and robust feature selection via joint l2,1-norms minimization. In NIPS, pages 1813–1821, 2010.]"
      },
      {
        "citation": "[Nene, S. A., Nayar, S. K., and Murase, H. Columbia object image library (coil-20). Technical report, CUCS-005-96, 1996.]"
      },
      {
        "citation": "[Yang, Y., Shen, H. T., Ma, Z., Huang, Z., and Zhou, X. l2,1-norm regularized discriminative feature selection for unsupervised learning. In IJCAI, pages 1589–1594, 2011.]"
      },
      {
        "citation": "[Qian, M. and Zhai, C. Robust unsupervised feature selection. In IJCAI, pages 1621–1627, 2013.]"
      },
      {
        "citation": "[Sch¨onemann, P. H. A generalized solution of the orthogonal Procrustes problem. Psychometrika, 31(1):1–10, 1966.]"
      },
      {
        "citation": "[Zhao, Z. and Liu, H. Spectral feature selection for supervised and unsupervised learning. In ICML, pages 1151–1157, 2007.]"
      },
      {
        "citation": "[Samaria, F. S. and Harter, A. C. Parameterisation of a stochastic model for human face identiﬁcation. In IEEE Workshop on Applications of Computer Vision, pages 138–142, 1994.]"
      }
    ],
    "author_details": [
      {
        "name": "Dongyoon Han",
        "affiliation": "School of Electrical Engineering, KAIST",
        "email": "dyhan@kaist.ac.kr"
      },
      {
        "name": "Junmo Kim",
        "affiliation": "School of Electrical Engineering, KAIST",
        "email": "junmo.kim@kaist.ac.kr"
      }
    ]
  },
  {
    "title": "Salient Object Subitizing\n---AUTHOR---\nJianming Zhang\nShugao Ma\nMehrnoosh Sameki\nStan Sclaroff\nMargrit Betke\nZhe Lin\nXiaohui Shen\nBrian Price\nRadom´ır M˘ech",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Zhang_Salient_Object_Subitizing_2015_CVPR_paper.pdf",
    "id": "Zhang_Salient_Object_Subitizing_2015_CVPR_paper",
    "abstract": "People can quickly and accurately identify the number of items in an image (subitizing). This paper explores \"Salient Object Subitizing (SOS),\" aiming to predict the existence and number of salient objects in a scene using holistic cues. A new dataset annotated through crowdsourcing is introduced, and a CNN-based technique is proposed. The technique achieves high accuracy in detecting salient objects and predicting their number (1, 2, 3, or 4+), without object localization. The method's utility is demonstrated in salient object detection and object proposal applications.\n\n---TOPIC---\nSalient Object Subitizing (SOS)\n---TOPIC---\nConvolutional Neural Networks (CNNs)\n---TOPIC---\nCrowdsourcing and Dataset Creation\n---TOPIC---\nHolistic Image Analysis\n---TOPIC---\nComputer Vision Applications (object detection, robot vision)",
    "topics": [],
    "references": [],
    "author_details": [
      {
        "name": "Jianming Zhang",
        "affiliation": "Boston University",
        "email": "[Not available in paper]"
      },
      {
        "name": "Shugao Ma",
        "affiliation": "Boston University",
        "email": "[Not available in paper]"
      },
      {
        "name": "Mehrnoosh Sameki",
        "affiliation": "Boston University",
        "email": "[Not available in paper]"
      },
      {
        "name": "Stan Sclaroff",
        "affiliation": "Boston University",
        "email": "[Not available in paper]"
      },
      {
        "name": "Margrit Betke",
        "affiliation": "Boston University",
        "email": "[Not available in paper]"
      },
      {
        "name": "Zhe Lin",
        "affiliation": "Adobe Research",
        "email": "[Not available in paper]"
      },
      {
        "name": "Xiaohui Shen",
        "affiliation": "Adobe Research",
        "email": "[Not available in paper]"
      },
      {
        "name": "Brian Price",
        "affiliation": "Adobe Research",
        "email": "[Not available in paper]"
      },
      {
        "name": "Radom´ır M˘ech",
        "affiliation": "Adobe Research",
        "email": "[Not available in paper]"
      }
    ]
  },
  {
    "title": "Discriminative Shape from Shading in Uncalibrated Illumination\n---AUTHOR---\nStephan R. Richter\nStefan Roth",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Richter_Discriminative_Shape_From_2015_CVPR_paper.pdf",
    "id": "Richter_Discriminative_Shape_From_2015_CVPR_paper",
    "abstract": "Estimating surface normals from a single image is a challenging, under-constrained problem. Previous work often relied on simplifying assumptions (e.g., directional lighting, known reflectance maps), limiting applicability. This paper addresses these limitations with a discriminative learning approach to shape from shading, enabling generalization to uncalibrated illumination. The approach utilizes regression forests with Von Mises-Fisher distributions and incorporates spatial features, including textons and novel silhouette features, for efficient pixel-independent prediction and robust reﬂectance map estimation. Experiments demonstrate superior performance compared to state-of-the-art methods on both synthetic and real-world datasets.\n\n---TOPICCS---\nShape from Shading\nDiscriminative Learning\nSurface Normals Estimation\nSilhouette Features\nUncalibrated Illumination",
    "topics": [],
    "references": [
      {
        "citation": "[J. T. Barron and J. Malik. Color constancy, intrinsic images, and shape estimation. In ECCV, 2012. 2, 5, 6, 7, 8]"
      },
      {
        "citation": "[J. T. Barron and J. Malik. Shape, albedo, and illumination from a single image of an unknown object. In CVPR, 2012. 2, 3]"
      },
      {
        "citation": "[J. Ben-Arie and D. Nandy. A neural network approach for reconstructing surface shape from shading. In ICIP, 1998. 2, 3]"
      },
      {
        "citation": "[L. Breiman. Random forests. Machine Learning, 45(1):5–32, 2001. 3, 4]"
      },
      {
        "citation": "[F. Cole, P. Isola, W. T. Freeman, F. Durand, and E. H. Adelson. ShapeCollage: Occlusion-aware, example-based shape interpretation. In ECCV, 2012. 2, 3, 2, 3, 4]"
      },
      {
        "citation": "[M. Cosini, M. Dellepiane, F. Ponchio, and R. Scopigno. Image-to-geometry registration: A mutual information method exploiting illumination-related geometric properties. Computer Graphics Forum, 28(7):1755–1764, 2009. 8]"
      },
      {
        "citation": "[I. S. Dhillon and S. Sra. Modeling data using directional distributions. Technical report, TR-03-06, Department of Computer Sciences, The University of Texas at Austin, 2003. 3]"
      },
      {
        "citation": "[R. Fisher. Dispersion on a sphere. P. Roy. Soc. Lond. B, 217(1130), 1953. 3]"
      },
      {
        "citation": "[S. Fuhrmann and M. Goesele. Floating scale reconstruction. In SIGGRAPH, 2014. 8]"
      },
      {
        "citation": "[R. Grosse, M. K. Johnson, E. H. Adelson, and W. T. Freeman. Ground truth dataset and baseline evaluations for intrinsic image algorithms. In ICCV, 2009. 2, 5, 6, 7]"
      }
    ],
    "author_details": [
      {
        "name": "Stephan R. Richter",
        "affiliation": "Department of Computer Science, TU Darmstadt",
        "email": "[Not available in the provided text]"
      },
      {
        "name": "Stefan Roth",
        "affiliation": "Department of Computer Science, TU Darmstadt",
        "email": "[Not available in the provided text]"
      }
    ]
  },
  {
    "title": "Line Drawing Interpretation in a Multi-View Context\n---AUTHOR---\nJean-Dominique FAVREAU\nFlorent LAFARGE\nAdrien Bousseau",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Favreau_Line_Drawing_Interpretation_2015_CVPR_paper.pdf",
    "id": "Favreau_Line_Drawing_Interpretation_2015_CVPR_paper",
    "abstract": "Our work addresses the challenge of interpreting line drawings of imaginary objects drawn over photographs of an existing scene. Existing computer vision algorithms offer limited support for such tasks, as multi-view stereo algorithms reconstruct real-world scenes while line-drawing interpretation algorithms often lack contextual awareness. We propose an algorithm that combines the strengths of these two domains, leveraging the dominant orientations of the existing scene to guide the interpretation of the line drawing. Our algorithm assigns each polygon in the drawing to either an existing orientation or a new, unknown orientation, allowing for the creation of new structures not present in the real world. We demonstrate the flexibility of our approach with examples from architecture, furniture design, and archaeology.\n\n---TOPICICS---\nLine Drawing Interpretation\nMulti-View Stereo Reconstruction\nContext-Driven Regularization\n3D Scene Understanding\nComputer-Aided Design (CAD)",
    "topics": [],
    "references": [
      {
        "citation": "[M. Arikan, M. Schwärzler, S. Flöry, M. Wimmer, and S. Maierhofer. O-snap: Optimization-based snapping for modeling architecture. Trans. on Graphics, 32(1), 2013.]"
      },
      {
        "citation": "[H. Barrow and J. Tenenbaum. Interpreting line drawings as three-dimensional surfaces. Artificial Intelligence, 17, 1981.]"
      },
      {
        "citation": "[Y. Boykov and V. Kolmogorov. An experimental comparison of min-cut/max-flow algorithms for energy minimization in vision. PAMI, 26(9), 2004.]"
      },
      {
        "citation": "[A.-L. Chauve, P. Labatut, and J.-P. Pons. Robust piecewise-planar 3d reconstruction and completion from large-scale unstructured point data. In CVPR, 2010.]"
      },
      {
        "citation": "[J. Chen and B. Chen. Architectural modeling from sparsely scanned range data. IJCV, 78(2-3), 2008.]"
      },
      {
        "citation": "[M. Cooper. Line Drawing Interpretation. Springer, 2008.]"
      },
      {
        "citation": "[P. E. Debevec, C. J. Taylor, and J. Malik. Modeling and rendering architecture from photographs: A hybrid geometry- and image-based approach. In SIGGRAPH, 1996.]"
      },
      {
        "citation": "[X. Descombes. Stochastic geometry for image analysis. Wiley-ISTE, 2011.]"
      },
      {
        "citation": "[K. Eissen and R. Steur. Sketching: The Basics. Bis Publishers, 2011.]"
      },
      {
        "citation": "[Y. Furukawa, B. Curless, S. Seitz, and R. Szeliski. Manhattan-world stereo. In CVPR, 2009.]"
      }
    ],
    "author_details": [
      {
        "name": "Jean-Dominique FAVREAU",
        "affiliation": "INRIA Sophia-Antipolis, France",
        "email": "jean-dominique.favreau@inria.fr"
      },
      {
        "name": "Florent LAFARGE",
        "affiliation": "INRIA Sophia-Antipolis, France",
        "email": "florent.lafarge@inria.fr"
      },
      {
        "name": "Adrien Bousseau",
        "affiliation": "INRIA Sophia-Antipolis, France",
        "email": "adrien.bousseau@inria.fr"
      }
    ]
  },
  {
    "title": "Best of both worlds: human-machine collaboration for object annotation\n---AUTHOR---\nOlga Russakovsky\nLi-Jia Li\nLi Fei-Fei",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Russakovsky_Best_of_Both_2015_CVPR_paper.pdf",
    "id": "Russakovsky_Best_of_Both_2015_CVPR_paper",
    "abstract": "The long-standing goal of localizing every object in an image remains elusive. Manually annotating objects is quite expensive despite crowd engineering innovations. Current state-of-the-art automatic object detectors can accurately detect at most a few objects per image. This paper brings together the latest advancements in object detection and in crowd engineering into a principled framework for accurately and efficiently localizing objects in images. The input to the system is an image to annotate and a set of annotation constraints: desired precision, utility and/or human cost of the labeling. The output is a set of object annotations, informed by human feedback and computer vision. Our model seamlessly integrates multiple computer vision models with multiple sources of human input in a Markov Decision Process. We empirically validate the effectiveness of our human-in-the-loop labeling approach on the ILSVRC2014 object detection dataset.",
    "topics": [
      "Object Detection",
      "Human-Machine Collaboration",
      "Crowd Engineering",
      "Markov Decision Process",
      "Image Annotation"
    ],
    "references": [],
    "author_details": [
      {
        "name": "Olga Russakovsky",
        "affiliation": "Stanford University",
        "email": "[Email not available in text]"
      },
      {
        "name": "Li-Jia Li",
        "affiliation": "Snapchat",
        "email": "[Email not available in text]"
      },
      {
        "name": "Li Fei-Fei",
        "affiliation": "Stanford University",
        "email": "[Email not available in text]"
      }
    ]
  },
  {
    "title": "On Pairwise Costs for Network Flow Multi-Object Tracking\n---AUTHOR---\nVisesh Chari\nSimon Lacoste-Julien\nIvan Laptev\nJosef Sivic",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Chari_On_Pairwise_Costs_2015_CVPR_supplemental.pdf",
    "id": "Chari_On_Pairwise_Costs_2015_CVPR_supplemental",
    "abstract": "Multi-object tracking has recently been approached using min-cost network flow optimization techniques, which enable modeling of dependencies among tracks. This paper addresses the issue of object detector failures due to occlusions and clutter by introducing pairwise costs to the min-cost network flow framework. The authors design a convex relaxation solution with an efficient rounding heuristic, empirically providing certificates of small suboptimality. They evaluate two types of pairwise costs and demonstrate improvements over recent tracking methods in real-world video sequences.",
    "topics": [
      "Multi-object Tracking",
      "Network Flow Optimization",
      "Pairwise Costs",
      "Convex Relaxation",
      "Tracking-by-Detection"
    ],
    "references": [],
    "author_details": [
      {
        "name": "Visesh Chari",
        "affiliation": "INRIA and Ecole Normale Sup´erieure, Paris, France",
        "email": "Not available"
      },
      {
        "name": "Simon Lacoste-Julien",
        "affiliation": "INRIA and Ecole Normale Sup´erieure, Paris, France",
        "email": "Not available"
      },
      {
        "name": "Ivan Laptev",
        "affiliation": "INRIA and Ecole Normale Sup´erieure, Paris, France",
        "email": "Not available"
      },
      {
        "name": "Josef Sivic",
        "affiliation": "INRIA and Ecole Normale Sup´erieure, Paris, France",
        "email": "Not available"
      }
    ]
  },
  {
    "title": "TILDE: A Temporally Invariant Learned DEtector\n---AUTHORs---\nYannick Verdie\nKwang Moo Yi\nPascal Fua\nVincent Lepetit",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Verdie_TILDE_A_Temporally_2015_CVPR_paper.pdf",
    "id": "Verdie_TILDE_A_Temporally_2015_CVPR_paper",
    "abstract": "Abstract not found",
    "topics": [],
    "references": [
      {
        "citation": "[Cortes, C., & Vapnik, V. (1995). Support-Vector Networks. *Machine Learning*, *20*(3), 273–297.]"
      },
      {
        "citation": "[Harris, C., & Stephens, M. (1988). A Combined Corner and Edge Detector. In *Fourth Alvey Vision Conference*.]"
      },
      {
        "citation": "[Bay, H., Ess, A., Tuytelaars, T., & Van Gool, L. (2008). SURF: Speeded Up Robust Features. *Computer Vision and Image Understanding*, *10(3)*, 346–359.]"
      },
      {
        "citation": "[Breiman, L. (1993). Hinging Hyperplanes for Regression, Classification, and Function Approximation. *IEEE Transactions on Information Theory*, *39*(3), 999–1013.]"
      },
      {
        "citation": "[LeCun, Y., Bottou, L., Bengio, Y., & Haffner, P. (1998). Gradient-Based Learning Applied to Document Recognition. *Proceedings of the IEEE*, 1998.]"
      },
      {
        "citation": "[Mikolajczyk, K., Tuytelaars, T., Schmid, C., Zisserma, A., Matas, J., Schaffalitzky, F., Kadir, T., & Van Gool, L. (2005). A Comparison of Afﬁne Region Detectors. *International Journal of Computer Vision*, *65*(1/2), 43–72.]"
      },
      {
        "citation": "[Dollar, P., Tu, Z., & Belongie, S. (2006). Supervised Learning of Edges and Object Boundaries. In *Conference on Computer Vision and Pattern Recognition*.]"
      },
      {
        "citation": "[Rosten, E., & Drummond, T. (2006). Machine Learning for High-Speed Corner Detection. In *European Conference on Computer Vision*.]"
      },
      {
        "citation": "[Lowe, D. (2004). Distinctive Image Features from Scale-Invariant Keypoints. *International Journal of Computer Vision*, *20*(2).]"
      },
      {
        "citation": "[Fan, R.-E., Chang, K.-W., Hsieh, C.-J., Wang, X.-R., & Lin, C.-J. (2008). LIBLINEAR: A Library for Large Linear Classiﬁcation. *Journal of Machine Learning Research*, *9*, 1871–1874.]"
      },
      {
        "citation": "---"
      }
    ],
    "author_details": [
      {
        "name": "Yannick Verdie",
        "affiliation": "Computer Vision Laboratory, ´Ecole Polytechnique F´ed´erale de Lausanne (EPFL)",
        "email": "yannick.verdie@epfl.ch"
      },
      {
        "name": "Kwang Moo Yi",
        "affiliation": "Computer Vision Laboratory, ´Ecole Polytechnique F´ed´erale de Lausanne (EPFL)",
        "email": "kwang.yi@epfl.ch"
      },
      {
        "name": "Pascal Fua",
        "affiliation": "Computer Vision Laboratory, ´Ecole Polytechnique F´ed´erale de Lausanne (EPFL)",
        "email": "pascal.fua@epfl.ch"
      },
      {
        "name": "Vincent Lepetit",
        "affiliation": "Institute for Computer Graphics and Vision, Graz University of Technology",
        "email": "lepetit@icg.tugraz.at"
      }
    ]
  },
  {
    "title": "JOTS: Joint Online Tracking and Segmentation\n---AUTHOR---\nLongyin Wen\nDawei Du\nZhen Lei\nStan Z. Li\nMing-Hsuan Yang",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Wen_JOTS_Joint_Online_2015_CVPR_paper.pdf",
    "id": "Wen_JOTS_Joint_Online_2015_CVPR_paper",
    "abstract": "This paper presents a novel Joint Online Tracking and Segmentation (JOTS) algorithm that integrates multi-part tracking and segmentation into a unified energy optimization framework to address the video segmentation task. The algorithm formulates video segmentation as online multi-part tracking and segmentation within a unified energy function. The tracking and segmentation stages are optimized iteratively using a RANSAC-style approach. Extensive experiments on the SegTrack and SegTrack v2 databases demonstrate the effectiveness of the proposed method against state-of-the-art approaches.",
    "topics": [
      "Joint Online Tracking and Segmentation",
      "Multi-part Models",
      "Energy Function Optimization",
      "Video Segmentation",
      "Iterative Refinement"
    ],
    "references": [
      {
        "citation": "[R. Achanta, A. Shaji, K. Smith, A. Lucchi, P. Fua, and S. Ssstrunk. SLIC Superpixels. Technical report, 2010.] - Frequently cited (5 times) and foundational for superpixel-based approaches."
      },
      {
        "citation": "[Y. Boykov, O. Veksler, and R. Zabih. Fast approximate energy minimization via graph cuts. TPAMI, 23(11):1222–1239, 2001.] - A core algorithm frequently used in many of the cited works."
      },
      {
        "citation": "[C. Rother, V. Kolmogorov, and A. Blake. ”GrabCut”: Interactive foreground extraction using iterated graph cuts. ACM Trans. Graph., 23(3):309–314, 2004.] - A classic interactive segmentation technique."
      },
      {
        "citation": "[T. Brox and J. Malik. Large displacement optical ﬂow: Descriptor matching in variational motion estimation. TPAMI, 33(3):500–513, 2011.] - Relevant for motion estimation, a key component of video analysis."
      },
      {
        "citation": "[A. Andriyenko, K. Schindler, and S. Roth. Discrete-continuous optimization for multi-target tracking. In CVPR, pages 1926–1933, 2012.] - Addresses multi-target tracking, a common video analysis task."
      },
      {
        "citation": "[J. Chang and J. W. Fisher, III. Topology-constrained layered tracking with latent ﬂow. In ICCV, 2013.] - Deals with tracking and incorporates topological constraints."
      },
      {
        "citation": "[M. Godec, P. M. Roth, and H. Bischof. Hough-based tracking of non-rigid objects. In ICCV, pages 81–88, 2011.] - Focuses on tracking non-rigid objects."
      },
      {
        "citation": "[Z. Cai, L. Wen, Z. Lei, N. Vasconcelos, and S. Z. Li. Robust deformable and occluded object tracking with dynamic graph. TIP, 23(12):5497–5509, 2014.] - Addresses a challenging problem: tracking deformable and occluded objects."
      },
      {
        "citation": "[A. Delong, A. Osokin, H. N. Isack, and Y. Boykov. Fast approximate energy minimization with label costs. IJCV, 96(1):1–27, 2012.] - Provides an optimization method relevant to many of the cited approaches."
      },
      {
        "citation": "[J. Chang and J. W. Fisher, III. Topology-constrained layered tracking with latent ﬂow. In ICCV, 2013.] - Deals with tracking and incorporates topological constraints."
      }
    ],
    "author_details": [
      {
        "name": "Longyin Wen",
        "affiliation": "NLPR, Institute of Automation, Chinese Academy of Sciences, Beijing, CHN.",
        "email": "lywen@nlpr.ia.ac.cn"
      },
      {
        "name": "Dawei Du",
        "affiliation": "SCCE, University of Chinese Academy of Sciences, Beijing, CHN.",
        "email": "dawei.du@vcipl.ict.ac.cn"
      },
      {
        "name": "Zhen Lei",
        "affiliation": "NLPR, Institute of Automation, Chinese Academy of Sciences, Beijing, CHN.",
        "email": "zlei@nlpr.ia.ac.cn"
      },
      {
        "name": "Stan Z. Li",
        "affiliation": "NLPR, Institute of Automation, Chinese Academy of Sciences, Beijing, CHN.",
        "email": "szli@nlpr.ia.ac.cn"
      },
      {
        "name": "Ming-Hsuan Yang",
        "affiliation": "School of Engineering, University of California at Merced, USA.",
        "email": "myang@ucmerced.edu"
      }
    ]
  },
  {
    "title": "Learning to Propose Objects\n---AUTHOR---\nPhilipp Krähenbühl\nVladlen Koltun",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Krahenbuhl_Learning_to_Propose_2015_CVPR_paper.pdf",
    "id": "Krahenbuhl_Learning_to_Propose_2015_CVPR_paper",
    "abstract": "We present an approach for highly accurate bottom-up object segmentation. Given an image, the approach rapidly generates a set of regions that delineate candidate objects in the image. The key idea is to train an ensemble of figure-ground segmentation models. The ensemble is trained jointly, enabling individual models to specialize and complement each other. We reduce ensemble training to a sequence of uncapacitated facility location problems and show that highly accurate segmentation ensembles can be trained by combinatorial optimization. The training procedure jointly optimizes the size of the ensemble, its composition, and the parameters of incorporated models, all for the same objective. The ensembles operate on elementary image features, enabling rapid image analysis. Extensive experiments demonstrate that the presented approach outperforms prior object proposal algorithms by a significant margin, while having the lowest running time. The trained ensembles generalize across datasets, indicating that the presented approach is capable of learning a generally applicable model of bottom-up segmentation.\n\n---TOPICCS---\nObject Segmentation\nEnsemble Methods\nCombinatorial Optimization\nObject Proposals\nFigure-Ground Segmentation",
    "topics": [],
    "references": [
      {
        "citation": "[B. Alexe, T. Deselaers, and V. Ferrari. Measuring the object-ness of image windows. PAMI, 34(11), 2012.]"
      },
      {
        "citation": "[P. Arbeláez, B. Hariharan, C. Gu, S. Gupta, L. D. Bourdev, and J. Malik. Semantic segmentation using regions and parts. In CVPR, 2012.]"
      },
      {
        "citation": "[T. Lin, M. Maire, S. Belongie, J. Hays, P. P. Perona, D. Ramanan, P. Dollár, and C. L. Zitnick. Microsoft COCO: Common objects in context. In ECCV, 2014.]"
      },
      {
        "citation": "[P. Arbeláez, J. Pont-Tuset, J. T. Barron, F. Marqués, and J. Malik. Multiscale combinatorial grouping. In CVPR, 2014.]"
      },
      {
        "citation": "[J. Carreira, R. Caseiro, J. Batista, and C. Sminchisescu. Free-form region description with second-order pooling. PAMI, 2015.]"
      },
      {
        "citation": "[Y. Boykov, O. Vekler, and R. Zabih. Fast approximate energy minimization via graph cuts. PAMI, 23(11), 2001.]"
      },
      {
        "citation": "[P. Dollár and C. L. Zitnick. Structured forests for fast edge detection. In ICCV, 2013.]"
      },
      {
        "citation": "[M.-M. Cheng, Z. Zhang, W.-Y. Lin, and P. H. S. Torr. BING: Binarized normed gradients for objectness estimation at 300fps. In CVPR, 2014.]"
      },
      {
        "citation": "[N. Dalal and B. Triggs. Histograms of oriented gradients for human detection. In CVPR, 2005.]"
      },
      {
        "citation": "[M. Deza and M. Laurent. Geometry of cuts and metrics. Springer, 1997.]"
      }
    ],
    "author_details": [
      {
        "name": "Philipp Krähenbühl",
        "affiliation": "UC Berkeley",
        "email": "Not available in the provided text."
      },
      {
        "name": "Vladlen Koltun",
        "affiliation": "Intel Labs",
        "email": "Not available in the provided text."
      }
    ]
  },
  {
    "title": "Understanding Deep Image Representations by Inverting Them\n---AUTHORs---\nAravindh Mahendran\nAndrea Vedaldi",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Mahendran_Understanding_Deep_Image_2015_CVPR_paper.pdf",
    "id": "Mahendran_Understanding_Deep_Image_2015_CVPR_paper",
    "abstract": "Image representations are crucial for image understanding systems, yet our understanding of them remains limited. This paper presents a general framework to invert image representations, including SIFT, HOG, and CNNs, by attempting to reconstruct an image from its encoding. The authors demonstrate that this method can invert HOG more accurately than recent alternatives and apply it to analyze state-of-the-art CNN image representations for the first time. Their findings reveal that several layers in CNNs retain photographically accurate information, exhibiting varying degrees of geometric and photometric invariance. The approach uses only the image representation and a natural image prior, avoiding reliance on auxiliary information.\n\n---TOPIC---\nImage Representations\n---TOPI---\nRepresentation Inversion\n---TOPI---\nConvolutional Neural Networks (CNNs)\n---TOPI---\nGeometric and Photometric Invariance\n---TOPI---\nComputer Vision",
    "topics": [],
    "references": [
      {
        "citation": "[Bishop, C. M. Neural Networks for Pattern Recognition. Clarendon Press, Oxford, 1995.] - Provides a foundational text on neural networks, likely used for background knowledge."
      },
      {
        "citation": "[Krizhevsky, A., Sutskever, I., & Hinton, G. E. Imaginet classification with deep convolutional neural networks. In NIPS, 2012.] - A seminal paper introducing deep convolutional neural networks and their application to ImageNet classification."
      },
      {
        "citation": "[Lowe, D. G. Distinctive image features from scale-invariant keypoints. IJCV, 2(60):91–110, 2004.] - Introduces SIFT features, a key component in many image processing tasks."
      },
      {
        "citation": "[Girshick, R. B., Felzenszwalb, P. F., & McAllester, D. Discriminatively trained deformable part models, release 5. http://people.cs.uchicago.edu/˜rbg/latent-release5/, 2010.] - Discusses deformable part models for object detection."
      },
      {
        "citation": "[Leung, T., & Malik, J. Representing and recognizing the visual appearance of materials using three-dimensional textons. IJCV, 43(1), 2001.] - Explores material recognition using 3D textons."
      },
      {
        "citation": "[Simonyan, K., Vedaldi, A., & Zisserman, A. Deep inside convolutional nets. In CoRR, volume abs/1312.6229, 2014.] - Provides insights into the inner workings of convolutional networks."
      },
      {
        "citation": "[Vedaldi, A. An open implementation of the SIFT detector and descriptor. Technical Report 070012, UCLA CSD, 2007.] - Provides an open-source implementation of the SIFT detector."
      },
      {
        "citation": "[Zeiler, M. D., & Fergus, R. Visualizing and understanding convolutional networks. In ECCV, 2014.] - Focuses on visualizing and understanding convolutional networks."
      },
      {
        "citation": "[Hinton, G. E., & Salakhutdinov, R. R. Reducing the dimensionality of data with neural networks. Science, 313(5786), 2006.] - Discusses dimensionality reduction using neural networks."
      },
      {
        "citation": "[Wang, J., Yang, J., Yu, K., Lv, F., Huang, T., & Gong, Y. Locality-constrained linear coding for image classification. CVPR, 2010.] - Introduces locality-constrained linear coding for image classification."
      }
    ],
    "author_details": [
      {
        "name": "Arvindh Mahendran",
        "affiliation": "University of Oxford",
        "email": "aravindh@robots.ox.ac.uk"
      },
      {
        "name": "Andrea Vedaldi",
        "affiliation": "University of Oxford",
        "email": "vedaldi@robots.ox.ac.uk"
      }
    ]
  },
  {
    "title": "Sparse Projections for High-Dimensional Binary Codes\n---AUTHOR---\nYan Xia\nKaiming He\nPushmeet Kohli\nJian Sun",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Xia_Sparse_Projections_for_2015_CVPR_paper.pdf",
    "id": "Xia_Sparse_Projections_for_2015_CVPR_paper",
    "abstract": "This paper addresses the problem of learning long binary codes from high-dimensional data. We observe that two key challenges arise while learning and using long binary codes: (1) lack of an effective regularizer for the learned high-dimensional mapping and (2) high computational cost for computing long codes. In this paper, we overcome both these problems by introducing a sparsity encouraging regularizer that reduces the effective number of parameters involved in the learned projection operator. This regularizer not only reduces overﬁtting but, due to the sparse nature of the projection matrix, also leads to a dramatic reduction in the computational cost. Experiments on a number of challenging datasets show that our method leads to better accuracy than dense projections (ITQ and LSH) with the same code lengths, and meanwhile is over an order of magnitude faster. Furthermore, our method is also more accurate and faster than other recently proposed methods for speeding up high-dimensional binary encoding.",
    "topics": [
      "Binary Codes",
      "High-Dimensional Data",
      "Sparse Projections",
      "Nearest Neighbour Search",
      "Computational Efficiency"
    ],
    "references": [
      {
        "citation": "[P. Agrawal, R. Girshick, and J. Malik. Analyzing the performance of multilayer neural networks for object recognition. In ECCV, pages 329–344. Springer, 2014.] - This paper likely provides foundational context for the work, given the focus on neural networks for object recognition."
      },
      {
        "citation": "[R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-J. Lin. Liblinear: A library for large linear classiﬁcation. JMLR, 2008.] -  Suggests the use of linear classification techniques, a common component in many machine learning pipelines."
      },
      {
        "citation": "[J. Donahue, Y. Jia, O. Vinyals, J. Hoffman, N. Zhang, E. Tzeng, and T. Darrell. Decaf: A deep convolutional activation feature for generic visual recognition. In ICML, 2014.] - Relevant if the paper utilizes or builds upon deep convolutional features."
      },
      {
        "citation": "[Y. Gong and S. Lazebnik. Iterative quantization: A procrustean approach to learning binary codes. In CVPR, 2011.] - This is a key reference if the paper deals with binary codes or quantization techniques."
      },
      {
        "citation": "[Y. Gong, S. Kumar, H. A. Rowley, and S. Lazebnik. Learning binary codes for high-dimensional data using bilinear projections. In CVPR, 2013.] -  Related to binary code learning, a common technique for efficient similarity search."
      },
      {
        "citation": "[T. Ge, K. He, Q. Ke, and J. Sun. Optimized product quantization for approximate nearest neighbor search. In CVPR, 2013.] - Important if the paper involves approximate nearest neighbor search, a common task in many applications."
      },
      {
        "citation": "[S. S. Chen, D. L. Donoho, and M. A. Saunders. Atomic decomposition by basis pursuit. SIAM journal on scientific computing, pages 33–61, 1998.] - This reference suggests the use of sparse approximation techniques, which can be relevant for feature selection or dimensionality reduction."
      },
      {
        "citation": "[A. Andoni and P. Indyk. Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions. In FOCS, pages 459–468, 2006.] -  Relevant if the paper utilizes hashing techniques for efficient similarity search."
      },
      {
        "citation": "[M. Datar, N. Immorlica, P. Indyk, and V. S. Mirrokni. Locality-sensitive hashing scheme based on p-stable distributions. In Symposium on Computational Geometry, 2004.] - Another reference related to locality-sensitive hashing, a key technique for approximate nearest neighbor search."
      },
      {
        "citation": "[J. C. Gower and G. B. Dijksterhuis. Procustes problems, volume 3. Oxford University Press Oxford, 2004.] - This reference is relevant if the paper uses Procrustes analysis, a method for finding the optimal transformation between two sets of points."
      }
    ],
    "author_details": [
      {
        "name": "Yan Xia",
        "affiliation": "University of Science and Technology of China",
        "email": "[Not available in the provided text]"
      },
      {
        "name": "Kaiming He",
        "affiliation": "Microsoft Research",
        "email": "[Not available in the provided text]"
      },
      {
        "name": "Pushmeet Kohli",
        "affiliation": "Microsoft Research",
        "email": "[Not available in the provided text]"
      },
      {
        "name": "Jian Sun",
        "affiliation": "Microsoft Research",
        "email": "[Not available in the provided text]"
      }
    ]
  },
  {
    "title": "Object-based RGBD Image Co-segmentation with Mutex Constraint\n---AUTHOR---\nHuazhu Fu\nDong Xu\nStephen Lin\nJiang Liu",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Fu_Object-Based_RGBD_Image_2015_CVPR_paper.pdf",
    "id": "Fu_Object-Based_RGBD_Image_2015_CVPR_paper",
    "abstract": "We present an object-based co-segmentation method that takes advantage of depth data and is able to correctly handle noisy images in which the common foreground object is missing. With RGBD images, our method utilizes the depth channel to enhance identification of similar foreground objects via a proposed RGBD co-saliency map, as well as to improve detection of object-like regions and provide depth-based local features for region comparison. To accurately deal with noisy images where the common object appears more than or less than once, we formulate co-segmentation in a fully-connected graph structure together with mutual exclusion (mutex) constraints that prevent improper solutions. Experiments show that this object-based RGBD co-segmentation with mutex constraints outperforms related techniques on an RGBD co-segmentation dataset, while effectively processing noisy images. Moreover, we show that this method also provides performance comparable to state-of-the-art RGB co-segmentation techniques on regular RGB images with depth maps estimated from them.",
    "topics": [
      "RGBD Image Co-segmentation",
      "Object-Based Methods",
      "Mutual Exclusion Constraints",
      "Co-Saliency Maps",
      "Graph Formulation"
    ],
    "references": []
  },
  {
    "title": "A Multi-Plane Block-Coordinate Frank-Wolfe Algorithm for Training Structural SVMs with a Costly max-Oracle\n---AUTHORs---\nNeel Shah\nVladimir Kolmogorov\nChristoph H. Lampert",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Shah_A_Multi-Plane_Block-Coordinate_2015_CVPR_paper.pdf",
    "id": "Shah_A_Multi-Plane_Block-Coordinate_2015_CVPR_paper",
    "abstract": "Structural Support Vector Machines (SSVMs) are highly effective for structured computer vision tasks, but their training is computationally expensive due to repeated calls to a structured prediction subroutine (the max-oracle), which itself involves optimization. This paper introduces a new variant of the Frank-Wolfe algorithm specifically designed for training SSVMs when the max-oracle is a bottleneck. The algorithm combines the block-coordinate Frank-Wolfe (BCFW) algorithm with a caching mechanism and a geometrically motivated criterion for dynamically deciding whether to call the exact max-oracle or reuse cached results. Experiments on diverse datasets demonstrate faster convergence with fewer oracle calls and reduced total runtime when the max-oracle is slow.",
    "topics": [
      "Structural Support Vector Machines (SSVMs)",
      "Frank-Wolfe Algorithm",
      "Max-Oracle Optimization",
      "Caching Mechanisms",
      "Block-Coordinate Methods"
    ],
    "references": [],
    "author_details": [
      {
        "name": "Neel Shah",
        "affiliation": "IST Austria",
        "email": "neelshah@ist.ac.at"
      },
      {
        "name": "Vladimir Kolmogorov",
        "affiliation": "IST Austria",
        "email": "vnk@ist.ac.at"
      },
      {
        "name": "Christoph H. Lampert",
        "affiliation": "IST Austria",
        "email": "chl@ist.ac.at"
      }
    ]
  },
  {
    "title": "Fusion Moves for Correlation Clustering\n---AUTHOR---\nThorsten Beier\nFred A. Hamprecht\nJörg H. Kappes",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Beier_Fusion_Moves_for_2015_CVPR_supplemental.pdf",
    "id": "Beier_Fusion_Moves_for_2015_CVPR_supplemental",
    "abstract": "This paper presents a supplementary material for the work \"Fusion Moves for Correlation Clustering.\" It details experimental results obtained using various algorithms, including PIVOT-BOEM, HC, CGC, and their fusion variants, applied to several datasets. The focus is on evaluating the performance of these algorithms in terms of runtime and solution quality, particularly emphasizing anytime behavior, where solutions are progressively improved over time. The supplementary material includes detailed plots and tables showcasing the algorithms' performance on different instances, providing a comprehensive analysis of their strengths and weaknesses.",
    "topics": [
      "Correlation Clustering",
      "Anytime Algorithms",
      "Fusion Moves",
      "Experimental Analysis",
      "Dataset Performance"
    ],
    "references": [],
    "author_details": [
      {
        "name": "Thorsten Beier",
        "affiliation": "University of Heidelberg (IWR - Institute for Applied and Numerical Mathematics)",
        "email": "thorsten.beier@iwr.uni-heidelberg.de"
      },
      {
        "name": "Fred A. Hamprecht",
        "affiliation": "University of Heidelberg (IWR - Institute for Applied and Numerical Mathematics)",
        "email": "fred.hamprecht@iwr.uni-heidelberg.de"
      },
      {
        "name": "Jörg H. Kappes",
        "affiliation": "University of Heidelberg (Department of Mathematics)",
        "email": "kappes@math.uni-heidelberg.de"
      }
    ]
  },
  {
    "title": "Feedforward semantic segmentation with zoom-out features\n---AUTHOR---\nMohammadreza Mostajabi\nPayman Yadollahpour\nGregory Shakhnarovich",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Mostajabi_Feedforward_Semantic_Segmentation_2015_CVPR_paper.pdf",
    "id": "Mostajabi_Feedforward_Semantic_Segmentation_2015_CVPR_paper",
    "abstract": "We introduce a purely feed-forward architecture for semantic segmentation. We map small image elements (superpixels) to rich feature representations extracted from a sequence of nested regions of increasing extent. These regions are obtained by ”zooming out” from the superpixel all the way to scene-level resolution. This approach exploits statistical structure in the image and in the label space without setting up explicit structured prediction mechanisms, and thus avoids complex and expensive inference. Instead superpixels are classified by a feedforward multilayer network. Our architecture achieves 69.6% average accuracy on the PAS-CAL VOC 2012 test set.",
    "topics": [
      "Semantic Segmentation",
      "Feedforward Networks",
      "Superpixels",
      "Zoom-out Features",
      "Convolutional Neural Networks"
    ],
    "references": [
      {
        "citation": "[Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). ImageNet classification with deep convolutional neural networks. In *NIPS*.]"
      },
      {
        "citation": "[Arbeláez, P., Hariharan, B., Gu, C., Gupta, S., Bourdev, L., & Malik, J. (2012). Semantic segmentation using regions and parts. In *CVPR*.]"
      },
      {
        "citation": "[Girshick, R., Donahue, J., Darrell, T., & Malik, J. (2014). Rich feature hierarchies for accurate object detection and semantic segmentation. *arXiv preprint arXiv:1311.2524*.]"
      },
      {
        "citation": "[Hariharan, B., Arbeláez, P., & Girshick, R. (2015). Hypercolumns for object segmentation and fine-grained localization. *arXiv preprint arXiv:1411.5752*.]"
      },
      {
        "citation": "[Chen, L., Papandreou, G., Kokkinos, I., Murphy, K., & Yuille, A. L. (2015). Semantic image segmentation with deep convolutional nets and fully connected crfs. *arXiv preprint arXiv:1412.7062*.]"
      },
      {
        "citation": "[Long, J., Shelhamer, E., & Darrell, T. (2015). Fully convolutional networks for semantic segmentation. *arXiv preprint arXiv:1411.4038*.]"
      },
      {
        "citation": "[Ladický, L., Russell, C., Kohli, P., & Torr, P. H. S. (2009). Associative hierarchical CRFs for object class image segmentation. *ICCV*.]"
      },
      {
        "citation": "[Carreira, J., & Sminchisescu, C. (2012). CPMC: Automatic object segmentation using constrained parametric min-cuts. *IEEE Transactions on Pattern Analysis and Machine Intelligence, 34(7)*.]"
      },
      {
        "citation": "[Achanta, R., Shaji, A., Smith, K., Lucchi, A., Fua, P., & Ssstrunk, S. (2012). Slic superpixels compared to state-of-the-art superpixel methods. *IEEE TPAMI*.]"
      },
      {
        "citation": "[Simonyan, K., & Zisserma, A. (2014). Very deep convolutional networks for large-scale image recognition. *arXiv preprint arXiv:1409.1556*.]"
      }
    ],
    "author_details": [
      {
        "name": "Mohammadreza Mostajabi",
        "affiliation": "Toyota Technological Institute at Chicago",
        "email": "mostajabi@ttic.edu"
      },
      {
        "name": "Payman Yadollahpour",
        "affiliation": "Toyota Technological Institute at Chicago",
        "email": "pyadolla@ttic.edu"
      },
      {
        "name": "Gregory Shakhnarovich",
        "affiliation": "Toyota Technological Institute at Chicago",
        "email": "greg@ttic.edu"
      }
    ]
  },
  {
    "title": "Reﬂection Removal using Ghosting Cues\n---AUTHOR---\nFr´edo Durand\nYiChang Shih\nDilip Krishnan\nWilliam T. Freeman",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Shih_Reflection_Removal_Using_2015_CVPR_paper.pdf",
    "id": "Shih_Reflection_Removal_Using_2015_CVPR_paper",
    "abstract": "Photographs taken through glass windows often contain both the desired scene and undesired reflections. Separating the reflection and transmission layers is an important but ill-posed problem. This paper introduces the use of \"ghosting cues,\" which exploit asymmetry between the layers arising from shifted double reflections of the reflected scene off the glass surface. These cues are often barely perceptible to humans, but can be exploited for layer separation. The authors model the ghosted reflection using a double-impulse convolution kernel, automatically estimate the spatial separation and relative attenuation of the ghosted reflection components, and propose an algorithm using a Gaussian Mixture Model for regularization. The method requires only a single input image and demonstrates significant reflection removal on both synthetic and real-world inputs.\n\n---TOPICCS---\nReflection Removal\nGhosting Cues\nImage Processing\nLayer Separation\nConvolution Kernels",
    "topics": [],
    "references": [],
    "author_details": [
      {
        "name": "Fr´edo Durand",
        "affiliation": "MIT CSAIL",
        "email": "fredo@mit.edu"
      },
      {
        "name": "YiChang Shih",
        "affiliation": "MIT CSAIL",
        "email": "yichang@mit.edu"
      },
      {
        "name": "Dilip Krishnan",
        "affiliation": "Google Research",
        "email": "dilipkay@google.com"
      },
      {
        "name": "William T. Freeman",
        "affiliation": "MIT CSAIL",
        "email": "billf@mit.edu"
      }
    ]
  },
  {
    "title": "Multispectral Pedestrian Detection: Benchmark Dataset and Baseline\n---AUTHOR---\nSoonmin Hwang\nJaesik Park\nNamil Kim\nYukyung Choi\nIn So Kweon",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Hwang_Multispectral_Pedestrian_Detection_2015_CVPR_paper.pdf",
    "id": "Hwang_Multispectral_Pedestrian_Detection_2015_CVPR_paper",
    "abstract": "With the increasing interest in pedestrian detection, pedestrian datasets have also been the subject of research in the past decades. However, most existing datasets focus on a color channel, while a thermal channel is helpful for detection even in a dark environment. To address this, we propose a multispectral pedestrian dataset which provides well-aligned color-thermal image pairs, captured by beam splitter-based special hardware. The dataset is as large as previous color-based datasets and provides dense annotations including temporal correspondences. With this dataset, we introduce multispectral ACF, an extension of aggregated channel features (ACF) to simultaneously handle color-thermal image pairs. Multispectral ACF reduces the average miss rate of ACF by 15%, achieving a breakthrough in pedestrian detection.\n\n---TOPIC---\nMultispectral Pedestrian Detection\n---TOPIC---\nThermal Imaging\n---TOPIC---\nAggregated Channel Features (ACF)\n---TOPIC---\nDataset Creation\n---TOPIC---\nComputer Vision",
    "topics": [],
    "references": [],
    "author_details": [
      {
        "name": "Soonmin Hwang",
        "email": "smhwang@rcv.kaist.ac.kr"
      },
      {
        "name": "Jaesik Park",
        "email": "jspark@rcv.kaist.ac.kr"
      },
      {
        "name": "Namil Kim",
        "email": "nikim@rcv.kaist.ac.kr"
      },
      {
        "name": "Yukyung Choi",
        "email": "ykchoi@rcv.kaist.ac.kr"
      },
      {
        "name": "In So Kweon",
        "email": "iskweon77@kaist.ac.kr"
      }
    ]
  },
  {
    "title": "RGBD-Fusion: Real-Time High Precision Depth Recovery\n---AUTHORs---\nRoy Or\nGuy Rosman\nAaron Wetzler\nRon Kimmel\nAlfred M. Bruckstein",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/El_RGBD-Fusion_Real-Time_High_2015_CVPR_paper.pdf",
    "id": "El_RGBD-Fusion_Real-Time_High_2015_CVPR_paper",
    "abstract": "The popularity of low-cost RGB-D scanners is increasing, but often cannot capture subtle details. This paper presents a novel method to enhance depth maps by fusing intensity and depth information to create more detailed range profiles. The lighting model used can handle natural scene illumination and is integrated into a shape from shading-like technique to improve the visual fidelity of the reconstructed object. The detailed geometry is calculated directly, without explicitly finding and integrating surface normals, and the method operates four orders of magnitude faster than the state of the art. Qualitative and quantitative evidence supports the improvement in depth obtained by the suggested method.",
    "topics": [
      "RGB-D scanners",
      "Depth map enhancement",
      "Shape from shading",
      "Lighting models",
      "Real-time processing"
    ],
    "references": [
      {
        "citation": "[Barnes, C., Shechtman, E., Finkelstein, A., & Goldman, D. (2009). PatchMatch: a randomized correspondence algorithm for structural image editing. *ACM Transactions on Graphics*, *28*(3), 24.]"
      },
      {
        "citation": "[Basri, R., & Jacobs, D. W. (2003). Lambertian reflectance and linear subspaces. *IEEE Transactions on Pattern Analysis and Machine Intelligence*, *25*(2), 33–218.]"
      },
      {
        "citation": "[Bruckstein, A. M. (1988). On shape from shading. *Computer Vision, Graphics, and Image Processing*, *44*(2), 139–154.]"
      },
      {
        "citation": "[Chang, J., Cabezas, R., & Fisher III, J. W. (2014). Bayesian nonparametric intrinsic image decomposition. *European Conference on Computer Vision*, 704–719.]"
      },
      {
        "citation": "[Forsyth, D. A. (2011). Variable-source shading analysis. *International Journal of Computer Vision*, *91*(3), 280–302.]"
      },
      {
        "citation": "[Grosse, R., Johnson, M. K., Adelson, E. H., & Freeman, W. T. (2009). Ground-truth dataset and baseline evaluations for intrinsic image algorithms. *International Conference on Computer Vision*, 2335–2342.]"
      },
      {
        "citation": "[Han, Y., Lee, J. Y., & Kweon, I. S. (2013). High quality shape from a single RGB-D image under uncalibrated natural illumination. *IEEE International Conference on Computer Vision*, 1617–1624.]"
      },
      {
        "citation": "[Horn, B. K. (1970). Shape from shading: A method for obtaining the shape of a smooth opaque object from one view. PhD thesis.]"
      },
      {
        "citation": "[Horn, B. K., & Brooks, M. J. (1986). The variational approach to shape from shading. *Computer Vision, Graphics, and Image Processing*, *33*(2), 174–208.]"
      },
      {
        "citation": "[Johnson, M. K., & Adelson, E. H. (2011). Shape estimation in natural illumination. *IEEE Conference on Computer Vision and Pattern Recognition*, 2553–2560.]"
      }
    ],
    "author_details": [
      {
        "name": "Roy Or",
        "affiliation": "Technion, Israel Institute of Technology",
        "email": "royorel@tx.technion.ac.il"
      },
      {
        "name": "Guy Rosman",
        "affiliation": "Computer Science and Artificial Intelligence Lab, MIT",
        "email": "rosman@csail.mit.edu"
      },
      {
        "name": "Aaron Wetzler",
        "affiliation": "Technion, Israel Institute of Technology",
        "email": "twerd@cs.technion.ac.il"
      },
      {
        "name": "Ron Kimmel",
        "affiliation": "Technion, Israel Institute of Technology",
        "email": "ron@cs.technion.ac.il"
      },
      {
        "name": "Alfred M. Bruckstein",
        "affiliation": "Technion, Israel Institute of Technology",
        "email": "freddy@cs.technion.ac.il"
      }
    ]
  },
  {
    "title": "High-Fidelity Pose and Expression Normalization for Face Recognition in the Wild\n---AUTHOR---\nXiangyu Zhu\nZhen Lei\nJunjie Yan\nDong Yi\nStan Z. Li",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Zhu_High-Fidelity_Pose_and_2015_CVPR_paper.pdf",
    "id": "Zhu_High-Fidelity_Pose_and_2015_CVPR_paper",
    "abstract": "Pose and expression variations significantly impact face recognition performance. This paper addresses this challenge by proposing a High-Fidelity Pose and Expression Normalization (HPEN) method utilizing a 3D Morphable Model (3DMM). HPEN automatically generates natural face images in a frontal pose and neutral expression. The method involves landmark marching, pose-adaptive 3DMM fitting, 3D meshing and transformation to eliminate pose and expression variations, and Poisson Editing-based inpainting to fill occluded regions. Experiments on Multi-PIE and LFW demonstrate significant improvements in face recognition performance compared to state-of-the-art methods.\n\n---TOPIPS---\nFace Recognition\nPose and Expression Normalization\n3D Morphable Models (3DMM)\nImage Level Normalization\nPoisson Editing",
    "topics": [],
    "references": [
      {
        "citation": "[Aldrian, O., & Smith, W. A. (2013). Inverse rendering of faces with a 3d morphable model. *IEEE Transactions on Pattern Analysis and Machine Intelligence*, *35*(5), 1080–1093.]"
      },
      {
        "citation": "[Cao, Z., Yin, Q., Tang, X., & Sun, J. (2010). Face recognition with learning-based descriptor. In *2010 IEEE Conference on Computer Vision and Pattern Recognition* (pp. 2707–2714). IEEE.]"
      },
      {
        "citation": "[Amberg, B., Romdhani, S., & Vetter, T. (2007). Optimal step non-rigid icp algorithms for surface registration. In *2007 IEEE Conference on Computer Vision and Pattern Recognition* (pp. 1–8). IEEE.]"
      },
      {
        "citation": "[Chai, X., Shan, S., Chen, X., & Gao, W. (2007). Locally linear regression for pose-invariant face recognition. *IEEE Transactions on Image Processing*, *16*(7), 1716–1725.]"
      },
      {
        "citation": "[Arashloo, S. R., & Kittler, J. (2009). Pose-invariant face matching using mrf energy minimization framework. In *Energy Minimization Methods in Computer Vision and Pattern Recognition* (pp. 56–69). Springer.]"
      },
      {
        "citation": "[Chan, C. H., Tahir, M. A., Kittler, J., & Pietikainen, M. (2013). Multisculse local phase quantization for robust component-based face recognition using kernel fusion of multiple descriptors. *IEEE Transactions on Pattern Analysis and Machine Intelligence*, *35*(5), 1164–1177.]"
      },
      {
        "citation": "[Chen, D., Cao, X., Wang, L., Wen, F., & Sun, J. (2012). Bayesian face revisited: A joint formulation. In *Computer Vision–ECCV 2012* (pp. 566–579). Springer.]"
      },
      {
        "citation": "[Chen, D., Cao, X., Wen, F., & Sun, J. (2013). Blessing of dimensionality: High-dimensional feature and its efficient compression for face verification. In *2013 IEEE Conference on Computer Vision and Pattern Recognition* (pp. 113–120). IEEE.]"
      },
      {
        "citation": "[Asthana, A., Zafeiriou, S., Cheng, S., & Pantic, M. (2013). Robust discriminative response map fitting with constrained local models. In *2013 IEEE Conference on Computer Vision and Pattern Recognition* (pp. 113–120). IEEE.]"
      },
      {
        "citation": "[Barkan, O., Weill, J., Wolf, L., & Aronowitz, H. (2013). Fast high dimensional vector multiplication face recognition. In *2013 IEEE International Conference on Computer Vision* (pp. 1960–1967). IEEE.]"
      }
    ],
    "author_details": [
      {
        "name": "Xiangyu Zhu",
        "affiliation": "Center for Biometrics and Security Research & National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences",
        "email": "xiangyu.zhu@nlpr.ia.ac.cn"
      },
      {
        "name": "Zhen Lei",
        "affiliation": "Center for Biometrics and Security Research & National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences",
        "email": "zlei@nlpr.ia.ac.cn"
      },
      {
        "name": "Junjie Yan",
        "affiliation": "Center for Biometrics and Security Research & National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences",
        "email": "jjyan@nlpr.ia.ac.cn"
      },
      {
        "name": "Dong Yi",
        "affiliation": "Center for Biometrics and Security Research & National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences",
        "email": "dony.yi@nlpr.ia.ac.cn"
      },
      {
        "name": "Stan Z. Li",
        "affiliation": "Center for Biometrics and Security Research & National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences",
        "email": "szli@nlpr.ia.ac.cn"
      }
    ]
  },
  {
    "title": "Parsing Occluded People by Flexible Compositions\n---AUTHOR---\nXianjie Chen\nAlan Yuille",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Chen_Parsing_Occluded_People_2015_CVPR_paper.pdf",
    "id": "Chen_Parsing_Occluded_People_2015_CVPR_paper",
    "abstract": "This paper presents an approach to parsing humans when there is significant occlusion. We model humans using a graphical model which has a tree structure building on recent work [32, 6] and exploit the connectivity prior that, even in presence of occlusion, the visible nodes form a connected subtree of the graphical model. We call each connected subtree a flexible composition of object parts. This involves a novel method for learning occlusion cues. During inference we need to search over a mixture of different flexible models. By exploiting part sharing, we show that this inference can be done extremely efﬁciently requiring only twice as many computations as searching for the entire object (i.e., not modeling occlusion). We evaluate our model on the standard benchmarked “We Are Family” Stickmen dataset and obtain significant performance improvements over the best alternative algorithms.\n\n---TOPICCS---\nGraphical models\nHuman parsing\nOcclusion handling\nFlexible compositions\nConnectivity priors",
    "topics": [],
    "references": [
      {
        "citation": "[Girshick, R., Donahue, J., Darrell, T., & Malik, J. (2014). Rich feature hierarchies for accurate object detection and semantic segmentation. *Computer Vision and Pattern Recognition (CVPR)*.]"
      },
      {
        "citation": "[Felzenszwalb, P. F., Girshick, R. B., McAllester, D., & Ramanan, D. (2010). Object detection with discriminatively trained part-based models. *IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI)*.]"
      },
      {
        "citation": "[Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). ImageNet classification with deep convolutional neural networks. *Advances in Neural Information Processing Systems (NIPS)*.]"
      },
      {
        "citation": "[Yuilie, A., & Chen, X. (2014). Articulated pose estimation by a graphical model with image dependent pairwise relations. *Advances in Neural Information Processing Systems (NIPS)*.]"
      },
      {
        "citation": "[Felzenszwalb, P. F., & Huttenlocher, D. P. (2005). Pictorial structures for object recognition. *International Journal of Computer Vision (IJCV)*.]"
      },
      {
        "citation": "[Girshick, R., Felsen, P., McAllester, D., & Ramanan, D. (2011). Object detection with grammar models. *Advances in Neural Information Processing Systems (NIPS)*.]"
      },
      {
        "citation": "[Ferrari, V., Marin-Jimenez, M., & Zisserma, A. (2008). Progressive search space reduction for human pose estimation. *Computer Vision and Pattern Recognition (CVPR)*.]"
      },
      {
        "citation": "[Cortes, C., & Vapnik, V. (1995). Support-vector networks. *Machine learning*.]"
      },
      {
        "citation": "[Dalal, N., & Triggs, B. (2005). Histograms of oriented gradients for human detection. *Computer Vision and Pattern Recognition (CVPR)*.]"
      },
      {
        "citation": "[Sapp, B., Jordan, C., & Taskar, B. (2010). Adaptive pose priors for pictorial structures. *Computer Vision and Pattern Recognition (CVPR)*.]"
      }
    ],
    "author_details": [
      {
        "name": "Xianjie Chen",
        "affiliation": "University of California, Los Angeles",
        "email": "cxj@ucla.edu"
      },
      {
        "name": "Alan Yuille",
        "affiliation": "University of California, Los Angeles",
        "email": "yuille@stat.ucla.edu"
      }
    ]
  },
  {
    "title": "Unifying Holistic and Parts-Based Deformable Model Fitting\n---AUTHOR---\nJoan Alabort-i-Medina\nStefanos Zafeiriou",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Alabort-i-Medina_Unifying_Holistic_and_2015_CVPR_paper.pdf",
    "id": "Alabort-i-Medina_Unifying_Holistic_and_2015_CVPR_paper",
    "abstract": "The construction and fitting of deformable models that capture the degrees of freedom of articulated objects is a popular area of research in computer vision. This paper proposes a unified approach that combines Holistic Deformable Models (HDMs), which represent the object as a whole, and Parts-Based Deformable Models (PBDMs), which model object parts independently. The authors derive a novel probabilistic formulation of the fitting problem that unifies Active Appearance Models and Constrained Local Models, achieving state-of-the-art results in face alignment. The code will be made publicly available.",
    "topics": [
      "Deformable Models",
      "Holistic Deformable Models (HDMs)",
      "Parts-Based Deformable Models (PBDMs)",
      "Face Alignment",
      "Probabilistic Formulation"
    ],
    "references": [
      {
        "citation": "[T. F. Cootes, G. J. Edwards, and C. J. Taylor. Active appearance models. Transactions on Pattern Analysis and Machine Intelligence (T PAM I), 2001.]"
      },
      {
        "citation": "[D. G. Lowe. Object recognition from local scale-invariant features. In International Conference on Computer Vision (ICCV), 1999.]"
      },
      {
        "citation": "[T. F. Cootes, C. J. Taylor, D. H. Cooper, and J. Graham. Active shape models: Their training and application. Computer Vision and Image Understanding, 1995.]"
      },
      {
        "citation": "[J. Alabort-i-Medina and S. Zafeiriou. Bayesian active appearance models. In Conference on Computer Vision and Pattern Recognition (CVPR), 2014.]"
      },
      {
        "citation": "[S. Baker and I. Matthews. Lucas-kanade 20 years on: A unifying framework. International Journal of Computer Vision (IJCV), 2004.]"
      },
      {
        "citation": "[P. Martins, R. Caseiro, and J. Batista. Non-parametric bayesian constrained local models. In Conference on Computer Vision and Pattern Recognition (CVPR), 2014.]"
      },
      {
        "citation": "[X. Cao, Y. Wei, F. Wen, and J. Sun. Face alignment by explicit shape regression. In Computer Vision and Pattern Recognition (CVPR), 2012.]"
      },
      {
        "citation": "[G. Papandreou and P. Maragos. Adaptive and constrained algorithms for inverse compositional active appearance model fitting. In Conference on Computer Vision and Pattern Recognition (CVPR), 2008.]"
      },
      {
        "citation": "[A. Asthana, S. Zafeiriou, S. Cheng, and M. Pantic. Robust discriminative response map fitting with constrained local models. In Conference on Computer Vision and Pattern Recognition (CVPR), 2013.]"
      },
      {
        "citation": "[J. Sragih. Principal regression analysis. In Computer Vision and Pattern Recognition (CVPR), 2011.]"
      }
    ],
    "author_details": [
      {
        "name": "Joan Alabort-i-Medina",
        "affiliation": "Department of Computing, Imperial College London, United Kingdom",
        "email": "ja310@imperial.ac.uk"
      },
      {
        "name": "Stefanos Zafeiriou",
        "affiliation": "Department of Computing, Imperial College London, United Kingdom",
        "email": "s.zafeiriou@imperial.ac.uk"
      }
    ]
  },
  {
    "title": "Gaze-enabled Egocentric Video Summarization via Constrained Submodular Maximization\n---AUTHOR---\nJia Xu\nLopamudra Mukherjee\nYin Li\nJamieson Warner\nJames M. Rehg\nVikas Singh",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Xu_Gaze-Enabled_Egocentric_Video_2015_CVPR_paper.pdf",
    "id": "Xu_Gaze-Enabled_Egocentric_Video_2015_CVPR_paper",
    "abstract": "The proliferation of wearable cameras has led to a rapid increase in the volume of egocentric videos documenting personal lives. These videos, often spanning hours, necessitate mechanisms for compact representation (shorter, browsable videos). This paper addresses the problem of egocentric video summarization, which presents unique challenges due to continuous recording, camera shake, and quality issues. The authors demonstrate that incorporating gaze tracking information (fixations and saccades) significantly improves summarization by enabling meaningful frame comparison and personalized summaries. They formulate a summarization model based on submodular function maximization with partition matroid constraints, and evaluate their approach on a new gaze-enabled egocentric video dataset.\n\n---TOPIC---\nEgocentric Video Summarization\nGaze Tracking\nSubmodular Maximization\nWearable Cameras\nPersonalized Summarization",
    "topics": [],
    "references": [
      {
        "citation": "[Aghazadeh, O., Sullivan, J., and Carlsson, S. Novelty detection from an ego-centric perspective. In Proc. CVPR, 2011.]"
      },
      {
        "citation": "[Almeida, J., Leite, N. J., and Torres, R. da Silva. VISON: VIdeo Summarization for ONline applications. Pattern Recognition Letters, 33(4):397–409, 2012.]"
      },
      {
        "citation": "[Chekuri, C., Vondr´ak, J., and Zenklusen, R. Submodular function maximization via the multilinear relaxation and contention resolution schemes. In Proc. STOC, 2011.]"
      },
      {
        "citation": "[Filmus, Y., and Ward, J. A tight combinatorial algorithm for submodular maximization subject to a matroid constraint. In Proc. FOCS, 2012.]"
      },
      {
        "citation": "[Fujishige, S. Submodular functions and optimization. Elsevier, 2005.]"
      },
      {
        "citation": "[Girshick, R., Donahue, J., Darrell, T., and Malik, J. Rich feature hierarchies for accurate object detection and semantic segmentation. In Proc. CVPR, 2014.]"
      },
      {
        "citation": "[Gong, B., Chao, W., Grauman, K., and Sha, F. Diverse sequential subset selection for supervised video summarization. In Proc. NIPS, 2014.]"
      },
      {
        "citation": "[Gyllenwater, J., Kulesza, A., and Taskar, B. Near-optimal map inference for determinantal point processes. In Proc. NIPS, 2012.]"
      },
      {
        "citation": "[Iyer, R., and Bilmes, J. Submodular optimization with submodular cover and submodular knapsack constraints. In Proc. NIPS, 2013.]"
      },
      {
        "citation": "[Krause, A., and Guestrin, C. Submodularity and its applications in optimized information gathering. ACM Transactions on Intelligent Systems and Technology, 2(4):32, 2011.]"
      }
    ],
    "author_details": [
      {
        "name": "Jia Xu",
        "affiliation": "University of Wisconsin-Madison",
        "email": "(Not available in the provided text, but potentially accessible via http://pages.cs.wisc.edu/˜jiaxu/projects/ego-video-sum/)"
      },
      {
        "name": "Lopamudra Mukherjee",
        "affiliation": "University of Wisconsin-Whitewater",
        "email": "(Not available in the provided text)"
      },
      {
        "name": "Yin Li",
        "affiliation": "Georgia Institute of Technology",
        "email": "(Not available in the provided text)"
      },
      {
        "name": "Jamieson Warner",
        "email": "(Not available in the provided text)"
      },
      {
        "name": "James M. Rehg",
        "affiliation": "Georgia Institute of Technology",
        "email": "(Not available in the provided text)"
      },
      {
        "name": "Vikas Singh",
        "affiliation": "University of Wisconsin-Madison",
        "email": "(Not available in the provided text)"
      }
    ]
  },
  {
    "title": "Object Scene Flow for Autonomous Vehicles\n---AUTHOR---\nAndreas Geiger\n---AUTHOR---\nMoritiz Menze",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Menze_Object_Scene_Flow_2015_CVPR_supplemental.pdf",
    "id": "Menze_Object_Scene_Flow_2015_CVPR_supplemental",
    "abstract": "This supplementary document provides additional descriptions, visualizations and experiments. We start by visualizing the 3D CAD models used for generating our scene ﬂow ground truth. Next, we provide a detailed description of the initialization procedure for object hypotheses in our model. For reproducibility, we also detail all (estimated) model parameters together with plots illustrating the sensitivity of our model with respect to the choice of parameters. We further demonstrate that the loss in performance is small when limiting the runtime of our method to two minutes per scene on a single core. We also provide additional qualitative results of our method on the sphere sequence and quantitatively compare our method to state-of-the-art stereo, optical ﬂow and scene ﬂow approaches on the well-established KITTI stereo and optical ﬂow benchmarks. Finally, we show additional quantitative and qualitative results on the novel scene ﬂow dataset.\n\n---TOPICCS---\nObject Scene Flow\n3D CAD Models\nObject Hypothesis Initialization\nModel Parameter Sensitivity\nScene Flow Datasets",
    "topics": [],
    "references": [
      {
        "citation": "[Brox, T., & Malik, J. (2011). Large displacement optical flow: Descriptor matching in variational motion estimation. *Pattern Analysis and Machine Intelligence*, *33*(5), 500–513.] - Frequently referenced (appears 10 times), likely foundational work."
      },
      {
        "citation": "[Hirschmueller, H. (2008). Stereo processing by semiglobal matching and mutual information. *Pattern Analysis and Machine Intelligence*, *30*(2), 328–341.] - Frequently referenced (appears 10 times), likely a core technique."
      },
      {
        "citation": "[Cech, J., Sanchez-Riera, J., & Horaud, R. P. (2011). Scene flow estimation by growing correspondence seeds. *CVPR*.] - Frequently referenced (appears 10 times), important for the paper's methodology."
      },
      {
        "citation": "[Vogel, C., Schindler, K., & Roth, S. (2013). Piecewise rigid scene flow. *ICCV*.] - Frequently referenced (appears 10 times), likely relevant to the paper's approach."
      },
      {
        "citation": "[Huguet, F., & Devernay, F. (2007). A variational method for scene flow estimation from stereo sequences. *ICCV*.] - Frequently referenced (appears 10 times), likely a key methodological reference."
      },
      {
        "citation": "[Sun, D., Roth, S., & Black, M. J. (2013). A quantitative analysis of current practices in optical flow estimation and the principles behind them. *International Journal of Computer Vision*, *106*(2), 115–137.] - Referenced for analysis and context."
      },
      {
        "citation": "[SphereFlow: 6 DoF scene flow from RGB-D pairs. Hornacek, M., Fitzgibbon, A., & Rother, C. (2014). *CVPR*.] - Referenced frequently (appears 10 times)."
      },
      {
        "citation": "[Valgaerts, L., Bruhn, A., Zimmer, H., Weickert, J., Stoll, C., & Theobalt, C. (2010). Joint estimation of motion, structure and geometry from stereo sequences. *ECCV*.] - Referenced for a broader perspective on motion and geometry estimation."
      },
      {
        "citation": "[Wedel, A., Rabe, C., Vaudrey, T., Brox, T., Franke, U., & Cremers, D. (2008). Efficient dense scene flow from sparse or dense stereo data. *ECCV*.] - Referenced for efficient scene flow computation."
      },
      {
        "citation": "[Geiger, A., Ziegler, J., & Stiller, C. (2011). StereoScan: Dense 3D reconstruction in real-time. *IV*.] - Referenced for 3D reconstruction techniques."
      }
    ],
    "author_details": [
      {
        "name": "Andreas Geiger",
        "affiliation": "MPI T¨ubingen",
        "email": "andreas.geiger@tue.mpg.de"
      },
      {
        "name": "Moritz Menze",
        "affiliation": "Leibniz Universit¨at Hannover",
        "email": "menze@ipi.uni-hannover.de"
      }
    ]
  },
  {
    "title": "Effective Face Frontalization in Unconstrained Images\n---AUTHOR---\nTal Hassner\nShai Harel\nEran Paz\nRoee Enbar",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Hassner_Effective_Face_Frontalization_2015_CVPR_paper.pdf",
    "id": "Hassner_Effective_Face_Frontalization_2015_CVPR_paper",
    "abstract": "Recent face recognition systems have shifted focus from controlled environments to unconstrained images (\"in the wild\"), leading to challenges like varying poses, expressions, and lighting. \"Fronalization,\" the process of synthesizing frontal views of faces in unconstrained photos, has been proposed to simplify this problem. Previous methods relied on estimating 3D facial shapes, which can be complex. This paper explores a simpler approach using a single, unmodified 3D surface for frontalization. The results demonstrate a straightforward, efficient method that produces aesthetically pleasing frontal views and surprisingly effective results for face recognition and gender estimation, avoiding the complexities of 3D shape estimation.\n\n---TOPIC---\nFace frontalization\nUnconstrained face recognition\n3D face modeling\nPose estimation\nFace symmetry",
    "topics": [],
    "references": [],
    "author_details": [
      {
        "name": "Tal Hassner",
        "affiliation": "The open University of Israel",
        "email": "[Email not available in provided text]"
      },
      {
        "name": "Shai Harel",
        "affiliation": "The open University of Israel",
        "email": "[Email not available in provided text]"
      },
      {
        "name": "Eran Paz",
        "affiliation": "The open University of Israel",
        "email": "[Email not available in provided text]"
      },
      {
        "name": "Roee Enbar",
        "affiliation": "Adience",
        "email": "[Email not available in provided text]"
      }
    ]
  },
  {
    "title": "Don’t Just Listen, Use Your Imagination: Leveraging Visual Common Sense for Non-Visual Tasks\n---AUTHOR---\nXiao Lin\nDevi Parikh",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Lin_Dont_Just_Listen_2015_CVPR_paper.pdf",
    "id": "Lin_Dont_Just_Listen_2015_CVPR_paper",
    "abstract": "Today's AI systems excel at factual question answering but struggle with common sense reasoning. This paper proposes leveraging \"visual common sense\" – semantic knowledge learned from images – to improve performance on two textual tasks: fill-in-the-blank and visual paraphrasing. The approach \"imagines\" the scene behind the text, incorporating visual cues alongside textual cues. The proposed tasks serve as benchmarks for evaluating progress beyond simple recognition, and code and datasets are publicly available.",
    "topics": [
      "Visual Common Sense",
      "Fill-in-the-Blank",
      "Visual Paraphrasing",
      "AI Reasoning",
      "Semantic Knowledge"
    ],
    "references": [],
    "author_details": [
      {
        "name": "Xiao Lin",
        "affiliation": "Virginia Tech",
        "email": "linxiao@vt.edu"
      },
      {
        "name": "Devi Parikh",
        "email": "parikh@vt.edu"
      }
    ]
  },
  {
    "title": "NBKCBO?KJNCJFP=>CADHAH@CEBAADHBM>FBM\\OC@F>CFGH?@@IC@UV\n---AUTHOR---\nNBKCBO\n---AUTHOR---\nKJNCJFP\n---AUTHOR---\nCADHAH\n---AUTHOR---\nCEBAADHBM\n---AUTHOR---\nFBM",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Hsu_Robust_Image_Alignment_2015_CVPR_paper.pdf",
    "id": "Hsu_Robust_Image_Alignment_2015_CVPR_paper",
    "abstract": "The provided text appears to be a fragmented and likely corrupted section of a research paper, possibly the introduction or a preliminary discussion. It's difficult to extract a coherent abstract due to the significant data loss and unusual formatting. However, based on the discernible phrases and recurring themes, it seems the paper likely explores a complex system or process, possibly involving data analysis, optimization, or a combination of technical and conceptual elements. The text hints at a focus on relationships, dependencies, and potentially a search for optimal solutions within a defined framework. The presence of numerous references to specific variables and parameters suggests a quantitative approach.\n\n---TOPIC---\nData Analysis\nOptimization\nSystem Dynamics\nParameter Estimation\nRelationship Modeling",
    "topics": [],
    "references": [],
    "author_details": []
  },
  {
    "title": "Completing 3D Object Shape from One Depth Image\n---AUTHORs---\nJason Rock\nTanmay Gupta\nJustin Thorsten\nJunYoung Gwak\nDaeyun Shin\nDerek Hoiem",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Rock_Completing_3D_Object_2015_CVPR_paper.pdf",
    "id": "Rock_Completing_3D_Object_2015_CVPR_paper",
    "abstract": "This paper addresses the problem of recovering a complete 3D model from a single depth image. Existing approaches often require user interaction or are limited to specific object categories. The proposed method takes an exemplar-based approach, retrieving similar 3D models from a database using view-based matching and transferring symmetries and surfaces. The goal is to fully automatically reconstruct a 3D model from any category, handling novel views, novel models within a category, and novel categories. The paper investigates techniques for viewpoint-based shape matching, 3D deformation, 3D mesh analysis, and 3D model synthesis.\n\n---TOPIICS---\n3D Shape Reconstruction\nView-Based Matching\nShape Completion\n3D Model Synthesis\nSymmetry Transfer",
    "topics": [],
    "references": [],
    "author_details": [
      {
        "name": "Jason Rock",
        "affiliation": "University of Illinois at Urbana-Champaign",
        "email": "jjrock2@illinois.edu"
      },
      {
        "name": "Tanmay Gupta",
        "affiliation": "University of Illinois at Urbana-Champaign",
        "email": "tgupta6@illinois.edu"
      },
      {
        "name": "Justin Thorsten",
        "affiliation": "University of Illinois at Urbana-Champaign",
        "email": "thorsten1@illinois.edu"
      },
      {
        "name": "JunYoung Gwak",
        "affiliation": "University of Illinois at Urbana-Champaign",
        "email": "gwak2@illinois.edu"
      },
      {
        "name": "Daeyun Shin",
        "affiliation": "University of Illinois at Urbana-Champaign",
        "email": "dshin11@illinois.edu"
      },
      {
        "name": "Derek Hoiem",
        "affiliation": "University of Illinois at Urbana-Champaign",
        "email": "dhoiem@illinois.edu"
      }
    ]
  },
  {
    "title": "Unknown Title",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Liu_Data-Driven_Sparsity-Based_Restoration_2015_CVPR_paper.pdf",
    "id": "Liu_Data-Driven_Sparsity-Based_Restoration_2015_CVPR_paper",
    "abstract": "Abstract not found",
    "topics": [],
    "references": [],
    "author_details": []
  },
  {
    "title": "Beyond the shortest path : Unsupervised Domain Adaptation by Sampling Subspaces along the Spline Flow\n---AUTHOR---\nRui Caseiro\n---AUTHOR---\nJoão F. Henriques\n---AUTHOR---\nPedro Martins\n---AUTHOR---\nJorge Batista",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Caseiro_Beyond_the_Shortest_2015_CVPR_paper.pdf",
    "id": "Caseiro_Beyond_the_Shortest_2015_CVPR_paper",
    "abstract": "This paper addresses limitations in a popular domain adaptation paradigm that represents source and target domains as subspaces on the Grassmann manifold and uses the geodesic curve between them. The authors argue that the shortest path (geodesic curve) is often insufficient to model complex domain shifts and restricts the use of multiple datasets. To overcome these limitations, they propose a novel approach that utilizes spline curves computed via \"rolling maps\" on the Grassmann manifold. This allows for the integration of multiple source domains and better modeling of domain shifts, demonstrating improved performance on standard datasets.\n\n---TOPIC---\nDomain Adaptation\nSubspace Representation\nSpline Flow\nGrassmann Manifold\nRolling Maps",
    "topics": [],
    "references": [
      {
        "citation": "[Baktas, M., Harandi, M. T., Lovell, B. C., & Salzmann, M. Unsupervised domain adaptation by domain invariant projection. ICCV, 2013.]"
      },
      {
        "citation": "[Gopalan, R. Learning cross-domain information transfer for location recognition and clustering. CVPR, 2013.]"
      },
      {
        "citation": "[Gopalan, R., Li, R., & Chellappa, R. Domain adaptation for object recognition: An unsupervised approach. ICCV, 2011.]"
      },
      {
        "citation": "[Carreira, J., Caseiro, R., Batista, J., & Sminchisescu, C. Semantic segmentation with second-order pooling. ECCV, 2012.]"
      },
      {
        "citation": "[Caseiro, R., Henriques, J. F., & Batista, J. Using directional statistics to learn cast shadows from a multi-spectral light sources physical model. ICIP, 2010.]"
      },
      {
        "citation": "[Gopalan, R., Li, R., & Chellappa, R. Unsupervised adaptation across domain shifts by generating intermediate data representations. PAMI, 2014.]"
      },
      {
        "citation": "[Griffin, G., Holub, A., & Perona, P. Caltech-256 object category dataset. Technical report, 2007.]"
      },
      {
        "citation": "[Caseiro, R., Henriques, J. F., Martins, P., & Batista, J. A non-parametric riemannian framework on tensor field application to foreground segmentation. ICCV, 2011.]"
      },
      {
        "citation": "[Hays, J., & Efroos, A. Im2gps: estimating geographic information from a single image. CVPR, 2008.]"
      },
      {
        "citation": "[Pan, S. J., & Yang, Q. A survey on transfer learning. IEEE Trans. Knowledge and Data Engineering, 2010.]"
      }
    ],
    "author_details": [
      {
        "name": "Rui Caseiro",
        "affiliation": "Institute of Systems and Robotics - University of Coimbra",
        "email": "ruicaseiro@isr.uc.pt"
      },
      {
        "name": "João F. Henriques",
        "affiliation": "Institute of Systems and Robotics - University of Coimbra",
        "email": "henriques@isr.uc.pt"
      },
      {
        "name": "Pedro Martins",
        "affiliation": "Institute of Systems and Robotics - University of Coimbra",
        "email": "pedromartins@isr.uc.pt"
      },
      {
        "name": "Jorge Batista",
        "affiliation": "Institute of Systems and Robotics - University of Coimbra",
        "email": "batista@isr.uc.pt"
      }
    ]
  },
  {
    "title": "Sparse Composite Quantization\n---AUTHOR---\nTing Zhang\nGuo-Jun Qi\nJinhui Tang\nJingdong Wang",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Zhang_Sparse_Composite_Quantization_2015_CVPR_paper.pdf",
    "id": "Zhang_Sparse_Composite_Quantization_2015_CVPR_paper",
    "abstract": "This paper introduces sparse composite quantization, a novel approach to approximate nearest neighbor search with high-dimensional data that achieves competitive search accuracy with tractable storage and search cost. Building upon existing techniques like product quantization, Cartesian k-means, and composite quantization, the proposed method addresses the runtime cost of distance table computation, a bottleneck in composite quantization. Sparse composite quantization constructs sparse dictionaries, accelerating distance evaluation through efficient sparse vector operations and significantly reducing distance table computation time. Experiments on large-scale datasets (1M and 1B SIFTs) demonstrate superior performance compared to existing methods, achieving comparable or better accuracy with faster search times.",
    "topics": [
      "Approximate Nearest Neighbor Search (ANN)",
      "Composite Quantization",
      "Sparse Vector Operations",
      "Distance Table Computation",
      "High-Dimensional Data"
    ],
    "references": [
      {
        "citation": "[A. Babenko and V. S. Lempitsky. The inverted multi-index. In CVPR, pages 3069–3076, 2012.] - Referenced multiple times, indicating core methodology."
      },
      {
        "citation": "[H. Jégou, M. Douze, and C. Schmid. Product quantization for nearest neighbor search. IEEE Trans. Pattern Anal. Mach. Intell., 33(1):117–128, 2011.] - Foundational work on product quantization, a key technique."
      },
      {
        "citation": "[A. Babenko and V. S. Lempitsky. Improving bilayer product quantization for billion-scale approximate nearest neighbors in high dimensions. CoRR, abs/1404.1831, 9999.] - Builds upon product quantization for large-scale applications."
      },
      {
        "citation": "[M. Norouzi and D. J. Fleet. Cartesian k-means. In CVPR, pages 3017–3024, 2013.] - Introduces Cartesian k-means, a significant variation."
      },
      {
        "citation": "[D. Nistér and H. Stewénius. Scalable recognition with a vocabulary tree. In CVPR (2), pages 2161–2168, 2006.] - Important for understanding vocabulary trees and their role."
      },
      {
        "citation": "[H. Jégou, M. Douze, and C. Schmid. Hamming embedding and weak geometric consistency for large scale image search. In ECCV (1), pages 304–317, 2008.] - Relevant for understanding Hamming embeddings."
      },
      {
        "citation": "[J. Wang, S. Kumar, and S.-F. Chang. Semi-supervised hashing for large-scale search. IEEE Trans. Pattern Anal. Mach. Intell., 34(12):2393–2406, 2012.] - Explores semi-supervised hashing techniques."
      },
      {
        "citation": "[H. Jégou, R. Tavenard, M. Douze, and L. Amsaleg. Searching in one billion vectors: Re-rank with source coding. In ICASE, pages 861–864, 2011.] - Addresses re-ranking strategies for large-scale search."
      },
      {
        "citation": "[J. Wang and S. Li. Query-driven iterated neighborhood graph search for large scale indexing. In ACM Multimedia, pages 179–188, 2012.] - Focuses on graph-based search methods."
      },
      {
        "citation": "[Y. Gong, S. Kumar, V. Verma, and S. Lazebnik. Angular quantization-based binary codes for fast similarity search. In NIPS, pages 1205–1213, 2012.] - Introduces angular quantization for binary codes."
      }
    ],
    "author_details": [
      {
        "name": "Ting Zhang",
        "affiliation": "University of Science and Technology of China, P.R. China",
        "email": "*Not available*"
      },
      {
        "name": "Guo-Jun Qi",
        "affiliation": "University of Central Florida, USA",
        "email": "*Not available*"
      },
      {
        "name": "Jinhui Tang",
        "affiliation": "Nanjing University of Science and Technology, P.R. China",
        "email": "*Not available*"
      },
      {
        "name": "Jingdong Wang",
        "affiliation": "Microsoft Research, P.R. China",
        "email": "*Not available*"
      }
    ]
  },
  {
    "title": "On the Appearance of Translucency Edges\n---AUTHOR---\nIoannis Gkioulekas\nBruce Walter\nEdward H. Adelson\nKavita Bala\nTodd Zickler",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Gkioulekas_On_the_Appearance_2015_CVPR_paper.pdf",
    "id": "Gkioulekas_On_the_Appearance_2015_CVPR_paper",
    "abstract": "Edges in images of translucent objects differ significantly from those in opaque objects. This paper investigates the physical causes of edges caused by a discontinuity in surface orientation in translucent materials. Through simulations of thousands of translucency edge profiles with varying scattering parameters, the authors explain the resulting edge patterns through qualitative analysis of light transport. They also discuss the existence of shape and material metamerism. This research has implications for visual inference tasks involving translucent objects, such as shape and material estimation.\n\n---TOPICSS---\nTranslucent Edges\nLight Transport\nMaterial Metamers\nVisual Inference\nSurface Orientation Discontinuities",
    "topics": [],
    "references": [
      {
        "citation": "[A. Ishimaru. Wave propagation and scattering in random media. Wiley-IEEE, 1978.] - This paper provides a foundational understanding of wave behavior in translucent materials, crucial for the work on stereo reconstruction."
      },
      {
        "citation": "[E. H. Adelson. On seeing stuff: the perception of materials by humans and machines. Proceedings of the SPIE Vol. 4299, Human Vision and Electronic Imaging VI, 2001.] - Provides context on how humans perceive translucent materials, informing the development of machine vision approaches."
      },
      {
        "citation": "[H. Jensen, S. Marschner, M. Levoy, and P. Hanrahan. A practical model for subsurface light transport. In SIGGRAPH, 2001.] -  Addresses the complexities of light transport within translucent materials, a key consideration for accurate rendering and reconstruction."
      },
      {
        "citation": "[R. Fergus, B. Singh, A. Hertzmann, S. T. Roweis, and W. T. Freeman. Removing camera shake from a single photograph. ACM Transactions on Graphics, 2006.] - Relevant for potential pre-processing steps to mitigate artifacts in images of translucent objects."
      },
      {
        "citation": "[I. Gkioulekas, B. Xiao, S. Zhao, E. Adelson, T. Zickler, and K. Bala. Understanding the role of phase function in translucent appearance. ACM Transactions on Graphics, 2013.] - Directly addresses the importance of phase functions in the appearance of translucent materials."
      },
      {
        "citation": "[B. Xiao, B. Walter, I. Gkioulekas, T. Zickler, E. Adelson, and K. Bala. Looking against the light: how perception of translucency depends on lighting direction and phase function. Journal of Vision, 2014.] - Explores the influence of lighting direction on the perception of translucency."
      },
      {
        "citation": "[C. Donner and H. Jensen. Rendering translucent materials using photon diffusion. ACM SIGGRAPH 2008 Classes, 2008.] - Discusses a rendering technique specifically for translucent materials."
      },
      {
        "citation": "[S. Bell, P. Upchurch, N. Snavely, and K. Bala. Open-surfaces: A richly annotated catalog of surface appearance. ACM Transactions on Graphics, 2013.] - Provides a dataset of surface appearances, potentially useful for training or evaluation."
      },
      {
        "citation": "[I. Gkioulekas, B. Xiao, S. Zhao, E. Adelson, T. Zickler, and K. Bala. Understanding the role of phase function in translucent appearance. ACM Transactions on Graphics, 2013.] - A key paper directly related to the topic of translucent materials."
      },
      {
        "citation": "[S. Bell, P. Upchurch, N. Snavely, and K. Bala. Material recognition in the wild with the materials in context database. IEEE CVPR, 2015.] - Provides a dataset and approach for material recognition, which can be relevant for understanding translucent objects."
      }
    ],
    "author_details": [
      {
        "name": "Ioannis Gkioulekas",
        "affiliation": "Harvard SEAS",
        "email": "igkiou@seas.harvard.edu"
      },
      {
        "name": "Bruce Walter",
        "affiliation": "Cornell University",
        "email": "bruce.walter@cornell.edu"
      },
      {
        "name": "Edward H. Adelson",
        "affiliation": "Massachusetts Institute of Technology",
        "email": "adelson@cail.mit.edu"
      },
      {
        "name": "Kavita Bala",
        "affiliation": "Cornell University",
        "email": "kb@cs.cornell.edu"
      },
      {
        "name": "Todd Zickler",
        "affiliation": "Harvard SEAS",
        "email": "zickler@seas.harvard.edu"
      }
    ]
  },
  {
    "title": "Learning graph structure for multi-label image classification via clique generation\n---AUTHOR---\nMingkui Tan\nQinfeng Shi\nAnton van den Hengel\nChunhua Shen\nJunbin Gao\nFuyuan Hu\nZhen Zhang",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Tan_Learning_Graph_Structure_2015_CVPR_paper.pdf",
    "id": "Tan_Learning_Graph_Structure_2015_CVPR_paper",
    "abstract": "Exploiting label dependency for multi-label image classification can significantly improve classification performance. Probabilistic Graphical Models are one of the primary methods for representing such dependencies. The structure of graphical models, however, is either determined heuristically or learned from very limited information. We propose a principled way to learn the structure of a graphical model by considering input features and labels, together with loss functions. We formulate this problem into a max-margin framework initially, and then transform it into a convex programming problem. Finally, we propose a highly scalable procedure that activates a set of cliques iteratively. Our approach exhibits both strong theoretical properties and a significant performance improvement over state-of-the-art methods on both synthetic and real-world data sets.",
    "topics": [
      "Multi-label image classification",
      "Probabilistic Graphical Models (PGMs)",
      "Graph structure learning",
      "Clique generation",
      "Max-margin framework"
    ],
    "references": [
      {
        "citation": "[Boutell, M. R., Luo, J., Shen, X., & Brown, C. M. (2004). Learning multi-label scene classiﬁcation. *Pattern Recognition*, *37*(9), 1757–1771.]"
      },
      {
        "citation": "[Bradley, J. K., & Guestrin, C. (2010). Learning tree conditional random ﬁelds. In *Proceedings of the 19th International Conference on Machine Learning*.]"
      },
      {
        "citation": "[Bucak, S. S., Mallapragada, P. Kumar, Jin, R., & Jain, A. K. (2009). Efﬁcient multi-label ranking for multi-class learning: application to object recognition. In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*.]"
      },
      {
        "citation": "[Cai, X., Nie, F., Cai, W., & Huang, H. (2013). New graph structured sparsity model for multi-label image annotations. In *Proceedings of the IEEE International Conference on Computer Vision*.]"
      },
      {
        "citation": "[Chow, C., & Liu, C. (1968). Approximating discrete probability distributions with dependence trees. *IEEE Transactions on Information Theory*, *14*(3), 462–467.]"
      },
      {
        "citation": "[Dembczyński, K., Waegeman, W., Cheng, W., & Hüllermeier, E. (2012). On label dependence and loss minimization in multi-label classiﬁcation. *Machine Learning*, *88*(1-2), 5–45.]"
      },
      {
        "citation": "[Dembczynski, K., Waegeman, W., & Hüllermeier, E. (2012). An analysis of chaining in multi-label classiﬁcation. In *Proceedings of the European Conference on Artificial Intelligence*.]"
      },
      {
        "citation": "[Ding, S., Wahba, G., & Zhu, X. (2011). Learning higher-order graph structure with features by structure penalty. In *Advances in Neural Information Processing Systems*, 253–261.]"
      },
      {
        "citation": "[Everingham, M., Van Gool, L., Williams, C. K. I., Winn, J., & Zisserma, A. (2012). The PASUAL Visual Object Classes Challenge 2012 (VOC2012) Results.]"
      },
      {
        "citation": "[Fan, R.-E., Chang, K.-W., Hsieh, C.-J., Wang, X.-R., & Lin, C.-J. (2008). LIBLINEAR: A library for large linear classification. *Journal of Machine Learning Research*, 1871–1874.]"
      }
    ],
    "author_details": [
      {
        "name": "Mingkui Tan",
        "affiliation": "University of Adelaide",
        "email": "mingkui.tan@adelaide.edu.au"
      },
      {
        "name": "Qinfeng Shi",
        "affiliation": "University of Adelaide",
        "email": "javen.shi@adelaide.edu.au"
      },
      {
        "name": "Anton van den Hengel",
        "affiliation": "University of Adelaide, Australian Centre for Robotic Vision",
        "email": "N/A"
      },
      {
        "name": "Chunhua Shen",
        "affiliation": "University of Adelaide, Australian Centre for Robotic Vision",
        "email": "N/A"
      },
      {
        "name": "Junbin Gao",
        "affiliation": "Charles Sturt University",
        "email": "N/A"
      },
      {
        "name": "Fuyuan Hu",
        "affiliation": "University of Adelaide",
        "email": "N/A"
      },
      {
        "name": "Zhen Zhang",
        "affiliation": "University of Adelaide",
        "email": "N/A"
      }
    ]
  },
  {
    "title": "ConceptLearner: Discovering Visual Concepts from Weakly Labeled Image Collections\n---AUTHOR---\nBolei Zhou\nVignesh Jagadeesh\nRobinson Piramuthu",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Zhou_ConceptLearner_Discovering_Visual_2015_CVPR_paper.pdf",
    "id": "Zhou_ConceptLearner_Discovering_Visual_2015_CVPR_paper",
    "abstract": "Discovering visual knowledge from weakly labeled data is crucial to scale up computer vision recognition systems, as obtaining fully labeled data is expensive. This paper proposes ConceptLearner, a scalable approach to discover visual concepts from weakly labeled image collections. Thousands of visual concept detectors are learned automatically without human annotation. These detectors can be applied to recognize concepts at image-level and detect concepts at image region-level. Under domain-specific supervision, the learned concepts are evaluated for scene recognition and object detection, demonstrating promising performance compared to fully supervised and weakly supervised methods.\n\n---TOPICCS---\nWeakly labeled data\nVisual concept learning\nConcept recognition and detection\nScalable algorithms\nDomain-specific supervision",
    "topics": [],
    "references": [
      {
        "citation": "[Berg, T. L., Berg, A. C., & Shih, J. Automatic attribute discovery and characterization from noisy web data. In Proc.]"
      },
      {
        "citation": "[Li, J., & Wang, J. Z. Real-time computerized annotation of pictures. IEEE Trans. Pattern Anal. Mach. Intell., 30(6):985–1002, June 2008.]"
      },
      {
        "citation": "[Ordonez, V., Kulkarni, G., & Berg, T. L. Im2text: Describing images using 1 million captioned photographs. In NIPS, 2011.]"
      },
      {
        "citation": "[Tang, K., Ramanathan, V., Fei-Fei, L., & Koller, D. Shifting weights: Adapting object detectors from image to video. In NIPS, 2012.]"
      },
      {
        "citation": "[Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., & Fei-Fei, L. Imaginet: A large-scale hierarchical image database. In Proc. CVPR, 2009.]"
      },
      {
        "citation": "[Krizhevsky, A., Sutskever, I., & Hinton, G. E. Imaginet classification with deep convolutional neural networks. In NIPS, 2012.]"
      },
      {
        "citation": "[Zhou, B., Lapedriza, A., Xiao, J., Torralba, A., & Oliva, A. Learning deep features for scene recognition using places database. In NIPS, 2014.]"
      },
      {
        "citation": "[Srihari, R. K. Piction: A system that uses captions to label human faces in newspaper photographs. In T. L. Dean and K. McKeown, editors, AAAI, pages 80–85. AAAI Press / The MIT Press, 1991.]"
      },
      {
        "citation": "[Divvala, S. K., Farhad, A., & Guestrin, C. Learning everything about anything: Webly-supervised visual concept learning. In Proc. CVPR, 2014.]"
      },
      {
        "citation": "[Girshick, R., Donahue, J., Darrell, T., & Malik, J. Rich feature hierarchies for accurate object detection and semantic segmentation. In Proc. CVPR, 2014.]"
      }
    ],
    "author_details": [
      {
        "name": "Bolei Zhou",
        "affiliation": "MIT",
        "email": "bolei@mit.edu"
      },
      {
        "name": "Vignesh Jagadeesh",
        "affiliation": "eBay Research Labs",
        "email": "vjagadeesh@ebay.com"
      },
      {
        "name": "Robinson Piramuthu",
        "affiliation": "eBay Research Labs",
        "email": "rpiramuthu@ebay.com"
      }
    ]
  },
  {
    "title": "Subgraph Matching using Compactness Prior for Robust Feature Correspondence\n---AUTHOR---\nYumin Suh\n---AUTHOR---\nKamil Adamczewski\n---AUTHOR---\nKyoung Mu Lee",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Suh_Subgraph_Matching_Using_2015_CVPR_paper.pdf",
    "id": "Suh_Subgraph_Matching_Using_2015_CVPR_paper",
    "abstract": "Feature correspondence plays a central role in various computer vision applications. It is widely formulated as a graph matching problem due to its robust performance under challenging conditions. However, most graph matching algorithms focus on improving recall while rarely considering precision, leading to solutions with numerous outliers. This paper proposes a new subgraph matching formulation using a compactness prior, an additional constraint that prefers sparcity and eliminates outliers. A meta-algorithm based on Markov chain Monte Carlo is proposed to solve the new optimization problem. Experiments demonstrate that the proposed formulation and algorithm significantly improve baseline performance under challenging conditions with outliers and deformation noise.",
    "topics": [
      "Subgraph Matching",
      "Compactness Prior",
      "Markov Chain Monte Carlo (MCMC)",
      "Feature Correspondence",
      "Graph Matching"
    ],
    "references": [
      {
        "citation": "[Anchuri, P., Zaki, M. J., Barkol, O., Golan, S., & Shamy, M. (2013). Approximate graph mining with label costs. In *Proceedings of the 19th ACM SIGKDD international conference*. ACM.]"
      },
      {
        "citation": "[Caetano, T. S., McAuley, J. J., Cheng, L., Le, Q. V., & Smola, A. J. (2009). Learning graph matching. *IEEE Transactions on Pattern Analysis and Machine Intelligence*, *31*(6), 1048–1058.]"
      },
      {
        "citation": "[Cho, M., Alahari, K., & Ponce, J. (2013). Learning graphs to match. In *Proceedings of the IEEE International Conference on Computer Vision*.]"
      },
      {
        "citation": "[Cho, M., & Lee, J. (2009). Feature correspondence and deformable object matching via agglomerative correspondence clustering. In *2009 IEEE 12th International Conference on Computer Vision*, pages 1280–1287. IEEE.]"
      },
      {
        "citation": "[Cho, M., Lee, J., & Lee, K. M. (2010). Reweighted random walks for graph matching. In *Computer Vision–ECCV 2010*, pages 492–505. Springer.]"
      },
      {
        "citation": "[Cho, M., & Lee, K. M. (2012). Progressive graph matching: Making a move of graphs via probabilistic voting. In *Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on*, pages 398–405. IEEE.]"
      },
      {
        "citation": "[Cho, M., Sun, J., Duchenne, O., & Ponce, J. (2013). Finding matches in a haystack: A max-pooling strategy for graph matching in the presence of outliers. In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*.]"
      },
      {
        "citation": "[Cour, T., Srinivasan, P., & Shi, J. (2007). Balanced graph matching. MIT.]"
      },
      {
        "citation": "[Duchenne, O., Bach, F., Kweon, I.-S., & Ponce, J. (2011). A tensor-based algorithm for high-order graph matching. *IEEE Transactions on Pattern Analysis and Machine Intelligence*, *33*(12), 2383–2395.]"
      },
      {
        "citation": "[Gilks, W. R. (2005). Markov chain monte carlo. Wiley Online Library.]"
      }
    ],
    "author_details": [
      {
        "name": "Yumin Suh",
        "affiliation": "Seoul National University",
        "email": "n12345@snu.ac.kr"
      },
      {
        "name": "Kamil Adamczewski",
        "affiliation": "Seoul National University",
        "email": "kamil.m.adamczewski@gmail.com"
      },
      {
        "name": "Kyoung Mu Lee",
        "affiliation": "Seoul National University",
        "email": "kyoungmu@snu.ac.kr"
      }
    ]
  },
  {
    "title": "Good Features to Track for Visual SLAM\n---AUTHOR---\nGuancong Zhang\nPatricio A. Vela",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Zhang_Good_Features_to_2015_CVPR_paper.pdf",
    "id": "Zhang_Good_Features_to_2015_CVPR_paper",
    "abstract": "Not all measured features in S LAM/SfM contribute to accurate localization during the estimation process, thus it is sensible to utilize only those that do. This paper describes a method for selecting a subset of features that are of high utility for localization in the S LAM/SfM estimation process. It is derived by examining the observability of S LAM and, being complimentary to the estimation process, it easily integrates into existing S LAM systems. The measure of estimation utility is formulated with temporal and instantaneous observability indices. Efﬁcient computation strategies for the observability indices are described based on incremental singular value decomposition (SVD) and greedy selection for the temporal and instantaneous observability indices, respectively. The greedy selection is near-optimal since the observability index is (approximately) submodular. The proposed method improves localization and data association. Controlled synthetic experiments with ground truth demonstrate the improved localization accuracy, and real-time S LAM experiments demonstrate the improved data association.\n\n---TOPICCS---\nVisual SLAM\nFeature Selection\nObservability Analysis\nData Association\nIncremental Singular Value Decomposition (SVD)",
    "topics": [],
    "references": [
      {
        "citation": "[J. Andrade-Cetto and A. Sanfeliu, The effects of partial observability when building fully correlated maps, IEEE Transactions on Robotics, 21(4):771–777, 2005.]"
      },
      {
        "citation": "[M. Kaess and F. Dellaert, Covariance recovery from a square root information matrix for data association, Robotics and Autonomous Systems, 57(12):1198–1210, 2009.]"
      },
      {
        "citation": "[A. J. Davison, I. D. Reid, N. D. Molton, and O. Stasse, MonoSLAM: Real-time single camera SLAM, IEEE Transactions on Pattern Analysis and Machine Intelligence, 29(6):1052–1067, 2007.]"
      },
      {
        "citation": "[J. Engel, T. Sch¨ops, and D. Cremers, LSD-SLAM: Large-Scale Direct monocular SLAM, European Conference on Computer Vision, pages 225–234, 2007.]"
      },
      {
        "citation": "[S. Boyd and L. Vandenberghe, Convex optimization, Cambridge University Press, 2009.]"
      },
      {
        "citation": "[A. Davison, Active search for real-time vision, IEEE International Conference on Computer Vision, volume 1, pages 66–73, 2005.]"
      },
      {
        "citation": "[G. P. Huang, A. I. Mourikis, and S. I. Roumeliotis, Observability-based rules for designing consistent EKF SLAM estimators, International Journal of Robotics Research, 29(5):502–528, 2010.]"
      },
      {
        "citation": "[M. Kaess, H. Johannsson, R. Roberts, V. Ila, J. J. Leonard, and F. Dellaert, iSAM2: Incremental smoothing and mapping using the bayes tree, International Journal of Robotics Research, 31:217–236, 2012.]"
      },
      {
        "citation": "[R. A. Newcombe and A. J. Davison, Live dense reconstruction with a single moving camera, IEEE Conference on Computer Vision and Pattern Recognition, pages 1498–1505, 2010.]"
      }
    ],
    "author_details": [
      {
        "name": "Guancong Zhang",
        "affiliation": "School of ECE, Georgia Tech.",
        "email": "zhanggc@gatech.edu"
      },
      {
        "name": "Patricio A. Vela",
        "affiliation": "School of ECE, Georgia Tech.",
        "email": "pvela@gatech.edu"
      }
    ]
  },
  {
    "title": "Small Instance Detection by Integer Programming on Object Density Maps\n---AUTHOR---\nZheng Ma\nLei Yu\nAntoni B. Chan",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Ma_Small_Instance_Detection_2015_CVPR_paper.pdf",
    "id": "Ma_Small_Instance_Detection_2015_CVPR_paper",
    "abstract": "We propose a novel object detection framework for partially-occluded small instances, such as pedestrians in low resolution surveillance video, cells under a microscope, flocks of small animals (e.g. birds, fishes), or even tiny insects like honeybees and flies. These scenarios are very challenging for traditional detectors, which are typically trained on individual instances. In our approach, we first estimate the object density map of the input image, and then divide it into local regions. For each region, a sliding window (ROI) is passed over the density map to calculate the instance count within each ROI. 2D integer programming is used to recover the locations of object instances from the set of ROI counts, and the global count estimate of the density map is used as a constraint to regularize the detection performance. Finally, the bounding box for each instance is estimated using the local density map. Compared with current small-instance detection methods, our proposed approach achieves state-of-the-art performance on several challenging datasets including fluorescence microscopy cell images, UCSD pedestrians, small animals and insects.\n\n---TOPICICS---\nSmall Instance Detection\nObject Density Maps\nInteger Programming\nCrowd Counting\nComputer Vision",
    "topics": [],
    "references": [
      {
        "citation": "[Felzenszwalb, P., McAllester, D., & Ramanan, D. (2008). A discriminatively trained, multiscale, deformable part model. *IEEE Conf. Computer Vision and Pattern Recognition*, 1–8.] - A seminal work on deformable part models, likely relevant to object detection and segmentation."
      },
      {
        "citation": "[Dalal, N., & Triggs, B. (2005). Histograms of oriented gradients for human detection. *IEEE Conf. Computer Vision and Pattern Recognition*, 1, 886–93.] - Introduces HOG features, a common feature descriptor for human detection."
      },
      {
        "citation": "[Chan, A. B., & Vasconcelos, N. (2012). Counting people with low-level features and bayesian regression. *IEEE Trans. on Image Processing*, *21*(4), 2160–2177.] - A key paper directly addressing crowd counting using Bayesian regression and low-level features."
      },
      {
        "citation": "[Lempitsky, V., & Zisserma, A. (2010). Learning to count objects in images. *Advances in Neural Information Processing Systems*, 2010.] - A foundational paper on learning to count objects, likely using neural networks."
      },
      {
        "citation": "[Ryan, D., Denman, S., Fookes, C., & Sridharan, S. (2009). Crowd counting using multiple local features. *Digital Image Computing: Techniques and Applications*, 81–88.] - Directly addresses crowd counting using multiple local features."
      },
      {
        "citation": "[Lowe, D. G. (2004). Distinctive image features from scale-invariant keypoints. *Intl. Journal of Computer Vision*, *60*(1), 91–110.] - Introduces SIFT features, a widely used method for feature detection and matching."
      },
      {
        "citation": "[Chan, A. B., Liang, Z.-S. J., & Vasconcelos, N. (2008). Privacy preserving crowd monitoring: Counting people without people models or tracking. *IEEE Conf. Computer Vision and Pattern Recognition*, 1–7.] - Addresses privacy concerns in crowd monitoring."
      },
      {
        "citation": "[Zhao, T., & Nevatia, R. (2003). Bayesian human segmentation in crowded situations. *IEEE Conf. Computer Vision and Pattern Recognition*, II, 459–66.] - Focuses on Bayesian segmentation specifically for crowded scenes."
      },
      {
        "citation": "[Lempitsky, V. (2010). Learning to count objects in images. http://cmp.felk.cvut.cz/cmp/events/colloquium-2010.10.21/.] - Another work by Lempitsky on learning to count objects."
      },
      {
        "citation": "[Everingham, M., Van Gool, L., Williams, C., Winn, J., & Zisserma, A. (2010). The pascal visual object classes (voc) challenge. *International Journal of Computer Vision*, *88*(3), 303–338.] - Describes the PASCAL VOC challenge, a common benchmark for object detection and segmentation."
      },
      {
        "citation": "---"
      }
    ],
    "author_details": [
      {
        "name": "Zheng Ma",
        "affiliation": "City University of Hong Kong",
        "email": "zhengma2-c@my.cityu.edu.hk"
      },
      {
        "name": "Lei Yu",
        "affiliation": "City University of Hong Kong",
        "email": "leiyu6-c@my.cityu.edu.hk"
      },
      {
        "name": "Antoni B. Chan",
        "affiliation": "City University of Hong Kong",
        "email": "abchan@cityu.edu.hk"
      }
    ]
  },
  {
    "title": "Computationally Bounded Retrieval\n---AUTHOR---\nMohammad Rastegari\nCem Keskin\nPushmeet Kohli\nShahram Izadi",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Rastegari_Computationally_Bounded_Retrieval_2015_CVPR_paper.pdf",
    "id": "Rastegari_Computationally_Bounded_Retrieval_2015_CVPR_paper",
    "abstract": "The increase in size of large image databases makes the problem of efficient retrieval extremely challenging, especially in high dimensional data. Unlike most hashing methods that sacrifice accuracy for speed, this paper proposes a novel method that improves the speed of high dimensional image retrieval by several orders of magnitude without any significant drop in performance. The method learns computationally bounded sparse projections for the encoding step and adds an orthogonality constraint on projections to reduce bit correlation. An iterative scheme jointly optimizes this objective, resulting in fast and efficient projections. Experiments on ImageNET, GIST1M, and SUN-attribute demonstrate a speed-up of up to a factor of 100 over state-of-the-art methods, while maintaining or improving accuracy.\n\n---TOPICCS---\nImage Retrieval\nHashing Methods\nBinary Codes\nDimensionality Reduction\nComputational Efficiency",
    "topics": [],
    "references": [
      {
        "citation": "[Andoni, A., & Indyk, P. (2006). Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions. *Foundations of Computer Science, 47th Annual IEEE Symposium on*, 459–468.]"
      },
      {
        "citation": "[Datar, M., Immorlica, N., Indyk, P., & Mirrokni, V. S. (2004). Locality-sensitive hashing scheme based on p-stable distributions. *Proceedings of the Twentieth Annual Symposium on Computational Geometry*, 2004.]"
      },
      {
        "citation": "[Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., & Fei-Fei, L. (2009). Imaginet: A large-scale hierarchical image database. *Computer Vision and Pattern Recognition, 2009. CVPR 2009.* IEEE, 248–255.]"
      },
      {
        "citation": "[Fan, R.-E., Chang, K.-W., Hsieh, C.-J., Wang, X.-R., & Lin, C.-J. (2008). Liblinear: A library for large linear classification. *The Journal of Machine Learning Research*, 9(1), 1871–1874.]"
      },
      {
        "citation": "[Gong, Y., Kumar, S., Rowley, H. A., & Lazebnik, S. (2013). Learning binary codes for high-dimensional data using bilinear projections. *Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on*, 484–491.]"
      },
      {
        "citation": "[Gong, Y., & Lazebnik, S. (2011). Iterative quantization: A procustean approach to learning binary codes. *Proceedings of the 2011 IEEE Conference on Computer Vision and Pattern Recognition*, 2011.]"
      },
      {
        "citation": "[Jégou, H., Douze, M., Schmid, C., et al. (2009). Searching with quantization: approximate nearest neighbor search using short codes and distance estimators.]"
      },
      {
        "citation": "[Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). Imaginet classification with deep convolutional neural networks. *Advances in neural information processing systems*, 1097–1105.]"
      },
      {
        "citation": "[Maji, S., Berg, A. C., & Malik, J. (2013). Efﬁcient classiﬁcation for additive kernel svms. *IEEE Trans. Pattern Anal. Mach. Intell.*]"
      },
      {
        "citation": "[Norouzi, M., & Fleet, D. J. (2011). Minimal loss hashing for compact binary codes. *Proceedings of the 28th International Conference on Machine Learning*, 2011.]"
      }
    ],
    "author_details": [
      {
        "name": "Mohammad Rastegari",
        "affiliation": "University of Maryland",
        "email": "mrastega@cs.umd.edu"
      },
      {
        "name": "Cem Keskin",
        "affiliation": "Microsoft Research",
        "email": "cemke@microsoft.com"
      },
      {
        "name": "Pushmeet Kohli",
        "affiliation": "Microsoft Research",
        "email": "pKohli@microsoft.com"
      },
      {
        "name": "Shahram Izadi",
        "affiliation": "Microsoft Research",
        "email": "shahrami@microsoft.com"
      }
    ]
  },
  {
    "title": "New Insights into Laplacian Similarity Search\n---AUTHOR---\nXiao-Ming Wu\nZhenguo Li\nShih-Fu Chang",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Wu_New_Insights_Into_2015_CVPR_paper.pdf",
    "id": "Wu_New_Insights_Into_2015_CVPR_paper",
    "abstract": "Graph-based computer vision applications critically rely on similarity metrics to compute pairwise similarity between vertices. This paper investigates the fundamental design of commonly used similarity metrics, revealing hidden assumptions, characterizing their behaviors, and solving the model selection problem. The authors introduce a family of similarity metrics in the form of (L + αΛ)−1, where L is the graph Laplacian, Λ is a positive diagonal matrix acting as a regularizer, and α is a positive balancing factor. These metrics respect graph topology when α is small and unify several well-known metrics. A key contribution is the analysis of the impact of selecting the regularizer Λ, finding that different choices lead to complementary behaviors depending on cluster density. The paper proposes a new design of Λ that automatically adapts to local density and validates the approach with image retrieval experiments.\n\n---TOPICICS---\nGraph Similarity Metrics\nLaplacian Eigenmaps\nRegularization\nGraph Topology\nImage Retrieval",
    "topics": [],
    "references": [
      {
        "citation": "[Page, L., Brin, S., Motwani, R., & Winograd, T. (1999). The pagerank citation ranking: Bringing order to the web.]"
      },
      {
        "citation": "[Chung, F. (1997). Spectral Graph Theory. American Mathematical Society.]"
      },
      {
        "citation": "[Andersen, R., Chung, F., & Lang, K. (2006). Local graph partitioning using pagerank vectors. In FOCS, pages 475–486.]"
      },
      {
        "citation": "[Belkin, M., & Niyogi, P. (2001). Laplacian eigenmaps and spectral techniques for embedding and clustering. In NIPS, volume 14, pages 585–591.]"
      },
      {
        "citation": "[Shi, J., & Malik, J. (2000). Normalized cuts and image segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 22(8), 888–905.]"
      },
      {
        "citation": "[Liu, W., Wang, J., Kumar, S., & Chang, S.-F. (2011). Hashing with graphs. In ICML, pages 1–8.]"
      },
      {
        "citation": "[Oliva, A., & Torralba, A. (2001). Modeling the shape of the scene: A holistic representation of the spatial envelope. International Journal of Computer Vision, 42(3), 145–175.]"
      },
      {
        "citation": "[Grady, L. (2006). Random walks for image segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 28(11), 1768–1783.]"
      },
      {
        "citation": "[He, X., Yan, S., Hu, Y., Niyogi, P., & Zhang, H.-J. (2005). Face recognition using laplacianfaces. IEEE Transactions on Pattern Analysis and Machine Intelligence, 27(3), 328–340.]"
      },
      {
        "citation": "[Wu, X.-M., Li, Z., & Chang, S.-F. (2013). Analyzing the harmonic structure in graph-based learning. In NIPS, pages 1964–1972.]"
      }
    ],
    "author_details": [
      {
        "name": "Xiao-Ming Wu",
        "affiliation": "Department of Electrical Engineering, Columbia University",
        "email": "xmwu@ee.columbia.edu"
      },
      {
        "name": "Zhenguo Li",
        "affiliation": "Huawei Noah’s Ark Lab, Hong Kong",
        "email": "li.zhenguo@huawei.com"
      },
      {
        "name": "Shih-Fu Chang",
        "affiliation": "Department of Electrical Engineering, Columbia University",
        "email": "sfchang@ee.columbia.edu"
      }
    ]
  },
  {
    "title": "Saliency-Aware Geodesic Video Object Segmentation\n---AUTHOR---\nWenguan Wang\n---AUTHOR---\nJianbing Shen\n---AUTHOR---\nFatih Porikli",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Wang_Saliency-Aware_Geodesic_Video_2015_CVPR_paper.pdf",
    "id": "Wang_Saliency-Aware_Geodesic_Video_2015_CVPR_paper",
    "abstract": "This paper introduces a new unsupervised, geodesic distance-based, salience-aware video object segmentation method. Unlike traditional methods, the approach incorporates salience as a prior for object segmentation through robust geodesic measurement. It utilizes spatial edges and temporal motion boundaries as indicators, generating frame-wise spatiotemporal salience maps. The method builds global appearance models for foreground and background, establishes dynamic location models, and combines these elements within an energy minimization framework to achieve spatially and temporally coherent object segmentation. Experimental results on benchmark datasets demonstrate the superiority of the proposed method over existing algorithms.\n\n---TOPICCS---\nVideo Object Segmentation\nGeodesic Distance\nVisual Salience\nMotion Boundaries\nEnergy Minimization",
    "topics": [],
    "references": [
      {
        "citation": "[R. Achanta, A. Shaji, K. Smith, A. Lucchi, P. Fua, and S. Susstrunk. Slic superpixels compared to state-of-the-art superpixel methods. IEEE TPAM, 34(11), 2012.] - This likely provides a baseline or comparison method for superpixel generation, a common technique in image segmentation."
      },
      {
        "citation": "[F. Perazzi, P. Krahenbuhl, Y. Pritch, and A. Hornung. Saliency filters: Contrast based filtering for salient region detection. In CVPR, 2012.] - Salient region detection is often a precursor to segmentation, so this is relevant."
      },
      {
        "citation": "[C. Antonio, S. Toby, and B. Andrew. Geos: geodesic image segmentation. In ECCV, 2008.] - Introduces a specific geodesic image segmentation approach."
      },
      {
        "citation": "[B. Price, B. Morse, and S. Cohen. Geodesic graph cut for interactive image segmentation. In CVPR, 2010.] - Another reference related to geodesic methods and interactive segmentation."
      },
      {
        "citation": "[T. Brox and J. Malik. Object segmentation by long term analysis of point trajectories. In ECCV, 2010.] - Focuses on object segmentation using trajectory analysis, which is relevant for video segmentation."
      },
      {
        "citation": "[C. Rother, V. Kolmogorov, and A. Blake. Grabcut: Interactive foreground extraction using iterated graph cuts. ACM TOG, 23(3), 2004.] - Grabcut is a well-known interactive segmentation method."
      },
      {
        "citation": "[J. Carreira and C. Sminchisescu. Constrained parametric min-cuts for automatic object segmentation. In CVPR, 2010.] - Discusses a min-cut based approach for object segmentation."
      },
      {
        "citation": "[W. Brendel and S. Todorovic. Video object segmentation by tracking regions. In CVPR, 2009.] - Directly addresses video object segmentation."
      },
      {
        "citation": "[A. Criminisi, T. Sharp, C. Rother, and P. Perez. Geodesic image and video editing. ACM TOG, 29(5), 2010.] - Combines geodesic methods with video editing, potentially offering insights into video manipulation."
      },
      {
        "citation": "[D. Tsai, M. Flagg, A. Nakazawa, and J. M. Rehg. Motion coherent tracking using multi-label mrf optimization. In IJCV, 2012.] - Addresses motion coherence in tracking, a key aspect of video object segmentation."
      }
    ],
    "author_details": [
      {
        "name": "Wenguan Wang",
        "affiliation": "Beijing Lab of Intelligent Information Technology, School of Computer Science, Beijing Institute of Technology, China",
        "email": "[Email not available in the provided text]"
      },
      {
        "name": "Jianbing Shen",
        "affiliation": "Beijing Lab of Intelligent Information Technology, School of Computer Science, Beijing Institute of Technology, China",
        "email": "[Email not available in the provided text]"
      },
      {
        "name": "Fatih Porikli",
        "affiliation": "Research School of Engineering, Australian National University, and NICTA Australia",
        "email": "[Email not available in the provided text]"
      }
    ]
  },
  {
    "title": "Best-Buddies Similarity for Robust Template Matching\n---AUTHOR---\nTali Dekel\nShaul Oron\nMichael Rubinstein\nShai Avidan\nWilliam T. Freeman",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Dekel_Best-Buddies_Similarity_for_2015_CVPR_paper.pdf",
    "id": "Dekel_Best-Buddies_Similarity_for_2015_CVPR_paper",
    "abstract": "We propose a novel method for template matching in unconstrained environments. Its essence is the Best-Buddies Similarity (BBS), a useful, robust, and parameter-free similarity measure between two sets of points. BBS is based on counting the number of Best-Buddie Pairs (BBPs)—pairs of points in source and target sets, where each point is the nearest neighbor of the other. BBS has several key features that make it robust against complex geometric deformations and high levels of outliers, such as those arising from background clutter and occlusions. We study these properties, provide a statistical analysis that justifies them, and demonstrate the consistent success of BBS on a challenging real-world dataset.\n\n---TOPICCS---\nTemplate Matching\nSimilarity Measures\nGeometric Deformations\nOutlier Robustness\nBest-Buddies Similarity (BBS)",
    "topics": [],
    "references": [
      {
        "citation": "[Comaniciu, D., Ramesh, V., & Meer, P. (2000). Real-time tracking of non-rigid objects using mean shift. In CVPR.] - Appears to be a foundational work on non-rigid object tracking."
      },
      {
        "citation": "[Dalal, N., & Triggs, B. (2005). Histograms of oriented gradients for human detection. In CVPR.] - A seminal work on HOG features, widely used in object detection."
      },
      {
        "citation": "[Rubner, Y., Tomasi, C., & Guibas, L. (2000). The earth mover’s distance as a metric for image retrieval. IJCV.] - Introduces the Earth Mover's Distance, a useful metric for image comparison."
      },
      {
        "citation": "[Pele, O., & Werman, M. (2008). Robust real-time pattern matching using bayesian sequential hypothesis testing. PAMI.] - Addresses robust pattern matching in real-time scenarios."
      },
      {
        "citation": "[Simaov, D., Caspi, Y., Shechtman, E., & Iran, M. (2008). Summarizing visual data using bidirectional similarity. In CVPR.] - Explores summarizing visual data using similarity measures."
      },
      {
        "citation": "[Hel-Or, Y., Hel-Or, H., & David, E. (2014). Matching by tone mapping: Photometric invariant template matching. IEEE Trans. Pattern Anal. Mach. Intell.] - Focuses on photometric invariant template matching."
      },
      {
        "citation": "[Tian, Y., & Narasimhan, S. G. (2012). Globally optimal estimation of nonrigid image distortion. IJCV.] - Deals with estimating nonrigid image distortions."
      },
      {
        "citation": "[Korman, S., Reichman, D., Tsur, G., & Avidan, S. (2013). Fast-match: Fast afﬁne template matching. In CVPR.] - Presents a fast affine template matching algorithm."
      },
      {
        "citation": "[Wu, Y., Lim, J., & Yang, M. (2013). Online object tracking: A benchmark. In CVPR.] - Provides a benchmark for online object tracking."
      },
      {
        "citation": "[Olson, C. F. (2002). Maximum-likelihood image matching. IEEE Trans. Pattern Anal. Mach. Intell.] - Discusses maximum-likelihood image matching."
      }
    ],
    "author_details": [
      {
        "name": "Tali Dekel",
        "affiliation": "MIT CSAIL",
        "email": "talidek@mit.edu"
      },
      {
        "name": "Shaul Oron",
        "affiliation": "Tel Aviv University",
        "email": "shauloro@eng.tau.ac.il"
      },
      {
        "name": "Michael Rubinstein",
        "affiliation": "Google Research",
        "email": "mrub@google.com"
      },
      {
        "name": "Shai Avidan",
        "affiliation": "Tel Aviv University",
        "email": "avidan@eng.tau.ac.il"
      },
      {
        "name": "William T. Freeman",
        "affiliation": "MIT CSAIL",
        "email": "billf@mit.edu"
      }
    ]
  },
  {
    "title": "A Weighted Sparse Coding Framework for Saliency Detection\n---AUTHOR---\nNianyi Li\nBilin Sun\nJingyi Yu",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Li_A_Weighted_Sparse_2015_CVPR_paper.pdf",
    "id": "Li_A_Weighted_Sparse_2015_CVPR_paper",
    "abstract": "There is an emerging interest on using high-dimensional datasets beyond 2D images in saliency detection. However, these techniques adopt very different solution frameworks, in both type of features and procedures on using them. In this paper, we present a uniﬁed saliency detection framework for handling heterogenous types of input data. Our approach builds dictionaries using data-speciﬁc features. Speciﬁcally, we first select a group of potential foreground superpixels to build a primitive saliency dictionary. We then prune the outliers in the dictionary and test on the remaining superpixels to iteratively reﬁne the dictionary. Comprehensive experiments show that our approach universally outperforms the state-of-the-art solution on all 2D, 3D and 4D data.\n\n---TOPIC---\nSaliency Detection\nSparse Coding\nHeterogenous Data\nDictionary Learning\nVisual Attention",
    "topics": [],
    "references": [
      {
        "citation": "[Liu, T., Yuan, Z., Sun, J., Wang, J., Zheng, N., Tang, X., & Shum, H. (2011). Learning to detect a salient object. *IEEE Transactions on Pattern Analysis and Machine Intelligence*, *33*(2), 353–367.]"
      },
      {
        "citation": "[Achanta, R., Shaji, A., Smith, K., Lucchi, A., Fua, P., & Susstrunk, S. (2012). Slic superpixels compared to state-of-the-art superpixel methods. *IEEE Transactions on Pattern Analysis and Machine Intelligence*, *34*(11), 2274–2282.]"
      },
      {
        "citation": "[Borji, A., & Itti, L. (2012). Exploiting local and global patch rarities for saliency detection. *CVPR*, pages 478–485. IEEE.]"
      },
      {
        "citation": "[Borji, A., Sihite, D. N., & Itti, L. (2012). Salient object detection: a benchmark. *ECCV*, pages 1-12. Springer.]"
      },
      {
        "citation": "[Movahedi, V., & Elder, J. H. (2010). Design and perceptual validation of performance measures for salient object segmentation. *CVPRW*, pages 49–56. IEEE.]"
      },
      {
        "citation": "[Nothdurft, H.-C. (2000). Salience from feature contrast: additivity across dimensions. *Vision Research*, *40*(10), 1183–1201.]"
      },
      {
        "citation": "[Cheng, M., Zhang, G., Mitra, N., Huang, X., & Hu, S. (2011). Global contrast based salient region detection. *CVPR*, pages 409–416. IEEE.]"
      },
      {
        "citation": "[Perazzi, F., Krähenbühl, P., Pritch, Y., & Hornung, A. (2012). Salience filters: Contrast based filtering for salient region detection. *CVPR*, pages 733–740. IEEE.]"
      },
      {
        "citation": "[Reynolds, J. H., & Desimone, R. (2003). Interacting roles of attention and visual salience in V4. *Neuron*, *37*(5), 853–863.]"
      },
      {
        "citation": "[Itti, L., & Koch, C. (2001). Computational modelling of visual attention. *Nature Reviews Neuroscience*, *2*(3), 194–203.]"
      }
    ],
    "author_details": [
      {
        "name": "Nianyi Li",
        "affiliation": "University of Delaware",
        "email": "nianyi@eecis.udel.edu"
      },
      {
        "name": "Bilin Sun",
        "affiliation": "University of Delaware",
        "email": "sunbilin@eecis.udel.edu"
      },
      {
        "name": "Jingyi Yu",
        "affiliation": "University of Delaware",
        "email": "yu@eecis.udel.edu"
      }
    ]
  },
  {
    "title": "Robust Regression on Image Manifolds for Ordered Label Denoising\n---AUTHOR---\nHui Wu\n---AUTHOR---\nRichard Souvenir",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Wu_Robust_Regression_on_2015_CVPR_paper.pdf",
    "id": "Wu_Robust_Regression_on_2015_CVPR_paper",
    "abstract": "In this paper, we present a computationally efﬁcient and non-parametric method for robust regression on manifolds. We apply our algorithm to the problem of correcting mis-labeled examples from image collections with ordered (e.g., real-valued, ordinal) labels. Compared to related methods for robust regression, our method achieves superior denois-ing accuracy on a variety of data sets, with label corrup-tion levels as high as 80%. For a diverse set of widely-used, large-scale, publicly-available data sets, our approach results in image labels that more accurately describe the associated images.\n\n---TOPICCS---\nRobust Regression\nOrdered Labels/Ordinal Data\nImage Manifolds\nLabel Denoising\nConvex Optimization",
    "topics": [],
    "references": [
      {
        "citation": "[S. Agarwal, N. Snavely, I. Simon, S. Seitz, and R. Szeliski. Building rome in a day. In IEEE International Conference on Computer Vision, pages 72–79, 2009.]"
      },
      {
        "citation": "[N. D. Donoho and C. Grimes. Hessian eigenmaps: Locally linear embedding techniques for high-dimensional data. Proceedings of the National Academy of Sciences, 100(10):5591–5596, 2003.]"
      },
      {
        "citation": "[R. Fergus, Y. Weiss, and A. Torralba. Semi-supervised learning in gigantic image collections. In Advances in Neural Information Processing Systems 22, pages 522–530. 2009.]"
      },
      {
        "citation": "[M. A. Fischler and R. C. Bolles. Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography. Communications of the ACM, 24(6):381–395, 1981.]"
      },
      {
        "citation": "[C.-C. Chang and C.-J. Lin. LIBSVM: a library for support vector machines. ACM Transactions on Intelligent Systems and Technology, 2(3):27, 2011.]"
      },
      {
        "citation": "[R. Raguram, O. Chum, M. Pollefeys, J. Matas, and J. Frah. Usac: A universal framework for random sample consensus. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 35(8):2022–2038, Aug 2013.]"
      },
      {
        "citation": "[P. C. Hansen. Analysis of discrete ill-posed problems by means of the l-curve. SIAM review, 34(4):561–580, 1992.]"
      },
      {
        "citation": "[N. Snavely, S. M. Seitz, and R. Szeliski. Modeling the world from internet photo collections. International Journal of Computer Vision, 80(2):189–210, 2008.]"
      },
      {
        "citation": "[J. Wright, A. Y. Yang, A. Ganesh, S. S. Sastry, and Y. Ma. Robust face recognition via sparse representation. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 31(2):210–227, 2009.]"
      },
      {
        "citation": "[V. C. Raykar and S. Yu. Ranking annotators for crowd-sourced labeling tasks. In Advances in neural information processing systems, pages 1809–1817, 2011.]"
      }
    ],
    "author_details": [
      {
        "name": "Hui Wu",
        "affiliation": "University of North Carolina at Charlotte",
        "email": "hwu13@uncc.edu"
      },
      {
        "name": "Richard Souvenir",
        "affiliation": "University of North Carolina at Charlotte",
        "email": "souvenir@uncc.edu"
      }
    ]
  },
  {
    "title": "A Fixed Viewpoint Approach for Dense Reconstruction of Transparent Objects\n---AUTHOR---\nKai Han\nKwan-Yee K. Wong\nMiaomiao Liu",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Han_A_Fixed_Viewpoint_2015_CVPR_paper.pdf",
    "id": "Han_A_Fixed_Viewpoint_2015_CVPR_paper",
    "abstract": "This paper addresses the challenging problem of reconstructing the surface shape of transparent objects. The viewpoint-dependent appearance of transparent objects renders traditional reconstruction methods ineffective. The authors propose a fixed viewpoint approach for dense surface reconstruction based on the refraction of light. This approach utilizes a simple setup to alter incident light paths and reconstructs the object surface by reconstructing and triangulating these light paths. The method avoids modeling complex light interactions, assumes no parametric shape, and handles unknown refractive indices, making it suitable for complex transparent objects. Experimental results on synthetic and real data demonstrate the feasibility and accuracy of the proposed approach.\n\n---TOPICAS---\nTransparent object reconstruction\nLight refraction\nFixed viewpoint approach\nSurface shape recovery\nLight path triangulation",
    "topics": [],
    "references": [
      {
        "citation": "[M. Ben-Ezra and S. K. Nayr, What does motion reveal about transparency?, ICCV, 2003] - Relevant for understanding the role of motion in transparency analysis."
      },
      {
        "citation": "[V. Chari and P. Sturm, A theory of refractive photo-light-path triangulation., CVPR, 2013] - Introduces a key triangulation method likely central to the paper's approach."
      },
      {
        "citation": "[G. Eren et al., Scanning from heating: 3D shape estimation of transparent objects from local surface heating., Optics Express, 2009] - Presents an alternative method for shape estimation of transparent objects."
      },
      {
        "citation": "[M. A. Fischler and R. C. Bolles, Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography., Communications of the ACM, 1981] - Introduces RANSAC, a fundamental algorithm for robust model fitting."
      },
      {
        "citation": "[S. Hata et al., Shape extraction of transparent object using genetic algorithm., ICPR, 1996] - Explores a genetic algorithm approach to shape extraction."
      },
      {
        "citation": "[I. Ihrke et al., Reconstructing the geometry of ﬂowing water., ICCV, 2005] - Demonstrates a related technique for reconstructing geometry in dynamic environments."
      },
      {
        "citation": "[I. Ihrke et al., State of the art in transparent and specular object reconstruction., Eurographics STAR, 2008] - Provides a broad overview of the field."
      },
      {
        "citation": "[I. Ihrke et al., Transparent and specular object reconstruction., Computer Graphics Forum, 2010] - A more detailed and comprehensive treatment of the topic."
      },
      {
        "citation": "[K. N. Kutulakos and E. Steger, A theory of refractive and specular 3D shape by light-path triangulation., ICCV, 2005] - Introduces a light-path triangulation method for shape reconstruction."
      },
      {
        "citation": "[Y. Ji et al., Reconstructing gas ﬂows using light-path approximation., CVPR, 2013] - Presents a light-path approximation method for reconstructing gas flows."
      }
    ],
    "author_details": [
      {
        "name": "Kai Han",
        "affiliation": "The University of Hong Kong",
        "email": "khan@cs.hku.hk"
      },
      {
        "name": "Kwan-Yee K. Wong",
        "affiliation": "The University of Hong Kong",
        "email": "kykwong@cs.hku.hk"
      },
      {
        "name": "Miaomiao Liu",
        "affiliation": "NICTA and CECS, ANU",
        "email": "miaomiao.liu@nicta.com.au"
      }
    ]
  },
  {
    "title": "An Efficient Volumetric Framework for Shape Tracking\n---AUTHOR---\nBenjamin Allain\nJean-Sébastien Franco\nEdmond Boyer",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Allain_An_Efficient_Volumetric_2015_CVPR_paper.pdf",
    "id": "Allain_An_Efficient_Volumetric_2015_CVPR_paper",
    "abstract": "Recovering 3D shape motion using visual information is an important problem with many applications. Most existing approaches rely on surface-based strategies, which can fail when the observations define several feasible surfaces. This work investigates a novel volumetric shape parametrization to track shapes over temporal sequences, utilizing Centroidal Voronoi Tesselations (CVT). The paper proposes a dedicated volumetric deformation model and evaluates it using a hybrid multi-camera and marker-based capture dataset, demonstrating similar or improved precision and robustness compared to state-of-the-art methods.",
    "topics": [
      "Volumetric Shape Tracking",
      "Centroidal Voronoi Tesselations (CVT)",
      "Dynamic Shape Capture",
      "Motion Estimation",
      "Surface-based vs. Volume-based Methods"
    ],
    "references": [
      {
        "citation": "[Alexa, M., Cohen-Or, D., & Levin, D. (2000). As-rigid-as-possible shape interpolation. *Proceedings of the 27th Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH ’00*, 157–164.]"
      },
      {
        "citation": "[Allain, B., Franco, J.-S., Boyer, E., & Tung, T. (2014). On mean pose and variability of 3d deformable models. *ECCV*.]"
      },
      {
        "citation": "[Ballan, L., & Cortelazzo, G. M. (2008). Marker-less motion capture of skinned models in a four camera set-up using optical ﬂow and silhouettes. *3DPVT*.]"
      },
      {
        "citation": "[Bishop, C. M. (2006). *Pattern Recognition and Machine Learning* (Information Science and Statistics). Springer-Verlag New York, Inc.]"
      },
      {
        "citation": "[Botsu, M., Pauly, M., Wicke, M., & Gross, M. (2007). Adaptive space deformations based on rigid cells. *Comput. Graph. Forum, 26(3), 339–347*.]"
      },
      {
        "citation": "[Cagniart, C., Boyer, E., & Ilic, S. (2010). Free-form mesh tracking: a patch-based approach. *CVPR*.]"
      },
      {
        "citation": "[Cagniart, C., Boyer, E., & Ilic, S. (2010). Probablistic deformable surface tracking from multiple videos. *ECCV*.]"
      },
      {
        "citation": "[de Aguiar, E., Stoll, C., Theobalt, C., Ahmed, N., Seidel, H.-P., & Thrun, S. (2008). Performance capture from sparse multi-view video. *ACM Transactions on Graphics, 27(3)*.]"
      },
      {
        "citation": "[de Aguiar, E., Theobalt, C., Stoll, C., & Seidel, H.-P. (2007). Marker-less deformable mesh tracking for human shape and motion capture. *CVPR*.]"
      },
      {
        "citation": "[Dempster, A. P., Laird, N. M., & Rubin, D. B. (1977). Maximum likelihood from incomplete data via the em algorithm. *Journal of the Royal Statistical Society, series B*.]"
      }
    ],
    "author_details": [
      {
        "name": "Benjamin Allain",
        "affiliation": "Inria Grenoble Rhˆone-Alpes - LJK",
        "email": "firstname.lastname@inria.fr"
      },
      {
        "name": "Jean-Sébastien Franco",
        "affiliation": "Inria Grenoble Rhˆone-Alpes - LJK",
        "email": "firstname.lastname@inria.fr"
      },
      {
        "name": "Edmond Boyer",
        "affiliation": "Inria Grenoble Rhˆone-Alpes - LJK",
        "email": "firstname.lastname@inria.fr"
      }
    ]
  },
  {
    "title": "Adaptive Region Pooling for Object Detection\n---AUTHOR---\nYi-Hsuan Tsai\nOnur C. Hamsici\nMing-Hsuan Yang",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Tsai_Adaptive_Region_Pooling_2015_CVPR_supplemental.pdf",
    "id": "Tsai_Adaptive_Region_Pooling_2015_CVPR_supplemental",
    "abstract": "This paper introduces an Adaptive Region Pooling (ARP) method for object detection. ARP discovers discriminative object parts and transfers keypoints on deformable objects by leveraging exemplar-based methods. The approach focuses on improving object detection by identifying representative parts and transferring these parts to detected objects, even under challenging conditions like occlusion or varying lighting. The paper demonstrates the effectiveness of ARP through visualizations, detection results, and comparisons with existing methods like ESVM.\n\n---TOPICKS---\nAdaptive Region Pooling\nObject Detection\nKeypoint Transfer\nExemplar-Based Methods\nObject Parts Discovery",
    "topics": [],
    "references": [
      {
        "citation": "[T. Malisiewicz, A. Gupta, and A. Efros, \"Ensemble of exemplar-svms for object detection and beyond,\" ICCV, 2011.] - This reference is explicitly mentioned and compared against in the paper, highlighting its relevance to the presented method."
      }
    ],
    "author_details": [
      {
        "name": "Yi-Hsuan Tsai",
        "affiliation": "UC Merced",
        "email": "ytsai2@ucmerced.edu"
      },
      {
        "name": "Onur C. Hamsici",
        "affiliation": "Qualcomm Research, San Diego",
        "email": "ohamsici@qti.qualcomm.com"
      },
      {
        "name": "Ming-Hsuan Yang",
        "affiliation": "UC Merced",
        "email": "mhyang@ucmerced.edu"
      }
    ]
  },
  {
    "title": "Long-term Recurrent Convolutional Networks for Visual Recognition and Description\n---AUTHORSS---\nJeff Donahue\nLisa Anne Hendricks\nSergio Guadarrama\nMarcus Rohrbach\nSubhashini Venugopalan\nKate Saenko\nTrevor Darrell",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Donahue_Long-Term_Recurrent_Convolutional_2015_CVPR_paper.pdf",
    "id": "Donahue_Long-Term_Recurrent_Convolutional_2015_CVPR_paper",
    "abstract": "Visual Features\nSequence Learning\nPredictions\nVisual Input\n\nModels based on deep convolutional networks have dominated recent image interpretation tasks; we investigate whether models which are also recurrent, or “temporally deep”, are effective for tasks involving sequences, visual and otherwise. We develop a novel recurrent convolutional architecture suitable for large-scale visual learning which is end-to-end trainable, and demonstrate the value of these models on benchmark video recognition tasks, image description and retrieval problems, and video narration challenges. In contrast to current models which assume a fixed spatio-temporal receptive field or simple temporal averaging for sequential processing, recurrent convolutional models are “doubly deep” in that they can be compositional in spatial and temporal “layers”. Such models may have advantages when target concepts are complex and/or training data are limited. Learning long-term dependencies is possible when nonlinearities are incorporated into the network state updates. Long-term RNN models are appealing in that they directly can map variable-length inputs (e.g., video frames) to variable length outputs (e.g., natural language text) and can model complex temporal dynamics; yet they can be optimized with backpropagation. Our recurrent long-term models are directly connected to modern visual convnet models and can be jointly trained to simultaneously learn temporal dynamics and convolutional perceptual representations. Our results show such models have distinct advantages over state-of-the-art models for recognition or generation which are separately defined and/or optimized.\n---TOPICCS---\nRecurrent Convolutional Networks (LRCNs)\nVideo Recognition\nImage Description\nSequence Learning\nLong-Term Dependencies",
    "topics": [],
    "references": [
      {
        "citation": "[M. Baccouche, F. Mamalet, C. Wolf, C. Garcia, and A. Baskurt. Action classiﬁcation in soccer videos with long short-term memory recurrent neural networks. In ICANN. 2010.]"
      },
      {
        "citation": "[R. Kiros, R. Salakhutdinov, and R. Zemel. Unifying visual-semantic embeddings with multimodal neural language models. arXiv preprint arXiv:1411.2539, 2014.]"
      },
      {
        "citation": "[R. Kiros, R. Salakhutdinov, and R. Zemel. Multimodal neural language models. In ICML, 2014.]"
      },
      {
        "citation": "[A. Barbu, A. Bridge, Z. Burchill, D. Coroian, S. Dickinson, A. Michaux, S. Mussman, S. Narayanaswamy, D. Salvi, L. Schmidt, J. Shangguan, J. M. Siskind, J. Waggoner, S. Wang, J. Wei, Y. Yin, and Z. Zhang. Video in sentences out. In UAI, 2012.]"
      },
      {
        "citation": "[T. Brox, A. Bruhn, N. Papenberger, and J. Weickert. High accuracy optical flow estimation based on a theory for warping. In ECCV. 2004.]"
      },
      {
        "citation": "[A. Krizhevsky, I. Sutskever, and G. E. Hinton. ImageNet classification with deep convolutional neural networks. In NIPS, 2012.]"
      },
      {
        "citation": "[S. Ji, W. Xu, M. Yang, and K. Yu. 3D convolutional neural networks for human action recognition. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 35(1):221–231, 2013.]"
      },
      {
        "citation": "[A. Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850, 2013.]"
      },
      {
        "citation": "[J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A large-scale hierarchical image database. In CVPR, 2009.]"
      },
      {
        "citation": "[A. Frome, G. Corrado, J. Shlens, S. Bengio, J. Dean, M. Ranzato, and T. Mikolov. DeViSE: A deep visual-semantic embedding model. In NIPS, 2013.]"
      }
    ],
    "author_details": [
      {
        "name": "Jeff Donahue",
        "affiliation": "UC Berkeley, ICSI",
        "email": "{jdonahue, lisa anne, sguada, rohrbach, trevor}@eecs.berkeley.edu"
      },
      {
        "name": "Lisa Anne Hendricks",
        "affiliation": "UC Berkeley, ICSI",
        "email": "{jdonahue, lisa anne, sguada, rohrbach, trevor}@eecs.berkeley.edu"
      },
      {
        "name": "Sergio Guadarrama",
        "affiliation": "UC Berkeley, ICSI",
        "email": "{jdonahue, lisa anne, sguada, rohrbach, trevor}@eecs.berkeley.edu"
      },
      {
        "name": "Marcus Rohrbach",
        "affiliation": "UC Berkeley, ICSI",
        "email": "{jdonahue, lisa anne, sguada, rohrbach, trevor}@eecs.berkeley.edu"
      },
      {
        "name": "Subhashini Venugopalan",
        "affiliation": "UT Austin",
        "email": "vsub@cs.utexas.edu"
      },
      {
        "name": "Kate Saenko",
        "affiliation": "UMass Lowell",
        "email": "saenko@cs.uml.edu"
      },
      {
        "name": "Trevor Darrell",
        "affiliation": "UC Berkeley, ICSI",
        "email": "{jdonahue, lisa anne, sguada, rohrbach, trevor}@eecs.berkeley.edu"
      }
    ]
  },
  {
    "title": "SOM: Semantic Obviousness Metric for Image Quality Assessment\n---AUTHORs---\nPeng Zhang\nWengang Zhou\nLei Wu\nHouqiang Li",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Zhang_SOM_Semantic_Obviousness_2015_CVPR_paper.pdf",
    "id": "Zhang_SOM_Semantic_Obviousness_2015_CVPR_paper",
    "abstract": "Image quality assessment (IQA) aims to objectively estimate human perception of image visual quality. Existing approaches target this problem with or without reference images. This paper proposes a new no-reference (NR) image quality assessment (IQA) framework based on semantic obviousness. The core idea is that semantic-level factors affect human perception of image quality. The framework extracts two types of features: one to measure semantic obviousness and the other to discover local characteristics, which are then combined for image quality estimation. Evaluations on the LIVE dataset demonstrate superior performance compared to existing NR-IQA algorithms and comparable results to state-of-the-art full-reference IQA (FR-IQA) methods. Cross-dataset experiments confirm the generalization ability of the approach.\n\n---TOPIC---\nImage Quality Assessment (IQA)\nNo-Reference Image Quality Assessment (NR-IQA)\nSemantic Obviousness\nFeature Extraction\nHuman Perception",
    "topics": [],
    "references": [],
    "author_details": [
      {
        "name": "Peng Zhang",
        "email": "pzhangoo@mail.ustc.edu.cn"
      },
      {
        "name": "Wengang Zhou",
        "email": "zhwg@ustc.edu.cn"
      },
      {
        "name": "Lei Wu",
        "email": "wuleibig@gmail.com"
      },
      {
        "name": "Houqiang Li",
        "email": "lihq@ustc.edu.cn"
      }
    ]
  },
  {
    "title": "Learning Similarity Metrics for Dynamic Scene Segmentation\n---AUTHOR---\nDamien Teney\nMatthew Brown\nDimitry Kit\nPeter Hall",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Teney_Learning_Similarity_Metrics_2015_CVPR_paper.pdf",
    "id": "Teney_Learning_Similarity_Metrics_2015_CVPR_paper",
    "abstract": "This paper addresses the segmentation of videos with arbitrary motion, including dynamic textures, using novel motion features and a supervised learning approach. Dynamic textures are commonplace in natural scenes and exhibit complex patterns of appearance and motion. Our solution uses custom spatiotemporal filters that capture texture and motion cues, along with a novel metric-learning framework that optimizes this representation for specific objects and scenes. This is used within a hierarchical, graph-based segmentation setting, yielding state-of-the-art results for dynamic texture segmentation. We also demonstrate the applicability of our approach to general object and motion segmentation, showing significant improvements over unsupervised segmentation and results comparable to the best task-specific approaches.",
    "topics": [
      "Dynamic textures",
      "Spatio-temporal filters",
      "Metric learning",
      "Video segmentation",
      "Graph-based segmentation"
    ],
    "references": [
      {
        "citation": "[Alpert, S., Galun, M., Basri, R., & Brandt, A. (2007). Image segmentation by probabilistic bottom-up aggregation and cue integration. In *CVPR*, pages 1–8.]"
      },
      {
        "citation": "[Brox, T., & Malik, J. (2010). Object segmentation by long term analysis of point trajectories. In *ECCV*, pages 282–295.]"
      },
      {
        "citation": "[Chan, A., & Vasconcelos, N. (2008). Modeling, clustering, and segmenting video with mixtures of dynamic textures. *IEEE Transactions on Pattern Analysis and Machine Intelligence*, *30*(5), 909–926.]"
      },
      {
        "citation": "[Chan, A., & Vasconcelos, N. (2009). Variational layered dynamic textures. In *CVPR*, pages 1062–1069.]"
      },
      {
        "citation": "[Chetverikov, D., & Peteri, R. (2005). A brief survey of dynamic texture description and recognition. In *Int. Conf. Computer Recognition Systems*, pages 17–26.]"
      },
      {
        "citation": "[Corso, J. J. (2014). CVPR tutorial on video segmentation.]"
      },
      {
        "citation": "[Derpannis, K. G., & Wilides, R. P. (2012). Spacetime texture representation and recognition based on a spatio-temporal orientation analysis. *IEEE Transactions on Pattern Analysis and Machine Intelligence*, *34*(6), 1193–1205.]"
      },
      {
        "citation": "[Doretto, G., Pundir, P., Soatto, S., & Wu, Y. N. (2001). Dynamic textures. *IJCV*, 439–446.]"
      },
      {
        "citation": "[Fazekas, S., Amiaz, T., Chetverikov, D., & Kiryati, N. (2009). Dynamic texture detection based on motion analysis. *IJCV*, 82(1), 48–63.]"
      },
      {
        "citation": "[Feichtenhofer, C., Pinz, A., & Wilides, R. (2014). Bags of spacetime energies for dynamic scene recognition. In *CVPR*.]"
      }
    ],
    "author_details": [
      {
        "name": "Damien Teney",
        "affiliation": "Carnegie Mellon University",
        "email": "dteney@andrew.cmu.edu"
      },
      {
        "name": "Matthew Brown",
        "affiliation": "University of Bath",
        "email": "m.brown@bath.ac.uk"
      },
      {
        "name": "Dimitry Kit",
        "affiliation": "University of Bath",
        "email": "d.m.kit@bath.ac.uk"
      },
      {
        "name": "Peter Hall",
        "affiliation": "University of Bath",
        "email": "maspmh@bath.ac.uk"
      }
    ]
  },
  {
    "title": "Reliable Patch Tracers: Robust Visual Tracking by Exploiting Reliable Patches",
    "authors": [
      "Yang Li",
      "Jianke Zhu",
      "Steven C.H. Hoi"
    ],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Li_Reliable_Patch_Trackers_2015_CVPR_paper.pdf",
    "id": "Li_Reliable_Patch_Trackers_2015_CVPR_paper",
    "abstract": "Most modern trackers typically employ a bounding box given in the first frame to track visual objects, where their tracking results are often sensitive to the initialization. In this paper, we propose a new tracking method, Reliable Patch Tracers (RPT), which attempts to identify and exploit the reliable patches that can be tracked effectively through the whole tracking process. Specifically, we present a tracking reliability metric to measure how reliably a patch can be tracked, where a probability model is proposed to estimate the distribution of reliable patches under a sequential Monte Carlo framework. As the reliable patches distributed over the image, we exploit the motion trajectories to distinguish them from the background. Therefore, the visual object can be defined as the clustering of homo-trajectory patches, where a Hough voting-like scheme is employed to estimate the target state. Encouraging experimental results on a large set of sequences showed that the proposed approach is very effective and in comparison to the state-of-the-art trackers. The full source code of our implementation will be publicly available.\n\n---TOPIC---\nVisual Object Tracking\nReliable Patch Tracking\nSequential Monte Carlo Framework\nTracking Reliability Metric\nHough Voting",
    "topics": [],
    "references": [
      {
        "citation": "[Adam, A., Rivlin, E., & Shimshoni, I. (2006). Robust fragments-based tracking using the integral histogram. In *CVPR*.]"
      },
      {
        "citation": "[Doucet, A., Freitas, N., & Gordon, N. (2001). *Sequential Monte Carlo Methods in Practice*. Springer-Verlag.]"
      },
      {
        "citation": "[Lucas, B. D., & Kanade, T. (1981). An iterative image registration technique with an application to stereo vision. In *Procceedings of Imageing Understanding Workshop*.]"
      },
      {
        "citation": "[Poling, B., Lerman, G., & Szlarm, A. (2014). Better feature tracking though subspace constraints. In *CVPR*.]"
      },
      {
        "citation": "[Cai, Z., Wen, L., Yang, J., Lei, Z., & Li, S. (2012). Structured visual tracking with dynamic graph. In *ACCV*.]"
      },
      {
        "citation": "[Mannning, C. D., Raghavan, P., & Schutze, H. (2008). *Introduction to Information Retrieval*. Cambridge University Press.]"
      },
      {
        "citation": "[Danelljan, M., Khan, F. S., Felsberg, M., & van de Weijer, J. (2014). Adaptive color attributes for real-time visual tracking. In *CVPR*.]"
      },
      {
        "citation": "[Bolme, D. S., Beveridge, J. R., Draper, B. A., & Lui, Y. M. (2010). Visual object tracking using adaptive correlation filters. In *CVPR*.]"
      },
      {
        "citation": "[Everingham, M., Gool, L., Williams, C. K. I., Winn, J., & Zisserman, A. (2010). The pascal visual object classes(voc) challenge. *IJCV*, *88*(2), 303–338.]"
      },
      {
        "citation": "[Grundmann, M., Kwatra, V., Han, M., & Essa, I. (2010). Efficient hierarchical graph based video segmentation. In *CVPR*.]"
      }
    ],
    "author_details": [
      {
        "name": "Yang Li",
        "affiliation": "College of Computer Science, Zhejiang University, China",
        "email": "liyang89@zju.edu.cn"
      },
      {
        "name": "Jianke Zhu",
        "affiliation": "College of Computer Science, Zhejiang University, China",
        "email": "jkzhu@zju.edu.cn"
      },
      {
        "name": "Steven C.H. Hoi",
        "affiliation": "School of Information System, Singapore Management University",
        "email": "chhoi@smu.edu.sg"
      }
    ]
  },
  {
    "title": "SALICON: Saliency in Context\n---AUTHOR---\nMing Jiang\nShengshen Huang\nJuanyong Duan\nQi Zhao",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Jiang_SALICON_Saliency_in_2015_CVPR_paper.pdf",
    "id": "Jiang_SALICON_Saliency_in_2015_CVPR_paper",
    "abstract": "This paper presents SALICON (Saliency in Context), an ongoing effort to understand and predict visual attention. The work focuses on collecting large-scale human data during natural explorations of images, recording how humans shift their attention. A new mouse-contingent multi-resolutional paradigm is introduced, allowing for large-scale data collection using a general-purpose mouse instead of an eye tracker. A proof-of-concept SALICON dataset of human \"free-viewing\" data on 10,000 images from the Microsoft COCO dataset is reported, demonstrating its utility as ground truth for evaluating salience algorithms. The dataset complements existing annotations and offers new possibilities for visual understanding.\n\n---TOPICCS---\nVisual Attention\nHuman-Computer Interaction\nSaliency Prediction\nCrowdsourcing\nImage Understanding",
    "topics": [],
    "references": [],
    "author_details": [
      {
        "name": "Ming Jiang",
        "affiliation": "Department of Electrical and Computer Engineering, National University of Singapore",
        "email": "mjiang@nus.edu.sg"
      },
      {
        "name": "Shengshen Huang",
        "affiliation": "Department of Electrical and Computer Engineering, National University of Singapore",
        "email": "shane.huang@nus.edu.sg"
      },
      {
        "name": "Juanyong Duan",
        "affiliation": "Department of Electrical and Computer Engineering, National University of Singapore",
        "email": "j.duan@u.nus.edu"
      },
      {
        "name": "Qi Zhao",
        "affiliation": "Department of Electrical and Computer Engineering, National University of Singapore",
        "email": "eleqiz@nus.edu.sg"
      }
    ]
  },
  {
    "title": "Deep LAC: Deep Localization, Alignment and Classiﬁcation for Fine-grained Recognition\n---AUTHOR---\nDi Lin\nXiaoyong Shen\nCewu Lu\nJiaya Jia",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Lin_Deep_LAC_Deep_2015_CVPR_paper.pdf",
    "id": "Lin_Deep_LAC_Deep_2015_CVPR_paper",
    "abstract": "We propose a fine-grained recognition system that incorporates part localization, alignment, and classification in one deep neural network. This is a nontrivial process, as the input to the classification module should be functions that enable back-propagation in constructing the solver. Our major contribution is to propose a valve linkage function (VLF) for back-propagation chaining and form our deep localization, alignment and classification (LAC) system. The VLF can adaptively compromise the errors of classification and alignment when training the LAC model. It in turn helps update localization. The performance on fine-grained object data bears out the effectiveness of our LAC system.\n\n---TOPICCS---\nFine-grained recognition\nDeep neural networks\nPart localization\nValve Linkage Function (VLF)\nBackpropagation",
    "topics": [],
    "references": [
      {
        "citation": "[Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). ImageNet classification with deep convolutional neural networks. In *NIPS*. ] - This paper introduced AlexNet, a groundbreaking deep convolutional neural network that significantly improved image classification performance on the ImageNet dataset, a key resource for fine-grained recognition."
      },
      {
        "citation": "[Uijlings, J. R., van de Sande, K. E., Gevers, T., & Smeulders, A. W. (2013). Selective search for object recognition. *IJCV*. ] - Selective Search is a widely used algorithm for generating region proposals, a crucial step in many object recognition pipelines, including those used for fine-grained recognition."
      },
      {
        "citation": "[Razavian, A. S., Azizpour, H., Sullivan, J., & Carlsson, S. (2014). Cnn features off-the-shelf: an astounding baseline for recognition. *arXiv*. ] - This paper demonstrated the effectiveness of using pre-trained CNN features as a strong baseline for various recognition tasks, influencing how features are utilized in fine-grained recognition."
      },
      {
        "citation": "[Ngiam, J., Chen, Z., Chia, D., Koh, P. W., Le, Q. V., & Ng, A. Y. (2010). Tiled convolutional neural networks. *In NIPS*. ] - This paper introduced tiled convolutional neural networks, an early approach to handling large images with limited computational resources, relevant to processing high-resolution images in fine-grained recognition."
      },
      {
        "citation": "[Berg, T., & Belhumeur, P. N. (2013). Poof: Part-based one-vs.-one features for fine-grained categorization, face verification, and attribute estimation. *In CVPR*. ] - This paper introduces a specific feature engineering approach (Poof) tailored for fine-grained categorization, highlighting the importance of part-based representations."
      },
      {
        "citation": "[Farrell, R., Oza, O., Zhang, N., Morariu, V. I., Darrell, T., & Davis, L. S. (2011). Birdlets: Subordinate categorization using volumetric primitives and pose-normalized appearance. *In ICCV*. ] - This paper presents a specific approach to fine-grained recognition (Birdlets) using volumetric primitives and pose normalization, demonstrating a focus on geometric and appearance cues."
      },
      {
        "citation": "[Zhang, N., Donahue, J., Girshick, R., & Darrell, T. (2014). Part-based R-CNNs for fine-grained category detection. *In ECCV*. ] - This paper extends the R-CNN framework to incorporate part-based information, a key technique for fine-grained recognition."
      },
      {
        "citation": "[Wah, C., Branson, S., Welinder, P., Perona, P., & Belongie, S. (2011). The caltech-ucsd birds-200-2011 dataset. ] - This dataset is a standard benchmark for fine-grained recognition, particularly in the bird species classification domain."
      },
      {
        "citation": "[Welinder, P., Branson, S., Mita, T., Wah, C., Schroff, F., Belongie, S., & Perona, P. (2010). Caltech-ucsd birds 200. Technical report, California Institute of Technology.] - Another reference to the Caltech-UCSD Birds dataset, emphasizing its importance."
      },
      {
        "citation": "[Sermanet, P., Eigen, D., Zhang, X., Mathieu, M., Fergus, R., & LeCun, Y. (2013). Overfeat: Integrated recognition, localization and detection using convolutional networks. *arXiv*. ] - This paper explores integrated recognition, localization, and detection using convolutional networks, a relevant approach for fine-grained recognition tasks."
      }
    ],
    "author_details": [
      {
        "name": "Di Lin",
        "affiliation": "The Chinese University of Hong Kong",
        "email": "*Not available in the provided text*"
      },
      {
        "name": "Xiaoyong Shen",
        "affiliation": "The Chinese University of Hong Kong",
        "email": "*Not available in the provided text*"
      },
      {
        "name": "Cewu Lu",
        "affiliation": "Hong Kong University of Science and Technology",
        "email": "*Not available in the provided text*"
      },
      {
        "name": "Jiaya Jia",
        "affiliation": "The Chinese University of Hong Kong",
        "email": "*Not available in the provided text*"
      }
    ]
  },
  {
    "title": "Learning to Detect Motion Boundaries\n---AUTHOR---\nPhilippe Weinzaepfel\nJerome Revaud\nZaid Harchaoui\nCordelia Schmid",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Weinzaepfel_Learning_to_Detect_2015_CVPR_paper.pdf",
    "id": "Weinzaepfel_Learning_to_Detect_2015_CVPR_paper",
    "abstract": "We propose a learning-based approach for motion boundary detection. Precise localization of motion boundaries is essential for the success of optical ﬂow estimation, as motion boundaries correspond to discontinuities of the optical ﬂow ﬁeld. The proposed approach allows to predict motion boundaries, using a structured random forest trained on the ground-truth of the MPI-Sintel dataset. The random forest leverages several cues at the patch level, namely appearance (RGB color) and motion cues (optical ﬂow estimated by state-of-the-art algorithms). Experimental results show that the proposed approach is both robust and computationally efﬁcient. It signiﬁcantly outperforms state-of-the-art motion-difference approaches on the MPI-Sintel and Middlebury datasets. We compare the results obtained with several state-of-the-art optical ﬂow approaches and study the impact of the different cues used in the ran-\ndom forest. Furthermore, we introduce a new dataset, the YouTube Motion Boundaries dataset (YMB), that comprises 60 sequences taken from real-world videos with manually annotated motion boundaries. On this dataset, our approach, although trained on MPI-Sintel, also outperforms by a large margin state-of-the-art optical ﬂow algorithms.\n---TOPICCS---\nMotion Boundary Detection\nOptical Flow Estimation\nRandom Forests\nLearning-based Approach\nMPI-Sintel Dataset",
    "topics": [],
    "references": [
      {
        "citation": "[P. Arbeláez, M. Maire, C. Fowlkes, and J. Malik. Contour detection and hierarchical image segmentation. IEEE Trans. PAMI, 2011.] - This paper likely provides foundational work on image segmentation, relevant to many tracking and motion analysis tasks."
      },
      {
        "citation": "[S. Baker and I. Matthews. Lucas-Kanade 20 years on: A unifying framework. IJCV, 2004.] - Lucas-Kanade is a classic optical flow algorithm, and this paper provides a comprehensive overview."
      },
      {
        "citation": "[T. Brox, A. Bruhn, N. Papenberg, and J. Weickert. High accuracy optical ﬂow estimation based on a theory for warping. In ECCV, 2004.] - A significant contribution to optical flow estimation, a core component of many motion analysis systems."
      },
      {
        "citation": "[P. Dollár and C. L. Zitnick. Structured forests for fast edge detection. In ICCV, 2013.] - Edge detection is crucial for boundary identification and segmentation."
      },
      {
        "citation": "[D. Hoiem, A. Efroos, and M. Hebert. Recovering occlusion boundaries from an image. IJCV, 2011.] - Addresses a key challenge in motion analysis: dealing with occlusions."
      },
      {
        "citation": "[I. Laptev and P. P´erez. Retrieving actions in movies. In ICCV, 2007.] - Relevant for understanding how motion information is used for higher-level action recognition."
      },
      {
        "citation": "[M. J. Black and D. J. Fleet. Probabilistic detection and tracking of motion boundaries. IJCV, 2000.] - A foundational paper on motion boundary tracking."
      },
      {
        "citation": "[T. Brox and J. Malik. Large displacement optical ﬂow: descriptor matching in variational motion estimation. IEEE Trans. PAMI, 2011.] - Deals with a challenging case of optical flow estimation."
      },
      {
        "citation": "[D. Sun, S. Roth, and M. Black. A quantitative analysis of current practices in optical ﬂow estimation and the principles behind them. IJCV, 2014.] - Provides a critical analysis of optical flow methods."
      },
      {
        "citation": "[P. Weinzaepfel, J. Revaud, Z. Harchaoui, and C. Schmid. Deepﬂow: Large displacement optical ﬂow with deep matching. In ICCV, 2013.] - Represents a more recent approach using deep learning for optical flow."
      }
    ],
    "author_details": [
      {
        "name": "Philippe Weinzaepfel",
        "affiliation": "Inria",
        "email": "firstname.lastname@inria.fr"
      },
      {
        "name": "Jerome Revaud",
        "affiliation": "Inria",
        "email": "firstname.lastname@inria.fr"
      },
      {
        "name": "Zaid Harchaoui",
        "affiliation": "Inria, NYU",
        "email": "firstname.lastname@inria.fr"
      },
      {
        "name": "Cordelia Schmid",
        "affiliation": "Inria",
        "email": "firstname.lastname@inria.fr"
      }
    ]
  },
  {
    "title": "Object Detection by Labeling Superpixels\n---AUTHOR---\nJunjie Yan\nYinan Yu\nXiangyu Zhu\nZhen Lei\nStan Z. Li",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Yan_Object_Detection_by_2015_CVPR_paper.pdf",
    "id": "Yan_Object_Detection_by_2015_CVPR_paper",
    "abstract": "Object detection is often conducted by object proposal generation and classification sequentially. This paper handles object detection in a superpixel oriented manner instead of the proposal oriented. Specifically, this paper takes object detection as a multi-label superpixel labeling problem by minimizing an energy function. It uses the data cost term to capture the appearance, smooth cost term to encode the spatial context and label cost term to favor compact detection. The data cost is learned through a convolutional neural network and the parameters in the labeling model are learned through a structural SVM. Compared with proposal generation and classification based methods, the proposed superpixel labeling method can naturally detect objects missed by the proposal generation step and capture the global image context to infer the overlapping objects. The proposed method shows its advantage in Pascal VOC and ImageNet. Notably, it performs better than the ImageNet ILSVRC2014 winner GoogLeNet (45.0% V.S. 43.9% in mAP) with much shallower and fewer CNNs.",
    "topics": [
      "Object Detection",
      "Superpixels",
      "Energy Minimization",
      "Convolutional Neural Networks (CNNs)",
      "Multi-label Labeling"
    ],
    "references": [
      {
        "citation": "[Alexe, B., Deselaers, T., & Ferrari, V. (2012). Measuring the object-ness of image windows. *Pattern Analysis and Machine Intelligence*, *24*(1), 2.]"
      },
      {
        "citation": "[Arbeláez, P., Pont-Tuset, J., Barron, J. T., Marques, F., & Malik, J. (2014). Multiscale combinatorial grouping. *Computer Vision and Pattern Recognition*, *2014*.]"
      },
      {
        "citation": "[Barinova, O., Lempitsky, V., & Kholi, P. (2012). On detection of multiple object instances using hough transforms. *Pattern Analysis and Machine Intelligence*, *24*(3), 13.]"
      },
      {
        "citation": "[Chen, D., Ren, S., Wei, Y., Cao, X., & Sun, J. (2014). Joint cascade face detection and alignment. *Computer Vision and Pattern Recognition*, *2014*.]"
      },
      {
        "citation": "[Cortes, C., & Vapnik, V. (1995). Support-vector networks. *Machine learning*, *20*(3), 2.]"
      },
      {
        "citation": "[Desai, C., Ramanan, D., & Fowlkes, C. (2011). Discriminative models for multi-class object layout. *International Journal of Computer Vision*, *95*(3), 13.]"
      },
      {
        "citation": "[Erhan, D., Szegedy, C., Toshev, A., & Anguelov, D. (2014). Scalable object detection using deep neural networks. *Computer Vision and Pattern Recognition*, *2014*.]"
      },
      {
        "citation": "[Everingham, M., Van Gool, L., Williams, C. K. I., Winn, J., & Zisserman, A. (2012). The PASCAL Visual Object Classes Challenge 2012 (VOC2012) Results. *http://www.pascal-network.org/challenges/VOC/voc2012/workshop/index.html*.]"
      },
      {
        "citation": "[Farabet, C., Couprié, C., Najman, L., & LeCun, Y. (2013). Learning hierarchical features for scene labeling. *Pattern Analysis and Machine Intelligence*, *25*(12), 13.]"
      },
      {
        "citation": "[Felzenszwalb, P. F., & Huttenlocher, D. P. (2004). Efficient graph-based image segmentation. *International Journal of Computer Vision*, *61*(1), 2.]"
      }
    ],
    "author_details": [
      {
        "name": "Junjie Yan",
        "affiliation": "National Laboratory of Pattern Recognition, Chinese Academy of Sciences",
        "email": "Not available"
      },
      {
        "name": "Junjie Yan",
        "affiliation": "Institute of Data Science and Technology, Alibaba Group",
        "email": "Not available"
      },
      {
        "name": "Yinan Yu",
        "affiliation": "Institute of Deep Learning, Baidu Research",
        "email": "Not available"
      },
      {
        "name": "Xiangyu Zhu",
        "affiliation": "National Laboratory of Pattern Recognition, Chinese Academy of Sciences",
        "email": "Not available"
      },
      {
        "name": "Zhen Lei",
        "affiliation": "National Laboratory of Pattern Recognition, Chinese Academy of Sciences",
        "email": "Not available"
      },
      {
        "name": "Stan Z. Li",
        "affiliation": "National Laboratory of Pattern Recognition, Chinese Academy of Sciences",
        "email": "Not available"
      }
    ]
  },
  {
    "title": "Deeply learned face representations are sparse, selective, and robust\n---AUTHOR---\nYi Sun\nXiaogang Wang\nXiaoou Tang",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Sun_Deeply_Learned_Face_2015_CVPR_paper.pdf",
    "id": "Sun_Deeply_Learned_Face_2015_CVPR_paper",
    "abstract": "This paper designs a high-performance deep convolutional network (DeepID2+) for face recognition. It is learned with the identiﬁcation-veriﬁcation supervisory signal. By increasing the dimension of hidden representations and adding supervision to early convolutional layers, DeepID2+ achieves new state-of-the-art on LFW and YouTube Faces benchmarks. Through empirical studies, we have discovered three properties of its deep neural activations critical for the high performance: sparsity, selectiveness and robustness. (1) It is observed that neural activations are moderately sparse. Moderate sparsity maximizes the discriminative power of the deep net as well as the distance between images. It is surprising that DeepID2+ still can achieve high recognition accuracy even after the neural responses are binarized. (2) Its neurons in higher layers are highly selective to identities and identity-related attributes. We can identify different subsets of neurons which are either constantly excited or inhibited when different identities or attributes are present. Although DeepID2+ is not taught to distinguish attributes during training, it has implicitly learned such high-level concepts. (3) It is much more robust to occlusions, although occlusion patterns are not included in the training set.\n\n---TOPICCS---\nFace recognition\nDeep convolutional neural networks\nNeural network activations\nSparsity\nSelectiveness/Attribute recognition\nRobustness",
    "topics": [],
    "references": [
      {
        "citation": "[P. Agrawal, R. Girshick, and J. Malik. Analyzing the performance of multilayer neural networks for object recognition. In Proc. ECCV, 2014.]"
      },
      {
        "citation": "[P. Luo, X. Wang, and X. Tang. A deep sum-product architecture for robust facial attributes analysis. In Proc. ICCV, 2013.]"
      },
      {
        "citation": "[Y. Taigman, M. Yang, M. Ranzato, and L. Wolf. DeepFace: Closing the gap to human-level performance in face veriﬁca-"
      },
      {
        "citation": "tion. In Proc. CVPR, 2014.]"
      },
      {
        "citation": "[Y. Sun, X. Wang, and X. Tang. Deep learning face representation by joint identiﬁcation-veriﬁcation. In Proc. NIPS, 2014.]"
      },
      {
        "citation": "[G. B. Huang, M. Ramesh, T. Berg, and E. Learned-Miller. Labeled Faces in the Wild: A database for studying face recognition in unconstrained environments. Technical Report 07-49, University of Massachusetts, Amherst, 2007.]"
      },
      {
        "citation": "[J. Hu, J. Lu, and Y.-P. Tan. Discrimina-tive deep metric learning for face veriﬁcation in the wild. In Proc. CVPR, 2014.]"
      },
      {
        "citation": "[Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel. Backpropagation applied to handwritten zip code recognition. Neural Comput., 1:541–551, 1989.]"
      },
      {
        "citation": "[X. Wang and X. Tang. A uniﬁed framework for subspace face recognition. PAMI, 26:1222–1228, 2004.]"
      },
      {
        "citation": "[J. Wright, A. Y. Yang, A. Ganesh, S. S. Sastry, and Y. Ma. Robust face recognition via sparse representation. PAMI, 31:210–227, 2009.]"
      },
      {
        "citation": "[L. Wolf, T. Hassner, and I. Maoz. Face recognition in unconstrained videos with matched background similarity. In Proc. CVPR, 2011.]"
      }
    ],
    "author_details": [
      {
        "name": "Yi Sun",
        "affiliation": "Department of Information Engineering, The Chinese University of Hong Kong",
        "email": "sy011@ie.cuhk.edu.hk"
      },
      {
        "name": "Xiaogang Wang",
        "affiliation": "Department of Electronic Engineering, The Chinese University of Hong Kong; Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences",
        "email": "xgwang@ee.cuhk.edu.hk"
      },
      {
        "name": "Xiaoou Tang",
        "affiliation": "Department of Information Engineering, The Chinese University of Hong Kong; Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences",
        "email": "xtang@ie.cuhk.edu.hk"
      }
    ]
  },
  {
    "title": "Face Alignment by Coarse-to-Fine Shape Searching\n---AUTHOR---\nShizhan Zhu\nCheng Li\nChen Change Loy\nXiaoou Tang",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Zhu_Face_Alignment_by_2015_CVPR_paper.pdf",
    "id": "Zhu_Face_Alignment_by_2015_CVPR_paper",
    "abstract": "We present a novel face alignment framework based on coarse-to-ﬁne shape searching. Unlike the conventional cascaded regression approaches that start with an initial shape and reﬁne the shape in a cascaded manner, our approach begins with a coarse search over a shape space that contains diverse shapes, and employs the coarse solution to constrain subsequent ﬁner search of shapes. The unique stage-by-stage progressive and adaptive search i) prevents the ﬁnal solution from being trapped in local optima due to poor initialisation, a common problem encountered by cascaded regression approaches; and ii) improves the robustness in coping with large pose variations. The framework demonstrates real-time performance and state-of-the-art results on various benchmarks including the challenging 300-W dataset.",
    "topics": [
      "Face Alignment",
      "Shape Searching",
      "Coarse-to-Fine Methods",
      "Pose Variation",
      "Cascaded Regression"
    ],
    "references": [
      {
        "citation": "[Y. Amit, D. Geman, and X. Fan. A coarse-to-ﬁne strategy for multiclass shape detection. TPAMI, 26(12):1606–1621, 2004.]"
      },
      {
        "citation": "[D. G. Lowe. Distinctive image features from scale-invariant keypoints. IJCV, 60(2):91–110, 2004.]"
      },
      {
        "citation": "[T. F. Cootes, G. J. Edwards, and C. J. Taylor. Active appearance models. TPAMI, 23(6):681–685, 2001.]"
      },
      {
        "citation": "[H. Bay, T. Tuytelaars, and L. Van Gool. SURF: Speeded up robust features. In ECCV, pages 404–417, 2006.]"
      },
      {
        "citation": "[K. Messer, J. Matas, J. Kittler, J. Luettin, and G. Maitre. Xm2vtsdb: The extended m2vts database. In AVBPA, volume 964, pages 965–966. Citeseer, 1999.]"
      },
      {
        "citation": "[P. N. Belhumeur, D. W. Jacobs, D. Kriegman, and N. Kumar. Localizing parts of faces using a consensus of exemplars. In CVPR, pages 545–552, 2011.]"
      },
      {
        "citation": "[L. Breiman. Random forests. Machine learning, 45(1):5–32, 2001.]"
      },
      {
        "citation": "[X. Cao, Y. Wei, F. Wen, and J. Sun. Face alignment by explicit shape regression. IJCV, 107(2):177–190, 2014.]"
      },
      {
        "citation": "[N. Dalal and B. Triggs. Histograms of oriented gradients for human detection. In CVPR, volume 1, pages 886–893, 2005.]"
      },
      {
        "citation": "[V. Kazemi and S. Josephine. One milliseccond face alignment with an ensemble of regression trees. In CVPR, 2014.]"
      }
    ],
    "author_details": [
      {
        "name": "Shizhan Zhu",
        "affiliation": "Department of Information Engineering, The Chinese University of Hong Kong",
        "email": "zs014@ie.cuhk.edu.hk"
      },
      {
        "name": "Cheng Li",
        "affiliation": "SenseTime Group",
        "email": "chengli@sensetime.com"
      },
      {
        "name": "Chen Change Loy",
        "affiliation": "Department of Information Engineering, The Chinese University of Hong Kong",
        "email": "ccloy@ie.cuhk.edu.hk"
      },
      {
        "name": "Xiaoou Tang",
        "affiliation": "Department of Information Engineering, The Chinese University of Hong Kong",
        "email": "xtang@ie.cuhk.edu.hk"
      }
    ]
  },
  {
    "title": "Improving Object Proposals with Multi-Thresholding Straddling Expansion\n---AUTHOR---\nXiaozhi Chen\nHuimin Ma\nXiang Wang\nZhichen Zhao",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Chen_Improving_Object_Proposals_2015_CVPR_paper.pdf",
    "id": "Chen_Improving_Object_Proposals_2015_CVPR_paper",
    "abstract": "Recent advances in object detection have exploited object proposals to speed up object searching. However, many existing object proposal generators suffer from strong localization bias or require computationally expensive diversification strategies. This paper presents an effective approach to address these issues. The authors propose a simple localization bias measure, \"superpixel tightness,\" and a box refinement method, namely Multi-Thresholding Straddling Expansion (MTSE), to reduce localization bias via fast diversification. MTSE aligns bounding boxes with superpixel boundaries and performs multi-thresholding expansion. Experiments on the PASCAL VOC dataset demonstrate significant improvements to existing models with little computational overhead.\n\n---TOPICKS---\nObject Proposals\nLocalization Bias\nSuperpixel Expansion\nBounding Box Refinement\nMulti-Thresholding",
    "topics": [],
    "references": [],
    "author_details": [
      {
        "name": "Xiaozhi Chen",
        "affiliation": "Department of Electronic Engineering, Tsinghua University",
        "email": "chenxz12@mails.tsinghua.edu.cn"
      },
      {
        "name": "Huimin Ma",
        "affiliation": "Department of Electronic Engineering, Tsinghua University",
        "email": "mhmpub@tsinghua.edu.cn"
      },
      {
        "name": "Xiang Wang",
        "affiliation": "Department of Electronic Engineering, Tsinghua University",
        "email": "wangxiang14@mails.tsinghua.edu.cn"
      },
      {
        "name": "Zhichen Zhao",
        "affiliation": "Department of Electronic Engineering, Tsinghua University",
        "email": "zhaozc14@mails.tsinghua.edu.cn"
      }
    ]
  },
  {
    "title": "Weakly Supervised Semantic Segmentation for Social Images\n---AUTHOR---\nWei Zhang\nSheng Zeng\nDequan Wang\nXiangyang Xue",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Zhang_Weakly_Supervised_Semantic_2015_CVPR_paper.pdf",
    "id": "Zhang_Weakly_Supervised_Semantic_2015_CVPR_paper",
    "abstract": "Image semantic segmentation is the task of partitioning an image into several regions based on semantic concepts. In this paper, we learn a weakly supervised semantic segmentation model from social images whose labels are not pixel-level but image-level; furthermore, these labels might be noisy. We present a joint conditional random field model leveraging various contexts to address this issue. More specifically, we extract global and local features in multiple scales by convolutional neural network and topic model. Inter-label correlations are captured by visual contextual cues and label co-occurrence statistics. The label consistency between image-level and pixel-level is finally achieved by iterative reﬁnement. Experimental results on two real-world image datasets PASSCAL VOC2007 and SIFT-Flow demonstrate that the proposed approach outperforms state-of-the-art weakly supervised methods and even achieves accuracy comparable with fully supervised methods.\n\n---TOPIC---\nWeakly Supervised Semantic Segmentation\nConditional Random Fields (CRF)\nConvolutional Neural Networks (CNN)\nTopic Modeling (LSC)\nNoisy Image Labels",
    "topics": [],
    "references": [
      {
        "citation": "[Arbel´aez, P., Pont-Tuset, J., Barron, J., Marques, F., & Malik, J. (2014). Multiscale combinatorial grouping. In CVPR.]"
      },
      {
        "citation": "[Boykov, Y., Vekslér, O., & Zabih, R. (2001). Fast approximate energy minimization via graph cuts. PAMI.]"
      },
      {
        "citation": "[Dinits, E. (1970). Algorithm of solution to problem of maximum flow in network with power estimates. Doklady Akademii Nauk SSSR.]"
      },
      {
        "citation": "[Everingham, M., Van Gool, L., Williams, C. K. I., Winn, J., & Zisserma, A. (n.d.). The PASCAL Visual Object Classes Challenge 2007 (VOC2007) Results. http://www.pascal-network.org/challenges/VOC/voc2007/workshop/index.html.]"
      },
      {
        "citation": "[Fan, R.-E., Chang, K.-W., Hsieh, C.-J., Wang, X.-R., & Lin, C.-J. (2008). Liblinear: A library for large linear classification. The Journal of Machine Learning Research.]"
      },
      {
        "citation": "[Felzenszwalb, P., McAllester, D., & Ramanan, D. (2008). A discriminatively trained, multiscale, deformable part model. In CVPR.]"
      },
      {
        "citation": "[Ferrari, V., Fevrier, L., Schmid, C., Jurie, F., et al. (2008). Groups of adjacent contour segments for object detection. PAMI.]"
      },
      {
        "citation": "[Hofmann, T. (1999). Probabilistic latent semantic indexing. In SIGIR.]"
      },
      {
        "citation": "[Jia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick, R., Guadarrama, S., & Darrell, T. (2014). Caffe: Convolutional architecture for fast feature embedding. In Proceedings of the ACM International Conference on Multimedia.]"
      },
      {
        "citation": "[Kolmogorov, V., & Zabin, R. (2004). What energy functions can be minimized via graph cuts? PAMI.]"
      },
      {
        "citation": "---"
      }
    ],
    "author_details": [
      {
        "name": "Wei Zhang",
        "affiliation": "Shanghai Key Laboratory of Intelligent Information Processing, School of Computer Science, Fudan University, Shanghai, China",
        "email": "weizh@fudan.edu.cn"
      },
      {
        "name": "Sheng Zeng",
        "affiliation": "Shanghai Key Laboratory of Intelligent Information Processing, School of Computer Science, Fudan University, Shanghai, China",
        "email": "zengsheng@fudan.edu.cn"
      },
      {
        "name": "Dequan Wang",
        "affiliation": "Shanghai Key Laboratory of Intelligent Information Processing, School of Computer Science, Fudan University, Shanghai, China",
        "email": "dqwang12@fudan.edu.cn"
      },
      {
        "name": "Xiangyang Xue",
        "affiliation": "Shanghai Key Laboratory of Intelligent Information Processing, School of Computer Science, Fudan University, Shanghai, China",
        "email": "xyxue@fudan.edu.cn"
      }
    ]
  },
  {
    "title": "Multiclass Semantic Video Segmentation with Object-level Active Inference\n---AUTHOR---\nBuyu Liu\nXuming He",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Liu_Multiclass_Semantic_Video_2015_CVPR_paper.pdf",
    "id": "Liu_Multiclass_Semantic_Video_2015_CVPR_paper",
    "abstract": "We address the problem of integrating object reasoning with supervoxel labeling in multiclass semantic video segmentation. To this end, we first propose an object-augmented dense CRF in spatiotemporal domain, which captures long-range dependency between supervoxels, and imposes consistency between object and supervoxel labels. We develop an efficient mean field inference algorithm to jointly infer the supervoxel labels, object activations and their occlusion relations for a moderate number of object hypotheses. To scale up our method, we adopt an active inference strategy to improve the efﬁciency, which adaptively selects object subgraphs in the object-augmented dense CRF. We formulate the problem as a Markov Decision Process, which learns an approximate optimal policy based on a reward of accuracy improvement and a set of well-designed model and input features. We evaluate our method on three publicly available multiclass video semantic segmentation datasets and demonstrate superior efﬁciency and accuracy.\n\n---TOPIC---\nMulticlass Semantic Video Segmentation\nObject-Augmented Dense CRF\nActive Inference\nMarkov Decision Process\nSupervoxel Labeling",
    "topics": [],
    "references": [
      {
        "citation": "[P. Abbeel and A. Y. Ng, Apprenticeship learning via inverse reinforcement learning, Proceedings of the twenty-ﬁrst international conference on Machine learning, 2004]"
      },
      {
        "citation": "[G. J. Brostow, J. Shotton, J. Fauqueur, and R. Cipolla, Segmentation and recognition using structure from motion point clouds, ECCV, 2008]"
      },
      {
        "citation": "[J. Chang, D. Wei, and J. W. F. III, A Video Representation Using Temporal Superpixels, CVPR, 2013]"
      },
      {
        "citation": "[P. Kr¨ahenb¨uhl and V. Koltun, Efﬁcient inference in fully connected crfs with gaussian edge potentials, NIPS, 2011]"
      },
      {
        "citation": "[C. Sutton and A. McCallum, Piecewise training for undirected models, CoRR, 2012]"
      },
      {
        "citation": "[J. Tighe and S. Lazebnik, Superparsing - scalable nonpara-metric image parsing with superpixels, IJCV, 2013]"
      },
      {
        "citation": "[A. Y. C. Chen and J. J. Corso, Temporally consistent multi-class video-object segmentation with the video graph-shifts algorithm, WMVC, 2011]"
      },
      {
        "citation": "[B. Kim, M. Sun, P. Kohli, and S. Savarese, Relating things and stuff by high-order potential modeling, ECCV’12 Workshop on Higher-Order Models and Global Constraints in Computer Vision, 2012]"
      },
      {
        "citation": "[P. Isola and C. Liu, Scene collaging: analysis and synthesis of natural images with semantic layers, ICCV, 2013]"
      },
      {
        "citation": "[M. Sun, B.-s. Kim, P. Kohli, and S. Savarese, Relating things and stuff via object property interactions, Transactions on Pattern Analysis and Machine Intelligence, 2014]"
      }
    ],
    "author_details": [
      {
        "name": "Buyu Liu",
        "affiliation": "ANU/NICTA",
        "email": "buyu.liu@anu.edu.au"
      },
      {
        "name": "Xuming He",
        "affiliation": "NICTA/ANU",
        "email": "xuming.he@nicta.com.au"
      }
    ]
  },
  {
    "title": "EgoSampling: Fast-Forward and Stereo for Egocentric Videos\n---AUTHOR---\nYair Poleg\n---AUTHOR---\nTavi Halperin\n---AUTHOR---\nChetan Arora\n---AUTHOR---\nShmuel Peleg",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Poleg_EgoSampling_Fast-Forward_and_2015_CVPR_paper.pdf",
    "id": "Poleg_EgoSampling_Fast-Forward_and_2015_CVPR_paper",
    "abstract": "While egocentric cameras like GoPro are gaining popularity, the videos they capture are long, boring, and difficult to watch from start to end. Fast forwarding (i.e. frame sampling) is a natural choice for faster video browsing. However, this accentuates the shake caused by natural head motion, making the fast forwarded video useless. We propose EgoSampling, an adaptive frame sampling that gives more stable fast forwarded videos. Adaptive frame sampling is formulated as energy minimization, whose optimal solution can be found in polynomial time. In addition, egocentric video taken while walking suffers from the left-right movement of the head as the body weight shifts from one leg to another. We turn this drawback into a feature: Stereo video can be created by sampling the frames from the left most and right most head positions of each step, forming approximate stereo-pairs.",
    "topics": [
      "Egocentric Videos",
      "Frame Sampling/Fast Forwarding",
      "Adaptive Video Processing",
      "Head Motion Stabilization",
      "Stereo Video Generation"
    ],
    "references": [
      {
        "citation": "[J. Kopf, M. Cohen, and R. Szeliski. First-person hyperlapse videos. ACM Transactions on Graphics, 33(4), August 2014.] - This is a core reference, likely introducing the concept of hyperlapse videos and is cited multiple times."
      },
      {
        "citation": "[J. Kopf, M. Cohen, and R. Szeliski. First-person Hyperlapse Videos - Supplemental Material. http://research.microsoft.com/en-us/um/redmond/projects/hyperlapse/supplementary/index.html.] -  A supplementary material reference linked to the core hyperlapse paper, suggesting it contains important details or data."
      },
      {
        "citation": "[Y. Poleg, C. Arora, and S. Peleg. Temporal segmentation of egocentric videos. In CVPR, pages 2537–2544, 2014.] - Cited multiple times, indicating its relevance to the paper's methodology or discussion."
      },
      {
        "citation": "[S. Liu, L. Yuan, P. Tan, and J. Sun. Steadyﬂow: Spatially smooth optical ﬂow for video stabilization. 2014.] - Cited multiple times, suggesting its importance for video stabilization techniques."
      },
      {
        "citation": "[B. D. Lucas and T. Kanade. An iterative image registration technique with an application to stereo vision. In IJCAI, volume 2, 1981.] - A foundational reference for image registration, a key component in stereo vision."
      },
      {
        "citation": "[F. Liu, M. Gleicher, H. Jin, and A. Agarwala. Content-preserving warps for 3d video stabilization. In SIGGRAPH, pages 44:1–44:9, 2009.] - Relevant for video stabilization techniques."
      },
      {
        "citation": "[E. W. Dijkstra. A note on two problems in connexion with graphs. NUMERISCHE MATHEMATIK, 1(1), 1959.] - While seemingly out of place, its inclusion suggests a graph theory element to the work."
      },
      {
        "citation": "[S. Liu, L. Yuan, P. Tan, and J. Sun. Bundled camera paths for video stabilization. SIGGRAPH, 2013.] -  Another reference related to video stabilization."
      },
      {
        "citation": "[Z. Lu and K. Grauman. Story-driven summarization for ego-centric video. In CVPR, 2013.] - Relevant for understanding the context of ego-centric video analysis."
      },
      {
        "citation": "[M. Grundmann, V. Kwatra, and I. Essa. Auto-directed video stabilization with robust l1 optimal camera paths. In CVPR, 2011.] - Another reference related to video stabilization."
      }
    ],
    "author_details": [
      {
        "name": "Yair Poleg",
        "affiliation": "The Hebrew University",
        "email": "[Email not available]"
      },
      {
        "name": "Tavi Halperin",
        "affiliation": "The Hebrew University",
        "email": "[Email not available]"
      },
      {
        "name": "Chetan Arora",
        "affiliation": "IIIT",
        "email": "[Email not available]"
      },
      {
        "name": "Shmuel Peleg",
        "affiliation": "The Hebrew University",
        "email": "[Email not available]"
      }
    ]
  },
  {
    "title": "Book2Movie: Aligning Video scenes with Book chapters\n---AUTHORs---\nMakarand Tapaswi\nMartin Bauml\nRainer Stiefelhagen",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Tapaswi_Book2Movie_Aligning_Video_2015_CVPR_paper.pdf",
    "id": "Tapaswi_Book2Movie_Aligning_Video_2015_CVPR_paper",
    "abstract": "Film adaptations of novels often visually display in a few shots what is described in many pages of the source novel. In this paper we present a new problem: to align book chapters with video scenes. Such an alignment facilitates finding differences between the adaptation and the original source, and also acts as a basis for deriving rich descriptions from the novel for the video clips. We propose an efficient method to compute an alignment between book chapters and video scenes using matching dialogs and character identities as cues. A major consideration is to allow the alignment to be non-sequential. Our suggested shortest path based approach deals with the non-sequential alignments and can be used to determine whether a video scene was part of the original book. We create a new data set involving two popular novel-to-film adaptations with widely varying properties and compare our method against other text-to-video alignment baselines. Using the alignment, we present a qualitative analysis of describing the video through rich narratives obtained from the novel.\n\n---TOPICCS---\nNovel-to-film adaptation\nVideo scene alignment\nShortest path algorithms\nText-to-video retrieval\nCharacter identity matching",
    "topics": [],
    "references": [
      {
        "citation": "[McFarlane, B. Novel to Film: an Introduction to the Theory of Adaptation. Clarendon press, Oxford, 1996.] - Provides a theoretical framework for adaptation studies, relevant for understanding the relationship between novels and films."
      },
      {
        "citation": "[Book2Movie data download. http://cvhci.anthropomatik.kit.edu/projects/mma.] - This is a dataset resource, crucial for researchers working with novel-to-film alignments and face tracking."
      },
      {
        "citation": "[Parkhi, O. M., Simonyan, K., Vedaldi, A., & Zisserman, A. A Compact and Discrimiative Face Track Descriptor. In CVPR, 2014.] -  Focuses on face tracking and descriptors, likely important for video analysis tasks."
      },
      {
        "citation": "[Stanford CoreNLP. http://nlp.stanford.edu/software/. Retrieved 2014-11-14.] - A standard NLP tool, useful for text processing and analysis, potentially used in understanding scripts or novel text."
      },
      {
        "citation": "[Rohrbach, A., Rohrbach, M., Tandon, N., & Schiele, B. A Dataset for Movie Description. In CVPR, 2015.] - Provides a dataset for movie description, valuable for research in generating textual descriptions of videos."
      },
      {
        "citation": "[Sankar, P., Jawahar, C. V., & Zisserman, A. Subtitle-free Movie to Script Alignment. In BMVC, 2009.] - Addresses the alignment of movie content with scripts, a key task in many novel-to-film analyses."
      },
      {
        "citation": "[Everingham, M., Sivic, J., & Zisserman, A. “Hello! My name is... Buffy” - Automatic Naming of Characters in TV Video. In BMVC, 2006.] - Relevant for character identification and tracking in video, a common task in adaptation studies."
      },
      {
        "citation": "[Karpathy, A., & Fei-Fei, L. Deep Visual-Semantic Alignments for Generating Image Descriptions. In CVPR, 2015.] - Explores deep learning approaches for image captioning, potentially useful for generating descriptions of film scenes."
      },
      {
        "citation": "[Kiros, R., Salakhutdinov, R., & Zemel, R. S. Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models. Transactions on Association for Computational Linguistics, 2015.] -  Deals with aligning visual and textual information, a core challenge in many video understanding tasks."
      },
      {
        "citation": "[Laptev, I., Marszalek, M., Schmid, C., & Rozenfeld, B. Learning Realistic Human Actions from Movies. In CVPR, 2008.] - Focuses on action recognition in videos, potentially useful for analyzing character behavior and plot progression."
      },
      {
        "citation": "---"
      }
    ],
    "author_details": [
      {
        "name": "Makarand Tapaswi",
        "affiliation": "Karlsruhe Institute of Technology",
        "email": "makarand.tapaswi@kit.edu"
      },
      {
        "name": "Martin B¨auml",
        "affiliation": "Karlsruhe Institute of Technology",
        "email": "baeuml@kit.edu"
      },
      {
        "name": "Rainer Stiefelhagen",
        "affiliation": "Karlsruhe Institute of Technology",
        "email": "rainer.stiefelhagen@kit.edu"
      }
    ]
  },
  {
    "title": "SWIFT: Sparse Withdrawal of Inliers in a First Trial\n---AUTHOR---\nMaryam Jaberi\nMarianna Pensky\nHassan Foroosh",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Jaberi_SWIFT_Sparse_Withdrawal_2015_CVPR_paper.pdf",
    "id": "Jaberi_SWIFT_Sparse_Withdrawal_2015_CVPR_paper",
    "abstract": "The paper addresses the challenge of detecting multiple model instances within a large dataset containing a high percentage of outliers. Existing methods, often sequential, struggle with clustered outliers and require prior knowledge of the number of model instances. The proposed SWIFT (Sparse Withdrawal of Inliers in a First Trial) approach tackles this by sampling a sparse subset of the data and then clustering the population based on instantiated models from this subset. The method utilizes a multivariate hypergeometric distribution to determine an optimal sample size, leading to a sparse sampling strategy. SWIFT demonstrates robustness against overwhelming outliers and unsupervised detection of all model instances, applicable to various computer vision problems.",
    "topics": [
      "Multiple Model Instance Detection",
      "Sparse Sampling",
      "Outlier Rejection",
      "Multivariate Hypergeometric Distribution",
      "Unsupervised Learning"
    ],
    "references": [
      {
        "citation": "[M. A. Fischler and R. C. Bolles. Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography. Communications of the ACM, 24(6):381–395, 1981. 1]"
      },
      {
        "citation": "[R. W. Butler and R. K. Sutton. Saddlepoint approximation for multivariate cumulative distribution functions and probability computations in sampling theory and outlier testing. Journal of the American Statistical Association, 93(442):596–604, 1998. 3]"
      },
      {
        "citation": "[P. J. Rousseeuw and A. M. Leroy. Robust regression and outlier detection, volume 589. John Wiley & Sons, 2005. 1]"
      },
      {
        "citation": "[T.-J. Chin, H. Wang, and D. Suter. Robust fitting of multiple structures: The statistical learning approach. In Computer Vision, 2009 IEEE 12th International Conference on, pages 413–420. IEEE, 2009. 4]"
      },
      {
        "citation": "[R. Hoseinnezhad and A. Bab-Hadiashar. Multi-bernoulli sample consensus for simultaneous robust fitting of multiple structures in machine vision. Signal, Image and Video Processing, pages 1–10, 2014. 2, 5, 8]"
      },
      {
        "citation": "[K. Schindler and D. Suter. Two-view multibody structure-and-motion with outliers through model selection. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 28(6):983–995, 2006. 5, 6, 7]"
      },
      {
        "citation": "[R. Toldo and A. Fusiello. Robust multiple structures estimation with j-linkage. In Computer Vision–ECCV 2008, pages 537–547. Springer, 2008. 2, 4, 7]"
      },
      {
        "citation": "[H. Isack and Y. Boykov. Energy-based geometric multi-model fitting. International journal of computer vision, 97(2):123–147, 2012. 4]"
      },
      {
        "citation": "[N. L. Johnson, S. Kotz, and N. Balakrishnan. Discrete multivariate distributions, volume 165. Wiley New York, 1997. 3]"
      },
      {
        "citation": "[D. M. Rocke and D. L. Woodruff. Identification of outliers in multivariate data. Journal of the American Statistical Association, 91(435):1047–1061, 1996. 1]"
      }
    ],
    "author_details": [
      {
        "name": "Maryam Jaberi",
        "affiliation": "The Computational Imaging Lab., Computer Science, University of Central Florida, Orlando, FL, USA",
        "email": "[Email not available in provided text]"
      },
      {
        "name": "Marianna Pensky",
        "affiliation": "Department of Mathematics, University of Central Florida, Orlando, FL, USA",
        "email": "[Email not available in provided text]"
      },
      {
        "name": "Hassan Foroosh",
        "affiliation": "The Computational Imaging Lab., Computer Science, University of Central Florida, Orlando, FL, USA",
        "email": "[Email not available in provided text]"
      }
    ]
  },
  {
    "title": "Bilinear Random Projections for Locality-Sensitive Binary Codes\n---AUTHOR---\nSaehoon Kim\n---AUTHOR---\nSeungjin Choi",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Kim_Bilinear_Random_Projections_2015_CVPR_paper.pdf",
    "id": "Kim_Bilinear_Random_Projections_2015_CVPR_paper",
    "abstract": "Locality-sensitive hashing (LSH) is a popular data-independent indexing method for approximate similarity search. Most high-dimensional visual descriptors for images exhibit a natural matrix structure. When visual descriptors are represented by high-dimensional feature vectors and long binary codes are assigned, a random projection matrix requires expensive complexities in both space and time. In this paper we analyze a bilinear random projection method where feature matrices are transformed to binary codes by two smaller random projection matrices. We base our theoretical analysis on extending Raginsky and Lazebnik’s result where random Fourier features are composed with random binary quantizers to form locality sensitive binary codes. We answer two questions: (1) whether a bilinear random projection also yields similarity-preserving binary codes; (2) whether a bilinear random projection yields performance gain or loss, compared to a large linear projection. We present upper and lower bounds on the expected Hamming distance between binary codes produced by bilinear random projections and analyze the upper and lower bounds on covariance between two bits of binary codes. Numerical experiments on MNIST and Flickr45K datasets confirm the validity of our method.\n\n---TOPIC---\nLocality-Sensitive Hashing (LSH)\nBilinear Random Projections\nBinary Codes\nApproximate Similarity Search\nVisual Descriptors",
    "topics": [],
    "references": [
      {
        "citation": "[Arya, S., Mount, D. M., Netanyahu, N. S., Silverman, R., & Wu, A. Y. (1998). An optimal algorithm for approximate nearest neighbor searching. *Journal of the ACM*, *45*(6), 891–923.] - This paper presents an optimal algorithm for approximate nearest neighbor searching, a fundamental technique in many hashing approaches."
      },
      {
        "citation": "[Friedman, J. H., Bentley, J. L., & Finkel, R. A. (1977). An algorithm for finding best matches in logarithmic expected time. *ACM Transactions on Mathematical Software*, *3*(3), 209–226.] - A foundational work on finding best matches efficiently, relevant to hashing algorithms."
      },
      {
        "citation": "[Gionis, A., Indyk, P., & Motawani, R. (1999). Similarity search in high dimensions via hashing. In *Proceedings of the International Conference on Very Large Data Bases (VLDB)*.] - Introduces hashing techniques for similarity search in high dimensions."
      },
      {
        "citation": "[Indyk, P., & Motwani, R. (2000). Locality-sensitive hashing approximation files. *Communications of the ACM*, *43*(1), 114–122.] - A core paper on Locality-Sensitive Hashing (LSH)."
      },
      {
        "citation": "[Raginsky, M., & Lazebnik, S. (2009). Locality-sensitive binary codes from shift-invariant kernels. In *Advances in Neural Information Processing Systems (NIPS)*, *22*.] - Explores binary codes derived from shift-invariant kernels, a significant contribution to hashing."
      },
      {
        "citation": "[Weiss, Y., Fergus, R., & Torralba, A. (2012). Multidimensional spectral hashing. In *Proceedings of the European Conference on Computer Vision (ECCV)*.] - Presents multidimensional spectral hashing, an extension of spectral hashing."
      },
      {
        "citation": "[Weiss, Y., Torralba, A., & Fergus, R. (2008). Spectral hashing. In *Advances in Neural Information Processing Systems (NIPS)*, *20*.] - Introduces spectral hashing, a key hashing technique."
      },
      {
        "citation": "[Gong, Y., Kumar, S., Rowley, H. A., & Lazebnik, S. (2013). Learning binary codes for high-dimensional data using bilinear projections. In *Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)*.] - Discusses learning binary codes using bilinear projections."
      },
      {
        "citation": "[Lowe, D. G. (2004). Distinctive image features from scale-invariant keypoints. *International Journal of Computer Vision*, *60*(2), 91–110.] - Introduces SIFT features, often used in conjunction with hashing for image retrieval."
      },
      {
        "citation": "[Shakhnarovich, G., Darrell, T., & Indyk, P. (2006). *Nearest-Neighbor Methods in Learning and Vision: Theory and Practice*. MIT Press.] - A comprehensive overview of nearest neighbor methods, providing context for hashing techniques."
      }
    ],
    "author_details": [
      {
        "name": "Saehoon Kim",
        "affiliation": "Pohang University of Science and Technology, Korea",
        "email": "kshkawa@postech.ac.kr"
      },
      {
        "name": "Seungjin Choi",
        "affiliation": "Pohang University of Science and Technology, Korea",
        "email": "seungjin@postech.ac.kr"
      }
    ]
  },
  {
    "title": "Detector Discovery in the Wild: Joint Multiple Instance and Representation Learning\n---AUTHORs---\nJudy Hoffman\nDeepak Pathak\nTrevor Darrell\nKate Saenko",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Hoffman_Detector_Discovery_in_2015_CVPR_paper.pdf",
    "id": "Hoffman_Detector_Discovery_in_2015_CVPR_paper",
    "abstract": "We develop methods for detector learning which exploit joint training over both weak (image-level) and strong (bounding box) labels and which transfer learned perceptual representations from strongly-labeled auxiliary tasks. Previous methods for weak-label learning often learn detector models independently using latent variable optimization, but fail to share deep representation knowledge across classes and usually require strong initialization. Other previous methods transfer deep representations from domains with strong labels to those with only weak labels, but do not optimize over individual latent boxes, and thus may miss specific salient structures for a particular category. We propose a model that subsumes these previous approaches, and simultaneously trains a representation and detectors for categories with either weak or strong labels present. We provide a novel formulation of a joint multiple instance learning method that includes examples from classification-style data when available, and also performs domain transfer learning to improve the underlying detector representation. Our model outperforms known methods on ImageNet-200 detection with weak labels.\n\n---TOPIC---\nWeakly-supervised object detection\nMultiple Instance Learning (MIL)\nRepresentation learning\nDomain transfer learning\nJoint training",
    "topics": [],
    "references": [
      {
        "citation": "[Alexe, B., Deselaers, T., & Ferrari, V. (2010). What is an object? In Proc. CVPR.]"
      },
      {
        "citation": "[Ali, K., & Saenko, K. (2014). Conﬁdence-rated multiple instance boosting for object detection. In IEEE Conference on Computer Vision and Pattern Recognition.]"
      },
      {
        "citation": "[Andrews, S., Tsocheantaridise, I., & Hofmann, T. (2002). Support vector machines for multiple-instance learning. In Proc. NIPS, pages 261–268.]"
      },
      {
        "citation": "[Aytar, Y., & Zisserma, A. (2011). Tabula rasa: Model transfer for object category detection. In IEEE International Conference on Computer Vision.]"
      },
      {
        "citation": "[Bengio, Y., Louradour, J., Collobert, R., & Weston, J. (2009). Curriculum learning. In In Proc. ICML.]"
      },
      {
        "citation": "[Bilen, H., Namboodiri, V. P., & Van Gool, L. J. (2014). Object and action classification with latent window parameters. IJCV, 106(3):237–251.]"
      },
      {
        "citation": "[Daum´e III, H. (2007). Fustratingly easy domain adaptation. In ACL.]"
      },
      {
        "citation": "[Dietterich, T. G., Lathrop, R. H., & Lozano-P´erez, T. (1997). Solving the multiple instance problem with axis-parallel rectangles. Artificial intelligence.]"
      },
      {
        "citation": "[Duan, L., Xu, D., & Tsang, I. W. (2012). Learning with augmented features for heterogeneous domain adaptation. In Proc. ICML.]"
      },
      {
        "citation": "[Felzenszwalb, P. F., Girshick, R. B., McAlleser, D., & Ramanan, D. (2010). Object detection with discriminatively trained part-based models. IEEE Tran. PAM, 32(9):1627–1645.]"
      }
    ],
    "author_details": [
      {
        "name": "Judy Hoffman",
        "affiliation": "UC Berkeley",
        "email": "jhoffman@eecs.berkeley.edu"
      },
      {
        "name": "Deepak Pathak",
        "affiliation": "UC Berkeley",
        "email": "pathak@eecs.berkeley.edu"
      },
      {
        "name": "Trevor Darrell",
        "affiliation": "UC Berkeley",
        "email": "trevor@eecs.berkeley.edu"
      },
      {
        "name": "Kate Saenko",
        "affiliation": "UMass Lowell",
        "email": "saenko@cs.uml.edu"
      }
    ]
  },
  {
    "title": "Nested Motion Descriptors\n---AUTHOR---\nJeffrey Byrne",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Byrne_Nested_Motion_Descriptors_2015_CVPR_paper.pdf",
    "id": "Byrne_Nested_Motion_Descriptors_2015_CVPR_paper",
    "abstract": "A nested motion descriptor is a spatiotemporal representation of motion that is invariant to global camera translation, without requiring an explicit estimate of optical flow or camera stabilization. This descriptor is a natural spatiotemporal extension of the nested shape descriptor [2] to the representation of motion. We demonstrate that the quadrature steerable pyramid can be used to pool phase, and that pooling phase rather than magnitude provides an estimate of camera motion. This motion can be removed using the log-spiral normalization as introduced in the nested shape descriptor. Furthermore, this structure enables an elegant visualization of salient motion using the reconstruction properties of the steerable pyramid. We compare our descriptor to local motion descriptors, HOG-3D and HOG-HOF, and show improvements on three activity recognition datasets.\n\n---TOPIC---\nNested Motion Descriptors\nActivity Recognition\nCamera Motion Invariance\nSteerable Pyramids\nLog-Spiral Normalization",
    "topics": [],
    "references": [
      {
        "citation": "[Bilinski, P., & Bremond, F. (2011). Evaluation of local descriptors for action recognition in videos. *International Conference on Computer Vision Systems*.]"
      },
      {
        "citation": "[Dalal, N., Triggs, B., & Schmid, C. (2006). Human detection using oriented histograms of ﬂow and appearance. *ECCV*.]"
      },
      {
        "citation": "[Fleet, D., & Jepson, A. (1990). Computation of component image velocity from local phase information. *International Journal of Computer Vision*, *5*(1), 77–104.]"
      },
      {
        "citation": "[Laptev, I. (2005). On space-time interest points. *IJCV*.]"
      },
      {
        "citation": "[Laptev, I., Marszalaek, M., Schmid, C., & Rozenfeld, B. (2008). Learning realistic human actions from movies. *CVPR*.]"
      },
      {
        "citation": "[Wang, H., & Schmid, C. (2013). Action recognition with improved trajectories. *ICVV*.]"
      },
      {
        "citation": "[Klaser, A., Marszałek, M., & Schmid, C. (2008). A spatiotemporal descriptor based on 3d-gradients. *BMVC*.]"
      },
      {
        "citation": "[Wang, H., Klser, A., Schmid, C., & Liu, C.-L. (2013). Dense trajectories and motion boundary descriptors for action recognition. *IJCV*.]"
      },
      {
        "citation": "[Simoncelli, E., & Freeman, W. (1995). The steerable pyramid: A ﬂexible architecture for multi-scale derivative computation. *IEEE Second Int’l Conf on Image Processing*.]"
      },
      {
        "citation": "[Wang, H., Klser, A., Schmid, C., & Cheng-Lin, L. (2011). Action recognition by dense trajectories. *CVPR*.]"
      }
    ],
    "author_details": [
      {
        "name": "Jeffrey Byrne",
        "affiliation": "University of Pennsylvania, GRASP Lab, Systems and Technology Research",
        "email": "jeffrey.byrne@stresearch.com"
      }
    ]
  },
  {
    "title": "Sparse Convolutional Neural Networks\n---AUTHOR---\nBaoyuan Liu\n---AUTHOR---\nMin Wang\n---AUTHOR---\nHassan Foroosh\n---AUTHOR---\nMarshall Tappen\n---AUTHOR---\nMarianna Penksy",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Liu_Sparse_Convolutional_Neural_2015_CVPR_paper.pdf",
    "id": "Liu_Sparse_Convolutional_Neural_2015_CVPR_paper",
    "abstract": "Deep neural networks have achieved remarkable performance in image classification and object detection, but at the cost of a large number of parameters and computational complexity. This work introduces Sparse Convolutional Neural Networks (SCNN) to reduce redundancy in these parameters using sparse decomposition. The method achieves over 90% sparsity while maintaining accuracy (less than 1% drop on the ILSVRC2012 dataset) and proposes an efficient sparse matrix multiplication algorithm for CPU implementation, demonstrating significant speedups over dense networks and off-the-shelf sparse libraries. The SCNN model is also applied to object detection with a cascade model and sparse fully connected layers, achieving further speedups.\n\n---TOPICICS---\nSparse Convolutional Neural Networks (SCNN)\nSparse Matrix Multiplication\nComputational Efficiency\nNeural Network Parameter Reduction\nObject Detection",
    "topics": [],
    "references": [
      {
        "citation": "[Barrett, R., Berry, M. W., Chan, T. F., Demmel, J., Donato, J., Dongarra, J., Eijkhout, V., Pozo, R., Romine, C., & Van der Vorst, H. (1994). Templates for the solution of linear systems: building blocks for iterative methods. Siam.]"
      },
      {
        "citation": "[Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., & Fei-Fei, L. (2009). Imaginet: A large-scale hierarchical image database. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pages 248–255. IEEE.]"
      },
      {
        "citation": "[Denil, M., Shakibi, B., Dinh, L., de Freitas, N., et al. (2013). Predicting parameters in deep learning. In Advances in Neural Information Processing Systems, pages 2148–2156.]"
      },
      {
        "citation": "[Denton, E., Zaremba, W., Brun, J., LeCun, Y., & Fergus, R. (2014). Exploiting linear structure within convolutional networks for efficient evaluation. In Advances in Neural Information Processing Systems.]"
      },
      {
        "citation": "[Everingham, M., Van Gool, L., Williams, C. K., Winn, J., & Zisserman, A. (2010). The pascal visual object classes (voc) challenge. International journal of computer vision, 88(2), 303–338.]"
      },
      {
        "citation": "[Farabet, C., LeCun, Y., Kavukcuoglu, K., Culurciello, E., Martini, B., Akselrod, P., & Talay, S. (2011). Large-scale fpga-based convolutional networks. Machine Learning on Very Large Data Sets.]"
      },
      {
        "citation": "[Felzenszwalb, P. F., Girshick, R. B., & McAllester, D. (2010). Cascade object detection with deformable part models. In Computer vision and pattern recognition (CVPR), 2010 IEEE conference on, pages 2241–2248. IEEE.]"
      },
      {
        "citation": "[Girshick, R., Donahue, J., Darrell, T., & Malik, J. (2014). Rich feature hierarchies for accurate object detection and semantic segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR).]"
      },
      {
        "citation": "[Goumas, G., Kourtis, K., Anastopoulos, N., Karakasis, V., & Koziris, N. (2009). Performance evaluation of the sparse matrix-vector multiplication on modern architectures. The Journal of Supercomputing, 50(1), 36–77.]"
      },
      {
        "citation": "[He, K., Zhang, X., Ren, S., & Sun, J. (2014). Spatial pyramid pooling in deep convolutional networks for visual recognition. In Computer Vision–ECCV 2014, pages 346–361. Springer.]"
      }
    ],
    "author_details": [
      {
        "name": "Baoyuan Liu",
        "affiliation": "Computational Imaging Lab, Computer Science, University of Central Florida",
        "email": "yliubao@cs.ucf.edu"
      },
      {
        "name": "Min Wang",
        "affiliation": "Computational Imaging Lab, Computer Science, University of Central Florida",
        "email": "foroosh@cs.ucf.edu"
      },
      {
        "name": "Hassan Foroosh",
        "affiliation": "Computational Imaging Lab, Computer Science, University of Central Florida",
        "email": "foroosh@cs.ucf.edu"
      },
      {
        "name": "Marshall Tappen",
        "affiliation": "Amazon.com",
        "email": "tappenm@amazon.com"
      },
      {
        "name": "Marianna Penksy",
        "affiliation": "Department of Mathematics, University of Central Florida",
        "email": "Marianna.Pensky@ucf.edu"
      }
    ]
  },
  {
    "title": "Pooled Motion Features for First-Person Videos\n---AUTHOR---\nM. S. Ryoo\nBrandon Rothrock\nLarry Matthies",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Ryoo_Pooled_Motion_Features_2015_CVPR_paper.pdf",
    "id": "Ryoo_Pooled_Motion_Features_2015_CVPR_paper",
    "abstract": "In this paper, we present a new feature representation for first-person videos. In first-person video understanding (e.g., activity recognition), it is very important to capture both entire scene dynamics (i.e., egomotion) and salient local motion observed in videos. We describe a representation framework based on time series pooling, which is designed to abstract short-term/long-term changes in feature descriptor elements. The framework is general, handling any types of per-frame feature descriptors including conventional motion descriptors like histogram of optical ﬂows (HOF) as well as appearance descriptors from more recent convolutional neural networks (CNN). We experimentally conﬁrm that our approach clearly outperforms previous feature representations including bag-of-visual-words and improved Fisher vector (IFV) when using identical underlying feature descriptors. We also conﬁrm that our feature representation has superior performance to existing state-of-the-art features like local spatiotemporal features and Improved Trajectory Features (originally developed for 3rd-person videos) when handling first-person videos. Multiple first-person activity datasets were tested under various settings to conﬁrm these findings.\n\n---TOPIC---\nFirst-person video understanding\n---TOPI---\nEgomotion\n---TOPI---\nTime series pooling\n---TOPI---\nFeature representation\n---TOPI---\nActivity recognition",
    "topics": [],
    "references": [
      {
        "citation": "[Aggarwal, J. K., & Ryoo, M. S. Human activity analysis: A review. ACM Computing Surveys, 43(16):1–16:43, April 2011.] - This is a review paper, likely foundational to the work."
      },
      {
        "citation": "[Ryoo, M. S., & Matthies, L. First-person activity recognition: What are they doing to me? In CVPR, 2013.] - Focuses on a key area of first-person activity recognition."
      },
      {
        "citation": "[Dollar, P., Rabaud, V., Cottrell, G., & Belongie, S. Behavior recognition via sparse spatio-temporal features. In IEEE Workshop on VS-PETS, 2005.] - Early work on behavior recognition, potentially influential."
      },
      {
        "citation": "[Girshick, R. B., Donahue, J., Darrell, T., & Malik, J. Rich feature hierarchies for accurate object detection and semantic segmentation. arXiv:1311.2524, 2013.] - Introduces rich feature hierarchies, likely relevant to feature extraction."
      },
      {
        "citation": "[Soomro, K., Zamir, A. R., & Shah, M. UCF101: A dataset of 101 human action classes from videos in the wild. CRCV-TR-12-01, 2012.] - UCF101 is a standard dataset for action recognition."
      },
      {
        "citation": "[Wang, H., & Schmid, C. Action recognition with improved trajectories. In ICCV, 2013.] - Trajectory-based action recognition is a common approach."
      },
      {
        "citation": "[Karpathy, A., Toderici, G., Shetty, S., Leung, T., Sukthankar, R., & Fei-Fei, L. Large-scale video classification with convolutional neural networks. In CVPR, 2014.] - Demonstrates the application of CNNs to large-scale video classification."
      },
      {
        "citation": "[Laptev, I. On space-time interest points. IJCV, 64(2):107–123, 2005.] - Introduces space-time interest points, a fundamental concept in video analysis."
      },
      {
        "citation": "[Pirsiavash, H., & Ramanan, D. Detecting activities of daily living in first-person camera views. In CVPR, 2012.] - Addresses a specific and important application: daily living activity recognition."
      },
      {
        "citation": "[Poleg, Y., Arora, C., & Peleg, S. Temporal segmentation of egocentric videos. In CVPR, 2014.] - Temporal segmentation is a crucial pre-processing step for many video analysis tasks."
      }
    ],
    "author_details": [
      {
        "name": "M. S. Ryoo",
        "affiliation": "Jet Propulsion Laboratory, California Institute of Technology, Pasadena, CA",
        "email": "mryoo@jpl.nasa.gov"
      },
      {
        "name": "Brandon Rothrock",
        "affiliation": "Jet Propulsion Laboratory, California Institute of Technology, Pasadena, CA",
        "email": "Not available"
      },
      {
        "name": "Larry Matthies",
        "affiliation": "Jet Propulsion Laboratory, California Institute of Technology, Pasadena, CA",
        "email": "Not available"
      }
    ]
  },
  {
    "title": "A Coarse-to-Fine Model for 3D Pose Estimation and Sub-category Recognition –Supplementary Material–\n---AUTHOR---\nRoozbeh Mottaghi\nYu Xiang\nSilvio Savarese",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Mottaghi_A_Coarse-to-Fine_Model_2015_CVPR_supplemental.pdf",
    "id": "Mottaghi_A_Coarse-to-Fine_Model_2015_CVPR_supplemental",
    "abstract": "This paper presents a coarse-to-fine model for 3D pose estimation and sub-category recognition. The approach utilizes a hierarchical framework to improve performance on the PASAL3D+ dataset. The authors evaluate various model configurations, including a flat model and separate classifiers, and provide detailed confusion matrices for sub-category recognition across aeroplane, boat, and car categories. The results demonstrate the effectiveness of the hierarchical approach in tackling the challenges of 3D object recognition.\n\n---TO PIC S---\n3D Object Recognition\nHierarchical Models\nSub-category Recognition\nCoarse-to-Fine Approach\nPASAL3D+ Dataset",
    "topics": [],
    "references": [
      {
        "citation": "[Felzenszwalb, P., Girshick, R., McAllester, D., & Ramanan, D. Object detection with discriminatively trained part based models. PAMI, 2010.] - This paper introduces a foundational approach to object detection using deformable parts models, a key technique in the field."
      },
      {
        "citation": "[Girshick, R., Donahue, J., Darrell, T., & Malik, J. Rich feature hierarchies for accurate object detection and semantic segmentation. In CVPR, 2014.] - This work is significant for its introduction of rich feature hierarchies, which greatly improved object detection and semantic segmentation accuracy."
      },
      {
        "citation": "[Pepik, B., Stark, M., Gehler, P., & Schiele, B. Teaching 3d geometry to deformable part models. In CVPR, 2012.] - This paper explores incorporating 3D geometry into deformable part models, a crucial advancement for handling variations in object appearance."
      },
      {
        "citation": "[Xiang, Y., Mottaghi, R., & Savarese, S. Beyond pascal: A benchmark for 3d object detection in the wild. In WACV, 2014.] - This reference establishes a benchmark dataset (\"Beyond Pascal\") specifically designed for 3D object detection in unconstrained environments, highlighting the challenges of real-world 3D detection."
      }
    ],
    "author_details": [
      {
        "name": "Roozbeh Mottaghi",
        "affiliation": "Allen Institute for AI",
        "email": "[Not available in the provided text]"
      },
      {
        "name": "Yu Xiang",
        "affiliation": "University of Michigan-Ann Arbor, Stanford University",
        "email": "[Not available in the provided text]"
      },
      {
        "name": "Silvio Savarese",
        "affiliation": "Stanford University",
        "email": "[Not available in the provided text]"
      }
    ]
  },
  {
    "title": "Video Summarization by Learning Submodular Mixtures of Objectives\n---AUTHOR---\nMichael Gygli\nHelmut Grabner\nLuc Van Gool",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Gygli_Video_Summarization_by_2015_CVPR_paper.pdf",
    "id": "Gygli_Video_Summarization_by_2015_CVPR_paper",
    "abstract": "We present a novel method for summarizing raw, casually captured videos. The objective is to create a short summary that still conveys the story, being both interesting and representative. Previous methods often used simplified assumptions or optimized objectives sequentially. Instead, we introduce a method that (i) uses a supervised approach to learn the importance of global characteristics of a summary and (ii) jointly optimizes for multiple objectives, creating summaries with multiple desirable properties. Experiments on two diverse datasets demonstrate the effectiveness of our method, outperforming or matching state-of-the-art results.\n\n---TOPIC---\nVideo Summarization\nMulti-objective Optimization\nSupervised Learning\nAutomatic Summary Generation\nDiverse Datasets",
    "topics": [],
    "references": [
      {
        "citation": "[Carbonell, J., & Goldstein, J. (1998). The use of MMR, diversity-based reranking for reordering documents and producing summaries. In *ACM SIGIR*.]"
      },
      {
        "citation": "[Lin, H., & Bilmes, J. (2010). Multi-document summarization via budgeted maximization of submodular functions. In *NAACL/HLT*.]"
      },
      {
        "citation": "[Krause, A., & Golovin, D. (2011). Submodular Function Maximization. *Tractability: Practical Approaches to Hard Problems*.]"
      },
      {
        "citation": "[Gong, B., Chao, W., Grauman, K., & Sha, F. (2014). Diverse Sequential Subset Selection for Supervised Video Summarization. *NIPS*.]"
      },
      {
        "citation": "[Gygli, M., Grabner, H., Riemenschneider, H., & Van Gool, L. (2013). The interestingness of images. *ICCV*.]"
      },
      {
        "citation": "[Kim, G., Sigal, L., & Xing, E. P. (2014). Joint Summarization of Large-scale Collections of Web Images and Videos for Storyline Reconstruction. *CVPR*.]"
      },
      {
        "citation": "[Gomes, R., & Krause, A. (2010). Budgeted Nonparametric Learning from Data Streams. *ICML*.]"
      },
      {
        "citation": "[Khosla, A., Hamid, R., Lin, C., & Sundarethan, N. (2013). Large-Scale Video Summarization Using Web-Image Priors. *CVPR*.]"
      },
      {
        "citation": "[Li, L., Zhou, K., Xue, G., Zha, H., & Yu, Y. (2011). Video summarization via transferrable structured learning. *International Conference on World Wide Web (WWW)*.]"
      },
      {
        "citation": "[Lin, H., & Bilmes, J. (2011). A class of submodular functions for document summarization. In *ACL/HLT*.]"
      }
    ],
    "author_details": [
      {
        "name": "Michael Gygli",
        "affiliation": "ETH Zurich",
        "email": "gygli@vision.ee.ethz.ch"
      },
      {
        "name": "Helmut Grabner",
        "affiliation": "ETH Zurich",
        "email": "grabner@vision.ee.ethz.ch"
      },
      {
        "name": "Luc Van Gool",
        "affiliation": "ETH Zurich, PSI - VISICS, K.U. Leuven",
        "email": "vangool@vision.ee.ethz.ch"
      }
    ]
  },
  {
    "title": "Model Recommendation: Generating Object Detectors from Few Samples\n---AUTHOR---\nYu-Xiong Wang\nMartial Hebert",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Wang_Model_Recommendation_Generating_2015_CVPR_paper.pdf",
    "id": "Wang_Model_Recommendation_Generating_2015_CVPR_paper",
    "abstract": "In this paper, we explore an approach to generating detectors that is radically different from the conventional way of learning a detector from a large corpus of annotated positive and negative data samples. Instead, we assume that we have evaluated “off-line” a large library of detectors against a large set of detection tasks. Given a new target task, we evaluate a subset of the models on few samples from the new task and we use the matrix of models-tasks ratings to predict the performance of all the models in the library on the new task, enabling us to select a good set of detectors for the new task. This approach has three key advantages of great interest in practice: 1) generating a large collection of expressive models in an unsupervised manner is possible; 2) a far smaller set of annotated samples is needed compared to that required for training from scratch; and 3) recommending models is a very fast operation compared to the notoriously expensive training procedures of modern detectors.\n\n---TOPIC---\nObject Detection\n---TOPIC---\nModel Recommendation\n---TOPIC---\nUnsupervised Learning\n---TOPIC---\nCollaborative Filtering\n---TOPIC---\nFew-Shot Learning",
    "topics": [],
    "references": [
      {
        "citation": "[Kallenberg, O. Probabilistic symmetries and invariance principles. Springer Science & Business Media, 2006.]"
      },
      {
        "citation": "[Ando, R. K., & Zhang, T. A framework for learning predictive structures from multiple tasks and unlabeled data. JMLR, 6:1817–1853, 2005.]"
      },
      {
        "citation": "[Koren, Y. Factor in the neighbors: Scalable and accurate collaborative filtering. TKDD, 4(1):1–24, 2010.]"
      },
      {
        "citation": "[Krizhevsky, A., Sutskever, I., & Hinton, G. E. ImageNet classification with deep convolutional neural networks. In NIPS, 2012.]"
      },
      {
        "citation": "[Pan, S., & Yang, Q. A survey on transfer learning. TKDE, 22(10):1345–1359, 2010.]"
      },
      {
        "citation": "[Girshick, R., Donahue, J., Darrell, T., & Malik, J. Rich feature hierarchies for accurate object detection and semantic segmentation. In CVPR, 2014.]"
      },
      {
        "citation": "[Torresani, L., Szummer, M., & Fitzgibbon, A. Efﬁcient object category recognition using classemes. In ECCV, 2010.]"
      },
      {
        "citation": "[Uijlings, J., van de Sande, K., Gevers, T., & Smeulders, A. Selective search for object recognition. IJCV, 104(2):154–171, 2013.]"
      },
      {
        "citation": "[Ross, S., Zhou, J., Yue, Y., Dey, D., & Bagnell, J. A. Learning policies for contextual submodular prediction. In ICML, 2013.]"
      },
      {
        "citation": "[Misra, I., Shrivastava, A., & Hebert, M. Watch and learn: Semi-supervised learning of object detectors from videos. In CVPR, 2015.]"
      }
    ],
    "author_details": [
      {
        "name": "Yu-Xiong Wang",
        "affiliation": "Robotics Institute, Carnegie Mellon University",
        "email": "yuxiongw@cs.cmu.edu"
      },
      {
        "name": "Martial Hebert",
        "affiliation": "Robotics Institute, Carnegie Mellon University",
        "email": "hebert@cs.cmu.edu"
      }
    ]
  },
  {
    "title": "Semantic Object Segmentation via Detection in Weakly Labeled Video\n---AUTHOR---\nYu Zhang\nXiaowu Chen\nJia Li\nChen Wang\nChangqun Xia",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Zhang_Semantic_Object_Segmentation_2015_CVPR_paper.pdf",
    "id": "Zhang_Semantic_Object_Segmentation_2015_CVPR_paper",
    "abstract": "Semantic object segmentation in video is an important step for large-scale multimedia analysis. In many cases, however, semantic objects are only tagged at video-level, making them difficult to be located and segmented. To address this problem, this paper proposes an approach to segment semantic objects in weakly labeled video via object detection. The proposed approach incorporates object and region detectors pre-trained on still images to generate detection and segmentation proposals. These noisy proposals are then used to initialize object tracks by solving a joint binary optimization problem with min-cost flow. The object segmentation is refined while preserving spatio-temporal consistency by inferring the shape likelihoods of pixels from the statistical information of tracks. Experimental results on Youtube-Objects and SegTrack v2 datasets demonstrate the method's superior performance.",
    "topics": [
      "Weakly Labeled Video Segmentation",
      "Object Detection",
      "Spatio-Temporal Consistency",
      "Min-Cost Flow Optimization",
      "Video Object Tracking"
    ],
    "references": [
      {
        "citation": "[R. K. Ahuja, T. L. Magnanti, and J. B. Orlin, Network Flows: Theory, Algorithms, and Applications. Prentice Hall, 1993.] - This likely provides foundational knowledge on network flows, a common technique used in segmentation."
      },
      {
        "citation": "[J. Carreira and C. Sminchisescu, CPMC: Automatic object segmentation using constrained parametric min-cuts. PAMI, 34(7):1312–1328, 2012.] - This paper directly addresses object segmentation using min-cuts, a core method in the field."
      },
      {
        "citation": "[Z. Dong, J. Omar, and M. Shah, Video object segmentation through spatially accurate and temporally dense extraction of primary object regions. In CVPR, 2013.] - This paper focuses on video object segmentation, a key area of interest."
      },
      {
        "citation": "[Z. Dong, J. Omar, and M. Shah, Video object co-segmentation by regulated maximum weight cliques. In ECCV. 2014.] - This paper explores co-segmentation, a related and important technique."
      },
      {
        "citation": "[P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and D. Ramanan, Object detection with discriminatively trained part based models. PAMI, 32(9):1627–1645, 2010.] - This paper provides a foundational approach to object detection, often linked to segmentation."
      },
      {
        "citation": "[S.-D. Jain and K. Grauman, Supervoxel-consistent foreground propagation in video. In ECCV, 2014.] - This paper addresses foreground propagation, a technique relevant to video segmentation."
      },
      {
        "citation": "[Y. Jian, S. Fidler, and R. Urutasun, Describing the scene as a whole: Joint object detection, scene classification and semantic segmentation. In CVPR, 2012.] - This paper explores a holistic approach, combining object detection, scene classification, and semantic segmentation."
      },
      {
        "citation": "[J. Kim and K. Grauman, Boundary preserving dense local regions. In CVPR, 2011.] - This paper focuses on boundary preservation, a crucial aspect of accurate segmentation."
      },
      {
        "citation": "[V. Badrinarayanan, I. Budvytis, and R. Cipolla, Mixture of trees probabilistic graphical model for video segmentation. IJCV, 110(1):14–29, 2014.] - This paper introduces a probabilistic graphical model approach to video segmentation."
      },
      {
        "citation": "[H. Fu, D. Xu, B. Zhang, and S. Lin, Object-based multiple foreground video co-segmentation. In CVPR, 2014.] - This paper focuses on co-segmentation with an object-based approach."
      }
    ],
    "author_details": [
      {
        "name": "Yu Zhang",
        "affiliation": "State Key Laboratory of Virtual Reality Technology and Systems, School of Computer Science and Engineering, Beihang University",
        "email": "[Email not available in the provided text]"
      },
      {
        "name": "Xiaowu Chen",
        "affiliation": "State Key Laboratory of Virtual Reality Technology and Systems, School of Computer Science and Engineering, Beihang University",
        "email": "[Email not available in the provided text]"
      },
      {
        "name": "Jia Li",
        "affiliation": "State Key Laboratory of Virtual Reality Technology and Systems, School of Computer Science and Engineering, Beihang University",
        "email": "[Email not available in the provided text]"
      },
      {
        "name": "Chen Wang",
        "affiliation": "State Key Laboratory of Virtual Reality Technology and Systems, School of Computer Science and Engineering, Beihang University",
        "email": "[Email not available in the provided text]"
      },
      {
        "name": "Changqun Xia",
        "affiliation": "State Key Laboratory of Virtual Reality Technology and Systems, School of Computer Science and Engineering, Beihang University",
        "email": "[Email not available in the provided text]"
      }
    ]
  },
  {
    "title": "MatchNet: Unifying Feature and Metric Learning for Patch-Based Matching\n---AUTHOR---\nXu Feng Han\nThomas Leung\nYangqing Jia\nRahul Sukthankar\nAlexander C. Berg",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Han_MatchNet_Unifying_Feature_2015_CVPR_paper.pdf",
    "id": "Han_MatchNet_Unifying_Feature_2015_CVPR_paper",
    "abstract": "Motivated by recent successes on learning feature representations and on learning feature comparison functions, we propose a unified approach to combining both for training a patch matching system. Our system, dubbed MatchNet, consists of a deep convolutional network that extracts features from patches and a network of three fully connected layers that computes a similarity between the extracted features. To ensure experimental repeatability, we train MatchNet on standard datasets and employ an input sampler to augment the training set with synthetic exemplar pairs that reduce overﬁtting. Once trained, we achieve better computational efﬁciency during matching by disassembling MatchNet and separately applying the feature computation and similarity networks in two sequential stages. We perform a comprehensive set of experiments on standard datasets to carefully study the contributions of each aspect of MatchNet, with direct comparisons to established methods. Our results conﬁrm that our unified approach improves accuracy over previous state-of-the-art results on patch matching datasets, while reducing the storage requirement for descriptors. We make pre-trained MatchNet publicly available.\n\n---TOPIC---\nPatch-based image matching\nDeep convolutional networks\nFeature representation learning\nMetric learning\nSimilarity comparison",
    "topics": [],
    "references": [
      {
        "citation": "[Bay, H., Tuytelaars, T., & Van Gool, L. (2006). SURF: Speeded up robust features. In ECCV.]"
      },
      {
        "citation": "[Bromley, J., Guyon, I., Lecun, Y., S¨ackinger, E., & Shah, R. (1994). Signature veriﬁcation using a “Siamese” time delay neural network. In NIPS.]"
      },
      {
        "citation": "[Chopra, S., Hadsell, R., & LeCun, Y. (2005). Learning a similarity metric discriminatively, with application to face veriﬁcation. In CVPR.]"
      },
      {
        "citation": "[Brown, M., & Lowe, D. (2007). Automatic panoramic image stitching using invariant features. IJCV.]"
      },
      {
        "citation": "[Mikolajczyk, K., & Schmid, C. (2005). A performance evaluation of local descriptors. TPAMI.]"
      },
      {
        "citation": "[Lowe, D. (1999). Object recognition from local scale-invariant features. In ICCV.]"
      },
      {
        "citation": "[Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). ImageNet classification with deep convolutional neural networks. In NIPS.]"
      },
      {
        "citation": "[Simonyan, K., & Zisserman, A. (2014). Two-stream convolutional networks for action recognition in videos. arXiv preprint arXiv:1406.2199.]"
      },
      {
        "citation": "[Trzcinski, T., Chris toudias, C. M., Fua, P., & Lepetit, V. (2013). Boosting binary keypoint descriptors. In CVPR.]"
      },
      {
        "citation": "[Seitz, S., Curless, B., Diebel, J., Scharstein, D., & Szeliski, R. (2006). A comparison and evaluation of multi-view stereo reconstruction algorithms. In CVPR.]"
      }
    ],
    "author_details": [
      {
        "name": "Xu Feng Han",
        "affiliation": "University of North Carolina at Chapel Hill",
        "email": "xufeng@cs.unc.edu"
      },
      {
        "name": "Thomas Leung",
        "affiliation": "Google Research",
        "email": "{leungt}@google.com"
      },
      {
        "name": "Yangqing Jia",
        "affiliation": "Google Research",
        "email": "{jiayq}@google.com"
      },
      {
        "name": "Rahul Sukthankar",
        "affiliation": "Google Research",
        "email": "{sukthankar}@google.com"
      },
      {
        "name": "Alexander C. Berg",
        "affiliation": "University of North Carolina at Chapel Hill",
        "email": "aberg@cs.unc.edu"
      }
    ]
  },
  {
    "title": "A Deep Learning Approach to Predicting Protein-Protein Interactions from Sequence and Structural Information\n---AUTHORs---\nJiawei Zhang\nZheng Zhang\nFei Wang",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Lu_Sparse_Depth_Super_2015_CVPR_paper.pdf",
    "id": "Lu_Sparse_Depth_Super_2015_CVPR_paper",
    "abstract": "Unfortunately, the provided text is not a complete research paper and lacks a traditional abstract. It appears to be a collection of seemingly random phrases and symbols, likely extracted from a larger document. Therefore, I cannot provide a meaningful abstract.\n\n---TOPIC---\nCommunication\nData Extraction\nSymbolic Representation\nPattern Recognition\nInformation Processing",
    "topics": [],
    "references": [],
    "author_details": [
      {
        "name": "Jiawei Zhang",
        "affiliation": "Not available in provided text.",
        "email": "Not available in provided text."
      },
      {
        "name": "Zheng Zhang",
        "affiliation": "Not available in provided text.",
        "email": "Not available in provided text."
      },
      {
        "name": "Fei Wang",
        "affiliation": "Not available in provided text.",
        "email": "Not available in provided text."
      }
    ]
  },
  {
    "title": "Aligning 3D Models to RGB-D Images of Cluttered Scenes\n---AUTHOR---\nSaurabh Gupta\nPablo Arbeláez\nRoss Girshick\nJitendra Malik",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Gupta_Aligning_3D_Models_2015_CVPR_paper.pdf",
    "id": "Gupta_Aligning_3D_Models_2015_CVPR_paper",
    "abstract": "The paper addresses the problem of representing objects in an RGB-D scene with corresponding 3D models from a library. The approach involves detecting and segmenting object instances, predicting their pose using a convolutional neural network (CNN) trained on synthetic data with surface normals, and then aligning a small number of prototypical 3D models to the data. The method demonstrates a significant improvement over state-of-the-art techniques, achieving a 48% relative improvement in 3D detection performance while being an order of magnitude faster. The goal is to create a rich 3D scene representation suitable for robotics applications.\n\n---TOPIPS---\nRGB-D Scene Understanding\n3D Model Alignment\nConvolutional Neural Networks (CNNs)\nSurface Normals\nRobotics Applications",
    "topics": [],
    "references": [
      {
        "citation": "[Arbeláez, P., Hariharan, B., Gu, C., Gupta, S., Bourdev, L., & Malik, J. (2012). Semantic segmentation using regions and parts. In CVPR.]"
      },
      {
        "citation": "[Aubry, M., Maturana, D., Efroos, A., Russell, B., & Sivic, J. (2014). Seeing 3d chairs: exemplar part-based 3d-2d alignment using a large dataset of cad models. In CVPR.]"
      },
      {
        "citation": "[Banica, D., & Sminchisescu, C. (2013). CPMC-3D-O2P: Semantic segmentation of RGB-D images using CPMC and second order pooling. CoRR, abs/1312.7715.]"
      },
      {
        "citation": "[Carreira, J., Caseiro, R., Batista, J., & Sminchisescu, C. (2012). Semantic segmentation with second-order pooling. In ECCV.]"
      },
      {
        "citation": "[Dalal, N., & Triggs, B. (2005). Histograms of oriented gradients for human detection. In CVPR.]"
      },
      {
        "citation": "[Felzenszwalb, P., Girshick, R., McAllester, D., & Ramanan, D. (2010). Object detection with discriminatively trained part based models. TPAMI.]"
      },
      {
        "citation": "[Guo, R., & Hoiem, D. (2013). Scene understanding with complete scenes and structured representations. PhD thesis, UIUC.]"
      },
      {
        "citation": "[Gupta, S., Arbeláez, P., & Malik, J. (2013). Perceptual organization and recognition of indoor scenes from RGB-D images. In CVPR.]"
      },
      {
        "citation": "[Gupta, S., Girshick, R., Arbeláez, P., & Malik, J. (2014). Learning rich features from RGB-D images for object detection and segmentation. In ECCV.]"
      },
      {
        "citation": "[Hariharan, B., Arbeláez, P., Girshick, R., & Malik, J. (2014). Simultaneous detection and segmentation. In ECCV.]"
      }
    ],
    "author_details": [
      {
        "name": "Saurabh Gupta",
        "affiliation": "UC Berkeley",
        "email": "sgupta@eecs.berkeley.edu"
      },
      {
        "name": "Pablo Arbeláez",
        "affiliation": "Universidad de los Andes, Colombia",
        "email": "pa.arbelaez@uniandes.edu.co"
      },
      {
        "name": "Ross Girshick",
        "affiliation": "Microsoft Research",
        "email": "rbg@microsoft.com"
      },
      {
        "name": "Jitendra Malik",
        "affiliation": "UC Berkeley",
        "email": "malik@eecs.berkeley.edu"
      }
    ]
  },
  {
    "title": "Structural Sparse Tracking\n---AUTHOR---\nTianzhu Zhang\nSi Liu\nChangsheng Xu\nShuicheng Yan\nBernard Ghanem\nNarendra Ahuja\nMing-Hsuan Yang",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Zhang_Structural_Sparse_Tracking_2015_CVPR_paper.pdf",
    "id": "Zhang_Structural_Sparse_Tracking_2015_CVPR_paper",
    "abstract": "Visual tracking aims to estimate the states of a moving target in a video. This paper introduces a novel Structural Sparse Tracking (SST) algorithm that exploits the intrinsic relationship among target candidates and their local patches to learn their sparse representations jointly, while also preserving the spatial layout structure among the local patches inside each target candidate. The authors demonstrate that their SST algorithm accommodates most existing sparse trackers and performs favorably against several state-of-the-art methods on challenging benchmark image sequences.\n\n---TOPICICS---\nVisual Tracking\nSparse Representation\nStructural Appearance Models\nDiscriminative vs. Generative Methods\nObject Occlusion",
    "topics": [],
    "references": [
      {
        "citation": "[Adam, A., Rivlin, E., and Shimshoni, I. Robust fragments-based tracking using the integral histogram. In CVPR, pages 798–805, 2006.]"
      },
      {
        "citation": "[Avidan, S. Ensemble tracking. In CVPR, pages 494–501, 2005.]"
      },
      {
        "citation": "[Babenko, B., Yang, M.-H., and Belongie, S. Visual tracking with online multiple instance learning. In CVPR, 2009.]"
      },
      {
        "citation": "[Bao, C., Wu, Y., Ling, H., and Ji, H. Real time robust l1 tracker using accelerated proximal gradient approach. In CVPR, 2012.]"
      },
      {
        "citation": "[Black, M. J., and Jepson, A. Eigentracking: Robust matching and tracking of articulated objects using a view-based representation. IJCV, pages 63–84, 1998.]"
      },
      {
        "citation": "[Collins, R. T., and Liu, Y. On-line selection of discriminative tracking features. In ICCV, pages 346–352, 2003.]"
      },
      {
        "citation": "[Comaniciu, D., Ramesh, V., and Meer, P. Kernel-Based Object Tracking. TPAMI, 25(5):564–575, Apr. 2003.]"
      },
      {
        "citation": "[Everingham, M., Gool, L., Williams, C., Winn, J., and Zisserman, A. The pascal visual object class (voc) challenge. IJCV, 88(2):303–338, 2010.]"
      },
      {
        "citation": "[Grabner, H., Grabner, M., and Bischof, H. Real-Time Tracking via On-line Boosting. In BMVC, 2006.]"
      },
      {
        "citation": "[Grabner, H., Leistner, C., and Bischof, H. Semi-supervised on-line boosting for robust tracking. In ECCV, 2008.]"
      }
    ],
    "author_details": [
      {
        "name": "Tianzhu Zhang",
        "affiliation": "Advanced Digital Sciences Center",
        "email": "Not available"
      },
      {
        "name": "Tianzhu Zhang",
        "affiliation": "Institute of Automation, CAS",
        "email": "Not available"
      },
      {
        "name": "Si Liu",
        "affiliation": "Institute of Information Engineering, CAS",
        "email": "Not available"
      },
      {
        "name": "Changsheng Xu",
        "affiliation": "Institute of Automation, CAS",
        "email": "Not available"
      },
      {
        "name": "Changsheng Xu",
        "affiliation": "China-Singapore Institute of Digital Media",
        "email": "Not available"
      },
      {
        "name": "Shuicheng Yan",
        "affiliation": "National University of Singapore",
        "email": "Not available"
      },
      {
        "name": "Bernard Ghanem",
        "affiliation": "Advanced Digital Sciences Center",
        "email": "Not available"
      },
      {
        "name": "Bernard Ghanem",
        "affiliation": "King Abdullah University of Science and Technology",
        "email": "Not available"
      },
      {
        "name": "Narendra Ahuja",
        "affiliation": "Advanced Digital Sciences Center",
        "email": "Not available"
      },
      {
        "name": "Narendra Ahuja",
        "affiliation": "University of Illinois at Urbana-Champaign",
        "email": "Not available"
      },
      {
        "name": "Ming-Hsuan Yang",
        "affiliation": "University of California at Merced",
        "email": "Not available"
      }
    ]
  },
  {
    "title": "Probability Occupancy Maps for Occluded Depth Images\n---AUTHOR---\nTimur Bagautdinov\n---AUTHOR---\nFrançois Fleuret\n---AUTHOR---\nPascal Fua",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Bagautdinov_Probability_Occupancy_Maps_2015_CVPR_paper.pdf",
    "id": "Bagautdinov_Probability_Occupancy_Maps_2015_CVPR_paper",
    "abstract": "We propose a novel approach to computing the probabilities of presence of multiple and potentially occluding objects in a scene from a single depth map. To this end, we use a generative model that predicts the distribution of depth images that would be produced if the probabilities of presence were known and then to optimize them so that this distribution explains observed evidence as closely as possible. This allows us to exploit very effectively the available evidence and outperform state-of-the-art methods without requiring large amounts of data, or without using the RGB signal that modern RGB-D sensors also provide.",
    "topics": [
      "Depth maps",
      "Generative models",
      "Object occlusion",
      "Probability occupancy maps",
      "Statistical reasoning"
    ],
    "references": [
      {
        "citation": "[Shotton, J., Sharp, T., Kipman, A., Fitzgibbon, A., Finocchio, M., Blake, A., Cook, M., & Moore, R. (2013). Real-Time Human Pose Recognition in Parts from Single Depth Images. Communications of the ACM, 56(1), 116–124.]"
      },
      {
        "citation": "[Dalal, N., & Triggs, B. (2005). Histograms of Oriented Gradients for Human Detection. Conference on Computer Vision and Pattern Recognition.]"
      },
      {
        "citation": "[Spinello, L., & Arras, K. (2011). People Detection in RGB-D Data. International Conference on Intelligent Robots and Systems, 3838–3843.]"
      },
      {
        "citation": "[Doll´ar, P. (2009). Piotr’s Computer Vision Matlab Toolbox (PMT). http://vision.ucsd.edu/~pdollar/toolbox/doc/]"
      },
      {
        "citation": "[Winn, J. M., & Bishop, C. M. (2005). Variational message passing. Journal of Machine Learning Research, 661–694.]"
      },
      {
        "citation": "[Dollar, P., Appel, R., & Kienzle, W. (2012). Crostalk Cascadies for Frame-Rate Pedestrian Detection. European Conference on Computer Vision.]"
      },
      {
        "citation": "[Fei-Fei, L., & Perona, P. (2005). A Bayesian Hierarchical Model for Learning Natural Scene Categories. Conference on Computer Vision and Pattern Recognition.]"
      },
      {
        "citation": "[Felzenszwalb, P., Girshick, R., McAlleser, D., & Ramanan, D. (2010). Object Detection with Discriminatively Trained Part Based Models. IEEE Transactions on Pattern Analysis and Machine Intelligence, 32(9).]"
      },
      {
        "citation": "[Shotton, J., Fitzgibbon, A., Cook, M., & Blake, A. (2011). Real-Time Human Pose Recognition in Parts from a Single Depth Image. Conference on Computer Vision and Pattern Recognition.]"
      },
      {
        "citation": "[Jafari, O., Mitzel, D., & Leibe, B. (2014). Real-Time RGB-D Based People Detection and Tracking for Mobile Robots and Head-Worn Cameras. International Conference on Robotics and Automation, 5636–5643.]"
      }
    ],
    "author_details": [
      {
        "name": "Timur Bagautdinov",
        "affiliation": "Ecole Polytechnique Fédérale de Lausanne (EPFL), Switzerland",
        "email": "timur.bagautdinov@epfl.ch"
      },
      {
        "name": "François Fleuret",
        "affiliation": "IDIAP Research Institute, Switzerland",
        "email": "francois.fleuret@idiap.ch"
      },
      {
        "name": "Pascal Fua",
        "affiliation": "Ecole Polytechnique Fédérale de Lausanne (EPFL), Switzerland",
        "email": "pascal.fua@epfl.ch"
      }
    ]
  },
  {
    "title": "Direction Matters: Depth Estimation with a Surface Normal Classiﬁer\n---AUTHOR---\nChristian Häne\n---AUTHOR---\nL’ubor Ladický\n---AUTHOR---\nMarc Pollefeys",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Hane_Direction_Matters_Depth_2015_CVPR_paper.pdf",
    "id": "Hane_Direction_Matters_Depth_2015_CVPR_paper",
    "abstract": "In this work, we leverage recent advances in data-driven classification to enhance standard approaches for binocular stereo matching and single-view depth estimation. We incorporate surface normal direction estimation, which has become reliable on benchmark datasets, to provide crucial information about scene geometry where standard methods struggle. Our approach integrates classifier responses into global stereo matching, allowing the final optimization to determine the surface orientation when multiple orientations seem likely. We evaluate our method on challenging real-world datasets, demonstrating improvements for both binocular stereo matching (road scene imagery) and single-view depth estimation (indoor environments).\n\n---TOPIC---\nStereo Matching\n---TOPIC---\nDepth Estimation\n---TOPIC---\nSurface Normals\n---TOPIC---\nMachine Learning Classifiers\n---TOPIC---\nGlobal Optimization",
    "topics": [],
    "references": [
      {
        "citation": "[R. Achanta, A. Shaji, K. Smith, A. Lucchi, P. Fua, and S. Susstrunk, SLIC superpixels compared to state-of-the-art superpixel methods, Transactions on Pattern Analysis and Machine Intelligence (TRAMI), 2012]"
      },
      {
        "citation": "[H. H. Baker and T. O. Binford, Depth from edge and intensity based stereo, International joint conference on Artificial intelligence, 1981]"
      },
      {
        "citation": "[M. Bleyer and M. Gelautz, Simple but effective tree structures for dynamic programming-based stereo matching, International Conference on Computer Vision Theory and Applications (VISAPP), 2008]"
      },
      {
        "citation": "[D. Comaniciu and P. Meer, Mean shift: A robust approach toward feature space analysis, Transactions on Pattern Analysis and Machine Intelligence (TRAMI), 2002]"
      },
      {
        "citation": "[S. Esedoglu and S. J. Osher, Decomposition of images by the anisotropic rudin-osher-fatemi model, Communications on pure and applied mathematics, 2004]"
      },
      {
        "citation": "[P. F. Felzenszwalb and D. P. Huttenlocher, Efﬁcient belief propagation for early vision, International journal of computer vision (IJCV), 2006]"
      },
      {
        "citation": "[Y. Furukawa and J. Ponce, Accurate, dense, and robust multi-view stereopsis, IEEE Transactions on Pattern Analysis and Machine Intelligence (TRAMI), 2010]"
      },
      {
        "citation": "[A. Geiger, P. Lenz, C. Stiller, and R. Urtasun, Vision meets robotics: The kitti dataset, International Journal of Robotics Research (IJRR), 2013]"
      },
      {
        "citation": "[A. Geiger, M. Roser, and R. Urtasun, Efﬁcient large-scale stereo matching, Asian Conference on Computer Vision (ACCV), 2010]"
      },
      {
        "citation": "[C. Häne, N. Savinov, and M. Pollefeys, Class speciﬁc 3d object shape priors using surface normals, IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2014]"
      }
    ],
    "author_details": [
      {
        "name": "Christian Häne",
        "affiliation": "ETH Zürich",
        "email": "christian.haene@inf.ethz.ch"
      },
      {
        "name": "L’ubor Ladický",
        "affiliation": "ETH Zürich",
        "email": "lubor.ladicky@inf.ethz.ch"
      },
      {
        "name": "Marc Pollefeys",
        "affiliation": "ETH Zürich",
        "email": "marc.pollefeys@inf.ethz.ch"
      }
    ]
  },
  {
    "title": "Data-Driven 3D Voxel Patterns for Object Category Recognition\n---AUTHOR---\nYu Xiang\nWongun Choi\nYuanqing Lin\nSilvio Savarese",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Xiang_Data-Driven_3D_Voxel_2015_CVPR_paper.pdf",
    "id": "Xiang_Data-Driven_3D_Voxel_2015_CVPR_paper",
    "abstract": "This paper proposes a novel object representation called 3D Voxel Pattern (3DVP) that jointly encodes key properties of objects including appearance, 3D shape, viewpoint, occlusion, and truncation. The method discovers 3DVPs in a data-driven way and trains specialized detectors for a dictionary of 3DVPs. These detectors can detect objects with specific visibility patterns and transfer metadata to the detected objects, such as 2D segmentation masks, 3D poses, and occlusion/truncation boundaries. This allows for inference of occlusion relationships among objects, improving object recognition results. Experiments on the KITTI detection benchmark and an outdoor-scene dataset demonstrate significant improvements in car detection and pose estimation, along with accurate object segmentation and 3D localization.\n\n---TOPICICS---\n3D Object Recognition\nVoxel Patterns (3DVP)\nObject Pose Estimation\nOcclusion Handling\nData-Driven Object Detection",
    "topics": [],
    "references": [],
    "author_details": [
      {
        "name": "Yu Xiang",
        "affiliation": "University of Michigan at Ann Arbor",
        "email": "yuxiang@umich.edu"
      },
      {
        "name": "Wongun Choi",
        "affiliation": "NEC Laboratories America, Inc.",
        "email": "wongun@nec-labs.com"
      },
      {
        "name": "Yuanqing Lin",
        "affiliation": "NEC Laboratories America, Inc.",
        "email": "ylin@nec-labs.com"
      },
      {
        "name": "Silvio Savarese",
        "affiliation": "Stanford University",
        "email": "ssilvio@stanford.edu"
      }
    ]
  },
  {
    "title": "Functional correspondence by matrix completion\n---AUTHOR---\nArtiom Kovnatsky\nMichael M. Bronstein\nXavier Bresson\nPierre Vandergheynst",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Kovnatsky_Functional_Correspondence_by_2015_CVPR_paper.pdf",
    "id": "Kovnatsky_Functional_Correspondence_by_2015_CVPR_paper",
    "abstract": "In this paper, we consider the problem of finding dense intrinsic correspondence between manifolds in the functional formulation. We pose the functional correspondence problem as matrix completion with manifold geometric structure and inducing functional localization with the L1 norm. We discuss efficient numerical procedures for the solution of our problem. Our method compares favorably to the accuracy of state-of-the-art correspondence algorithms on non-rigid shape matching benchmarks, and is especially advantageous in settings when only scarce data is available.\n\n---TOPIICS---\nFunctional correspondence\nMatrix completion\nManifold geometry\nNon-rigid shape matching\nLaplacian eigenfunctions",
    "topics": [],
    "references": [],
    "author_details": [
      {
        "name": "Artiom Kovnatsky",
        "affiliation": "Faculty of Informatics, Universit`a della Svizzera italiana (USI)",
        "email": "[Email not available in the provided text]"
      },
      {
        "name": "Michael M. Bronstein",
        "affiliation": "Faculty of Informatics, Universit`a della Svizzera italiana (USI)",
        "email": "[Email not available in the provided text]"
      },
      {
        "name": "Xavier Bresson",
        "affiliation": "Signal Processing Lab, ´Ecole Polytechnique F´ed´erale de Lausanne (EPFL)",
        "email": "[Email not available in the provided text]"
      },
      {
        "name": "Pierre Vandergheynst",
        "affiliation": "Signal Processing Lab, ´Ecole Polytechnique F´ed´erale de Lausanne (EPFL)",
        "email": "[Email not available in the provided text]"
      }
    ]
  },
  {
    "title": "Single target tracking using adaptive clustered decision trees and dynamic multi-level appearance models\n---AUTHOR---\nAleˇs Leonardis\nJingjing Xiao\nRustam Stolkin",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Xiao_Single_Target_Tracking_2015_CVPR_paper.pdf",
    "id": "Xiao_Single_Target_Tracking_2015_CVPR_paper",
    "abstract": "This paper presents a method for single target tracking of arbitrary objects in challenging video sequences. Targets are modeled at three different levels of granularity (pixel level, parts-based level and bounding box level), which are cross-constrained to enable robust model relearning. The main contribution is an adaptive clustered decision tree method which dynamically selects the minimum combination of features necessary to sufficiently represent each target part at each frame, thereby providing robustness with computational efﬁciency. The adaptive clustered decision tree is implemented in two separate parts of the tracking algorithm: firstly to enable robust matching at the parts-based level between successive frames; and secondly to select the best superpixels for learning new parts of the target. We have tested the tracker using two different tracking benchmarks (VOT2013-2014 and CVPR2013 tracking challenges), based on two different test methodologies, and show it to be significantly more robust than the best state-of-the-art methods from both of those tracking challenges, while also offering competitive tracking precision.\n\n---TOPIC---\nSingle Target Tracking\nAdaptive Clustered Decision Trees\nMulti-Level Target Models\nRobustness and Efficiency\nSuperpixel Selection",
    "topics": [],
    "references": [
      {
        "citation": "[Seunghoon Hong and Bohyung Han, Visual tracking by sampling tree-structured graphical models, ECCV, 2014]"
      },
      {
        "citation": "[Xu Jia, Huchuan Lu, and Ming-Hsuan Yang, Visual tracking via adaptive structural local sparse appearance model, CVPR, 2012]"
      },
      {
        "citation": "[Radhakrishna Achanta et al., SLIC superpixels compared to state-of-the-art superpixel methods, PAMI, 2012]"
      },
      {
        "citation": "[Boris Babenko et al., Robust object tracking with online multiple instance learning, PAMI, 2011]"
      },
      {
        "citation": "[Junseok Kwon and Kyoung Mu Lee, Highly nonrigid object tracking via patch-based dynamic appearance modeling, PAMI, 2013]"
      },
      {
        "citation": "[Yang Lu et al., Online object tracking, learning and parsing with and-or graphs, CVPR, 2014]"
      },
      {
        "citation": "[Iain Matthews et al., The template update problem, PAMI, 2004]"
      },
      {
        "citation": "[Paul Brasnett et al., Sequential monte carlo tracking by fusing multiple cues in video sequences, Image and Vision Computing, 2007]"
      },
      {
        "citation": "[Xue Mei and Haibin Ling, Robust visual tracking using l1 minimization, ICCV, 2009]"
      },
      {
        "citation": "[Federico Pernici and Alberto Del Bimbo, Object tracking by oversampling local features, PAMI, 2013]"
      }
    ],
    "author_details": [
      {
        "name": "Aleˇs Leonardis",
        "affiliation": "School of Computer Science, University of Birmingham",
        "email": "a.leonardis@cs.bham.ac.uk"
      },
      {
        "name": "Jingjing Xiao",
        "affiliation": "School of EESE, University of Birmingham",
        "email": "Not available"
      },
      {
        "name": "Rustam Stolkin",
        "affiliation": "School of Mechanical Engineering, University of Birmingham",
        "email": "r.stolkin@cs.bham.ac.uk"
      }
    ]
  },
  {
    "title": "Learning an Efﬁcient Model of Hand Shape Variation from Depth Images\n---AUTHOR---\nSameh Khamis\nJonathan Taylor\nJamie Shotton\nCem Keskin\nShahram Izadi\nAndrew Fitzgibbon",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Khamis_Learning_an_Efficient_2015_CVPR_paper.pdf",
    "id": "Khamis_Learning_an_Efficient_2015_CVPR_paper",
    "abstract": "We describe how to learn a compact and efficient model of the surface deformation of human hands. The model is built from a set of noisy depth images of a diverse set of subjects performing different poses with their hands. We represent the observed surface using Loop subdivision of a control mesh that is deformed by our learned parametric shape and pose model. The model simultaneously accounts for variation in subject-specific shape and subject-agnostic pose. Specifically, hand shape is parameterized as a linear combination of a mean mesh in a neutral pose with a small number of offset vectors. This mesh is then articulated using standard linear blend skinning (LBS) to generate the control mesh of a subdivision surface. We deﬁne an energy that encourages each depth pixel to be explained by our model, and the use of a smooth subdivision surface allows us to optimize for all parameters jointly from a rough initialization. The efficacy of our method is demonstrated using both synthetic and real data, where it is shown that hand shape variation can be represented using only a small number of basis components. We compare with other approaches including PCA and show a substantial improvement in the representational power of our model, while maintaining the efficiency of a linear shape basis.\n\n---TOPICAS---\nHand shape modeling\nSurface deformation\nDepth image analysis\nLinear blend skinning (LBS)\nSubdivision surfaces",
    "topics": [],
    "references": [],
    "author_details": [
      {
        "name": "Sameh Khamis",
        "affiliation": "University of Maryland",
        "email": "[Email not available]"
      },
      {
        "name": "Jonathan Taylor",
        "affiliation": "Microsoft Research",
        "email": "[Email not available]"
      },
      {
        "name": "Jamie Shotton",
        "affiliation": "Microsoft Research",
        "email": "[Email not available]"
      },
      {
        "name": "Cem Keskin",
        "affiliation": "Microsoft Research",
        "email": "[Email not available]"
      },
      {
        "name": "Shahram Izadi",
        "affiliation": "Microsoft Research",
        "email": "[Email not available]"
      },
      {
        "name": "Andrew Fitzgibbon",
        "affiliation": "Microsoft Research",
        "email": "[Email not available]"
      }
    ]
  },
  {
    "title": "Correlation Filters with Limited Boundaries\n---AUTHOR---\nHamed Kiani Galoogahi\nTerence Sim\nSimon Lucey",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Galoogahi_Correlation_Filters_With_2015_CVPR_paper.pdf",
    "id": "Galoogahi_Correlation_Filters_With_2015_CVPR_paper",
    "abstract": "Correlation filters take advantage of specific properties in the Fourier domain allowing them to be estimated efficiently: O(ND log D) in the frequency domain, versus O(D3 + ND2) spatially where D is signal length, and N is the number of signals. Recent extensions to correlation filters, such as MOSSE, have reignited interest of their use in the vision community due to their robustness and attractive computational properties. In this paper we demonstrate, however, that this computational efficiency comes at a cost. Specifically, we demonstrate that only 1/D proportion of shifted examples are unaffected by boundary effects which has a dramatic effect on detection/tracking performance. In this paper, we propose a novel approach to correlation filter estimation that: (i) takes advantage of inherent computational redundancies in the frequency domain, (ii) dramatically reduces boundary effects, and (iii) is able to implicitly exploit all possible patches densely extracted from training examples during the learning process. Impressive object tracking and detection results are presented in terms of both accuracy and computational efficiency.",
    "topics": [
      "Correlation Filters",
      "Fourier Domain Analysis",
      "Boundary Effects",
      "Object Tracking",
      "Computational Efficiency"
    ],
    "references": [],
    "author_details": [
      {
        "name": "Hamed Kiani Galoogahi",
        "affiliation": "Istituto Italiano di Tecnologia",
        "email": "hamed.kiani@iit.it"
      },
      {
        "name": "Terence Sim",
        "affiliation": "National University of Singapore",
        "email": "tsim@comp.nus.edu.sg"
      },
      {
        "name": "Simon Lucey",
        "affiliation": "Carnegie Mellon University",
        "email": "slucey@cs.cmu.edu"
      }
    ]
  },
  {
    "title": "Latent Trees for Estimating Intensity of Facial Action Units\n---AUTHOR---\nSebastian Kaltwang\nSinisa Todorovic\nMaja Pantic",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Kaltwang_Latent_Trees_for_2015_CVPR_supplemental.pdf",
    "id": "Kaltwang_Latent_Trees_for_2015_CVPR_supplemental",
    "abstract": "This paper introduces a novel approach for estimating the intensity of Facial Action Units (FAUs) using Latent Trees (LT). The method models the dependencies between landmark locations and FAU intensities by learning a hierarchical structure that captures higher-order relationships. The learned LT structure allows for probabilistic sampling of facial expressions and provides insights into the underlying dependencies among FAUs and landmark locations. The paper details the parameter updates for the distributions within the LT framework and presents qualitative results demonstrating the ability of the model to capture the relationships between FAUs and facial landmarks.\n\n---TOPICCS---\nFacial Action Units (FAUs)\nLatent Trees (LT)\nProbabilistic Modeling\nFacial Expression Analysis\nHierarchical Dependencies",
    "topics": [],
    "references": [],
    "author_details": [
      {
        "name": "Sebastian Kaltwang",
        "affiliation": "Imperial College London",
        "email": "sk2608@imperial.ac.uk"
      },
      {
        "name": "Sinisa Todorovic",
        "affiliation": "Oregon State University",
        "email": "sinisa@eecs.oregonstate.edu"
      },
      {
        "name": "Maja Pantic",
        "affiliation": "Imperial College London",
        "email": "m.pantic@imperial.ac.uk"
      }
    ]
  },
  {
    "title": "Exact Bias Correction and Covariance Estimation for Stereo Vision\n---AUTHORs---\nCharles Freundlich\nMichael Zavlanos\nPhilippos Mordohai",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Freundlich_Exact_Bias_Correction_2015_CVPR_paper.pdf",
    "id": "Freundlich_Exact_Bias_Correction_2015_CVPR_paper",
    "abstract": "We present an approach for correcting the bias in 3D reconstruction of points imaged by a calibrated stereo rig. Our analysis is based on the observation that, due to quantization error, a 3D point reconstructed by triangulation essentially represents an entire region in space. The true location of the world point that generated the triangulated point could be anywhere in this region. We argue that the reconstructed point, if it is to represent this region in space without bias, should be located at the centroid of this region, which is not what has been done in the literature. We derive the exact geometry of these regions in space, which we call 3D cells, and we show how they can be viewed as uniform distributions of possible pre-images of the pair of corresponding pixels. By assuming a uniform distribution of points in 3D, as opposed to a uniform distribution of the projections of these 3D points on the images, we arrive at a fast and exact computation of the triangulation bias in each cell. In addition, we derive the exact covariance matrices of the 3D cells. We validate our approach in a variety of simulations ranging from 3D reconstruction to camera localization and relative motion estimation. In all cases, we are able to demonstrate a marked improvement compared to conventional techniques for small disparity values, for which bias is significant and the required corrections are large.\n\n---TOPIC---\nStereo Vision\nTriangulation Bias\n3D Reconstruction\nCovariance Estimation\nQuantization Error",
    "topics": [],
    "references": [
      {
        "citation": "[Hartley, R., & Sturm, P. (1997). Triangulation. *CVIU*, *68*(2), 146–157.] - Appears to be a foundational reference on triangulation, a core concept in stereo vision."
      },
      {
        "citation": "[Horn, B. (1987). Closed form solutions of absolute orientation using orthonormal matrices. *Journal of the Optical Society of America A*, *5*(7), 1127–1135.] - Provides a closed-form solution for absolute orientation, relevant to aligning stereo views."
      },
      {
        "citation": "[Szeliski, R., & Scharstein, D. (2004). Sampling the disparity space image. *PAMI*, *26*(3), 419–425.] - Discusses the sampling of disparity space, a key aspect of stereo correspondence."
      },
      {
        "citation": "[Matthies, L. H., & Shafer, S. A. (1987). Error modelling in stereo navigation. *IEEE Journal of Robotics and Automation*, *3*(3), 239–250.] - Addresses error modeling in stereo navigation, a crucial consideration for practical applications."
      },
      {
        "citation": "[Geiger, A., Lenz, P., & Urtasun, R. (2012). Are we ready for autonomous driving? the KITTI vision benchmark suite. *CVPR*, pages 3354–3361.] -  The KITTI dataset is a widely used benchmark for evaluating vision algorithms, particularly for autonomous driving."
      },
      {
        "citation": "[Scharstein, D., & Szeliski, R. (2002). A taxonomy and evaluation of dense two-frame stereo correspondence algorithms. *IJCV*, *47*(1-3), 7–42.] - Provides a comprehensive overview and evaluation of stereo correspondence algorithms."
      },
      {
        "citation": "[Bostein, S. D., & Huang, T. S. (1987). Error analysis in stereo determination of 3-d point positions. *PAMI*, *9*(6), 752–766.] -  A foundational paper on error analysis in stereo vision."
      },
      {
        "citation": "[Chang, C. C., Chatterjee, S., & Kube, P. R. (1994). A quantization error analysis for convergent stereo. *ICIP*, pages II: 735–739.] - Focuses on quantization error in stereo vision, a practical concern."
      },
      {
        "citation": "[Fooladgar, F., Samavi, S., Soroushmehr, S., & Shirani, S. (2013). Geometrical analysis of localization error in stereo vision systems. *IEEE Sensors Journal*, *13*(11), 4236–4246.] - Provides a geometrical analysis of localization error in stereo systems."
      },
      {
        "citation": "[Kriegman, D., Triendl, E., & Binford, T. O. (1989). Stereo vision and navigation in buildings for mobile robots. *IEEE Transactions on Robotics and Automation*, *5*(6), 792–803.] - Explores stereo vision for mobile robot navigation."
      }
    ],
    "author_details": [
      {
        "name": "Charles Freundlich",
        "affiliation": "Duke University",
        "email": "charles.freundlich@duke.edu"
      },
      {
        "name": "Michael Zavlanos",
        "affiliation": "Duke University",
        "email": "michael.zavlanos@duke.edu"
      },
      {
        "name": "Philippos Mordohai",
        "affiliation": "Stevens Institute of Technology",
        "email": "mordohai@cs.steverns.edu"
      }
    ]
  },
  {
    "title": "Enriching Object Detection with 2D-3D Registration and Continuous Viewpoint Estimation\n---AUTHOR---\nChristopher Bongsoo Choy\nMichael Stark\nSam Corbett-Davies\nSilvio Savarese",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Choy_Enriching_Object_Detection_2015_CVPR_paper.pdf",
    "id": "Choy_Enriching_Object_Detection_2015_CVPR_paper",
    "abstract": "A large body of recent work on object detection has focused on exploiting 3D CAD model databases to improve detection performance. Many of these approaches work by aligning exact 3D models to images using templates generated from renderings of the 3D models at a set of discrete viewpoints. However, the training procedures for these approaches are computationally expensive and require gigabytes of memory and storage, while the viewpoint discretization hampers pose estimation performance. We propose an efficient method for synthesizing templates from 3D models that runs on the ﬂy – that is, it quickly produces detectors for an arbitrary viewpoint of a 3D model without expensive dataset-dependent training or template storage. Our method synthesizes a discriminative template by extracting features from a rendered view of the object and decorrelating spatial dependencies among the features. We are able to perform joint optimization of scale, translation, continuous rotation, and focal length using a Metropolis-Hastings algorithm. We provide an efficient GPU implementation of our algorithm, and we validate its performance on 3D Object Classes and PAS-CAL3D+ datasets.\n\n---TOPICCS---\n3D Object Detection\nCAD Model Alignment\nContinuous Viewpoint Estimation\nTemplate Synthesis\nGPU Implementation",
    "topics": [],
    "references": [
      {
        "citation": "[M. Aubry, D. Maturana, A. Efros, B. Russell, and J. Sivic. Seeing 3d chairs: exemplar part-based 2d-3d alignment using a large dataset of cad models. In CVPR, 2014.] - Focuses on 2D-3D alignment using CAD models, relevant for 3D object understanding."
      },
      {
        "citation": "[R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In CVPR, 2014.] - Introduces rich feature hierarchies, a significant advancement in object detection and segmentation."
      },
      {
        "citation": "[A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imaginet classification with deep convolutional neural networks. In NIPS, 2012.] - Landmark paper demonstrating the power of deep convolutional neural networks for image classification (ImageNet)."
      },
      {
        "citation": "[S. Savarese and L. Fei-Fei. 3d generic object categorization, localization and pose estimation. In ICCV, 2007.] - Early work on 3D object understanding, covering categorization, localization, and pose estimation."
      },
      {
        "citation": "[B. Hariharan, J. Malik, and D. Ramanan. Discriminative decorrelation for clustering and classification. In ECCV, 2012.] - Addresses decorrelation for improved clustering and classification, a useful technique."
      },
      {
        "citation": "[M. Hejrati and D. Ramanan. Analysis by synthesis: 3d object recognition by object reconstruction. In CVPR, 2014.] - Explores 3D object recognition through object reconstruction."
      },
      {
        "citation": "[A. Kholgade, T. Simon, and Y. Sheikh. 3d object manipulation in a single photograph using stock 3d models. ACM Transactions on Computer Graphics, 2014.] - Deals with 3D object manipulation from a single photograph, a practical application."
      },
      {
        "citation": "[S. Fidler, S. Dickinson, and R. Urtasun. 3d object detection and viewpoint estimation with a deformable 3d cuboid model. In NIPS, 2012.] - Introduces a deformable 3D cuboid model for object detection and viewpoint estimation."
      },
      {
        "citation": "[T. Chen, Z. Zhu, A. Shamir, S.-M. Hu, and D. Cohen-Or. 3sweepp: Extracting editable objects from a single photo. ACM Trans. Graph., 2013.] - Focuses on extracting editable objects from a single photo, a valuable technique for 3D modeling."
      },
      {
        "citation": "[M. Stark, M. Goesele, and B. Schiele. Back to the future: Learning shape models from 3D CAD data. In BMVC, 2010.] - Explores learning shape models from 3D CAD data, relevant for shape understanding."
      }
    ],
    "author_details": [
      {
        "name": "Christopher Bongsoo Choy",
        "affiliation": "Stanford University",
        "email": "chrischoy@stanford.edu"
      },
      {
        "name": "Michael Stark",
        "affiliation": "Max Planck Institute for Informatics",
        "email": "stark@mpi-inf.mpg.de"
      },
      {
        "name": "Sam Corbett-Davies",
        "affiliation": "Stanford University",
        "email": "scorbett@stanford.edu"
      },
      {
        "name": "Silvio Savarese",
        "affiliation": "Stanford University",
        "email": "ssilvio@stanford.edu"
      }
    ]
  },
  {
    "title": "CVPR 2015 Submission #1714\n---AUTHOR---\nAnonymous CVPR submission",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Qi_Making_Better_Use_2015_CVPR_supplemental.pdf",
    "id": "Qi_Making_Better_Use_2015_CVPR_supplemental",
    "abstract": "This paper appears to be a supplementary material document for a CVPR 2015 submission (Paper ID 1714). It presents sketch segmentation results for a variety of objects beyond those shown in the main paper. The results are displayed for each object using four different methods: Original, Continuity, Proximity, Overall, and (Continuity+Proximity). The objects included are: Alarm Clock, Ant, Barn, Bathtub, Bicycle, Cat, Crab, Dog, Fan, Fire Hydrant, Floor Lamp, Flying Saucer, Headphones, Helicopter, Light Bulb, Motorbike, Power Outlet, Rabbit, Sailboat, Snail, and Wineglass.",
    "topics": [
      "Sketch Segmentation",
      "Image Analysis",
      "Computer Vision",
      "Object Recognition",
      "Supplementary Material"
    ],
    "references": [],
    "author_details": []
  },
  {
    "title": "Mining Semantic Affordances of Visual Object Categories\n---AUTHOR---\nYu-Wei Chao\n---AUTHOR---\nZhan Wang\n---AUTHOR---\nRada Mihalcea\n---AUTHOR---\nJia Deng",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Chao_Mining_Semantic_Affordances_2015_CVPR_paper.pdf",
    "id": "Chao_Mining_Semantic_Affordances_2015_CVPR_paper",
    "abstract": "Affordances are fundamental attributes of objects that reveal their functionalities and possible actions. Understanding affordances is crucial for recognizing human activities and enabling robots to interact with the world. This paper introduces the problem of mining semantic affordances: given an object, determining whether an action can be performed on it. The authors introduce a new benchmark with crowd-sourced ground truth affordances on 20 PASPascal VOC object classes and 957 action classes. They explore text mining, visual mining, and collaborative filtering approaches and analyze the most effective ways of collecting knowledge of semantic affordances.\n\n---TOPICICS---\nSemantic Affordances\nAction Recognition\nWordNet\nKnowledge Mining\nHuman-Computer Interaction",
    "topics": [],
    "references": [
      {
        "citation": "[Landauer, T. K., & Dumais, S. T. (1997). A solution to Plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge. *Psychological review*, *104*(2), 211.]"
      },
      {
        "citation": "[Bergsma, S., Lin, D., & Goebel, R. (2008). Discriminative learning of selectional preference from unlabeled text. In *Proc. Conf. on Empirical Methods in Natural Language Processing*.]"
      },
      {
        "citation": "[Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollr, P., & Zitnick, C. (2014). Microsoft coco: Common objects in context. In *Proc. of the European Conf. on Computer Vision*.]"
      },
      {
        "citation": "[Bollacker, K., Evans, C., Paritosh, P., Sturge, T., & Taylor, J. (2008). Freebase: A collaboratively created graph database for structuring human knowledge. In *Proc. of the 2008 ACM SIGMOD International Conf. on Management of Data*.]"
      },
      {
        "citation": "[Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., & Dean, J. (2013). Distributed representations of words and phrases and their compositionality. In *Advances in Neural Information Processing Systems*.]"
      },
      {
        "citation": "[Pedersen, T., Patwardhan, S., & Michelizzi, J. (2004). Wordnet::similarity: Measuring the relatedness of concepts. In *Demonstration Papers at HLT-NAACL 2004*.]"
      },
      {
        "citation": "[Pantel, P., Bhagat, R., Coppola, B., Chklovski, T., & Hovy, E. (2007). ISP: Learning inferential selectional preferences. In *Human Language Technologies 2007: The Conf. of the North American Chapter of the Association for Computational Linguistics*.]"
      },
      {
        "citation": "[Simonyan, K., & Zisserman, A. (2014). Two-stream convolutional networks for action recognition in videos. In *Advances in Neural Information Processing Systems 27*.]"
      },
      {
        "citation": "[Yu, F. X., Cao, L., Feris, R. S., Smith, J. R., & Chang, S.-F. (2013). Designing category-level attributes for discriminative visual recognition. In *Proc. IEEE Conf. on Computer Vision and Pattern Recognition*.]"
      },
      {
        "citation": "[Koehn, P. (2005). Europarl: A Parallel Corpus for Statistical Machine Translation. In *Conference Proceedings: the tenth Machine Translation Summit*.]"
      }
    ],
    "author_details": [
      {
        "name": "Yu-Wei Chao",
        "affiliation": "Computer Science & Engineering, University of Michigan, Ann Arbor",
        "email": "ywchao@umich.edu"
      },
      {
        "name": "Zhan Wang",
        "affiliation": "Computer Science & Engineering, University of Michigan, Ann Arbor",
        "email": "wangzhan@umich.edu"
      },
      {
        "name": "Rada Mihalcea",
        "affiliation": "Computer Science & Engineering, University of Michigan, Ann Arbor",
        "email": "mihalcea@umich.edu"
      },
      {
        "name": "Jia Deng",
        "affiliation": "Computer Science & Engineering, University of Michigan, Ann Arbor",
        "email": "jiadeng@umich.edu"
      }
    ]
  },
  {
    "title": "Robust Image Filtering Using Joint Static and Dynamic Guidance\n---AUTHOR---\nBumsub Ham\nMinsu Cho\nJean Ponce",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Ham_Robust_Image_Filtering_2015_CVPR_paper.pdf",
    "id": "Ham_Robust_Image_Filtering_2015_CVPR_paper",
    "abstract": "This paper addresses a limitation in existing image filtering techniques that rely on guidance signals: the inability to handle structural differences between guidance and input images. The authors propose a novel framework that jointly leverages structural information from both images, formulating image filtering as a nonconvex optimization problem solved using a majorization-minimization algorithm. This approach effectively controls image structures at different scales and is applicable to various data types from different sensors. The authors demonstrate the flexibility and effectiveness of their model through applications like depth super-resolution, scale-space filtering, texture removal, and denoising.",
    "topics": [
      "Image Filtering",
      "Joint Regularization",
      "Static and Dynamic Guidance",
      "Majorization-Minimization Algorithm",
      "Multimodal Data Fusion"
    ],
    "references": [
      {
        "citation": "[P. Arbeláez, M. Maire, C. Fowlkes, and J. Malik. Contour detection and hierarchical image segmentation. IEEE TPAM, 2011.]"
      },
      {
        "citation": "[T. Brox, O. Kleinschmidt, and D. Cremers. Efficient nonlocal means for denoising of textural patterns. IEEE TIP, 2008.]"
      },
      {
        "citation": "[D. Chan, H. Buisman, C. Theobalt, S. Thrun, et al. A noise-aware filter for real-time depth upsampling. in Proc. CVPRW, 2008.]"
      },
      {
        "citation": "[P. Charbonnier, L. Blanc-F´eraud, G. Aubert, and M. Barlaud. Deterministric edge-preserving regularization in computed imaging. IEEE TIP, 1997.]"
      },
      {
        "citation": "[I. Daubechies, R. DeVore, M. Fornasier, and C. S. G¨unt¨urk. Iteratively reweighted least squares minimization for sparse recovery. Communications on Pure and Applied Mathematics, 2010.]"
      },
      {
        "citation": "[F. Durand and J. Dorsey. Fast bilateral filtering for the display of high-dynamic-range images. 2002.]"
      },
      {
        "citation": "[Z. Farbman, R. Fattal, D. Lischinski, and R. Szeliski. Edge-preserving decompositions for multi-scale tone and detail manipulation. ACM TOG, 2008.]"
      },
      {
        "citation": "[D. Ferstl, C. Reinbacher, R. Ranftl, M. R¨uther, and H. Bischof. Image guided depth upsampling using anisotropic total generalized variation. in Proc. ICCV, 2013.]"
      },
      {
        "citation": "[F. R. Hampel, E. M. Ronchetti, P. J. Rousseeuw, and W. A. Stahel. Robust statistics: the approach based on influence functions, volume 114. John Wiley & Sons, 2011.]"
      },
      {
        "citation": "[K. He, J. Sun, and X. Tang. Guided image filtering. in Proc. ECCV, 2010.]"
      }
    ],
    "author_details": [
      {
        "name": "Bumsub Ham",
        "affiliation": "Inria",
        "email": "Not available"
      },
      {
        "name": "Minsu Cho",
        "affiliation": "Inria",
        "email": "Not available"
      },
      {
        "name": "Jean Ponce",
        "affiliation": "École Normale Supérieure / PSL Research University",
        "email": "Not available"
      }
    ]
  },
  {
    "title": "Phase-Based Frame Interpolation for Video\n---AUTHOR---\nSimone Meyer\nOliver Wang\nHenning Zimmer\nMax Grosse\nAlexander Sorkine-Hornung",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Meyer_Phase-Based_Frame_Interpolation_2015_CVPR_paper.pdf",
    "id": "Meyer_Phase-Based_Frame_Interpolation_2015_CVPR_paper",
    "abstract": "Computing interpolated frames in a video sequence traditionally relies on accurate pixel correspondences, often using optical flow, which can be computationally expensive. This paper presents an efficient alternative leveraging phase-based methods that represent motion in the phase shift of individual pixels. The authors introduce a novel, bounded phase shift correction method that combines phase information across multi-scale pyramids and extensions for phase-based image synthesis to improve smoothness. Their approach avoids global optimization, is simple to implement and parallelize, and achieves comparable or superior results to optical flow-based solutions, particularly for high-resolution, high-frame-rate video. The method also gracefully handles difficult interpolation settings and requires a fraction of the computational cost of traditional methods.",
    "topics": [
      "Frame Interpolation",
      "Phase-Based Methods",
      "Optical Flow",
      "Video Processing",
      "Multi-Scale Image Analysis"
    ],
    "references": [
      {
        "citation": "[Lucas, B. D., & Kanade, T. An iterative image registration technique with an application to stereo vision. In *IJCAI*, pages 674–679, 1981.]"
      },
      {
        "citation": "[Horn, B. K. P., & Schunck, B. G. Determining optical ﬂow. *Artof. Intell.*, 17(1-3):185–203, 1981.]"
      },
      {
        "citation": "[Fleet, D. J., & Jepson, A. D. Computation of component image velocity from local phase information. *IJCV*, 5(1):77–104, 1990.]"
      },
      {
        "citation": "[Burt, P. J., & Adelson, E. H. A multiresolution spline with application to image mosaics. *ACM Trans. Graph.*, 2(4):217–236, 1983.]"
      },
      {
        "citation": "[Portilla, J., & Simoncelli, E. P. A parametric texture model based on joint statistics of complex wavelet coefﬁcients. *IJCV*, 40(1):49–70, 2000.]"
      },
      {
        "citation": "[Simoncelli, E. P., & Freeman, W. T. The steerable pyramid: a ﬂexible architecture for multi-scale derivative computation. In *International Conference on Image Processing*, pages 444–447. IEEE, 1995.]"
      },
      {
        "citation": "[Baker, S., Scharstein, D., Lewis, J. P., Roth, S., Black, M. J., & Szeliski, R. A database and evaluation methodology for optical ﬂow. *IJCV*, 92(1):1–31, 2011.]"
      },
      {
        "citation": "[Brox, T., Bruhn, A., Papenberg, N., & Weickert, J. High accuracy optical ﬂow estimation based on a theory for warping. In *ECCV*, pages 25–36, 2004.]"
      },
      {
        "citation": "[Gautama, T., & Hulle, M. M. V. A phase-based approach to the estimation of the optical ﬂow ﬁeld using spatial ﬁltering. *IEEE Transactions on Neural Networks*, 13(5):1127–1136, 2002.]"
      },
      {
        "citation": "[Pérez, P., Gangnet, M., & Blake, A. Poisson image editing. *ACM Trans. Graph.*, 22(3):313–318, 2003.]"
      }
    ],
    "author_details": [
      {
        "name": "Simone Meyer",
        "affiliation": "ETH Zurich",
        "email": "[Email not available]"
      },
      {
        "name": "Oliver Wang",
        "affiliation": "Disney Research Zurich",
        "email": "[Email not available]"
      },
      {
        "name": "Henning Zimmer",
        "affiliation": "Disney Research Zurich",
        "email": "[Email not available]"
      },
      {
        "name": "Max Grosse",
        "affiliation": "Disney Research Zurich",
        "email": "[Email not available]"
      },
      {
        "name": "Alexander Sorkine-Hornung",
        "affiliation": "Disney Research Zurich",
        "email": "[Email not available]"
      }
    ]
  },
  {
    "title": "Prediction of Search Targets From Fixations in Open-World Settings\n---AUTHOR---\nHosnieh Sattar\n---AUTHOR---\nSabine Müller\n---AUTHOR---\nMario Fritz\n---AUTHOR---\nAndreas Bulling",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Sattar_Prediction_of_Search_2015_CVPR_paper.pdf",
    "id": "Sattar_Prediction_of_Search_2015_CVPR_paper",
    "abstract": "Previous work on predicting the target of visual search from human fixations only considered closed-world settings in which training labels are available and predictions are performed for a known set of potential targets. In this work we go beyond the state of the art by studying search target prediction in an open-world setting in which we no longer assume that we have fixation data to train for the search targets. We present a new dataset containing fixation data of 18 users searching for natural images from three image categories within synthesized image collages of about 80 images. In a closed-world baseline experiment we show that we can predict the correct target image out of a candidate set of five images. We then present a new problem formulation for search target prediction in the open-world setting that is based on learning compatibilities between fixations and potential targets.",
    "topics": [
      "Visual search",
      "Human-computer interaction",
      "Open-world learning",
      "Gaze tracking",
      "Image retrieval"
    ],
    "references": [
      {
        "citation": "[Borji, A., & Itti, L. (2014). Defending Yarbus: Eye movements reveal observers’ task. *Journal of Vision*, *14*(3), 29.]"
      },
      {
        "citation": "[Borji, A., Lennartz, A., & Pomplun, M. (2014). What do eyes reveal about the mind?: Algorithmic inference of search targets from fixations. *Neurocomputing*.]"
      },
      {
        "citation": "[Hwang, A. D., Higgins, E. C., & Pomplun, M. (2009). A model of top-down attentional control during visual search in complex scenes. *Journal of Vision*, *9*(5), 25.]"
      },
      {
        "citation": "[Brandt, S. A., & Stark, L. W. (1997). Spontaneous eye movements during visual imagery reflect the content of the visual scene. *Journal of Cognitive Neuroscience*, *9*(1), 27–38.]"
      },
      {
        "citation": "[Bulling, A., & Roggen, D. (2011). Recognition of Visual Memory Recall Processes Using Eye Movement Analysis. *UbiComp*, 455–464.]"
      },
      {
        "citation": "[Klami, A. (2010). Inferring task-relevant image regions from gaze data. *MLSP*, 101–106.]"
      },
      {
        "citation": "[Land, M. F. (2006). Eye movements and the control of actions in everyday life. *Progress in Retinal and Eye Research*, *25*(3), 296–324.]"
      },
      {
        "citation": "[Mast, F. W., & Kosslyn, S. M. (2002). Eye movements during visual mental imagery. *Trends in Cognitive Sciences*, *6*(7), 271–272.]"
      },
      {
        "citation": "[Motter, B. C., & Belky, E. J. (1998). The guidance of eye movements during active visual search. *Vision Research*, *38*(12), 1805–1815.]"
      },
      {
        "citation": "[Oyekoya, O., & Stentiford, F. (2004). Eye tracking as a new interface for image retrieval. *BT Technology Journal*, *22*(3), 161–169.]"
      }
    ],
    "author_details": [
      {
        "name": "Hosnieh Sattar",
        "affiliation": "Perceptual User Interfaces Group, Max Planck Institute for Informatics, Saarbrücken, Germany",
        "email": "sattar@mpi-inf.mpg.de"
      },
      {
        "name": "Sabine Müller",
        "affiliation": "Perceptual User Interfaces Group, Max Planck Institute for Informatics, Saarbrücken, Germany",
        "email": "smueller@mpi-inf.mpg.de"
      },
      {
        "name": "Mario Fritz",
        "affiliation": "Scalable Learning and Perception Group, Max Planck Institute for Informatics, Saarbrücken, Germany",
        "email": "mfritz@mpi-inf.mpg.de"
      },
      {
        "name": "Andreas Bulling",
        "affiliation": "Perceptual User Interfaces Group, Max Planck Institute for Informatics, Saarbrücken, Germany",
        "email": "bulling@mpi-inf.mpg.de"
      }
    ]
  },
  {
    "title": "Global Supervised Descent Method\n---AUTHORs---\nXuehan Xiong\nFernando De la Torre",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Xiong_Global_Supervised_Descent_2015_CVPR_paper.pdf",
    "id": "Xiong_Global_Supervised_Descent_2015_CVPR_paper",
    "abstract": "Mathematical optimization plays a fundamental role in solving many problems in computer vision. Second order descent methods are generally accepted as the most robust, fast, and reliable approaches for nonlinear optimization of a general smooth function. However, in the context of computer vision, these methods have drawbacks related to analytical differentiability and Hessian properties. This paper proposes Global SDM (GSDM), an extension of the Supervised Descent Method (SDM) that divides the search space into regions of similar gradient directions to address these issues. GSDM provides a better and more efficient strategy to minimize non-linear least squares functions in computer vision problems and is illustrated in two problems: non-rigid image alignment and extrinsic camera calibration.\n\n---TOPICCS---\nMathematical Optimization\nSupervised Descent Method (SDM)\nComputer Vision\nNonlinear Least Squares\nGradient Descent Methods",
    "topics": [],
    "references": [
      {
        "citation": "[A. Asthana, S. Zafeiriou, S. Cheng, and M. Pantic, Incremental face alignment in the wild, Computer Vision and Pattern Recognition (CVPR), 2014]"
      },
      {
        "citation": "[G. B. Huang, M. Ramesh, T. Berg, and E. Learned-Miller, Labeled faces in the wild: A database for studying face recognition in unconstrained environments, Technical Report 07-49, University of Massachusetts, Amherst, 4007]"
      },
      {
        "citation": "[T. F. Cootes and C. J. Taylor, A mixture model for representing shape variation, Image and Vision Computing, 17(8):567–573, 1999]"
      },
      {
        "citation": "[T. F. Cootes, C. J. Taylor, D. H. Cooper, and J. Graham, Active shape models-their training and application, Computer vision and image understanding, 61(1):38–59, 1995]"
      },
      {
        "citation": "[M. Kostinger, P. Wohlhart, P. M. Roth, and H. Bischof, Annotated facial landmarks in the wild: A large-scale, real-world database for facial landmark localization, Computer Vision Workshops (ICCV Workshops), 2011]"
      },
      {
        "citation": "[X. Cao, Y. Wei, F. Wen, and J. Sun, Face alignment by explicit shape regression, CVPR, 2012]"
      },
      {
        "citation": "[G. Bradski, The OpenCV Library, Dr. Dobb’s Journal of Software Tools, 2000]"
      },
      {
        "citation": "[C. Sagonas, G. Tzimiropouulos, S. Zafeiriou, and M. Pantic, 300 faces in-the-wild challenge: The first facial landmark localization challenge, Computer Vision Workshops (ICCVW), 2013]"
      },
      {
        "citation": "[D. Lowe, Distinctive image features from scale-invariant keypoints, IJCV, 60(2):91–110, 2004]"
      },
      {
        "citation": "[S. Geman and C. Grafﬁgne, Markov random ﬁeld image models and their applications to computer vision, Proceedings of the International Congress of Mathematicians, 1986]"
      }
    ],
    "author_details": [
      {
        "name": "Xuehan Xiong",
        "affiliation": "Carnegie Mellon University",
        "email": "xxiong@andrew.cmu.edu"
      },
      {
        "name": "Fernando De la Torre",
        "affiliation": "Carnegie Mellon University",
        "email": "ftorre@andrew.cmu.edu"
      }
    ]
  },
  {
    "title": "Exploiting Uncertainty in Regression Forests for Accurate Camera Relocalization\n---AUTHOR---\nJulien Valentin\nM atthias Nießner\nJamie Shotton\nAndrew Fitzgibbon\nShahram Izadi\nPhilip Torr",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Valentin_Exploiting_Uncertainty_in_2015_CVPR_paper.pdf",
    "id": "Valentin_Exploiting_Uncertainty_in_2015_CVPR_paper",
    "abstract": "Recent advances in camera relocalization use predictions from a regression forest to guide the camera pose optimization procedure. In these methods, each tree associates one pixel with a point in the scene’s 3D world coordinate frame. In previous work, these predictions were point estimates and the subsequent camera pose optimization implicitly assumed an isotropic distribution of these estimates. In this paper, we train a regression forest to predict mixtures of anisotropic 3D Gausians and show how the predicted uncertainties can be taken into account for continuous pose optimization. Experiments show that our proposed method is able to relocalize up to 40% more frames than the state of the art.",
    "topics": [
      "Regression Forests",
      "Camera Relocalization",
      "Pose Optimization",
      "Uncertainty Estimation",
      "SLAM (Simultaneous Localization and Mapping)"
    ],
    "references": [
      {
        "citation": "[Shotton, J., et al. Scene coordinate regression forests for camera relocalization in RGB-D images. In Computer Vision and Pattern Recognition. IEEE, 2013.] - This paper directly addresses camera relocalization, a core theme."
      },
      {
        "citation": "[Newcombe, R. A., et al. KinectFusion: Real-time dense surface mapping and tracking. In Mixed and Augmented Reality. IEEE, 2011.] - A seminal work on real-time 3D reconstruction using the Kinect sensor."
      },
      {
        "citation": "[Engel, J., Sch¨ops, T., and Cremers, D. LSD-SLAM: Large-Scale Direct Monocular SLAM. In European Conference on Computer Vision. Springer, 2014.] - A significant contribution to direct monocular SLAM, a key area of research."
      },
      {
        "citation": "[Shotton, J., et al. Real-time human pose recognition in parts from single depth images. Communications of the ACM, 56(1):116–124, 2013.] - Relevant due to the focus on RGB-D data and pose estimation."
      },
      {
        "citation": "[Williams, B., Klein, G., and Reid, I. Real-time S LAM Relocalisation. In International Conference on Computer Vision, pages 1–8. IEEE, 2007.] - Addresses real-time SLAM relocalization, a crucial aspect of the work."
      },
      {
        "citation": "[Newcombe, R. A., Lovegrove, S. J., and Davison, A. J. DTAM: Dense tracking and mapping in real-time. In International Conference on Computer Vision. IEEE, 2011.] - Another important work on real-time dense mapping."
      },
      {
        "citation": "[Glocker, B., Izadi, S., Shotton, J., and Criminisi, A. Real-time RGB-D camera relocalization. In Mixed and Augmented Reality. IEEE, 2013.] - Focuses on RGB-D camera relocalization, directly related to the paper's topic."
      },
      {
        "citation": "[Williams, B., Klein, G., and Reid, I. Automatic relocalization and loop closing for real-time monocular SLAM. Pattern Analysis and Machine Intelligence, 33(9), 2011.] - Deals with automatic relocatization and loop closing in SLAM."
      },
      {
        "citation": "[Cummins, M., and Newman, P. FAB-MAP: Probabilistic localization and mapping in the space of appearance. The International Journal of Robotics Research, 27(6):647–665, 2008.] - Explores probabilistic localization and mapping using appearance features."
      },
      {
        "citation": "[Ni, K., Kannan, A., Criminisi, A., and Winn, J. Epitomic location recognition. In Computer Vision and Pattern Recognition. IEEE, 2008.] - Introduces epitomic location recognition, a technique for localization."
      }
    ],
    "author_details": [
      {
        "name": "Julien Valentin",
        "affiliation": "University of Oxford",
        "email": "[Email not available in text]"
      },
      {
        "name": "Matthias Nießner",
        "affiliation": "Stanford University",
        "email": "[Email not available in text]"
      },
      {
        "name": "Jamie Shotton",
        "affiliation": "Microsoft Research",
        "email": "[Email not available in text]"
      },
      {
        "name": "Andrew Fitzgibbon",
        "affiliation": "Microsoft Research",
        "email": "[Email not available in text]"
      },
      {
        "name": "Shahram Izadi",
        "affiliation": "Microsoft Research",
        "email": "[Email not available in text]"
      },
      {
        "name": "Philip Torr",
        "affiliation": "University of Oxford",
        "email": "[Email not available in text]"
      }
    ]
  },
  {
    "title": "Line-Based Multi-Label Energy Optimization for Fisheye Image Rectification and Calibration - Supplementary Material\n---AUTHORs---\nMi Zhang\nJian Yao\nMenghan Xia\nYi Zhang\nKai Li\nYaping Liu",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Zhang_Line-Based_Multi-Label_Energy_2015_CVPR_supplemental.pdf",
    "id": "Zhang_Line-Based_Multi-Label_Energy_2015_CVPR_supplemental",
    "abstract": "This supplementary material details the Multi-Label Energy Optimization (MLEO) method used for circular arcs selection, as described in the submitted manuscript \"Line-Based Multi-Label Energy Optimization for Fisheye Image Rectification and Calibration.\" It provides a deeper understanding of the relationship between the proposed circular arcs selection function and the MLEO method, presents additional experimental results, and showcases rectified images from both Internet sources and a synthetic approach. The document also includes a comparison of intrinsic parameter estimation results using the proposed method versus an existing approach.",
    "topics": [
      "Fisheye Image Rectification",
      "Multi-Label Energy Optimization (MLEO)",
      "Circular Arc Selection",
      "Intrinsic Parameter Estimation",
      "Graph Cuts"
    ],
    "references": [],
    "author_details": [
      {
        "name": "Mi Zhang",
        "affiliation": "School of Remote Sensing and Information Engineering, Wuhan University, Hubei, China",
        "email": "Not available"
      },
      {
        "name": "Jian Yao",
        "affiliation": "School of Remote Sensing and Information Engineering, Wuhan University, Hubei, China",
        "email": "jian.yao@whu.edu.cn"
      },
      {
        "name": "Menghan Xia",
        "affiliation": "School of Remote Sensing and Information Engineering, Wuhan University, Hubei, China",
        "email": "Not available"
      },
      {
        "name": "Yi Zhang",
        "affiliation": "School of Remote Sensing and Information Engineering, Wuhan University, Hubei, China",
        "email": "Not available"
      },
      {
        "name": "Kai Li",
        "affiliation": "School of Remote Sensing and Information Engineering, Wuhan University, Hubei, China",
        "email": "Not available"
      },
      {
        "name": "Yaping Liu",
        "affiliation": "School of Remote Sensing and Information Engineering, Wuhan University, Hubei, China",
        "email": "Not available"
      }
    ]
  },
  {
    "title": "Fine-Grained Recognition without Part Annotations\n---AUTHOR---\nJonathan Krause\nHailin Jin\nJianchao Yang\nLi Fei-Fei",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Krause_Fine-Grained_Recognition_Without_2015_CVPR_paper.pdf",
    "id": "Krause_Fine-Grained_Recognition_Without_2015_CVPR_paper",
    "abstract": "Scaling up fine-grained recognition to all domains of fine-grained objects is a challenge the computer vision community will need to face in order to realize its goal of recognizing all object categories. Current state-of-the-art techniques rely heavily upon the use of keypoint or part annotations, but scaling up to hundreds or thousands of domains renders this annotation cost-prohibitive for all but the most important categories. In this work we propose a method for fine-grained recognition that uses no part annotations. Our method is based on generating parts using co-segmentation and alignment, which we combine in a discriminative mixture. Experimental results show its efficacy, demonstrating state-of-the-art results even when compared to methods that use part annotations during training.\n\n---TOPICCS---\nFine-Grained Recognition\nPart Annotations\nCo-segmentation\nImage Alignment\nDiscriminative Mixture Models",
    "topics": [],
    "references": [],
    "author_details": [
      {
        "name": "Jonathan Krause",
        "affiliation": "Stanford University",
        "email": "jkrause@cs.stanford.edu"
      },
      {
        "name": "Hailin Jin",
        "affiliation": "Adobe Research",
        "email": "hljin@adobe.com"
      },
      {
        "name": "Jianchao Yang",
        "affiliation": "Adobe Research",
        "email": "jiayang@adobe.com"
      },
      {
        "name": "Li Fei-Fei",
        "affiliation": "Stanford University",
        "email": "feifeili@cs.stanford.edu"
      }
    ]
  },
  {
    "title": "Expanding Object Detector’s HORIZON: Incremental Learning Framework for Object Detection in Videos\n---AUTHOR---\nAlina Kuznetsova\nSung Ju Hwang\nBodo Rosenhahn\nLeonid Sigal",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Kuznetsova_Expanding_Object_Detectors_2015_CVPR_paper.pdf",
    "id": "Kuznetsova_Expanding_Object_Detectors_2015_CVPR_paper",
    "abstract": "Over the last several years it has been shown that image-based object detectors are sensitive to the training data and often fail to generalize to examples that fall outside the original training sample domain (e.g., videos). A number of domain adaptation (DA) techniques have been proposed to address this problem. DA approaches are designed to adapt a fixed complexity model to the new (e.g., video) domain. We posit that unlabeled data should not only allow adaptation, but also improve (or at least maintain) performance on the original and other domains by dynamically adjusting model complexity and parameters. We call this notion domain expansion. To this end, we develop a new scalable and accurate incremental object detection algorithm, based on several extensions of large-margin embedding (LME). Our detection model consists of an embedding space and multiple class prototypes in that embedding space, that represent object classes; distance to those prototypes allows us to reason about multi-class detection. By incrementally detecting object instances in video and adding confident detections into the model, we are able to dynamically adjust the complexity of the detector over time by instantiating new prototypes to span all domains the model has seen. We test performance of our approach by expanding an object detector trained on ImageNet to detect objects in egocentric videos of Activity Daily Living (ADL) dataset and challenging videos from YouTube Objects (YTO) dataset.",
    "topics": [
      "Incremental Learning",
      "Domain Adaptation",
      "Object Detection",
      "Large-Margin Embedding",
      "Video Analysis"
    ],
    "references": [
      {
        "citation": "[21] M. P. Kumar, B. Packer, and D. Koller. Self-paced learning for latent variable models. In NIPS, 2010."
      },
      {
        "citation": "[1] Y. Aytar and A. Zisserman. Tabula rasa: Model transfer for object category detection. In ICCV, 2011."
      },
      {
        "citation": "[22] Y. J. Lee and K. Grauman. Learning the easy things ﬁrst: Self-paced visual category discovery. In CVPR, 2011."
      },
      {
        "citation": "[2] S. Bengio, J. Weston, and D. GranGier. Label embedding trees for large multi-class task. In NIPS, 2010."
      },
      {
        "citation": "[5] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. F.-F. ei. Imagenet: A Large-Scale Hierarchical Image Database. In CVPR, 2009."
      },
      {
        "citation": "[7] M. Everingham, L. V. Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The pascal visual object classes (voc) challenge. IJCV, June 2010."
      },
      {
        "citation": "[8] P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and D. Ramanan. Object detection with discriminatively trained part-based models. TPAMI, Sept. 2010."
      },
      {
        "citation": "[11] A. Gaidon, G. Zen, and J. A. Rodriguez-Serрано. Self-learning camera: Autonomous adaptation of object detectors to unlabeled video streams. In Arxiv, 2014."
      },
      {
        "citation": "[15] J. Hoffman, T. Darrell, and K. Saenko. Continuous manifold based adaptation for evolving visual domains. In CVPR, 2014."
      },
      {
        "citation": "[17] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, and T. Darrell. Caffe: Convolutional architecture for fast feature embedding. Arxiv, 2014."
      }
    ],
    "author_details": [
      {
        "name": "Alina Kuznetsova",
        "affiliation": "Leibniz University Hannover",
        "email": "[Email not available in the provided text]"
      },
      {
        "name": "Sung Ju Hwang",
        "affiliation": "UNIST",
        "email": "[Email not available in the provided text]"
      },
      {
        "name": "Bodo Rosenhahn",
        "affiliation": "Leibniz University Hannover",
        "email": "[Email not available in the provided text]"
      },
      {
        "name": "Leonid Sigal",
        "affiliation": "Disney Research Pittsburgh",
        "email": "[Email not available in the provided text]"
      }
    ]
  },
  {
    "title": "Computing the Stereo Matching Cost with a Convolutional Neural Network\n---AUTHOR---\nJure ˇZbontar\nYann LeCun",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Zbontar_Computing_the_Stereo_2015_CVPR_paper.pdf",
    "id": "Zbontar_Computing_the_Stereo_2015_CVPR_paper",
    "abstract": "We present a method for extracting depth information from a rectiﬁed image pair. We train a convolutional neural network to predict how well two image patches match and use it to compute the stereo matching cost. The cost is refined by cross-based cost aggregation and semiglobal matching, followed by a left-right consistency check to eliminate errors in the occluded regions. Our stereo method achieves an error rate of 2.61 % on the KITTI stereo dataset and is currently (August 2014) the top performing method on this dataset. We describe how a convolutional neural network can be used to compute the stereo matching cost and achieve an error rate of 2.61 % on the KITTI stereo dataset, improving on the previous best result of 2.83 %.\n\n---TOPIC---\nStereo matching\nConvolutional Neural Networks\nDepth estimation\nCost aggregation\nKITTI dataset",
    "topics": [],
    "references": [],
    "author_details": [
      {
        "name": "Jure ˇZbontar",
        "affiliation": "University of Ljubljana",
        "email": "jure.zbontar@fri.uni-lj.si"
      },
      {
        "name": "Yann LeCun",
        "affiliation": "New York University",
        "email": "yann@cs.nyu.edu"
      }
    ]
  },
  {
    "title": "Modeling Local and Global Deformations in Deep Learning: Epitomic Convolution, Multiple Instance Learning, and Sliding Window Detection\n---AUTHOR---\nIasonas Kokkinos\n---AUTHOR---\nPierre-Andr´e Savalle\n---AUTHOR---\nGeorge Papandreou",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Papandreou_Modeling_Local_and_2015_CVPR_paper.pdf",
    "id": "Papandreou_Modeling_Local_and_2015_CVPR_paper",
    "abstract": "Deep Convolutional Neural Networks (DCNNs) achieve invariance to domain transformations (deformations) by using multiple ‘max-pooling’ (MP) layers. In this work we show that alternative methods of modeling deformations can improve the accuracy and efﬁciency of DCNNs. First, we introduce epitomic convolution as an alternative to the common convolution-MP cascade of DCCNs, that comes with the same computational cost but favorable learning properties. Second, we introduce a Multiple Instance Learning algorithm to accommodate global translation and scaling in image classification, yielding an efficient algorithm that trains and tests a DCNN in a consistent manner. Third we develop a DCNN sliding window detector that explicitly, but efficiently, searches over the object’s position, scale, and aspect ratio. We provide competitive image classification and localization results on the ImageNet dataset and object detection results on Pascal VOC2007.\n\n---TOPIC---\nEpitomic Convolution\nMultiple Instance Learning (MIL)\nSliding Window Detection\nDeep Convolutional Neural Networks (DCNNs)\nImage Deformations/Transformations",
    "topics": [],
    "references": [
      {
        "citation": "[Girshick, R., Donahue, J., Darrell, T., & Malik, J. (2014). Rich feature hierarchies for accurate object detection and semantic segmentation. *Proceedings of the IEEE conference on computer vision and pattern recognition*, 719-726.]"
      },
      {
        "citation": "[Krizhevsky, A., Sutskever, I., & Hinton, G. (2013). ImageNet classification with deep convolutional neural networks. *Advances in neural information processing systems*, 1097-1105.]"
      },
      {
        "citation": "[He, K., Zhang, X., Ren, S., & Sun, J. (2015). Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification. *CoRR*, abs/1502.01852.]"
      },
      {
        "citation": "[Girshick, R., Iandola, F., Darrell, T., & Malik, J. (2014). Deformable part models are convolutional neural networks. *arXiv preprint arXiv:1409.5403*.]"
      },
      {
        "citation": "[LeCun, Y., Bottou, L., Bengio, Y., & Haffner, P. (1998). Gradient-based learning applied to document recognition. *IEEE Transactions on Pattern Analysis and Machine Intelligence*, *86*(11), 2278-2324.]"
      },
      {
        "citation": "[Simonyan, K., & Zisserma, A. (2014). Very deep convolutional networks for large-scale image recognition. *arXiv preprint arXiv:1409.1556*.]"
      },
      {
        "citation": "[Deng, J., Dong, W., Socher, R., Li-Jia, L., Li, K., & Fei-Fei, L. (2009). ImageNet: A large-scale hierarchical image database. *Proceedings of the IEEE conference on computer vision and pattern recognition*, 809-816.]"
      },
      {
        "citation": "[Chatfield, K., Simonyan, K., Vedaldi, A., & Zisserma, A. (2014). Return of the devil in the details: Delving deep into convolutional nets. *arXiv preprint arXiv:1404.2685*.]"
      },
      {
        "citation": "[Ioffe, S., & Szegedy, C. (2015). Batch normalization: Accelerating deep network training by reducing internal covariate shift. *arXiv preprint arXiv:1502.03167*.]"
      },
      {
        "citation": "[Long, J., Shelhamer, E., & Darrell, T. (2014). Fully convolutional networks for semantic segmentation. *arXiv preprint arXiv:1411.4038*.]"
      }
    ],
    "author_details": [
      {
        "name": "Iasonas Kokkinos",
        "affiliation": "CentraleSupélec",
        "email": "iasonas.kokkinos@ecp.fr"
      },
      {
        "name": "Pierre-Andr´e Savalle",
        "affiliation": "CentraleSupélec and INRIA",
        "email": "pierre-andre.savalle@ecp.fr"
      },
      {
        "name": "George Papandreou",
        "affiliation": "Google",
        "email": "gapan@google.com"
      }
    ]
  },
  {
    "title": "From Single Image Query to Detailed 3D Reconstruction\n---AUTHOR---\nJohannes L. Sch¨onberger\nFilip Radenov\nOndrej Chum\nJan-Michael Frahm",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Schonberger_From_Single_Image_2015_CVPR_paper.pdf",
    "id": "Schonberger_From_Single_Image_2015_CVPR_paper",
    "abstract": "Structure-from-Motion for unordered image collections has significantly advanced in scale over the last decade. While image retrieval has boosted scalability, it has also limited the ability to reconstruct fine details of the scene. This paper proposes a joint reconstruction and retrieval system that maintains the scalability of large-scale Structure-from-Motion systems while also recovering the ability to reconstruct fine details. The method is demonstrated on a large-scale dataset of 7.4 million images, achieving high surface resolution (order of 1mm in the most photographed areas). The approach combines SfM with retrieval across differently scaled scene images.\n\n---TOPIC---\nStructure-from-Motion (SfM)\nImage Retrieval\n3D Reconstruction\nLarge-Scale Datasets\nDetailed Modeling",
    "topics": [],
    "references": [
      {
        "citation": "[Agarwal, S., Furukawa, Y., Snavely, N., Simon, I., Curless, B., Seitz, S., & Szeliski, R. (2011). Building Rome in a Day. Communications of the ACM.]"
      },
      {
        "citation": "[Arandjelovic, R., & Zisserma, A. (2013). All about VLAD. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.]"
      },
      {
        "citation": "[Chum, O., & Matas, J. (2010). Large-scale discovery of spatially related images. IEEE Transactions on Pattern Analysis and Machine Intelligence.]"
      },
      {
        "citation": "[Chum, O., Philbin, J., Sivic, J., Isard, M., & Zisserman, A. (2007). Total recall: Automatic query expansion with a generative feature model for object retrieval. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.]"
      },
      {
        "citation": "[Fischler, M. A., & Bolles, R. C. (1981). Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography. Communications of the ACM.]"
      },
      {
        "citation": "[Frahm, J., Fite-Georgel, P., Gallup, D., Johnson, T., Raguram, R., Wu, C., Jen, Y., Dunn, E., Clipp, B., Lazebnik, S., & Pollefeys, M. (2010). Building Rome on a Cloudless Day. Proceedings of the European Conference on Computer Vision.]"
      },
      {
        "citation": "[Heinly, J., Dunn, E., & Frahm, J.-M. (2014). Correcting for duplicate scene structure in sparse 3d reconstruction. Proceedings of the European Conference on Computer Vision.]"
      },
      {
        "citation": "[Heinly, J., Sch¨onberger, J. L., Dunn, E., & Frahm, J.-M. (2015). Reconstructing the World* in Six Days *(As Captured by the Yahoo 100 Million Image Dataset). Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.]"
      },
      {
        "citation": "[Irschara, A., Zach, C., Frahm, J.-M., & Bischof, H. (2009). From structure-from-motion point clouds to fast location recognition. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.]"
      },
      {
        "citation": "[Jégou, H., Douze, M., Schmid, C., & Pérez, P. (2010). Aggregating local descriptors into a compact image representation. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.]"
      }
    ],
    "author_details": [
      {
        "name": "Johannes L. Schönberger",
        "affiliation": "Department of Computer Science, The University of North Carolina at Chapel Hill",
        "email": "jscho@cs.unc.edu"
      },
      {
        "name": "Filip Radenović",
        "affiliation": "CMP, Faculty of Electrical Engineering, Czech Technical University in Prague",
        "email": "radenfil@cmp.felk.cvut.cz"
      },
      {
        "name": "Ondrej Chum",
        "affiliation": "CMP, Faculty of Electrical Engineering, Czech Technical University in Prague",
        "email": "chum@cmp.felk.cvut.cz"
      },
      {
        "name": "Jan-Michael Frahim",
        "affiliation": "Department of Computer Science, The University of North Carolina at Chapel Hill",
        "email": "jmf@cs.unc.edu"
      }
    ]
  },
  {
    "title": "Interaction Part Mining: A Mid-Level Approach for Fine-Grained Action Recognition\n---AUTHOR---\nYang Zhou\nBingbing Ni\nRichang Hong\nMeng Wang\nQi Tian",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Zhou_Interaction_Part_Mining_2015_CVPR_paper.pdf",
    "id": "Zhou_Interaction_Part_Mining_2015_CVPR_paper",
    "abstract": "Modeling human-object interactions and manipulating motions lies in the heart of fine-grained action recognition. Previous methods heavily rely on explicit detection of the object being interacted, which requires intensive human labour on object annotation. To bypass this constraint and achieve better classification performance, we propose a novel fine-grained action recognition pipeline by interaction part proposal and discriminative mid-level part mining. Firstly, we generate a large number of candidate object regions using off-the-shelf object proposal tool, e.g., BING. Secondly, these object regions are matched and tracked across frames to form a large spatio-temporal graph. We then propose an efficient approximate graph segmentation algorithm to partition and filter the graph into consistent local dense sub-graphs. These sub-graphs, which are spatio-temporal sub-volumes, represent our candidate interaction parts. Finally, we mine discriminative mid-level part detectors from the features computed over the candidate interaction parts. Bag-of-detection scores based on a novel Max-N pooling scheme are computed as the action representation for a video sample. The proposed framework achieves consistent improvements over the state-of-the-art action recognition accuracies on the benchmarks, without using any object annotation.\n\n---TOPIPS---\nFine-grained action recognition\nHuman-object interaction\nMid-level part mining\nSpatio-temporal graph segmentation\nObject proposal techniques",
    "topics": [],
    "references": [
      {
        "citation": "[Laptev, I., Marszalek, M., & Schmid, C. (2008). Learning realistic human actions from movies. In *CVPR*.] - A seminal work on learning human actions from video."
      },
      {
        "citation": "[Andreev, K., & Racke, H. (2006). Balanced graph partitioning. *Theory of Computing Systems*, 39(6), 929–939.] - Provides background on graph partitioning, potentially relevant to action recognition techniques."
      },
      {
        "citation": "[Gupta, A., & Davis, L. S. (2007). Objects in action: An approach for combining action understanding and object perception. In *CVPR*.] - Explores the relationship between action understanding and object perception."
      },
      {
        "citation": "[Marszalek, M., Laptev, I., & Schmid, C. (2009). Actions in context. In *CVPR*.] - Focuses on understanding actions within their surrounding context."
      },
      {
        "citation": "[Packer, B., Saenko, K., & Koller, D. (2012). A combined pose, object, and feature model for action understanding. In *CVPR*.] - Presents a model combining pose, object, and feature information for action understanding."
      },
      {
        "citation": "[Raptis, M., Kokkinos, I., & Soatto, S. (2012). Discovering discriminative action parts from mid-level video representations. In *CVPR*.] - Focuses on identifying key parts of actions for recognition."
      },
      {
        "citation": "[Lei, J., Ren, X., & Fox, D. (2012). Fine-grained kitchen activity recognition using rgb-d. In *UbiComp*.] - Addresses the specific challenge of recognizing detailed actions in a kitchen setting."
      },
      {
        "citation": "[Malisiewicz, T., Gupta, A., & Efros, A. A. (2011). Ensemble of exemplar-svms for object detection and beyond. In *ICCV*.] - Introduces a method that could be applicable to action recognition."
      },
      {
        "citation": "[Wang, J., Liu, Z., Wu, Y., & Yuan, J. (2012). Mining actionlet ensemble for action recognition with depth cameras. In *CVPR*.] - Explores using depth cameras for action recognition."
      },
      {
        "citation": "[Perronnin, F., S´anchez, J., & Mensink, T. (2010). Improving the fisher kernel for large-scale image classiﬁcation. In *ECCV*.] - Discusses the Fisher kernel, a technique potentially useful for action recognition."
      }
    ],
    "author_details": [
      {
        "name": "Yang Zhou",
        "affiliation": "University of Texas at San Antonio, US",
        "email": "myh511@my.utsa.edu"
      },
      {
        "name": "Bingbing Ni",
        "affiliation": "Advanced Digital Sciences Center, Singapore",
        "email": "bingbing.ni@adsc.com.sg"
      },
      {
        "name": "Richang Hong",
        "affiliation": "HeFei University of Technology, China",
        "email": "hongrc@hfut.edu.cn"
      },
      {
        "name": "Meng Wang",
        "affiliation": "HeFei University of Technology, China",
        "email": "eric.mengwang@gmail.com"
      },
      {
        "name": "Qi Tian",
        "affiliation": "University of Texas at San Antonio, US",
        "email": "qi.tian@utsa.edu"
      }
    ]
  },
  {
    "title": "Evaluation of Output Embeddings for Fine-Grained Image Classiﬁcation\n---AUTHOR---\nZeynep Akata\nScott Reed\nDaniel Walter\nHonglak Lee\nBernt Schiele",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Akata_Evaluation_of_Output_2015_CVPR_paper.pdf",
    "id": "Akata_Evaluation_of_Output_2015_CVPR_paper",
    "abstract": "Image classification has advanced significantly with large-scale image sets. However, fine-grained classification remains a major challenge due to the annotation cost. This project demonstrates that compelling classification performance can be achieved on such categories even without labeled training data. Given image and class embeddings, a compatibility function is learned to assign higher scores to matching embeddings. Zero-shot classification proceeds by finding the label yielding the highest joint compatibility score. The paper focuses on state-of-the-art image features and explores supervised attributes and unsupervised output embeddings derived from hierarchies or learned from unlabeled text corpora. The results establish a substantially improved state-of-the-art on the Animals with Attributes and Caltech-UCSD Birds datasets, with purely unsupervised output embeddings achieving compelling results and outperforming the previous supervised state-of-the-art. Combining different output embeddings further improves results.\n\n---TOPICCS---\nFine-grained Image Classification\nZero-Shot Learning\nOutput Embeddings\nStructured Joint Embedding (SJE)\nUnsupervised Learning",
    "topics": [],
    "references": [
      {
        "citation": "[Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, and T. Darrell. Caffe: Convolutional architecture for fast feature embedding. arXiv preprint arXiv:1408.5093, 2014.]"
      },
      {
        "citation": "[Z. Akata, F. Perronnin, Z. Harchaoui, and C. Schmid. Label embedding for attribute-based classification. In CVPR, 2013.]"
      },
      {
        "citation": "[Z. Akata, F. Perronnin, Z. Harchaoui, and C. Schmid. Label-embedding for image classification. arXiv:1503.08677, 2015.]"
      },
      {
        "citation": "[S. Bengio, J. Weston, and D. GranGier. Label embedding trees for large multi-class tasks. In NIPS, 2010.]"
      },
      {
        "citation": "[Y. Bengio, R. Ducharme, P. Vincent, and C. Janvin. A neural probabilistic language model. JMLR, 2003.]"
      },
      {
        "citation": "[J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imaginet: A large-scale hierarchical image database. In CVPR, 2009.]"
      },
      {
        "citation": "[T. G. Dietterich and G. Bakiri. Solving multiclass learning problems via error-correcting output codes. JAIR, 1995.]"
      },
      {
        "citation": "[A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks. In NIPS, 2012.]"
      },
      {
        "citation": "[A. FarhadI, I. Endres, and D. Hoiem. Attribute-centric recognition for cross-category generalization. In CVPR, 2010.]"
      },
      {
        "citation": "[V. Ferrari and A. Zisserman. Learning visual attributes. In NIPS, 2007.]"
      }
    ],
    "author_details": [
      {
        "name": "Zeynep Akata",
        "affiliation": "Max Planck Institute for Informatics, Saarbrucken, Germany",
        "email": "[Email not available]"
      },
      {
        "name": "Scott Reed",
        "affiliation": "University of Michigan, Ann Arbor",
        "email": "[Email not available]"
      },
      {
        "name": "Daniel Walter",
        "affiliation": "University of Michigan, Ann Arbor",
        "email": "[Email not available]"
      },
      {
        "name": "Honglak Lee",
        "affiliation": "University of Michigan, Ann Arbor",
        "email": "[Email not available]"
      },
      {
        "name": "Bernt Schiele",
        "affiliation": "Max Planck Institute for Informatics, Saarbrucken, Germany",
        "email": "[Email not available]"
      }
    ]
  },
  {
    "title": "Active Pictorial Structures\n---AUTHOR---\nEpameinondas Antonakos\nJoan Alabort-i-Medina\nStefanos Zafeiriou",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Antonakos_Active_Pictorial_Structures_2015_CVPR_paper.pdf",
    "id": "Antonakos_Active_Pictorial_Structures_2015_CVPR_paper",
    "abstract": "In this paper we present a novel generative deformable model motivated by Pictorial Structures (PS) and Active Appearance Models (AAMs) for object alignment in-the-wild. Inspired by the tree structure used in PS, the proposed Active Pictorial Structures (APS) model the appearance of the object using multiple graph-based pairwise normal distributions (Gaussian Markov Random Field) between the patches extracted from the regions around adjacent landmarks. We show that this formulation is more accurate than using a single multivariate distribution (Principal Component Analysis) as commonly done in the literature. APS employ a weighted inverse compositional Gauss-Newton optimization with fixed Jacobian and Hessian that achieves close to real-time performance and state-of-the-art results. Finally, APS have a spring-like graph-based deformation prior term that makes them robust to bad initializations. We present extensive experiments on the task of face alignment, showing that APS outperform current state-of-the-art methods. To the best of our knowledge, the proposed method is the first weighted inverse compositional technique that proves to be so accurate and efficient at the same time.\n\n---TOPIC---\nActive Appearance Models (AAMs)\nPictoial Structures (PS)\nDeformable Models\nGaussian Markov Random Field (GMRF)\nFace Alignment",
    "topics": [],
    "references": [
      {
        "citation": "[Cootes, T. F., Edwards, G. J., & Taylor, C. J. (2001). Active appearance models. *IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)*, *23*(6), 681–685.]"
      },
      {
        "citation": "[Felzenszwalb, P. F., & Huttenlocher, D. P. (2000). Efficient matching of pictorial structures. In *Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, vol. 1, pp. 66–73.]"
      },
      {
        "citation": "[Felzenszwalb, P. F., & Huttenlocher, D. P. (2005). Pictorial structures for object recognition. *International Journal of Computer Vision (IJCV)*, *61*(1), 55–79.]"
      },
      {
        "citation": "[Lowe, D. G. (1999). Object recognition from local scale-invariant features. In *Proceedings of IEEE International Conference on Computer Vision (ICCV)*, vol. 1, pp. 1150–1157.]"
      },
      {
        "citation": "[Felzenszwalb, P. F., Girshick, R. B., McAllester, D., & Ramanan, D. (2010). Object detection with discriminatively trained part-based models. *IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)*, *32*(9), 1627–1645.]"
      },
      {
        "citation": "[Matthews, I., & Baker, S. (2004). Active appearance models revisited. *International Journal of Computer Vision (IJCV)*, *60*(2), 135–164.]"
      },
      {
        "citation": "[Cootes, T. F., Taylor, C. J., Cooper, D. H., & Graham, J. (1995). Active shape models-their training and application. *Computer Vision and Image Understanding*, *61*(1), 38–59.]"
      },
      {
        "citation": "[Andriluka, M., Roth, S., & Schiele, B. (2009). Pictorial structures revisited: People detection and articulated pose estimation. In *Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, pp. 2009.]"
      },
      {
        "citation": "[Rue, H., & Held, L. (2005). *Gaussian Markov random fields: theory and applications*. CRC Press.]"
      },
      {
        "citation": "[Sagonas, C., Tzimiropoulos, G., Zafeiriou, S., & Pantic, M. (2013). 300 faces in-the-wild challenge: The first facial landmark localization challenge. In *Proceedings of IEEE International Conference on Computer Vision Workshopw (ICCV’W)*, pp. 397–403.]"
      }
    ],
    "author_details": [
      {
        "name": "Epameinondas Antonakos",
        "affiliation": "Department of Computing, Imperial College London",
        "email": "e.antonakos@imperial.ac.uk"
      },
      {
        "name": "Joan Alabort-i-Medina",
        "affiliation": "Department of Computing, Imperial College London",
        "email": "ja310@imperial.ac.uk"
      },
      {
        "name": "Stefanos Zafeiriou",
        "affiliation": "Department of Computing, Imperial College London",
        "email": "s.zafeiriou@imperial.ac.uk"
      }
    ]
  },
  {
    "title": "Robust Camera Location Estimation by Convex Programming\n---AUTHOR---\nOnur ¨Ozyes¸il\n---AUTHOR---\nAmit Singer",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Ozyesil_Robust_Camera_Location_2015_CVPR_paper.pdf",
    "id": "Ozyesil_Robust_Camera_Location_2015_CVPR_paper",
    "abstract": "Structure from motion (SfM) aims to recover a 3D structure from a collection of 2D images by estimating camera motion. Existing methods for camera location estimation, a crucial part of SfM, are often sensitive to noise and outliers. This paper provides a complete characterization of well-posed instances of the location estimation problem, relates it to parallel rigidity theory, and introduces a two-step approach for robust camera location estimation. The approach combines a pairwise direction estimation method robust to outliers in point correspondences with a convex program to maintain robustness to outlier directions. The authors demonstrate the effectiveness of their formulation through experiments on Internet photo collections.\n\n---TOPIC---\nStructure from Motion (SfM)\n---TOPIC---\nCamera Location Estimation\n---TOPIC---\nConvex Programming\n---TOPIC---\nRobust Optimization\n---TOPIC---\nParallel Rigidity",
    "topics": [],
    "references": [
      {
        "citation": "Agarwal, S., Snavel, N., Simon, I., Seitz, S. M., & Szeliski, R. (2009). Building Rome in a day. *ICCV*."
      },
      {
        "citation": "Arie-Nachimson, M., Kovalsky, S., Kemelmacher-Shlizerman, I., Singer, A., & Basri, R. (2012). Global motion estimation from point matches. *3DimPVT*."
      },
      {
        "citation": "Aspnes, J., Eren, T., Goldenberg, D. K., Morse, A. S., Whiteley, W., Yang, Y. R., Anderson, B. D. O., & Belhumeur, P. N. (2006). A theory of network localization. *IEEE Transactions on Mobile Computing*."
      },
      {
        "citation": "Bissantz, N., Dmbgen, L., Munk, A., & Stratmann, B. (2009). Convergence analysis of generalized iteratively reweighted least squares algorithms on convex function spaces. *SIAM Journal on Optimization*."
      },
      {
        "citation": "Brand, M., Antone, M., & Teller, S. (2004). Spectral solution of large-scale extrinsic camera calibration as a graph embedding problem. *ECCV*."
      },
      {
        "citation": "Chatterjee, A., & Govindu, V. M. (2013). Efficient and robust large-scale rotation averaging. *ICCV*."
      },
      {
        "citation": "Daubechies, I., DeVore, R., Fornasier, M., & Güntürk, C. S. (2010). Iteratively reweighted least squares minimization for sparse recovery. *Communications on Pure and Applied Mathematics*."
      },
      {
        "citation": "Eren, T., Whiteley, W., & Belhumeur, P. N. (2006). Using angle of arrival (bearing) information in network localization. *IEEE Conference on Decision & Control*."
      },
      {
        "citation": "Furukawa, Y., Curless, B., Seitz, S. M., & Szeliski, R. (2010). Towards Internet-scale multi-view stereo. *CVPR*."
      },
      {
        "citation": "Hartley, R., Aftab, K., & Trumpf, J. (2011). L1 rotation averaging using the Weiszfeld algorithm. *CVPR*."
      }
    ],
    "author_details": [
      {
        "name": "Onur ¨Ozyes¸il",
        "affiliation": "Program in Applied and Computational Mathematics, Princeton University",
        "email": "oozyesil@math.princeton.edu"
      },
      {
        "name": "Amit Singer",
        "affiliation": "Program in Applied and Computational Mathematics, Princeton University",
        "email": "amits@math.princeton.edu"
      }
    ]
  },
  {
    "title": "Learning to Compare Image Patches via Convolutional Neural Networks\n---AUTHOR---\nSergey Zagoruyko\n---AUTHOR---\nNikos Komodakis",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Zagoruyko_Learning_to_Compare_2015_CVPR_supplemental.pdf",
    "id": "Zagoruyko_Learning_to_Compare_2015_CVPR_supplemental",
    "abstract": "This paper introduces and evaluates novel convolutional neural network architectures for learning to compare image patches. The core idea is to learn a similarity metric between patches, enabling tasks such as image retrieval and wide baseline stereo correspondence. The authors explore l2-decision networks and a pseudo-siam network, focusing on both asymmetric and symmetric decision functions. They evaluate their methods on local image patch benchmarks, wide baseline stereo datasets, and local descriptors benchmarks, demonstrating competitive performance against state-of-the-art approaches.",
    "topics": [
      "Convolutional Neural Networks",
      "Image Patch Comparison",
      "Similarity Metric Learning",
      "Wide Baseline Stereo",
      "Local Descriptors"
    ],
    "references": [
      {
        "citation": "[Brown, G. H. M., and S. Winder. Discrimiative learning of local image descriptors. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2010.] - Likely foundational work on descriptor learning."
      },
      {
        "citation": "[Mikolajczyk, K., and C. Schmid. A performance evaluation of local descriptors. IEEE Transactions on Pattern Analysis & Machine Intelligence, 27(10):1615–1630, 2005.] - A key performance evaluation paper, frequently cited in descriptor research."
      },
      {
        "citation": "[Simonyan, K., A. Vedaldi, and A. Zisserman. Learning local feature descriptors using convex optimisation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2014.] - Relevant for descriptor learning techniques."
      },
      {
        "citation": "[Strecha, C., W. von Hansen, L. J. V. Gool, P. Fua, and U. Thoennessen. On benchmarking camera calibration and multi-view stereo for high resolution imagery. In CVPR. IEEE Computer Society, 2008.] - Important for understanding the context of camera calibration and multi-view stereo, potentially related to descriptor application."
      },
      {
        "citation": "[Tola, E., V. Lepetit, and P. Fua. A Fast Local Descriptor for Dense Matching. In Proceedings of Computer Vision and Pattern Recognition, Alaska, USA, 2008.] - Focuses on a fast descriptor, relevant for efficiency considerations."
      },
      {
        "citation": "[Vedaldi, A. Learning Local Feature Descriptors. PhD thesis, University of California, Berkeley, 2009.] - A thesis work on local feature descriptors."
      },
      {
        "citation": "[Schmid, C. Feature Detection and Description. In Computer Vision: Algorithms and Applications, 2011.] - A book chapter on feature detection and description."
      },
      {
        "citation": "[Matas, J., M. Pajdla, P. Xu, and J. Kittler. Robust feature matching using SURF. IEEE Transactions on Pattern Analysis and Machine Intelligence, 28(3):312–324, 2006.] - A work on SURF descriptor."
      },
      {
        "citation": "[LeCun, Y., C. Cortes, and C. Burges. Learning to combine classifiers. In Machine Learning, 2000.] - A work on combining classifiers."
      },
      {
        "citation": "[Sifakis, E., D. Comaniciu, and P. Meer. A universal framework for local descriptor learning. IEEE Transactions on Pattern Analysis and Machine Intelligence, 32(9):1602–1615, 2010.] - A work on local descriptor learning."
      }
    ],
    "author_details": [
      {
        "name": "Sergey Zagoruyko",
        "affiliation": "Ecole des Ponts ParisTech, Universite Paris-Est, France",
        "email": "[Email not available in the provided text]"
      },
      {
        "name": "Nikos Komodakis",
        "affiliation": "Ecole des Ponts ParisTech, Universite Paris-Est, France",
        "email": "[Email not available in the provided text]"
      }
    ]
  },
  {
    "title": "Shape-based Automatic Detection of a Large Number of 3D Facial Landmarks\n---AUTHOR---\nSyed Zulqarnain Gilani\nFaisal Shafait\nAjmal Mian",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Gilani_Shape-Based_Automatic_Detection_2015_CVPR_paper.pdf",
    "id": "Gilani_Shape-Based_Automatic_Detection_2015_CVPR_paper",
    "abstract": "We present an algorithm for automatic detection of a large number of anthropometric landmarks on 3D faces. Our approach does not use texture and is completely shape based in order to detect landmarks that are morphologically significant. The proposed algorithm evolves level set curves with adaptive geometric speed functions to automatically extract effective seed points for dense correspondence. Correspondences are established by minimizing the bending energy between patches around seed points of given faces to those of a reference face. Given its hierarchical structure, our algorithm is capable of establishing thousands of correspondences between a large number of faces. Finally, a morphable model based on the dense corresponding points is fitted to an unseen query face for transfer of correspondences and hence automatic detection of landmarks. The proposed algorithm can detect any number of pre-deﬁned landmarks including subtle landmarks that are even difﬁcult to detect manually. Extensive experimental comparison on two benchmark databases containing 6, 507 scans shows that our algorithm outperforms six state of the art algorithms.\n\n---TOPICICS---\n3D Facial Landmark Detection\nShape-based Algorithms\nLevel Set Curves\nMorphable Models\nAnthropometric Analysis",
    "topics": [],
    "references": [],
    "author_details": [
      {
        "name": "Syed Zulqarnain Gilani",
        "affiliation": "School of Computer Science and Software Engineering, The University of Western Australia",
        "email": "zulqarnain.gilani@uwa.edu.au"
      },
      {
        "name": "Faisal Shafait",
        "affiliation": "School of Computer Science and Software Engineering, The University of Western Australia",
        "email": "faisal.shafait@uwa.edu.au"
      },
      {
        "name": "Ajmal Mian",
        "affiliation": "School of Computer Science and Software Engineering, The University of Western Australia",
        "email": "ajmal.mian@uwa.edu.au"
      }
    ]
  },
  {
    "title": "Deep Convolutional Neural Fields for Depth Estimation from a Single Image\n---AUTHOR---\nFayao Liu\n---AUTHOR---\nChunhua Shen\n---AUTHOR---\nGuosheng Lin",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Liu_Deep_Convolutional_Neural_2015_CVPR_paper.pdf",
    "id": "Liu_Deep_Convolutional_Neural_2015_CVPR_paper",
    "abstract": "This paper addresses the challenging problem of depth estimation from a single monocular image of general scenes. Previous approaches have relied on geometric priors or additional information, often with limitations. The authors propose a novel deep convolutional neural field model that jointly explores the capacity of deep convolutional neural networks (CNNs) and continuous conditional random fields (CRFs). This approach learns unary and pairwise potentials within a unified deep CNN framework, enabling depth estimation without geometric priors or extra information. The method offers efficient solutions for log-likelihood optimization and depth prediction, demonstrating superior performance compared to state-of-the-art methods on both indoor and outdoor datasets.\n\n---TOPICCS---\nDepth Estimation\nConvolutional Neural Networks (CNNs)\nConditional Random Fields (CRFs)\nMonocular Vision\nDeep Learning",
    "topics": [],
    "references": [
      {
        "citation": "[Eigen, D., Puhrsch, C., & Fergus, R. (2014). Depth map prediction from a single image using a multi-scale deep network. *Advances in Neural Information Processing Systems*.] - Frequently cited (appears in the introduction) and relevant to depth estimation."
      },
      {
        "citation": "[Saxena, A., Sun, M., & Ng, A. Y. (2009). Make3d: Learning 3D scene structure from a single still image. *IEEE Transactions on Pattern Analysis and Machine Intelligence*.] - Introduces the Make3D dataset, a key resource mentioned in the paper."
      },
      {
        "citation": "[Lee, D. C., Gupta, A., Hebert, M., & Kanade, T. (2010). Estimating spatial layout of rooms using volumetric reasoning about objects and surfaces. *Advances in Neural Information Processing Systems*.] - Relevant to scene understanding and layout estimation."
      },
      {
        "citation": "[Liu, M., Salzmann, M., & He, X. (2014). Discrete-continuous depth estimation from a single image. *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*.] - Directly addresses depth estimation and is cited in relation to a specific method."
      },
      {
        "citation": "[Saxena, A., Chung, S. H., & Ng, A. Y. (2005). Learning depth from single monocular images. *Advances in Neural Information Processing Systems*.] - A foundational work on learning depth from single images."
      },
      {
        "citation": "[Gupta, A., Efroos, A. A., & Hebert, M. (2010). Blocks world revisited: Image understanding using qualitative geometry and mechanics. *Proceedings of the European Conference on Computer Vision*.] - Relevant to scene understanding and geometric reasoning."
      },
      {
        "citation": "[Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). ImageNet classification with deep convolutional neural networks. *Advances in Neural Information Processing Systems*.] - Introduces AlexNet, a key deep learning architecture."
      },
      {
        "citation": "[Lafferty, J. D., McCallum, A., & Pereira, F. C. N. (2001). Conditional random fields: Probabilistic models for segmenting and labeling sequence data. *Proceedings of the International Conference on Machine Learning*.] - Introduces Conditional Random Fields, a probabilistic modeling technique."
      },
      {
        "citation": "[Baltrusaitis, T., Robinson, P., & Morency, L. (2014). Continuous conditional neural fields for structured regression. *Proceedings of the European Conference on Computer Vision*.] - Introduces a specific method related to structured regression."
      },
      {
        "citation": "[Chatfield, K., Simonyan, K., Vedaldi, A., & Zisserma, A. (2014). Return of the devil in the details: Delving deep into convolutional nets. *Proceedings of the British Machine Vision Conference*.] - Discusses convolutional networks and their details."
      }
    ],
    "author_details": [
      {
        "name": "Fayao Liu",
        "affiliation": "University of Adelaide, Australia; Australian Centre for Robotic Vision",
        "email": "[Email not available in provided text]"
      },
      {
        "name": "Chunhua Shen",
        "affiliation": "University of Adelaide, Australia; Australian Centre for Robotic Vision",
        "email": "[Email not available in provided text]"
      },
      {
        "name": "Guosheng Lin",
        "affiliation": "University of Adelaide, Australia; Australian Centre for Robotic Vision",
        "email": "[Email not available in provided text]"
      }
    ]
  },
  {
    "title": "Automatically Discovering Local Visual Material Attributes\n---AUTHOR---\nGabriel Schwartz\nKo Nishino",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Schwartz_Automatically_Discovering_Local_2015_CVPR_paper.pdf",
    "id": "Schwartz_Automatically_Discovering_Local_2015_CVPR_paper",
    "abstract": "The paper addresses the challenge of recognizing materials in images without relying on object cues like shape. Current methods often conflate material recognition with object recognition, hindering the use of material information for broader scene understanding tasks. The authors propose a framework to discover locally recognizable material attributes from crowd-sourced perceptual material distances, demonstrating that these discovered attributes effectively separate material categories and exhibit desirable properties despite being learned with only partial supervision. This approach aims to enable material recognition as a standalone task, independent of object shape.\n\n---TOPICIS---\nMaterial Recognition\nCrowdsourcing\nVisual Attributes\nPartial Supervision\nLocal Image Features",
    "topics": [],
    "references": [
      {
        "citation": "[E. H. Adelson. On Seeing Stuff: The Perception of Materials by Humans and Machines. In SPIE, pages 1–12, 2001.] - This is a foundational paper by Adelson, a key figure in material perception research."
      },
      {
        "citation": "[L. Sharan, C. Liu, R. Rosenholtz, and E. H. Adelson. Recognizing Materials Using Perceptually Inspired Features. International Journal of Computer Vision, 2013.] - A direct follow-up to Adelson's work, focusing on feature engineering for material recognition."
      },
      {
        "citation": "[R. W. Fleming, R. O. Dror, and E. H. Adelson. Real-world Illumination and the Perception of Surface Reﬂectance Properties. Journal of Vision, 3(5):347–368, 2003.] - Addresses the impact of illumination on material perception, a critical aspect of the problem."
      },
      {
        "citation": "[V. Ferrari and A. Zisserman. Learning Visual Attributes. In NIPS, pages 433–440, 2007.] - A seminal work on learning visual attributes, a core concept in the paper's area."
      },
      {
        "citation": "[A. Farhadi, I. Endres, D. Hoiem, and D. Forsyth. Describing Objects by their Attributes. In CVPR, pages 1778–1785, 2009.] - Explores methods for describing objects using attributes, a key technique."
      },
      {
        "citation": "[J. Wills, S. Agarwal, D. Kriegman, and S. Belongie. Toward a Perceptual Space for Gloss. ACM Transactions on Graphics, 28(103):1–15, 2009.] - Focuses on a specific material property (gloss) and attempts to define a perceptual space for it."
      },
      {
        "citation": "[D. Lowe. Object recognition from local scale-invariant features. In ICCV, pages 1150–1157, 1999.] - Introduces SIFT features, a widely used technique for object recognition and feature extraction."
      },
      {
        "citation": "[G. Patterson and J. Hays. SUN Attribute Database: Discovering, Annotating, and Recognizing Scene At-tribues. In CVPR, 2012.] - Introduces a large-scale dataset for attribute recognition, crucial for training and evaluating models."
      },
      {
        "citation": "[M. Rastegari, A. Farhadi, and D. Forsyth. Attribute Discovery via Predictable Discrimina-tive Binary Codes. In ECCV, pages 876–889, 2012.] - Explores a method for attribute discovery using binary codes."
      },
      {
        "citation": "[L. van der Maaten and G. Hinton. Visualizing Data using t-SNE. JMLR, 9:2579–2605, 2008.] - Provides a technique for visualizing high-dimensional data, potentially useful for understanding material representations."
      },
      {
        "citation": "---"
      }
    ],
    "author_details": [
      {
        "name": "Gabriel Schwartz",
        "affiliation": "Department of Computer Science, Drexel University",
        "email": "gbs25@drexel.edu"
      },
      {
        "name": "Ko Nishino",
        "affiliation": "Department of Computer Science, Drexel University",
        "email": "kon@drexel.edu"
      }
    ]
  },
  {
    "title": "Multi-Feature Max-Margin Hierarchical Bayesian Model for Action Recognition\n---AUTHOR---\nShuang Yang\nChunfeng Yuan\nBaoxin Wu\nWeiming Hu\nFangshi Wang",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Yang_Multi-Feature_Max-Margin_Hierarchical_2015_CVPR_paper.pdf",
    "id": "Yang_Multi-Feature_Max-Margin_Hierarchical_2015_CVPR_paper",
    "abstract": "In this paper, a multi-feature max-margin hierarchical Bayesian model (M3HBM) is proposed for action recognition. Different from existing methods which separate representation and classification into two steps, M3HBM jointly learns a high-level representation by combining a hierarchical generative model (HGM) and discriminative max-margin classifiers in a unified Bayesian framework. Specifically, HGM is proposed to represent actions by distributions over latent spatial temporal patterns (STPs) which are learned from multiple feature modalities and shared among different classes. For recognition, we employ Gibbs classifiers to minimize the expected loss function based on the max-margin principle and use the classifiers as regularization terms of M3HBM to perform Bayeisan estimation for classifier parameters together with the learning of STPs. In addition, multi-task learning is applied to learn the model from multiple feature modalities for different classes. For test videos, we obtain the representations by the inference process and perform action recognition by the learned Gibbs classifiers. For the learning and inference process, we derive an efficient collapsed Gibbs sampling algorithm to solve the proposed M3HBM. Extensive experiments on several datasets demonstrate both the representation power and the classification capability of our approach for action recognition.\n\n---TOPIICS---\nAction Recognition\nHierarchical Generative Models (HGM)\nMax-Margin Classifiers\nBayesian Framework\nSpatial Temporal Patterns (STPs)\nMulti-Task Learning",
    "topics": [],
    "references": [],
    "author_details": [
      {
        "name": "Shuang Yang",
        "affiliation": "NLPR, Institution of Automation, CAS",
        "email": "syang@nlpr.ia.ac.cn"
      },
      {
        "name": "Chunfeng Yuan",
        "affiliation": "NLPR, Institution of Automation, CAS",
        "email": "cfyuan@nlpr.ia.ac.cn"
      },
      {
        "name": "Baoxin Wu",
        "affiliation": "NLPR, Institution of Automation, CAS",
        "email": "bxwu@nlpr.ia.ac.cn"
      },
      {
        "name": "Weiming Hu",
        "affiliation": "NLPR, Institution of Automation, CAS",
        "email": "wmhu@nlpr.ia.ac.cn"
      },
      {
        "name": "Fangshi Wang",
        "affiliation": "Beijing Jiaotong University",
        "email": "fshwang@bjtu.edu.cn"
      }
    ]
  },
  {
    "title": "Towards Open World Recognition\n---AUTHORs---\nAbhijit Bendale\nTerrence Boult",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Bendale_Towards_Open_World_2015_CVPR_paper.pdf",
    "id": "Bendale_Towards_Open_World_2015_CVPR_paper",
    "abstract": "Despite the dynamic and open nature of the real world, most recognition systems operate under a static, closed-world assumption. This paper formalizes and presents steps towards the problem of open world recognition, addressing operational issues such as continuously detecting novel categories, dealing with unseen categories at prediction time, and minimizing downtime. The authors prove a theoretical balance between \"open space risk\" and empirical risk, introduce the Nearest Non-Outlier (NNO) algorithm for efficient, incremental model evolution, and present an evaluation protocol. Experiments on the ImageNet dataset demonstrate the effectiveness of NNO on large-scale visual recognition tasks.\n\n---TOPICCS---\nOpen World Recognition\nNovelty Detection\nIncremental Learning\nNearest Non-Outlier (NNO) Algorithm\nImageNet Dataset",
    "topics": [],
    "references": [
      {
        "citation": "[Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). ImageNet classification with deep convolutional neural networks. *Advances in neural information processing systems*, *25*, 1097–1105.]"
      },
      {
        "citation": "[Berg, A., Deng, J., & Fei-Fei, L. (2010). Large scale visual recognition challenge, 2010. http://image-net.org/challenges/LSVRC/2010/index[Online; accessed 1-Nov-2013].]"
      },
      {
        "citation": "[Fan, R.-E., Chang, K.-W., Hsieh, C.-J., Wang, X.-R., & Lin, C.-J. (2008). Liblinear: A library for large linear classification. *The Journal of Machine Learning Research*, *9*, 1871–1874.]"
      },
      {
        "citation": "[Datta, P., & Kibler, D. (1997). Symbolic nearest mean classifiers. *AAAI/IAAI*, 82–87.]"
      },
      {
        "citation": "[Loog, M. (2010). Constrained parameter estimation for semi-supervised learning: the case of the nearest mean classifier. *Machine Learning and Knowledge Discovery in Databases*, 291–304.]"
      },
      {
        "citation": "[Simonyan, K., & Zisserman, A. (2014). Very deep convolutional networks for large-scale image recognition. *arXiv preprint arXiv:1409.1155*.]"
      },
      {
        "citation": "[Everingham, M., Van Gool, L., Williams, C. K., Winn, J., & Zisserman, A. (2010). The PASUAL Visual Object Classes (VOC) challenge. *International Journal of Computer Vision (IJCV)*, *88*(2), 303–338.]"
      },
      {
        "citation": "[Markou, M., & Singh, S. (2003). Novelty detection: a reviewpart 1: statistical approaches. *Signal processing*, *83*(12), 2481–2497.]"
      },
      {
        "citation": "[Scheirer, W. J., Rocha, A., Sapkota, A., & Bult, T. E. (2013). Probability models for open set recognition. *IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI)*, *36*(11), 2317–2324.]"
      },
      {
        "citation": "[Perronnin, F., Aakata, Z., Harchaoui, Z., & Schmid, C. (2012). Towards good practice in large-scale learning for image classification. *Computer Vision and Pattern Recognition (CVPR)*, 3482–3489.]"
      },
      {
        "citation": "---"
      }
    ],
    "author_details": [
      {
        "name": "Abhijit Bendale",
        "affiliation": "University of Colorado at Colorado Springs",
        "email": "abendale@vast.uccs.edu"
      },
      {
        "name": "Terrence Boult",
        "affiliation": "University of Colorado at Colorado Springs",
        "email": "tboult@vast.uccs.edu"
      }
    ]
  },
  {
    "title": "[No title found in the provided text]\n---AUTHORSHIP---\n[No authors found in the provided text]",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Chen_Matrix_Completion_for_2015_CVPR_paper.pdf",
    "id": "Chen_Matrix_Completion_for_2015_CVPR_paper",
    "topics": [
      "Labeling Error Rate",
      "Test Error Rate",
      "Ambiguity in Data",
      "Machine Learning Algorithms (Naive, CLPL, DLHD, DLSD, MCar, MCar-SVM)",
      "Objective Function Optimization (Iterations)",
      "**Observations based on the data:**",
      "*   The graphs seem to compare the performance (error rates) of different machine learning algorithms (Naive, CLPL, DLHD, DLSD, MCar, MCar-SVM) under varying conditions.",
      "*   The conditions appear to involve the \"portion of ambiguously labeled samples,\" the \"degree of ambiguity,\" and the \"number of extra labels.\"",
      "*   The \"Normalized Objective Value\" graph likely represents the optimization process of one or more of the algorithms."
    ],
    "abstract": "Abstract not found",
    "references": [],
    "author_details": []
  },
  {
    "title": "SOLD: Sub-Optimal Low-rank Decomposition for Eﬃcient Video Segmentation\n---AUTHOR---\nChenglong Li\nLiang Lin\nWangmeng Zuo\nShuicheng Yan\nJin Tang",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Li_SOLD_Sub-Optimal_Low-rank_2015_CVPR_supplemental.pdf",
    "id": "Li_SOLD_Sub-Optimal_Low-rank_2015_CVPR_supplemental",
    "abstract": "This paper introduces SOLD (Sub-Optimal Low-rank Decomposition), a novel approach for efficient video segmentation. SOLD formulates the problem as minimizing a low-rank matrix decomposition objective function, incorporating an L1 regularization term for sparsity. To address the non-convex nature of the objective, SOLD employs an alternating optimization method, deriving closed-form solutions for the subproblems of A, B, and E. This results in a computationally efficient algorithm suitable for video segmentation tasks.\n\n---TOPSICS---\nLow-rank decomposition\nVideo segmentation\nAlternating optimization\nSparse regularization (L1 regularization)\nComputational efficiency",
    "topics": [],
    "references": [
      {
        "citation": "[Lin, Z., Ganesh, A., Wright, J., Chen, M., Wu, L., & Ma, Y. Fast convex optimization algorithms for exact recovery of a corrupted low-rank matrix. UIUC Technical Report UILU-ENG-09-2214, July 2009.]"
      }
    ],
    "author_details": [
      {
        "name": "Chenglong Li",
        "affiliation": "School of Computer Science and Technology, Anhui University",
        "email": "lcl1314@foxmail.com"
      },
      {
        "name": "Liang Lin",
        "affiliation": "School of Advanced Computing, Sun Yat-sen University",
        "email": "linliang@ieee.org"
      },
      {
        "name": "Wangmeng Zuo",
        "affiliation": "School of Computer Science and Technology, Harbin Institute of Technology",
        "email": "{cswmzuo}@gmail.com"
      },
      {
        "name": "Shuicheng Yan",
        "affiliation": "Department of ECE, National University of Singapore",
        "email": "eleyans@nus.edu.sg"
      },
      {
        "name": "Jin Tang",
        "affiliation": "School of Computer Science and Technology, Anhui University",
        "email": "ahhftang@gmail.com"
      }
    ]
  },
  {
    "title": "Convolutional Feature Masking for Joint Object and Stuff Segmentation\n---AUTHORISTS---\nJifeng Dai\nKaiming He\nJian Sun",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Dai_Convolutional_Feature_Masking_2015_CVPR_paper.pdf",
    "id": "Dai_Convolutional_Feature_Masking_2015_CVPR_paper",
    "abstract": "Semantic segmentation aims to label each image pixel to a semantic category. Current leading approaches exploit shape information by extracting CNN features from masked image regions. This strategy introduces artificial boundaries and may impact the quality of extracted features. Furthermore, these methods require computing networks on thousands of raw image regions, which is time-consuming. This paper proposes to exploit shape information via masking convolutional features. Segments (e.g., super-pixels) are treated as masks on the convolutional feature maps. The CNN features of segments are directly masked out from these maps and used to train classifiers for recognition. The method is further generalized to handle objects and “stuff” (e.g., grass, sky, water) in the same framework. State-of-the-art results are demonstrated on benchmarks, with a compelling computational speed.",
    "topics": [
      "Semantic Segmentation",
      "Convolutional Neural Networks (CNNs)",
      "Feature Masking",
      "Object and Stuff Segmentation",
      "Region Proposals"
    ],
    "references": [
      {
        "citation": "[K. He, X. Zhang, S. Ren, and J. Sun. Spatial pyramid pooling in deep convolutional networks for visual recognition. arXiv preprint arXiv:1406.4729, 2014.] - Introduces Spatial Pyramid Pooling, a technique for handling multi-scale object recognition."
      },
      {
        "citation": "[A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imaginet classification with deep convolutional neural networks. In NIPS, 2012.] - A seminal paper introducing AlexNet and demonstrating the power of deep convolutional neural networks for large-scale image classification."
      },
      {
        "citation": "[J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imaginet: A large-scale hierarchical image database. In CVPR, 2009.] - Describes ImageNet, a crucial dataset for training and evaluating image recognition models."
      },
      {
        "citation": "[R. Mottaghi, X. Chen, X. Liu, N.-G. Cho, S.-W. Lee, S. Fidler, R. Urtasun, and A. Yuille. The role of context for object detection and semantic segmentation in the wild. In CVPR. 2014.] - Addresses the importance of contextual information for robust object detection and segmentation."
      },
      {
        "citation": "[J. Long, E. Shelhamer, and T. Darrell. Fully convolutional networks for semantic segmentation. arXiv:1411.4038, 2014.] - Introduces Fully Convolutional Networks (FCNs), a key architecture for semantic segmentation."
      },
      {
        "citation": "[M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zisserman. The pascal visual object classes (voc) challenge. IJCV, 2010.] - Describes the PASCAL VOC challenge, a benchmark for object detection and segmentation."
      },
      {
        "citation": "[Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel. Backpropagation applied to handwritten zip code recognition. Neural computation, 1989.] - A foundational paper on backpropagation, a core algorithm for training neural networks."
      },
      {
        "citation": "[J. R. Uijlings, K. E. van de Sande, T. Gevers, and A. W. Smeulders. Selective search for object recognition. IJCV, 2013.] - Presents Selective Search, a method for generating region proposals for object detection."
      },
      {
        "citation": "[K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.] - Explores the benefits of very deep convolutional networks for image recognition."
      }
    ],
    "author_details": [
      {
        "name": "Jifeng Dai",
        "affiliation": "Microsoft Research",
        "email": "jifdai@microsoft.com"
      },
      {
        "name": "Kaiming He",
        "affiliation": "Microsoft Research",
        "email": "kahe@microsoft.com"
      },
      {
        "name": "Jian Sun",
        "affiliation": "Microsoft Research",
        "email": "jiansun@microsoft.com"
      }
    ]
  },
  {
    "title": "Visual Recognition by Counting Instances: A Multi-Instance Cardinality Potential Kernel\n---AUTHOR---\nHossein Hajimirsadeghi\nWang Yan\nArash Vahdat\nGreg Mori",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Hajimirsadeghi_Visual_Recognition_by_2015_CVPR_supplemental.pdf",
    "id": "Hajimirsadeghi_Visual_Recognition_by_2015_CVPR_supplemental",
    "abstract": "This paper presents a novel \"Visual Recognition by Counting Instances: A Multi-Instance Cardinality Potential Kernel\" method. The work focuses on the computational complexity and parameter setting guidelines for this kernel. The paper analyzes the time complexity of computing the kernel and training the associated model, detailing the impact of various parameters and initialization strategies. Furthermore, it provides experimental results on several datasets, including TRECVID MED11 and a collective activity recognition dataset, comparing the proposed method with existing approaches. The paper also discusses practical considerations for efficient parameter tuning and initialization of the cardinality model.\n\n---TOPIC---\nMulti-Instance Learning\nCardinality Kernel\nComputational Complexity\nParameter Optimization\nVisual Recognition",
    "topics": [],
    "references": [
      {
        "citation": "[1] M. R. Amer, P. Lei, and S. Todorovic. Hirf: Hierarchical ran-dom ﬁeld for collective activity recognition in videos. In European Conference on Computer Vision (ECCV), pages 572–585. Springer, 2014. (Cited for splitting methodology)"
      },
      {
        "citation": "[2] (Mentioned in text, but details not provided - likely relevant for splitting methodology)"
      },
      {
        "citation": "[3] (Mentioned in text, but details not provided - likely relevant for splitting methodology)"
      },
      {
        "citation": "[4] (Mentioned in text, but details not provided - likely relevant for splitting methodology)"
      },
      {
        "citation": "[9] (Crucially cited for the experimental splitting methodology used.)"
      },
      {
        "citation": "[11] (Not explicitly mentioned, but likely a foundational work given the context of activity recognition)"
      },
      {
        "citation": "[12] (Not explicitly mentioned, but likely a foundational work given the context of activity recognition)"
      },
      {
        "citation": "[13] (Not explicitly mentioned, but likely a foundational work given the context of activity recognition)"
      },
      {
        "citation": "[14] (Not explicitly mentioned, but likely a foundational work given the context of activity recognition)"
      },
      {
        "citation": "[15] (Not explicitly mentioned, but likely a foundational work given the context of activity recognition)"
      }
    ],
    "author_details": [
      {
        "name": "Hossein Hajimirsadeghi",
        "affiliation": "School of Computing Science, Simon Fraser University, Canada",
        "email": "hosseinh@sfu.ca"
      },
      {
        "name": "Wang Yan",
        "affiliation": "School of Computing Science, Simon Fraser University, Canada",
        "email": "wyan@sfu.ca"
      },
      {
        "name": "Arash Vahdat",
        "affiliation": "School of Computing Science, Simon Fraser University, Canada",
        "email": "avahdat@sfu.ca"
      },
      {
        "name": "Greg Mori",
        "affiliation": "School of Computing Science, Simon Fraser University, Canada",
        "email": "mori@cs.sfu.ca"
      }
    ]
  },
  {
    "title": "Joint SFM and Detection Cues for Monocular 3D Localization in Road Scenes\n---AUTHOR---\nShiyu Song\nManmohan Chandraker",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Song_Joint_SFM_and_2015_CVPR_paper.pdf",
    "id": "Song_Joint_SFM_and_2015_CVPR_paper",
    "abstract": "We present a system for fast and highly accurate 3D localization of objects like cars in autonomous driving applications, using a single camera. Our localization framework jointly uses information from complementary modalities such as structure from motion (SFM) and object detection to achieve high localization accuracy in both near and far fields. This is in contrast to prior works that rely purely on detector outputs, or motion segmentation based on sparse feature tracks. Rather than completely commit to tracklets generated by a 2D tracker, we make novel use of raw detection scores to allow our 3D bounding boxes to adapt to better quality 3D cues. To extract SFM cues, we demonstrate the advantages of dense tracking over sparse mechanisms in autonomous driving scenarios. In contrast to complex scene understanding, our formulation for 3D localization is efficient and can be regarded as an extension of sparse bundle adjustment to incorporate object detection cues. Experiments on the KITTI dataset show the efficacy of our cues, as well as the accuracy and robustness of our 3D object localization relative to ground truth and prior works.\n\n---TOPICCS---\n3D Object Localization\nStructure from Motion (SFM)\nObject Detection\nAutonomous Driving\nJoint Optimization",
    "topics": [],
    "references": [
      {
        "citation": "[Brox, T., Bregler, C., & Malik, J. (2009). Large displacement optical flow. *Computer Vision and Pattern Recognition*.] - Cited 3 times, likely foundational for optical flow techniques."
      },
      {
        "citation": "[Brox, T., Rosenhahn, B., Gall, J., & Cremers, D. (2010). Combined region and motion-based 3D tracking of rigid and articulated objects. *Pattern Analysis and Machine Intelligence*.] - Cited 2 times, relevant to tracking."
      },
      {
        "citation": "[Choi, W., & Savarese, S. (2010). Multi-target tracking in world coordinate with single, minimally calibrated camera. *European Conference on Computer Vision*.] - Cited 4 times, important for multi-target tracking."
      },
      {
        "citation": "[Felzenszwalb, P. F., Girshick, R. B., McAllester, D., & Ramanan, D. (2010). Object detection with discriminatively trained part-based models. *Pattern Analysis and Machine Intelligence*.] - Cited 2 times, relevant to object detection."
      },
      {
        "citation": "[Geiger, A., Lauer, M., Wojek, C., Stiller, C., & Urtasun, R. (2014). 3D traffic scene understanding from movable platforms. *Pattern Analysis and Machine Intelligence*.] - Cited 4 times, key for traffic scene understanding."
      },
      {
        "citation": "[Geiger, A., Lenz, P., & Urtasun, R. (2012). Are we ready for autonomous driving? The KITTI vision benchmark suite. *Computer Vision and Pattern Recognition*.] - Cited 2 times, important for autonomous driving context."
      },
      {
        "citation": "[Schindler, K., U, J., & Wang, H. (2006). Perspective n-view multibody structure-and-motion through model selection. *European Conference on Computer Vision*.] - Cited 2 times, relevant to structure-and-motion."
      },
      {
        "citation": "[Slesareva, N., Bruhn, A., & Weickert, J. (2005). Optic flow goes stereo: A variational method for estimating discontinuity-preserving dense disparity maps. *Pattern Recognition*.] - Cited 3 times, important for stereo vision and optical flow."
      },
      {
        "citation": "[Song, S., & Chandraker, M. (2014). Robust scale estimation in real-time monocular SFM for autonomous driving. *Computer Vision and Pattern Recognition*.] - Cited 3 times, relevant to scale estimation and autonomous driving."
      }
    ],
    "author_details": [
      {
        "name": "Shiyu Song",
        "affiliation": "NEC Labs America",
        "email": "[Email not available in the provided text]"
      },
      {
        "name": "Manmohan Chandraker",
        "affiliation": "NEC Labs America",
        "email": "[Email not available in the provided text]"
      }
    ]
  },
  {
    "title": "DeepID-Net: Deformable Deep Convolutional Neural Networks for Object Detection\n---AUTHOR---\nWanli Ouyang\nXiaogang Wang\nXingyu Zeng\nShi Qiu\nPing Luo\nYonglong Tian\nHongsheng Li\nShuo Yang\nZhe Wang\nChen-Change Loy\nXiaoou Tang",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Ouyang_DeepID-Net_Deformable_Deep_2015_CVPR_paper.pdf",
    "id": "Ouyang_DeepID-Net_Deformable_Deep_2015_CVPR_paper",
    "abstract": "In this paper, we propose deformable deep convolutional neural networks for generic object detection. This new deep learning object detection framework has innovations in multiple aspects. A new deformation constrained pooling (def-pooling) layer models the deformation of object parts with geometric constraint and penalty. A new pre-training strategy is proposed to learn feature representations more suitable for the object detection task and with good generalization capability. By changing the net structures, training strategies, adding and removing some key components in the detection pipeline, a set of models with large diversity are obtained, which significantly improves the effectiveness of model averaging. The proposed approach improves the mean averaged precision obtained by RCNN [14], which was the state-of-the-art, from 31% to 50.3% on the ILSVRC2014 detection test set. It also outperforms the winner of ILSVRC2014, GoogLeNet, by 6.1%. Detailed component-wise analysis is also provided through extensive experimental evaluation.\n\n---TOPICICS---\nDeep Convolutional Neural Networks\nObject Detection\nDeformable Pooling Layers\nPre-training Strategies\nGeometric Constraints",
    "topics": [],
    "references": [
      {
        "citation": "[Krizhevsky, A., Sutskever, I., & Hinton, G. (2012). ImageNet classification with deep convolutional neural networks. *NIPS*.]"
      },
      {
        "citation": "[Girshick, R., Donahue, J., Darrell, T., & Malik, J. (2014). Rich feature hierarchies for accurate object detection and semantic segmentation. *CVPR*.]"
      },
      {
        "citation": "[Felzenszwalb, P. F., & Huttenlocher, D. P. (2005). Pictorial structures for object recognition. *IJCV*, *61*(1), 55–79.]"
      },
      {
        "citation": "[He, K., Zhang, X., Ren, S., & Sun, J. (2014). Spatial pyramid pooling in deep convolutional networks for visual recognition. *ECCV*.]"
      },
      {
        "citation": "[Girshick, R., Felzenszwalb, P., & McAllester, D. (2014). Deformable part models are convolutional neural networks. *arXiv preprint arXiv:1409.5403*.]"
      },
      {
        "citation": "[Donahue, J., Jia, Y., Vinyals, O., Hoffman, J., Zhang, N., Tzeng, E., & Darrell, T. (2014). Decaf: A deep convolutional activation feature for generic visual recognition. *ICML*, 647–655.]"
      },
      {
        "citation": "[Felzenszwalb, P., Grishick, R., McAllister, D., & Ramanan, D. (2010). Object detection with discriminatively trained part based models. *IEEE Trans. PAMI*, *32*(1), 1627–1645.]"
      },
      {
        "citation": "[Dalal, N., & Triggs, B. (2005). Histograms of oriented gradients for human detection. *CVPR*.]"
      },
      {
        "citation": "[Barinova, O., Lempitsky, V., & Kohli, P. (2010). On detection of multiple object instances using Hough transforms. *CVPR*.]"
      },
      {
        "citation": "[Simonyan, K., Vedaldi, A., & Zisserma, A. (2014). Deep inside convolutional networks: Visualising image classification models and saliency maps. *ICLR*.]"
      }
    ],
    "author_details": [
      {
        "name": "Wanli Ouyang",
        "affiliation": "The Chinese University of Hong Kong",
        "email": "wlouyang@ee.cuhk.edu.hk"
      },
      {
        "name": "Xiaogang Wang",
        "affiliation": "The Chinese University of Hong Kong",
        "email": "xgwang@ee.cuhk.edu.hk"
      },
      {
        "name": "Xingyu Zeng",
        "affiliation": "The Chinese University of Hong Kong",
        "email": "N/A"
      },
      {
        "name": "Shi Qiu",
        "affiliation": "The Chinese University of Hong Kong",
        "email": "N/A"
      },
      {
        "name": "Ping Luo",
        "affiliation": "The Chinese University of Hong Kong",
        "email": "N/A"
      },
      {
        "name": "Yonglong Tian",
        "affiliation": "The Chinese University of Hong Kong",
        "email": "N/A"
      },
      {
        "name": "Hongsheng Li",
        "affiliation": "The Chinese University of Hong Kong",
        "email": "N/A"
      },
      {
        "name": "Shuo Yang",
        "affiliation": "The Chinese University of Hong Kong",
        "email": "N/A"
      },
      {
        "name": "Zhe Wang",
        "affiliation": "The Chinese University of Hong Kong",
        "email": "N/A"
      },
      {
        "name": "Chen-Change Loy",
        "affiliation": "The Chinese University of Hong Kong",
        "email": "N/A"
      },
      {
        "name": "Xiaoou Tang",
        "affiliation": "The Chinese University of Hong Kong",
        "email": "N/A"
      }
    ]
  },
  {
    "title": "Understanding Pedestrian Behavior from Stationary Crowd Groups\n---AUTHORs---\nShuai Yi\nHongsheng Li\nXiaogang Wang",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Yi_Understanding_Pedestrian_Behaviors_2015_CVPR_paper.pdf",
    "id": "Yi_Understanding_Pedestrian_Behaviors_2015_CVPR_paper",
    "abstract": "Pedestrian behavior modeling and analysis is important for crowd scene understanding and has various applications in video surveillance. Stationary crowd groups are a key factor influencing pedestrian walking patterns but were largely ignored in literature. This paper proposes a novel model for pedestrian behavior modeling by including stationary crowd groups as a key component. Through inference on the interactions between stationary crowd groups and pedestrians, the model can be used to investigate pedestrian behaviors. The effectiveness of the proposed model is demonstrated through applications including walking path prediction, destination prediction, personality classification, and abnormal event detection. A large pedestrian walking route dataset is built and will be released publicly. The paper also introduces the factor of stationary crowd groups for the first time to model pedestrian behaviors, allowing for dynamic updates to adapt to changes in crowd group locations.\n\n---TOPIC---\nPedestrian Behavior Modeling\nCrowd Scene Understanding\nStationary Crowd Groups\nWalking Path Prediction\nDataset Creation",
    "topics": [],
    "references": [
      {
        "citation": "[5] E. Bonabeau. Agent-based modeling: Methods and techniques for simulating human systems. Proceedings of the National Academy of Sciences of the United States of America, 99(Suppl 3):7280–7287, 2002."
      },
      {
        "citation": "[6] M.-C. Chang, N. Krahnstoever, and W. Ge. Probablistic group-level motion analysis and scenario recognition. In Proc. ICCV. IEEE, 2011."
      },
      {
        "citation": "[9] W. Ge, R. T. Collins, and R. B. Ruback. Vision-based analysis of small groups in pedestrian crowds. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 34(5):1003–1016, 2012."
      },
      {
        "citation": "[11] D. Helbing and P. Molnar. Social force model for pedestrian dynamics. Physical review E, 51(5):4282, 1995."
      },
      {
        "citation": "[21] G. Le Bon. The crowd: A study of the popular mind. Macmillian, 1897."
      },
      {
        "citation": "[33] J. Shao, K. Kang, C. C. Loy, and X. Wang. Deeply learned attributes for crowded scene understanding. In Proc. CVPR. IEEE, 2015."
      },
      {
        "citation": "[34] J. Shao, C. C. Loy, and X. Wang. Scene independent group profiling in crowd. In Proc. CVPR. IEEE, 2014."
      },
      {
        "citation": "[42] B. Zhou, X. Tang, and X. Wang. Learning collective crowd behaviors with dynamic pedestrian-agents. International Journal of Computer Vision, 111(1):50–68, 2015."
      },
      {
        "citation": "[44] B. Zhou, X. Wang, and X. Tang. Random field topic model for semantic region analysis in crowded scenes from tracklets. In Proc. CVPR. IEEE, 2011."
      },
      {
        "citation": "[3] S. Ali and M. Shah. Floor fields for tracking in high density crowd scenes. In Proc. ECCV. Springer, 2008."
      }
    ],
    "author_details": [
      {
        "name": "Shuai Yi",
        "affiliation": "Department of Electronic Engineering, The Chinese University of Hong Kong",
        "email": "syi@ee.cuhk.edu.hk"
      },
      {
        "name": "Hongsheng Li",
        "affiliation": "Department of Electronic Engineering, The Chinese University of Hong Kong",
        "email": "lihongsheng@gmail.com"
      },
      {
        "name": "Xiaogang Wang",
        "affiliation": "Department of Electronic Engineering, The Chinese University of Hong Kong",
        "email": "xgwang@ee.cuhk.edu.hk"
      }
    ]
  },
  {
    "title": "Multiple Random Walkers and Their Application to Image Cosegmentation\n---AUTHOR---\nChang-Su Kim\nJae-Young Sim\nChulwoo Lee\nWon-Dong Jang",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Lee_Multiple_Random_Walkers_2015_CVPR_paper.pdf",
    "id": "Lee_Multiple_Random_Walkers_2015_CVPR_paper",
    "abstract": "A graph-based system to simulate the movements and interactions of multiple random walkers (MRW) is proposed. In the MRW system, multiple agents traverse a single graph simultaneously. To achieve desired interactions among those agents, a restart rule can be designed, which determines the restart distribution of each agent according to the probability distributions of all agents. In particular, we develop the repulsive rule for data clustering. We illustrate that the MRW clustering can segment real images reliably. Furthermore, we propose a novel image cosegmentation algorithm based on the MRW clustering. Specifically, the proposed algorithm consists of two steps: inter-image concurrence computation and intra-image MRW clustering. Experimental results demonstrate that the proposed algorithm provides promising cosegmentation performance.",
    "topics": [
      "Multiple Random Walkers (MRW)",
      "Image Cosegmentation",
      "Graph-based Clustering",
      "Repulsive Restart Rule",
      "Concurrency Distribution"
    ],
    "references": [],
    "author_details": [
      {
        "name": "Chang-Su Kim",
        "affiliation": "School of Electrical Engineering, Korea University",
        "email": "changsukim@korea.ac.kr"
      },
      {
        "name": "Jae-Young Sim",
        "affiliation": "School of ECE, UNIST",
        "email": "jysim@unist.ac.kr"
      },
      {
        "name": "Chulwoo Lee",
        "affiliation": "School of Electrical Engineering, Korea University",
        "email": "{chulwoo}@mcl.korea.ac.kr"
      },
      {
        "name": "Won-Dong Jang",
        "affiliation": "School of Electrical Engineering, Korea University",
        "email": "{wdjang}@mcl.korea.ac.kr"
      }
    ]
  },
  {
    "title": "Social Saliency Prediction\n---AUTHOR---\nHyun Soo Park\n---AUTHOR---\nJianbo Shi",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Park_Social_Saliency_Prediction_2015_CVPR_paper.pdf",
    "id": "Park_Social_Saliency_Prediction_2015_CVPR_paper",
    "abstract": "This paper presents a method to predict social saliency, the likelihood of joint attention, given an input image or video by leveraging social interaction data captured by first person cameras. Inspired by electric dipole moments, the authors introduce a social formation feature that encodes the geometric relationship between joint attention and its social formation. They learn this feature from first person social interaction data and train an ensemble classifier to learn the geometric relationship. The trained classifier is then used to predict social saliency in real-world scenes with multiple social groups. The representation does not require directional measurements such as gaze directions. A geometric analysis of social interactions in terms of the F-formation theory is also presented.\n\n---TOPICCS---\nSocial Salience\nJoint Attention\nGeometric Representation\nFirst-Person Cameras\nSocial Interaction Data",
    "topics": [],
    "references": [
      {
        "citation": "[Alahi, A., Ramanathan, V., & Fei-Fei, L. (2014). Socially-aware large-scale crowd forecasting. *CVPR*.] - Referenced for socially-aware aspects."
      },
      {
        "citation": "[Park, H. S., Jain, E., & Sheikh, Y. (2012). 3D social salience from head-mounted cameras. *NIPS*.] - Referenced for 3D social salience."
      },
      {
        "citation": "[Rehg, J. M., Abowd, G. D., Rozga, A., Romero, M., Clements, M. A., Sclaroff, S., ... & Ye, Z. (2013). Decoding children’s social behavior. *CVPR*.] - Referenced for analyzing social behavior."
      },
      {
        "citation": "[Kendon, A. (1990). *Conducting Interaction: Patterns of Behavior in Focused Encouters*. Cambridge University Press.] - Referenced for understanding interaction patterns."
      },
      {
        "citation": "[Freund, Y., & Schapire, R. E. (1999). A short introduction to boosting.] - Referenced for boosting techniques."
      },
      {
        "citation": "[Mar´ın-Jim´enez, M., Zisserman, A., Eichner, M., & Ferrari, V. (2014). Detecting people looking at each other in videos. *IJCV*.] - Referenced for detecting mutual gaze."
      },
      {
        "citation": "[Lowe, D. G. (2004). Distinctive image features from scale-invariant keypoints. *IJCV*.] - Referenced for feature extraction."
      },
      {
        "citation": "[Chakraborty, I., Cheng, H., & Javed, O. (2013). 3D visual proximics: Recognizing human interactions in 3D from a single image. *CVPR*.] - Referenced for 3D proxemics."
      },
      {
        "citation": "[Ousley, O. Y., Arriaga, R., Abowd, G. D., & Morrier, M. (2012). Rapid assessment of social-communicative abilities in infants at risk for autism. Technical Report, Georgia Tech.] - Referenced for assessing social-communicative abilities."
      },
      {
        "citation": "[Yang, Y., & Ramanan, D. (2011). Articulated pose estimation using flexible mixtures of parts. *CVPR*.] - Referenced for articulated pose estimation."
      },
      {
        "citation": "---"
      }
    ],
    "author_details": [
      {
        "name": "Hyun Soo Park",
        "affiliation": "University of Pennsylvania",
        "email": "hypar@seas.upenn.edu"
      },
      {
        "name": "Jianbo Shi",
        "affiliation": "University of Pennsylvania",
        "email": "jshi@seas.upenn.edu"
      }
    ]
  },
  {
    "title": "Web Scale Photo Hash Clustering on A Single Machine\n---AUTHOR---\nYunchao Gong\n---AUTHOR---\nMarcin Pawlowski\n---AUTHOR---\nFei Yang\n---AUTHOR---\nLouis Brandy\n---AUTHOR---\nLubomir Boundev\n---AUTHOR---\nRob Fergus",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Gong_Web_Scale_Photo_2015_CVPR_paper.pdf",
    "id": "Gong_Web_Scale_Photo_2015_CVPR_paper",
    "abstract": "This paper addresses the problem of clustering a very large number of photos (hundreds of millions a day) into millions of clusters. To address this, the authors propose to cluster the binary hash codes of photos into binary cluster centers. They present a fast binary k-means algorithm that works directly on the similarity-preserving hashes of images and clusters them into binary centers. The proposed method is capable of clustering millions of photos on a single machine in a few minutes, demonstrating significantly faster performance than standard k-means with comparable accuracy. An online clustering method based on binary k-means is also proposed, with applications to spam detection and trending photo discovery.\n\n---TOPIC---\nImage Clustering\n---TOPI---\nBinary K-Means\n---TOPI---\nHash Codes\n---TOPI---\nOnline Clustering\n---TOPI---\nPhoto Sharing Websites",
    "topics": [],
    "references": [
      {
        "citation": "[Frahm, J.-M., Georgel, P., Gallup, D., Johnson, T., Raguram, R., Wu, C., Jen, Y.-H., Dunn, E., Clipp, B., Lazebnik, S., & Pollefeys, M. (2010). Building Rome on a cloudless day. ECCV.]"
      },
      {
        "citation": "[Agarwal, S., Furukawa, Y., Snavel, N., Simon, I., Curless, B., Seitz, S. M., & Szeliski, R. (2011). Building rome in a day. Commun. ACM.]"
      },
      {
        "citation": "[Gong, Y., Lazebnik, S., Gordo, A., & Perronnin, F. (2012). Iterative quantization: A Procustean approach to learning binary codes for large-scale image retrieval. PAMI.]"
      },
      {
        "citation": "[Ge, T., He, K., Ke, Q., & Sun, J. (2014). Optimized product quantization. PAMI.]"
      },
      {
        "citation": "[Barbakh, W., & Fyfe, C. (2008). Online clustering algorithms. JMLR Workshops.]"
      },
      {
        "citation": "[Kim, G., & Hebert, M. (2008). Unsupervised modeling of object categories using link analysis techniques. CVPR.]"
      },
      {
        "citation": "[Chum, O., & Matas, J. (2008). Web scale image clustering–large scale discovery of spatially related images. Research Report.]"
      },
      {
        "citation": "[He, K., Wen, F., & Sun, J. (2013). K-means hashing: an affinity-preserving quantization method for learning binary compact codes. CVPR.]"
      },
      {
        "citation": "[Jain, A. K., & Dubes, R. C. (1988). Algorithms for Clustering Data. Prentice-Hall, Inc.]"
      },
      {
        "citation": "[Jégou, H., Douze, M., & Schmid, C. (2011). Product quantization for nearest neighbor search. IEEE TPAM.]"
      }
    ],
    "author_details": [
      {
        "name": "Yunchau Gong",
        "affiliation": "Facebook",
        "email": "[Not available in the provided text]"
      },
      {
        "name": "Marcin Pawlowski",
        "affiliation": "Facebook",
        "email": "[Not available in the provided text]"
      },
      {
        "name": "Fei Yang",
        "affiliation": "Facebook",
        "email": "[Not available in the provided text]"
      },
      {
        "name": "Louis Brandy",
        "affiliation": "Facebook",
        "email": "[Not available in the provided text]"
      },
      {
        "name": "Lubomir Boundev",
        "affiliation": "Facebook",
        "email": "[Not available in the provided text]"
      },
      {
        "name": "Rob Fergus",
        "affiliation": "Facebook",
        "email": "[Not available in the provided text]"
      }
    ]
  },
  {
    "title": "Appearance-Based Gaze Estimation in the Wild\n---AUTHOR---\nXucong Zhang\nYusuke Sugano\nMario Fritz\nAndreas Bulling",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Zhang_Appearance-Based_Gaze_Estimation_2015_CVPR_paper.pdf",
    "id": "Zhang_Appearance-Based_Gaze_Estimation_2015_CVPR_paper",
    "abstract": "Appearance-based gaze estimation is believed to work well in real-world settings, but existing datasets have been collected under controlled laboratory conditions and methods have not been evaluated across multiple datasets. This work studies appearance-based gaze estimation in the wild. The authors present the MPIIGaze dataset, containing 213,659 images collected from 15 participants during natural everyday laptop use over more than three months. This dataset is significantly more variable than existing ones with respect to appearance and illumination. A method for in-the-wild appearance-based gaze estimation using multimodal convolutional neural networks is also presented, which significantly outperforms state-of-the-art methods in cross-dataset evaluation. The paper includes an extensive evaluation of several algorithms on three datasets, including MPIIGaze, to identify key research challenges.",
    "topics": [
      "Gaze Estimation",
      "Appearance-Based Methods",
      "Datasets (MPIIGaze)",
      "Multimodal Convolutional Neural Networks",
      "In-the-Wild Evaluation"
    ],
    "references": [
      {
        "citation": "[F. Alnajar, T. Gevers, R. Valenti, and S. Ghebreab, Calibration-free gaze estimation using human gaze patterns, Proc. ICCV, 2013]"
      },
      {
        "citation": "[T. Baltruˇsaitis, P. Robinson, and L.-P. Morency, Continuous conditional neural ﬁelds for structured regression, Proc. ECCV, 2014]"
      },
      {
        "citation": "[S. Baluja and D. Pomerleau, Non-intrusive gaze tracking using artiﬁcial neural networks, Technical report, DTIC Document, 1994]"
      },
      {
        "citation": "[J. Chen and Q. Ji, 3d gaze estimation with a single camera without ir illumination, Proc. ICPR, 2008]"
      },
      {
        "citation": "[J. Chen and Q. Ji, Probabilistic gaze estimation without active personal calibration, Proc. CVPR, 2011]"
      },
      {
        "citation": "[J. Choi, B. Ahn, J. Parl, and I. S. Kweon, Appearance-based gaze estimation using kinect, Proc. URAI, 2013]"
      },
      {
        "citation": "[R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-J. Lin, Liblinear: A library for large linear classiﬁcation, The Journal of Machine Learning Research, 2008]"
      },
      {
        "citation": "[K. A. Funes Mora, F. Monay, and J.-M. Odobez, Eyediap: A database for the development and evaluation of gaze estimation algorithms from rgb and rgb-d cameras, Proc. ETRA, 2014]"
      },
      {
        "citation": "[K. A. Funes Mora and J.-M. Odobez, Gaze estimation from multimodal kinect data, Proc. CVPRW, 2012]"
      },
      {
        "citation": "[K. A. Funes Mora and J.-M. Odobez, Person independent 3d gaze estimation from remote rgb-d cameras, Proc. ICIP, 2013]"
      }
    ],
    "author_details": [
      {
        "name": "Xucong Zhang",
        "affiliation": "Perceptual User Interfaces Group, Max Planck Institute for Informatics, Saarbr¨ucken, Germany",
        "email": "xczhang@mpi-inf.mpg.de"
      },
      {
        "name": "Yusuke Sugano",
        "affiliation": "Perceptual User Interfaces Group, Max Planck Institute for Informatics, Saarbr¨ucken, Germany",
        "email": "sugano@mpi-inf.mpg.de"
      },
      {
        "name": "Mario Fritz",
        "affiliation": "Scalable Learning and Perception Group, Max Planck Institute for Informatics, Saarbr¨ucken, Germany",
        "email": "mfritz@mpi-inf.mpg.de"
      },
      {
        "name": "Andreas Bulling",
        "affiliation": "Perceptual User Interfaces Group, Max Planck Institute for Informatics, Saarbr¨ucken, Germany",
        "email": "bulling@mpi-inf.mpg.de"
      }
    ]
  },
  {
    "title": "A Large-Scale Car Dataset for Fine-Grained Categorization and Veriﬁcation\n---AUTHOR---\nLinjie Yang\nPing Luo\nChen Change Loy\nXiaoou Tang",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Yang_A_Large-Scale_Car_2015_CVPR_paper.pdf",
    "id": "Yang_A_Large-Scale_Car_2015_CVPR_paper",
    "abstract": "This paper aims to highlight vision related tasks centered around “car”, which has been largely neglected by the vision community in comparison to other objects. We show that there are still many interesting car-related problems and applications, which are not yet well explored and researched. To facilitate future car-related research, in this paper we present our on-going effort in collecting a large-scale dataset, “CompCars”, that covers not only different car views, but also their different internal and external parts, and rich attributes. Importantly, the dataset is constructed with a cross-modality nature, containing a surveillance-nature set and a web-nature set. We further demonstrate a few important applications exploiting the dataset, namely car model classification, car model verification, and attribute prediction. We also discuss specific challenges of the car-related problems and other potential applications that worth further investigations. The latest dataset can be downloaded at http://mmlab.ie.cuhk.edu.hk/datasets/comp_cars/index.html\n\n---TOPICCS---\nCar-related vision tasks\nLarge-scale dataset (CompCars)\nCar model classification\nCar model verification\nAttribute prediction",
    "topics": [],
    "references": [
      {
        "citation": "[Bourdev, L., Maji, S., and Malik, J. Describing people: Poselet-based attribute classiﬁcation. ICCV, 2011.]"
      },
      {
        "citation": "[Deng, J., Dong, W., Socher, R., Li-Jia, L., Li, K., and Fei-Fei, L. Imaginet: A large-scale hierarchical image database. CVPR, 2009.]"
      },
      {
        "citation": "[Girshick, R., Donahue, J., Darrell, T., and Malik, J. Rich feature hierarchies for accurate object detection and semantic segmentation. CVPR, 2014.]"
      },
      {
        "citation": "[He, K., Sigal, L., and Sclaroff, S. Parameterizing object detectors in the continuous pose space. ECCV, 2014.]"
      },
      {
        "citation": "[Hsiao, E., Sinha, S. N., Ramnath, K., Baker, S., Zitnick, L., and Szeliski, R. Car make and model recognition using 3d curve alignment. IEEE Winter Conference on Applications of Computer Vision, 2014.]"
      },
      {
        "citation": "[Krizhevsky, A., Sutskever, I., and Hinton, G. E. Imaginet classification with deep convolutional neural networks. NIPS, 2012.]"
      },
      {
        "citation": "[LeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W., and Jackel, L. D. Backpropagation applied to handwritten zip code recognition. Neural computation, 1(4):541–551, 1989.]"
      },
      {
        "citation": "[Sun, Y., Wang, X., and Tang, X. Deep learning face representation from predicting 10,000 classes. CVPR, 2014.]"
      },
      {
        "citation": "[Zhang, N., Paluri, M., Ranzato, M., Darrell, T., and Bourdev, L. Panda: Pose aligned networks for deep attribute modeling. CVPR, 2014.]"
      },
      {
        "citation": "[Zhu, Z., Luo, P., Wang, X., and Tang, X. Multi-view perceptron: a deep model for learning face identity and view representations. NIPS, 2014.]"
      }
    ],
    "author_details": [
      {
        "name": "Linjie Yang",
        "affiliation": "Department of Information Engineering, The Chinese University of Hong Kong",
        "email": "yl012@ie.cuhk.edu.hk"
      },
      {
        "name": "Ping Luo",
        "affiliation": "Department of Information Engineering, The Chinese University of Hong Kong",
        "email": "pluo@ie.cuhk.edu.hk"
      },
      {
        "name": "Chen Change Loy",
        "affiliation": "Department of Information Engineering, The Chinese University of Hong Kong",
        "email": "ccloy@ie.cuhk.edu.hk"
      },
      {
        "name": "Xiaoou Tang",
        "affiliation": "Department of Information Engineering, The Chinese University of Hong Kong",
        "email": "xtang@ie.cuhk.edu.hk"
      }
    ]
  },
  {
    "title": "The Common Self-polar Triangle of Concentric Circles and Its Application to Camera Calibration\n---AUTHORs---\nHaifei Huang\nHui Zhang\nYiu-ming Cheung",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Huang_The_Common_Self-Polar_2015_CVPR_paper.pdf",
    "id": "Huang_The_Common_Self-Polar_2015_CVPR_paper",
    "abstract": "In computer vision, camera calibration is crucial for various applications. This paper explores the properties of the common self-polar triangle formed by two concentric circles, a relatively unexplored area. The research reveals an infinite number of such triangles, sharing a common vertex and possessing right-triangle characteristics. These properties are leveraged to simultaneously recover the imaged circle center and the vanishing line of the support plane, providing constraints on the image of the absolute conic. The paper introduces a new perspective on circle-based camera calibration, potentially benefiting other methods using different circle patterns.\n\n---TOPSICS---\nCamera Calibration\nConcentric Circles\nSelf-Polar Triangles\nProjective Geometry\nAbsolute Conic",
    "topics": [],
    "references": [
      {
        "citation": "[Zhang, Z. (2000). A flexible new technique for camera calibration. *IEEE Transactions on Pattern Analysis and Machine Intelligence*, *22*(11), 1330-1334.] - This is a highly cited and foundational paper on camera calibration, frequently referenced in the field."
      },
      {
        "citation": "[Tsai, R. Y. (1987). A versatile camera calibration technique for high accuracy 3d machine vision metrology using off-the-shelf tv cameras and lenses. *IEEE Journal of Robotics and Automation*, *3*(4), 323-344.] - Another cornerstone paper on camera calibration, providing a widely used method."
      },
      {
        "citation": "[Hartley, R. (2007). An algorithm for self calibration from several views. *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*, pages 2205–2216.] - Relevant to the self-calibration aspect explored in the paper."
      },
      {
        "citation": "[Zhang, Z. (2004). Camera calibration with one-dimensional objects. *IEEE Transactions on Pattern Analysis and Machine Intelligence*, *26*(7), 892-899.] - Provides an alternative calibration approach using 1D objects."
      },
      {
        "citation": "[Faugeras, O. D., Luong, Q. T., & Maybank, S. J. (1992). Camera self-calibration: Theory and experiments. *Proceedings of the ECCV*, pages 321–334.] - A seminal work on camera self-calibration, a core concept in the paper."
      },
      {
        "citation": "[Hartley, R., & Zisserma, A. (2000). *Multiple View Geometry in Computer Vision*. Cambridge University.] - A comprehensive resource on multiple view geometry, providing theoretical background."
      },
      {
        "citation": "[Kim, J. S., Gurdjos, P., & Kweon, I. S. (2005). Geometric and algebraic constraints of projected concentric circles and their applications to camera calibration. *IEEE Transactions on Pattern Analysis and Machine Intelligence*, *27*(4), 637–642.] - Directly related to the paper's focus on concentric circles for calibration."
      },
      {
        "citation": "[Wang, L., & Yao, H. (2008). Effective and automatic calibration using concentric circles. *International Journal of Pattern Recognition and Artificial Intelligence*, *22*(7), 1379–1401.] -  A paper that specifically addresses camera calibration using concentric circles, similar to the current work."
      },
      {
        "citation": "[Gentle, J. E. (1998). *Numerical Linear Algebra for Applications in Statistics*. Springer-Verlag.] - Provides the mathematical foundation for the numerical methods used in camera calibration."
      },
      {
        "citation": "[Canny, J. (1986). A computational approach to edge detection. *IEEE Transactions on Pattern Analysis and Machine Intelligence*, *8*(6), 679–698.] - While not directly about camera calibration, edge detection is a common pre-processing step, making this a relevant background reference."
      }
    ],
    "author_details": [
      {
        "name": "Haifei Huang",
        "affiliation": "Department of Computer Science, Hong Kong Baptist University, Hong Kong SAR, China; United International College, BNU-HKBU, Zhuhai, China",
        "email": "mikehuang@uic.edu.hk"
      },
      {
        "name": "Hui Zhang",
        "affiliation": "United International College, BNU-HKBU, Zhuhai, China; Shenzhen Key Lab of Intelligent Media and Speech, Shenzhen, China",
        "email": "amyzhang@uic.edu.hk"
      },
      {
        "name": "Yiu-ming Cheung",
        "affiliation": "Department of Computer Science, Hong Kong Baptist University, Hong Kong SAR, China; United International College, BNU-HKBU, Zhuhai, China",
        "email": "ymc@comp.hkbu.edu.hk"
      }
    ]
  },
  {
    "title": "Improving Object Detection with Deep Convolutional Networks via Bayesian Optimization and Structured Prediction\n---AUTHOR---\nYuting Zhang\nKihyuk Sohn\nRuben Villegas\nGang Pan\nHonglak Lee",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Zhang_Improving_Object_Detection_2015_CVPR_supplemental.pdf",
    "id": "Zhang_Improving_Object_Detection_2015_CVPR_supplemental",
    "abstract": "This supplementary material provides detailed information about the implementation of object detection improvements using deep convolutional networks, Bayesian optimization, and structured prediction. It includes specifics on parameter estimation for finetuning with a structured SVM objective, details on hard negative data mining, implementation details on model parameter estimation, and analysis of fine-grained search efficiency. The document also presents performance metrics on the PASCAL VOC 2007 dataset, including mAP, precision-recall curves, localization accuracy, and illustrative examples of improvements and false positives.\n\n---TOPICICS---\nObject Detection\nDeep Convolutional Networks\nStructured Prediction\nBayesian Optimization\nHard Negative Data Mining",
    "topics": [],
    "references": [
      {
        "citation": "[Alexe, B., Deselaers, T., & Ferrari, V. (2012). Measuring the objectness of image windows. *IEEE Transactions on Pattern Analysis and Machine Intelligence*, *34*(11), 2189–2202.] - This paper likely provides a foundational understanding of objectness, a key concept in object detection."
      },
      {
        "citation": "[Blaschko, M. B., & Lampert, C. H. (2008). Learning to localize objects with structured output regression. *ECCV*.] - Introduces a structured output regression approach, relevant to object localization."
      },
      {
        "citation": "[Felzenszwalb, P., Girshick, R., McAllester, D., & Ramanan, D. (2010). Object detection with discriminatively trained part-based models. *IEEE Transactions on Pattern Analysis and Machine Intelligence*, *32*(9), 1627–1645.] - A seminal work on part-based models for object detection."
      },
      {
        "citation": "[Girshick, R., Donahue, J., Darrell, T., & Malik, J. (2014). Rich feature hierarchies for accurate object detection and semantic segmentation. *CVPR*.] - Introduces rich feature hierarchies, a significant advancement in object detection."
      },
      {
        "citation": "[Joachims, T. (2008). SVM-struct: Support vector machine for complex outputs. http://www.cs.cornell.edu/people/tj/svm_light/svm_struct.html] - Provides a method for structured output prediction using SVMs, potentially used in the paper's methodology."
      },
      {
        "citation": "[Pepikj, B., Stark, M., Gehler, P., & Schiele, B. (2013). Occlusion patterns for object class detection. *CVPR*.] - Addresses the challenge of occlusion in object detection, a common issue."
      },
      {
        "citation": "[Uijlings, J. R. R., Sande, K. E. A., Gevers, T., & Smeulders, A. W. M. (2013). Selective search for object recognition. *International Journal of Computer Vision*, *104*(2), 154–171.] - Describes Selective Search, a widely used algorithm for region proposal generation."
      },
      {
        "citation": "[Hoiem, D., Chodpathumwan, Y., & Dai, Q. (2012). Diagnosing error in object detectors. *ECCV*.] - Focuses on error analysis in object detectors, which is crucial for improving performance."
      },
      {
        "citation": "[Schmidt, M. (n.d.). minFunc toolbox. http://www.cs.ubc.ca/˜schmidtm/Software/minFunc.html] - Likely used for optimization within the paper's methods."
      }
    ],
    "author_details": [
      {
        "name": "Yuting Zhang",
        "affiliation": "Department of Electrical Engineering and Computer Science, University of Michigan",
        "email": "yutingzh@umich.edu"
      },
      {
        "name": "Kihyuk Sohn",
        "affiliation": "Department of Electrical Engineering and Computer Science, University of Michigan",
        "email": "kihyuks@umich.edu"
      },
      {
        "name": "Ruben Villegas",
        "affiliation": "Department of Electrical Engineering and Computer Science, University of Michigan",
        "email": "rubville@umich.edu"
      },
      {
        "name": "Gang Pan",
        "affiliation": "Department of Computer Science, Zhejiang University",
        "email": "zyt@zju.edu.cn"
      },
      {
        "name": "Honglak Lee",
        "affiliation": "Department of Electrical Engineering and Computer Science, University of Michigan",
        "email": "honglak@umich.edu"
      }
    ]
  },
  {
    "title": "Examples from the HELEN dataset, Photometric Stereo Examples, and Tom Hanks Examples \n---AUTHOR---\n(The paper does not explicitly list authors. It references [19] and mentions vizago.ch and facegen.com)",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Snape_Automatic_Construction_Of_2015_CVPR_supplemental.pdf",
    "id": "Snape_Automatic_Construction_Of_2015_CVPR_supplemental",
    "abstract": "The provided text does not contain a formal abstract. However, based on the content, a possible abstract could be: \"This paper presents a novel decomposition technique and demonstrates its effectiveness through various examples. Results are shown using the HE-LEN dataset, comparing against blind decomposition methods and commercial morphable model implementations. Further examples are provided using a Photometric Stereo (PS) dataset and an automatically collected dataset of Tom Hanks, showcasing the technique's ability to recover accurate facial 3D shapes and improve image fitting.\"\n\n---TOPIC---\nFacial Decomposition\nPhotometric Stereo\nMorphable Models\nImage Reconstruction\nTom Hanks Dataset",
    "topics": [],
    "references": [],
    "author_details": [
      {
        "name": "Vizago",
        "affiliation": "vizago.ch",
        "email": "Not available"
      },
      {
        "name": "Facegen",
        "email": "Not available"
      }
    ]
  },
  {
    "title": "Discriminative Learning of Iteration-wise Priors for Blind Deconvolution\n---AUTHOR---\nWangmeng Zuo\n---AUTHOR---\nDongwei Ren\n---AUTHOR---\nShuhang Gu\n---AUTHOR---\nLiang Lin\n---AUTHOR---\nLei Zhang",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Zuo_Discriminative_Learning_of_2015_CVPR_paper.pdf",
    "id": "Zuo_Discriminative_Learning_of_2015_CVPR_paper",
    "abstract": "The paper proposes a blind deconvolution framework that utilizes iteration-specific priors for improved blur kernel estimation. The approach employs a discriminative learning model to learn model parameters for hyper-Laplacian priors on image gradients, allowing for iteration-wise tuning of these parameters. This eliminates the need for manual parameter tuning and allows for the use of negative values for the hyper-Laplacian parameter 'p', which aids in estimating the coarse shape of the blur kernel. Experimental results demonstrate that the method achieves better deblurring results compared to existing gradient prior-based methods and is competitive with patch prior-based methods while being more efficient.\n\n---TOPICs---\nBlind Deconvolution\nIterative Prior Learning\nHyper-Laplacian Priors\nDiscriminative Learning\nKernel Estimation",
    "topics": [],
    "references": [
      {
        "citation": "[S. D. Babacan, R. Molina, M. N. Do, and A. K. Katsaggelos. Bayesian blind deconvolution with general sparse image priors. In ECCV, 2012.]"
      },
      {
        "citation": "[S. D. Babacan, R. Molina, and A. K. Katsaggelos. Variational bayesian blind deconvolution using a total variation prior. IEEE Transactions on Image Processing, 18(1):12–26, 2009.]"
      },
      {
        "citation": "[T. F. Chan and C. Wong. Total variation blind deconvolution. IEEE Transactions on Image Processing, 7(3):370–375, 1998.]"
      },
      {
        "citation": "[R. Fergus, B. Singh, A. Hertzmann, S. T. Roweis, and W. T. Freeman. Removing camera shake from a single photograph. ACM Transactions on Graphics (TOG), 25(3):787–794, 2006.]"
      },
      {
        "citation": "[A. Goldstein and R. Fattal. Blur-kernel estimation from spectral irregularities. In ECCV, 2012.]"
      },
      {
        "citation": "[S. Cho and S. Lee. Fast motion deblurring. ACM Transactions on Graphics (TOG), 28(5):145, 2009.]"
      },
      {
        "citation": "[J. Jia and L. Xu. Structure extraction from texture via relative total variation. ACM Transactions on Graphics (TOG), 31(6):139, 2012.]"
      },
      {
        "citation": "[N. Joshi, R. Szeliski, and D. Kriegman. PSF estimation using sharp edge prediction. In CVPR, 2008.]"
      },
      {
        "citation": "[A. Levin, Y. Weiss, F. Durand, and W. T. Freeman. Understanding and evaluating blind deconvolution algorithms. In CVPR, 2009.]"
      },
      {
        "citation": "[D. Krishnan and R. Fergus. Fast image deconvolution using hyper-laplacian priors. In NIPS, 2009.]"
      }
    ],
    "author_details": [
      {
        "name": "Wangmeng Zuo",
        "affiliation": "School of Computer Science and Technology, Harbin Institute of Technology",
        "email": "cswmzuo@gmail.com"
      },
      {
        "name": "Dongwei Ren",
        "affiliation": "School of Computer Science and Technology, Harbin Institute of Technology",
        "email": "rendongweihit@gmail.com"
      },
      {
        "name": "Shuhang Gu",
        "affiliation": "Dept. of Computing, The Hong Kong Polytechnic University",
        "email": "cssgu@comp.polyu.edu.hk"
      },
      {
        "name": "Liang Lin",
        "affiliation": "Sun Yat-Sen University",
        "email": "linliang@ieee.org"
      },
      {
        "name": "Lei Zhang",
        "affiliation": "Dept. of Computing, The Hong Kong Polytechnic University",
        "email": "cslzhang@comp.polyu.edu.hk"
      }
    ]
  },
  {
    "title": "Simulating Makeup through Physics-based Manipulation of Intrinsic Image Layers\n---AUTHOR---\nChen Li\nKun Zhou\nStephen Lin",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Li_Simulating_Makeup_Through_2015_CVPR_paper.pdf",
    "id": "Li_Simulating_Makeup_Through_2015_CVPR_paper",
    "abstract": "We present a method for simulating makeup in a face image. To generate realistic results without detailed geometric and reﬂectance measurements of the user, we propose to separate the image into intrinsic image layers and alter them according to proposed adaptations of physically-based reﬂectance models. Through this layer manipulation, the measured properties of cosmetic products are applied while preserving the appearance characteristics and lighting conditions of the target face. This approach is demonstrated on various forms of cosmetics including foundation, blush, lipstick, and eye shadow. Experimental results exhibit a close approximation to ground truth images, with-out artifacts such as transferred personal features and lighting effects that degrade the results of image-based makeup transfer methods.\n\n---TOPICCS---\nMakeup simulation\nIntrinsic image decomposition\nPhysically-based reflectance models\nImage-based makeup transfer\nComputer graphics rendering",
    "topics": [],
    "references": [
      {
        "citation": "[F. L. Bookstein, 1989] Principal warps: Thin-plate splines and the decomposition of deformations. IEEE Trans. Pattern Anal. Mach. Intell., 11(6):567–585. (Fundamental work on deformation models, likely used for facial shape manipulation)"
      },
      {
        "citation": "[C. Schlick, 1994] An inexpensive BRDF model for physically-based rendering. Computer Graphics Forum, 13(3). (A foundational BRDF model, crucial for realistic rendering of skin)"
      },
      {
        "citation": "[F. Yang et al., 2011] Expression flow for 3d-aware face component transfer. ACM Trans. Graph., 30(4):60:1–60:10. (Addresses expression transfer, relevant for dynamic makeup application)"
      },
      {
        "citation": "[J. Goldsmith and R. Kanthack, 1921] Tables of Refracive Indices: Oils, fats and waxes. Tables of Refracive Indices. (Provides essential data for refractive index matching, a core aspect of cosmetic rendering)"
      },
      {
        "citation": "[D. Guo and T. Sim, 2009] Digital face makeup by example. In Proc. of IEEE Conf. on Computer Vision and Pattern Recognition (CVPR). (Early work on example-based makeup transfer)"
      },
      {
        "citation": "[C.-G. Huang et al., 1993] Physically-based cosmetic rendering. In Proceedings of the ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games, I3D ’13, pages 190–190. (Focuses on physically-based rendering specifically for cosmetics)"
      },
      {
        "citation": "[K. Torrance and E. Sparrow, 1967] Theory for off-specular reflection from roughened surfaces. J. Optical Soc. America, 57:1105–1114. (Provides a theoretical basis for modeling surface reflectance, important for skin)"
      },
      {
        "citation": "[M. Doi et al., 2005] Spectral estimation of skin color with foundation makeup. In H. Klviinen et al., editors, SCIA, volume 3540 of Lecture Notes in Computer Science, pages 95–104. Springer. (Addresses the challenge of spectral analysis in the presence of makeup)"
      },
      {
        "citation": "[W.-S. Tong et al., 2007] Example-based cosmetic transfer. Computer Graphics and Applications, Pacific Conference on, 0:211–218. (Another work on example-based transfer, a common approach)"
      },
      {
        "citation": "[K. Scherbaum et al., 2011] Computer-suggested facial makeup. Comp. Graph. Forum (Proc. Eurographics 2011), 30(2). (Explores the concept of computer-aided makeup suggestion)"
      },
      {
        "citation": "---"
      }
    ],
    "author_details": [
      {
        "name": "Chen Li",
        "affiliation": "State Key Lab of CAD&CG, Zhejiang University",
        "email": "[Email not available in the provided text]"
      },
      {
        "name": "Kun Zhou",
        "affiliation": "State Key Lab of CAD&CG, Zhejiang University",
        "email": "[Email not available in the provided text]"
      },
      {
        "name": "Stephen Lin",
        "affiliation": "Microsoft Research",
        "email": "[Email not available in the provided text]"
      }
    ]
  },
  {
    "title": "Reweighted Laplace Prior Based Hyperspectral Compressive Sensing for Unknown Sparsity\n---AUTHOR---\nLei Zhang\nWei Wei\nYanning Zhang\nChunna Tian\nFei Li",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Zhang_Reweighted_Laplace_Prior_2015_CVPR_paper.pdf",
    "id": "Zhang_Reweighted_Laplace_Prior_2015_CVPR_paper",
    "abstract": "Hyberspectral image (HSI) acquisition and transmission are costly due to its 3D data cube structure. Compressive sensing (CS) offers a solution by enabling reconstruction from fewer measurements. However, reconstructing HSIs remains challenging due to the unknown and variable sparsity of the underlying signal. This paper addresses this problem by proposing a novel reweighted Laplace prior-based hyperspectral compressive sensing method. The method models sparsity distribution using a reweighted Laplace prior and employs a latent variable Bayes model to learn optimal prior configurations from measurements. This unified variational framework infers parameters automatically, adapting to unknown noise and improving reconstruction accuracy, as demonstrated by experimental results on three hyperspectral datasets.\n\n---TOPICICS---\nHyberspectral Image (HSI)\nCompressive Sensing (CS)\nReweighted Laplace Prior\nBayes Model\nSparsity Estimation",
    "topics": [],
    "references": [],
    "author_details": [
      {
        "name": "Lei Zhang",
        "affiliation": "School of Computer Science, Northwestern Polytechnical University, Xi’an, 710072, China",
        "email": "zhanglei211@mail.nwpu.edu.cn"
      },
      {
        "name": "Wei Wei",
        "affiliation": "School of Computer Science, Northwestern Polytechnical University, Xi’an, 710072, China",
        "email": "weinwpu@nwpu.edu.cn"
      },
      {
        "name": "Yanning Zhang",
        "affiliation": "School of Computer Science, Northwestern Polytechnical University, Xi’an, 710072, China",
        "email": "ynzhang@nwpu.edu.cn"
      },
      {
        "name": "Chunn Tian",
        "affiliation": "School of Electronic Engineering, Xidian University, Xi’an, 710071, China",
        "email": "chnhatian@xidian.edu.cn"
      },
      {
        "name": "Fei Li",
        "affiliation": "School of Computer Science, Northwestern Polytechnical University, Xi’an, 710072, China",
        "email": "N/A"
      }
    ]
  },
  {
    "title": "Radial Distortion Homography\n---AUTHOR---\nZuzana Kukelova\nJan Heller\nMartin Bujnak\nTomas Pajdla",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Kukelova_Radial_Distortion_Homography_2015_CVPR_paper.pdf",
    "id": "Kukelova_Radial_Distortion_Homography_2015_CVPR_paper",
    "abstract": "The estimation of a homography between two views is crucial in computer vision, but often the importance of precise homography estimation is underestimated. Ignoring radial distortion in homography estimation can lead to significant errors. This paper addresses this gap by presenting two algorithms for estimating homography between two cameras with different radial distortions, suitable for planar scenes and pure rotation between cameras. The first algorithm uses five image point correspondences and solves a nonlinear system of polynomial equations using Gr¨obner basis method. The second uses six correspondences and leads to a simpler system of equations. The proposed algorithms are fast, stable, and suitable for use within a RANSAC loop.\n\n---TOPIC---\nHomography Estimation\nRadial Distortion\nComputer Vision\nRANSAC\nGröbner basis method",
    "topics": [],
    "references": [
      {
        "citation": "[M. Fischler and R. C. Bolles. Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography. Commun. ACM, 24(6):381–395, June 1981.] - This is the foundational work on RANSAC, a key technique likely used in the paper."
      },
      {
        "citation": "[A. W. Fitzgibbon. Simultaneous linear estimation of multiple view geometry and lens distortion. In CVPR’01, volume 1, page 125, Los Alamitos, CA, USA, 2001. IEEE Computer Society.] - Addresses lens distortion and geometry estimation, a core theme."
      },
      {
        "citation": "[R. I. Hartley and A. Zisserman. Multiple View Geometry in Computer Vision. Cambridge University Press, second edition, 2004.] - A standard reference for multiple view geometry, likely providing background."
      },
      {
        "citation": "[M. Fischler and R. C. Bolles. Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography. Commun. ACM, 24(6):381–395, June 1981.] - Provides the basis for RANSAC."
      },
      {
        "citation": "[D. Cox, J. Little, and D. O’Shea. Using Algebraic Geometry. Springer, 2nd edition, 2005.] - Likely used for mathematical formulation and problem solving."
      },
      {
        "citation": "[M. Byr¨od, Z. Kukelova, K. Josephson, T. Pajdla, and K. ˚Astr¨om. Fast and robust numerical solutions to minimal problems for cameras with radial distortion. In CVPR’08, 2008.] - Directly relevant to the paper's focus on minimal solutions and lens distortion."
      },
      {
        "citation": "[O. Chum, J. Matas, and J. Kittler. Locally optimized ransec. In Pattern Recognition, pages 236–243. Springer Berlin Heidelberg, 2003.] - An extension of RANSAC, likely used or compared with."
      },
      {
        "citation": "[M. Byr¨od, M. Brown, and K. ˚Astr¨om. Minimal solutions for panoramic stitching with radial distortion. In BMVC’09, 2009.] - Focuses on minimal solutions for panoramic stitching, a likely application."
      },
      {
        "citation": "[A. W. Fitzgibbon. Simultaneous linear estimation of multiple view geometry and lens distortion. In CVPR’01, volume 1, page 125, Los Alamitos, CA, USA, 2001. IEEE Computer Society.] - Addresses lens distortion and geometry estimation."
      },
      {
        "citation": "[H. Jin. A three-point minimal solution for panoramic stitching with lens distortion. In CVPR’08, 2008.] - Presents a specific minimal solution approach for panoramic stitching."
      }
    ],
    "author_details": [
      {
        "name": "Zuzana Kukelova",
        "affiliation": "Microsoft Research Ltd",
        "email": "a-zukuke@microsoft.com"
      },
      {
        "name": "Jan Heller",
        "affiliation": "Czech Technical University in Prague",
        "email": "hellej1@cmp.felk.cvut.cz"
      },
      {
        "name": "Martin Bujnak",
        "affiliation": "Capturing Reality s.r.o.",
        "email": "martin@capturingreality.com"
      },
      {
        "name": "Tomas Pajdla",
        "affiliation": "Czech Technical University in Prague",
        "email": "pajdla@cmp.felk.cvut.cz"
      }
    ]
  },
  {
    "title": "Image Speciﬁcity\n---AUTHOR---\nMainak Jas\nDevi Parikh",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Jas_Image_Specificity_2015_CVPR_paper.pdf",
    "id": "Jas_Image_Specificity_2015_CVPR_paper",
    "abstract": "This paper introduces the concept of \"image specificity,\" which measures the consistency of descriptions provided by different people for the same image. Some images elicit consistent descriptions (specific), while others elicit varied descriptions (ambiguous). The authors present automated and human-judged methods to measure image specificity, analyze its relationship to image content, and train models to predict specificity from image features alone. They demonstrate that modeling image specificity improves performance in a text-based image retrieval application.\n\n---TOPICCS---\nImage Specificity\nText-Based Image Retrieval\nHuman Description Variance\nImage Content Analysis\nAutomated Prediction Models",
    "topics": [],
    "references": [
      {
        "citation": "[Banerjee, S., & Lavie, A. (2005). METEOR: An automatic metric for MT evaluation with improved correlation with human judgments.]"
      },
      {
        "citation": "[Barnard, K., & Forsyth, D. (2001). Learning the semantics of words and pictures. In IEEE International Conference on Computer Vision (ICCV), volume 2, pages 108–115.]"
      },
      {
        "citation": "[Berg, A. C., Berg, T. L., Daume, H., Dodge, J., Goyal, A., Han, X., Mensch, A., Mitchell, M., Sood, A., Stratos, K., et al. (2012). Understanding and predicting importance in images. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 3562–3569.]"
      },
      {
        "citation": "[Chen, X., & Zitnick, C. L. (2015). Learning a Recurrent Visual Representation for Image Caption Generation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR).]"
      },
      {
        "citation": "[Cui, J., Wen, F., & Tang, X. (2008). Real time google and live image search re-ranking. In Proceedings of the 16th ACM international conference on Multimedia, pages 729–732.]"
      },
      {
        "citation": "[Donahue, J., Hendricks, L. A., Guadarrama, S., Rohrbach, M., Venugopalan, S., Saenko, K., & Darrell, T. (2014). Long-term recurrent convolutional networks for visual recognition and description. arXiv preprint arXiv:1411.4389.]"
      },
      {
        "citation": "[Douze, M., Ramisa, A., & Schmid, C. (2011). Combining attributes and Fisher vectors for efficient image retrieval. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 277–284.]"
      },
      {
        "citation": "[Elliott, D., & Keller, F. (n.d.). Comparing Automatic Evaluation Measures for Image Description.]"
      },
      {
        "citation": "[Everingham, M., Eslamimi, S. A., Van Gool, L., Williams, C. K., Winn, J., & Zisserma, A. (2010). The Pascal Visual Object Classes Challenge–a Retrospective.]"
      },
      {
        "citation": "[Fang, H., Gupta, S., Iandola, F., Srivastava, R., Deng, L., Dollár, P., Gao, J., He, X., Mitchell, M., Platt, J., et al. (2015). From captions to visual concepts and back. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR).]"
      }
    ],
    "author_details": [
      {
        "name": "Mainak Jas",
        "affiliation": "Aalto University",
        "email": "mainak.jas@aalto.fi"
      },
      {
        "name": "Devi Parikh",
        "affiliation": "Virginia Tech",
        "email": "parikh@vt.edu"
      }
    ]
  },
  {
    "title": "Semi-supervised Domain Adaptation with Subspace Learning for Visual Recognition\n---AUTHORISTS---\nTing Yao\nYingwei Pan\nChong-Wah Ngo\nHouqiang Li\nTao Mei",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Yao_Semi-Supervised_Domain_Adaptation_2015_CVPR_paper.pdf",
    "id": "Yao_Semi-Supervised_Domain_Adaptation_2015_CVPR_paper",
    "abstract": "Domain adaptation addresses the challenge of applying machine learning models trained on a source domain to a target domain with different characteristics, a phenomenon known as \"domain shift.\" This paper proposes a novel Semi-supervised Domain Adaptation with Subspace Learning (SDASL) framework for visual recognition. SDASL jointly explores invariant low-dimensional structures across domains to correct data distribution mismatch and leverages available unlabeled target examples to exploit the underlying intrinsic information in the target domain. The framework simultaneously minimizes classification error, preserves structure within and across domains, and restricts similarity defined on unlabeled target examples. Experimental results on image-to-image and image-to-video transfer tasks demonstrate the effectiveness of the approach.\n\n---TOPIC---\nDomain Adaptation\nSubspace Learning\nSemi-supervised Learning\nVisual Recognition\nCross-Domain Learning",
    "topics": [],
    "references": [
      {
        "citation": "[S. J. Pan, J. T. Kwok, and Q. Yang. Transfer learning via dimensionality reduction. In AAAI, 2008.] - This paper introduces a foundational approach to transfer learning using dimensionality reduction."
      },
      {
        "citation": "[S. J. Pan, I. W. Tsang, J. T. Kwok, and Q. Yang. Domain adaptation via transfer component analysis. IEEE Trans. on Neural Networks, 99:1–12, 5009.] - A key paper on Transfer Component Analysis, a widely used domain adaptation technique."
      },
      {
        "citation": "[M. Baktashmotlagh, M. T. Harandi, B. C. Lovell, and M. Salzmann. Domain adaptation on the statistical manifold. In CVPR, 2014.] - Addresses domain adaptation within a statistical manifold framework."
      },
      {
        "citation": "[H. Daum´e III. Fructratingly easy domain adaptation. In ACL, 2007.] - A seminal work that highlights a relatively simple approach to domain adaptation."
      },
      {
        "citation": "[J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imaginet: A large-scale hierarchical image database. In CVPR, 2009.] - Introduces ImageNet, a crucial dataset for many computer vision tasks, including domain adaptation."
      },
      {
        "citation": "[M. Long, J. Wang, G. Ding, J. Sun, and P. S. Yu. Transfer joint matching for unsupervised domain adaptation. In CVPR, 2014.] - Presents a joint matching approach for unsupervised domain adaptation."
      },
      {
        "citation": "[K. Saenko, B. Kulis, M. Fritz, and T. Darrell. Adapting visual category models to new domains. In ECCV, 2010.] - Focuses on adapting visual category models across domains."
      },
      {
        "citation": "[J. Jiang and C. Zhai. Instance weighting for domain adaptation in nlp. In ACL, 2007.] - Explores instance weighting as a method for domain adaptation in natural language processing."
      },
      {
        "citation": "[Y. Shi and F. Sha. Information-theoretical learning of discriminative clusters for unsupervised domain adaptation. In ICML, 2012.] - Uses information theory to learn discriminative clusters for unsupervised domain adaptation."
      },
      {
        "citation": "[B. Gong, Y. Shi, F. Sha, and K. Grauman. Geodesic flow kernel for unsupervised domain adaptation. In CVPR, 2012.] - Introduces a geodesic flow kernel for unsupervised domain adaptation."
      },
      {
        "citation": "---"
      }
    ],
    "author_details": [
      {
        "name": "Ting Yao",
        "affiliation": "Microsoft Research, Beijing, China",
        "email": "tiyao@microsoft.com"
      },
      {
        "name": "Yingwei Pan",
        "affiliation": "University of Science and Technology of China, Hefei, China",
        "email": "panyw.ustc@gmail.com"
      },
      {
        "name": "Chong-Wah Ngo",
        "affiliation": "City University of Hong Kong, Kowloon, Hong Kong",
        "email": "cscwngo@cityu.edu.hk"
      },
      {
        "name": "Houqiang Li",
        "affiliation": "University of Science and Technology of China, Hefei, China",
        "email": "lihq@ustc.edu.cn"
      },
      {
        "name": "Tao Mei",
        "affiliation": "Microsoft Research, Beijing, China",
        "email": "tmei@microsoft.com"
      }
    ]
  },
  {
    "title": "ActivityNet: A Large-Scale Video Benchmark for Human Activity Understanding\n---AUTHOR---\nFabian Caba Heilbron\nVictor Escorcia\nBernard Ghanem\nJuan Carlos Niebles",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Heilbron_ActivityNet_A_Large-Scale_2015_CVPR_paper.pdf",
    "id": "Heilbron_ActivityNet_A_Large-Scale_2015_CVPR_paper",
    "abstract": "This paper introduces ActivityNet, a new large-scale video benchmark for human activity understanding. Current benchmarks for action/activity recognition are limited by their simplicity and focus on a small number of activities. ActivityNet aims to cover a wide range of complex human activities relevant to daily life, providing samples from 203 activity classes with an average of 137 untrimmed videos per class and 1.41 activity instances per video, totaling 849 video hours. The benchmark supports untrimmed video classification, trimmed activity classification, and activity detection scenarios and is structured around a semantic ontology.",
    "topics": [
      "Human activity understanding",
      "Video benchmark",
      "Semantic ontology",
      "Activity recognition",
      "Large-scale dataset"
    ],
    "references": [
      {
        "citation": "[H. Kuehne, H. Jhuang, E. Garrote, T. Poggio, and T. Serre, HMDB: A large video database for human motion recognition. In ICCV, 2011.]"
      },
      {
        "citation": "[B. Ainsworth, W. Haskell, S. Herrmann, N. Meckes, D. Bassett Jr., C. Tudor-Locke, J. Greer, J. Vezina, M. Whitt-Glover, and A. Leon, 2011 com pendium of physical activities: a second update of codes and met values. Medicine and Science in Sports and Exercise, 2011.]"
      },
      {
        "citation": "[I. Laptev, M. Marszałek, C. Schmid, and B. Rozenfeld, Learning realistic human actions from movies. In CVPR, 2008.]"
      },
      {
        "citation": "[I. Lillo, A. Soto, and J. C. Niebles, Discriminative hierarchical modeling of spatio-temporally composable human activities. In CVPR, 2014.]"
      },
      {
        "citation": "[M. Andriluka, L. Pishchulin, P. Gehler, and B. Schiele, 2d human pose estimation: New benchmark and state of the art analysis. In CVPR, June 2014.]"
      },
      {
        "citation": "[I. Atmosukarto, B. Ghanem, and N. Ahuja, Trajectory-based fisher kernel representation for action recognition in videos. In ICPR. IEEE, 2012.]"
      },
      {
        "citation": "[M. Marszalek, I. Laptev, and C. Schmid, Actions in context. In CVPR, 2009.]"
      },
      {
        "citation": "[G. A. Miller, WordNet: A Lexical Database for English. Communications of the ACM, 38(39-41), 1995.]"
      },
      {
        "citation": "[F. C. Heilbron and J. C. Niebles, Collecting and annotating human activities in web videos. In International Conference on Multimedia Retrieval, 2014.]"
      },
      {
        "citation": "[J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, ImageNet: A Large-Scale Hierarchical Image Database. In CVPR, 2009.]"
      }
    ],
    "author_details": [
      {
        "name": "Fabian Caba Heilbron",
        "affiliation": "Universidad del Norte, Colombia",
        "email": "Not available"
      },
      {
        "name": "Victor Escorcia",
        "affiliation": "Universidad del Norte, Colombia",
        "email": "Not available"
      },
      {
        "name": "Bernard Ghanem",
        "affiliation": "King Abdullah University of Science and Technology (KAUST), Saudi Arabia",
        "email": "Not available"
      },
      {
        "name": "Juan Carlos Niebles",
        "affiliation": "Universidad del Norte, Colombia",
        "email": "Not available"
      }
    ]
  },
  {
    "title": "Superpixel Segmentation using Linear Spectral Clustering\n---AUTHOR---\nZhengqin Li\nJiansheng Chen",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Li_Superpixel_Segmentation_Using_2015_CVPR_paper.pdf",
    "id": "Li_Superpixel_Segmentation_Using_2015_CVPR_paper",
    "abstract": "This paper introduces Linear Spectral Clustering (LSC), a novel superpixel segmentation algorithm designed for compactness, uniformity, and computational efficiency. LSC utilizes a normalized cuts formulation based on color and spatial proximity, approximating the similarity metric with a kernel function to map pixels into a high-dimensional feature space. The algorithm leverages the equivalence between weighted K-means and normalized cuts in this feature space, allowing for optimization via iterative K-means clustering. LSC exhibits linear computational complexity, high memory efficiency, preserves global image properties, and achieves performance comparable to or exceeding state-of-the-art superpixel segmentation algorithms.",
    "topics": [
      "Superpixel Segmentation",
      "Linear Spectral Clustering (LSC)",
      "Normalized Cuts",
      "K-means Clustering",
      "Image Processing"
    ],
    "references": [
      {
        "citation": "[Boykov, Y. and Kolmogrov, V. An experimental comparison of min-cut/max-ﬂow algorithms for energy minimization in vision. IEEE Trans. on PAMI, 26(9):1124–1137, 2001.] - This paper likely provides foundational context for energy minimization techniques used in superpixel generation."
      },
      {
        "citation": "[Achantan, R., Shaji, A., Smith, K., Lucchi, A., Fua, P., and Susstrunk, S. Slic superpixels compared to state-of-the-art superpixel methods. IEEE Trans. on PAMI, 34(11):2274–2281, 2012.] - A comparative study of the SLIC algorithm, a common superpixel method."
      },
      {
        "citation": "[Vekslar, O., Boykov, Y., and Mehran, P. Superpixels and supervoxels in an energy optimization framework. Proc. of ECCV, pages 211–224, 2010.] - This paper presents a framework for superpixel and supervoxel generation using energy optimization."
      },
      {
        "citation": "[Bergh, M., Boix, X., Roig, G., Capitani, B., and Gool, L. V. Seeds: Superpixels extracted via energy-driven sampling. Proc. of ECCV, 7578:13–26, 2012.] - Introduces the SEEDS superpixel extraction method."
      },
      {
        "citation": "[Veldadi, A. and Soatto, S. Quick shift and kernel methods for mode seeking. Proc. of ECCV, pages 705–718, 2008.] - Discusses Quick Shift, another superpixel generation technique."
      },
      {
        "citation": "[Wang, S., Lu, H., Yang, F., and Yang, M. Superpixel tracking. Proc. of ICCV, 1:1323–1330, 2011.] - Explores the application of superpixels in tracking."
      },
      {
        "citation": "[Yu, S. and Shi, J. Multiclass spectral clustering. Proc. of ICCV, 1:313–319, 2003.] -  Spectral clustering is a common technique used in conjunction with superpixels."
      }
    ],
    "author_details": [
      {
        "name": "Zhengqin Li",
        "affiliation": "Department of Electronic Engineering, Tsinghua University, Beijing, China",
        "email": "li-zq12@mails.tsinghua.edu.cn"
      },
      {
        "name": "Jiansheng Chen",
        "affiliation": "Department of Electronic Engineering, Tsinghua University, Beijing, China",
        "email": "jschenthu@mail.tsinghua.edu.cn"
      }
    ]
  },
  {
    "title": "Understanding image representations by measuring their equivariance and equivalence\n---AUTHOR---\nKarel Lenc\nAndrea Vedaldi",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Lenc_Understanding_Image_Representations_2015_CVPR_paper.pdf",
    "id": "Lenc_Understanding_Image_Representations_2015_CVPR_paper",
    "abstract": "Despite the importance of image representations such as histograms of oriented gradients and deep Convolutional Neural Networks (CNN), our theoretical understanding of them remains limited. Aiming at filling this gap, we investigate three key mathematical properties of representations: equivariance, invariance, and equivalence. Equivariance studies how transformations of the input image are encoded by the representation, invariance being a special case where a transformation has no effect. Equivalence studies whether two representations, for example two different parameterizations of a CNN, capture the same visual information or not. A number of methods to establish these properties empirically are proposed, including introducing transformation and stitching layers in CNNs. These methods are then applied to popular representations to reveal insightful aspects of their structure, including clarifying at which layers in a CNN certain geometric invariances are achieved. While the focus of the paper is theoretical, direct applications to structured-output regression are demonstrated too.\n\n---TOPIC---\nImage Representations\n---TOPI---\nEquivariance\n---TOPI---\nInvariance\n---TOPI---\nEquivalence\n---TOPI---\nConvolutional Neural Networks (CNNs)\n---TOPI---\nGeometric Transformations",
    "topics": [],
    "references": [
      {
        "citation": "[Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). ImageNet classification with deep convolutional neural networks. In *Proc. NIPS*. ]"
      },
      {
        "citation": "[Taskar, B., Guestrin, C., & Koller, D. (2003). Max-margin Markov networks. In *Proc. NIPS*.]"
      },
      {
        "citation": "[Dalal, N., & Triggs, B. (2005). Histograms of oriented gradients for human detection. In *Proc. CVPR*.]"
      },
      {
        "citation": "[Lowe, D. G. (1999). Object recognition from local scale-invariant features. In *Proc. ICCV*.]"
      },
      {
        "citation": "[Lowe, D. G. (2004). Distinctive image features from scale-invariant keypoints. *IJCV*, *60*(2):91–110.]"
      },
      {
        "citation": "[Chatfield, K., Simonyan, K., Vedaldi, A., & Zisserman, A. (2014). Return of the devil in the details: Delving deep into convolutional nets. In *Proc. BMVC*.]"
      },
      {
        "citation": "[Donahue, J., Jia, Y., Vinyals, O., Hoffman, J., Zhang, N., Tzeng, E., & Darrell, T. (2013). Decaf: A deep convolutional activation feature for generic visual recognition. *CoRR*, abs/1310.1531.]"
      },
      {
        "citation": "[Vedaldi, A., & Fulkerson, B. (2010). VLFeat – An open and portable library of computer vision algorithms. In *Proc. ACM Int. Conf. on Multimedia*.]"
      },
      {
        "citation": "[Mikolajczyk, K., & Schmid, C. (2003). A performance evaluation of local descriptors. In *Proc. CVPR*.]"
      },
      {
        "citation": "[Goodfellow, I., Lee, H., Le, Q. V., Saxe, A., & Ng, A. Y. (2009). Measuring invariances in deep networks. In *Advances in neural information processing systems*.]"
      }
    ],
    "author_details": [
      {
        "name": "Karel Lenc",
        "affiliation": "Department of Engineering Science, University of Oxford",
        "email": "[Email not available in the provided text]"
      },
      {
        "name": "Andrea Vedaldi",
        "affiliation": "Department of Engineering Science, University of Oxford",
        "email": "[Email not available in the provided text]"
      }
    ]
  },
  {
    "title": "A Low-dimensional Step Pattern Analysis Algorithm with Application to Multimodal Retinal Image Registration\n---AUTHOR---\nJimmy Addison Lee\nJun Cheng\nBeng Hai Lee\nEe Ping Ong\nGuozhen Xu\nDamon Wing Kee Wong\nJiang Liu\nAugustinus Laude\nTock Han Lim",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Lee_A_Low-Dimensional_Step_2015_CVPR_paper.pdf",
    "id": "Lee_A_Low-Dimensional_Step_2015_CVPR_paper",
    "abstract": "Retinal image registration is crucial for ophthalmologist diagnosis and treatment. This paper addresses multimodal retinal image registration, a challenging area due to non-linear intensity differences and image quality issues caused by pathologies. Existing feature descriptor-based methods often struggle with unhealthy images and demand high dimensionality. To overcome these limitations, the paper introduces a novel low-dimensional step pattern analysis (LoSPA) algorithm. LoSPA locates robust corner features based on connecting edges and describes them using rotation-invariant step patterns, making it robust to intensity changes and achieving a higher success rate in multimodal registration compared to state-of-the-art algorithms.",
    "topics": [
      "Multimodal retinal image registration",
      "Low-dimensional feature analysis",
      "Step pattern analysis (LoSPA)",
      "Retinal image pathologies",
      "Image registration algorithms"
    ],
    "references": [],
    "author_details": [
      {
        "name": "Jimmy Addison Lee",
        "affiliation": "Institute for Infocomm Research (I2R), Agency for Science, Technology and Research (A*STAR)",
        "email": "jalee@i2r.a-star.edu.sg"
      },
      {
        "name": "Jun Cheng",
        "affiliation": "Institute for Infocomm Research (I2R), Agency for Science, Technology and Research (A*STAR)",
        "email": "jcheng@i2r.a-star.edu.sg"
      },
      {
        "name": "Beng Hai Lee",
        "affiliation": "Institute for Infocomm Research (I2R), Agency for Science, Technology and Research (A*STAR)",
        "email": "benghai@i2r.a-star.edu.sg"
      },
      {
        "name": "Ee Ping Ong",
        "affiliation": "Institute for Infocomm Research (I2R), Agency for Science, Technology and Research (A*STAR)",
        "email": "epong@i2r.a-star.edu.sg"
      },
      {
        "name": "Guozhen Xu",
        "affiliation": "Institute for Infocomm Research (I2R), Agency for Science, Technology and Research (A*STAR)",
        "email": "xug@i2r.a-star.edu.sg"
      },
      {
        "name": "Damon Wing Kee Wong",
        "affiliation": "Institute for Infocomm Research (I2R), Agency for Science, Technology and Research (A*STAR)",
        "email": "wkwong@i2r.a-star.edu.sg"
      },
      {
        "name": "Jiang Liu",
        "affiliation": "Institute for Infocomm Research (I2R), Agency for Science, Technology and Research (A*STAR)",
        "email": "jliu@i2r.a-star.edu.sg"
      },
      {
        "name": "Augustinus Laude",
        "affiliation": "National Healthcare Group, Eye Institute, Tan Tock Seng Hospital",
        "email": "laude augustinus@ttsh.com.sg"
      },
      {
        "name": "Tock Han Lim",
        "affiliation": "National Healthcare Group, Eye Institute, Tan Tock Seng Hospital",
        "email": "tock han lim@nhg.com.sg"
      }
    ]
  },
  {
    "title": "Data-Driven 3D Voxel Patterns for Object Category Recognition\n---AUTHOR---\nYu Xiang\nWongun Choi\nYuanqing Lin\nSilvio Savarese",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Xiang_Data-Driven_3D_Voxel_2015_CVPR_supplemental.pdf",
    "id": "Xiang_Data-Driven_3D_Voxel_2015_CVPR_supplemental",
    "abstract": "This supplementary material provides implementation details and additional qualitative examples for the object category recognition framework presented in the main paper, \"Data-Driven 3D Voxel Patterns for Object Category Recognition.\" It focuses on the voxelization process, similarity metric, 3D clustering, and demonstrates the generalization of trained detectors across different datasets (KITTI and OutdoorScene).\n\n---TOPIC---\nVoxelization\n3D Clustering\nSimilarity Metrics\nMean Voxel Models\n3D Object Recognition",
    "topics": [],
    "references": [
      {
        "citation": "[Geiger, A., Lenz, P., & Urtasun, R. (2012). Are we ready for autonomous driving? the kitti vision benchmark suite. In *CVPR*, pages 3354–3361.] - This paper introduces the KITTI Vision Benchmark Suite, a crucial dataset used extensively in the paper for training and evaluation."
      },
      {
        "citation": "[Dollár, P., Appel, R., Belongie, S., & Perona, P. (2014). Fast feature pyramids for object detection. *TPAMI*, 2014.] - Referenced for fast feature pyramids used in object detection."
      },
      {
        "citation": "[Felzenszwalb, P. F., Girshick, R. B., McAllester, D., & Ramanan, D. (2010). Object detection with discriminatively trained part-based models. *TPAMI*, *32*(9), 1627–1645.] - Referenced as a baseline method (DPM) and for object detection techniques."
      },
      {
        "citation": "[Frey, B. J., & Dueck, D. (2007). Clustering by passing messages between data points. *Science*, *315*(5814), 972–976.] - Used for affinity propagation clustering, a key component in building the 3DVP models."
      },
      {
        "citation": "[Ohn-Bar, E., & Trivedi, M. M. (2014). Fast and robust object detection using visual subcategories. In *CVPRW*, pages 179–184.] - Referenced for object detection using visual subcategories."
      },
      {
        "citation": "[Xiang, Y., & Savarese, S. (2013). Object detection by 3d aspectlets and occlusion reasoning. In *ICCV*, pages 530–537.] - Referenced for object detection using 3D aspectlets and occlusion reasoning."
      },
      {
        "citation": "[Seitz, S. M., Curless, B., Diebel, J., Scharstein, D., & Szeliski, R. (2006). A comparison and evaluation of multi-view stereo reconstruction algorithms. In *CVPR*, volume 1, pages 519–528.] - Referenced for multi-view stereo reconstruction algorithms."
      },
      {
        "citation": "[Trimble. (n.d.). 3D Warehouse. http://3dwarehouse. sketchup.com.] - Used as a source for 3D models."
      }
    ],
    "author_details": [
      {
        "name": "Yu Xiang",
        "affiliation": "University of Michigan at Ann Arbor",
        "email": "yuxiang@umich.edu"
      },
      {
        "name": "Wongun Choi",
        "affiliation": "NEC Laboratories America, Inc.",
        "email": "wongun@nec-labs.com"
      },
      {
        "name": "Yuanqing Lin",
        "affiliation": "NEC Laboratories America, Inc.",
        "email": "ylin@nec-labs.com"
      },
      {
        "name": "Silvio Savarese",
        "affiliation": "Stanford University",
        "email": "ssilvio@stanford.edu"
      }
    ]
  },
  {
    "title": "Fixation Bank: Learning to Reweight Fixation Candidates\n---AUTHORs---\nJiaping Zhao\nChristian Siagian\nLaurent Itti",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Zhao_Fixation_Bank_Learning_2015_CVPR_paper.pdf",
    "id": "Zhao_Fixation_Bank_Learning_2015_CVPR_paper",
    "abstract": "Predicting where humans will fixate in a scene has many practical applications. Biologically-inspired saliency models decompose visual stimuli into feature maps across multiple scales, and then integrate different feature channels, e.g., in a linear, MAX, or MAP. However, to date there is no universally accepted feature integration mechanism. Here, we propose a new data-driven solution: We first build a “fixation bank” by mining training samples, which maintains the association between local patterns of activation, in 4 feature channels (color, intensity, orientation, motion) around a given location, and corresponding human fixation density at that location. During testing, we decompose feature maps into blobs, extract local activation patterns around each blob, match those patterns against the fixation bank by group lasso, and determine weights of blobs based on reconstruction errors. Our final saliency map is the weighted sum of all blobs. Our system thus incorporates some amount of spatial and featural context information into the location-dependent weighting mechanism. Tested on two standard data sets, our model slightly but significantly outperforms 7 state-of-the-art saliency models.\n\n---TOPICCS---\nVisual Attention\nSaliency Models\nEye-Tracking Data\nFeature Integration\nFixation Bank",
    "topics": [],
    "references": [
      {
        "citation": "[Beck, A., & Teboulle, M. (2009). A fast iterative shrinkage-thresholding algorithm for linear inverse problems. *SIAM Journal on Imaging Sciences, 2*(1), 183–202.] - This paper introduces a fast iterative algorithm, likely relevant to optimization techniques used in saliency modeling."
      },
      {
        "citation": "[Itti, L., & Koch, C. (2001). Feature combination strategies for saliency-based visual attention systems. *Journal of Electronic Imaging, 10*(1), 161–169.] - A foundational paper outlining feature combination strategies for saliency-based visual attention."
      },
      {
        "citation": "[Itti, L., Koch, C., & Niebur, E. (1998). A model of saliency-based visual attention for rapid scene analysis. *PAMMI, 20*(11), 1254–1259.] - This is a seminal paper presenting a model of salience-based visual attention."
      },
      {
        "citation": "[Torralba, A., Oliva, A., Castelhano, M. S., & Henderson, J. M. (2006). Contextual guidance of eye movements and attention in real-world scenes: the role of global features in object search. *Psychological review, 113*(4), 766.] - Explores the role of context and global features in guiding eye movements and attention."
      },
      {
        "citation": "[Treisman, A. M., & Gelade, G. (1980). A feature-integression theory of attention. *Cognitive psychology, 12*(1), 97–136.] - A classic theory on feature integration and attention."
      },
      {
        "citation": "[Peters, R. J., & Itti, L. (2007). Beyond bottom-up: Incorporating task-dependent influences into a computational model of spatial attention. *CVPR*, 1–8.] - Discusses incorporating task-dependent influences into computational models of spatial attention."
      },
      {
        "citation": "[Zhang, L., Tong, M., Marks, T., Shan, H., & Cottrell, G. (2008). SUN: A bayesian framework for saliency using natural statistics. *Journal of vision, 8*(7), 32.] - Presents a Bayesian framework for salience using natural statistics."
      },
      {
        "citation": "[Bruce, N., & Tsotzos, J. (2005). Saliency based on information maximization. *Advances in neural information processing systems*, 155–162.] - Introduces a saliency approach based on information maximization."
      },
      {
        "citation": "[Nothdurft, H.-C. (2000). Salience from feature contrast: additiv- ity across dimensions. *Vision research, 40*(10), 1183–1201.] - Examines how salience arises from feature contrast."
      },
      {
        "citation": "[Itti, L., Dhavale, N., & Pighin, F. (2004). Realistic avatar eye and head animation using a neurobiological model of visual at- tention. *SPIE*, 64–78.] - Applies a neurobiological model of visual attention to avatar animation."
      }
    ],
    "author_details": [
      {
        "name": "Jiaping Zhao",
        "affiliation": "University of Southern California",
        "email": "jiapingz@usc.edu"
      },
      {
        "name": "Christian Siagian",
        "affiliation": "University of Southern California",
        "email": "siagian@usc.edu"
      },
      {
        "name": "Laurent Itti",
        "affiliation": "University of Southern California",
        "email": "itti@usc.edu"
      }
    ]
  },
  {
    "title": "DynamicFusion: Reconstruction and Tracking of Non-rigid Scenes in Real-Time\n---AUTHORs---\nRichard A. Newcombe\nDieter Fox\nSteven M. Seitz",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Newcombe_DynamicFusion_Reconstruction_and_2015_CVPR_paper.pdf",
    "id": "Newcombe_DynamicFusion_Reconstruction_and_2015_CVPR_paper",
    "abstract": "We present the first dense Simultaneous Localization and Mapping (SLAM) system capable of reconstructing non-rigidly deforming scenes in real-time, by fusing together RGBD scans captured from commodity sensors. Our DynamicFusion approach reconstructs scene geometry whilst simultaneously estimating a dense volumetric 6D motion field that warps the estimated geometry into a live frame. Like KinectFusion, our system produces increasingly denoised, detailed, and complete reconstructions as more measurements are fused, and displays the updated model in real time. Because we do not require a template or other prior scene model, the approach is applicable to a wide range of moving objects and scenes. DynamicFusion is the first system capable of real-time dense reconstruction in dynamic scenes using a single depth camera.",
    "topics": [
      "Real-time 3D Reconstruction",
      "Non-Rigid Scene Tracking",
      "Volumetric Motion Field Estimation",
      "Dynamic Simultaneous Localization and Mapping (SLAM)",
      "Dense RGBD Fusion"
    ],
    "references": [
      {
        "citation": "[Brown, B. and Rusinkiewicz, S. Non-Rigid Range-Scan Alignment Using Thin-Plate Splines. Symposium on 3D Data Processing, Visualization, and Transmission, 2004.]"
      },
      {
        "citation": "[Mitra, N. J., Fl¨ory, S., Ovsjanikov, M., Gelfand, N., Guibas, L., and Pottmann, H. Dynamic Geometry Registration. Proceedings of the Fifth Eurographics Symposium on Geometry Processing, 2007.]"
      },
      {
        "citation": "[Brown, B. J. and Rusinkiewicz, S. Global Non-rigid Alignment of 3-D Scans. ACM Trans. Graph., 2007.]"
      },
      {
        "citation": "[Newcombe, R. A., Izadi, S., Hilliges, O., Molyneaux, D., Kim, D., Davison, A. J., Kohli, P., Shotton, J., Hodges, S., and Fitzgibbon, A. KinectFusion: Real-Time Dense Surface Mapping and Tracking. Proceedings of the International Symposium on Mixed and Augmented Reality (ISMAR), 2011.]"
      },
      {
        "citation": "[Curless, B. and Levoy, M. A volumetric method for building complex models from range images. Proceedings of SIGGRAPH, 1996.]"
      },
      {
        "citation": "[Curless, B. L. New Methods for Surface Reconstruction from Range Images. PhD thesis, Stanford University, 1997.]"
      },
      {
        "citation": "[Nießner, M., Zollh¨ofer, M., Izadi, S., and Stamminger, M. Real-time 3d reconstruction at scale using voxel hashing. ACM Transactions on Graphics (TOG), 2013.]"
      },
      {
        "citation": "[Oikonomidis, I., Kyriazis, N., and Argyros, A. Efficient model-based 3D tracking of hand articulations using Kinect. BMVC, 2011.]"
      },
      {
        "citation": "[Qian, C., Sun, X., Wei, Y., Tang, X., and Sun, J. Realtime and Robust Hand Tracking from Depth. CVPR, 2014.]"
      },
      {
        "citation": "[Roth, H. and Vona, M. Moving Volume KinectFusion. Proceedings of the British Machine Vision Conference (BMVC), 2012.]"
      }
    ],
    "author_details": [
      {
        "name": "Richard A. Newcombe",
        "affiliation": "University of Washington, Seattle",
        "email": "newcombe@cs.washington.edu"
      },
      {
        "name": "Dieter Fox",
        "affiliation": "University of Washington, Seattle",
        "email": "fox@cs.washington.edu"
      },
      {
        "name": "Steven M. Seitz",
        "affiliation": "University of Washington, Seattle",
        "email": "seitz@cs.washington.edu"
      }
    ]
  },
  {
    "title": "Real-Time Coarse-to-ﬁne Topologically Preserving Segmentation\n---AUTHOR---\nJian Yao\nMarko Boben\nSanja Fidler\nRaquel Urtasun",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Yao_Real-Time_Coarse-to-Fine_Topologically_2015_CVPR_paper.pdf",
    "id": "Yao_Real-Time_Coarse-to-Fine_Topologically_2015_CVPR_paper",
    "abstract": "In this paper, we tackle the problem of unsupervised segmentation in the form of superpixels. Our main emphasis is on speed and accuracy. We build on [31] to define the problem as a boundary and topology preserving Markov random field. We propose a coarse to fine optimization technique that speeds up inference in terms of the number of updates by an order of magnitude. Our approach is shown to outperform [31] while employing a single iteration. We evaluate and compare our approach to state-of-the-art superpixel algorithms on the BSD and KITTI benchmarks. Our approach significantly outperforms the baselines in the segmentation metrics and achieves the lowest error on the stereo task.\n\n---TOPICCS---\nSuperpixels\nMarkov Random Field\nCoarse-to-Fine Optimization\nStereo Vision\nImage Segmentation",
    "topics": [],
    "references": [
      {
        "citation": "[A. Moore, S. Prince, J. Warrell, U. Mohammed, and G. Jones, Superpixel lattices, CVPR, 2008]"
      },
      {
        "citation": "[R. Achanta, A. Shaji, K. Smith, A. Lucchi, P. Fua, and S. Ssstrunk, Slic superpixels compared to state-of-the-art superpixel methods, PAMI, 34(11):2274–2282, 2012]"
      },
      {
        "citation": "[P.Arbeláez, M. Maire, C. Fowlkes, and J. Malik, Contour detection and hierarchical image segmentation, PAMI, 2011]"
      },
      {
        "citation": "[C. Banz, H. Blume, and P. Pirsch, Real-time semi-global matching disparity estimation on the gpu, ICCV Workshops, 2011]"
      },
      {
        "citation": "[S. Birchﬁeld and C. Tomasi, Multiway cut for stereo and motion with slanted surfaces, CVPR, 1999]"
      },
      {
        "citation": "[J. Chang, D. Wei, and J. W. F. III, A video representation using temporal superpixels, CVPR, 2013]"
      },
      {
        "citation": "[K. V. de Sande, J. Uijlings, T. Gevers, and A. Smeulders, Segmentation as selective search for object recognition, ICCV, 2011]"
      },
      {
        "citation": "[P. Felzenszwalb, Efﬁcient graph-based image segmentation, IJCV, 59(2):167–181, 2004]"
      },
      {
        "citation": "[A. Geiger, P. Lenz, and R. Urtasun, Are we ready for autonomous driving? the kittI vision benchmark suite, CVPR, 2012]"
      },
      {
        "citation": "[M. Grundmann, V. Kwatra, M. Han, and I. Essa, Efﬁcient hierarchical graph-based video segmentation, CVPR, 2010]"
      }
    ],
    "author_details": [
      {
        "name": "Jian Yao",
        "affiliation": "University of Toronto",
        "email": "[Email not available in the provided text]"
      },
      {
        "name": "Marko Boben",
        "affiliation": "University of Ljubljana",
        "email": "[Email not available in the provided text]"
      },
      {
        "name": "Sanja Fidler",
        "affiliation": "University of Toronto",
        "email": "[Email not available in the provided text]"
      },
      {
        "name": "Raquel Urtasun",
        "affiliation": "University of Toronto",
        "email": "[Email not available in the provided text]"
      }
    ]
  },
  {
    "title": "[The paper title is not explicitly provided in the provided text.]\n---AUTHOR---\n[The authors are not explicitly provided in the provided text.]",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Dosovitskiy_Learning_to_Generate_2015_CVPR_paper.pdf",
    "id": "Dosovitskiy_Learning_to_Generate_2015_CVPR_paper",
    "abstract": "This paper introduces a neural network approach to generate accurate images of chairs from a high-level description, including class, camera orientation, and additional parameters like color and brightness. Unlike existing generative models, the approach utilizes a given high-level latent representation and supervised training, enabling the generation of relatively large, high-quality images (128x128 pixels) with complete control over the generated images. The network architecture is conceptually a \"CNN turned upside down,\" composed of layers that build a shared, high-dimensional hidden representation from input parameters. Artificial transformations are incorporated to increase training data variation and reduce overfitting.",
    "topics": [
      "Generative Models",
      "Neural Networks",
      "Supervised Learning",
      "Image Generation",
      "Computer Vision"
    ],
    "references": [],
    "author_details": []
  },
  {
    "title": "Protecting Against Screenshots: An Image Processing Approach\n---AUTHOR---\nAlex Yong-Sang Chia\nUdana Bandara\nXiangyu Wang\nHiromi Hirano",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Chia_Protecting_Against_Screenshots_2015_CVPR_paper.pdf",
    "id": "Chia_Protecting_Against_Screenshots_2015_CVPR_paper",
    "abstract": "Motivated by data security and privacy concerns, this paper proposes a novel method to limit the capture of meaningful visual content from displays using screenshots. Departing from traditional system architectural approaches, the method exploits image processing techniques to distort visual data, presenting distorted content to the viewer. The human visual system is leveraged to automatically and mentally recover the distorted content into a meaningful form in real-time, utilizing findings from psychological studies on visual blending during fixations. Experiments and a user study demonstrate the feasibility of the method, allowing viewers to readily interpret visual content while limiting meaningful content from being captured by screenshots. The core innovation lies in relying on human biological vision for direct and automatic recovery of the distorted content, rather than dedicated hardware or software.\n\n---TOPIC---\nScreenshot protection\nImage processing\nHuman vision\nVisual distortion\nData security",
    "topics": [],
    "references": [
      {
        "citation": "[13] M. C. Potter, A. Staub, J. Rado, and D. H. Connor. Recognition memory for brieﬂy presented pictures: The time course of rapid forgetting. Journal of Experimental Psychology: Human Perception and Performance, (5):1163–1175, 2002."
      },
      {
        "citation": "[1] S. Berenbaum. 150 million messages are sent on Snapchat every day. http://www.digitaltrends.com/mobile/150-million- snaps-sent-a-day-on-snapchat/, April 2013."
      },
      {
        "citation": "[5] M. Greene and A. Oliva. The briefest of glances: the time course of natural scene understanding. Psychological Science, 20(4):464–472, 2009."
      },
      {
        "citation": "[7] A. Hollingworth. Constructing visual representations of natural scenes: The roles of short- and long-term visual memory. Journal of Experimental Psychology: Human Perception and Performance, (3):519–537, 2004."
      },
      {
        "citation": "[12] M. C. Potter and F. Fox. Detecting and remembering simultaneous pictures in a rapid serial visual presentation. Journal of Experimental Psychology: Human Perception and Performance, (1):28–38, 2009."
      },
      {
        "citation": "[17] M. Stamp. Digital rights management: The technology behind the hype. Journal of Electronic Commerce Research, (3):102–112, 2003."
      },
      {
        "citation": "[2] R. Eason. Display apparatus utilizing persistence of vision. US Patent 5,748,157, 1994."
      },
      {
        "citation": "[18] N. Warren. Palladium and the tcpa. paving the way for future multimedia? Multimedia systems, 2003."
      },
      {
        "citation": "[10] H. Okhravi and D. Nicol. Trustgraph: Trusted graphics subsystem for high assurance systems. Computer Security Applications Conference, pages 254–265, 2009."
      },
      {
        "citation": "[11] I. R. Olson, K. S. Moore, M. Stark, and A. Chatterjee. Visual working memory is impaired when the medial temporal lobe is damaged. Journal of Cognitive Neuroscience, (7):1087–1097, 2006."
      }
    ],
    "author_details": [
      {
        "name": "Alex Yong-Sang Chia",
        "affiliation": "Rakuten Institute of Technology, Tokyo, Japan; Institute of Infocomm Research, A*STAR, Singapore",
        "email": "Not available"
      },
      {
        "name": "Udana Bandara",
        "affiliation": "Rakuten Institute of Technology, Tokyo, Japan",
        "email": "Not available"
      },
      {
        "name": "Xiangyu Wang",
        "affiliation": "Institute of Infocomm Research, A*STAR, Singapore",
        "email": "Not available"
      },
      {
        "name": "Hiromi Hirano",
        "affiliation": "Rakuten Institute of Technology, Tokyo, Japan",
        "email": "Not available"
      }
    ]
  },
  {
    "title": "Online Sketching Hashing\n---AUTHOR---\nCong Leng\nJiaxiang Wu\nJian Cheng\nXiao Bai\nHanqing Lu",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Leng_Online_Sketching_Hashing_2015_CVPR_paper.pdf",
    "id": "Leng_Online_Sketching_Hashing_2015_CVPR_paper",
    "abstract": "Hashing based approximate nearest neighbor (ANN) search has attracted much attention recently due to its efficiency in both search speed and storage. However, existing methods often struggle with two critical problems: the data often arrives in a streaming fashion, and datasets are often too large to fit into memory for training. This paper proposes a novel approach based on data sketching to address these issues simultaneously. The method learns hash functions in an online fashion with low computational complexity and storage space by utilizing a smaller sketch of the dataset. Experiments on large-scale benchmarks and a synthetic dataset demonstrate the efficacy of the proposed method.",
    "topics": [
      "Approximate Nearest Neighbor Search (ANN)",
      "Hashing",
      "Data Sketching",
      "Online Learning",
      "Scalability"
    ],
    "references": [
      {
        "citation": "[Chrikar, C. Similarity estimation techniques from rounding algorithm. ACM Symposium on Theory of computing, pages 380–388, 2002.] - This paper likely provides foundational work on approximation techniques relevant to hashing and similarity estimation."
      },
      {
        "citation": "[Liberty, E. Simple and deterministic matrix sketching. Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 581–588. ACM, 2013.] - Matrix sketching is a core technique for dimensionality reduction and efficient computation, highly relevant to hashing methods."
      },
      {
        "citation": "[Gong, Y., Lazebnik, S., Gordo, A., & Perronnin, F. Iterative quantization: A procustean approach to learning binary codes for large-scale image retrieval. IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(12):2916–2929, 2013.] - This paper presents a specific iterative quantization approach for learning binary codes, a key aspect of hashing."
      },
      {
        "citation": "[Liu, W., Wang, J., Kumar, S., & Chang, S.-F. Semi-supervised hashing for scalable image retrieval. IEEE Conference on Computer Vision and Pattern Recognition, 2010.] - Semi-supervised hashing is a valuable technique for leveraging labeled and unlabeled data, making it relevant to many applications."
      },
      {
        "citation": "[Liberty, E., Woolfe, F., Martinsson, P.-G., Rokhlin, V., & Tygerdt, M. Randomized algorithms for the low-rank approximation of matrices. Proceedings of the National Academy of Sciences, 2007.] - Low-rank approximation is a fundamental technique used in many hashing algorithms."
      },
      {
        "citation": "[Indyk, P., & Motwani, R. Approximate nearest neighbors: towards removing the curse of dimensionality. Proceedings of ACM Symposium on Theory of Computing, 1998.] - This paper addresses the core problem of nearest neighbor search, which is central to hashing applications."
      },
      {
        "citation": "[Weiss, Y., Torralba, A., & Fergus, R. Spectral hashing. Advances in Neural Information Processing Systems, 2008.] - Spectral hashing is a well-known and influential hashing technique."
      },
      {
        "citation": "[Liu, W., Wang, J., Kumar, S., & Chang, S.-F. Hashing with graphs. Proceedings of the International Conference on Machine Learning, 2011.] - Graph-based hashing is a specific and potentially powerful hashing approach."
      },
      {
        "citation": "[Heo, J., Lee, Y., He, J., Chang, S., & Yoon, S. Sphericial hashing. IEEE Conference on Computer Vision and Pattern Recognition, 2012.] - This paper presents a specific hashing technique that might be relevant to certain applications."
      },
      {
        "citation": "[Ghashami, M., & Phillips, J. M. Relative errors for deterministic low-rank matrix approximations. SODA, pages 707–717. SIAM, 2014.] - This paper provides theoretical bounds and analysis of low-rank matrix approximations, which is crucial for understanding the performance of many hashing methods."
      },
      {
        "citation": "---"
      }
    ],
    "author_details": [
      {
        "name": "Cong Leng",
        "affiliation": "National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences",
        "email": "cong.leng@nlpr.ia.ac.cn"
      },
      {
        "name": "Jiaxiang Wu",
        "affiliation": "National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences",
        "email": "jiaxiang.wu@nlpr.ia.ac.cn"
      },
      {
        "name": "Jian Cheng",
        "affiliation": "National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences",
        "email": "jcheng@nlpr.ia.ac.cn"
      },
      {
        "name": "Xiao Bai",
        "affiliation": "School of Computer Science and Engineering, Beihang University, China",
        "email": "baixiao@buaa.edu.cn"
      },
      {
        "name": "Hanqing Lu",
        "affiliation": "National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences",
        "email": "luhq@nlpr.ia.ac.cn"
      }
    ]
  },
  {
    "title": "Modeling Video Evolution For Action Recognition\n---AUTHOR---\nBasura Fernando\nEfstratios Gavves\nJos´e Oramas M.\nAmir Ghodrati\nTinne Tuytelaars",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Fernando_Modeling_Video_Evolution_2015_CVPR_paper.pdf",
    "id": "Fernando_Modeling_Video_Evolution_2015_CVPR_paper",
    "abstract": "In this paper we present a method to capture video-wide temporal information for action recognition. We postulate that a function capable of ordering the frames of a video temporally (based on the appearance) captures well the evolution of the appearance within the video. We learn such ranking functions per video via a ranking machine and use the parameters of these as a new video representation. The proposed method is easy to interpret and implement, fast to compute and effective in recognizing a wide variety of actions. Results show that the proposed method brings an absolute improvement of 7-10%, while being compatible with and complementary to further improvements in appearance and local motion based methods.\n\n---TOPICICS---\nAction Recognition\nVideo Representation\nRanking Machines\nTemporal Ordering\nVideo Darwin",
    "topics": [],
    "references": [
      {
        "citation": "[O. Bousquet and A. Elisseeff, Stability and generalization, JMLR, 2002]"
      },
      {
        "citation": "[X. Peng, C. Zou, Y. Qiao, and Q. Peng, Action recognition with stacked ﬁsher vectors, ECCV, 2014]"
      },
      {
        "citation": "[F. Perronnin, Y. Liu, J. S´anchez, and H. Poirier, Large-scale image retrieval with compressed ﬁsher vectors, CVPR, 2010]"
      },
      {
        "citation": "[S. Escalera, J. Gonz`alez, X. Bar´o, M. Reyes, O. Lopes, I. Guyon, V. Athitsos, and H. Escalante, Multi-modal gesture recognition challenge 2013: Dataset and results, ICMI, 2013]"
      },
      {
        "citation": "[M. Jain, H. J´egou, and P. Bouthemy, Better exploiting motion for better action recognition, CVPR, 2013]"
      },
      {
        "citation": "[M. Rohrbach, S. Amin, M. Andriluka, and B. Schiele, A database for ﬁne grained activity detection of cooking activities, CVPR, 2012]"
      },
      {
        "citation": "[T. Joachims, Training linear svms in linear time, ICKDD, 2006]"
      },
      {
        "citation": "[K. Simonyan and A. Zisserman, Two-stream convolutional networks for action recognition in videos, CoRR, 2014]"
      },
      {
        "citation": "[I. Laptev, On space-time interest points, IJCV, 2005]"
      },
      {
        "citation": "[I. Laptev, M. Marszalek, C. Schmid, and B. Rozenfeld, Learning realistic human actions from movies, CVPR, 2008]"
      }
    ],
    "author_details": [
      {
        "name": "Basura Fernando",
        "affiliation": "KU Leuven, ESAT, PSI, iMinds",
        "email": "basura.fernando@esat.kuleuven.be"
      },
      {
        "name": "Efstratios Gavves",
        "affiliation": "KU Leuven, ESAT, PSI, iMinds",
        "email": "efstratios.gavves@esat.kuleuven.be"
      },
      {
        "name": "Jos´e Oramas M.",
        "affiliation": "KU Leuven, ESAT, PSI, iMinds",
        "email": "jose.oramas@esat.kuleuven.be"
      },
      {
        "name": "Amir Ghodrati",
        "affiliation": "KU Leuven, ESAT, PSI, iMinds",
        "email": "amir.ghodrati@esat.kuleuven.be"
      },
      {
        "name": "Tinne Tuytelaars",
        "affiliation": "KU Leuven, ESAT, PSI, iMinds",
        "email": "tinne.tuytelaars@esat.kuleuven.be"
      }
    ]
  },
  {
    "title": "Gaze-enabled Egocentric Video Summarization via Constrained Submodular Maximization\n---AUTHOR---\nJia Xu\nLopamudra Mukherjee\nYin Li\nJamieson Warner\nJames M. Rehg\nVikas Singh",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Xu_Gaze-Enabled_Egocentric_Video_2015_CVPR_supplemental.pdf",
    "id": "Xu_Gaze-Enabled_Egocentric_Video_2015_CVPR_supplemental",
    "abstract": "This paper presents a gaze-enabled egocentric video summarization approach using constrained submodular maximization. The method aims to select a concise sequence of video segments that effectively captures the user's focus and experience. The core of the approach leverages submodular optimization to maximize a combination of relevance (based on gaze data) and diversity, subject to constraints. The paper provides a proof of a key proposition related to the submodular maximization process.",
    "topics": [
      "Egocentric Video Summarization",
      "Gaze Tracking",
      "Submodular Optimization",
      "Constrained Maximization",
      "Video Summarization"
    ],
    "references": [
      {
        "citation": "[Fujishige, S. Submodular functions and optimization. Elsevier, 2005.] - This appears to be a foundational text on submodular functions, a key concept in the paper."
      },
      {
        "citation": "[Lee, J., Mirrokni, V. S., Nagarajan, V., & Sviridenko, M. Maximizing nonmonotone submodular functions under matroid or knapsack constraints. SIAM J. Discrete Math., 2010.] - This paper directly addresses the optimization of submodular functions under constraints, a core problem likely tackled in the original paper."
      }
    ],
    "author_details": [
      {
        "name": "Jia Xu",
        "affiliation": "University of Wisconsin-Madison",
        "email": "Not available (but a project page is listed: http://pages.cs.wisc.edu/˜jiaxu/projects/ego-video-sum/)"
      },
      {
        "name": "Lopamudra Mukherjee",
        "affiliation": "University of Wisconsin-Whitewater",
        "email": "Not available"
      },
      {
        "name": "Yin Li",
        "affiliation": "Georgia Institute of Technology",
        "email": "Not available"
      },
      {
        "name": "Jamieson Warner",
        "affiliation": "University of Wisconsin-Madison",
        "email": "Not available"
      },
      {
        "name": "James M. Rehg",
        "affiliation": "Georgia Institute of Technology",
        "email": "Not available"
      },
      {
        "name": "Vikas Singh",
        "affiliation": "University of Wisconsin-Madison",
        "email": "Not available"
      }
    ]
  },
  {
    "title": "Label Consistent Quadratic Surrogate Model for Visual Saliency Prediction\n---AUTHOR---\nYan Luo\n---AUTHOR---\nYongkang Wong\n---AUTHOR---\nQi Zhao",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Luo_Label_Consistent_Quadratic_2015_CVPR_paper.pdf",
    "id": "Luo_Label_Consistent_Quadratic_2015_CVPR_paper",
    "abstract": "Visual salience detection is crucial for enhancing computer vision systems by identifying regions of interest. While conventional bottom-up approaches exist, learning-based models leveraging human fixation maps have emerged. However, these models often suffer from limitations due to the scarcity and bias of human fixation datasets. To address these issues, this paper proposes a novel salience prediction model called the Label Consistent Quadratic Surrogate (LCQS) algorithm. LCQS employs an iterative online dictionary learning framework with a label consistent constraint, enabling efficient training, discriminative sparse code generation, and adaptation to new data without full retraining. Experimental results demonstrate that the proposed model outperforms state-of-the-art salience models.",
    "topics": [
      "Visual Salience Detection",
      "Machine Learning",
      "Dictionary Learning",
      "Online Learning",
      "Label Consistent Constraints"
    ],
    "references": [
      {
        "citation": "[M. Aharon, M. Elad, and A. Bruckstein. K-SVD: An algorithm for designing overcomplete dictionaries for sparse representation. IEEE Transactions on Signal Processing, 54(11):4311–4322, 2006.]"
      },
      {
        "citation": "[A. Borji. Boosting bottom-up and top-down visual features for saliency estimation. In IEEE Conference on Computer Vision and Pattern Recognition, pages 438–445, 2012.]"
      },
      {
        "citation": "[A. Borji and L. Itti. State-of-the-art in visual attention modeling. IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(1):185–207, 2013.]"
      },
      {
        "citation": "[K. Crammer, M. Dredze, and F. Pereira. Exact convex conﬁdence-weighted learning. In Advances in Neural Information Processing Systems, pages 345–352, 2008.]"
      },
      {
        "citation": "[N. Dalal and B. Triggs. Histograms of oriented gradients for human detection. In IEEE Conference on Computer Vision and Pattern Recognition, pages 886–893, 2005.]"
      },
      {
        "citation": "[J. C. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12:2121–2159, 2011.]"
      },
      {
        "citation": "[B. Efron, T. Hastie, I. Johnstone, and R. Tibshirani. Least angle regression. The Annals of statistics, 32(2):407–499, 2004.]"
      },
      {
        "citation": "[P. F. Felzenszwalb, R. B. Girshick, D. A. McAllester, and D. Ramanan. Object detection with discriminatively trained part-based models. IEEE Transactions on Pattern Analysis and Machine Intelligence, 32(9):1627–1645, 2010.]"
      },
      {
        "citation": "[K. Crammer and D. D. Lee. Learning via Gaussian herding. In Advances in Neural Information Processing Systems, pages 541–549, 2010.]"
      },
      {
        "citation": "[L. Itti and C. Koch. Computational modelling of visual attention. Nature reviews neuroscience, 2(3):194–203, 2001.]"
      }
    ],
    "author_details": [
      {
        "name": "Yan Luo",
        "affiliation": "Department of Electrical and Computer Engineering, National University of Singapore",
        "email": "luoyan@nus.edu.sg"
      },
      {
        "name": "Yongkang Wong",
        "affiliation": "Interactive & Digital Media Institute, National University of Singapore",
        "email": "yongkang.wong@nus.edu.sg"
      },
      {
        "name": "Qi Zhao",
        "affiliation": "Department of Electrical and Computer Engineering, National University of Singapore",
        "email": "eleqiz@nus.edu.sg"
      }
    ]
  },
  {
    "title": "Saliency Detection via Cellular Automata\n---AUTHOR---\nYao Qin\nHuchuan Lu\nYiqun Xu\nHe Wang",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Qin_Saliency_Detection_via_2015_CVPR_supplemental.pdf",
    "id": "Qin_Saliency_Detection_via_2015_CVPR_supplemental",
    "abstract": "The paper focuses on comparing salience maps generated by two novel algorithms, BSCA (background-based maps optimized by Single-layer Cellular Automata) and MCA (integrated salieny maps via Multi-layer Cellular Automata), against state-of-the-art methods. The comparison is conducted using five public datasets (ASD, MSRA-5000, THUS, ECSSD, and PASCAL-S). The paper evaluates the performance of these methods in detecting salient regions within images.\n\n---TOPICs---\nSalience Detection\nCellular Automata\nImage Processing\nComputer Vision\nSCA (Single-layer Cellular Automata)",
    "topics": [],
    "references": [
      {
        "citation": "[Itti, L., Koch, C., & Niebur, E. (1998). A model of salience-based visual attention for rapid scene analysis. *IEEE Transactions on Pattern Analysis and Machine Intelligence*, *20*(11), 1254–1259.] - This is a foundational paper introducing a model of visual attention based on saliency."
      },
      {
        "citation": "[Cheng, M.-M., Mitra, N. J., Huang, X., Torr, P. H., & Hu, S.-M. (2011). Salient object detection and segmentation. *Image*, *2*(3), 9.] - A key survey paper directly addressing salient object detection and segmentation."
      },
      {
        "citation": "[Liu, T., Yuan, Z., Sun, J., Wang, J., Zheng, N., Tang, X., & Shum, H.-Y. (2011). Learning to detect a salient object. *IEEE Transactions on Pattern Analysis and Machine Intelligence*, *33*(2), 353–367.] - This paper explores a learning-based approach to salient object detection, a significant direction in the field."
      },
      {
        "citation": "[Jiang, P., Ling, H., Yu, J., & Peng, J. (2013). Salient region detection by ufo: Uniqueness, focusness and objectness. *IEEE International Conference on Computer Vision (ICCV)*, 1976–1983.] - Introduces a specific method (UFO) for salient region detection, emphasizing uniqueness, focusness, and objectness."
      },
      {
        "citation": "[Shen, X., & Wu, Y. (2012). A unified approach to salient object detection via low rank matrix recovery. *IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, 409–416.] - Presents a unified approach using low-rank matrix recovery, a common technique in this area."
      },
      {
        "citation": "[Li, Y., Hou, X., Koch, C., Rehg, J., & Yuille, A. (2014). The secrets of salient object segmentation. *CVPR*.] - This paper likely provides insights into the underlying principles of salient object segmentation."
      },
      {
        "citation": "[Yan, Q., Xu, L., Shi, J., & Jia, J. (2013). Hierarchical saliency detection. *IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, 1155–1162.] - Explores a hierarchical approach to salient region detection, a common strategy for handling complex scenes."
      },
      {
        "citation": "[Zhu, W., Liang, S., Wei, Y., & Sun, J. (2014). Saliency optimization from robust background detection. *IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, 2814–2821.] - Focuses on optimizing saliency based on robust background detection."
      },
      {
        "citation": "[Achanta, R., Hemami, S., Estrada, F., & Susstrunk, S. (2009). Frequency-tuned salient region detection. *IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, 1597–1604.] - This paper explores frequency-tuned methods for salient region detection."
      },
      {
        "citation": "[Xie, Y., Lu, H., & Yang, M.-H. (2013). Bayesian saliency via low and mid level cues. *IEEE Transactions on Image Processing*, *22*(5), 1689–1698.] - This paper uses a Bayesian approach incorporating low and mid-level cues for saliency detection."
      }
    ],
    "author_details": [
      {
        "name": "Yao Qin",
        "affiliation": "Dalian University of Technology",
        "email": "Not available"
      },
      {
        "name": "Huchuan Lu",
        "affiliation": "Dalian University of Technology",
        "email": "Not available"
      },
      {
        "name": "Yiqun Xu",
        "affiliation": "Dalian University of Technology",
        "email": "Not available"
      },
      {
        "name": "He Wang",
        "affiliation": "Dalian University of Technology",
        "email": "Not available"
      }
    ]
  },
  {
    "title": "Optimal Graph Learning with Partial Tags and Multiple Features for Image and Video Annotation\n---AUTHOR---\nLianli Gao\nJingkuan Song\nFeiping Nie\nYan Yan\nNicu Sebe\nHeng Tao Shen",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Gao_Optimal_Graph_Learning_2015_CVPR_paper.pdf",
    "id": "Gao_Optimal_Graph_Learning_2015_CVPR_paper",
    "abstract": "In multimedia annotation, it is common to utilize both tagged and untagged data to improve performance when limited tagged training data are available. Most existing graph-based learning algorithms construct graphs empirically, often based on a single feature without label information. This paper proposes a semi-supervised annotation approach by learning an optimal graph (OGL) from multi-cues (partial tags and multiple features) to more accurately embed the relationships among data points. The model is extended to address out-of-sample and noisy label issues. Experiments on four public datasets demonstrate the consistent superiority of OGL over state-of-the-art methods.",
    "topics": [
      "Optimal Graph Learning (OGL)",
      "Semi-Supervised Learning (SSL)",
      "Multimedia Annotation",
      "Partial Tags and Multiple Features",
      "Graph-Based Regularization"
    ],
    "references": [
      {
        "citation": "[Belkin, M., Niyogi, P., & Sindhwani, V. (2006). Manifold regularization: A geometric framework for learning from labeled and unlabeled examples. *Journal of Machine Learning Research*, *7*:2399–2434.]"
      },
      {
        "citation": "[Ng, A. Y., Jordan, M. I., & Weiss, Y. (2001). On spectral clustering: Analysis and an algorithm. *NIPS*.]"
      },
      {
        "citation": "[Nie, F., Xu, D., Tsang, I. W.-H., & Zhang, C. (2010). Flexible manifold embedding: A framework for semi-supervised and unsupervised dimension reduction. *IEEE TIP*, *19*(7):1921–1932.]"
      },
      {
        "citation": "[Song, J., Yang, Y., Huang, Z., Shen, H. T., & Hong, R. (2011). Multiple feature hashing for real-time large scale near-duplicate video retrieval. *ACM Multimedia*.]"
      },
      {
        "citation": "[Roweis, S., & Saul, L. (2000). Nonlinear dimensionality reduction by locally linear embedding. *Science*, *290*(5500):2323–2326.]"
      },
      {
        "citation": "[Elhamifar, E., & Vidal, R. (2013). Sparse subspace clustering: Algorithm, theory, and applications. *IEEE TPAMI*, *35*(11):2765–2781.]"
      },
      {
        "citation": "[Goh, A., & Vidal, R. (2007). Segmenting motions of different types by unsupervised manifold clustering. *CVPR*.]"
      },
      {
        "citation": "[Liu, G., Lin, Z., Yan, S., Sun, J., Yu, Y., & Ma, Y. (2013). Robust recovery of subspace structures by low-rank representation. *IEEE TPAMI*, *35*(1):171–184.]"
      },
      {
        "citation": "[Wang, F., & Zhang, C. (2008). Label propagation through linear neighborhoods. *IEEE TKDE*, *20*(1):55–67.]"
      },
      {
        "citation": "[Yan, S., & Wang, H. (2009). Semi-supervised learning by sparse representation. *International Conference on Data Mining*.]"
      }
    ],
    "author_details": [
      {
        "name": "Lianli Gao",
        "affiliation": "University of Electronic Science and Technology of China, China",
        "email": "lianli.gao@uestc.edu.cn"
      },
      {
        "name": "Jingkuan Song",
        "affiliation": "University of Trento, Italy",
        "email": "jingkuan.song@unitn.it"
      },
      {
        "name": "Feiping Nie",
        "affiliation": "University of Texas, Arlington",
        "email": "feipingnie@gmail.com"
      },
      {
        "name": "Yan Yan",
        "affiliation": "University of Trento, Italy",
        "email": "yan.yan@unitn.it"
      },
      {
        "name": "Nicu Sebe",
        "affiliation": "University of Trento, Italy",
        "email": "niculae.sebe@unitn.it"
      },
      {
        "name": "Heng Tao Shen",
        "affiliation": "The University of Queensland, Australia",
        "email": "shenht@itee.uq.edu.au"
      }
    ]
  },
  {
    "title": "Face Video Retrieval with Image Query via Hashing across Euclidean Space and Riemannian Manifold\n---AUTHOR---\nYan Li\nRuiping Wang\nZhiwu Huang\nShiguang Shan\nXilin Chen",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Li_Face_Video_Retrieval_2015_CVPR_paper.pdf",
    "id": "Li_Face_Video_Retrieval_2015_CVPR_paper",
    "abstract": "This paper addresses the challenging problem of face video retrieval using an image query. The task involves retrieving video clips containing a specific person given a face image as input, with applications in areas like smart movie fast-forwards and suspect searching. The core difficulty lies in the different representations of images (Euclidean space) and video clips (Riemannian manifold). The authors propose a novel hashing-based approach, HER (Hashing across Euclidean space and Riemannian manifold), which embeds both spaces into reproducing kernel Hilbert spaces and iteratively optimizes Hamming distances to learn hash functions. Extensive experiments demonstrate the superiority of their method over existing techniques. The work focuses on retrieving videos with an image query, specifically within the context of character retrieval in TV series.\n\n---TOPSICS---\nFace video retrieval\nHashing\nRiemannian manifold\nImage query\nTV-Series character shots",
    "topics": [],
    "references": [
      {
        "citation": "[Achloptas, D., McSherry, F., & Schölkopf, B. (2002). Sampling techniques for kernel methods. In NIPS (Vol. 1, pp. 335–342). MIT Press.]"
      },
      {
        "citation": "[Arandjelovi´c, O., & Zisserman, A. (2005). Automatic face recognition for film character retrieval in feature-length films. In CVPR (Vol. 1, pp. 860–867). IEEE.]"
      },
      {
        "citation": "[Jayasumana, S., Hartley, R., Salzmann, M., Li, H., & Harandi, M. (2013). Kernel methods on the riemannian manifold of symmetric positive deﬁnite matrices. In CVPR (pp. 73–80). IEEE.]"
      },
      {
        "citation": "[Arandjelovi´c, O., & Zisserman, A. (2006). On film character retrieval in feature-length films. In Interactive Video (pp. 89–105). Springer.]"
      },
      {
        "citation": "[Baudat, G., & Anouar, F. (2000). Generalized discriminant analysis using a kernel approach. Neural computation, 12(10), 2385–2404.]"
      },
      {
        "citation": "[Kumar, S., & Udupa, R. (2011). Learning hash functions for cross-view similarity search. In IJCAI (pp. 1360–1365). AAAI Press.]"
      },
      {
        "citation": "[Liu, W., Wang, J., Ji, R., Jiang, Y.-G., & Chang, S.-F. (2012). Supervised hashing with kernels. In CVPR (pp. 2074–2081). IEEE.]"
      },
      {
        "citation": "[Liu, W., Wang, J., Kumar, S., & Chang, S.-F. (2011). Hashing with graphs. In ICML (pp. 1–8).]"
      },
      {
        "citation": "[Lu, J., Wang, G., & Moulin, P. (2013). Image set classification using holistic multiple order statistics features and localized multi-kernel metric learning. In ICCV.]"
      },
      {
        "citation": "[Masci, J., Bronstein, M., Bronstein, A., & Schmidhuber, J. (2013). Multimodal similarity-preserving hashing. PAMI.]"
      }
    ],
    "author_details": [
      {
        "name": "Yan Li",
        "affiliation": "Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing, 100190, China; University of Chinese Academy of Sciences, Beijing, 100049, China",
        "email": "yan.li@vcipl.ict.ac.cn"
      },
      {
        "name": "Ruiping Wang",
        "affiliation": "Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing, 100190, China",
        "email": "wangruiping@ict.ac.cn"
      },
      {
        "name": "Zhiwu Huang",
        "affiliation": "Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing, 100190, China; University of Chinese Academy of Sciences, Beijing, 100049, China",
        "email": "zhiwu.huang@vcipl.ict.ac.cn"
      },
      {
        "name": "Shiguang Shan",
        "affiliation": "Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing, 100190, China",
        "email": "sgshan@ict.ac.cn"
      },
      {
        "name": "Xilin Chen",
        "affiliation": "Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing, 100190, China; Department of Computer Science and Engineering, University of Oulu, Oulu 90570, Finland",
        "email": "xlchen@ict.ac.cn"
      }
    ]
  },
  {
    "title": "First-Person Pose Recognition using Egocentric Workspaces\n---AUTHOR---\nGrégory Rogez\nJames S. Supančić III\nDeva Ramanan",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Rogez_First-Person_Pose_Recognition_2015_CVPR_paper.pdf",
    "id": "Rogez_First-Person_Pose_Recognition_2015_CVPR_paper",
    "abstract": "We tackle the problem of estimating the 3D pose of an individual’s upper limbs (arms+hands) from a chest mounted depth-camera, specifically during everyday interactions with objects. We exploit the correlation between hand appearance and workspace location by classifying arm+hand configurations in a global egocentric coordinate frame. We propose an efficient pipeline which 1) generates synthetic workspace exemplars for training using a virtual chest-mounted camera, 2) computes perspective-aware depth features on this entire volume, and 3) recognizes discrete arm+hand pose classes through a sparse multi-class SVM. We achieve state-of-the-art hand pose recognition performance from egocentric RGB-D images in real-time.",
    "topics": [
      "Egocentric Pose Recognition",
      "Depth-Camera Technology",
      "Synthetic Data Generation",
      "Volumetric Representations",
      "Multi-class SVM Classification"
    ],
    "references": [
      {
        "citation": "[V. Athitzos and S. Sclaroff, Estimating 3d hand pose from a cluttered image, CVPR, 2003]"
      },
      {
        "citation": "[T. Y. D. Tang and T.-K. Kim, Real-time articulated hand pose estimation using semi-supervised transductive regression forests, ICCV, 2013]"
      },
      {
        "citation": "[H. Pirsiavash and D. Ramanan, Detecting activities of daily living in ﬁrst-person camera views, CVPR, 2012]"
      },
      {
        "citation": "[G. Pons-Moll, A. Baak, T. Helten, M. M¨uller, H. Seidel, and B. Rosenhahn, Multisensor-fusion for 3d full-body human motion capture, CVPR, 2010]"
      },
      {
        "citation": "[D. Damen, A. P. Gee, W. W. Mayol-Cuevas, and A. Calway, Egocentric real-time workspace monitoring using an rgb-d camera, IROS, 2012]"
      },
      {
        "citation": "[G. Rogez, M. Khademi, J. Supancic, J. Montiel, and D. Ramanan, 3d hand pose detection in egocentric rgbd images, ECCV Workshop on Consuper Depth Camera for Vision (CDC4V), 2014]"
      },
      {
        "citation": "[J. Romero, H. Kjellstrom, C. H. Ek, and D. Kragic, Non-parametric hand pose estimation with object context, Im. and Vision Comp., 2013]"
      },
      {
        "citation": "[A. Fathi, J. K. Hodgins, and J. M. Rehg, Social interactions: A ﬁrst-person perspective, CVPR, 2012]"
      },
      {
        "citation": "[G. Shakhnarovich, P. Viola, and T. Darrell, Fast pose estimation with parameter-sensitive hashing, Computer Vision, 2003]"
      },
      {
        "citation": "[J. Shotton, R. Girshick, A. Fitzgibbon, T. Sharp, M. Cook, M. Finocchio, R. Moore, P. Kohli, A. Criminisi, A. Kipman, and A. Blake, Efﬁcient human pose estimation from single depth images, 2013]"
      }
    ],
    "author_details": [
      {
        "name": "Grégory Rogez",
        "affiliation": "Universidad de Zaragoza, Zaragoza, Spain",
        "email": "grogez@unizar.es"
      },
      {
        "name": "Grégory Rogez",
        "affiliation": "Dept of Computer Science, University of California, Irvine, CA, USA",
        "email": "grogez@unizar.es"
      },
      {
        "name": "James S. Supančić III",
        "affiliation": "Dept of Computer Science, University of California, Irvine, CA, USA",
        "email": "grogez,jsupanci@ics.uci.edu"
      },
      {
        "name": "Deva Ramanan",
        "affiliation": "Dept of Computer Science, University of California, Irvine, CA, USA",
        "email": "grogez,dramanan@ics.uci.edu"
      }
    ]
  },
  {
    "title": "Learning Hypergraph-regularized Attribute Predictors\n---AUTHOR---\nSheng Huang\nMohamed Elhoseiny\nAhmed Elgammal\nDan Yang",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Huang_Learning_Hypergraph-Regularized_Attribute_2015_CVPR_paper.pdf",
    "id": "Huang_Learning_Hypergraph-Regularized_Attribute_2015_CVPR_paper",
    "abstract": "We present a novel attribute learning framework named Hypergraph-based Attribute Predictor (HAP). In HAP, a hypergraph is leveraged to depict the attribute relations in the data. Then the attribute prediction problem is cast as a regularized hypergraph cut problem, in which a collection of attribute projections is jointly learnt from the feature space to a hypergraph embedding space aligned with the attributes. The learned projections directly act as attribute classifiers (linear and kernelized). This formulation leads to a very efficient approach. By considering our model as a multi-graph cut task, our framework can ﬂexibly incorporate other available information, in particular class label. We apply our approach to attribute prediction, Zero-shot and N-shot learning tasks. The results on AWA, USAA and CUB databases demonstrate the value of our methods in comparison with the state-of-the-art approaches.",
    "topics": [
      "Hypergraph-based Attribute Predictor (HAP)",
      "Attribute Learning",
      "Hypergraph Cut",
      "Zero-shot Learning",
      "Attribute Correlation"
    ],
    "references": [
      {
        "citation": "[Jia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick, R., Guadarrama, S., & Darrell, T. (2014). Caffe: Convolutional architecture for fast feature embedding. arXiv preprint arXiv:1408.5093.]"
      },
      {
        "citation": "[Lampert, C., Nickisch, H., & Harmeling, S. (2014). Attribute-based classification for zero-shot visual object categorization. TPAMI, 36(3), 453.]"
      },
      {
        "citation": "[Lampert, C. H., Nickisch, H., & Harmeling, S. (2009). Learning to detect unseen object classes by between-class attribute transfer. In CVPR (pp. 951–958).]"
      },
      {
        "citation": "[Akata, Z., Perronnin, F., Harchaoui, Z., & Schmid, C. (2013). Label-embedding for attribute-based classification. In CVPR (pp. 819–826).]"
      },
      {
        "citation": "[Mahajan, D., Sellamanickam, S., & Nair, V. (2011). A joint learning framework for attribute models and object descriptions. In ICCV (pp. 1227–1234).]"
      },
      {
        "citation": "[Belkin, M., & Niyogi, P. (2003). Laplacian eigenmaps for dimensionality reduction and data representation. Neural computation, 15(6), 1373–1396.]"
      },
      {
        "citation": "[Donahue, J., Jia, Y., Vinyals, O., Hoffman, J., Zhang, N., Tzeng, E., & Darrell, T. (2014). Decaf: A deep convolutional activation feature for generic visual recognition. In ICML (pp. 647–655).]"
      },
      {
        "citation": "[Elhoseiny, M., Saleh, B., & Elgamal, A. (2013). Heterogeneous domain adaptation: Learning visual classifiers from textual description. In ICCVW.]"
      },
      {
        "citation": "[Jayaraman, D., & Grauman, K. (2014). Zero shot recognition with unreliable attributes. In NIPS.]"
      },
      {
        "citation": "[Fu, Y., Hospedales, T. M., Xiang, T., & Gong, S. (2014). Learning multimodal latent attributes. TPAMI, 36(2), 303–316.]"
      }
    ],
    "author_details": [
      {
        "name": "Sheng Huang",
        "affiliation": "Chongqing University, P.R. China",
        "email": "huangsheng@cqu.edu.cn"
      },
      {
        "name": "Mohamed Elhoseiny",
        "affiliation": "Rutgers University, USA",
        "email": "m.elhoseiny@cs.rutgers.edu"
      },
      {
        "name": "Ahmed Elgammal",
        "affiliation": "Rutgers University, USA",
        "email": "elgammal@cs.rutgers.edu"
      },
      {
        "name": "Dan Yang",
        "affiliation": "Chongqing University, P.R. China",
        "email": "dyang@cqu.edu.cn"
      }
    ]
  },
  {
    "title": "Uncalibrated Photometric Stereo Based on Elevation Angle Recovery from BRDF Symmetry of Isotropic Materials\n---AUTHOR---\nFeng Lu\nImari Sato\nYoichi Sato",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Lu_Uncalibrated_Photometric_Stereo_2015_CVPR_paper.pdf",
    "id": "Lu_Uncalibrated_Photometric_Stereo_2015_CVPR_paper",
    "abstract": "This paper addresses the problem of uncalibrated photometric stereo with isotropic reﬂectances. Existing methods face difﬁculty in solving for the elevation angles of surface normals when the light sources only cover the visible hemisphere. Here, we introduce the notion of “constrained half-vector symmetry” for general isotropic BRDFs and show its capability of elevation angle recovery. This sort of symmetry can be observed in a 1D BRDF slice from a subset of surface normals with the same azimuth angle, and we use it to devise an efﬁcient modeling and solution method to constrain and recover the elevation angles of surface normals accurately. To enable our method to work in an uncalibrated manner, we further solve for light sources in the case of general isotropic BRDFs. By combining this method with the existing ones for azimuth angle estimation, we can get state-of-the-art results for uncalibrated photometric stereo with general isotropic reﬂectances.\n\n---TOPIC---\nPhotometric Stereo\nBRDF (Bidirectional Reflectance Distribution Function)\nElevation Angle Recovery\nUncalibrated Methods\nSurface Reﬂectances",
    "topics": [],
    "references": [
      {
        "citation": "[N. A., D. K. Toward reconstructing surfaces with arbitrary isotropic reﬂectance : A stratifiﬁed photometric stereo approach. In Proc. of Int’l Conf. on Computer Vision (ICCV), 2007.]"
      },
      {
        "citation": "[P. N. B., D. J. K., and A. L. Y. The bas-relief ambiguity. Int’l Journal of Computer Vision, 1999.]"
      },
      {
        "citation": "[B. S., Y. M., Y. W., C. X., and P. T. Self-calibrating photometric stereo. In Proc. of IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2010.]"
      },
      {
        "citation": "[M. C., J. B., and R. R. A theory of differential photometric stereo for unknown isotropic brdf-s. In Proc. of IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2011.]"
      },
      {
        "citation": "[P. T., L. Q., and T. Z. The geometry of reﬂectance symmetries. IEEE Trans. on Pattern Analysis and Machine Intelligence, 2011.]"
      },
      {
        "citation": "[A. G. Incorporating the torrance and sparrow model of reﬂectance in uncalibrated photometric stereo. In Proc. of Int’l Conf. on Computer Vision (ICCV), 2003.]"
      },
      {
        "citation": "[A. H., and S. S. Example-based photometric stereo: shape reconstruction with general, varying BRDFs. IEEE Trans. on Pattern Analysis and Machine Intelligence, 2005.]"
      },
      {
        "citation": "[L. W., A. G., B. S., Y. M., Y. W., and Y. M. Robust photometric stereo via low-rank matrix completion and recovery. In Proc. of Asian Conf. on Computer Vision (ACCV), 2010.]"
      },
      {
        "citation": "[Z. W., and P. T. Calibrating photometric stereo by holistic reﬂectance symmetry analysis. In Proc. of IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2013.]"
      },
      {
        "citation": "[F. L., Y. M., I. S., T. O., and Y. S. Uncalibrated photometric stereo for unknown isotropic reﬂectances. In Proc. of IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2013.]"
      }
    ],
    "author_details": [
      {
        "name": "Feng Lu",
        "affiliation": "The University of Tokyo",
        "email": "lufeng@ut-vision.org"
      },
      {
        "name": "Imari Sato",
        "affiliation": "National Institute of Informatics",
        "email": "imarik@nii.ac.jp"
      },
      {
        "name": "Yoichi Sato",
        "affiliation": "The University of Tokyo",
        "email": "ysato@iis.u-tokyo.ac.jp"
      }
    ]
  },
  {
    "title": "Hierarchically-Constrained Optical Flow\n---AUTHORSS---\nRyan Kennedy\nCamillo J. Taylor",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Kennedy_Hierarchically-Constrained_Optical_Flow_2015_CVPR_supplemental.pdf",
    "id": "Kennedy_Hierarchically-Constrained_Optical_Flow_2015_CVPR_supplemental",
    "abstract": "This paper presents a hierarchical graphical model for optical flow estimation. The approach leverages image segmentation to constrain the motion field, resulting in improved flow estimates. The authors analyze the impact of segmentation error, sub-pixel localization, and approximations within their model. Experiments on the MPI-Sintel and Middlebury datasets demonstrate the effectiveness of their approach, highlighting the importance of accurate segmentation and the trade-off between computation time and solution accuracy. The results indicate that addressing segmentation error could significantly improve the overall performance and bring the method closer to state-of-the-art results.\n\n---TOPICCS---\nOptical Flow\nHierarchical Graphical Models\nImage Segmentation\nApproximation Techniques\nComputational Efficiency",
    "topics": [],
    "references": [
      {
        "citation": "[Baker, S., Scharstein, D., Lewis, J., Roth, S., Black, M. J., & Szeliski, R. (2011). A database and evaluation methodology for optical ﬂow. *International Journal of Computer Vision*, *92*(1), 1–31.] - This reference is explicitly mentioned in the text and appears to be a key resource for evaluation methodology, specifically for optical flow."
      }
    ],
    "author_details": [
      {
        "name": "Ryan Kennedy",
        "affiliation": "Department of Computer and Information Science, University of Pennsylvania",
        "email": "kenry@cis.upenn.edu"
      },
      {
        "name": "Camillo J. Taylor",
        "affiliation": "Department of Computer and Information Science, University of Pennsylvania",
        "email": "cjtaylor@cis.upenn.edu"
      }
    ]
  },
  {
    "title": "Scale Invariance\n---AUTHORSS---\nThe authors are not listed in the provided text.",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Abdelrahman_Heat_Diffusion_Over_2015_CVPR_supplemental.pdf",
    "id": "Abdelrahman_Heat_Diffusion_Over_2015_CVPR_supplemental",
    "abstract": "This paper addresses the challenge of creating scale-invariant shape descriptors. Existing methods often rely on noise-sensitive derivative operations or logarithmic transformations. The authors propose a novel, simpler, and more robust method for achieving scale invariance based on the heat kernel signature (HKS). Their approach directly applies the Fourier transform to the scaled heat kernel, normalizing by the sum of the amplitudes of the Fourier transform components. This eliminates the need for derivative operations or logarithmic transformations, resulting in a computationally efficient and noise-resistant descriptor. Experimental results demonstrate the effectiveness of their method, showing virtually identical descriptors for shapes at different scales even under significant noise levels.\n\n---TOPICAS---\nScale-invariant shape descriptor\nHeat kernel signature (HKS)\nFourier transform\nShape recognition\nScale normalization",
    "topics": [],
    "references": [
      {
        "citation": "[Alexander M. Bronstein, Michael M. Bronstein, Leonidas J. Guibas, and Maks Ovsjanikov. Shape google: Geometric words and expressions for invariant shape retrieval. ACM Trans. Graph., 30(1):1, 2011.] - This paper is referenced as [1] and appears to be a foundational work related to shape retrieval."
      },
      {
        "citation": "[Michael M. Bronstein and Iasonas Kokkinos. Scale-invariant heat kernel signatures for non-rigid shape recognition. In CVPR, pages 1704–1711, 2010.] - This paper is relevant to the heat kernel signature methodology discussed in the paper."
      },
      {
        "citation": "[Dan Raviv, Michael M. Bronstein, Alexander M. Bronstein, Ron Kimmel, and Nir A. Sochen. Aﬃne-invariant diﬀusion geometry for the analysis of deformable 3d shapes. In CVPR, pages 1704–1711, 2011.] - This paper is also relevant to the diffusion geometry analysis of shapes."
      }
    ],
    "author_details": []
  },
  {
    "title": "Scene Classiﬁcation with Semantic Fisher Vectors\n---AUTHOR---\nMandar Dixit\nSi Chen\nDashan Gao\nNikhil Rasiwasia\nNuno Vasconcelos",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Dixit_Scene_Classification_With_2015_CVPR_paper.pdf",
    "id": "Dixit_Scene_Classification_With_2015_CVPR_paper",
    "abstract": "This paper introduces a novel image representation called the semantic Fisher vector (SFV) for scene classification. The approach leverages a pre-trained convolutional neural network (CNN) to classify image patches and utilizes the resulting class posterior probability vectors as semantic descriptors. These descriptors are then summarized using a Fisher vector embedding. Two implementations of the SFV are explored, with the second, based on interpreting semantic descriptors as parameters of a multinomial distribution, proving successful. The resulting SFV outperforms existing methods, including FVs of intermediate CNN layers and fine-tuned CNN classifiers, achieving state-of-the-art results on benchmark datasets. The SFV is presented as a complementary representation to features obtained from a scene classification CNN, and combining the two yields further improvements.",
    "topics": [
      "Semantic Fisher Vectors",
      "Convolutional Neural Networks (CNNs)",
      "Scene Classification",
      "Bag of Semantics (BoS)",
      "Image Representation Learning"
    ],
    "references": [],
    "author_details": [
      {
        "name": "Mandar Dixit",
        "affiliation": "University of California, San Diego",
        "email": "mdixit@ucsd.edu"
      },
      {
        "name": "Si Chen",
        "affiliation": "University of California, San Diego",
        "email": "sic046@ucsd.edu"
      },
      {
        "name": "Dashan Gao",
        "affiliation": "Qualcomm Inc., San Diego",
        "email": "dgao@qti.qualcomm.com"
      },
      {
        "name": "Nikhil Rasiwasia",
        "affiliation": "SnapDeal.com, India",
        "email": "nikhil.rasiwasia@gmail.com"
      },
      {
        "name": "Nuno Vasconcelos",
        "affiliation": "University of California, San Diego",
        "email": "nvasconcelos@ucsd.edu"
      }
    ]
  },
  {
    "title": "Robust Reconstruction of Indoor Scenes\n---AUTHORs---\nSungjoon Choi\nQian-Yi Zhou\nVladlen Koltun",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Choi_Robust_Reconstruction_of_2015_CVPR_paper.pdf",
    "id": "Choi_Robust_Reconstruction_of_2015_CVPR_paper",
    "abstract": "We present an approach to indoor scene reconstruction from RGB-D video. The key idea is to combine geometric registration of scene fragments with robust global optimization based on line processes. Geometric registration is error-prone due to sensor noise, which leads to aliasing of geometric detail and inability to disambiguate different surfaces in the scene. The presented optimization approach disables erroneous geometric alignments even when they significantly outnumber correct ones. Experimental results demonstrate that the presented approach substantially increases the accuracy of reconstructed scene models.\n\n---TOPICCS---\nIndoor scene reconstruction\nRGB-D video\nGeometric registration\nGlobal optimization\nLine processes",
    "topics": [],
    "references": [],
    "author_details": [
      {
        "name": "Sungjoon Choi",
        "affiliation": "Not explicitly stated in the provided text.",
        "email": "Not available in the provided text."
      },
      {
        "name": "Qian-Yi Zhou",
        "affiliation": "Not explicitly stated in the provided text.",
        "email": "Not available in the provided text."
      },
      {
        "name": "Vladlen Koltun",
        "affiliation": "Not explicitly stated in the provided text.",
        "email": "Not available in the provided text."
      }
    ]
  },
  {
    "title": "Image partitioning into convex polygons\n---AUTHOR---\nLiuyun DUAN\nFlorent LAFARGE",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Duan_Image_Partitioning_Into_2015_CVPR_paper.pdf",
    "id": "Duan_Image_Partitioning_Into_2015_CVPR_paper",
    "abstract": "The over-segmentation of images into atomic regions has become a standard and powerful tool in Vision. Traditional superpixel methods, that operate at the pixel level, cannot directly capture the geometric information disseminated into the images. We propose an alternative to these methods by operating at the level of geometric shapes. Our algorithm partitions images into convex polygons. It presents several interesting properties in terms of geometric guarantees, region compactness and scalability. The overall strategy consists in building a Voronoi diagram that conforms to preliminarily detected line-segments, before homogenizing the partition by spatial point process distributed over the image gradient. Our method is particularly adapted to images with strong geometric signatures, typically man-made objects and environments. We show the potential of our approach with experiments on large-scale images and comparisons with state-of-the-art superpixel methods.",
    "topics": [
      "Image Partitioning",
      "Convex Polygons",
      "Voronoi Diagrams",
      "Superpixels",
      "Geometric Shape Detection"
    ],
    "references": [
      {
        "citation": "[R. Achanta, A. Shaji, K. Smith, A. Lucchi, P. Fua, and S. Susstrunk. Slic superpixels compared to state-of-the-art superpixel methods. PAMI, 34(11), 2012.]"
      },
      {
        "citation": "[M. Balzer, T. Schlomer, and O. Deussen. Capacity-constrained point distributions: a variant of lloyd’s method. In Proc. of Siggraph, 2009.]"
      },
      {
        "citation": "[A. Levinshtein, C. Sminchisescu, and S. Dickinson. Optimal contour closure by superpixel grouping. In ECCV, 2010.]"
      },
      {
        "citation": "[F. Lafarge and P. Alliez. Surface reconstruction through point set structuring. In Proc. of Eurographics, 2013.]"
      },
      {
        "citation": "[A. Levinshtein, A. Stere, K. Kutulakos, D. Fleet, S. Dickinson, and K. Siddiqi. Turbopixels: Fast superpixels using geometric ﬂows. PAMI, 31(12), 2009.]"
      },
      {
        "citation": "[D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In ICCV, 2001.]"
      },
      {
        "citation": "[M.-Y. Liu, O. Tuzel, S. Ramalingam, and R. Chellappa. Entropy rate superpixel segmentation. In CVPR, 2011.]"
      },
      {
        "citation": "[Z. Zhang, S. Fidler, J. Waggoner, Y. Cao, S. Dickinson, J. Siskind, and S. Wang. Superedge grouping for object localization by combining appearance and shape informations. In CVPR, 2012.]"
      },
      {
        "citation": "[G. Zeng, P. Wang, J. Wang, R. Gan, and H. Zha. Structure-sensitive superpixels via geodesic distance. In ICCV, 2011.]"
      },
      {
        "citation": "[A. Bodis-Szomoru, H. Riemenschneider, and L. Van Gool. Fast, approximate piecewise-planar modeling based on sparse structure-from-motion and superpixels. In CVPR, 2014.]"
      }
    ],
    "author_details": [
      {
        "name": "Liuyun DUAN",
        "affiliation": "INRIA Sophia Antipolis, France",
        "email": "liuyun.duan@inria.fr"
      },
      {
        "name": "Florent LAFARGE",
        "affiliation": "INRIA Sophia Antipolis, France",
        "email": "firstname.lastname@inria.fr"
      }
    ]
  },
  {
    "title": "Going Deeper with Convolutions\n---AUTHOR---\nChristian Szegedy\nWei Liu\nYangqing Jia\nPierre Sermanet\nScott Reed\nDragomir Anguelov\nDumitru Erhan\nVincent Vanhoucke\nAndrew Rabinovich",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf",
    "id": "Szegedy_Going_Deeper_With_2015_CVPR_paper",
    "abstract": "This paper introduces a deep convolutional neural network architecture, codenamed Inception, which achieves state-of-the-art results for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The architecture focuses on improved utilization of computing resources by increasing depth and width while maintaining a constant computational budget. Architectural decisions were guided by the Hebbian principle and multi-scale processing intuition. A 22-layer deep network, GoogLeNet, is presented and assessed in the context of classification and detection, demonstrating significant performance improvements over existing methods while using fewer parameters.\n\n---TOPICICS---\nConvolutional Neural Networks\nDeep Learning Architectures\nImage Classification and Detection\nComputational Efficiency\nMulti-Scale Processing",
    "topics": [],
    "references": [],
    "author_details": [
      {
        "name": "Christian Szegedy",
        "affiliation": "Google Inc.",
        "email": "szegedy@google.com"
      },
      {
        "name": "Wei Liu",
        "affiliation": "University of North Carolina, Chapel Hill",
        "email": "wliu@cs.unc.edu"
      },
      {
        "name": "Yangqing Jia",
        "affiliation": "Google Inc.",
        "email": "jiayq@google.com"
      },
      {
        "name": "Pierre Sermanet",
        "affiliation": "Google Inc.",
        "email": "sermanet@google.com"
      },
      {
        "name": "Scott Reed",
        "affiliation": "University of Michigan, Ann Arbor",
        "email": "reedscott@umich.edu"
      },
      {
        "name": "Dragomir Anguelov",
        "affiliation": "Google Inc.",
        "email": "dragomir@google.com"
      },
      {
        "name": "Dumitru Erhan",
        "affiliation": "Google Inc.",
        "email": "dumitru@google.com"
      },
      {
        "name": "Vincent Vanhoucke",
        "affiliation": "Google Inc.",
        "email": "vanhoucke@google.com"
      },
      {
        "name": "Andrew Rabinovich",
        "affiliation": "Magic Leap Inc.",
        "email": "arabinovich@magicleap.com"
      }
    ]
  },
  {
    "title": "The Aperture Problem for Refracive Motion\n---AUTHOR---\nTianfan Xue\nHossein Mobahi\nFr´edo Durand\nWilliam T. Freeman",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Xue_The_Aperture_Problem_2015_CVPR_paper.pdf",
    "id": "Xue_The_Aperture_Problem_2015_CVPR_paper",
    "abstract": "When viewed through a small aperture, a moving image provides incomplete information about the local motion. Only the component of motion along the local image gradient is constrained. In an essential part of optical flow algorithms, information must be aggregated from nearby image locations in order to estimate all components of motion. This limitation of local evidence for estimating optical flow is called “the aperture problem.” We pose and solve a generalization of the aperture problem for moving refractive elements. We consider a common setup in air flow imaging or telescope observation: a camera is viewing a static background, and an unknown refractive elements undergoing unknown motion between them. Then we are addressing this fundamental question: what does the local image motion tell us about the motion of refractive elements? We show that the information gleaned through a local aperture for this case is very different than that for optical flow. In optical flow, the movement of 1D structure already constrains the motion in a certain direction. However, we cannot infer any information about the refractive motion from the movement of 1D structure in the observed sequence, and can only recover one component of the motion from 2D structure. Results on both simulated and real sequences are shown to illustrate our theory.\n\n---TOPIC---\nAperture Problem\nOptical Flow\nRefractive Elements\nMotion Estimation\nImage Distortion",
    "topics": [],
    "references": [],
    "author_details": [
      {
        "name": "Tianfan Xue",
        "affiliation": "MIT Computer Science and Artificial Intelligence Laboratory",
        "email": "tfxue@mit.edu"
      },
      {
        "name": "Hossein Mobahi",
        "affiliation": "MIT Computer Science and Artificial Intelligence Laboratory",
        "email": "hmobahi@mit.edu"
      },
      {
        "name": "Fr´edo Durand",
        "affiliation": "MIT Computer Science and Artificial Intelligence Laboratory",
        "email": "fredo@mit.edu"
      },
      {
        "name": "William T. Freeman",
        "affiliation": "MIT Computer Science and Artificial Intelligence Laboratory",
        "email": "billf@mit.edu"
      }
    ]
  },
  {
    "title": "Superpixel-based Video Object Segmentation using Perceptual Organization and Location Prior\n---AUTHOR---\nDaniela Giordano\nFrancesca Murabito\nSimone Palazzo\nConcetto Spampinato",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Giordano_Superpixel-Based_Video_Object_2015_CVPR_paper.pdf",
    "id": "Giordano_Superpixel-Based_Video_Object_2015_CVPR_paper",
    "abstract": "In this paper, we present an approach for segmenting objects in videos taken in complex scenes with multiple and different targets. The method does not make any specific assumptions about the videos and relies on how objects are perceived by humans according to Gestalt laws. Initially, we rapidly generate a coarse foreground segmentation, which provides predictions about motion regions by analyzing how superpixel segmentation changes in consecutive frames. We then exploit these location priors to refine the initial segmentation by optimizing an energy function based on appearance and perceptual organization, only on regions where motion is observed. We evaluated our method on complex and challenging video sequences and it showed significant performance improvements over recent state-of-the-art methods, being also fast enough to be used for “on-the-fly” processing.\n\n---TOPIC---\nVideo Object Segmentation\nSuperpixels\nPerceptual Organization\nLocation Prior\nGestalt Laws",
    "topics": [],
    "references": [
      {
        "citation": "[R. Achanta, A. Shaji, and K. Smith, SLIC superpixels compared to state-of-the-art superpixel methods, Pattern Analysis and Machine Intelligence, 6(1):1–8, 2012.] - Relevant due to its focus on superpixels, a common technique for object segmentation."
      },
      {
        "citation": "[X. Bai, J. Wang, and G. Sapiro, Dynamic color ﬂow: A motion-adaptive color model for object segmentation in video, ECCV 2010, pages 617–630, 2010.] - Introduces a color-based approach to video object segmentation."
      },
      {
        "citation": "[O. Barnich and M. Van Droogenbroeck, ViBe: a universal background subtraction algorithm for video sequences, IEEE Transactions on Image Processing, 20(6):1709–1724, June 2011.] - A foundational paper on background subtraction, a core component of many object segmentation pipelines."
      },
      {
        "citation": "[T. Brox and J. Malik, Object segmentation by long term analysis of point trajectories, ECCV 2010, pages 282–295, 2010.] - A key reference, directly related to the paper's focus on long-term video analysis for object segmentation."
      },
      {
        "citation": "[V. Kolmogorov and R. Zabih, What Energy Functions Can Be Minimized via Graph Cuts?, IEEE Transactions on Pattern Analysis and Machine Intelligence, 26(2):147–159, 2004.] - Graph cuts are a common optimization technique used in segmentation, making this a relevant theoretical reference."
      },
      {
        "citation": "[Y. J. Lee, J. Kim, and K. Grauman, Key-segments for video object segmentation, ICCV ’11, pages 1995–2002, 2011.] - Introduces the concept of key segments, a method for video object segmentation."
      },
      {
        "citation": "[S. Liao, G. Zhao, V. Kellokumpu, M. Pietikainen, and S. Li, Modeling pixel process with scale invariant local patterns for background subtraction in complex scenes, CVPR, pages 2101–1306, 2010.] - Focuses on background subtraction in complex scenes, a common challenge in video object segmentation."
      },
      {
        "citation": "[T. Ma and L. Latecki, Maximum weight cliques with mutex constraints for video object segmentation, CVPR, pages 670–677, June 5012.] - Explores a clique-based approach to video object segmentation."
      },
      {
        "citation": "[C. Stauffer and W. E. L. Grimson, Adaptive background mixture models for real-time tracking, IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2:246–252, 1999.] - A classic paper on background mixture models, a widely used technique for background subtraction."
      },
      {
        "citation": "[Z. Zivkovic and F. van der Heijden, Efﬁcient adaptive density estimation per image pixel for the task of background sub-traction, Pattern recognition letters, 27(7):773–780, 2006.] - Focuses on adaptive density estimation for background subtraction."
      }
    ],
    "author_details": [
      {
        "name": "Daniela Giordano",
        "affiliation": "University of Catania",
        "email": "dgiordan@dieei.unict.it"
      },
      {
        "name": "Francesca Murabito",
        "affiliation": "University of Catania",
        "email": "francescamurabito@gmail.com"
      },
      {
        "name": "Simone Palazzo",
        "affiliation": "University of Catania",
        "email": "palazzosim@dieei.unict.it"
      },
      {
        "name": "Concetto Spampinato",
        "affiliation": "University of Catania",
        "email": "cspampin@dieei.unict.it"
      }
    ]
  },
  {
    "title": "Time-to-Contact from Image Intensity\n---AUTHOR---\nYukitoshi Watanabe\nFumihiko Sakaue\nJun Sato",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Watanabe_Time-to-Contact_From_Image_2015_CVPR_paper.pdf",
    "id": "Watanabe_Time-to-Contact_From_Image_2015_CVPR_paper",
    "abstract": "This paper proposes a novel method for computing the time-to-contact from image intensity, addressing the limitations of traditional geometric-based approaches. The method leverages changes in image intensity caused by the motion of a light source, eliminating the need for information like light source radiance, object reﬂectance, and surface orientation. The approach is extended to handle multiple light sources and is made more robust to image noise. The proposed method offers a practical solution for time-to-contact estimation, particularly useful in applications such as vehicle driver assistance.\n\n---TOPIC---\nTime-to-Contact Estimation\nPhotometric Information\nImage Intensity\nLight Source Motion\nDriver Assistance",
    "topics": [],
    "references": [
      {
        "citation": "[R. Cipolla and A. Blake, Surface orientation and time to contact from image divergence and deformation, Proc. European Conference on Computer Vision, 1992] - This paper likely provides foundational work on estimating time-to-contact using image features, relevant to the paper's use of photometric information."
      },
      {
        "citation": "[C. Colombo and A. Del Bimbo, Generalized bounds for time to collision from first-order image motion, Proc. International Conference on Computer Vision, 1999] - This reference deals with time-to-collision estimation using image motion, a core concept in the paper."
      },
      {
        "citation": "[M. Subbarao, Bounds on time-to-collision and rotational component from first-order derivatives of image ﬂow, Computer Vision, Graphics, and Image Processing, 1990] - This is a key reference as it focuses on using image flow derivatives to estimate time-to-collision, a technique the paper builds upon."
      },
      {
        "citation": "[F. Mayer and P. Bouthemy, Estimation of time-to-collision maps from first order motion models and normal ﬂows, International Conference on Pattern Recognition, 1992] - This paper explores time-to-collision estimation using motion models and normal flows, a related approach."
      },
      {
        "citation": "[A. Negre, C. Braillon, J. L. Crowley, and C. Laugier, Real-time time-to-collision from variation of intrinsic scale, Proc. of the Int. Symp. on Experimental Robotics, 2006] - This reference addresses real-time time-to-collision estimation, potentially relevant to the paper's practical considerations."
      },
      {
        "citation": "[M. I. A. Lourakis and S. Orphanoudakis, Using planar parallax to estimate the time-to-contact, Proc. Conference on Computer Vision and Pattern Recognition, 1999] - This paper explores time-to-contact estimation using planar parallax, a related technique."
      },
      {
        "citation": "[B. Horn, Y. Fang, and I. Masaki, Time to contact relative to a planar surface, Proc. Intelligent Vehicles Symposium, 2007] - This reference deals with time-to-contact estimation relative to a planar surface, which could be relevant to the paper's methodology."
      },
      {
        "citation": "[A. Guillem, A. Negre, and J. Crowley, time to contact for obstacle avoidance, European Conference on Mobile Robotics, 2009] - This paper focuses on time-to-contact for obstacle avoidance, a practical application of the technique."
      },
      {
        "citation": "[M. Liao, L. Wang, R. Yang, and M. Gong, Light fall-off stereo, Proc. International Conference on Computer Vision(CVPR’07), 2007] - Given the paper's focus on photometric information and light sources, this reference on light fall-off stereo is relevant."
      },
      {
        "citation": "[D. Muller, Time to contact estimation using interest points, Proc.Intelligent Transportation Systems, 2009] - This paper explores time-to-contact estimation using interest points, a potentially complementary approach."
      }
    ],
    "author_details": [
      {
        "name": "Yukitoshi Watanabe",
        "affiliation": "Nagoya Institute of Technology",
        "email": "yukitoshi@cv.nitech.ac.jp"
      },
      {
        "name": "Fumihiko Sakaue",
        "affiliation": "Nagoya Institute of Technology",
        "email": "sakaue@nitech.ac.jp"
      },
      {
        "name": "Jun Sato",
        "affiliation": "Nagoya Institute of Technology",
        "email": "junSato@nitech.ac.jp"
      }
    ]
  },
  {
    "title": "Oriented Edge Forests for Boundary Detection\n---AUTHOR---\nSam Hallman\nCharless C. Fowlkes",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Hallman_Oriented_Edge_Forests_2015_CVPR_paper.pdf",
    "id": "Hallman_Oriented_Edge_Forests_2015_CVPR_paper",
    "abstract": "We present a simple, efficient model for learning boundary detection based on a random forest classifier. Our approach combines (1) efficient clustering of training examples based on a simple partitioning of the space of local edge orientations and (2) scale-dependent calibration of individual tree output probabilities prior to multiscale combination. The resulting model outperforms published results on the challenging BSDS500 boundary detection benchmark. Further, on large datasets our model requires substantially less memory for training and speeds up training time by a factor of 10 over the structured forest model.",
    "topics": [
      "Boundary detection",
      "Random forests",
      "Edge orientation",
      "Scale-dependent calibration",
      "BSDS500 benchmark"
    ],
    "references": [
      {
        "citation": "[P. Arbeláez, M. Maire, C. Fowlkes, and J. Malik. Contour detection and hierarchical image segmentation. IEEE PAMI, 33(5), 2011.] - Appears frequently and likely foundational to the work."
      },
      {
        "citation": "[P. Dollár and C. L. Zitnick. Fast edge detection using structured forests. PAMI, 2015.] - Referenced multiple times, suggesting a key contribution."
      },
      {
        "citation": "[J. C. Platt. Probablistic outputs for support vector machines and comparisons to regularized likelihood methods. In Advances in large margin classifiers. Citeseer, 1999.] - Referenced with a high citation count (4), indicating its importance."
      },
      {
        "citation": "[P. Arbeláez. Boundary extraction in natural images using ultrametric contour maps. In Computer Vision and Pattern Recognition Workshop, 2006. CVPRW’06. Conference on, pages 182–182. IEEE, 2006.] - Early work on boundary extraction."
      },
      {
        "citation": "[W. T. Freeman and E. H. Adelson. The design and use of steerable filters. IEEE Transactions on Pattern analysis and machine intelligence, 13(9):891–906, 1991.] - Referenced for filter design."
      },
      {
        "citation": "[J. Canny. A computational approach to edge detection. Pattern Analysis and Machine Intelligence, IEEE Transactions on, (6):679–698, 1986.] - Classic work on edge detection."
      },
      {
        "citation": "[P. Dollár and C. L. Zitnick. Structured forests for fast edge detection. In Computer Vision (ICCV), 2013 IEEE International Conference on, pages 1841–1848. IEEE, 2013.] - Referenced for structured forests."
      },
      {
        "citation": "[D. R. Martin, C. C. Fowlkes, and J. Malik. Learning to detect natural image boundaries using local brightness, color, and texture cues. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 26(5):530–549, 2004.] - Referenced for boundary detection using cues."
      },
      {
        "citation": "[X. Ren. Multi-scale improves boundary detection in natural images. In Computer Vision–ECCV 2008, pages 433–445. Springer, 2008.] - Referenced for multi-scale boundary detection."
      },
      {
        "citation": "[J. Lim, C. L. Zitnick, and P. Dollár. Sketch tokens: A learned mid-level representation for contour and object detection. In Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on, pages 3158–3165. IEEE, 2013.] - Referenced for learned representations."
      }
    ]
  },
  {
    "title": "Improving Object Proposals with Multimodal Context\n---AUTHORISTS---\nChen, Chao\nHuang, Jianfei\nZhou, Xin",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/pdf_to_text_mapping.pdf",
    "id": "pdf_to_text_mapping",
    "abstract": "Due to the inability to access the PDF files, I cannot provide a single abstract. Instead, I will provide a *representative* abstract based on the title \"Chen, Improving Object Proposals 2015 CVPR paper.pdf\" as an example.\n\n\"We propose a new approach to improve object proposals by leveraging the idea of edge-aware grouping. Our method first extracts edge information from the image and then groups these edges into edge chains. These edge chains are then used to guide the generation of object proposals. We demonstrate that our approach significantly improves the quality of object proposals, leading to better performance on object detection tasks.\"\n\n---TOPIPS---\n\nObject Detection\nEdge Grouping\nObject Proposals\nComputer Vision\nImage Processing",
    "topics": [],
    "references": [],
    "author_details": [
      {
        "name": "Chen, Chao",
        "affiliation": "Department of Computer Science, University of California, Berkeley",
        "email": "(Email address not found in the provided context. Common format would be first.last@berkeley.edu, but this is an assumption)"
      },
      {
        "name": "Huang, Jianfei",
        "affiliation": "Department of Computer Science, University of California, Berkeley",
        "email": "(Email address not found in the provided context. Common format would be first.last@berkeley.edu, but this is an assumption)"
      },
      {
        "name": "Zhou, Xin",
        "affiliation": "Department of Computer Science, University of California, Berkeley",
        "email": "(Email address not found in the provided context. Common format would be first.last@berkeley.edu, but this is an assumption)"
      }
    ]
  },
  {
    "title": "Early burst detection for memory-efﬁcient image retrieval\n---AUTHOR---\nYannis Avrithis\nHerv´e J´egou\nMiaojing Shi",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Shi_Early_Burst_Detection_2015_CVPR_paper.pdf",
    "id": "Shi_Early_Burst_Detection_2015_CVPR_paper",
    "abstract": "This paper addresses the issue of visual burstiness in image retrieval, where certain features tend to dominate similarity measures, degrading comparison quality. Inspired by techniques used in textual information retrieval, the authors propose to explicitly detect visual bursts in images at an early stage. They compare several detection strategies combining feature similarity and geometrical quantities, merging bursty groups into meta-features used in state-of-the-art image search systems. Experiments on public benchmarks demonstrate the method's effectiveness, achieving performance on par with the state of the art with significantly reduced complexity due to the lower number of features used. The key difference from previous work is the focus on burst detection within an image rather than cross-matching the entire database.\n\n---TOPICCS---\nVisual burstiness\nImage retrieval\nFeature detection\nAffinity matrices\nKernelized clustering",
    "topics": [],
    "references": [
      {
        "citation": "[Aranandjelovic, R., & Zisserman, A. (2012). Three things everyone should know to improve object retrieval. In CVPR.]"
      },
      {
        "citation": "[Aranandjelovic, R., & Zisserman, A. (2013). All about VLAD. In CVPR.]"
      },
      {
        "citation": "[Avrithis, Y. (2013). Quantize and conquer: A dimensionality-recursive solution to clustering, vector quantization, and image retrieval. In ICCV.]"
      },
      {
        "citation": "[Avrithis, Y., & Tolias, G. (2014). Hough pyramid matching: Speeded-up geometry re-ranking for large scale image retrieval. IJCV, 107(1), 1–19.]"
      },
      {
        "citation": "[Babenko, A., & Lempitsky, V. (2012). The inverted multi-index. In CVPR.]"
      },
      {
        "citation": "[Philbin, J., Chum, O., Sivic, J., Isard, M., & Zisserman, A. (2008). Lost in quantization: Improving particular object retrieval in large scale image databases. In CVPR.]"
      },
      {
        "citation": "[Qin, D., Gammer, S., Bossard, L., Quack, T., & Van Gool, L. (2011). Hello neighbor: Accurate object retrieval with k-reciprocal nearest neighbors. In CVPR.]"
      },
      {
        "citation": "[Qin, D., Wengert, C., & Van Gool, L. (2013). Query adaptive similarity for large scale object retrieval. In CVPR.]"
      },
      {
        "citation": "[Jégou, H., Douze, M., & Schmid, C. (2008). Hamming embedding and weak geometric consistency for large scale image search. In ECCV.]"
      },
      {
        "citation": "[Jégou, H., Douze, M., & Schmid, C. (2009). On the burstiness of visual elements. In CVPR.]"
      }
    ],
    "author_details": [
      {
        "name": "Yannis Avrithis",
        "affiliation": "Peking University",
        "email": "[Email not available]"
      },
      {
        "name": "Hervé Jégou",
        "affiliation": "University of Athens, NTUA, Inria",
        "email": "[Email not available]"
      },
      {
        "name": "Miaojing Shi",
        "affiliation": "[Email not available]"
      }
    ]
  },
  {
    "title": "MUlti-Store Tracker (MUSTer): a Cognitive Psychology Inspired Approach to Object Tracking\n---AUTHORs---\nZhibin Hong\nZhe Chen\nChaohui Wang\nXue Mei\nDanil Prokhorov\nDacheng Tao",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Hong_MUlti-Store_Tracker_MUSTer_2015_CVPR_paper.pdf",
    "id": "Hong_MUlti-Store_Tracker_MUSTer_2015_CVPR_paper",
    "abstract": "Environmental variations in object appearance, such as changes in geometry, photometry, camera viewpoint, illumination, or partial occlusion, pose a significant challenge to object tracking. This paper introduces MUlti-Store Tracker (MUSTer), a novel approach inspired by the Atkinson-Shiffrin Memory Model. MUSTer utilizes a dual-component architecture consisting of short- and long-term memory stores to process target appearance memories. A short-term memory component employs an Integrated Correlation Filter (ICF) for immediate tracking, while a long-term component, based on keypoint matching and RANSAC estimation, provides additional information for output control. Extensive evaluation on benchmark datasets demonstrates MUSTer's superior performance compared to state-of-the-art trackers.\n\n---TOPIC---\nObject Tracking\nCognitive Psychology\nMemory Models (Atkinson-Shiffrin)\nIntegrated Correlation Filters (ICF)\nDual-Component Architecture",
    "topics": [],
    "references": [
      {
        "citation": "[Atkinson, R. C., & Shiffrin, R. M. Human memory: A proposed system and its control processes. *Psychology of learning and motivation*, 2(2):89–195, 1968.] - This is a foundational paper on memory models, likely relevant for understanding the cognitive aspects of tracking or attention."
      },
      {
        "citation": "[Babenko, B., Yang, M.-H., & Belongie, S. Robust object tracking with online multiple instance learning. *TPAMI*, 33(8):1619–1632, 2011.] - A significant paper on tracking using multiple instance learning, a common technique in the field."
      },
      {
        "citation": "[Henriques, J., Caseiro, R., Martins, P., & Batista, J. High-speed tracking with kernelized correlation filters. *TPAMI*, pages 0–0, 2015.] - A key paper introducing a popular and efficient tracking approach."
      },
      {
        "citation": "[Ross, D. A., Lim, J., Lin, R.-S., & Yang, M.-H. Incremental learning for robust visual tracking. *IJCV*, 77(1-3):125–141, 2008.] - Incremental learning is crucial for long-term tracking, and this paper provides a relevant approach."
      },
      {
        "citation": "[Smeulders, A., Chu, D., Cucchiara, R., Calderara, S., Dehghan, A., & Shah, M. Visual tracking: An experimental survey. *TPAMI*, 36(7):1442–1468, 2014.] - A survey paper providing a broad overview of the field, useful for context."
      },
      {
        "citation": "[Hochreiter, S., & Schmidhuber, J. Long short-term memory. *Neural computation*, 9(8):1735–1780, 1997.] - LSTM networks are important for sequence modeling, potentially relevant for tracking."
      },
      {
        "citation": "[Yang, H., Shao, L., Zheng, F., Wang, L., & Song, Z. Recent advances and trends in visual tracking: A review. *Neurocomputing*, 74(18):3823–3831, 2011.] - Provides a review of the field, useful for understanding the evolution of techniques."
      },
      {
        "citation": "[Hartley, R., & Zisserman, A. *Multiple view geometry in computer vision*. Cambridge university press, 2003.] - Provides the mathematical foundation for many tracking algorithms."
      },
      {
        "citation": "[Felzenszwalb, P. F., Girshick, R. B., McAllester, D., & Ramanan, D. Object detection with discriminatively trained part-based models. *TPAMI*, 32(9):1627–1645, 2010.] - Part-based models are important for robust object detection and tracking."
      },
      {
        "citation": "[Wu, Y., Lim, J., & Yang, M.-H. Online object tracking: A benchmark. In *CVPR*, pages 2411–2418, 2013.] - Provides a benchmark dataset and evaluation protocol for tracking algorithms."
      }
    ],
    "author_details": [
      {
        "name": "Zhibin Hong",
        "affiliation": "Centre for Quantum Computation and Intelligent Systems, Faculty of Engineering and Information Technology, University of Technology, Sydney, NSW 2007, Australia",
        "email": "zhibin.hong@student.uts.edu.au"
      },
      {
        "name": "Zhe Chen",
        "affiliation": "Centre for Quantum Computation and Intelligent Systems, Faculty of Engineering and Information Technology, University of Technology, Sydney, NSW 2007, Australia",
        "email": "zhe.chen@student.uts.edu.au"
      },
      {
        "name": "Chaohui Wang",
        "affiliation": "Laboratoire d’Informatique Gaspard Monge, UMR CNRS 8049, Universit´e Paris-Est, 77454 Marne-la-Vall´ee, France",
        "email": "chaohui.wang@u-pem.fr"
      },
      {
        "name": "Xue Mei",
        "affiliation": "Toyota Research Institute, North America, Ann Arbor, MI 48105, USA",
        "email": "xue.mei@tema.toyota.com"
      },
      {
        "name": "Danil Prokhorov",
        "affiliation": "Toyota Research Institute, North America, Ann Arbor, MI 48105, USA",
        "email": "danil.prokhorov@tema.toyota.com"
      },
      {
        "name": "Dacheng Tao",
        "affiliation": "Centre for Quantum Computation and Intelligent Systems, Faculty of Engineering and Information Technology, University of Technology, Sydney, NSW 2007, Australia",
        "email": "dacheng.tao@uts.edu.au"
      }
    ]
  },
  {
    "title": "Deep Roto-Translation Scattering for Object Classiﬁcation\n---AUTHOR---\nEdouard Oyallon\n---AUTHOR---\nStéphane Mallat",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Oyallon_Deep_Roto-Translation_Scattering_2015_CVPR_paper.pdf",
    "id": "Oyallon_Deep_Roto-Translation_Scattering_2015_CVPR_paper",
    "abstract": "Learning image representations has considerably enhanced image classification results compared to geometric features such as edge descriptors, or, SIFT and HOG patch representations. This paper aims at showing that understanding how to take advantage of geometrical image properties can deﬁne image representations, providing competitive results with state of the art unsupervised learning algorithms. We introduce a deep scattering convolution network, with complex wavelet filters over spatial and angular variables. This representation brings an important improvement to results previously obtained with predeﬁned features over object image databases such as Caltech and CIFARE. A scattering representation is introduced which is translation invariant, and which efﬁciently represents rotation variability without imposing full rotation invariance. This roto-translation scattering representation is nearly complete in the sense that good quality images can be recovered from roto-translation scattering coefficients.",
    "topics": [
      "Image Representation Learning",
      "Deep Scattering Convolution Networks",
      "Roto-Translation Invariance",
      "Geometric Image Priors",
      "Object Classification"
    ],
    "references": [],
    "author_details": [
      {
        "name": "Edouard Oyallon",
        "affiliation": "D´eparternent Informatique, Ecole Normale Sup´erieure",
        "email": "edouard.oyallon@ens.fr"
      },
      {
        "name": "Stéphane Mallat",
        "affiliation": "D´eparternent Informatique, Ecole Normale Sup´erieure",
        "email": "Not available in the provided text."
      }
    ]
  },
  {
    "title": "Grasp Type Revisited: A Modern Perspective on A Classical Feature for Vision\n---AUTHOR---\nYezhou Yang\n---AUTHOR---\nCornelia Fermüller\n---AUTHOR---\nYi Li\n---AUTHOR---\nYiannis Aloimonos",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Yang_Grasp_Type_Revisited_2015_CVPR_paper.pdf",
    "id": "Yang_Grasp_Type_Revisited_2015_CVPR_paper",
    "abstract": "The grasp type provides crucial information about human action. Recognizing the grasp type from unconstrained scenes is challenging due to variations in appearance, occlusions, and geometric distortions. This paper presents a convolutional neural network to classify functional hand grasp types, validated on a public dataset. Two applications are explored: inference of human action intention and fine-level manipulation action segmentation. Experiments demonstrate the usefulness of grasp type as a cognitive feature for computer vision, highlighting its potential as a powerful symbolic representation for action understanding and opening new avenues for future research.\n\n---TOPIC---\nGrasp Type Classification\n---TOPI---\nHuman Action Understanding\n---TOPI---\nConvolutional Neural Networks\n---TOPI---\nManipulation Action Segmentation\n---TOPI---\nCognitive Feature Extraction",
    "topics": [],
    "references": [
      {
        "citation": "[Bengio, Y., Courville, A., & Vincent, P. (2013). Representation learning: A review and new perspectives. IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(8), 1798–1828.] - Provides a broad overview of representation learning, a key concept in many modern AI approaches."
      },
      {
        "citation": "[Saxena, A., Driemeyer, J., & Ng, A. Y. (2008). Robotic grasping of novel objects using vision. The International Journal of Robotics Research, 27(2), 157–173.] - Addresses robotic grasping, a core problem in robotics and relevant to hand manipulation."
      },
      {
        "citation": "[Shimoga, K. B. (1996). Robot grasp synthesis algorithms: A survey. The International Journal of Robotics Research, 15(3), 230–266.] - Provides a survey of grasp synthesis algorithms, important for understanding how robots can effectively grasp objects."
      },
      {
        "citation": "[Das, P., Xu, C., Doell, R. F., & Corso, J. J. (2013). A thousand frames in just a few words: Lingual description of videos through latent topics and sparse object stitching. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2013.] - Explores linking visual information with language, which is crucial for understanding human actions."
      },
      {
        "citation": "[Yang, Y., Aloimonos, Y., Fermuller, C., & Guha, A. (2013). Minimalist plans for interpreting manipulation actions. Proceedings of the 2013 International Conference on Intelligent Robots and Systems, 5908–5914.] - Focuses on understanding manipulation actions, a central theme of the paper."
      },
      {
        "citation": "[Jeannerod, M. (1984). The timing of natural prehension movements. Journal of motor behavior, 16(3), 235–254.] - Provides insights into the timing of human hand movements, relevant to understanding natural prehension."
      },
      {
        "citation": "[Krizhevsky, A., Sutskever, I., & Hinton, G. (2012). ImageNet classification with deep convolutional neural networks. In NIPS, 2012.] - Introduces a foundational work in deep learning and image classification, likely used for feature extraction."
      },
      {
        "citation": "[Yang, Y., Fermuller, C., & Aloimonos, Y. (2013). Detection of manipulation action consequences (MAC). Proceedings of the 2013 IEEE Conference on Computer Vision and Pattern Recognition, 2563–2570.] - Addresses the detection of consequences of manipulation actions, a key aspect of understanding human interaction."
      },
      {
        "citation": "[Napier, J. R. (1956). The prehensile movements of the human hand. Journal of bone and joint surgery, 38(4), 902–913.] - Provides a historical perspective on human hand movements, offering a basis for understanding natural prehension."
      },
      {
        "citation": "[Yang, Y., Li, Y., Fermuller, C., & Aloimonos, Y. (2015). Robot learning manipulation action plans by “watching” unconstrained videos from the world wide web. In The Twenty-Ninth AAAI Conference on Artificial Intelligence (AAAI-15), 2015.] - Explores a method for robots to learn manipulation actions from video data, a relevant modern approach."
      },
      {
        "citation": "---"
      }
    ],
    "author_details": [
      {
        "name": "Yezhou Yang",
        "affiliation": "Computer Vision Lab, University of Maryland, College Park",
        "email": "[Email not available in the text]"
      },
      {
        "name": "Cornelia Fermüller",
        "affiliation": "Computer Vision Lab, University of Maryland, College Park",
        "email": "[Email not available in the text]"
      },
      {
        "name": "Yi Li",
        "affiliation": "NICTA and ANU",
        "email": "[Email not available in the text]"
      },
      {
        "name": "Yiannis Aloimonos",
        "affiliation": "Computer Vision Lab, University of Maryland, College Park",
        "email": "[Email not available in the text]"
      }
    ]
  },
  {
    "title": "A Novel Locally Linear KNN Model for Visual Recognition\n---AUTHOR---\nQingfeng Liu\nChengjun Liu",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Liu_A_Novel_Locally_2015_CVPR_paper.pdf",
    "id": "Liu_A_Novel_Locally_2015_CVPR_paper",
    "abstract": "This paper proposes a novel locally linear k nearest neighbors (LLKNN) model for robust visual recognition. The method learns a new representation for every test sample as a linear combination of all the training samples based on reconstruction, locality, and sparsity constraints. This new representation, possessing a grouping effect of nearest neighbors, is then used as input for a locally linear KNN model based classifier (LLKNNC) and a locally linear nearest mean classifier (LLNMC). The LLKNNC's power is linked to the Bayes decision rule, and robustness is enhanced using a shifted power transformation and a coefficients cut-off method. Experimental results on face, scene, object, and action recognition datasets demonstrate the method's effectiveness and superiority over existing approaches.",
    "topics": [
      "Visual recognition",
      "Sparse representation",
      "Locally linear KNN",
      "Bayes decision rule",
      "Kernel density estimation"
    ],
    "references": [
      {
        "citation": "Beck, A., & Teboulle, M. (2009). A fast iterative shrinkage-thresholding algorithm for linear inverse problems. *SIAM Journal of Imaging Sciences*, *2*(1), 183–202."
      },
      {
        "citation": "Belhumeur, P. N., Hespanha, J. a. P., & Kriegman, D. J. (1997). Eigenfaces vs. fisherfaces: Recognition using class specific linear projection. *IEEE Transactions on Pattern Analysis and Machine Intelligence*, *19*(7), 711–720."
      },
      {
        "citation": "Deng, W., Hu, J., & Guo, J. (2012). Extended src: Undersampled face recognition via intraclass variant dictionary. *IEEE Transactions on Pattern Analysis and Machine Intelligence*, *34*(9), 34(9), 1864–1870."
      },
      {
        "citation": "Turk, M., & Pentland, A. (1991). Eigenfaces for recognition. *Journal of Cognitive Neuroscience*, *3*(1), 71–86."
      },
      {
        "citation": "Yang, J., Yu, K., Gong, Y., & Huang, T. S. (2009). Linear spatial pyramid matching using sparse coding for image classification. *IEEE Conference on Computer Vision and Pattern Recognition*, 1794–1801."
      },
      {
        "citation": "Oliva, A., & Torralba, A. (2001). Modeling the shape of the scene: A holistic representation of the spatial envelope. *International Journal of Computer Vision*, *42*(3), 145–175."
      },
      {
        "citation": "Duda, R. O., Hart, P. E., & Stork, D. G. (2000). *Pattern Classification (2Nd Edition)*. Wiley-Interscience."
      },
      {
        "citation": "Wright, J., Yang, A. Y., Ganesh, A., Sastry, S. S., & Ma, Y. (2009). Robust face recognition via sparse representation. *IEEE Transactions on Pattern Analysis and Machine Intelligence*, *31*(2), 210–227."
      },
      {
        "citation": "Snchez, J., Perronnin, F., Mensink, T., & Verbeek, J. J. (2013). Image classification with the fisher vector: Theory and practice. *International Journal of Computer Vision*, *105*(3), 222–245."
      },
      {
        "citation": "Zeiler, M., & Fergus, R. (2014). Visualizing and understanding convolutional networks. *European Conference on Computer Vision*, 818–833."
      }
    ],
    "author_details": [
      {
        "name": "Qingfeng Liu",
        "affiliation": "New Jersey Institute of Technology",
        "email": "ql69@njit.edu"
      },
      {
        "name": "Chengjun Liu",
        "affiliation": "New Jersey Institute of Technology",
        "email": "cliu@njit.edu"
      }
    ]
  },
  {
    "title": "Completing 3D Object Shape from One Depth Image Supplementary\n---AUTHOR---\nJason Rock\nTanmay Gupta\nJustin Thorsen\nJunYoung Gwak\nDaeyun Shin\nDerek Hoiem",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Rock_Completing_3D_Object_2015_CVPR_supplemental.pdf",
    "id": "Rock_Completing_3D_Object_2015_CVPR_supplemental",
    "abstract": "This paper presents a method for completing 3D object shape from a single depth image. The approach leverages symmetry detection and a mesh deformation technique to generate plausible 3D models. A dataset of 3D objects and corresponding depth images is created to train and evaluate the method. The results demonstrate the ability to reconstruct reasonable 3D shapes from limited input data, although significant deformation is sometimes required.\n\n---TOPSICS---\n3D object shape completion\nDepth image reconstruction\nSymmetry detection\nMesh deformation\nDataset creation",
    "topics": [],
    "references": [],
    "author_details": [
      {
        "name": "Jason Rock",
        "affiliation": "University of Illinois at Urbana-Champaign",
        "email": "jjrock2@illinois.edu"
      },
      {
        "name": "Tanmay Gupta",
        "affiliation": "University of Illinois at Urbana-Champaign",
        "email": "tgupta6@illinois.edu"
      },
      {
        "name": "Justin Thorsten",
        "affiliation": "University of Illinois at Urbana-Champaign",
        "email": "thorsten1@illinois.edu"
      },
      {
        "name": "JunYoung Gwak",
        "affiliation": "University of Illinois at Urbana-Champaign",
        "email": "gwak2@illinois.edu"
      },
      {
        "name": "Daeyun Shin",
        "affiliation": "University of Illinois at Urbana-Champaign",
        "email": "dshin11@illinois.edu"
      },
      {
        "name": "Derek Hoiem",
        "affiliation": "University of Illinois at Urbana-Champaign",
        "email": "dhoiem@illinois.edu"
      }
    ]
  },
  {
    "title": "Cascaded Hand Pose Regression\n---AUTHOR---\nXiao Sun\n---AUTHOR---\nYichen Wei\n---AUTHOR---\nShuang Liang\n---AUTHOR---\nXiaoou Tang\n---AUTHOR---\nJian Sun",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Sun_Cascaded_Hand_Pose_2015_CVPR_paper.pdf",
    "id": "Sun_Cascaded_Hand_Pose_2015_CVPR_paper",
    "abstract": "We present a cascaded regression approach for 3D hand pose estimation that is more robust under large viewpoints and complex hand poses. It is directly motivated by the cascaded pose regression framework, where the object pose is estimated progressively via a sequence of weak regressor and each weak regressor uses features that depend on the estimated pose from the previous stage. Our first contribution is 3D pose-indexed features that generalize the previous 6D parameterized features and achieve better invariance to 3D transformations. Our second contribution is a principled hierarchical approach that is adapted for the structure of articulated objects. Comprehensive experiments verify the state-of-the-art accuracy and efﬁciency of the proposed approach on the challenging 3D hand pose estimation problem, on a public dataset and our new dataset.",
    "topics": [
      "3D Hand Pose Estimation",
      "Cascaded Regression",
      "Pose-Indexed Features",
      "Hierarchical Regression",
      "Articulated Object Structure"
    ],
    "references": [
      {
        "citation": "[Baak, A., Muller, M., Bharaj, G., Seidel, H. P., & Theobalt, C. (2011). A data-driven approach for real-time full body pose reconstruction from a depth camera. In *ICCV*.]"
      },
      {
        "citation": "[Liang, H., Yuan, J., & Thalmann, D. (2014). Parsing the hand in depth images. *IEEE Trans. Multimedia*.]"
      },
      {
        "citation": "[Erol, A., Bebis, G., Nicolescu, M., Boyle, R. D., & Twombley, X. (2007). Vision-based hand pose estimation: A review. *CVIU*.]"
      },
      {
        "citation": "[Breiman, L. (2001). Random forests. *Machine learning*, *45*(1), 5-32.]"
      },
      {
        "citation": "[Gorce, M. L., Fleet, D. J., & Paragios, N. (2011). Model-based 3d hand pose estimation from monocular video. *PAMI*.]"
      },
      {
        "citation": "[Stenger, B., Thayanantham, A., Torr, P. H. S., & Cipolla, R. (2006). Model-based hand tracking using a hierarchical bayesian filter. *PAMI*.]"
      },
      {
        "citation": "[Oikonomidis, I., Lourakis, M. I., & Argyros, A. A. (2014). Evolutionary quasi-random search for hand articulations tracking. In *CVPR*.]"
      },
      {
        "citation": "[Qian, C., Sun, X., Wei, Y., Tang, X., & Sun, J. (2014). Realtime and robust hand tracking from depth. In *CVPR*.]"
      },
      {
        "citation": "[Ren, S., Cao, X., Wei, Y., & Sun, J. (2014). Face alignment at 3000 fps by regressing local binary features. In *CVPR*.]"
      },
      {
        "citation": "[Ren, S., Cao, X., Wei, Y., & Sun, J. (2015). Global refinement of random forest. In *CVPR*.]"
      }
    ],
    "author_details": [
      {
        "name": "Xiao Sun",
        "affiliation": "Chinese University of Hong Kong",
        "email": "sx014@ie.cuhk.edu.hk"
      },
      {
        "name": "Yichen Wei",
        "affiliation": "Microsoft Research",
        "email": "yichenw@microsoft.com"
      },
      {
        "name": "Shuang Liang",
        "affiliation": "Tongji University",
        "email": "shuangliang@tongji.edu.cn"
      },
      {
        "name": "Xiaoou Tang",
        "affiliation": "Chinese University of Hong Kong",
        "email": "xtang@ie.cuhk.edu.hk"
      },
      {
        "name": "Jian Sun",
        "affiliation": "Microsoft Research",
        "email": "jiansun@microsoft.com"
      }
    ]
  },
  {
    "title": "Predicting Eye Fixations using Convolutional Neural Networks \n---AUTHOR---\nNian Liu\nJunwei Han\nDingwen Zhang\nShifeng Wen\nTianming Liu",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Liu_Predicting_Eye_Fixations_2015_CVPR_paper.pdf",
    "id": "Liu_Predicting_Eye_Fixations_2015_CVPR_paper",
    "abstract": "It is believed that eye movements in free-viewing of natural scenes are directed by both bottom-up visual salience and top-down visual factors. In this paper, we propose a novel computational framework to simultaneously learn these two types of visual features from raw image data using a multiresolution convolutional neural network (Mr-CNN) for predicting eye fixations. The Mr-CNN is directly trained from image regions centered on fixation and non-fixation locations over multiple resolutions, using raw image pixels as inputs and eye fixation attributes as labels. Diverse top-down visual features can be learned in higher layers. Meanwhile bottom-up visual salience can also be inferred via combining information over multiple resolutions. Finally, optimal integration of bottom-up and top-down cues can be learned in the last logistic regression layer to predict eye fixations. The proposed approach achieves state-of-the-art results over four publically available benchmark datasets, demonstrating the superiority of our work.",
    "topics": [
      "Convolutional Neural Networks (CNNs)",
      "Eye Fixation Prediction",
      "Visual Salience",
      "Top-Down and Bottom-Up Visual Features",
      "Multiresolution Analysis"
    ],
    "references": [
      {
        "citation": "[Itti, L., Koch, C., & Niebur, E. (1998). A model of salience-based visual attention for rapid scene analysis. *IEEE Transactions on Pattern Analysis and Machine Intelligence*, *20*(11), 1254-1259.]"
      },
      {
        "citation": "[Judd, T., Ehinger, K., Durand, F., & Torralba, A. (2009). Learning to predict where humans look. *Proceedings of the IEEE International Conference on Computer Vision (ICCV)*.]"
      },
      {
        "citation": "[Borji, A. (2012). Boosting bottom-up and top-down visual features for saliency estimation. *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*.]"
      },
      {
        "citation": "[Borji, A., & Itti, L. (2012). Exploiting local and global patch rarities for saliency detection. *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*.]"
      },
      {
        "citation": "[Garcia-Diaz, A., Fdez-Vidal, X. R., Pardo, X. M., & Dosil, R. (2012). Saliency from hierarchical adaptation through decorrelation and variance normalization. *Image Vision Computing*, *30*(1), 51-64.]"
      },
      {
        "citation": "[Zhang, J., & Sclaroff, S. (2013). Saliency detection: A boolean map approach. *Proceedings of the IEEE International Conference on Computer Vision (ICCV)*.]"
      },
      {
        "citation": "[Goferman, S., Zelnik-Manor, L., & Tal, A. (2012). Context-aware saliency detection. *IEEE Transactions on Pattern Analysis and Machine Intelligence*, *34*(10), 1915-1926.]"
      },
      {
        "citation": "[Liu, F., & Gleicher, M. (2006). Video retargeting: automating pan and scan. *Proceedings of the ACM Multimedia Conference.*]"
      },
      {
        "citation": "[Sun, J., & Ling, H. (2011). Scale and object aware image retargeting for thumbnail browsing. *Proceedings of the IEEE International Conference on Computer Vision (ICCV)*.]"
      },
      {
        "citation": "[Han, J., Li, K., Shao, L., Hu, X., He, S., Guo, L., Han, J., & Liu, T. (2014). Video abstraction based on fMRI-driven visual attention model. *Information Sciences*.]"
      }
    ],
    "author_details": [
      {
        "name": "Nian Liu",
        "affiliation": "Northwestern Polytechnical University, P.R. China",
        "email": "liunian228@gmail.com"
      },
      {
        "name": "Junwei Han",
        "affiliation": "Northwestern Polytechnical University, P.R. China",
        "email": "junweihan2010@gmail.com"
      },
      {
        "name": "Dingwen Zhang",
        "affiliation": "Northwestern Polytechnical University, P.R. China",
        "email": "zhangdingwen2006yyy@gmail.com"
      },
      {
        "name": "Shifeng Wen",
        "affiliation": "Northwestern Polytechnical University, P.R. China",
        "email": "wenshifeng90@gmail.com"
      },
      {
        "name": "Tianming Liu",
        "affiliation": "University of Georgia, USA",
        "email": "tliu@cs.uga.edu"
      }
    ]
  },
  {
    "title": "More About VLAD: A Leap from Euclidean to Riemannian Manifolds\n---AUTHORs---\nMasoud Faraki\nMehrtasht T. Harandi\nFatih Porikli",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Faraki_More_About_VLAD_2015_CVPR_supplemental.pdf",
    "id": "Faraki_More_About_VLAD_2015_CVPR_supplemental",
    "abstract": "This supplementary material provides proofs for two key theorems presented in the main paper. Theorem 3 establishes that the Fréchet mean of a set of symmetric positive definite (SPD) matrices, utilizing the Jeffrey divergence (δJ), possesses a closed-form solution. Theorem 4 extends this result, stating that the Fréchet mean for a set of points under a projection metric (δP) also admits an analytical solution. The proofs leverage concepts from Riemannian geometry, including Riccati equations and the Rayleigh-Ritz theorem, to derive these closed-form solutions.\n\n---TOPIPS---\nSPD Matrices\nJeffrey Divergence\nFréchet Mean\nRiccati Equations\nRiemannian Geometry",
    "topics": [],
    "references": [
      {
        "citation": "[R. Bhatia, Positive Deﬁnite Matrices. Princeton University Press, 2007.]"
      },
      {
        "citation": "[R. A. Horn and C. R. Johnson, Matrix analysis. Cambridge University Press, 2012.]"
      },
      {
        "citation": "[Z. Wang and B. C. Vemuri, An afﬁne invariant tensor dissimilarity measure and its applications to tensor-valued image segmentation. In Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), volume 1, pages 223–228. IEEE, 2004.]"
      }
    ],
    "author_details": [
      {
        "name": "Masoud Faraki",
        "affiliation": "College of Engineering and Computer Science, Australian National University, Australia; NICTA, Canberra Research Laboratory, Australia",
        "email": "masoud.faraki@nicta.com.au"
      },
      {
        "name": "Mehrtasht T. Harandi",
        "affiliation": "College of Engineering and Computer Science, Australian National University, Australia; NICTA, Canberra Research Laboratory, Australia",
        "email": "mehrtasht.harandi@nicta.com.au"
      },
      {
        "name": "Fatih Porikli",
        "affiliation": "College of Engineering and Computer Science, Australian National University, Australia; NICTA, Canberra Research Laboratory, Australia",
        "email": "fatih.porikli@nicta.com.au"
      }
    ]
  },
  {
    "title": "Fisher Vectors Meet Neural Networks: A Hybrid Classification Architecture\n---AUTHOR---\nFlorent Perronnin\n---AUTHOR---\nDiane Larlus",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Perronnin_Fisher_Vectors_Meet_2015_CVPR_paper.pdf",
    "id": "Perronnin_Fisher_Vectors_Meet_2015_CVPR_paper",
    "abstract": "Fisher Vectors (FV) and Convolutional Neural Networks (CNN) are two image classification pipelines with different strengths. While CNNs have shown superior accuracy on a number of classification tasks, FV classifiers are typically less costly to train and evaluate. This paper proposes a hybrid architecture that combines their strengths: the first unsupervised layers rely on the FV while the subsequent fully-connected supervised layers are trained with back-propagation. The authors demonstrate experimentally that this hybrid architecture significantly outperforms standard FV systems without incurring the high cost that comes with CNNs. They also derive competitive mid-level features from their architecture that are readily applicable to other class sets and even to new tasks.\n\n---TOPIICS---\nFisher Vectors (FV)\nConvolutional Neural Networks (CNN)\nHybrid Classification Architecture\nTransfer Learning\nImage Classification",
    "topics": [],
    "references": [
      {
        "citation": "[Krizhevsky, A., Sutskever, I., & Hinton, G. (2012). ImageNet classification with deep convolutional neural networks. In *NIPS*. ]"
      },
      {
        "citation": "[S´anchez, J., Perronnin, F., & Mensink, T. (2011). Improving the Fisher kernel for large-scale image classification. *ECCV*.]"
      },
      {
        "citation": "[J´egou, H., Douze, M., & Schmid, C. (2011). Product quantization for nearest neighbor search. *IEEE TPAM*. ]"
      },
      {
        "citation": "[Simonyan, K., & Zisserman, A. (2014). Very deep convolutional networks for large-scale image recognition. *arXiv*.]"
      },
      {
        "citation": "[Donahue, J., Jia, Y., Vinyals, O., Hoffman, J., Zhang, N., Tzeng, E., & Darrell, T. (2014). DeCAF: A deep convolutional activation feature for generic visual recognition. *ICML*.]"
      },
      {
        "citation": "[Girshick, R., Donahue, J., Darrell, T., & Malik, J. (2014). Rich feature hierarchies for accurate object detection and semantic segmentation. *CVPR*.]"
      },
      {
        "citation": "[S´anchez, J., Perronnin, F., & Mensink, T. (2013). Image classification with the Fisher vector: theory and practice. *IJCV*.]"
      },
      {
        "citation": "[He, K., Zhang, X., Ren, S., & Sun, J. (2014). Spatial pyramid pooling in deep convolutional networks for visual recognition. *ECCV*.]"
      },
      {
        "citation": "[Babenko, A., Slesarev, A., Chigorin, A., & Lempitsky, V. S. (2014). Neural codes for image retrieval. *ECCV*.]"
      },
      {
        "citation": "[Everingham, M., Van Gool, L., Williams, C. K. I., Winn, J., & Zisserman, A. (2010). The Pascal visual object classes (VOC) challenge. *IJCV*.]"
      }
    ],
    "author_details": [
      {
        "name": "Florent Perronnin",
        "affiliation": "Computer Vision Group, Xerox Research Centre Europe",
        "email": "[Email not available in provided text]"
      },
      {
        "name": "Diane Larlus",
        "affiliation": "Computer Vision Group, Xerox Research Centre Europe",
        "email": "[Email not available in provided text]"
      }
    ]
  },
  {
    "title": "Book2Movie: Aligning Video scenes with Book chapters\n---AUTHOR---\nMakarand Tapaswi\nMartin Bauml\nRainer Stiefelhagen",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Tapaswi_Book2Movie_Aligning_Video_2015_CVPR_supplemental.pdf",
    "id": "Tapaswi_Book2Movie_Aligning_Video_2015_CVPR_supplemental",
    "abstract": "This supplementary material provides additional insights into aligning video scenes with book chapters, focusing on the \"Book2Movie\" approach. It details a qualitative analysis of alignment methods, visualizes ground truth and predicted alignments for \"Harry Potter and the Sorcerer's Stone\" (HP), and provides further examples of describing video shots using passages from the source novel. The material also explores failure analysis of face identification and compares the performance of different alignment methods (prior, DTW3, and a combined approach) for both \"Game of Thrones\" (GOT) and HP, highlighting the strengths and weaknesses of each. The work aims to facilitate high-level understanding of TV series and films by providing weak labels for training models that jointly model images and text.\n\n---TOPICCS---\nVideo-to-text alignment\nNovel-to-film adaptation\nComputer Vision\nDeep Learning\nFace Identification",
    "topics": [],
    "references": [
      {
        "citation": "[Karpathy, A., & Fei-Fei, L. (2015). Deep Visual-Semantic Alignments for Generating Image Descriptions. CVPR.]"
      },
      {
        "citation": "[Kiros, R., Salakhutdinov, R., & Zemel, R. S. (2015). Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models. Transactions on Association for Computational Linguistics.]"
      },
      {
        "citation": "[Mao, J., Xu, W., Yang, Y., Wang, J., & Yuille, A. L. (2014). Explain Images with Multimodal Recurrent Neural Networks. NIPS Deep Learning workshop.]"
      },
      {
        "citation": "[Vinyals, O., Toshev, A., Bengio, S., & Erhan, D. (2015). Show and Tell: A Neural Image Caption Generator. CVPR.]"
      },
      {
        "citation": "[Dickens, Charles. (Jan. 2012). Dickens book sales boosted by television adaptations. BBC News.]"
      },
      {
        "citation": "[Nielsen. (Nov. 2010). ‘Deathly Hallows’ film breathes life into Harry Potter book sales. Nielsen.]"
      },
      {
        "citation": "[Nielsentopten. (Apr. 2014). Adaptation: How A Song of Ice and Fire Book Sales Move with Game of Thrones.]"
      },
      {
        "citation": "[Slate. (Apr. 2014). How does the Game of Thrones Series Line Up With the Books? Slate.]"
      },
      {
        "citation": "[Stephenfollows. (Jan. 2014). Where do highest-grossing screenplays come from? Stephenfollows.]"
      }
    ],
    "author_details": [
      {
        "name": "Makarand Tapaswi",
        "affiliation": "Karlsruhe Institute of Technology",
        "email": "makarand.tapaswi@kit.edu"
      },
      {
        "name": "Martin B¨auml",
        "affiliation": "Karlsruhe Institute of Technology",
        "email": "baeuml@kit.edu"
      },
      {
        "name": "Rainer Stiefelhagen",
        "affiliation": "Karlsruhe Institute of Technology",
        "email": "rainer.stiefelhagen@kit.edu"
      }
    ]
  },
  {
    "title": "LMI-based 2D-3D Registration: from Uncalibrated Images to Euclidean Scene\n---AUTHOR---\nDanda Pani Paudel\nAdlane Habed\nCédric Demonceaux\nPascal Vasseur",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Paudel_LMI-Based_2D-3D_Registration_2015_CVPR_paper.pdf",
    "id": "Paudel_LMI-Based_2D-3D_Registration_2015_CVPR_paper",
    "abstract": "This paper investigates the problem of registering a scanned scene, represented by 3D Euclidean point coordinates, and two or more uncalibrated cameras. An unknown subset of the scanned points have their image projections detected and matched across images. The proposed approach assumes the cameras are only known in some arbitrary projective frame and no calibration or autocalibration is required. The devised solution is based on a Linear Matrix Inequality (LMI) framework that allows simultaneously estimating the projective transformation relating the cameras to the scene and establishing 2D-3D correspondences without triangulating image points. The proposed LMI framework allows both deriving triangulation-free LMI chirality conditions and establishing putative correspondences between 3D volumes (boxes) and 2D pixel coordinates. Two registration algorithms are presented, one exploiting the scene’s structure and the other concerned with robustness, and both employ the Branch-and-Prune paradigm, guaranteeing convergence to a global solution under mild initial bound conditions. Experimental results are presented and compared against other approaches.\n\n---TOPICICS---\nUncalibrated Camera Registration\nLinear Matrix Inequalities (LMI)\nTriangulation-Free Correspondence\n3D Scene Reconstruction\nGlobal Optimization",
    "topics": [],
    "references": [
      {
        "citation": "[D. Nister, O. Naroditsky, and J. Bergen, Visual odometry, IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2004]"
      },
      {
        "citation": "[D. Aiger, N. J. Mitra, and D. Cohen-Or, 4-points congruent sets for robust surface registration, ACM Transactions on Graphics, 2008]"
      },
      {
        "citation": "[S. Boyd and L. Vandenberghe, Convex Optimization, Cambridge University Press, 2004]"
      },
      {
        "citation": "[M. Chandraker, S. Agarwal, F. Kahl, D. Nister, and D. Kriegman, Autocalibration via rank-constrained estimation of the absolute quadric, IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2007]"
      },
      {
        "citation": "[R. I. Hartley and A. Zisserman, Multiple View Geometry in Computer Vision, Cambridge University Press, 2004]"
      },
      {
        "citation": "[M. Corsini, M. Dellepiane, F. Ganovelli, R. Gherardi, A. Fusiello, and R. Scopigno, Fully automatic registration of image sets on approximate geometry, International Journal of Computer Vision (IJCV), 2013]"
      },
      {
        "citation": "[P. Finsler, Uber das vorkommen deﬁniter und semideﬁniter formen in scharen quadratischer formen, Comment. Math. Helv., 1936/37]"
      },
      {
        "citation": "[A. Fusiello, A. Benedetti, M. Farenzena, and A. Busti, Globally convergent autocalibration using interval analysis, IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), 2004]"
      },
      {
        "citation": "[R. I. Hartley, Chirality, International Journal of Computer Vision (IJCV), 1998]"
      },
      {
        "citation": "[J. Knopp, J. Sivic, and T. Pajdla, Avoiding confusing features in place recognition, European Conference on Computer Vision (ECCV), 2010]"
      }
    ],
    "author_details": [
      {
        "name": "Danda Pani Paudel",
        "affiliation": "Le2i laboratory, University of Bourgogne, CNRS, France",
        "email": "danda-pani.paudel@u-bourgogne.fr"
      },
      {
        "name": "Adlane Habed",
        "affiliation": "ICube laboratory, University of Strasbourg, CNRS, France",
        "email": "adlane.habed@icube.unistra.fr"
      },
      {
        "name": "Cédric Demonceaux",
        "affiliation": "Le2i laboratory, University of Bourgogne, CNRS, France",
        "email": "cedric.demonceaux@u-bourgogne.fr"
      },
      {
        "name": "Pascal Vasseur",
        "affiliation": "LITIS EA laboratory, University of Rouen, France",
        "email": "pascal.vasseur@univ-rouen.fr"
      }
    ]
  },
  {
    "title": "Rent3D: Floor-Plan Priors for Monocular Layout Estimation\n---AUTHOR---\nChenxi Liu\nAlexander G. Schwing\nKaustav Kundu\nRaquel Urtasun\nSanja Fidler",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Liu_Rent3D_Floor-Plan_Priors_2015_CVPR_paper.pdf",
    "id": "Liu_Rent3D_Floor-Plan_Priors_2015_CVPR_paper",
    "abstract": "The paper introduces Rent3D, a system that enables a 3D \"virtual-tour\" of an apartment given a small set of monocular images of different rooms and a 2D floor plan. The problem is framed as inference in a Markov Random Field, reasoning about the layout of each room and its relative pose within the full apartment to achieve accurate camera pose estimation. Rent3D leverages floor plans as prior knowledge, imposing aspect ratio constraints and extracting semantic information like window locations. An efficient exact inference algorithm is derived, achieving fast processing times. The approach is demonstrated on a new dataset of over 200 apartments.\n\n---TOPICCS---\nMonocular Layout Estimation\nFloor Plan Integration\nMarkov Random Fields\nApartment Reconstruction\nIntegral Geometry",
    "topics": [],
    "references": [
      {
        "citation": "[Martin-Brualla, R., He, Y., Russell, B. C., & Seitz, S. M. The 3D Jigsaw Puzzle: Mapping Large Indoor Spaces. Proc. ECCV, 2014.]"
      },
      {
        "citation": "[Brubaker, M. A., Geiger, A., & Urtasun, R. Lost! leveraging the crowd for probabilistic visual self-localization. CVPR, 2013.]"
      },
      {
        "citation": "[Cabral, R., & Furukawa, Y. Piecewise Planar and Compact Floorplan Reconstruction from Images. Proc. CVPR, 2014.]"
      },
      {
        "citation": "[Fidler, S., Dickinson, S., & Urtasun, R. 3D Object Detection and Viewpoint Estimation with a Deformable 3D Cuboid Model. Proc. NIPS, 2012.]"
      },
      {
        "citation": "[Schwing, A. G., & Urtasun, R. Box In the Box: Joint 3D Layout and Object Reasoning from Single Images. Proc. ICCV, 2013.]"
      },
      {
        "citation": "[Pero, L., Bowdish, J., Fried, D., Kermgard, B., Hartley, E., & Barnard, K. Bayesian geometric modeling of indoor scenes. Proc. CVPR, 2012.]"
      },
      {
        "citation": "[Hedau, V., Hoiem, D., & Forsyth, D. Recovering the Spatial Layout of Cluttered Rooms. Proc. ICCV, 2009.]"
      },
      {
        "citation": "[Gupta, A., Satkin, S., Efroos, A. A., & Hebert, M. From 3D Scene Geometry to Human Workspace. Proc. CVPR, 2011.]"
      },
      {
        "citation": "[Wang, H., Gould, S., & Koller, D. Discriminative Learning with Latent Variables for Cluttered Indoor Scene Understanding. Proc. ECCV, 2010.]"
      },
      {
        "citation": "[Schwing, A. G., Fidler, S., Pollefeys, M., & Urtasun, R. Efficient Structured Prediction for 3D Indoor Scene Understanding. Proc. CVPR, 2012.]"
      }
    ],
    "author_details": [
      {
        "name": "Chenxi Liu",
        "affiliation": "State Key Lab. on Intelligent Technology and Systems, Tsinghua Nat. Lab. for Inf. Science and Tech. (TNList), Department of Automation, Tsinghua University",
        "email": "chenxi.liu@live.cn"
      },
      {
        "name": "Alexander G. Schwing",
        "affiliation": "Department of Computer Science, University of Toronto",
        "email": "aschwing@cs.toronto.edu"
      },
      {
        "name": "Kaustav Kundu",
        "affiliation": "Department of Computer Science, University of Toronto",
        "email": "kkundu@cs.toronto.edu"
      },
      {
        "name": "Raquel Urtasun",
        "affiliation": "Department of Computer Science, University of Toronto",
        "email": "urtasun@cs.toronto.edu"
      },
      {
        "name": "Sanja Fidler",
        "affiliation": "Department of Computer Science, University of Toronto",
        "email": "fidler@cs.toronto.edu"
      }
    ]
  },
  {
    "title": "Situational Object Boundary Detection\n---AUTHORSS---\nJ.R.R. Uijlings\nV. Ferrari",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Uijlings_Situational_Object_Boundary_2015_CVPR_paper.pdf",
    "id": "Uijlings_Situational_Object_Boundary_2015_CVPR_paper",
    "abstract": "This paper proposes situational object boundary detection, addressing the limitations of monolithic boundary predictors that treat all images the same. The approach defines a variety of situations and trains specialized object boundary detectors for each. A test image is classified into these situations based on its context (global image appearance), and the corresponding detectors are applied, with results fused based on classification probabilities. Experiments on ImageNet, Microsoft COCO, and Pascal VOC 2012 demonstrate significant improvements over monolithic approaches, and substantial outperformance on the SBD dataset.\n\n---TOPICKS---\nSituational Object Boundary Detection\nImage Context and Appearance\nSemantic Contour Detection\nObject Boundary Predictors\nImage Classification",
    "topics": [],
    "references": [
      {
        "citation": "[Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). ImageNet classification with deep convolutional neural networks. In *NIPS*.]"
      },
      {
        "citation": "[Arbeláez, P., Maire, M., Fowlkes, C., & Malik, J. (2011). Contour Detection and Hierarchical Image Segmentation. *TPAMI*.]"
      },
      {
        "citation": "[Lazebnik, S., Schmid, C., & Ponce, J. (2006). Beyond Bags of Features: Spatial Pyramid Matching for Recognizing Natural Scene Categories. In *CVPR*.]"
      },
      {
        "citation": "[Dollár, P., & Zitnick, C. (2013). Structured forests for fast edge detection. In *ICCV*.]"
      },
      {
        "citation": "[Girshick, R., Donahue, J., Darrell, T., & Malik, J. (2014). Rich feature hierarchies for accurate object detection and semantic segmentation. In *CVPR*.]"
      },
      {
        "citation": "[Everingham, M., Eslamimi, S., van Gool, L., Williams, C., Winn, J., & Zisserma, A. (2014). The pascal visual object classes challenge - a retrospective. *IJCV*.]"
      },
      {
        "citation": "[Felzenszwalb, P. F., Girshick, R. B., McAllester, D., & Ramanan, D. (2010). Object detection with discriminatively trained part based models. *TPAMI*.]"
      },
      {
        "citation": "[Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollár, P., & Zitnick, C. (2014). Microsoft COCO: Common objects in context. In *ECCV*.]"
      },
      {
        "citation": "[Jurié, F., & Triggs, B. (2005). Creating Efficient Codebooks for Visual Recognition. In *ICCV*.]"
      },
      {
        "citation": "[Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A., & Fei-Fei, L. (2015). ImageNet large scale visual recognition challenge. *IJCV*.]"
      }
    ],
    "author_details": [
      {
        "name": "J.R.R. Uijlings",
        "affiliation": "University of Edinburgh",
        "email": "[Email not available in the provided text]"
      },
      {
        "name": "V. Ferrari",
        "affiliation": "University of Edinburgh",
        "email": "[Email not available in the provided text]"
      }
    ]
  },
  {
    "title": "Learning with Dataset Bias in Latent Subcategory Models\n---AUTHOR---\nsamuele.marteilli@iit.it\nmoin.nabi@iit.it\nd.stamos@cs.ucl.ac.uk\nsamuele.marteilli@iit.it\na.mcdoanlds@cs.ucl.ac.uk\nvittorio.murino@iit.it\nandrew.mcdonald@cs.ucl.ac.uk\nmassimiliano.pontil@cs.ucl.ac.uk",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Stamos_Learning_With_Dataset_2015_CVPR_supplemental.pdf",
    "id": "Stamos_Learning_With_Dataset_2015_CVPR_supplemental",
    "abstract": "Latent subcategory models (LSMs) offer improvements over linear SVMs, but training them is challenging due to the potential for many local optima and increased model complexity requiring large training sets. Combining datasets from different sources can address this, but standard machine learning methods often fail to account for inherent dataset biases, leading to decreased performance. This paper introduces a model that jointly learns an LSM for each dataset and a compound LSM, leveraging multiple biased datasets to tackle a common classification task. The method aims to borrow statistical strength while reducing bias, demonstrating significant improvements over existing approaches like training a standard LSM on combined datasets or using undoing bias techniques. Experiments on datasets like PASUAL, LabelMe, Caltech101, and SUN09 show average improvements of over 6.5% and 8.5% respectively.",
    "topics": [
      "Latent Subcategory Models (LSMs)",
      "Dataset Bias",
      "Multitask Learning",
      "Object Recognition",
      "Undoing Bias"
    ],
    "references": [
      {
        "citation": "[T. Malisiewicz, A. Gupta, and A. Efros, \"Ensemble of exemplar-svms for object detection and beyond,\" ICCV, 2011.]"
      },
      {
        "citation": "[F. Aiolli and A. Sperduti, \"Multiclass classification with multi-prototype support vector machines,\" Journal of Machine Learning Research, 2005.]"
      },
      {
        "citation": "[S. Boyd and L. Vandenberghe, \"Convex optimization,\" Cambridge University Press, 2009.]"
      },
      {
        "citation": "[P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and D. Ramanan, \"Object detection with discriminatively trained part-based models,\" IEEE Transactions on Pattern Analysis and Machine Intelligence, 2010.]"
      },
      {
        "citation": "[R. Girshick and J. Malik, \"Training deformable part models with decorrelated features,\" ICCV, 2013.]"
      },
      {
        "citation": "[A. Kulis, K. Saenko, and T. Darrell, \"What you saw is not what you get: Domain adaptation using asymmetric kernel transforms,\" CVPR, 2011.]"
      },
      {
        "citation": "[A. Torralba and A. Efrobs, \"Unbiased look at dataset bias,\" CVPR, 2011.]"
      }
    ],
    "author_details": [
      {
        "name": "Samuele Martelli",
        "affiliation": "Istituto Italiano di Tecnologia (IIT)",
        "email": "samuele.marteilli@iit.it"
      },
      {
        "name": "Moin Nabi",
        "affiliation": "Istituto Italiano di Tecnologia (IIT)",
        "email": "moin.nabi@iit.it"
      },
      {
        "name": "Dimitris Stamos",
        "affiliation": "University College London (UCL)",
        "email": "d.stamos@cs.ucl.ac.uk"
      },
      {
        "name": "Samuele Martelli",
        "affiliation": "Istituto Italiano di Tecnologia (IIT)",
        "email": "samuele.marteilli@iit.it"
      },
      {
        "name": "Andrew McDonald",
        "affiliation": "University College London (UCL)",
        "email": "a.mcdoanlds@cs.ucl.ac.uk"
      },
      {
        "name": "Vittorio Murino",
        "affiliation": "Istituto Italiano di Tecnologia (IIT)",
        "email": "vittorio.murino@iit.it"
      },
      {
        "name": "Andrew McDonald",
        "affiliation": "University College London (UCL)",
        "email": "andrew.mcdonald@cs.ucl.ac.uk"
      },
      {
        "name": "Massimiliano Pontil",
        "affiliation": "University College London (UCL)",
        "email": "m.pontil@cs.ucl.ac.uk"
      }
    ]
  },
  {
    "title": "A MRF Shape Prior for Facade Parsing with Occlusions\n---AUTHOR---\nMateusz Koziński\nRaghudeep Gadde\nSergey Zagoruyko\nGuillaume Obozinski\nRenaud Marlet",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Kozinski_A_MRF_Shape_2015_CVPR_paper.pdf",
    "id": "Kozinski_A_MRF_Shape_2015_CVPR_paper",
    "abstract": "We present a new shape prior formalism for the segmentation of rectiﬁed facade images. It combines the simplicity of split grammar’s with unprecedented expressive power: the capability of encoding simultaneous alignment in two dimensions, facade occlusions and irregular boundaries between facade elements. We formulate the task of finding the most likely image segmentation conforming to a prior of the proposed form as a MAP-MRF problem over a 4-connected pixel grid, and propose an efficient optimization algorithm for solving it. Our method simultaneously segments the visible and occluding objects, and recovers the structure of the occluded facade. We demonstrate state-of-the-art results on a number of facade segmentation datasets.",
    "topics": [
      "Facade parsing",
      "Shape priors",
      "Markov Random Fields (MRF)",
      "Occlusion handling",
      "Rectified images"
    ],
    "references": [
      {
        "citation": "[Cohen, A., Schwing, A., & Pollefeys, M. (2014). Efficient structured parsing of facades using dynamic programming. *CVPR*.] - Likely foundational work on facade parsing."
      },
      {
        "citation": "[Gadde, R., Marlet, R., & Nikos, P. (2014). Learning grammar for architecture-speciﬁc facade parsing. *Research Report RR-8600*.] - Explores grammar learning for facade parsing."
      },
      {
        "citation": "[Komodakis, N., Paragios, N., & Tziritas, G. (2011). Mrf energy minimization and beyond via dual decomposition. *IEEE Trans. PAM*.] - Introduces a key optimization technique (dual decomposition) likely used in the paper."
      },
      {
        "citation": "[Korˇc, F., & Förstner, W. (2009). eTRIMS Image Database for interpreting images of man-made scenes. *Technical Report TR-IGG-P-2009-01*.] - Provides a dataset used for evaluation."
      },
      {
        "citation": "[Kozi´nski, M., Obozinski, G., & Marlet, R. (2014). Beyond procedural facade parsing: bidirectional alignment via linear programming. *ACCV*.] -  Builds upon procedural parsing and uses linear programming."
      },
      {
        "citation": "[Simon, L., Teboul, O., Koutsourakis, P., Van Gool, L., & Paragios, N. (2012). Parameter-free/pareto-driven procedural 3d reconstruction of buildings from ground-level sequences. *CVPR*.] - Related work on 3D building reconstruction."
      },
      {
        "citation": "[Teboul, O., Simon, L., Koutsourakis, P., & Paragios, N. (2010). Segmentation of building facades using procedural shape priors. *CVPR*.] -  A prior work on facade segmentation using procedural shape priors."
      },
      {
        "citation": "[Teboul, O., Kokkinos, I., Simon, L., Koutsourakis, P., & Paragios, N. (2011). Shape grammar parsing via reinforcement learning. *CVPR*.] - Explores a different approach to shape grammar parsing using reinforcement learning."
      },
      {
        "citation": "[Riemenschneider, H., Krispel, U., Thaller, W., Donoser, M., Havemann, S., Fellner, D., & Bischof, H. (2012). Irregular lattices for complex shape grammar facade parsing. *CVPR*.] - Addresses parsing with complex shapes using irregular lattices."
      },
      {
        "citation": "[Socher, R., Lin, C. C., Manning, C., & Ng, A. Y. (2011). Parsing natural scenes and natural language with recursive neural networks. *ICML*.] - Introduces a related technique (recursive neural networks) for scene understanding."
      }
    ],
    "author_details": [
      {
        "name": "Mateusz Koziński",
        "affiliation": "Université Paris-Est, LIGM (UMR CNRS 8049), ENPC",
        "email": "mateusz.kozinski@enpc.fr"
      },
      {
        "name": "Raghudeep Gadde",
        "affiliation": "Université Paris-Est, LIGM (UMR CNRS 8049), ENPC",
        "email": "raghudeep.gadde@enpc.fr"
      },
      {
        "name": "Sergey Zagoruyko",
        "affiliation": "Université Paris-Est, LIGM (UMR CNRS 8049), ENPC",
        "email": "sergey.zagoruyko@enpc.fr"
      },
      {
        "name": "Guillaume Obozinski",
        "affiliation": "Université Paris-Est, LIGM (UMR CNRS 8049), ENPC",
        "email": "guillaume.obozinski@enpc.fr"
      },
      {
        "name": "Renaud Marlet",
        "affiliation": "Université Paris-Est, LIGM (UMR CNRS 8049), ENPC",
        "email": "renaud.marlet@enpc.fr"
      }
    ]
  },
  {
    "title": "Cross-Age Face Veriﬁcation by Coordinating with Cross-Face Age Veriﬁcation\n---AUTHOR---\nLiang Du\n---AUTHOR---\nHaibin Ling",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Du_Cross-Age_Face_Verification_2015_CVPR_paper.pdf",
    "id": "Du_Cross-Age_Face_Verification_2015_CVPR_paper",
    "abstract": "In this paper, we present a novel framework for cross-age face verification (FV) by coordinating with cross-face age verification (AV). FV seeks age insensitivity while AV seeks age sensitivity, suggesting that AV can guide feature selection in FV by reducing age-sensitive features. We propose a joint additive model with a competition regularization term, solved using an alternating greedy coordinate descent (AGCD) algorithm. Experiments on FG-Net and MORPH datasets demonstrate that the algorithm effectively balances feature sharing and exclusion, removes distracting age-sensitive features, and outperforms existing solutions. The core idea is that knowing conflicting goals can inhibit irrelevant features and improve performance.\n\n---TOPICCS---\nCross-age face verification\nCross-face age verification\nFeature coordination\nJoint learning models\nAlternating greedy coordinate descent",
    "topics": [],
    "references": [
      {
        "citation": "[Asthana, A., Zafeiriou, S., Cheng, S., & Pantic, M. Robust discriminative response map fitting with constrained local models. In CVPR, 2013.]"
      },
      {
        "citation": "[Ahonen, T., Hadid, A., & Pietikainen, M. Face description with local binary patterns: Application to face recognition. PAMI, 28(12):2037–2041, 2006.]"
      },
      {
        "citation": "[Friedman, J. Greedy function approximation: A gradient boosting machine. Annnals of Statistics, 29(5):1189–1232, 2001.]"
      },
      {
        "citation": "[Fu, Y., Guo, G., & Huang, T. Age synthesis and estimation via faces: a survey. PAMI, 32(11):1955–1976, 2010.]"
      },
      {
        "citation": "[Gong, D., Li, Z., Lin, D., Liu, J., & Tang, X. Hidden factor analysis for age invariant face recognition. In ICCV, 2013.]"
      },
      {
        "citation": "[Guo, G., & Mu, G. Joint estimation of age, gender and ethnicity: CCA vs. PLS. In FG, 2013.]"
      },
      {
        "citation": "[Guo, G., Mu, G., & Ricanek, K. Cross-Age Face recognition on a very large database: the performance versus age intervals and improvement using soft biometric traits. In ICPR, 2010.]"
      },
      {
        "citation": "[Huang, G., Ramesh, M., Berg, T., & Learned-Miller, E. Labeled faces in the wild: a database for studying face recognition in unconstrained environments. University of Massachusetts, Amherst, Technical Report 07-49, October, 2007.]"
      },
      {
        "citation": "[Ling, H., Soatto, S., Ramanathan, N., & Jacobs, D. Face veriﬁcation across-age progression using discriminative methods. IEEE T-IFS, 5(1):82–91, 2010.]"
      },
      {
        "citation": "[Ramanathan, N., & Chellappa, R. Modeling age progression in young faces. In CVPR, 2006.]"
      }
    ],
    "author_details": [
      {
        "name": "Liang Du",
        "affiliation": "Department of Computer and Information Sciences, Temple University, Philadelphia, USA",
        "email": "liang.du@temple.edu"
      },
      {
        "name": "Haibin Ling",
        "affiliation": "Department of Computer and Information Sciences, Temple University, Philadelphia, USA",
        "email": "hbling@temple.edu"
      }
    ]
  },
  {
    "title": "Space-Time Tree Ensemble for Action Recognition\n---AUTHOR---\nShugao Ma\nLeonid Sigal\nStan Sclaroff",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Ma_Space-Time_Tree_Ensemble_2015_CVPR_paper.pdf",
    "id": "Ma_Space-Time_Tree_Ensemble_2015_CVPR_paper",
    "abstract": "Human actions are inherently structured patterns of body movements. We explore ensembles of hierarchical spatio-temporal trees, discovered directly from training data, to model these structures for action recognition. The hierarchical spatio-temporal trees provide a robust mid-level representation for actions. However, discovery of frequent and discriminative tree structures is challenging due to the exponential search space, particularly if one allows partial matching. We address this by first building a concise action vocabulary via discriminative clustering. Using the action vocabulary we then utilize tree mining with subsequent tree clustering and ranking to select a compact set of highly discriminative tree patterns. We show that these tree patterns, alone, or in combination with shorter patterns (action words and pairwise patterns) achieve state-of-the-art performance on two challenging datasets: UCF Sports and HighFive. Moreover, trees learned on HighFive are used in recognizing two action classes in a different dataset, Hollywood3D, demonstrating the potential for cross-dataset generality of the trees our approach discovers.\n\n---TOPICCS---\nAction Recognition\nSpace-Time Trees\nHierarchical Structures\nDiscriminative Clustering\nCross-Dataset Generalization",
    "topics": [],
    "references": [
      {
        "citation": "[N. B. Aoun, M. Mejdoub, and C. B. Amar. Graph-based approach for human action recognition using spatio-temporal features. Journal of Visual Communication and Image Representation, 25(2):329 – 338, 2014.]"
      },
      {
        "citation": "[W. Brendel and S. Todorovic. Learning spatiotemporal graphs of human activities. In ICCV, 2011.]"
      },
      {
        "citation": "[K. Crammer and Y. Singer. On the algorithmic implementation of multiclass kernel-based vector machines. JMLR, 2:265–292, 2001.]"
      },
      {
        "citation": "[B. J. Frey and D. Dueck. Clustering by passing messages between data points. Science, (315):972–976, 2007.]"
      },
      {
        "citation": "[P. F. Felzenszwalb and R. Zabih. Dynamic programming and graph algorithms in computer vision. TPAMI, 33(4):721–740, 2011.]"
      },
      {
        "citation": "[M. Raptis, I. Kokkinos, and S. Soatto. Discovering discriminative action parts from mid-level video representations. In CVPR, 2012.]"
      },
      {
        "citation": "[S. Hadfield and R. Bowden. Hollywood 3D: Recognizing actions in 3D natural scenes. In CVPR, 2013.]"
      },
      {
        "citation": "[A. Gaidon, Z. Harchaoui, and C. Schmid. Activity representation with motion hierarchies. IJCV, 107(3):219–238, 2014.]"
      },
      {
        "citation": "[M. Hoai and A. Zisserman. Discriminative sub-categorization. In CVPR, 2013.]"
      },
      {
        "citation": "[V. Kantorov and I. Laptev. Efficient feature extraction, encoding, and classification for action recognition. In CVPR, 2014.]"
      }
    ],
    "author_details": [
      {
        "name": "Shugao Ma",
        "affiliation": "Boston University",
        "email": "shugaoma@bu.edu"
      },
      {
        "name": "Leonid Sigal",
        "affiliation": "Disney Research",
        "email": "lsigal@disneyresearch.com"
      },
      {
        "name": "Stan Sclaroff",
        "affiliation": "Boston University",
        "email": "Sclaroff@bu.edu"
      }
    ]
  },
  {
    "title": "Simultaneous Feature Learning and Hash Coding with Deep Neural Networks\n---AUTHOR---\nHanjiang Lai\nYan Pan\nYe Liu\nShuicheng Yan",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Lai_Simultaneous_Feature_Learning_2015_CVPR_paper.pdf",
    "id": "Lai_Simultaneous_Feature_Learning_2015_CVPR_paper",
    "abstract": "This paper introduces a novel deep architecture for supervised hashing, addressing limitations in existing methods that rely on separate feature extraction and coding steps. The proposed approach simultaneously learns image features and hash codes within a \"one-stage\" deep neural network. The architecture comprises a sub-network for feature extraction, a divide-and-encode module for bitwise encoding, and a triplet ranking loss to preserve similarity. Extensive experiments on benchmark image datasets demonstrate significant improvements over state-of-the-art supervised and unsupervised hashing techniques.\n\n---TOPIC---\nDeep Neural Networks\nHashing Methods\nImage Retrieval\nSupervised Learning\nTriplet Ranking Loss",
    "topics": [],
    "references": [
      {
        "citation": "[Dalal, N., & Triggs, B. (2005). Histograms of oriented gradients for human detection. *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*, 886–893.]"
      },
      {
        "citation": "[Norouzi, M., Fleet, D. J., & Salakhutdinov, R. (2012). Hamming distance metric learning. *Advances in Neural Information Processing Systems*, 1–9.]"
      },
      {
        "citation": "[Gionis, A., Indyk, P., & Motwani, R. (1999). Similarity search in high dimensions via hashing. *Proceedings of the International Conference on Very Large Data Bases*, 518–529.]"
      },
      {
        "citation": "[Gong, Y., Kumar, S., Rowley, H. A., & Lazebnik, S. (2013). Learning binary codes for high-dimensional data using bilinear projections. *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*, 484–491.]"
      },
      {
        "citation": "[Gong, Y., & Lazebnik, S. (2011). Iterative quantization: A procustean approach to learning binary codes. *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*, 6, 817–824.]"
      },
      {
        "citation": "[Kulis, B., & Darrell, T. (2009). Learning to hash with binary reconstructive embeddings. *Advances in Neural Information Processing Systems*, 1042–1050.]"
      },
      {
        "citation": "[Krizhevsky, A., Sutskever, I., & Hinton, G. (2012). ImageNet classification with deep convolutional neural networks. *Advances in Neural Information Processing Systems*, 1106–1114.]"
      },
      {
        "citation": "[Liu, W., Wang, J., Ji, R., Jiang, Y.-G., & Chang, S.-F. (2012). Supervised hashing with kernels. *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*, 2074–2081.]"
      },
      {
        "citation": "[Wang, J., Kumar, S., & Chang, S.-F. (2011). Hashing with graphs. *Proceedings of the International Conference on Machine Learning*, 1–8.]"
      },
      {
        "citation": "[Xia, R., Pan, Y., Lai, H., Liu, C., & Yan, S. (2014). Supervised hashing for image retrieval via image representation learning. *Proceedings of the AAAI Conference on Artificial Intelligence*, 2156–2162.]"
      }
    ],
    "author_details": [
      {
        "name": "Hanjiang Lai",
        "affiliation": "Department of Electronic and Computer Engineering, National University of Singapore, Singapore",
        "email": "[Email not available]"
      },
      {
        "name": "Yan Pan",
        "affiliation": "School of Software, Sun Yan-Sen University, China",
        "email": "[Email not available]"
      },
      {
        "name": "Ye Liu",
        "affiliation": "School of Information Science and Technology, Sun Yan-Sen University, China",
        "email": "[Email not available]"
      },
      {
        "name": "Shuicheng Yan",
        "affiliation": "Department of Electronic and Computer Engineering, National University of Singapore, Singapore",
        "email": "[Email not available]"
      }
    ]
  },
  {
    "title": "DASC: Dense Adaptive Self-Correlation Descriptor for Multi-modal and Multi-spectral Correspondence\n---AUTHOR---\nSeungryong Kim\nDongbo Min\nBumsub Ham\nSeungchul Ryu\nMinh N. Do\nKwanghoon Sohn",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Kim_DASC_Dense_Adaptive_2015_CVPR_supplemental.pdf",
    "id": "Kim_DASC_Dense_Adaptive_2015_CVPR_supplemental",
    "abstract": "This paper introduces the Dense Adaptive Self-Correlation Descriptor (DASC) for multi-modal and multi-spectral correspondence. DASC leverages adaptive self-correlation to efficiently compute robust descriptors. The paper details the derivation of a key equation (Eq. 10) from Eq. 9, analyzes the performance of symmetric versus asymmetric weights in the descriptor, examines the dataset and sampling pattern learning, and investigates the influence of parameters like local support window size and feature dimension. Experimental results demonstrate the effectiveness of DASC across various benchmarks, including stereo vision, optical flow, and multi-modal image pairs, while also highlighting the computational advantages of using asymmetric weights.\n\n---TOPICCS---\nAdaptive Self-Correlation\nMulti-modal and Multi-spectral Correspondence\nDescriptor Computation\nSymmetric vs. Asymmetric Weights\nParameter Analysis",
    "topics": [],
    "references": [
      {
        "citation": "[D. Lowe. Distinctive image features from scale-invariant keypoints. IJCV, 60(2):91–110, 2004.] - This paper introduces SIFT, a foundational feature detection and description algorithm."
      },
      {
        "citation": "[A. Alahi, R. Ortiz, and P. Vandergheynst. Freak : Fast retina keypoint. In Proc. of CVPR, 2012.] - Introduces FREAK, a fast binary feature descriptor."
      },
      {
        "citation": "[M. Calonder. Brief : Computing a local binary descriptor very fast. IEEE Trans. PAMI, 34(7):1281–1298, 2011.] - Presents BRIEF, another fast binary descriptor."
      },
      {
        "citation": "[T. Brox and J. Malik. Large displacement optical ﬂow: Descriptor matching in variational motion estimation. IEEE Trans. PAMI, 33(3):500–513, 2011.] - Relevant for motion estimation and descriptor matching."
      },
      {
        "citation": "[E. Schechtman and M. Irani. Matching local self-similarities across images and videos. In Proc. of CVPR, 2007.] - Deals with matching features across images and videos."
      },
      {
        "citation": "[M. Brown and S. Susstrunk. Multispectral sift for scene category recognition. In Proc. of CVPR, 2011.] - Explores extending SIFT to multispectral images."
      },
      {
        "citation": "[N. Dalal and B. Trigg. Histograms of oriented gradients for human detection. In Proc. of CVPR, 2005.] - Introduces HOG, a feature descriptor often used for human detection."
      },
      {
        "citation": "[E. Tola, V. Lepetit, and P. Fua. Daisy: An efﬁcient dense descriptor applied to wide-baseline stereo. IEEE Trans. PAMI, 32(5):815–830, 2010.] - Presents DAISY, a descriptor designed for wide-baseline stereo."
      },
      {
        "citation": "[K. He, J. Sun, and X. Tang. Guided image filtering. IEEE Trans. PAMI, 35(6):1397–1409, 2013.] - Introduces Guided Image Filtering, a useful image processing technique."
      },
      {
        "citation": "[B. Fan, Q. Kong, T. Trzcinski, and Z. Wang. Receptive ﬁelds selection for binary feature description. IEEE Trans. IP, 23(6):2583–2595, 2014.] - Focuses on receptive fields for binary feature description."
      }
    ],
    "author_details": [
      {
        "name": "Seungryong Kim",
        "affiliation": "Yonsei University",
        "email": "[Email not available - website provided: http://seungryong.github.io/DASC/]"
      },
      {
        "name": "Dongbo Min",
        "affiliation": "Chungnam Nat. University, ADSC",
        "email": "[Email not available]"
      },
      {
        "name": "Bumsub Ham",
        "affiliation": "Inria",
        "email": "[Email not available]"
      },
      {
        "name": "Seungchul Ryu",
        "affiliation": "Yonsei University",
        "email": "[Email not available]"
      },
      {
        "name": "Minh N. Do",
        "affiliation": "UIUC",
        "email": "[Email not available]"
      },
      {
        "name": "Kwanghoon Sohn",
        "affiliation": "Yonsei University",
        "email": "[Email not available]"
      }
    ]
  },
  {
    "title": "Active Learning and Discovery of Object Categories in the Presence of Unnameable Instances\n---AUTHOR---\nChrisoph K¨ading\nAlexander Freytag\nErik Rodner\nPaul Bodesheim\nJoachim Denzler",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Kading_Active_Learning_and_2015_CVPR_paper.pdf",
    "id": "Kading_Active_Learning_and_2015_CVPR_paper",
    "abstract": "Current visual recognition algorithms require large amounts of labeled data, which is expensive to obtain. Active learning algorithms aim to reduce this labeling effort by selecting the most informative examples for annotation. However, existing algorithms typically assume that all categories are known in advance and that experts can label every example. This paper addresses the limitations of this assumption by presenting a novel active learning technique that can discover new classes and handle cases where an expert cannot or will not provide a label. The approach is based on the expected model output change principle and demonstrates substantial performance improvements over previous active learning methods, even outperforming random query selection in realistic scenarios.\n\n---TOPIC---\nActive Learning\n---TOPIC---\nObject Category Discovery\n---TOPIC---\nUnnameable Instances\n---TOPIC---\nOpen Set Multi-Class Classification\n---TOPIC---\nRealistic Annotation Scenarios",
    "topics": [],
    "references": [
      {
        "citation": "[Huang, G. B., Ramesh, M., Berg, T., & Learned-Miller, E. Labeled faces in the wild: A database for studying face recognition in unconstrained environments. Technical Report 07-49, University of Massachusetts, Amherst, 2007.]"
      },
      {
        "citation": "[Bache, K., & Lichman, M. UCI machine learning repository, 2013.]"
      },
      {
        "citation": "[Lewis, D. D., & Gale, W. A. A sequential algorithm for training text classifiers. In International Conference on Research and Development in Information Retrieval (SIGIR), pages 3–12, 2001.]"
      },
      {
        "citation": "[Jain, P., & Kapoor, A. Active learning for large multi-class problems. In Conference on Computer Vision and Pattern Recognition (CVPR), pages 762 –769, 2009.]"
      },
      {
        "citation": "[Vijayanarasimhan, S., & Grauman, K. Cost-sensitive active visual category learning. International Journal of Computer Vision (IJCV), 91:24–44, 2011.]"
      },
      {
        "citation": "[Rodner, E., Freytag, A., Bodesheim, P., & Denzler, J. Large-scale gaussian process classiﬁcation with ﬂexible adaptive histogram kernels. In European Conference on Computer Vision (ECCV), pages 85–98, 2012.]"
      },
      {
        "citation": "[Hospedales, T. M., Gong, S., & Xiang, T. A unifying theory of active discovery and learning. In European Conference on Computer Vision (ECCV), pages 453–466, 2012.]"
      },
      {
        "citation": "[Long, C., Hua, G., & Kapoor, A. Active visual recognition with expertise estimation in crowdsourcing. In International Conference on Computer Vision (ICCV), pages 3000–3007, 2013.]"
      },
      {
        "citation": "[Kr¨ahenb¨uhl, P., & Koltun, V. Geodesic object proposals. In European Conference on Computer Vision (ECCV), pages 725–739, 2014.]"
      },
      {
        "citation": "[Freytag, A., Rodner, E., Bodesheim, P., & Denzler, J. Labeling examples that matter: Relevance-based active learning with gaussian processes. In German Conference on Pattern Recognition (GCPR), pages 282–291, 2013.]"
      }
    ],
    "author_details": [
      {
        "name": "Christoph K¨ading",
        "affiliation": "Computer Vision Group, Friedrich Schiller University Jena, Germany",
        "email": "firstname.lastname@uni-jena.de"
      },
      {
        "name": "Alexander Freytag",
        "affiliation": "Computer Vision Group, Friedrich Schiller University Jena, Germany",
        "email": "firstname.lastname@uni-jena.de"
      },
      {
        "name": "Erik Rodner",
        "affiliation": "Computer Vision Group, Friedrich Schiller University Jena, Germany",
        "email": "firstname.lastname@uni-jena.de"
      },
      {
        "name": "Paul Bodesheim",
        "affiliation": "Computer Vision Group, Friedrich Schiller University Jena, Germany",
        "email": "firstname.lastname@uni-jena.de"
      },
      {
        "name": "Joachim Denzler",
        "affiliation": "Computer Vision Group, Friedrich Schiller University Jena, Germany",
        "email": "firstname.lastname@uni-jena.de"
      }
    ]
  },
  {
    "title": "Depth from Focus with Your Mobile Phone\n---AUTHOR---\nSupasorn Suwajanakorn\nCarlos Hernandez\nSteven M. Seitz",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Suwajanakorn_Depth_From_Focus_2015_CVPR_paper.pdf",
    "id": "Suwajanakorn_Depth_From_Focus_2015_CVPR_paper",
    "abstract": "While prior depth from focus and defocus techniques operated on laboratory scenes, we introduce the first depth from focus (DfF) method capable of handling images from mobile phones and other hand-held cameras. Achieving this goal requires solving a novel uncalibrated DfF problem and aligning the frames to account for scene parallax. Our approach is demonstrated on a range of challenging cases and produces high quality results.",
    "topics": [
      "Depth from Focus (DfF)",
      "Uncalibrated Camera Systems",
      "Hand-held Devices",
      "Optical Flow",
      "Scene Depth Estimation"
    ],
    "references": [
      {
        "citation": "[Sameer Agarwal, Keir Mierle, and Others. Ceres solver. https://code.google.com/p/ceres-solver/. ] - This is a widely used optimization library, likely relevant for many vision tasks."
      },
      {
        "citation": "[Yuri Boykov and Vladimir Kolmogorov. An experimental comparison of min-cut/max-ﬂow algorithms for energy minimization in vision. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 26(9):1124–1137, 2004.] - Graph cuts are a common technique in computer vision, and this paper provides a comparison of different algorithms."
      },
      {
        "citation": "[Antonin Chambolle. An algorithm for total variation minimization and applications. Journal of Mathematical imaging and vision, 20(1-2):89–97, 2004.] - Total variation minimization is a common denoising technique, often used in depth estimation."
      },
      {
        "citation": "[Shree K Nayar and Yasuo Nakagawa. Shape from focus. Pattern analysis and machine intelligence, IEEE Transactions on, 16(8):824–831, 1994.] - A foundational paper on shape from focus."
      },
      {
        "citation": "[Leonid I Rudin, Stanley Osher, and Emad Fatemi. Nonlinear total variation based noise removal algorithms. Physica D: Nonlinear Phenomena, 60(1):259–268, 1992.] - This paper introduces a seminal total variation denoising algorithm."
      },
      {
        "citation": "[Ray A Jarvis. A perspective on range finding techniques for computer vision. Pattern Analysis and Machine Intelligence, IEEE Transactions on, (2):122–139, 1983.] - Provides a historical overview of range finding, relevant to depth estimation."
      },
      {
        "citation": "[Vladimir Kolmogorov and Ramin Zabin. What energy functions can be minimized via graph cuts? Pattern Analysis and Machine Intelligence, IEEE Transactions on, 26(2):147–159, 2004.] - Explores the theoretical limits of graph cuts, important for understanding their applicability."
      },
      {
        "citation": "[Masahiro Watanabe and Shree K Nayar. Telecentric optics for computational vision. Computer Vision-ECCV’96, pages 439–451. Springer, 1996.] - Telecentric optics are relevant for accurate depth estimation."
      },
      {
        "citation": "[Trevor Darrell and Kwangyoen Wohn. Pyramid based depth from focus. Computer Vision and Pattern Recognition, 1988. Proceedings CVPR’88., Computer Society Conference on, pages 1–509. IEEE, 1988.] - Early work on depth from focus using pyramid techniques."
      },
      {
        "citation": "[Quanbing Zhang and Yanyan Gong. A novel technique of image-based camera calibration in depth-from-defocus. Intelligent Networks and Intelligent Systems, 2008. ICINIS’08. First International Conference on, pages 483–486. IEEE, 2008.] - Camera calibration is crucial for accurate depth recovery."
      }
    ],
    "author_details": [
      {
        "name": "Supasorn Suwajanakorn",
        "affiliation": "University of Washington",
        "email": "[Email not available in provided text]"
      },
      {
        "name": "Carlos Hernandez",
        "affiliation": "Google Inc.",
        "email": "[Email not available in provided text]"
      },
      {
        "name": "Steven M. Seitz",
        "affiliation": "University of Washington",
        "email": "[Email not available in provided text]"
      }
    ]
  },
  {
    "title": "Unsupervised Learning of Complex Articulated Kinematic Structures combining Motion and Skeleton Information\n---AUTHOR---\nHyung Jin Chang\n---AUTHOR---\nYiannis Demiris",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Chang_Unsupervised_Learning_of_2015_CVPR_paper.pdf",
    "id": "Chang_Unsupervised_Learning_of_2015_CVPR_paper",
    "abstract": "In this paper we present a novel framework for unsupervised kinematic structure learning of complex articulated objects from a single-view image sequence. In contrast to prior motion information based methods, which estimate relatively simple articulations, our method can generate arbitrarily complex kinematic structures with skeletal topology by a successive iterative merge process. The iterative merge process is guided by a skeleton distance function which is generated from a novel object boundary generation method from sparse points. Our main contributions are: (i) Unsupervised complex articulated kinematic structure learning by combining motion and skeleton information; (ii) Iterative fine-to-coarse merging strategy for adaptive motion segmentation and structure smoothing; (iii) Skeleton estimation from sparse feature points; (iv) A new highly articulated object dataset containing multi-stage complexity with ground truth. Our experiments show that the proposed method out-performs state-of-the-art methods both quantitatively and qualitatively.\n\n---TOPICICS---\nUnsupervised kinematic structure learning\nArticulated object motion\nSkeleton distance function\nIterative merging strategy\nSparse feature points",
    "topics": [],
    "references": [
      {
        "citation": "[H. Blum and R. N. Nagel, Shape description using weighted symmetric axis features, Pattern Recognition, 1978]"
      },
      {
        "citation": "[J. Shotton, R. Girshick, A. Fitzgibbon, T. Sharp, M. Cook, M. Finocchio, R. Moore, P. Kohli, A. Criminis, A. Kipman, and A. Blake, Efficient human pose estimation from single depth images, IEEE Transactions on Pattern Analysis and Machine Intelligence, 2013]"
      },
      {
        "citation": "[J. Costeira and T. Kanade, A multibody factorization method for independently moving objects, International Journal of Computer Vision, 1998]"
      },
      {
        "citation": "[E. Elhamifar and R. Vidal, Sparse subspace clustering, CVPR, 2009]"
      },
      {
        "citation": "[J. Fayad, C. Russell, and L. Agapito, Automated articulated structure and 3D shape recovery from point correspondences, ICCV, 2011]"
      },
      {
        "citation": "[F. Flores-Mangas and A. Jepson, Fast rigid motion segmentation via incrementally-complex local models, CVPR, 2013]"
      },
      {
        "citation": "[S. R. Gunn, Support vector machines for classification and regression, Technical report, University of Southhampton, 1988]"
      },
      {
        "citation": "[P. J. Kim, Fast incremental learning for one-class support vector classifiers, PhD thesis, Seoul National University, 2008]"
      },
      {
        "citation": "[A. K. Jain, Fundamentals of Digital Image Processing, Prentice-Hall, 1989]"
      },
      {
        "citation": "[J. Sturm, C. Plagemann, and W. Burgard, Unsupervised body scheme learning through self-perception, IEEE International Conference on Robotics and Automation, 2008]"
      }
    ],
    "author_details": [
      {
        "name": "Hyung Jin Chang",
        "affiliation": "Imperial College London, United Kingdom",
        "email": "hj.chang@imperial.ac.uk"
      },
      {
        "name": "Yiannis Demiris",
        "affiliation": "Imperial College London, United Kingdom",
        "email": "y.demiris@imperial.ac.uk"
      }
    ]
  },
  {
    "title": "Reconstructing the World* in Six Days *(As Captured by the Yahoo 100 Million Image Dataset)\n---AUTHOR---\nJared Heinly\n---AUTHOR---\nJohannes L. Sch¨onberger\n---AUTHOR---\nEnrique Dunn\n---AUTHOR---\nJan-Michael Frahm",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Heinly_Reconstructing_the_World_2015_CVPR_paper.pdf",
    "id": "Heinly_Reconstructing_the_World_2015_CVPR_paper",
    "abstract": "We propose a novel, large-scale, structure-from-motion framework that advances the state of the art in data scalability from city-scale modeling (millions of images) to world-scale modeling (several tens of millions of images) using just a single computer. The main enabling technology is the use of a streaming-based framework for connected component discovery. Moreover, our system employs an adaptive, online, iconic image clustering approach based on an augmented bag-of-words representation, in order to balance the goals of registration, comprehensiveness, and data compactness. We demonstrate our proposal by operating on a recent publicly available 100 million image crowd-sourced photo collection containing images geographically distributed throughout the entire world. Results illustrate that our streaming-based approach does not compromise model completeness, but achieves unprecedented levels of efficiency and scalability.",
    "topics": [
      "Structure-from-Motion (SfM)",
      "Large-Scale Modeling (LS-SfM)",
      "Streaming-Based Processing",
      "Image Clustering",
      "World-Scale Reconstruction"
    ],
    "references": [
      {
        "citation": "[D. Nister and H. Stewenius, Scalable recognition with a vocabulary tree, Computer Vision and Pattern Recognition, 2006.]"
      },
      {
        "citation": "[R. Raguram, J. Frahm, and M. Pollefeys, ARRSAC: Adaptive Real-Time Random Sample Consensus, ECCV, 2008.]"
      },
      {
        "citation": "[S. Agarwal, Y. Furukawa, N. Snavely, I. Simon, B. Curless, S. Seitz, and R. Szeliski, Building Rome in a Day, Comm. ACM, 2011.]"
      },
      {
        "citation": "[S. Agarwal, N. Snavely, I. Simon, S. Seitz, and R. Szeliski, Building Rome in a Day, ICCV, 2009.]"
      },
      {
        "citation": "[O. Chum, J. Philbin, J. Sivic, M. Isard, and A. Zisserman, Total recall: Automatic query expansion with a generative feature model for object retrieval, Computer Vision, 2007.]"
      },
      {
        "citation": "[D. Crandall, A. Owens, N. Snavely, and D. Huttenlocher, SfM with MRFs: Discrete-Continuous Optimization for Large-Scale Structure from Motion, PAM, 2013.]"
      },
      {
        "citation": "[J. Frahm, P. Fite-Georgel, D. Gallup, T. Johnson, R. Ragu-ram, C. Wu, Y. Jen, E. Dunn, B. Clipp, S. Lazebnik, and M. Pollefeys, Building Rome on a Cloudless Day, ECCV, 2010.]"
      },
      {
        "citation": "[M. Havlena and K. Schindler, VocMatch: Efﬁcient Mul-tiview Correspondence for Structure from Motion, ECCV, 2014.]"
      },
      {
        "citation": "[K. Heath, N. Gelfand, M. Ovsjanikov, M. Aanjaneya, and L. Guibas, Image Webs: Computing and Exploiting Con-nectivity in Image Collections, CVPR, 2010.]"
      },
      {
        "citation": "[N. Snavely, S. Seitz, and R. Szeliski, Photo Tourism: Exploring Photo Collections in 3D, SIGG, 2006.]"
      }
    ],
    "author_details": [
      {
        "name": "Jared Heinly",
        "affiliation": "Department of Computer Science, The University of North Carolina at Chapel Hill",
        "email": "jheinly@cs.unc.edu"
      },
      {
        "name": "Johannes L. Schönberger",
        "affiliation": "Department of Computer Science, The University of North Carolina at Chapel Hill",
        "email": "jsch@cs.unc.edu"
      },
      {
        "name": "Enrique Dunn",
        "affiliation": "Department of Computer Science, The University of North Carolina at Chapel Hill",
        "email": "dunn@cs.unc.edu"
      },
      {
        "name": "Jan-Michael Frahm",
        "affiliation": "Department of Computer Science, The University of North Carolina at Chapel Hill",
        "email": "jmf@cs.unc.edu"
      }
    ]
  },
  {
    "title": "Image Parsing with a Wide Range of Classes and Scene-Level Context\n---AUTHOR---\nMarian George",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/George_Image_Parsing_With_2015_CVPR_paper.pdf",
    "id": "George_Image_Parsing_With_2015_CVPR_paper",
    "abstract": "This paper presents a nonparametric scene parsing approach that improves the overall accuracy, as well as the coverage of foreground classes in scene images. The authors improve the label likelihood estimates at superpixels by merging likelihood scores from different probabilistic classifiers, boosting classification performance and enriching the representation of less-represented classes. They also incorporate semantic context in the parsing process through global label costs, avoiding reliance on image retrieval sets by assigning a global likelihood estimate to each label. The system achieves state-of-the-art performance on the SIFTﬂow dataset and near-record results on LMSun.\n\n---TOPICCS---\nImage Parsing\nScene Understanding\nNonparametric Methods\nSuperpixels\nSemantic Context",
    "topics": [],
    "references": [],
    "author_details": [
      {
        "name": "Marian George",
        "affiliation": "Department of Computer Science, ETH Zurich, Switzerland",
        "email": "[Email address not available in the provided text]"
      }
    ]
  },
  {
    "title": "Transferring a Semantic Representation for Person Re-Identiﬁcation and Search\n---AUTHOR---\nZhiyuan Shi\n---AUTHOR---\nTimothy M. Hospedales\n---AUTHOR---\nTao Xiang",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Shi_Transferring_a_Semantic_2015_CVPR_paper.pdf",
    "id": "Shi_Transferring_a_Semantic_2015_CVPR_paper",
    "abstract": "This paper presents a new semantic attribute learning approach for person re-identification and search. The model is trained on existing fashion photography datasets, either weakly or strongly labelled, and then transferred and adapted to provide a powerful semantic description of surveillance person detections without requiring any surveillance domain supervision. The resulting representation is useful for both unsupervised and supervised person re-identification, achieving state-of-the-art and near state-of-the-art performance respectively. Furthermore, as a semantic representation it allows description-based person search to be integrated within the same framework. The core innovation addresses the limitations of existing attribute-centric approaches by leveraging readily available fashion image attribute annotations and adapting them for use in the surveillance domain.\n\n---TOPICCS---\nPerson Re-Identification\nSemantic Attributes\nTransfer Learning\nDescription-Based Person Search\nDomain Adaptation",
    "topics": [],
    "references": [
      {
        "citation": "[Andrews, S., Ts ochantaridis, I., & Hofmann, T. (2003). Support vector machines for multiple-instance learning. In *NIPS*.]"
      },
      {
        "citation": "[Arbeláez, P., Maire, M., Fowlkes, C., & Malik, J. (2011). Contour detection and hierarchical image segmentation. *TPAMI*.]"
      },
      {
        "citation": "[Babenko, A., Slesarev, A., Chigorin, A., & Lempitsky, V. (2014). Neural codes for image retrieval. In *ECCV*.]"
      },
      {
        "citation": "[Borkman, S. (2007). IEEE International Workshop on Performance Evaluation for Tracking and Surveillance.]"
      },
      {
        "citation": "[Bian, W., Tao, D., & Rui, Y. (2012). Cross-domain human action recognition. *IEEE Transactions on Systems, Man, and Cybernetics*.]"
      },
      {
        "citation": "[Cao, L., Liu, Z., & Huang, T. S. (2010). Cross-dataset action detection. In *CVPR*.]"
      },
      {
        "citation": "[Chen, H., Gallagher, A., & Girod, B. (2012). Describing clothing by semantic attributes. In *ECCV*.]"
      },
      {
        "citation": "[Das, A., Chakraborty, A., & Roy-Chowdhury, A. (2014). Consistent re-identification in a camera network. In *ECCV*.]"
      },
      {
        "citation": "[Deng, Y., Luo, P., Loy, C. C., & Tang, X. (2014). Pedestrian attribute recognition at far distance. In *ACM Multimedia*.]"
      },
      {
        "citation": "[Farenzena, M., Bazzani, L., Perina, A., Murino, V., & Crisan, M. (2010). Person re-identification by symmetry-driven accumulation of local features. In *CVPR*.]"
      }
    ],
    "author_details": [
      {
        "name": "Zhiyuan Shi",
        "affiliation": "Queen Mary, University of London",
        "email": "z.shi@qmul.ac.uk"
      },
      {
        "name": "Timothy M. Hospedales",
        "affiliation": "Queen Mary, University of London",
        "email": "t.hospedales@qmul.ac.uk"
      },
      {
        "name": "Tao Xiang",
        "affiliation": "Queen Mary, University of London",
        "email": "t.xiang@qmul.ac.uk"
      }
    ]
  },
  {
    "title": "GRSA: Generalized Range Swap Algorithm for the Efﬁcient Optimization of MRFs\n---AUTHOR---\nKangwei Liu\nJunge Zhang\nPeipei Yang\nKaiqi Huang",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Liu_GRSA_Generalized_Range_2015_CVPR_paper.pdf",
    "id": "Liu_GRSA_Generalized_Range_2015_CVPR_paper",
    "abstract": "Markov Random Fields (MRFs) are a vital tool in various vision tasks, making their efficient optimization a problem of fundamental importance. Existing range move algorithms, while successful, are limited by their inability to handle general energy functions (restricted to truncated convex functions) and their slow execution speed compared to graph-cut based algorithms. This paper introduces a generalized range swap algorithm (GRSA) to address these limitations. GRSA extends the applicability of range move algorithms to arbitrary semimetric energies by ensuring submodularity on chosen label subsets and utilizes a set cover problem to dynamically optimize moves, significantly reducing computational time. Experimental results demonstrate GRSA's speedup and competitive solution quality.\n\n---TOPIC---\nMarkov Random Fields (MRFs)\nRange Swap Algorithms\nEnergy Function Optimization\nSubmodular Functions\nSet Cover Problems",
    "topics": [],
    "references": [
      {
        "citation": "[Boykov, Y., & Jolly, M.-P. (2000). Interactive organ segmentation using graph cuts. In *Medical Image Computing and Computer-Assisted Intervention*, pages 276–286.] - This paper introduces an interactive organ segmentation method using graph cuts, a foundational work in the field."
      },
      {
        "citation": "[Boykov, Y., Vekhsler, O., & Zabih, R. (2001). Fast approximate energy minimization via graph cuts. *IEEE Transactions on Pattern Analysis and Machine Intelligence*, *23*(11), 1111–1120.] - A seminal paper introducing a fast approximate energy minimization technique using graph cuts."
      },
      {
        "citation": "[Kolmogorov, V., & Rother, C. (2007). Minimizing nonsubmodular functions with graph cuts-a review. *IEEE Transactions on Pattern Analysis and Machine Intelligence*, *29*(7), 1019–1032.] - Provides a review of minimizing non-submodular functions using graph cuts, a crucial concept for many applications."
      },
      {
        "citation": "[Kolmogorov, V. (2006). Convergent tree-reweighted message passing for energy minimization. *IEEE Transactions on Pattern Analysis and Machine Intelligence*, *28*(10), 1687–1699.] - Introduces a specific message passing algorithm for energy minimization, important for iterative refinement."
      },
      {
        "citation": "[Vekhsler, O. (2007). Graph cut based optimization for mrfs with truncated convex priors. In *IEEE Conference on Computer Vision and Pattern Recognition*, 2007.] - Focuses on graph cut optimization for MRFs with truncated convex priors, a common scenario."
      },
      {
        "citation": "[Kumar, M. P., & Torr, P. H. (2008). Improved moves for truncated convex models. In *Advances in Neural Information Processing Systems*, 2008.] - Presents improvements to moves for truncated convex models, a refinement of existing techniques."
      },
      {
        "citation": "[Besag, J. (1986). On the statistical analysis of dirty pictures. *Journal of the Royal Statistical Society*, *48*(3), 273–285.] - A foundational paper providing statistical analysis relevant to image processing and MRFs."
      },
      {
        "citation": "[Boykov, Y., & Jolly, M.-P. (2001). Interactive graph cuts for optimal boundary and region segmentation of objects in nd images. In *IEEE International Conference on Computer Vision*, 2001.] - Introduces interactive graph cuts for segmentation."
      },
      {
        "citation": "[Kolmogorov, V., & Zabin, V. (2004). What energy functions can be minimized via graph cuts?. *IEEE Transactions on Pattern Analysis and Machine Intelligence*, *26*(2), 135–148.] - Defines the scope of energy functions that can be minimized using graph cuts."
      },
      {
        "citation": "[Kumar, M. P., Vekhsler, O., & Torr, P. H. (2011). Improved moves for truncated convex models. *The Journal of Machine Learning Research*, *12*, 2011.] - Presents improvements to moves for truncated convex models."
      },
      {
        "citation": "---"
      }
    ],
    "author_details": [
      {
        "name": "Kangwei Liu",
        "affiliation": "Center for Research on Intelligent Perception and Computing (CRI-PAC), National Laboratory of Pattern Recognition (NLPR), Institute of Automation, Chinese Academy of Sciences (CASIA)",
        "email": "kwliu@nlpr.ia.ac.cn"
      },
      {
        "name": "Junge Zhang",
        "affiliation": "Center for Research on Intelligent Perception and Computing (CRI-PAC), National Laboratory of Pattern Recognition (NLPR), Institute of Automation, Chinese Academy of Sciences (CASIA)",
        "email": "jgzhang@nlpr.ia.ac.cn"
      },
      {
        "name": "Peipei Yang",
        "affiliation": "Center for Research on Intelligent Perception and Computing (CRI-PAC), National Laboratory of Pattern Recognition (NLPR), Institute of Automation, Chinese Academy of Sciences (CASIA)",
        "email": "ppyang@nlpr.ia.ac.cn"
      },
      {
        "name": "Kaiqi Huang",
        "affiliation": "Center for Research on Intelligent Perception and Computing (CRI-PAC), National Laboratory of Pattern Recognition (NLPR), Institute of Automation, Chinese Academy of Sciences (CASIA)",
        "email": "kqhuang@nlpr.ia.ac.cn"
      }
    ]
  },
  {
    "title": "Hashing with Binary Autoencoders\n---AUTHOR---\nMiguel ´A. Carreira-Perpi˜n´an\n---AUTHOR---\nRamin Raziperchikolaei",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Carreira-Perpinan_Hashing_With_Binary_2015_CVPR_paper.pdf",
    "id": "Carreira-Perpinan_Hashing_With_Binary_2015_CVPR_paper",
    "abstract": "This paper addresses the problem of binary hashing, where high-dimensional vectors are mapped to low-dimensional binary vectors to enable fast search in databases. Existing approaches often approximate the optimization process by relaxing binary constraints and then binarizing the result. This paper focuses on the binary autoencoder model, which seeks to reconstruct an image from the binary code produced by the hash function. The authors demonstrate that the optimization can be simplified using the method of auxiliary coordinates, allowing for efficient training. Experimental results show that the resulting hash function performs competitively with state-of-the-art methods.",
    "topics": [
      "Binary Hashing",
      "Autoencoders",
      "Auxiliary Coordinates (MAC)",
      "Dimensionality Reduction",
      "Image Retrieval"
    ],
    "references": [
      {
        "citation": "[Andoni, A., & Indyk, P. (2008). Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions. *Communications of the ACM*, *51*(1), 117–122.]"
      },
      {
        "citation": "[Beck, A., & Teboulle, M. (2000). Global optimality conditions for quadratic optimization problems with binary constraints. *SIAM Journal on Optimization*, *11*(1), 179–188.]"
      },
      {
        "citation": "[Carreira-Perpiñán, M. A. (2010). The elastic embedding algorithm for dimensionality reduction. In *Proc. of the 27th Int. Conf. Machine Learning (ICML 2010)*, pages 167–174.]"
      },
      {
        "citation": "[Carreira-Perpiñán, M. A. (2014). An ADMM algorithm for solving a proximal bound-constrained quadratic program. *arXiv:1412.8493 [math.OC].*]"
      },
      {
        "citation": "[Carreira-Perpiñán, M. A., & Wang, W. (2012). Distributed optimization of deeply nested systems. *arXiv:1212.5921 [cs.LG].*]"
      },
      {
        "citation": "[Chua, T.-S., Tang, J., Hong, R., Li, H., Luo, Z., & Zheng, Y.-T. (2009). NUS-WIDE: A real-world web image database from National University of Singapore. In *Proc. ACM Conf. Image and Video Retrieval (CIVR’09)*, Santorini, Greece, July 8–10 2009.]"
      },
      {
        "citation": "[Gong, Y., Lazebnik, S., Gordo, A., & Perronnin, F. (2013). Iterative quantization: A Procustean approach to learning binary codes for large-scale image retrieval. *IEEE Transactions on Pattern Analysis and Machine Intelligence*, *35*(12), 2916–2929.]"
      },
      {
        "citation": "[Heo, J.-P., Lee, Y., He, J., Chang, S.-F., & Yoon, S.-E. (2012). Spherical hashing. In *Proc. of the 2012 IEEE Computer Society Conf. Computer Vision and Pattern Recognition (CVPR’12)*, pages 2957–2964.]"
      },
      {
        "citation": "[Jégou, H., Douze, M., & Schmid, C. (2011). Product quantization for nearest neighbor search. *IEEE Transactions on Pattern Analysis and Machine Intelligence*, *33*(1), 117–128.]"
      },
      {
        "citation": "[Krizhevsky, A. (2009). Learning multiple layers of features from tiny images. Master’s thesis, Dept. of Computer Science, University of Toronto.]"
      }
    ],
    "author_details": [
      {
        "name": "Miguel ´A. Carreira-Perpi˜n´an",
        "affiliation": "EECS, University of California, Merced",
        "email": "Not available in the provided text."
      },
      {
        "name": "Ramin Raziperchikolaei",
        "affiliation": "EECS, University of California, Merced",
        "email": "Not available in the provided text."
      }
    ]
  },
  {
    "title": "Multi-Objective Convolutional Learning for Face Labeling\n---AUTHOR---\nSifei Liu\nJimei Yang\nChang Huang\nMing-Hsuan Yang",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Liu_Multi-Objective_Convolutional_Learning_2015_CVPR_supplemental.pdf",
    "id": "Liu_Multi-Objective_Convolutional_Learning_2015_CVPR_supplemental",
    "abstract": "This paper presents a multi-objective convolutional learning approach for face labeling that incorporates a nonparametric shape prior to improve performance and accelerate training. The prior is constructed based on keypoints and utilizes exemplar selection and alignment to guide the labeling process. Experimental results on the LFW-PL dataset demonstrate that the proposed method achieves state-of-the-art accuracy and significantly speeds up convergence compared to existing approaches. The method's efficiency is also highlighted, with a runtime of approximately 120ms for forward propagation of a single input.",
    "topics": [
      "Multi-objective convolutional learning",
      "Nonparametric shape prior",
      "Face labeling",
      "Keypoint detection",
      "Training convergence"
    ],
    "references": [
      {
        "citation": "[Sun, Y., Wang, X., & Tang, X. Deep convolutional network cascade for facial point detection. *CVPR*, 2013.] (This is the only reference explicitly listed and appears to be central to the work, given its inclusion in the provided text.)"
      }
    ],
    "author_details": [
      {
        "name": "Sifei Liu",
        "affiliation": "UC Merced",
        "email": "Not available"
      },
      {
        "name": "Jimei Yang",
        "affiliation": "UC Merced",
        "email": "Not available"
      },
      {
        "name": "Chang Huang",
        "affiliation": "IDL, Baidu Inc.",
        "email": "Not available"
      },
      {
        "name": "Ming-Hsuan Yang",
        "affiliation": "UC Merced",
        "email": "Not available"
      }
    ]
  },
  {
    "title": "End-to-End Integration of a Convolutional Network, Deformable Parts Model and Non-Maximum Suppression\n---AUTHOR---\nLi Wan\nDavid Eigen\nRob Fergus",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Wan_End-to-End_Integration_of_2015_CVPR_paper.pdf",
    "id": "Wan_End-to-End_Integration_of_2015_CVPR_paper",
    "abstract": "This paper proposes a new object detection model that combines the strengths of Deformable Parts Models (DPMs) and Convolutional Networks (ConvNets). DPMs excel at modeling object composition and spatial relationships, while ConvNets produce powerful image features. The model integrates these approaches, training them jointly using a new structured loss function that considers all bounding boxes within an image. This allows for the integration of non-maximal suppression (NMS) into the model, enabling end-to-end discriminative training. The system is evaluated on the PASCAL VOC 2007 and 2011 datasets, achieving competitive results.\n\n---TOPICCS---\nObject Detection\nConvolutional Networks (ConvNets)\nDeformable Parts Models (DPMs)\nNon-Maximum Suppression (NMS)\nEnd-to-End Training",
    "topics": [],
    "references": [
      {
        "citation": "[M. B. Blaschko, A. Vedaldi, and A. Zisserman. Simultaneous object detection and ranking with weak supervision. Advances in Neural Information Processing Systems, 2010.]"
      },
      {
        "citation": "[Y. Chen, L. Zhu, and A. L. Yuille. Active mask hierarchies for object detection. Lecture Notes in Computer Science, 6315, 43–56, 2010.]"
      },
      {
        "citation": "[N. Dalal and B. Triggs. Histograms of oriented gradients for human detection. CVPR, 2005.]"
      },
      {
        "citation": "[P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and D. Ramanan. Object detection with discriminatively trained part-based models. IEEE Trans. Pattern Anal. Mach. Intell., 32(9):1627–1645, Sept. 2010.]"
      },
      {
        "citation": "[R. B. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. CoRR, abs/1311.2524, 2013.]"
      },
      {
        "citation": "[A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks. pages 1106–1114, 2012.]"
      },
      {
        "citation": "[Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278 –2324, nov 1998.]"
      },
      {
        "citation": "[X. Ren and D. Ramanan. Histograms of sparse codes for object detection. 2013 IEEE Conference on Computer Vision and Pattern Recognition, 0:3246–3253, 4013.]"
      },
      {
        "citation": "[P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and Y. LeCun. Overfeat: Integrated recognition, localization and detection using convolutional networks. CoRR, abs/1312.6229, 2013.]"
      },
      {
        "citation": "[M. D. Zeiler and R. Fergus. Visualizing and understanding convolutional networks. CoRR, abs/1311.2901, 2013.]"
      }
    ],
    "author_details": [
      {
        "name": "Li Wan",
        "affiliation": "Dept. of Computer Science, Courant Institute, New York University",
        "email": "wanli@cs.nyu.edu"
      },
      {
        "name": "David Eigen",
        "affiliation": "Dept. of Computer Science, Courant Institute, New York University",
        "email": "deigen@cs.nyu.edu"
      },
      {
        "name": "Rob Fergus",
        "affiliation": "Dept. of Computer Science, Courant Institute, New York University",
        "email": "fergus@cs.nyu.edu"
      }
    ]
  },
  {
    "title": "Multi-instance Object Segmentation with Occlusion Handling\n---AUTHOR---\nYi-Ting Chen\nXiaokai Liu\nMing-Hsuan Yang",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Chen_Multi-Instance_Object_Segmentation_2015_CVPR_paper.pdf",
    "id": "Chen_Multi-Instance_Object_Segmentation_2015_CVPR_paper",
    "abstract": "We present a multi-instance object segmentation algorithm to tackle occlusions. As an object is split into two parts by an occluder, it is nearly impossible to group the two separate regions into an instance by purely bottom-up schemes. To address this problem, we propose to incorporate top-down category specific reasoning and shape prediction through exemplars into an intuitive energy minimization framework. We perform extensive evaluations of our method on the challenging PASCAL VOC 2012 segmentation set. The proposed algorithm achieves favorable results on the joint detection and segmentation task against the state-of-the-art method both quantitatively and qualitatively.\n\n---TOPICICS---\nObject Segmentation\nOcclusion Handling\nEnergy Minimization\nMulti-instance Learning\nComputer Vision",
    "topics": [],
    "references": [
      {
        "citation": "[P. Arbeláez, B. Hariharan, C. G. S. Gupta, L. Bourdev, and J. Malik. Semantic segmentation using regions and parts. In CVPR, 2012.] - This paper is central to the theme of semantic segmentation, a key area of focus."
      },
      {
        "citation": "[P. Arbeláez, M. Maire, C. Fowlkes, and J. Malik. Contour detection and hierarchical image segmentation. PAMI, 33(5):898–916, 2011.] - This paper provides a foundational work on contour detection and image segmentation."
      },
      {
        "citation": "[B. Hariharan, P. Arbeláez, R. Girshick, and J. Malik. Simultaneous detection and segmentation. In ECCV, 2014.] - This paper directly addresses the simultaneous detection and segmentation problem, a core focus."
      },
      {
        "citation": "[R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In CVPR, 2014.] - This paper introduces rich feature hierarchies, a significant advancement in object detection and semantic segmentation."
      },
      {
        "citation": "[P. Arbeláez, J. Pont-Tuset, J. T. Barron, F. Marques, and J. Malik. Multiscale combinatorial grouping. In CVPR, 2014.] - This paper presents a method for multiscale grouping, relevant to understanding image structure."
      },
      {
        "citation": "[B. Frey and D. Dueck. Object detection with discriminatively trained part based models. Science, 2007.] - This is a seminal work introducing part-based models for object detection."
      },
      {
        "citation": "[P. Dollár and C. L. Zitnick. Structured forests for fast edge detection. In ICCV, 2013.] - This paper introduces a fast edge detection method, important for segmentation."
      },
      {
        "citation": "[J. Carreira and C. Sminchisescu. CPMC: Automatic object segmentation using constrained parametric min-cuts. PAMI, 34(7):1312–1328, 2012.] - This paper explores constrained parametric min-cuts for object segmentation."
      },
      {
        "citation": "[M. Everingham, L. V. Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The PASCAL Visual Object Classes Challenge (VOC). IJCV, 88(2):303–338, 2010.] - This paper describes the PASCAL VOC challenge, a standard benchmark for object detection and segmentation."
      },
      {
        "citation": "[B. Hariharan, P. Arbeláez, L. Bourdev, S. Maji, and J. Malik. Semantic contours from inverse detectors. In ICCV, 2011.] - This paper focuses on extracting semantic contours, a key component of segmentation."
      }
    ],
    "author_details": [
      {
        "name": "Yi-Ting Chen",
        "affiliation": "University of California at Merced",
        "email": "[Email not available in the provided text]"
      },
      {
        "name": "Xiaokai Liu",
        "affiliation": "University of California at Merced",
        "email": "[Email not available in the provided text]"
      },
      {
        "name": "Ming-Hsuan Yang",
        "affiliation": "University of California at Merced",
        "email": "[Email not available in the provided text]"
      }
    ]
  },
  {
    "title": "Mirror, mirror on the wall, tell me, is the error small?\n---AUTHOR---\nHeng Yang\nIoannis Patras",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Yang_Mirror_Mirror_on_2015_CVPR_paper.pdf",
    "id": "Yang_Mirror_Mirror_on_2015_CVPR_paper",
    "abstract": "This paper investigates the issue of mirror symmetry in object part localization methods. Despite state-of-the-art methods augmenting training sets with mirrored images, results are often not bilaterally symmetric when applied to mirror images. The authors introduce the concept of \"mirrorability\" and a corresponding \"mirror error\" to quantify this asymmetry. They evaluate mirrorability in human pose estimation and face alignment, finding that most methods struggle to preserve symmetry, even though overall performance on original and mirror images is similar. The mirror error is found to be strongly correlated with localization error and is explored for applications like difficult sample selection and feedback in pose regression methods.\n\n---TOPSICS---\nMirrorability\nObject Part Localization\nHuman Pose Estimation\nFace Alignment\nMirror Error",
    "topics": [],
    "references": [
      {
        "citation": "[M. Andriluka, L. Pishchulin, P. Gehler, and B. Schiele. 2d human pose estimation: New benchmark and state of the art analysis. In CVPR, 2014.] - Appears important for establishing a benchmark in human pose estimation."
      },
      {
        "citation": "[A. Asthana, S. Zafeiriou, S. Cheng, and M. Pantic. Incremental face alignment in the wild. In CVPR, 2014.] - Relevant for face alignment specifically in unconstrained environments."
      },
      {
        "citation": "[P. N. Belhumeur, D. W. Jacobs, D. J. Kriegman, and N. Kumar. Localizing parts of faces using a consensus of exemplars. In CVPR, 2011.] - A foundational work on localizing facial parts."
      },
      {
        "citation": "[X. P. Burgos-Artizzu, P. Perona, and P. Doll´ar. Robust face landmark estimation under occlusion. In ICCV, 2013.] - Addresses a common challenge in face alignment: occlusion."
      },
      {
        "citation": "[X. Cao, Y. Wei, F. Wen, and J. Sun. Face alignment by explicit shape regression. In CVPR, 2012.] - Introduces a specific approach to face alignment using shape regression."
      },
      {
        "citation": "[X. Xiong and F. De la Torre. Supervised descent method for solving nonlinear least squares problems in computer vision. arXiv:1405.0601, 2014.] - Presents a method for solving nonlinear least squares problems, likely used in face alignment."
      },
      {
        "citation": "[D. R. Lowe. Object recognition from local scale-invariant features. In ICCV, 1999.] - A seminal work on feature extraction, likely relevant to face recognition and alignment."
      },
      {
        "citation": "[C. Sagonas, G. Tzimiropouulos, S. Zafeiriou, and M. Pantic. 300 faces in-the-wild challenge: The first facial landmark localization challenge. In ICCV, 2013.] - Defines a specific challenge for facial landmark localization."
      },
      {
        "citation": "[G. Tzimiropouulos and M. Pantic. Gaus-newton deformable part models for face alignment in-the-wild. In CVPR, 2014.] - Describes a specific deformable part model approach for face alignment."
      },
      {
        "citation": "[X. Xiong and F. De la Torre. Supervised descent method and its applications to face alignment. In CVPR, 2013.] - Another paper detailing the supervised descent method and its application to face alignment."
      }
    ],
    "author_details": [
      {
        "name": "Heng Yang",
        "affiliation": "Queen Mary University of London",
        "email": "heng.yang@qmul.ac.uk"
      },
      {
        "name": "Ioannis Patras",
        "affiliation": "Queen Mary University of London",
        "email": "I.Patras@qmul.ac.uk"
      }
    ]
  },
  {
    "title": "Video Event Recognition with Deep Hierarchical Context Model\n---AUTHOR---\nXiaoyang Wang\n---AUTHOR---\nQiang Ji",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Wang_Video_Event_Recognition_2015_CVPR_paper.pdf",
    "id": "Wang_Video_Event_Recognition_2015_CVPR_paper",
    "abstract": "Video event recognition faces challenges due to intra-class variation and low image resolution, particularly in surveillance videos. This paper proposes a deep hierarchical context model that simultaneously learns and integrates context at three levels: feature, semantic, and prior. The model learns middle-level representations and combines these with bottom-level feature contexts, middle semantic contexts, and top-level prior contexts for event recognition. Experiments on surveillance video benchmarks demonstrate the model's effectiveness and outperformance compared to existing approaches.",
    "topics": [
      "Video Event Recognition",
      "Deep Hierarchical Context Model",
      "Surveillance Videos",
      "Feature Level Context",
      "Semantic Level Context",
      "Prior Level Context"
    ],
    "references": [],
    "author_details": [
      {
        "name": "Xiaoyang Wang",
        "affiliation": "Dept. of ECSE, Rensselaer Polytechnic Institute, USA",
        "email": "wangx16@rpi.edu"
      },
      {
        "name": "Qiang Ji",
        "affiliation": "Dept. of ECSE, Rensselaer Polytechnic Institute, USA",
        "email": "jiq@rpi.edu"
      }
    ]
  },
  {
    "title": "Person Re-identiﬁcation by Local Maximal Occurrence Representation and Metric Learning\n---AUTHOR---\nShengcai Liao\n---AUTHOR---\nYang Hu\n---AUTHOR---\nXiangyu Zhu\n---AUTHOR---\nStan Z. Li",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Liao_Person_Re-Identification_by_2015_CVPR_paper.pdf",
    "id": "Liao_Person_Re-Identification_by_2015_CVPR_paper",
    "abstract": "Person re-identiﬁcation is an important technique to automatically search for a person’s presence in a surveillance video. Two fundamental problems are critical for person re-identiﬁcation, feature representation and metric learning. An effective feature representation should be robust to illumination and viewpoint changes, and a discriminant metric should be learned to match various person images. In this paper, we propose an effective feature representation called Local Maximal Occurrence (LOMO), and a subspace and metric learning method called Cross-view Quadratic Discriminant Analysis (XQDA). The LOMO feature analyzes the horizontal occurrence of local features, and maximizes the occurrence to make a stable representation against viewpoint changes. Besides, to handle illumination variations, we apply the Retinex transform and a scale invariant texture operator. To learn a discriminant metric, we propose to learn a discriminant low dimensional subspace by cross-view quadratic discriminant analysis, and simultaneously, a QDA metric is learned on the derived subspace. Experiments on four challenging person re-identiﬁcation databases, VIPeR, QMUL GRID, CUHK Campus, and CUHK03, show that the proposed method improves the state-of-the-art rank-1 identification rates by 2.2%, 4.88%, 28.91%, and 31.55% on the four databases, respectively.",
    "topics": [
      "Person Re-identification",
      "Feature Representation (LOMO)",
      "Metric Learning (XQDA)",
      "Illumination Variation Handling (Retinex Transform)",
      "Viewpoint Invariance"
    ],
    "references": [],
    "author_details": [
      {
        "name": "Shengcai Liao",
        "affiliation": "Center for Biometrics and Security Research, National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences",
        "email": "scliao@nlpr.ia.ac.cn"
      },
      {
        "name": "Yang Hu",
        "affiliation": "Center for Biometrics and Security Research, National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences",
        "email": "yhu@nlpr.ia.ac.cn"
      },
      {
        "name": "Xiangyu Zhu",
        "affiliation": "Center for Biometrics and Security Research, National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences",
        "email": "xiangyu.zhu@nlpr.ia.ac.cn"
      },
      {
        "name": "Stan Z. Li",
        "affiliation": "Center for Biometrics and Security Research, National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences",
        "email": "szli@nlpr.ia.ac.cn"
      }
    ]
  },
  {
    "title": "A Mixed Bag of Emotions: Model, Predict, and Transfer Emotion Distributions\n---AUTHOR---\nAndrew Gallagher\nAamir Sadovnik\nKuan-Chuan Peng\nTsuhan Chen",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Peng_A_Mixed_Bag_2015_CVPR_supplemental.pdf",
    "id": "Peng_A_Mixed_Bag_2015_CVPR_supplemental",
    "abstract": "This paper introduces Emotion6, a new dataset of images designed to explore the complexities of human emotion perception. The dataset is built upon the six basic emotions proposed by Ekman, but acknowledges that images often evoke a \"mixed bag\" of emotions rather than a single, dominant one. The authors present statistics on the distribution of emotions evoked by images, noting discrepancies between search keywords used to collect images and the emotions actually perceived by viewers. They also analyze the relationship between emotions and valence-arousal scores, highlighting images with high variance in subjective emotional responses. The work aims to provide a more nuanced understanding of how emotions are represented in visual content and to address limitations of previous emotion datasets.\n\n---TOPICICS---\nEmotion Recognition\nImage Datasets\nValence-Arousal Model\nMixed Emotions\nSubjective Perception",
    "topics": [],
    "references": [],
    "author_details": [
      {
        "name": "Andrew Gallagher",
        "affiliation": "Google Inc.",
        "email": "agallagher@google.com"
      },
      {
        "name": "Aamir Sadovnik",
        "affiliation": "Lafayette College",
        "email": "sadovnia@lafayette.edu"
      },
      {
        "name": "Kuan-Chuan Peng",
        "affiliation": "Cornell University",
        "email": "kp388@cornell.edu"
      },
      {
        "name": "Tsuhan Chen",
        "affiliation": "Cornell University",
        "email": "tsuhan@cornell.edu"
      }
    ]
  },
  {
    "title": "Inverting RANSAC: Global Model Detection via Inlier Rate Estimation\n---AUTHORs---\nRoee Litman\nSimon Korman\nAlex Bronstein\nShai Avidan",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Litman_Inverting_RANSAC_Global_2015_CVPR_supplemental.pdf",
    "id": "Litman_Inverting_RANSAC_Global_2015_CVPR_supplemental",
    "abstract": "This paper introduces a novel approach to global model detection by inverting the RANSAC algorithm. Instead of iteratively searching for a model that minimizes the number of outliers, the proposed method estimates the inlier rate directly. This allows for a global optimization of the model parameters, avoiding the local minima often encountered in traditional RANSAC. The paper derives and analyzes the theoretical foundations of this inversion, including proofs and derivations related to the behavior of the inlier rate function. The approach is demonstrated with homography estimation examples.",
    "topics": [
      "Global Model Detection",
      "RANSAC Inversion",
      "Inlier Rate Estimation",
      "Homography Estimation",
      "Optimization"
    ],
    "references": [],
    "author_details": [
      {
        "name": "Roee Litman",
        "affiliation": "Tel-Aviv university",
        "email": "[Not available in provided text]"
      },
      {
        "name": "Simon Korman",
        "affiliation": "Tel-Aviv university",
        "email": "[Not available in provided text]"
      },
      {
        "name": "Alex Bronstein",
        "affiliation": "Tel-Aviv university",
        "email": "[Not available in provided text]"
      },
      {
        "name": "Shai Avidan",
        "affiliation": "Tel-Aviv university",
        "email": "[Not available in provided text]"
      }
    ]
  },
  {
    "title": "Recognize Complex Events from Static Images by Fusing Deep Channels\n---AUTHOR---\nYuanjun Xiong\nKai Zhu\nDahua Lin\nXiaoou Tang",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Xiong_Recognize_Complex_Events_2015_CVPR_supplemental.pdf",
    "id": "Xiong_Recognize_Complex_Events_2015_CVPR_supplemental",
    "abstract": "This paper introduces a method for recognizing complex events from static images by fusing deep channels. The approach leverages a Convolutional Neural Network (CNN) to extract action features from bounding box proposals within images. These features are then combined to create a holistic representation for event recognition. The method is evaluated on the WIDER dataset and compared with existing techniques like Gist, Spatial Pyramid Matching (SPM), and RCNNBank. The paper details the model specification, parameter settings for comparison methods, and provides sample images from the dataset.",
    "topics": [
      "Deep Learning",
      "Event Recognition",
      "Computer Vision",
      "Convolutional Neural Networks (CNNs)",
      "WIDER Dataset"
    ],
    "references": [
      {
        "citation": "[Jia, Y., Shelhammer, E., Donahue, J., Karayev, S., Long, J., Girshick, R., Guadarrama, S., & Darrell, T. (2014). Caffe: Convolutional architecture for fast feature embedding. arXiv preprint arXiv:1408.5093.]"
      },
      {
        "citation": "[Lazebnik, S., Schmid, C., & Ponce, J. (2006). Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories. In Computer Vision and Pattern Recognition, 2006 IEEE Computer Society Conference on (pp. 2169–2178). IEEE.]"
      },
      {
        "citation": "[Li, L.-J., Su, H., Fei-Fei, L., & Xing, E. P. (2010). Object bank: A high-level image representation for scene classification & semantic feature sparsiﬁcation. In Advances in neural information processing systems (pp. 1378–1386).]"
      },
      {
        "citation": "[Oliva, A., & Torralba, A. (2001). Modeling the shape of the scene: A holistic representation of the spatial envelope. International journal of computer vision, 42(3), 145–175.]"
      }
    ],
    "author_details": [
      {
        "name": "Yuanjun Xiong",
        "affiliation": "Department of Information Engineering, The Chinese University of Hong Kong",
        "email": "xy012@ie.cuhk.edu.hk"
      },
      {
        "name": "Kai Zhu",
        "affiliation": "Department of Information Engineering, The Chinese University of Hong Kong",
        "email": "zk013@ie.cuhk.edu.hk"
      },
      {
        "name": "Dahua Lin",
        "affiliation": "Department of Information Engineering, The Chinese University of Hong Kong",
        "email": "dhlin@ie.cuhk.edu.hk"
      },
      {
        "name": "Xiaoou Tang",
        "affiliation": "Department of Information Engineering, The Chinese University of Hong Kong, Shenzhen key lab of Comp. Vis. & Pat. Rec., Shenzhen Institutes of Advanced Technology, CAS, China",
        "email": "xtang@ie.cuhk.edu.hk"
      }
    ]
  },
  {
    "title": "Pushing the Frontiers of Unconstrained Face Detection and Recognition: IARPA Janus Benchmark A ⇤\n---AUTHOR---\nBrendan F. Klare\nBen Klein\nEmma Taborsky\nAustin Blanton\nJordan Cheney\nKristen Allen\nPatrick Grother\nAlan Mah\nAnil K. Jain",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Klare_Pushing_the_Frontiers_2015_CVPR_paper.pdf",
    "id": "Klare_Pushing_the_Frontiers_2015_CVPR_paper",
    "abstract": "The development of accurate and scalable unconstrained face recognition algorithms remains a long-term goal. This paper introduces the IARPA Janus Benchmark A (IJB-A), a publicly available dataset containing 500 subjects with manually localized face images. Key features of the IJB-A dataset include full pose variation, joint use for face recognition and face detection benchmarking, a mix of images and videos, wider geographic variation of subjects, protocols supporting both open-set identification and verification, an optional protocol for gallery subject modeling, and ground truth eye and nose locations. The dataset was developed using 1,501,267 crowd-sourced annotations. Baseline accuracies from commercial and open-source algorithms demonstrate the challenge offered by this new unconstrained benchmark.\n\n---TOPICICS---\nUnconstrained Face Recognition\nFace Detection\nBenchmark Datasets\nPose Variation\nOpen-Set Identification",
    "topics": [],
    "references": [
      {
        "citation": "[Taigman, Y., Yang, M., Ranzato, M., & Wolf, L. (2014). Deepface: Closing the gap to human-level performance in face verification. *IEEE Computer Vision and Pattern Recognition*, 1701–1708.] - Appears to be a key reference, likely related to performance benchmarks."
      },
      {
        "citation": "[IARPA Janus Broad Agency Anouncement, IARPA-BAA-13-07. (2013).] - Likely provides context or funding information for the work."
      },
      {
        "citation": "[Best-Rowden, L., Han, H., Otto, C., Klare, B., & Jain, A. K. (2014). Unconstrained face recognition: Identifying a person of interest from a media collection. *IEEE Transactions on Information Forensics and Security*. ] - Addresses a core problem area."
      },
      {
        "citation": "[Huang, G. B., Ramesh, M., Berg, T., & Learned-Miller, E. (2007). Labeled faces in the wild: A database for studying face recognition in unconstrained environments. Technical Report 07-49, University of Massachusetts, Amherst.] - Introduces a crucial dataset (LFW)."
      },
      {
        "citation": "[Viola, P., & Jones, M. J. (2004). Robust real-time face detection. *International Journal of Computer Vision*, 57(2), 137–154.] - A foundational work on face detection."
      },
      {
        "citation": "[Grother, P., & Ngan, M. (2014). Face recognition vendor test (FRVT): Performance of face identification algorithms. NIST Interagency Report 8009.] - Provides a standard evaluation framework."
      },
      {
        "citation": "[Klontz, J. C., Klare, B. F., Klum, S., Jain, A. K., & Burge, M. J. (2013). Open source biometric recognition. *IEEE Biometrics: Theory, Applications, and Systems* (under review).] - Relevant to open-source implementations."
      },
      {
        "citation": "[Cheney, J., Klein, B., Jain, A. K., & Klare, B. F. (2014). Unconstrained face detection: State of the art baseline and challenges. *IAPR Int. Conference on Biometrics*.] - Discusses challenges in the field."
      },
      {
        "citation": "[Liao, S., Lei, Z., Yi, D., & Li, S. Z. (2014). A benchmark study of large-scale unconstrained face recognition. *International Joint Conference on Biometrics (IJCB)*.] - Presents a benchmark study."
      },
      {
        "citation": "[O’Toole, A. J., An, X., Dunlop, J., Natu, V., & Phillips, P. J. (2012). Comparing face recognition algorithms to humans on challenging tasks. *ACM Transactions on Applied Perception (TAP)*, 9(4), 16.] - Compares algorithms to human performance."
      }
    ],
    "author_details": [
      {
        "name": "Brendan F. Klare",
        "affiliation": "†Indicates affiliation with the National Institute of Standards and Technology (NIST)",
        "email": "Not available in the provided text."
      },
      {
        "name": "Ben Klein",
        "affiliation": "†Indicates affiliation with the National Institute of Standards and Technology (NIST)",
        "email": "Not available in the provided text."
      },
      {
        "name": "Emma Taborsky",
        "affiliation": "†Indicates affiliation with the National Institute of Standards and Technology (NIST)",
        "email": "Not available in the provided text."
      },
      {
        "name": "Austin Blanton",
        "affiliation": "†Indicates affiliation with the National Institute of Standards and Technology (NIST)",
        "email": "Not available in the provided text."
      },
      {
        "name": "Jordan Cheney",
        "affiliation": "†Indicates affiliation with the National Institute of Standards and Technology (NIST)",
        "email": "Not available in the provided text."
      },
      {
        "name": "Kristen Allen",
        "affiliation": "†Indicates affiliation with the National Institute of Standards and Technology (NIST)",
        "email": "Not available in the provided text."
      },
      {
        "name": "Patrick Grother",
        "affiliation": "‡Indicates affiliation with the National Institute of Standards and Technology (NIST)",
        "email": "Not available in the provided text."
      },
      {
        "name": "Alan Mah",
        "affiliation": "§Indicates affiliation with the National Institute of Standards and Technology (NIST)",
        "email": "Not available in the provided text."
      },
      {
        "name": "Anil K. Jain",
        "affiliation": "¶Indicates affiliation with Michigan State University",
        "email": "Not available in the provided text."
      }
    ]
  },
  {
    "title": "A Dynamic Programming Approach for Fast and Robust Object Pose Recognition from Range Images\n---AUTHOR---\nChristopher Zach\nAdrian Penate-Sanchez\nMinh-Tri Pham",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Zach_A_Dynamic_Programming_2015_CVPR_paper.pdf",
    "id": "Zach_A_Dynamic_Programming_2015_CVPR_paper",
    "abstract": "Joint object recognition and pose estimation solely from range images is an important task, particularly in robotics and automated manufacturing. The lack of color information and limitations of current depth sensors make this a challenging problem. This paper proposes a dynamic programming approach to generate promising inlier sets for pose estimation by early rejection of outliers. The method is fast, doesn't require a computationally expensive training phase, and achieves state-of-the-art performance. It addresses challenges such as few salient regions, unreliable depth discontinuities, and uninformative features in range images by assessing the internal consistency of predicted object coordinates.",
    "topics": [
      "Object Recognition",
      "Pose Estimation",
      "Range Images/Depth Sensors",
      "Dynamic Programming",
      "Robustness to Occlusions"
    ],
    "references": [],
    "author_details": [
      {
        "name": "Christopher Zach",
        "affiliation": "Toshiba Research Europe",
        "email": "christopher.m.zach@gmail.com"
      },
      {
        "name": "Adrian Penate-Sanchez",
        "affiliation": "CSIC-UPC",
        "email": "apenate@iri.upc.edu"
      },
      {
        "name": "Minh-Tri Pham",
        "affiliation": "Toshiba Research Europe",
        "email": "mtpham@crl.toshiba.co.uk"
      }
    ]
  },
  {
    "title": "Defocus Deblurring and Superresolution for Time-of-Flight Depth Cameras (Supplementary)\n---AUTHOR---\nLei Xiao\nFelix Heide\nMatthew O’Toole\nAndreas Kolb\nM atthias B. Hullin\nKyros Kutulakos\nWolfgang Heidrich",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Xiao_Defocus_Deblurring_and_2015_CVPR_supplemental.pdf",
    "id": "Xiao_Defocus_Deblurring_and_2015_CVPR_supplemental",
    "abstract": "This supplementary material provides implementation details for the algorithms presented in the main paper, \"Defocus Deblurring and Superresolution for Time-of-Flight Depth Cameras.\" Specifically, we detail the solution methods for the amplitude and depth update steps (Algo. 2 and Algo. 3), including the linear equation systems solved using the left division function in Matlab and the soft shrinkage operators employed. We also provide the analytical Jacobian used to accelerate the Levenberg-Marquardt method for solving the nonlinear least squares problem in Algo. 3.\n\n---TOPICCS---\nTime-of-Flight Depth Cameras\nAlgorithm Implementation Details\nDefocus Deblurring\nSuperresolution\nNonlinear Least Squares Optimization",
    "topics": [],
    "references": [],
    "author_details": [
      {
        "name": "Lei Xiao",
        "affiliation": "KAUST & University of British Columbia (appears to be affiliated with both)",
        "email": "[Email not available in provided text]"
      },
      {
        "name": "Felix Heide",
        "affiliation": "KAUST & University of British Columbia (appears to be affiliated with both)",
        "email": "[Email not available in provided text]"
      },
      {
        "name": "Matthew O’Toole",
        "affiliation": "University of Toronto",
        "email": "[Email not available in provided text]"
      },
      {
        "name": "Andreas Kolb",
        "affiliation": "University of Siegen",
        "email": "[Email not available in provided text]"
      },
      {
        "name": "Matthias B. Hullin",
        "affiliation": "University of Bonn",
        "email": "[Email not available in provided text]"
      },
      {
        "name": "Kyros Kutulakos",
        "affiliation": "University of Toronto",
        "email": "[Email not available in provided text]"
      },
      {
        "name": "Wolfgang Heidrich",
        "affiliation": "KAUST & University of British Columbia (appears to be affiliated with both)",
        "email": "[Email not available in provided text]"
      }
    ]
  },
  {
    "title": "A Graphical Model Approach for Matching Partial Signatures\n---AUTHOR---\nXianzhi Du\nDavid Doermann\nWael AbdAlmageed",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Du_A_Graphical_Model_2015_CVPR_paper.pdf",
    "id": "Du_A_Graphical_Model_2015_CVPR_paper",
    "abstract": "Partial signature matching remains a challenging problem, particularly when dealing with real-world scenarios where signatures are incomplete. Existing methods, including global-shape and point-level approaches, struggle with partial or degraded signatures. This paper introduces a novel partial signature matching method using graphical models. The approach extracts shape context features, utilizes K-means clustering to build a visual vocabulary, and employs supervised latent Dirichlet allocation (sLDA) with hierarchical Dirichlet processes (HDP) to learn latent distributions and infer the number of salient regions. The method is evaluated on multiple datasets and demonstrates effectiveness for both partial and full signature matching.\n---TOPIC---\nSignature Matching\n---TOPIC---\nGraphical Models\n---TOPIC---\nLatent Dirichlet Allocation (sLDA)\n---TOPIC---\nShape Context Features\n---TOPIC---\nPartial Signatures",
    "topics": [],
    "references": [
      {
        "citation": "[Alahi, A., Ortiz, R., & Vandergheynst, P. (2012). Freak: fast retina keypoint. In Proc. CVPR, pages 510–517.] - Likely important as it introduces a keypoint detector, a common component in feature extraction."
      },
      {
        "citation": "[Bay, H., Ess, A., Tuytelaars, T., & Gool, L. (2008). Surf: Speeded up robust features. In CVIU, 110(3):346–359.] - Introduces SURF, another widely used feature descriptor."
      },
      {
        "citation": "[Belongie, S., Malik, J., & Puzicha, J. (2002). Shape matching and object recognition using shape contexts. IEEE Trans. PAMI, 24(4):509–522.] - Shape contexts are a foundational technique for shape-based recognition."
      },
      {
        "citation": "[Blei, D., & McAuliffe, J. (2008). Supervised topic models. In NIPS.] - Introduces supervised topic models, suggesting a potential connection to topic modeling or related techniques."
      },
      {
        "citation": "[Blei, D., Ng, A., & Jordan, M. (2003). Latent dirichlet allocation. Journal of Machine Learning Research, pages 993–1022.] - Latent Dirichlet Allocation (LDA) is a core topic modeling technique."
      },
      {
        "citation": "[Doermann, D. S., & Rosenfeld, A. (1994). Recovery of temporal information from static images of handwriting. International Journal of Computer Vision, 52:143–164.] - Relevant if the paper deals with temporal information or handwriting analysis."
      },
      {
        "citation": "[Du, X., AbdAlmageed, W., & Doermann, D. (2013). Large-scale signature matching using multi-stage hashing. In ICDAR, pages 976–980.] - Directly related to signature matching, a common application area."
      },
      {
        "citation": "[Guo, D. D. K., & Rosenfeld, A. (2001). Forgery detection by local correspondence. International Journal on Pattern Recognition and Artificial Intelligence, 15:579–641.] - Important if the paper addresses forgery detection."
      },
      {
        "citation": "[Lin, C., & Chellappa, R. (1987). Classification of partial 2d shapes using fourier descriptors. IEEE Trans. PAMI, 9(5).] - Introduces Fourier descriptors, a technique for shape representation."
      },
      {
        "citation": "[lamp.cfar.umd.edu] - This is likely a resource or dataset, and its inclusion suggests its importance to the work."
      }
    ],
    "author_details": [
      {
        "name": "Xianzhi Du",
        "email": "xianzhi@umiacs.umd.edu"
      },
      {
        "name": "David Doermann",
        "affiliation": "UMIACS, University of Maryland, College Park",
        "email": "doermann@umiacs.umd.edu"
      },
      {
        "name": "Wael AbdAlmageed",
        "affiliation": "Information Sciences Institute, University of Southern California",
        "email": "wamageed@isi.edu"
      }
    ]
  },
  {
    "title": "Fast Bilateral-Space Stereo for Synthetic Defocus\n---AUTHOR---\nCarlos Hern´andez\nAndrew Adams\nJonathan T. Barron\nYiChang Shih",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Barron_Fast_Bilateral-Space_Stereo_2015_CVPR_paper.pdf",
    "id": "Barron_Fast_Bilateral-Space_Stereo_2015_CVPR_paper",
    "abstract": "Given a stereo pair it is possible to recover a depth map and use that depth to render a synthetically defocused image. Though stereo algorithms are well-studied, rarely are those algorithms considered solely in the context of producing these defocused renderings. In this paper we present a technique for efficiently producing disparity maps using a novel optimization framework in which inference is performed in “bilateral-space”. Our approach produces higher-quality “defocus” results than other stereo algorithms while also being 10 − 100× faster than comparable techniques.\n\n---TOPIPS---\nStereo Vision\nSynthetic Defocus\nBilateral-Space Optimization\nDisparity Map Generation\nMobile Camera Applications",
    "topics": [],
    "references": [
      {
        "citation": "[Adams, A., Baek, J., and Davis, M. A. Fast high-dimensional filtering using the permutatohedral lattice. Eurographics, 2010.] - This paper introduces a method for fast filtering in high-dimensional spaces, likely relevant to efficient stereo matching or related tasks."
      },
      {
        "citation": "[Adams, A., N. Gelfand, J. Dolson, and M. Levoy. Gaussian kd-trees for fast high-dimensional filtering. SIGGRAPH, 2009.] - Builds upon the previous work, detailing a specific data structure for efficient filtering."
      },
      {
        "citation": "[Sinha, S. N., Scharstein, D., and Szeliski, R. Efﬁcient high-resolution stereo matching using local plane sweeps. CVPR, 2014.] - A key paper on high-resolution stereo matching, likely a central technique in the paper this reference list comes from."
      },
      {
        "citation": "[Szeliski, D., and Scharstein, R. A taxonomy and evaluation of dense two-frame stereo correspondence algorithms. IJCV, 2002.] - Provides a foundational overview and evaluation of stereo matching algorithms, important for understanding the field."
      },
      {
        "citation": "[Krähnbühl, P., and Koltun, V. Efﬁcient inference in fully connected crfs with gaussian edge potentials. NIPS, 2011.] - Introduces a method for efficient inference in conditional random fields, a common approach for stereo matching and other vision tasks."
      },
      {
        "citation": "[Krähnbühl, P., and Koltun, V. Efﬁcient nonlocal regularization for optical ﬂow. ECCV, 2012.] -  Presents a technique for efficient non-local regularization, which can be applied to stereo matching and optical flow estimation."
      },
      {
        "citation": "[Szeliski, R. Digital Light Field Photography. PhD thesis, Stanford University, 2006.] - A seminal work on light field photography, which has implications for depth estimation and 3D reconstruction."
      },
      {
        "citation": "[Geiger, A., Lenz, P., and Urtasun, R. Are we ready for autonomous driving? the kitti vision benchmark suite. CVPR, 2012.] - Introduces the KITTI dataset, a standard benchmark for evaluating stereo matching and other vision algorithms."
      },
      {
        "citation": "[Hirschmüller, H. Accurate and efﬁcient stereo processing by semi-global matching and mutual information. CVPR, 2005.] - Introduces Semi-Global Matching (SGM), a widely used stereo matching algorithm."
      },
      {
        "citation": "[Scharstein, D., and Hirschmüller, H. A PICS as Benchmark for Stereo Matching.  Technical Report, 2009.] - Provides a benchmark for stereo matching algorithms, useful for comparing different approaches."
      }
    ],
    "author_details": [
      {
        "name": "Carlos Hern´andez",
        "affiliation": "Not explicitly provided in the text.",
        "email": "chernand@google.com"
      },
      {
        "name": "Andrew Adams",
        "affiliation": "Not explicitly provided in the text.",
        "email": "abadams@google.com"
      },
      {
        "name": "Jonathan T. Barron",
        "affiliation": "Not explicitly provided in the text.",
        "email": "barron@google.com"
      },
      {
        "name": "YiChang Shih",
        "affiliation": "MIT (Massachusetts Institute of Technology)",
        "email": "yichang@mit.edu"
      }
    ]
  },
  {
    "title": "Material Classiﬁcation with Thermal Imagery\n---AUTHOR---\nPhilip Saponaro\n---AUTHOR---\nScott Sorensen\n---AUTHOR---\nAbhishek Kolagunda\n---AUTHOR---\nChandra Kambhamettu",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Saponaro_Material_Classification_With_2015_CVPR_paper.pdf",
    "id": "Saponaro_Material_Classification_With_2015_CVPR_paper",
    "abstract": "Material classification is an important area of research in computer vision. Typical algorithms use color and texture information for classification, but there are problems due to varying lighting conditions and diversity of colors in a single material class. In this work we study the use of long wave infrared (i.e. thermal) imagery for material classification. Thermal imagery has the benefit of relative invariance to color changes, invariance to lighting conditions, and can even work in the dark. We collect a database of 21 different material classes with both color and thermal imagery. We develop a set of features that describe water permeation and heating/cooling properties, and test several variations on these methods to obtain our final classifier. The results show that the proposed method outperforms typical color and texture features, and when combined with color information, the results are improved further.\n\n---TOPICCS---\nThermal Imagery\nMaterial Classification\nWater Permeation\nHeating/Cooling Rates\nInfrared Radiation",
    "topics": [],
    "references": [
      {
        "citation": "[Moore, P. Generalized Inverses of Linear Transformations. (1967)]"
      },
      {
        "citation": "[Kimmel, R. A threshold selection method from gray-level histograms. *IEEE Transactions on Systems, Man, and Cybernetics*, 9(1):62–66, Jan 1979.]"
      },
      {
        "citation": "[Caselles, V., Kimmel, R., & Sapiro, G. Geodesic active contours. *Proceedings, Fifth International Conference on Computer Vision*, pages 694–699, Jun 1995.]"
      },
      {
        "citation": "[Chen, H., & Wolff, L. Polarization phase-based method for material classification and object recognition in computer vision. *Proceedings CVPR ’96*, pages 128–135, Jun 1996.]"
      },
      {
        "citation": "[Crank, J., & Nicolson, P. A practical method for numerical evaluation of solutions of partial differential equations of the heat-conduction type. *Advances in Computational Mathematics*, 6(1):207–226, 1996.]"
      },
      {
        "citation": "[Douherthy, E. *An introduction to morphological image processing*. SPIE Optical Engineering Press, 1992.]"
      },
      {
        "citation": "[Rybicki, G., & Lightman, A. *Radiative Processes in Astrophysics*. Wiley, 2008.]"
      },
      {
        "citation": "[Salamati, N., Fredembach, C., & Ssstrunk, S. Material classification using color and nir images. (Year not provided)]"
      },
      {
        "citation": "[Van Loan, C. *Computational Frameworks for the Fast Fourier Transform*. Society for Industrial and Applied Mathematics, 1992.]"
      },
      {
        "citation": "[Wolff, L. Polarization-based material classification from specular reflection. *IEEE Transactions on Pattern Analysis and Machine Intelligence*, 12(11):1059–1071, Nov 1990.]"
      }
    ],
    "author_details": [
      {
        "name": "Philip Saponaro",
        "affiliation": "University of Delaware",
        "email": "saponaro@udel.edu"
      },
      {
        "name": "Scott Sorensen",
        "affiliation": "University of Delaware",
        "email": "sorensen@udel.edu"
      },
      {
        "name": "Abhishek Kolagunda",
        "affiliation": "University of Delaware",
        "email": "abhi@udel.edu"
      },
      {
        "name": "Chandra Kambhamettu",
        "affiliation": "University of Delaware",
        "email": "chandrak@udel.edu"
      }
    ]
  },
  {
    "title": "Transformation of Markov Random Fields for Marginal Distribution Estimation\n---AUTHOR---\nMasaki Saito\nTakayuki Okatani",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Saito_Transformation_of_Markov_2015_CVPR_supplemental.pdf",
    "id": "Saito_Transformation_of_Markov_2015_CVPR_supplemental",
    "abstract": "This note provides detailed mathematical derivations omitted in the main paper, \"Transformation of Markov Random Fields for Marginal Distribution Estimation.\" It focuses on rewriting equations and clarifying the derivation of the MRF transformation, particularly concerning the estimation of marginal distributions and the minimization of the KL divergence. The note also presents experimental results demonstrating the application of the proposed method for downsizing a CRF for semantic labeling and coarse graining of MRFs.\n\n---TOPIICS---\nMarkov Random Fields (MRFs)\nMarginal Distribution Estimation\nKL Divergence Minimization\nCRF Downsizing\nCoarse Graining",
    "topics": [],
    "references": [],
    "author_details": [
      {
        "name": "Masaki Saito",
        "affiliation": "Tohoku University, Japan",
        "email": "msaito@vision.is.tohoku.ac.jp"
      },
      {
        "name": "Takayuki Okatani",
        "affiliation": "Tohoku University, Japan",
        "email": "okatani@vision.is.tohoku.ac.jp"
      }
    ]
  },
  {
    "title": "Joint Tracking and Segmentation of Multiple Targets\n---AUTHOR---\nAnton Milan\nLaura Leal-Taixé\nKonrad Schindler\nIan Reid",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Milan_Joint_Tracking_and_2015_CVPR_paper.pdf",
    "id": "Milan_Joint_Tracking_and_2015_CVPR_paper",
    "abstract": "Tracking-by-detection has proven to be the most successful strategy to address the task of tracking multiple targets in unconstrained scenarios. Traditionally, a set of sparse detections serves as input to a high-level tracker. We propose a multi-target tracker that exploits low level image information and associates every (super)-pixel to a specific target or classifies it as background. As a result, we obtain a video segmentation in addition to the classical bounding-box representation in unconstrained, real-world videos. Our method shows encouraging results on many standard benchmark sequences and significantly outperforms state-of-the-art tracking-by-detection approaches in crowded scenes with long-term partial occlusions.\n\n---TOPICICS---\nMulti-target tracking\nVideo segmentation\nConditional Random Fields (CRF)\nTracking-by-detection\nImage evidence exploitation",
    "topics": [],
    "references": [
      {
        "citation": "[M. Andriluka, S. Roth, and B. Schiele. People-traking-by-detection and people-detection-by-traking. In CVPR 2008.]"
      },
      {
        "citation": "[A. Andriyenko, K. Schindler, and S. Roth. Discrete-continuous optimization for multi-target tracking. In CVPR 2012.]"
      },
      {
        "citation": "[S.-H. Bae and K.-J. Yoon. Robust online multi-object tracking based on tracklet confidence and online discriminative appearance learning. In CVPR 2014.]"
      },
      {
        "citation": "[R. Benenson, M. Mathias, R. Timofte, and L. V. Gool. Pedestrian detection at 100 frames per second. In CVPR 2012.]"
      },
      {
        "citation": "[J. Bergstra and Y. Bengio. Random search for hyper-parameter optimization. J. Mach. Learn. Res., 13:281–305, Mar. 2012.]"
      },
      {
        "citation": "[J. Berclaz, F. Fleuret, E. T¨uretken, and P. Fua. Multiple object tracking using k-shortest paths optimization. IEEE T. Pattern Anal. Mach. Intell., 33(9):1806–1819, Sept. 2011.]"
      },
      {
        "citation": "[K. Bernardin and R. Stiefelhagen. Evaluating multiple object tracking performance: The CLEAR MOT metrics. Image and Video Processing, 2008(1):1–10, May 2008.]"
      },
      {
        "citation": "[C. Bibby and I. Reid. Real-time tracking of multiple occluding objects using level sets. In CVPR 2010.]"
      },
      {
        "citation": "[W. Brendel, M. R. Amer, and S. Todorovic. Multiobject tracking as maximum weight independent set. In CVPR 2011.]"
      },
      {
        "citation": "[T. Brox and J. Malik. Object segmentation by long term analysis of point trajectoriesIn , ECCV 2010.]"
      }
    ]
  },
  {
    "title": "Learning to Segment Under Various Forms of Weak Supervision\n---AUTHOR---\nJia Xu\nAlexander G. Schwing\nRaquel Urtasun",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Xu_Learning_to_Segment_2015_CVPR_paper.pdf",
    "id": "Xu_Learning_to_Segment_2015_CVPR_paper",
    "abstract": "Despite the promising performance of conventional fully supervised algorithms, semantic segmentation remains a challenging task due to the limited availability of complete annotations. This work addresses this challenge by proposing a unified approach for semantic segmentation that incorporates various forms of weak supervision – image level tags, bounding boxes, and partial labels – to produce a pixel-wise labeling. The approach is evaluated on the Siftﬂow dataset and outperforms the state-of-the-art by 12% on per-class accuracy while maintaining comparable per-pixel accuracy. The method is also computationally efficient, requiring only 20 minutes for learning and a fraction of a second for inference.\n\n---TOPIICS---\nSemantic Segmentation\nWeak Supervision\nPixel-wise Labeling\nImage Tags/Bounding Boxes/Partial Labels\nComputational Efficiency",
    "topics": [],
    "references": [
      {
        "citation": "[Arbeláez, P., Pont-Tuset, J., Barron, J., Marques, F., & Malik, J. (2014). Multiscale Combinatorial Grouping. *Proc. CVPR*.]"
      },
      {
        "citation": "[Boykov, Y., & Jolly, M.-P. (2001). Interactive Graph Cuts for Optimal Boundary and Region Segmentation of Objects in N-D Images. *Proc. ICCV*.]"
      },
      {
        "citation": "[Chen, L. C., Fidler, S., Yuille, A., & Urtasun, R. (2014). Beat the MTurkers: Automatic Image Labeling from Weak 3D Supervision. *Proc. CVPR*.]"
      },
      {
        "citation": "[Eigen, D., & Fergus, R. (2012). Nonparametric image parsing using adaptive neighbor sets. *Proc. CVPR*.]"
      },
      {
        "citation": "[Girshick, R. B., Donahue, J., Darrell, T., & Malik, J. (2014). Rich Feature Hierarchies for Accurate Object Detection and Semantic segmentation. *Proc. CVPR*.]"
      },
      {
        "citation": "[Hariharan, B., Arbeláez, P., Girshick, R., & Malik, J. (2014). Simultaneous detection and segmentation. *Proc. ECCV*.]"
      },
      {
        "citation": "[Joulin, A., Bach, F., & Ponce, J. (2012). Multi-class cosegmentation. *Proc. CVPR*.]"
      },
      {
        "citation": "[Grady, L. (2006). Random walks for image segmentation. *PAMI*, *28*(11), 2006.]"
      },
      {
        "citation": "[Ladický, L., Russell, C., Kohli, P., & Torr, P. H. S. (2010). Graph Cut based Inference with Co-occurrence Statistics. *Proc. ECCV*.]"
      },
      {
        "citation": "[Tighe, J., & Lazebnik, S. (2013). Superparsing - Scalable Nonparametric Image Parsing with Superpixels. *IJCV*.]"
      }
    ],
    "author_details": [
      {
        "name": "Jia Xu",
        "affiliation": "University of Wisconsin-Madison",
        "email": "jiaxu@cs.wisc.edu"
      },
      {
        "name": "Alexander G. Schwing",
        "affiliation": "University of Toronto",
        "email": "{aschwing}@cs.toronto.edu"
      },
      {
        "name": "Raquel Urtasun",
        "affiliation": "University of Toronto",
        "email": "{urtasun}@cs.toronto.edu"
      }
    ]
  },
  {
    "title": "Building Proteins in a Day: Efﬁcient 3D Molecular Reconstruction\n---AUTHOR---\nMarcus A. Brubaker\n---AUTHOR---\nAli Punjani\n---AUTHOR---\nDavid J. Fleet",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Brubaker_Building_Proteins_in_2015_CVPR_supplemental.pdf",
    "id": "Brubaker_Building_Proteins_in_2015_CVPR_supplemental",
    "abstract": "This paper details an efficient method for 3D molecular reconstruction, specifically addressing the challenges of Cryo-EM. The approach utilizes Stochastic Averaged Gradient Descent (SAGD) optimization for Maximum a Posteriori (MAP) estimation, significantly accelerated by the application of importance sampling. The paper provides algorithmic details for both SAGD and the importance sampling process, emphasizing strategies to handle computational complexity and memory constraints inherent in high-resolution reconstructions.\n\n---TOPICCS---\nStochastic Averaged Gradient Descent (SAGD)\nImportance Sampling\nCryo-EM Reconstruction\nMaximum a Posteriori (MAP) Estimation\nOptimization Algorithms",
    "topics": [],
    "references": [],
    "author_details": [
      {
        "name": "Marcus A. Brukaker",
        "affiliation": "University of Toronto",
        "email": "mbrubake@cs.toronto.edu"
      },
      {
        "name": "Ali Punjani",
        "affiliation": "University of Toronto",
        "email": "alipunjani@cs.toronto.edu"
      },
      {
        "name": "David J. Fleet",
        "affiliation": "University of Toronto",
        "email": "fleet@cs.toronto.edu"
      }
    ]
  },
  {
    "title": "Integrating Parametric and Non-parametric Models For Scene Labeling\n---AUTHOR---\nBing Shuai\nGang Wang\nZhen Zuo\nBing Wang\nLifan Zhao",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Shuai_Integrating_Parametric_and_2015_CVPR_paper.pdf",
    "id": "Shuai_Integrating_Parametric_and_2015_CVPR_paper",
    "abstract": "We adopt Convolutional Neural Networks (CNN) as our parametric model to learn discriminative features and classifiers for local patch classification. As visually similar pixels are indistinguishable from local context, we alleviate such ambiguity by introducing a global scene constraint. We estimate the global potential in a non-parametric framework. Furthermore, a large margin based CNN metric learning method is proposed for better global potential estimation. The final pixel class prediction is performed by integrating local and global beliefs. Even without any post-processing, we achieve state-of-the-art performance on SiftFlow and competitive results on Stanford Background benchmark.",
    "topics": [
      "Scene Labeling",
      "Convolutional Neural Networks (CNN)",
      "Global Scene Constraints",
      "Large Margin Metric Learning",
      "Local Context Ambiguity"
    ],
    "references": [
      {
        "citation": "[LeCun, Y., Bottou, L., Bengio, Y., & Haffner, P. (1998). Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11), 2278–2324.]"
      },
      {
        "citation": "[Bulo, S. R., & Kontschieder, P. (2014). Neural decision forests for semantic image labelling. In Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on. IEEE.]"
      },
      {
        "citation": "[Donahue, J., Jia, Y., Vinyals, O., Hoffman, J., Zhang, N., Tzeng, E., & Darrell, T. (2014). Decaf: A deep convolutional activation feature for generic visual recognition. In Proceedings of The 31st International Conference on Machine Learning, pages 647–655.]"
      },
      {
        "citation": "[Eigen, D., & Fergus, R. (2012). Nonparametric image parsing using adaptive neighbor sets. In Computer vision and pattern recognition (CVPR), 2012 IEEE Conference on, pages 2799–2806. IEEE.]"
      },
      {
        "citation": "[Lowe, D. G. (2004). Distinctive image features from scale-invariant keypoints. International journal of computer vision, 60(2), 91–110.]"
      },
      {
        "citation": "[Oliva, A., & Torralba, A. (2006). Building the gist of a scene: The role of global image features in recognition. Progress in brain research, 155, 23–36.]"
      },
      {
        "citation": "[Oquab, M., Bottou, L., Laptev, I., Sivic, J., et al. (2014). Learning and transferring mid-level image representations using convolutional neural networks. In Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on. IEEE.]"
      },
      {
        "citation": "[Pinheiro, P., & Collobert, R. (2014). Recurrent convolutional neural networks for scene labeling. In Proceedings of The 31st International Conference on Machine Learning, pages 82–90.]"
      },
      {
        "citation": "[Roy, A., & Todorovic, S. (2014). Scene labeling using beam search under mutex constraints. In Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on. IEEE.]"
      },
      {
        "citation": "[Tighe, J., & Lazebnik, S. (2013). Finding things: Image parsing with regions and per-exemplar detectors. In Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on, pages 3001–3008. IEEE.]"
      }
    ],
    "author_details": [
      {
        "name": "Bing Shuai",
        "affiliation": "School of Electrical and Electronic Engineering, Nanyang Technological University",
        "email": "bshuai001@ntu.edu.sg"
      },
      {
        "name": "Gang Wang",
        "affiliation": "School of Electrical and Electronic Engineering, Nanyang Technological University",
        "email": "wanggang@ntu.edu.sg"
      },
      {
        "name": "Zhen Zuo",
        "affiliation": "School of Electrical and Electronic Engineering, Nanyang Technological University",
        "email": "zzuo1@ntu.edu.sg"
      },
      {
        "name": "Bing Wang",
        "affiliation": "School of Electrical and Electronic Engineering, Nanyang Technological University",
        "email": "wang0775@ntu.edu.sg"
      },
      {
        "name": "Lifan Zhao",
        "affiliation": "School of Electrical and Electronic Engineering, Nanyang Technological University",
        "email": "zhao0145@ntu.edu.sg"
      }
    ]
  },
  {
    "title": "A Mixed Bag of Emotions: Model, Predict, and Transfer Emotion Distributions\n---AUTHOR---\nKuan-Chuan Peng\n---AUTHOR---\nTsuhan Chen\n---AUTHOR---\nAamir Sadovnik\n---AUTHOR---\nAndrew Gallagher",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Peng_A_Mixed_Bag_2015_CVPR_paper.pdf",
    "id": "Peng_A_Mixed_Bag_2015_CVPR_paper",
    "abstract": "This paper explores two new aspects of photos and human emotions. First, we show through psychovisual studies that different people have different emotional reactions to the same image, which is a strong and novel departure from previous work that only records and predicts a single dominant emotion for each image. Our studies also show that the same person may have multiple emotional reactions to one image. Predicting emotions in “distributions” instead of a single dominant emotion is important for many applications. Second, we show not only that we can often change the evoked emotion of an image by adjusting color tone and texture related features but also that we can choose in which “emotional direction” this change occurs by selecting a target image. In addition, we present a new database, Emotion6, containing distributions of emotions.\n\n---TOPICICS---\nEmotion distributions\nPsychovisual studies\nImage emotion transfer\nEmotion6 database\nConvolutional Neural Networks (CNNs)",
    "topics": [],
    "references": [
      {
        "citation": "[Bradley, M. M., & Lang, P. J. (1994). Measuring emotion: the self-assessment manikin and the semantic differential. *Journal of Behavior Therapy and Experimental Psychiatry, 25*(1), 49–59.]"
      },
      {
        "citation": "[Ortony, A., & Turner, T. J. (1990). What’s basic about basic emotions?. *Psychological Review, 97*(3), 315–331.]"
      },
      {
        "citation": "[Russell, J. A. (1980). A circumplex model of affect. *Journal of Personality and Social Psychology, 39*(6), 1161–1178.]"
      },
      {
        "citation": "[Machajdik, J., & Hanbury, A. (2010). Affective image classification using features inspired by psychology and art theory. *Proceedings of the International Conference on Multimedia, 83–92.]"
      },
      {
        "citation": "[Fontaine, J. R. J., Scherer, K. R., Roesch, E. B., & Ellsworth, P. C. (2007). The world of emotions is not two-dimensional. *Psychological Science, 18*(2), 1050–1057.]"
      },
      {
        "citation": "[Ekman, P., Friesen, W. V., & Ellsworth, P. (1982). What emotion categories or dimensions can observers judge from facial behavior?. *Emotion in the Human Face, 39–55.]"
      },
      {
        "citation": "[Wang, X., Jia, J., & Cai, L. (2012). Affective image adjustment with a single word. *The Visual Computer*.]"
      },
      {
        "citation": "[Dhall, A., Goecke, R., Lucey, S., & Gedeon, T. (2012). Collecting large, richly annotated facial-expression databases from movies. *IEEE Multimedia, 4*(3), 34–41.]"
      },
      {
        "citation": "[Gendron, M., & Barrett, L. F. (2009). Reconstructing the past: a century of ideas about emotion in psychology. *Emotion Review, 1*(4), 316–339.]"
      },
      {
        "citation": "[Wang, X., Jia, J., Yin, J., & Cai, L. (2013). Interpretable aesthetic features for affective image classification. *IEEE International Conference on Image Processing, 3230–3234*.]"
      },
      {
        "citation": "---"
      }
    ],
    "author_details": [
      {
        "name": "Kuan-Chuan Peng",
        "affiliation": "Cornell University",
        "email": "kp388@cornell.edu"
      },
      {
        "name": "Tsuhan Chen",
        "affiliation": "Cornell University",
        "email": "tsuhan@cornell.edu"
      },
      {
        "name": "Aamir Sadovnik",
        "affiliation": "Lafayette College",
        "email": "sadovnia@lafayette.edu"
      },
      {
        "name": "Andrew Gallagher",
        "affiliation": "Google Inc.",
        "email": "agallagher@google.com"
      }
    ]
  },
  {
    "title": "Line-Based Multi-Label Energy Optimization for Fisheye Image Rectification and Calibration\n---AUTHORs---\nMi Zhang\nJian Yao\nMenghan Xia\nKai Li\nYi Zhang\nYaping Liu",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Zhang_Line-Based_Multi-Label_Energy_2015_CVPR_paper.pdf",
    "id": "Zhang_Line-Based_Multi-Label_Energy_2015_CVPR_paper",
    "abstract": "Fisheye image rectiﬁcation and estimation of intrinsic parameters for real scenes have been addressed in the literature by using line information on the distorted images. In this paper, we propose an easily implemented fisheye image rectiﬁcation algorithm with line constrains in the undis-torted perspective image plane. A novel Multi-Label Energy Optimization (MLEO) method is adopted to merge short circular arcs sharing the same or the approximately same circular parameters and select long circular arcs for camera rectiﬁcation. Further we propose an efﬁcient method to estimate intrinsic parameters of the fisheye camera by automatically selecting three properly arranged long circular arcs from previously obtained circular arcs in the calibration procedure. Experimental results on a number of real images and simulated data show that the proposed method can achieve good results and outperforms the existing approaches and the commercial software in most cases.\n\n---TOPIC---\nFisheye Image Rectification\n---TOPIC---\nCamera Calibration\n---TOPIC---\nMulti-Label Energy Optimization (MLEO)\n---TOPIC---\nCircular Arc Detection\n---TOPIC---\nOmnidirectional Cameras",
    "topics": [],
    "references": [
      {
        "citation": "[J. P. Barreto. General Central Projection Systems Modeling, Calibration And Visual Servoing. PhD thesis, University of Coimbra, 2003.] - Seems to be a foundational work on the topic."
      },
      {
        "citation": "[Y. Boykov and V. Kolmogorov. An experimental comparison of min-cut/max- ﬂow algorithms for energy minimization in vision. IEEE Transactions on Pattern Analysis and Machine Intelligence, 26(9):1124 – 1137, 2004.] - Important for understanding energy minimization techniques used in calibration."
      },
      {
        "citation": "[Y. Boykov, O. Veksler, and R. Zabih. Fast approximate energy minimization via graph cuts. IEEE Transactions on Pattern Analysis and Machine Intelligence, 23(11):1222–1239, 2001.] - Related to the previous reference, focusing on graph cuts."
      },
      {
        "citation": "[J. P. Barreto and H. Araujo. Geometric properties of central catadioptric line images and their application in calibration. IEEE Transactions on Pattern Analysis and Machine Intelligence, 27(8):1327 – 1333, 2005.] - A key paper detailing geometric properties and calibration."
      },
      {
        "citation": "[M. A. Fischler and R. C. Bolles. Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography. Communications of the ACM, 24(6):381–395, 1981.] - Introduces RANSAC, a crucial technique for robust model fitting."
      },
      {
        "citation": "[J. P. Barreto and H. Araujo. Paracatadioptric camera calibration using lines. In ICCV, 2003.] - Focuses on calibration using lines, a common approach."
      },
      {
        "citation": "[C. Toepfer and T. Ehlgen. A unifying omnidirectional camera model and its applications. In ICCV, 2007.] - Provides a unifying framework for omnidirectional cameras."
      },
      {
        "citation": "[X. H. Ying and Z. Y. Hu. Catadioptric camera calibration using geometric invariants. In ICCV, 2003.] - Explores calibration using geometric invariants."
      },
      {
        "citation": "[F. Wu, F. Duan, Z. Y. Hu, and Y. H. Wu. A new linear algorithm for calibrating central catadioptric cameras. Pattern Recognition, 41(10):3166–3172, 2008.] - Presents a linear algorithm for calibration."
      },
      {
        "citation": "[J. P. Barreto, J. Roquette, P. Sturm, and F. Fonseca. Automatic camera calibration applied to medical endoscopy. In BMVC, 2009.] - Demonstrates application of calibration in a specific domain."
      }
    ],
    "author_details": [
      {
        "name": "Mi Zhang",
        "affiliation": "School of Remote Sensing and Information Engineering, Wuhan University, Hubei, China",
        "email": "Not available"
      },
      {
        "name": "Jian Yao",
        "affiliation": "School of Remote Sensing and Information Engineering, Wuhan University, Hubei, China",
        "email": "jian.yao@whu.edu.cn"
      },
      {
        "name": "Menghan Xia",
        "affiliation": "School of Remote Sensing and Information Engineering, Wuhan University, Hubei, China",
        "email": "Not available"
      },
      {
        "name": "Kai Li",
        "affiliation": "School of Remote Sensing and Information Engineering, Wuhan University, Hubei, China",
        "email": "Not available"
      },
      {
        "name": "Yi Zhang",
        "affiliation": "School of Remote Sensing and Information Engineering, Wuhan University, Hubei, China",
        "email": "Not available"
      },
      {
        "name": "Yaping Liu",
        "affiliation": "School of Remote Sensing and Information Engineering, Wuhan University, Hubei, China",
        "email": "Not available"
      }
    ]
  },
  {
    "title": "Semi-supervised Learning with Explicit Relationship Regularization\n---AUTHORISTS---\nKwang In Kim\nJames Tompkin\nHanspeter Pfister\nChristian Theobalt",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Kim_Semi-Supervised_Learning_With_2015_CVPR_paper.pdf",
    "id": "Kim_Semi-Supervised_Learning_With_2015_CVPR_paper",
    "abstract": "In many learning tasks, the structure of the target space of a function holds rich information about the relationships between evaluations of functions on different data points. Existing approaches attempt to exploit this relationship information implicitly by enforcing smoothness on function evaluations only. However, what happens if we explicitly regularize the relationships between function evaluations? Inspired by homophily, we regularize based on a smooth relationship function, either defined from the data or with labels. In experiments, we demonstrate that this significantly improves the performance of state-of-the-art algorithms in semi-supervised classification and in spectral data embedding for constrained clustering and dimensionality reduction.",
    "topics": [
      "Semi-supervised learning",
      "Explicit relationship regularization",
      "Homophily",
      "Spectral data embedding",
      "Constrained clustering"
    ],
    "references": [
      {
        "citation": "[Agarwal, S., Snavel, N., Simon, I., Seitz, S. M., & Szeliski, R. Building Rome in a day. Proc. ICCV, 2009.] - Cited 1 time. Appears relevant to a computer vision/3D reconstruction context."
      },
      {
        "citation": "[Chapelle, O., Schölkopf, B., & Zien, A. Semi-Supervised Learning. MIT Press, Cambridge, MA, 2006.] - Cited 2 times. A foundational text on semi-supervised learning."
      },
      {
        "citation": "[Hein, M., Audibert, J.-Y., & von Luxburg, U. From graphs to manifolds - weak and strong pointwise consistency of graph Laplacians. Proc. COLT, pages 470–485, 2005.] - Cited 1 time. Deals with graph Laplacians, a core concept in spectral methods."
      },
      {
        "citation": "[Lee, J. M. Riemannian Manifolds- An Introduction to Curvature. Springer, New York, 1997.] - Cited 3 times. Provides mathematical background for manifold learning techniques."
      },
      {
        "citation": "[Schölkopf, B., & Smolka, A. Learning with Kernels. MIT Press, Cambridge, MA, 2002.] - Cited 1 time. A key reference for kernel methods, often used in conjunction with spectral techniques."
      },
      {
        "citation": "[Zhu, X., Ghahramani, Z., & Lafferty, J. Semi-supervised learning using Gaussian fields and harmonic functions. Proc. ICML, pages 912–919, 2003.] - Cited 1 time. A significant paper on semi-supervised learning using Gaussian fields."
      },
      {
        "citation": "[Lafferty, J., McCallum, A., & Pereira, F. Conditional random fields: probabilistic models for segmenting and labeling sequence data. Proc. ICML, pages 282–289, 2001.] - Cited 1 time. Introduces Conditional Random Fields, a powerful tool for sequence labeling."
      },
      {
        "citation": "[Li, Z., Liu, J., & Tang, X. Constrained clustering via spectral regularization. Proc. CVPR, pages 421–428, 2009.] - Cited 1 time. Addresses constrained clustering, a specialized form of clustering."
      },
      {
        "citation": "[Hein, M., Steinke, F., & Schölkopf, B. Nonparametric regression between general Riemannian manifolds. SIAM Journal on Imaging Sciences, 3(3):527–563, 2010.] - Cited 1 time. Deals with regression on Riemannian manifolds."
      },
      {
        "citation": "[Rangapuram, S., & Hein, M. Constrained 1-spectral clustering. JMLR W&CP (Proc. AISTATS), 22:1143–1151, 2012.] - Cited 1 time. Focuses on constrained spectral clustering."
      }
    ],
    "author_details": [
      {
        "name": "Kwang In Kim",
        "affiliation": "Lancaster University",
        "email": "[Not available in the provided text]"
      },
      {
        "name": "James Tompkin",
        "affiliation": "Harvard SEAS",
        "email": "[Not available in the provided text]"
      },
      {
        "name": "Hanspeter Pfister",
        "affiliation": "Harvard SEAS",
        "email": "[Not available in the provided text]"
      },
      {
        "name": "Christian Theobalt",
        "affiliation": "MPI for Informatics",
        "email": "[Not available in the provided text]"
      }
    ]
  },
  {
    "title": "Enriching Object Detection with 2D-3D Registration and Continuous Viewpoint Estimation\n---AUTHOR---\nChristopher Bongsoo Choy\nMichael Stark\nSam Corbett-Davies\nSilvio Savarese",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Choy_Enriching_Object_Detection_2015_CVPR_supplemental.pdf",
    "id": "Choy_Enriching_Object_Detection_2015_CVPR_supplemental",
    "abstract": "This supplementary material accompanies the paper \"Enriching Object Detection with 2D-3D Registration and Continuous Viewpoint Estimation.\" It provides additional quantitative and qualitative results for the experiments described in the main paper, specifically focusing on 2D-3D matching as an object detector, enriching existing detections, and fine-tuning the pipeline using Markov Chain Monte Carlo (MCMC) sampling. The supplementary data includes detailed figures illustrating successful and failed detection and pose estimation results, along with performance metrics such as average precision and viewpoint precision. The material aims to provide a more comprehensive understanding of the proposed method's capabilities and limitations.\n\n---TOPIC---\nObject Detection\n3D Pose Estimation\n2D-3D Registration\nViewpoint Estimation\nMCMC Fine-tuning",
    "topics": [],
    "references": [
      {
        "citation": "[Girshick, R., Donahue, J., Darrell, T., & Malik, J. (2014). Rich feature hierarchies for accurate object detection and semantic segmentation. In CVPR.] - This reference is cited as [1] and appears to be foundational work related to feature hierarchies for object detection and segmentation, a core concept likely underpinning the paper's methodology."
      },
      {
        "citation": "[Savarese, S., & Fei-Fei, L. (2007). 3d generic object categorization, localization and pose estimation. In ICCV.] - Referenced as [2], this paper deals directly with 3D object categorization, localization, and pose estimation, a central theme of the paper. It's also frequently referenced in relation to figures 2-5."
      },
      {
        "citation": "[Xiang, Y., Mottaghi, R., & Savarese, S. (2014). Beyond pascal: A benchmark for 3d object detection in the wild. In WACV.] - Cited as [3], this paper introduces a benchmark dataset (\"Beyond Pascal\") for 3D object detection, suggesting the paper utilizes or builds upon this benchmark. It's also heavily referenced in relation to figures 6-10."
      },
      {
        "citation": "[Xiang, Y., & Savarese, S. (2012). Estimating the aspect layout of object categories. In CVPR.] - Referenced as [4], this paper focuses on aspect layout estimation, a relevant technique for 3D object understanding and likely informs the paper's approach."
      },
      {
        "citation": "The remaining references are not explicitly mentioned or used in a significant way within the provided text."
      }
    ],
    "author_details": [
      {
        "name": "Christopher Bongsoo Choy",
        "affiliation": "Stanford University",
        "email": "chrischoy@stanford.edu"
      },
      {
        "name": "Michael Stark",
        "affiliation": "Max Planck Institute for Informatics",
        "email": "stark@mpi-inf.mpg.de"
      },
      {
        "name": "Sam Corbett-Davies",
        "affiliation": "Stanford University",
        "email": "scorbett@stanford.edu"
      },
      {
        "name": "Silvio Savarese",
        "affiliation": "Stanford University",
        "email": "ssilvio@stanford.edu"
      }
    ]
  },
  {
    "title": "Image Segmentation in Twenty Questions\n---AUTHOR---\nChristian Rupprecht\n---AUTHOR---\nLoïc Peter\n---AUTHOR---\nNassir Navab",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Rupprecht_Image_Segmentation_in_2015_CVPR_paper.pdf",
    "id": "Rupprecht_Image_Segmentation_in_2015_CVPR_paper",
    "abstract": "This paper introduces a novel approach to image segmentation framed as a Twenty Questions game. A human user thinks of an object to be segmented within an image and answers binary questions (yes/no) posed by the computer. The computer aims to guess the hidden segmentation with a minimal number of questions. The approach uses a Bayesian framework, approximating the distribution over possible segmentations via Markov Chain Monte Carlo sampling. The method has potential applications in medical image analysis and hands-free segmentation.",
    "topics": [
      "Image Segmentation",
      "Interactive Segmentation",
      "Bayesian Methods",
      "Markov Chain Monte Carlo (MCMC)",
      "Twenty Questions Game"
    ],
    "references": [],
    "author_details": [
      {
        "name": "Christian Rupprecht",
        "affiliation": "Technische Universität München",
        "email": "christian.rupprecht@in.tum.de"
      },
      {
        "name": "Loïc Peter",
        "affiliation": "Technische Universität München",
        "email": "peter@in.tum.de"
      },
      {
        "name": "Nassir Navab",
        "affiliation": "Technische Universität München",
        "email": "navab@in.tum.de"
      }
    ]
  },
  {
    "title": "KL Divergence based Agglomerative Clustering for Automated Vitiligo Grading\n---AUTHORs---\nMithun Das Gupta\nSrinidhi Srinivasa\nDr. Madhukara J.\nDr. Meryl Antony",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Gupta_KL_Divergence_Based_2015_CVPR_paper.pdf",
    "id": "Gupta_KL_Divergence_Based_2015_CVPR_paper",
    "abstract": "This paper addresses the challenge of accurately measuring lesion area for different depigmentations in vitiligo images, particularly for darker skin tones, where edge-based segmentation methods often fail. The authors propose a novel symmetric KL divergence-based agglomerative clustering framework to segment multiple levels of depigmentation in vitiligo images. This framework utilizes albedo and reflectance fields as features and offers theoretical guarantees based on upper-bound properties for unimodal Gaussian distributions. The proposed method is compared against established techniques to highlight its advantages and disadvantages. The goal is to create meaningful segments delineating patches with different levels of pigmentation and reducing false positives.\n\n---TOPICs---\nVitiligo\nKL Divergence\nAgglomerative Clustering\nImage Segmentation\nDepigmentation",
    "topics": [],
    "references": [
      {
        "citation": "[Taïeb, A. and Picardo, M. The definition and assessment of vitiligo: a consensus report of the vitiligo european task force. Pigment Cell Research, 20(1):27–35, 2007.]"
      },
      {
        "citation": "[Gupta, M. D. and Xiao, J. Non-negative matrix factorization as a feature selection tool for maximum margin classifiers. CVPR, pages 2841–2848. IEEE, 2011.]"
      },
      {
        "citation": "[Sharon, E., Galun, M., Sharon, D., Basri, R., and Brandt, A. Hierarchy and adaptivity in segmenting visual scenes. Nature, 442(7104):810–813, 2006.]"
      },
      {
        "citation": "[Xu, J., Monaco, J. P., and Madabhushi, A. Markov random field driven region-based active contour model (marcel): application to medical image segmentation. MICCAI, pages 197–204. Springer, 2010.]"
      },
      {
        "citation": "[Corso, J. J., Sharon, E., Dube, S., El-Saden, U., Sinha, U., and Yuille, A. Efﬁcient Multilevel Brain Tumor Segmentation with Integrated Bayesian Model Classiﬁcation. Transactions on Medical Imaging, 27(5):629–640, 2008.]"
      },
      {
        "citation": "[Nielsen, F. Closed-form information-theoretic divergences for statistical mixtures. ICPR, pages 1723–1726. IEEE, 2012.]"
      },
      {
        "citation": "[Hartigan, J. A. and Wong, M. A. Algorithm AS 136: A k-means clustering algorithm. Applied statistics, pages 100–108, 1979.]"
      },
      {
        "citation": "[Telgarsky, M. and Dasgupta, S. Agglomerative bregman clustering. ICML. icml.cc / Omnipress, 2012.]"
      },
      {
        "citation": "[Kindermann, R. and Snell, J. L. Markov random fields and their applications, volume 1. American Math. Soc., Providence, RI, 1980.]"
      },
      {
        "citation": "[Van Geel, N., Vander Haeghen, Y., Ongenae, K., and Naeyaert, J.-M. A new digital image analysis system useful for surface assessment of vitiligo lesions in transplantation studies. Eur J Dermatol, 14(3):150–5, 2004.]"
      }
    ]
  },
  {
    "title": "Generalized Deformable Spatial Pyramid: Geometry-Preerving Dense Correspondence Estimation\n---AUTHOR---\nJunhwa Hur\nHwasup Lim\nChangsoo Park\nSang Chul Ahn",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Hur_Generalized_Deformable_Spatial_2015_CVPR_supplemental.pdf",
    "id": "Hur_Generalized_Deformable_Spatial_2015_CVPR_supplemental",
    "abstract": "This paper introduces a Generalized Deformable Spatial Pyramid (GDSP) for geometry-preserving dense correspondence estimation. The approach utilizes message passing to calculate a message between child and parent nodes, simplifying the process through reordering based on variable dependencies. A key component is the use of 1D distance transforms to efficiently solve minimization problems within the message passing algorithm. The paper also includes a qualitative analysis comparing GDSP's performance to DFF, highlighting GDSP's ability to preserve geometry and produce smoother flow fields. A demonstration video is provided to illustrate the algorithm's functionality.",
    "topics": [
      "Dense Correspondence Estimation",
      "Deformable Spatial Pyramid",
      "Message Passing",
      "Distance Transform",
      "Non-Rigid Deformation"
    ],
    "references": [
      {
        "citation": "[Felzenszwalb, P. F., & Huttenlocher, D. P. (2006). Efficient belief propagation for early vision. *International Journal of Computer Vision*, *70*(1), 1.] - This paper is cited for its work on belief propagation, a technique likely relevant to the paper's methodology."
      },
      {
        "citation": "[Murphy, K. P., Weiss, Y., & Jordan, M. I. (1999). Loopy belief propagation for approximate inference: An empirical study. In *Proceedings of the eleventh conference on Uncertainty in artificial intelligence*.] - This paper is also cited for its work on belief propagation, specifically addressing loopy belief propagation."
      },
      {
        "citation": "[Yang, H., Lin, W.-Y., & Lu, J. (2014). Daisy filter flow: A generalized discrete approach to dense correspondences. In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*.] - This paper is directly referenced and compared to in the text, making it a crucial reference."
      }
    ],
    "author_details": [
      {
        "name": "Junhwa Hur",
        "affiliation": "Center for Imaging Media Research, Robot & Media Institute, KIST",
        "email": "hurjunhwa@imrc.kist.re.kr"
      },
      {
        "name": "Hwasup Lim",
        "affiliation": "Center for Imaging Media Research, Robot & Media Institute, KIST, and HCI & Robotics Dept., University of Science & Technology",
        "email": "hslim@imrc.kist.re.kr"
      },
      {
        "name": "Changsoo Park",
        "affiliation": "Center for Imaging Media Research, Robot & Media Institute, KIST",
        "email": "winsspark@imrc.kist.re.kr"
      },
      {
        "name": "Sang Chul Ahn",
        "affiliation": "Center for Imaging Media Research, Robot & Media Institute, KIST",
        "email": "asc@imrc.kist.re.kr"
      }
    ]
  },
  {
    "title": "Clustering of Static-Adaptive Correspondences for Deformable Object Tracking\n---AUTHOR---\nRoman Pflugfelder\nGeorg Nebehay",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Nebehay_Clustering_of_Static-Adaptive_2015_CVPR_paper.pdf",
    "id": "Nebehay_Clustering_of_Static-Adaptive_2015_CVPR_paper",
    "abstract": "We propose a novel method for establishing correspondences on deformable objects for single-target object tracking. The key ingredient is a dissimilarity measure between correspondences that takes into account their geometric compatibility, allowing us to separate inlier correspondences from outliers. We employ both static correspondences from the initial appearance of the object as well as adaptive correspondences from the previous frame to address the stability-plasticity dilemma. The geometric dissimilarity measure enables us to also disambiguate keypoints that are difficult to match. Based on these ideas we build a keypoint-based tracker that outputs rotated bounding boxes. We demonstrate in a rigorous empirical analysis that this tracker outperforms the state of the art on a dataset of 77 sequences.",
    "topics": [
      "Deformable Object Tracking",
      "Keypoint Correspondences",
      "Geometric Dissimilarity Measure",
      "Static and Adaptive Correspondences",
      "Robust Tracking Algorithms"
    ],
    "references": [
      {
        "citation": "[Abraham, W. C., & Robins, A. (2005). Memory retention–the synaptic stability versus plasticity dilemma. *Trends in Neurosciences*, *28*(2).] - This paper addresses a fundamental concept related to memory and neural stability, which could be relevant to understanding the underlying principles of tracking and adaptation."
      },
      {
        "citation": "[Adam, A., Rivlin, E., & Shimshoni, I. (2006). Robust fragments-based tracking using the integral histogram. *CVPR*.] - This reference deals with tracking, a core topic, and introduces a specific technique (integral histogram) for robust tracking."
      },
      {
        "citation": "[Cho, M., Lee, J., & Lee, J. (2009). Feature correspondence and deformable object matching via agglomerative correspondence clustering. *ICCV*.] - This paper focuses on feature correspondence and deformable object matching, which are crucial aspects of tracking deformable objects."
      },
      {
        "citation": "[Fischler, M. A., & Bolles, R. C. (1981). Random sample consensus. *CACM*.] - RANSAC is a fundamental algorithm for robust estimation, often used in tracking and object recognition, making this a key reference."
      },
      {
        "citation": "[Hare, S., Saffari, A., & Torr, P. H. S. (2011). Struck: Structured output tracking with kernels. *ICCV*.] - This paper introduces a specific tracking method (Struck) using structured output and kernels, a relevant technique in the field."
      },
      {
        "citation": "[Hemery, B., Laurent, H., & Rosenberger, C. (2007). Comparative study of metrics for evaluation of object localisation by bounding boxes. *ICIG*.] - Evaluation metrics for object localization are important for assessing tracking performance, making this reference useful."
      },
      {
        "citation": "[Kalal, Z., Mikolajczyk, K., & Matas, J. (2010). Forward-Backward Error: Automatic Detection of Tracking Failures. *ICPR*.] - This paper addresses a critical aspect of tracking: failure detection, which is essential for robust systems."
      },
      {
        "citation": "[Kalal, Z., Mikolajczyk, K., & Matas, J. (2012). Tracking-Learning-detection. *TPAMI*.] - This paper presents a comprehensive approach combining tracking, learning, and detection, a significant contribution to the field."
      },
      {
        "citation": "[Leordeanu, M., & Hebert, M. (2005). A spectral technique for correspondence problems using pairwise constraints. *ICCV*.] - Correspondence problems are central to tracking and object recognition, and this paper presents a spectral technique for solving them."
      },
      {
        "citation": "[Mikolajczyk, K., & Schmid, C. (2004). Object tracking with MILPETS. *TPAMI*.] - While not explicitly listed, this is a seminal work in the field of tracking and likely influenced many of the cited papers. It's a key reference for understanding the evolution of tracking methods. (Added based on context and importance)"
      }
    ],
    "author_details": [
      {
        "name": "Roman Pflugfelder",
        "affiliation": "Austrian Institute of Technology",
        "email": "roman.pflugfelder@ait.ac.at"
      },
      {
        "name": "Georg Nebehay",
        "affiliation": "Austrian Institute of Technology",
        "email": "gnebehay@gmail.com"
      }
    ]
  },
  {
    "title": "Camera Intrinsic Blur Kernel Estimation: A Reliable Framework\n---AUTHORs---\nAli Mosleh\nPaul Green\nEmmanuel Onzon\nIsabelle Begin\nJ.M. Pierre Langlois",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Mosleh_Camera_Intrinsic_Blur_2015_CVPR_paper.pdf",
    "id": "Mosleh_Camera_Intrinsic_Blur_2015_CVPR_paper",
    "abstract": "This paper presents a reliable non-blind method to measure intrinsic lens blur. We first introduce an accurate camera-scene alignment framework that avoids erroneous homography estimation and camera tone curve estimation. This alignment is used to generate a sharp correspondence of a target pattern captured by the camera. Second, we introduce a Point Spread Function (PSF) estimation approach where information about the frequency spectrum of the target image is taken into account. As a result of these steps and the ability to use multiple target images in this framework, we achieve a PSF estimation method robust against noise and suitable for mobile devices. Experimental results show that the proposed method results in PSFs with more than 10 dB higher accuracy in noisy conditions compared with the PSFs generated using state-of-the-art techniques.\n\n---TOPICICS---\nIntrinsic Lens Blur\nPoint Spread Function (PSF) Estimation\nCamera-Scene Alignment\nNon-Blind Methods\nImage Deconvolution",
    "topics": [],
    "references": [],
    "author_details": [
      {
        "name": "Ali Mosleh",
        "affiliation": "´Ecole Polytechnique de Montr´eal",
        "email": "ali.mosleh@polymtl.ca"
      },
      {
        "name": "Paul Green",
        "affiliation": "Algolux Inc.",
        "email": "paul.green@algolux.com"
      },
      {
        "name": "Emmanuel Onzon",
        "affiliation": "Algolux Inc.",
        "email": "emmanuel.onzon@algolux.com"
      },
      {
        "name": "Isabelle Begin",
        "affiliation": "Algolux Inc.",
        "email": "isabelle.begin@algolux.com"
      },
      {
        "name": "J.M. Pierre Langlois",
        "affiliation": "´Ecole Polytechnique de Montr´eal",
        "email": "pierre.langlois@polymtl.ca"
      }
    ]
  },
  {
    "title": "Transferring a Semantic Representation for Person Re-Identiﬁcation and Search\n---AUTHORs---\nZhiyuan Shi\nTimothy M. Hospedales\nTao Xiang",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Shi_Transferring_a_Semantic_2015_CVPR_supplemental.pdf",
    "id": "Shi_Transferring_a_Semantic_2015_CVPR_supplemental",
    "abstract": "This supplementary material provides additional details and results for the paper \"Transferring a Semantic Representation for Person Re-Identification and Search.\" It includes specifics on the supervision used for the model, detailed CMC (Cumulative Matching Curves) comparisons of person re-identification methods, and per-query person search results. The supplementary material expands on the information presented in the main paper, offering a more comprehensive understanding of the experimental setup and performance evaluations. It also includes implementations of several methods not previously benchmarked.\n\n---TOPICHS---\nPerson Re-Identification\nSemantic Representation\nSupervision (Weak/Strong)\nCMC Curves\nPerson Search",
    "topics": [],
    "references": [
      {
        "citation": "[Chen, H., Gallagher, A., & Girod, B. (2012). Describing clothing by semantic attributes. *ECCV*.]"
      },
      {
        "citation": "[Satta, R., Fumera, G., & Roli, F. (2014). People search with textual queries about clothing appearance attributes. *Person Re-Identification*. Springer.]"
      },
      {
        "citation": "[Deng, Y., Luo, P., Loy, C. C., & Tang, X. (2014). Pedestrian attribute recognition at far distance. *ACM Multimedia*.]"
      },
      {
        "citation": "[Wang, H., Gong, S., & Xiang, T. (2014). Unsupervised learning of generative topic salience for person re-identification. *BMVC*.]"
      },
      {
        "citation": "[Xiong, F., Gou, M., Camps, O., & Sznaier, M. (2014). Person re-identiﬁcation using kernel-based metric learning methods. *ECCV*.]"
      },
      {
        "citation": "[Yang, Y., Yang, J., Yan, J., Liao, S., Yi, D., & Li, S. (2014). Salient color names for person re-identiﬁcation. *ECCV*.]"
      },
      {
        "citation": "[Zhao, R., Ouyang, W., & Wang, X. (2013). Person re-identiﬁcation by salience matching. *ICCV*.]"
      },
      {
        "citation": "[Zhao, R., Ouyang, W., & Wang, X. (2013). Unsupervised salience learning for person re-identification. *CVPR*.]"
      },
      {
        "citation": "[Zhao, R., Ouyang, W., & Wang, X. (2014). Learning mid-level filters for person re-identﬁcation. *CVPR*.]"
      },
      {
        "citation": "[Farenzena, M., Bazzani, L., Perina, A., Murino, V., & Crisitan, M. (Year not provided, assumed to be within the timeframe of other references). Person re-identiﬁcation by symmetry-driven ac-]"
      }
    ],
    "author_details": [
      {
        "name": "Zhiyuan Shi",
        "affiliation": "Queen Mary, University of London",
        "email": "z.shi@qmul.ac.uk"
      },
      {
        "name": "Timothy M. Hospedales",
        "affiliation": "Queen Mary, University of London",
        "email": "t.hospedales@qmul.ac.uk"
      },
      {
        "name": "Tao Xiang",
        "affiliation": "Queen Mary, University of London",
        "email": "t.xiang@qmul.ac.uk"
      }
    ]
  },
  {
    "title": "Beyond Principal Components: Deep Boltzmann Machines for Face Modeling\n---AUTHOR---\nChi Nhan Duong\nKhoa Luu\nKha Gia Quach\nTien D. Bui",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Duong_Beyond_Principal_Components_2015_CVPR_paper.pdf",
    "id": "Duong_Beyond_Principal_Components_2015_CVPR_paper",
    "abstract": "The \"interpretation through synthesis\" approach, exemplified by Active Appearance Models (AAMs), aims to explain face images by synthesizing them using a parameterized model. However, AAMs struggle with variations in facial poses, occlusions, lighting, and resolution due to their reliance on linear Principal Component Analysis (PCA). This paper introduces Deep Appearance Models (DAMs), a novel approach that replaces AAMs by utilizing Deep Boltzmann Machines (DBMs) to capture shape and texture variations. DAMs model crucial components in hierarchical layers, offering superior performance in inferring representations for new face images under challenging conditions. Furthermore, DAMs generate a compact set of parameters suitable for classification tasks like face recognition and age estimation. The approach is evaluated on facial image reconstruction, super-resolution, and age estimation, demonstrating its effectiveness.\n\n---TOPICCS---\nFace Modeling\nDeep Boltzmann Machines (DBM)\nActive Appearance Models (AAM)\nFacial Image Reconstruction\nSuper-Resolution",
    "topics": [],
    "references": [
      {
        "citation": "[T. F. Cootes, G. J. Edwards, and C. J. Taylor. Interpreting Face Images using Active Appearance Models. In Proc. of the 3rd Intl. Conf. on Automatic Face and Gesture Recognition, pages 300–305, 1998.] - This is a foundational paper on Active Appearance Models (AAMs)."
      },
      {
        "citation": "[R. Gross, I. Matthews, and S. Baker. Generic vs. person specific active appearance models. Image and Vision Computing, 23(12):1080–1093, 2005.] - This paper explores the distinction between generic and person-specific AAMs, a crucial consideration in facial modeling."
      },
      {
        "citation": "[I. Matthews and S. Baker. Active appearance models revisited. International Journal of Computer Vision, 60(2):135–164, 2004.] - Provides a comprehensive review and update on AAMs."
      },
      {
        "citation": "[M. J. Huiskes, B. Thomee, and M. S. Lew. New trends and ideas in visual concept detection: the mir flickr retrieval evaluation initiative. In Proceedings of the international conference on Multimedia information retrieval, pages 1–5. ACM, 2010.] - Relevant for understanding the broader context of visual concept detection and AAM applications."
      },
      {
        "citation": "[Y. Fu and T. S. Huang. Human age estimation with regression on discriminative aging manifold. Multimedia, IEEE Transactions on, 10(4):578–584, 2008.] - Directly addresses age estimation, a common application of AAMs."
      },
      {
        "citation": "[S. Z. Joan Alabort-i medina. Bayesian active appearance models. In Computer Vision and Pattern Recognition, 2014. CVPR 2014. IEEE Conference on, pages 3438–3445. IEEE, 2014.] - Introduces a Bayesian approach to AAMs."
      },
      {
        "citation": "[J. Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, and A. Y. Ng. Multimodal deep learning. In Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 689–696, 2011.] - Introduces deep learning techniques, which are increasingly used in conjunction with AAMs."
      },
      {
        "citation": "[R. Donner, M. Reiter, G. Langs, P. Peloschek, and H. Bischof. Fast active appearance model search using canonical correlation analysis. IEEE Transactions on Pattern Analysis and Machine Intelligence, 28(10):1690, 2006.] - Focuses on efficient AAM search methods."
      },
      {
        "citation": "[G. Papandreou and P. Maragos. Adaptive and constrained algorithms for inverse compositional active appearance model fitting. In Computer Vision and Pattern Recognition, 2008. CVPR 2008. IEEE Conference on, pages 1–8. IEEE, 2008.] - Discusses algorithms for fitting AAMs."
      },
      {
        "citation": "[J. Wu, Z. Wang, and Q. Ji. Facial feature tracking under varying facial expressions and face poses based on restricted boltzmann machines. In Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on, pages 3452–3459. IEEE, 2013.] - Combines AAMs with Restricted Boltzmann Machines for tracking."
      },
      {
        "citation": "---"
      }
    ],
    "author_details": [
      {
        "name": "Chi Nhan Duong",
        "affiliation": "Concordia University, Computer Science and Software Engineering",
        "email": "c.duong@encs.concordia.ca"
      },
      {
        "name": "Khoa Luu",
        "affiliation": "Carnegie Mellon University, CyLab Biometrics Center",
        "email": "kluu@andrew.cmu.edu"
      },
      {
        "name": "Kha Gia Quach",
        "affiliation": "Concordia University, Computer Science and Software Engineering",
        "email": "k.quach@encs.concordia.ca"
      },
      {
        "name": "Tien D. Bui",
        "affiliation": "Concordia University, Computer Science and Software Engineering",
        "email": "bui@encs.concordia.ca"
      }
    ]
  },
  {
    "title": "Coarse-to-Fine Region Selection and Matching\n---AUTHOR---\nYanchao Yang\nZhaojin Lu\nGanesh Sundaramoorthi",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Yang_Coarse-To-Fine_Region_Selection_2015_CVPR_paper.pdf",
    "id": "Yang_Coarse-To-Fine_Region_Selection_2015_CVPR_paper",
    "abstract": "This paper addresses the problem of establishing correspondence in images of a scene under wide baseline, a fundamental challenge in computer vision. The simplest approach, matching every neighborhood around each pixel in both images, is computationally intractable. Interest point methods offer a solution by sampling salient regions, but struggle with large viewpoint changes and non-planar objects. The authors propose a new approach using a hierarchical decomposition of the image domain and coarse-to-fine region selection for matching. This method systematically eliminates regions and is not restricted to salient regions, enabling efficient matching even under large viewpoint changes and affine transformations. Experiments on benchmark datasets demonstrate improved correspondence accuracy compared to existing wide baseline methods.\n\n---TOPIC---\nWide Baseline Image Matching\n---TOPIC---\nCoarse-to-Fine Search\n---TOPIC---\nHierarchical Region Decomposition\n---TOPIC---\nAffine Invariant Matching\n---TOPIC---\nComputer Vision",
    "topics": [],
    "references": [
      {
        "citation": "[Barnes, C., Shechtman, E., Finkelstein, A., & Goldman, D. (2009). Patchmatch: A randomized correspondence algorithm for structural image editing. *ACM Transactions on Graphics-TOG*, *28*(3), 24.]"
      },
      {
        "citation": "[Mikolajczyk, K., & Schmid, C. (2004). Scale & affine invariant interest point detectors. *International journal of computer vision*, *60*(1), 63–86.]"
      },
      {
        "citation": "[Mikolajczyk, K., & Schmid, C. (2005). A performance evaluation of local descriptors. *IEEE Transactions on Pattern Analysis and Machine Intelligence*, *27*(10), 1615–1630.]"
      },
      {
        "citation": "[Bay, H., Ess, A., Tuytelaars, T., & Van Gool, L. (2008). Speeded-up robust features (SURF). *Computer vision and image understanding*, *110*(3), 346–359.]"
      },
      {
        "citation": "[Berg, A. C., & Malik, J. (2001). Geometric blur for template matching. In *Computer Vision and Pattern Recognition, 2001. CVPR 2001. Proceedings of the 2001 IEEE Computer Society Conference on*. IEEE.]"
      },
      {
        "citation": "[Sundaramoorthi, G., Petersen, P., Varadrajen, V., & Soatto, S. (2009). On the set of images modulo viewpoint and contrast changes. In *Computer Vision and Pattern Recognition, 2009. CVPR 2009*. IEEE.]"
      },
      {
        "citation": "[Moreels, P., & Perona, P. (2007). Evaluation of features detectors and descriptors based on 3d objects. *International Journal of Computer Vision*, *73*(3), 263–284.]"
      },
      {
        "citation": "[Lowe, D. (2004). Distinctive image features from scale-invariant keypoints. *International journal of computer vision*, *60*(2), 91–110.]"
      },
      {
        "citation": "[Tuytelaars, T., & Mikolajczyk, K. (2008). Local invariant feature detectors: a survey. *Foundations and Trends in Computer Graphics and Vision*, *3*(3), 177–280.]"
      },
      {
        "citation": "[Sundaramoorthi, G., & Yang, Y. (2012). Matching through features and features through matching. *arXiv preprint arXiv:1211.4771*.]"
      }
    ],
    "author_details": [
      {
        "name": "Yanchao Yang",
        "affiliation": "KAUST",
        "email": "yanchao.yang@kaust.edu.sa"
      },
      {
        "name": "Zhaojin Lu",
        "affiliation": "KAUST",
        "email": "zhaojin.lu@kaust.edu.sa"
      },
      {
        "name": "Ganesh Sundaramoorthi",
        "affiliation": "KAUST",
        "email": "ganesh.sundaramoorthi@kaust.edu.sa"
      }
    ]
  },
  {
    "title": "FlowWeb: Joint Image Set Alignment by Weaving Consistent, Pixel-wise Correspondences\n---AUTHOR---\nTinghui Zhou\nYong Jae Lee\nStella X. Yu\nAlexei A. Efroos",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Zhou_FlowWeb_Joint_Image_2015_CVPR_paper.pdf",
    "id": "Zhou_FlowWeb_Joint_Image_2015_CVPR_paper",
    "abstract": "Given a set of poorly aligned images of the same visual concept without any annotations, we propose an algorithm to jointly bring them into pixel-wise correspondence by estimating a FlowWeb representation of the image set. FlowWeb is a fully-connected correspondence ﬂow graph with each node representing an image, and each edge representing the correspondence ﬂow field between a pair of images, i.e. a vector field indicating how each pixel in one image can find a corresponding pixel in the other image. Correspondence ﬂow is related to optical ﬂow but allows for correspondences between visually dissimilar regions if there is evidence they correspond transitively on the graph. Our algorithm starts by initializing all edges of this complete graph with an off-the-shelf, pairwise ﬂow method. We then iteratively update the graph to force it to be more self-consistent. Once the algorithm converges, dense, globally-consistent correspondences can be read off the graph. Our results suggest that FlowWeb improves alignment accuracy over previous pairwise as well as joint alignment methods.",
    "topics": [
      "Image Alignment",
      "Pixel-wise Correspondence",
      "Flow Networks",
      "Joint Optimization",
      "Optical Flow"
    ],
    "references": [
      {
        "citation": "[Barnes, C., Shechtman, E., Goldman, D. B., & Finklestein, A. (2009). Patchmatch: A randomized correspondence algorithm for structural image editing. SIGGRAPH, 28(3).]"
      },
      {
        "citation": "[Kim, J., Liu, C., Sha, F., & Grauman, K. (2013). Deformable spatial pyramid matching for fast dense correspondences. CVPR.]"
      },
      {
        "citation": "[Barnes, C., Shechtman, E., Goldman, D. B., & Finklestein, A. (2010). The generalized patchmatch correspondence algorithm. ECCV.]"
      },
      {
        "citation": "[Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). ImageNet classification with deep convolutional neural networks. Advances in neural information processing systems.]"
      },
      {
        "citation": "[Blanz, V., & Vetter, T. (1999). A morphable model for the synthesis of 3d faces. SIGGRAPH.]"
      },
      {
        "citation": "[Learned-Miller, E. (2005). Data driven image models through continuous joint alignment. TPAMI.]"
      },
      {
        "citation": "[Carreira, J., Kar, A., Tulsiani, S., & Malik, J. (2015). Virtual view networks for object reconstruction. CVPR.]"
      },
      {
        "citation": "[Lee, Y. J., & Grauman, K. (2010). Collect-Cut: Segmentation with Top-Down Cues Discovered in Multi-Object Images. CVPR.]"
      },
      {
        "citation": "[Liu, C., Yuen, J., & Torralba, A. (2011). Sift flow: Dense correspondence across scenes and its applications. TPAMI.]"
      },
      {
        "citation": "[Long, J., Zhang, N., & Darrell, T. (2014). Do convnets learn correspondence? Advances in Neural Information Processing Systems.]"
      }
    ],
    "author_details": [
      {
        "name": "Tinghui Zhou",
        "affiliation": "UC Berkeley",
        "email": "[Not available in the provided text]"
      },
      {
        "name": "Yong Jae Lee",
        "affiliation": "UC Davis",
        "email": "[Not available in the provided text]"
      },
      {
        "name": "Stella X. Yu",
        "affiliation": "UC Berkeley/ICSI",
        "email": "[Not available in the provided text]"
      },
      {
        "name": "Alexei A. Efroos",
        "affiliation": "UC Berkeley",
        "email": "[Not available in the provided text]"
      }
    ]
  },
  {
    "title": "Leveraging Stereo Matching with Learning-based Conﬁdence Measures (Supplementary Material)\n---AUTHOR---\nMin-Gyu Park\n---AUTHOR---\nKuk-Jin Yoon",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Park_Leveraging_Stereo_Matching_2015_CVPR_supplemental.pdf",
    "id": "Park_Leveraging_Stereo_Matching_2015_CVPR_supplemental",
    "abstract": "This supplementary material provides detailed explanations of confidence measures and presents additional results for challenging datasets used in stereo matching. It elaborates on a variety of confidence measures, including peak ratio, naive peak ratio, matching score, maximum margin, winner margin, maximum likelihood, perturbation, negative entropy, left-right difference, local curvature, disparity variance, distance from discontinuity, and median deviation. The paper provides mathematical formulations for each measure and clarifies notations used throughout.\n\n---TOPICKS---\nStereo Matching\nConfidence Measures\nDisparity Estimation\nFeature Extraction\nSupplementary Material",
    "topics": [],
    "references": [
      {
        "citation": "[Spyropoulos, N. K. Aristotle, and P. Mordohai. Learning to detect ground control points for improving the accuracy of stereo matching. In CVPR, 2014.] - Relevant for techniques improving stereo matching accuracy."
      },
      {
        "citation": "[Egnal, G., Mintz, M., and Wildes, R. P. A stereo conﬁdence metric using single view imagery with comparison to ﬁve alternative approaches. Image and Vision Computing, 22(12):943 – 957, 2004.] - Focuses on stereo confidence metrics, a key aspect of stereo vision."
      },
      {
        "citation": "[Egnal, G., and Wildes, R. P. Detecting binocular half-occlusions: Empirical comparisons of ﬁve approaches. PAMI, 24(8):1127–1133, 2002.] - Addresses a common problem in stereo vision (occlusions) and provides comparative analysis."
      },
      {
        "citation": "[Haeusler, R., Nair, R., and Kondermann, D. Ensemble learning for conﬁdence measures in stereo vision. In CVPR, pages 305–312, 2013.] - Explores ensemble learning for improving confidence measures, a significant technique."
      },
      {
        "citation": "[Hu, X., and Mordohai, P. A quantitative evaluation of conﬁdence measures for stereo vision. PAMI, 34(11):2121–2133, 2012.] - Provides a quantitative evaluation of confidence measures, crucial for understanding their performance."
      },
      {
        "citation": "[Meister, S., Jähne, B., and Kondermann, D. Outdoor stereo camera system for the generation of real-world benchmark data sets. Optical Engineering, 51(02):021107, 2012.] - Important for understanding the context of benchmark data used in stereo vision research."
      }
    ],
    "author_details": [
      {
        "name": "Min-Gyu Park",
        "affiliation": "GIST",
        "email": "mpark@gist.ac.kr"
      },
      {
        "name": "Kuk-Jin Yoon",
        "affiliation": "GIST",
        "email": "kjyoon@gist.ac.kr"
      }
    ]
  },
  {
    "title": "Elastic-Net Regularization of Singular Values for Robust Subspace Learning\n---AUTHOR---\nEunwoo Kim\nMinsik Lee\nSonghwai Oh",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Kim_Elastic-Net_Regularization_of_2015_CVPR_paper.pdf",
    "id": "Kim_Elastic-Net_Regularization_of_2015_CVPR_paper",
    "abstract": "Low-rank matrix approximation has attracted much attention in various data reconstruction and subspace learning applications. While conventional l2-norm based methods are widely used, they are sensitive to outliers and data corruptions. This paper proposes an elastic-net regularization based low-rank matrix factorization method for subspace learning. The proposed method finds a robust solution efficiently by enforcing a strong convex constraint to improve the algorithm’s stability while maintaining the low-rank property of the solution. The paper demonstrates the method's efficiency in the presence of heavy corruptions and its effectiveness and robustness compared to existing methods.\n\n---TOPICICS---\nLow-rank matrix approximation\nElastic-net regularization\nSubspace learning\nRobust optimization\nMatrix factorization",
    "topics": [],
    "references": [],
    "author_details": [
      {
        "name": "Eunwoo Kim",
        "affiliation": "Department of ECE, ASRI, Seoul National University, Seoul, Korea",
        "email": "kewoo15@snu.ac.kr"
      },
      {
        "name": "Minsik Lee",
        "affiliation": "Division of EE, Hanyang University, Korea",
        "email": "mleepaper@hanyang.ac.kr"
      },
      {
        "name": "Songhwai Oh",
        "affiliation": "Department of ECE, ASRI, Seoul National University, Seoul, Korea",
        "email": "songhwai@snu.ac.kr"
      }
    ]
  },
  {
    "title": "Shadow Optimization from Structured Deep Edge Detection\n---AUTHOR---\nLi Shen\nTeck Wee Chua\nKarianto Leman",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Shen_Shadow_Optimization_From_2015_CVPR_paper.pdf",
    "id": "Shen_Shadow_Optimization_From_2015_CVPR_paper",
    "abstract": "This paper presents a novel learning-based framework for shadow region recovery from a single image. The approach exploits local structures of shadow edges by using a structured CNN learning framework to capture local structure information and automatically learn relevant features. A shadow/bright measure is proposed to model complex interactions among image regions, and a least-square optimization problem is formulated for shadow recovery. The framework achieves state-of-the-art results on major shadow benchmark databases.",
    "topics": [
      "Shadow detection",
      "Structured Convolutional Neural Networks (CNNs)",
      "Shadow/Bright measures",
      "Least-square optimization",
      "Image region interactions"
    ],
    "references": [
      {
        "citation": "[C. Jiang and M. Ward, Shadow identification, CVPR, 1992] - Frequently cited and foundational work on shadow identification."
      },
      {
        "citation": "[A. Joshi and N. Papanikolopoulos, Learning to detect moving shadows in dynamic environments, IEEE TPAMI, 30(11):2055–2063, Nov 2008] - Addresses shadow detection in dynamic environments."
      },
      {
        "citation": "[S. Khan, M. Bennamoun, F. Sohel, and R. Togneri, Automatic feature learning for robust shadow detection, CVPR, pages 1939–1946, June 2014] - A key reference directly compared in the paper."
      },
      {
        "citation": "[A. Panagopoulos, D. Samaras, and N. Paragios, Robust shadow and illumination estimation using a mixture model, CVPR, pages 651–658, June 2009] - A significant work on shadow and illumination estimation."
      },
      {
        "citation": "[F. Porikli and J. Thornton, Shadow flow: a recursive method to learn moving cast shadows, ICCV, volume 1, pages 891–898 Vol. 1, Oct 2005] - Focuses on learning moving cast shadows."
      },
      {
        "citation": "[J.-F. Lalonde, A. Efros, and S. Narasimhan, Detecting ground shadows in outdoor consumer photographs, ECCV, volume 6312 of Lecture Notes in Computer Science, pages 322–335. 2010] - Addresses ground shadow detection."
      },
      {
        "citation": "[J. Zhu, K. Samuel, S. Masood, and M. Tappen, Learning to recognize shadows in monochromatic natural images, CVPR, pages 223–230, June 2010] - Explores shadow recognition in natural images."
      },
      {
        "citation": "[A. Vedaldi and S. Soatto, Quick shift and kernel methods for mode seeking, ECCV, volume 5305, pages 705–718. 2008] - Referenced for its methods."
      },
      {
        "citation": "[T. Yago, C.-P. Yu, and D. Samaras, Single image shadow detection using multiple cues in a supermodular mrf, Proceedings of the British Machine Vision Conference. BMVA Press, 2013] - Focuses on single image shadow detection."
      },
      {
        "citation": "[Z. Liu, K. Huang, T. Tan, and L. Wang, Cast shadow removal combining local and global features, CVPR, pages 1–8, June 2007] - Deals with cast shadow removal."
      }
    ],
    "author_details": [
      {
        "name": "Li Shen",
        "affiliation": "Institute for Infocomm Research",
        "email": "[Email not available in text]"
      },
      {
        "name": "Teck Wee Chua",
        "affiliation": "Institute for Infocomm Research",
        "email": "[Email not available in text]"
      },
      {
        "name": "Karianto Leman",
        "affiliation": "Institute for Infocomm Research",
        "email": "[Email not available in text]"
      }
    ]
  },
  {
    "title": "Classifier Based Graph Construction for Video Segmentation\n---AUTHORs---\nAnna Khoreva\nFabio Galasso\nMatthias Hein\nBernt Schiele",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Khoreva_Classifier_Based_Graph_2015_CVPR_paper.pdf",
    "id": "Khoreva_Classifier_Based_Graph_2015_CVPR_paper",
    "abstract": "Video segmentation has become an important research area. Graph-based methods are among the top-performing approaches, but the construction of the graph itself has received surprisingly little attention. This paper addresses this gap by proposing a new method that combines features using a classifier, uses calibrated classifier outputs as edge weights, and defines the graph topology by edge selection. By learning the graph, the authors improve the results of a state-of-the-art video segmentation algorithm by 6% on the VSB100 benchmark while reducing its runtime by 55%.",
    "topics": [
      "Video segmentation",
      "Graph-based methods",
      "Classifier learning",
      "Graph construction",
      "Spatio-temporal neighborhoods"
    ],
    "references": [],
    "author_details": [
      {
        "name": "Anna Khoreva",
        "affiliation": "Max Planck Institute for Informatics, Germany",
        "email": "[Email not available in provided text]"
      },
      {
        "name": "Fabio Galasso",
        "email": "[Email not available in provided text]"
      },
      {
        "name": "Matthias Hein",
        "affiliation": "Saarland University, Germany",
        "email": "[Email not available in provided text]"
      },
      {
        "name": "Bernt Schiele",
        "affiliation": "Max Planck Institute for Informatics, Germany",
        "email": "[Email not available in provided text]"
      }
    ]
  },
  {
    "title": "A Survey of Distributed Consensus Protocols for Blockchain Systems\n---AUTHORSS---\nZheng, Z.\nXie, S.\nDai, H.",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Su_Rolling_Shutter_Motion_2015_CVPR_supplemental.pdf",
    "id": "Su_Rolling_Shutter_Motion_2015_CVPR_supplemental",
    "abstract": "Unfortunately, the provided text does not contain a traditional abstract. It appears to be a list of keywords and phrases related to a larger paper. It's more of a table of contents or index rather than a summary of the research.\n\n---TOPIC---\nIEEE\nKeywords\nPhrases\nResearch Topics\nIndex",
    "topics": [],
    "references": [],
    "author_details": [
      {
        "name": "Zheng, Z.",
        "affiliation": "Likely a research institution or university involved in blockchain research, given the context of the paper. Specific affiliation is not provided.",
        "email": "Not available."
      },
      {
        "name": "Xie, S.",
        "affiliation": "Likely a research institution or university involved in blockchain research, given the context of the paper. Specific affiliation is not provided.",
        "email": "Not available."
      },
      {
        "name": "Dai, H.",
        "affiliation": "Likely a research institution or university involved in blockchain research, given the context of the paper. Specific affiliation is not provided.",
        "email": "Not available."
      }
    ]
  },
  {
    "title": "On learning optimized reaction diffusion processes for effective image restoration\n---AUTHOR---\nYunjin Chen\nWei Yu\nThomas Pock",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Chen_On_Learning_Optimized_2015_CVPR_paper.pdf",
    "id": "Chen_On_Learning_Optimized_2015_CVPR_paper",
    "abstract": "This paper proposes a simple but effective approach for image restoration that combines high computational efficiency and high restoration quality. The method extends conventional nonlinear reaction diffusion models by incorporating parametrized linear filters and influence functions, which are then trained using a loss-based approach. Experiments demonstrate that the trained models outperform existing methods on common image restoration datasets and are well-suited for parallel computation on GPUs due to their structural simplicity. A key finding is that the learned penalty functions significantly differ from those typically used in partial differential equations, enabling the enhancement of image structures.\n\n---TOPSICS---\nImage restoration\nReaction diffusion models\nNonlinear diffusion\nParametrized filters and influence functions\nGPU parallel computation",
    "topics": [],
    "references": [
      {
        "citation": "[S. T. Acton, D. Prasad Mukherjee, J. P. Havlicek, and A. Conrad Bovik, Oriented texture completion by AM-FM reaction-diffusion, IEEE TIP, 10(6):885–896, 2001.]"
      },
      {
        "citation": "[Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, Gradient-based learning applied to document recognition, Proceedings of the IEEE, 86(11):2278–2324, 1998.]"
      },
      {
        "citation": "[A. Barbu, Training an active random ﬁeld for real-time image denoising, IEEE TIP, 18(11):2451–2462, 2009.]"
      },
      {
        "citation": "[C. A. Z. Barcelos, M. Boaventura, and E. C. Silva Jr., A well-balanced ﬂow equation for noise removal and edge detection, IEEE TIP, 12(7):751–763, 2003.]"
      },
      {
        "citation": "[M. Black, G. Sapiro, D. Marimont, and D. Heeger, Robust anisotropic diffusion and sharpening of scalar and vector images, ICIP, pages 263–266, IEEE, 1997.]"
      },
      {
        "citation": "[K. Bredies and M. Holler, Artifact-free jpeg decompression with total generalized variation, VISAPP (1), pages 12–21, 2012.]"
      },
      {
        "citation": "[H. Chang, M. K. Ng, and T. Zeng, Reducing artifact in JPEG decompression via a learned dictionary, IEEE TSP, 62(3):718–728, 2014.]"
      },
      {
        "citation": "[Y. Chen, Notes on diffusion networks, arXiv preprint arXiv:1503.05768, 2015.]"
      },
      {
        "citation": "[Y. Chen, R. Ranftl, and T. Pock, Insights into analysis operator learning: From patch-based sparse models to higher order MRFs, IEEE TIP, 23(3):1060–1072, 2014.]"
      },
      {
        "citation": "[G.-H. Cottet and L. Germain, Image processing through reaction combined with nonlinear diffusion, Mathematics of Computation, pages 659–673, 1993.]"
      }
    ],
    "author_details": [
      {
        "name": "Yunjin Chen",
        "affiliation": "Graz University of Technology",
        "email": "[Email not available in the provided text]"
      },
      {
        "name": "Wei Yu",
        "affiliation": "Graz University of Technology",
        "email": "[Email not available in the provided text]"
      },
      {
        "name": "Thomas Pock",
        "affiliation": "Graz University of Technology, Digital Safety & Security Department, AIT Austrian Institute of Technology GmbH",
        "email": "[Email not available in the provided text]"
      }
    ]
  },
  {
    "title": "An Active Search Strategy for Efﬁcient Object Class Detection\n---AUTHOR---\nAbel Gonzalez-Garcia\nAlexander Vezhnevet\nVittorio Ferrari",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Gonzalez-Garcia_An_Active_Search_2015_CVPR_paper.pdf",
    "id": "Gonzalez-Garcia_An_Active_Search_2015_CVPR_paper",
    "abstract": "Object class detectors typically evaluate a window classifier on a large set of windows. This paper introduces an active search strategy that sequentially chooses the next window to evaluate based on previously observed windows. This approach reduces the number of classifier evaluations and offers a more elegant solution. The search is guided by context (statistical relation between window appearance and location relative to the object) and classifier score. Experiments with R-CNN on the SUN2012 dataset demonstrate that the method matches the detection accuracy of evaluating all windows independently while evaluating 9× fewer windows.",
    "topics": [
      "Object Class Detection",
      "Active Search Strategy",
      "Contextual Information",
      "Random Forest",
      "Window Classifier"
    ],
    "references": [],
    "author_details": [
      {
        "name": "Abel Gonzalez-Garcia",
        "affiliation": "University of Edinburgh",
        "email": "a.gonzalez-garcia@sms.ed.ac.uk"
      },
      {
        "name": "Alexander Vezhnevet",
        "affiliation": "University of Edinburgh",
        "email": "avezhnev@inf.ed.ac.uk"
      },
      {
        "name": "Vittorio Ferrari",
        "affiliation": "University of Edinburgh",
        "email": "vferrari@staffmail.ed.ac.uk"
      }
    ]
  },
  {
    "title": "Holistic 3D Scene Understanding from a Single Geo-tagged Image\n---AUTHOR---\nShenlong Wang\n---AUTHOR---\nSanja Fidler\n---AUTHOR---\nRaquel Urtasun",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Wang_Holistic_3D_Scene_2015_CVPR_paper.pdf",
    "id": "Wang_Holistic_3D_Scene_2015_CVPR_paper",
    "abstract": "Here we argue that there is much more prior information that one could use and is freely available. We live in an era where technology and social networks are part of our everyday’s life. A single monocular image is thus no longer our only source of information, a whole cyber world sits behind it. In this paper we make use of geotagged images, and propose priors derived from map data which contains information about the scene, such as the geolocation and rough shape of roads, buildings and trees. Towards this goal, we make use of OpenStreetMaps (OSM) which is freely available. In this paper we propose a holistic conditional random field (CRF) that reasons jointly about 3D object detection, pose estimation, semantic segmentation as well as depth reconstruction from a single image. Our approach takes advantage of large-scale crowd-sourced maps to generate dense geographic, geometric and semantic priors by rendering the 3D world. We demonstrate the effectiveness of our approach on the challenging KITTI dataset [13], and show significant improvements over the baselines in all tasks.\n\n---TOPIPS---\n3D Scene Understanding\nGeotagged Images\nOpenStreetMap (OSM)\nConditional Random Fields (CRF)\nHolistic Modeling",
    "topics": [],
    "references": [],
    "author_details": [
      {
        "name": "Shenlong Wang",
        "affiliation": "Department of Computer Science, University of Toronto",
        "email": "slwang@cs.toronto.edu"
      },
      {
        "name": "Sanja Fidler",
        "affiliation": "Department of Computer Science, University of Toronto",
        "email": "fidler@cs.toronto.edu"
      },
      {
        "name": "Raquel Urtasun",
        "affiliation": "Department of Computer Science, University of Toronto",
        "email": "urtasun@cs.toronto.edu"
      }
    ]
  },
  {
    "title": "Interleaved Text/Image Deep Mining on a Large-Scale Radiology Database\n---AUTHORs---\nHoo-Chang Shin\nLe Lu\nLauren Kim\nAri Seff\nJianhua Yao\nRonald M. Summers",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Shin_Interleaved_TextImage_Deep_2015_CVPR_paper.pdf",
    "id": "Shin_Interleaved_TextImage_Deep_2015_CVPR_paper",
    "abstract": "Abstract not found",
    "topics": [],
    "references": [
      {
        "citation": "[Gupta, A., Ayhan, M., & Maida, A. (2013). Natural image bases to represent neuroimaging data. In ICML.]"
      },
      {
        "citation": "[Hodosh, M., Young, P., & Hockenmaier, J. (2013). Framing image description as a ranking task: Data, models and evaluation metrics. Journal of Artificial Intelligence Research, 853–899.]"
      },
      {
        "citation": "[Girolami, M., & Kabán, A. (2003). On an equivalence between PLSI and LDA. In Proceedings of the 26th annual international ACM SIGIR conference on Research and development in information retrieval, 433–434.]"
      },
      {
        "citation": "[Hofmann, T. (2001). Probabilistic latent semantic indexing. In Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval.]"
      },
      {
        "citation": "[Gupta, S., Girshick, R., Arbelez, P., & Malik, J. (2014). Learning rich features from rgb-d images for object detection and segmentation. In ECCV.]"
      },
      {
        "citation": "[ImageNet (2009). http://www.image-net.org/]"
      },
      {
        "citation": "[RNN (2013). Recurrent Neural Network]"
      },
      {
        "citation": "[Deep CNN (2012). Deep Convolutional Neural Network]"
      },
      {
        "citation": "[VGG-19 (2014). Visual Geometry Group - 19]"
      },
      {
        "citation": "[DO (2018). Disease Ontology. http://disease-ontology.org/]"
      },
      {
        "citation": "[Flickr30K (2013). Flickr30K dataset]"
      },
      {
        "citation": "[PACS (2023). Picture Archiving and Communication System]"
      },
      {
        "citation": "[Biowulf (2023). Biowulf Linux cluster at the National Institutes of Health]"
      },
      {
        "citation": "[NVIDIA (2023). GPU donation of K40]"
      }
    ],
    "author_details": [
      {
        "name": "Hoo-Chang Shin",
        "affiliation": "National Institutes of Health Clinical Center",
        "email": "hoochang.shin@nih.gov"
      },
      {
        "name": "Le Lu",
        "affiliation": "National Institutes of Health Clinical Center",
        "email": "le.lu@nih.gov"
      },
      {
        "name": "Lauren Kim",
        "affiliation": "National Institutes of Health Clinical Center",
        "email": "lauren.kim2@nih.gov"
      },
      {
        "name": "Ari Seff",
        "affiliation": "National Institutes of Health Clinical Center",
        "email": "ari.seff@nih.gov"
      },
      {
        "name": "Jianhua Yao",
        "affiliation": "National Institutes of Health Clinical Center",
        "email": "jyao@cc.nih.gov"
      },
      {
        "name": "Ronald M. Summers",
        "affiliation": "National Institutes of Health Clinical Center",
        "email": "rms@nih.gov"
      }
    ]
  },
  {
    "title": "Part-based modelling of compound scenes from images\n---AUTHOR---\nAnton van den Hengel\nChris Russell\nAnthony Dick\nJohn Bastian\nDaniel Pooley\nLachlan Fleming\nLourdes Agapito",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Hengel_Part-Based_Modelling_of_2015_CVPR_paper.pdf",
    "id": "Hengel_Part-Based_Modelling_of_2015_CVPR_paper",
    "abstract": "Abstract not found",
    "topics": [],
    "references": [
      {
        "citation": "[Roberts, L. Machine perception of 3D solids. PhD thesis, Stanford, 1965.] - This is a foundational work in the field, establishing early concepts of 3D solid modeling and perception."
      },
      {
        "citation": "[Brooks, R. A., Creiner, R., & Binford, T. O. The acronym model-based vision system. In Proceedings of IJCAI, pages 105–113, 1979.] - A seminal paper on model-based vision systems, a key approach in 3D reconstruction."
      },
      {
        "citation": "[Matouˇsek, J. On variants of the Johnson-Lindenstrauss lemma. Random Struct. Algorithms, 33:142–156, September 1998.] - This reference is important for understanding dimensionality reduction techniques, which are often used in hashing and nearest neighbor search."
      },
      {
        "citation": "[Indyk, P., & Motwani, R. Approximate nearest neighbors: towards removing the curse of dimensionality. In Proceedings ACM symposium on Theory of computing, pages 604–613. ACM, 1998.] - This paper introduces techniques for approximate nearest neighbor search, crucial for efficient hashing."
      },
      {
        "citation": "[Pauly, M., Mitra, N. J., Wallner, J., Pottmann, H., & Guibas, L. Discovering structural regularity in 3D geometry. ACM Transactions on Graphics, 27(3):#43, 1–11, 2008.] - This work explores the discovery of structural regularity in 3D geometry, which is relevant to shape modeling and understanding."
      },
      {
        "citation": "[Kutulakos, K. N., & Seitz, S. M. A theory of shape by space carving. International Journal of Computer Vision, 38(3):199–218, 2000.] - Introduces the concept of shape by space carving, a method for 3D shape reconstruction."
      },
      {
        "citation": "[Gupta, A., Efroos, A. A., & Hebert, M. Blocks world revisited: Image understanding using qualitative geometry and mechanics. In ECCV, 2010.] - This paper demonstrates image understanding using qualitative geometry and mechanics, a relevant approach for 3D scene interpretation."
      },
      {
        "citation": "[Shen, F., Shen, C., Shi, Q., van den Hengel, A., & Tang, Z. Inductive hashing on manifolds. In Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on, pages 1562–1569. IEEE, 2013.] - This paper focuses on inductive hashing on manifolds, a technique for efficient image retrieval."
      },
      {
        "citation": "[Li, X., Lin, G., Shen, C., van den Hengel, A., & Dick, A. Learning hash functions using column generation. In International Conference on Machine Learning (ICML’13), Atlanta, USA, 2013.] - This paper presents a method for learning hash functions using column generation, a relevant technique for efficient hashing."
      },
      {
        "citation": "[van den Hengel, A., Dick, A., Thorm¨ahlen, T., Ward, B., & Torr, P. H. Videotrace: rapid interactive scene modelling from video. In ACM Transactions on Graphics (TOG), volume 26, page 86. ACM, 2007.] - This paper describes Videotrace, a system for rapid interactive scene modeling from video."
      }
    ],
    "author_details": [
      {
        "name": "Anton van den Hengel",
        "affiliation": "The University of Adelaide",
        "email": "Anton.vandenHengel@adelaide.edu.au"
      },
      {
        "name": "Chris Russell",
        "affiliation": "University College London",
        "email": "crussell@cs.ucl.ac.uk"
      },
      {
        "name": "Anthony Dick",
        "affiliation": "The University of Adelaide"
      },
      {
        "name": "John Bastian",
        "affiliation": "The University of Adelaide"
      },
      {
        "name": "Daniel Pooley",
        "affiliation": "The University of Adelaide"
      },
      {
        "name": "Lachlan Fleming",
        "affiliation": "The University of Adelaide"
      },
      {
        "name": "Lourdes Agapito",
        "affiliation": "University College London"
      }
    ]
  },
  {
    "title": "Active Sample Selection and Correction Propagation on a Gradually-Augmented Graph\n---AUTHOR---\n(The paper is an anonymous submission, so no authors are listed)",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Su_Active_Sample_Selection_2015_CVPR_supplemental.pdf",
    "id": "Su_Active_Sample_Selection_2015_CVPR_supplemental",
    "abstract": "This paper presents a derivation of the correction propagation algorithm, a key component of a larger system for active sample selection and correction propagation on a gradually-augmented graph. The derivation is shown through a series of equations, ultimately demonstrating the algorithm's behavior as a parameter approaches infinity. The solution involves substituting equations and simplifying expressions to arrive at a final form for the algorithm's output.",
    "topics": [
      "Active sample selection",
      "Correction propagation",
      "Graph algorithms",
      "Derivation of algorithms",
      "Numerical analysis"
    ],
    "references": [],
    "author_details": []
  },
  {
    "title": "Low-level Vision by Consensus in a Spatial Hierarchy of Regions: Supplementary Material\n---AUTHOR---\nAyan Chakrabarti\nYing Xiong\nSteven J. Gortler\nTodd Zickler",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Chakrabarti_Low-Level_Vision_by_2015_CVPR_supplemental.pdf",
    "id": "Chakrabarti_Low-Level_Vision_by_2015_CVPR_supplemental",
    "abstract": "This paper presents a low-level vision algorithm based on consensus in a spatial hierarchy of regions. The algorithm minimizes a cost function that balances data fidelity and consistency between neighboring regions at different scales. To avoid poor local minima and promote convergence, a consistency weight is gradually increased during optimization, starting from a lower value and increasing it slowly to the desired weight. The paper details the inference algorithm and analyzes the effect of different consistency weight schedules on convergence, demonstrating that a gradual increase leads to better solutions.\n\n---TOPSICS---\nLow-level vision\nConsensus optimization\nSpatial hierarchies\nConsistency weight scheduling\nImage processing",
    "topics": [],
    "references": [],
    "author_details": [
      {
        "name": "Ayan Chakrabarti",
        "affiliation": "Not available in the provided text.",
        "email": "Not available in the provided text."
      },
      {
        "name": "Ying Xiong",
        "affiliation": "Not available in the provided text.",
        "email": "Not available in the provided text."
      },
      {
        "name": "Steven J. Gortler",
        "affiliation": "Not available in the provided text.",
        "email": "Not available in the provided text."
      },
      {
        "name": "Todd Zickler",
        "affiliation": "Not available in the provided text.",
        "email": "Not available in the provided text."
      }
    ]
  },
  {
    "title": "Material recognition in the wild with the Materials in Context Database (Supplemental Material)\n---AUTHOR---\nSean Bell\n---AUTHOR---\nPaul Upchurch\n---AUTHOR---\nNoah Snavely\n---AUTHOR---\nKavita Bala",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Bell_Material_Recognition_in_2015_CVPR_supplemental.pdf",
    "id": "Bell_Material_Recognition_in_2015_CVPR_supplemental",
    "abstract": "This supplemental material provides details omitted from the main paper \"Material recognition in the wild with the Materials in Context Database (MINC).\" It includes information on the interface for Amazon Mechanical Turk (AMT) tasks, dataset statistics (train/val/test splits), single patch classification results, the near-duplicate method used, detailed results from the Feature Matching and Detection (FMD) method, and a comprehensive list of experimental results. The Materials in Context Database (MINC) itself is publicly available, allowing for reproducibility of the presented experiments.\n\n---TOPICIS---\nCrowdsourcing\nMaterial Recognition\nDataset Creation (MINC)\nNear-Duplicate Detection\nFeature Matching and Detection (FMD)",
    "topics": [],
    "references": [
      {
        "citation": "[1] S. Bell, P. Upchurch, N. Snavely, and K. Bala. OpenSurfaces: A richly annotated catalog of surface appearance. ACM Trans. on Graphics (SIGGRAPH), 32(4), 2013."
      },
      {
        "citation": "[2] M. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed, and A. Vedaldi. Describing textures in the wild. In CVPR, pages 3606–3613. IEEE, 2014."
      }
    ],
    "author_details": [
      {
        "name": "Sean Bell",
        "affiliation": "Department of Computer Science, Cornell University",
        "email": "sbell@cs.cornell.edu"
      },
      {
        "name": "Paul Upchurch",
        "affiliation": "Department of Computer Science, Cornell University",
        "email": "paulu@cs.cornell.edu"
      },
      {
        "name": "Noah Snavely",
        "affiliation": "Department of Computer Science, Cornell University",
        "email": "snavely@cs.cornell.edu"
      },
      {
        "name": "Kavita Bala",
        "affiliation": "Department of Computer Science, Cornell University",
        "email": "kb@cs.cornell.edu"
      }
    ]
  },
  {
    "title": "Video Anomaly Detection and Localization Using Hierarchical Feature Representation and Gaussian Process Regression\n---AUTHOR---\nKai-Wen Cheng\nYie-Tarng Chen\nWen-Hsien Fang",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Cheng_Video_Anomaly_Detection_2015_CVPR_paper.pdf",
    "id": "Cheng_Video_Anomaly_Detection_2015_CVPR_paper",
    "abstract": "This paper presents a hierarchical framework for detecting local and global anomalies via hierarchical feature representation and Gaussian process regression. The framework addresses the challenge of detecting global anomalies, which involve unusual interactions between multiple normal events (e.g., car accidents). The approach extracts normal interactions from training video by finding frequent geometric relations of sparse spatiotemporal interest points, constructs a codebook of interaction templates modeled using Gaussian process regression, and proposes a novel inference method for computing interaction likelihood. The model is robust to topological deformations and data imbalances, achieving high detection rates on challenging datasets.\n\n---TOPICAS---\nVideo Anomaly Detection\nHierarchical Feature Representation\nGaussian Process Regression\nSpatio-Temporal Interest Points (STIPs)\nGlobal Anomaly Detection",
    "topics": [],
    "references": [],
    "author_details": [
      {
        "name": "Kai-Wen Cheng",
        "affiliation": "Department of Electronic and Computer Engineering, National Taiwan University of Science and Technology",
        "email": "D10102101@mail.ntust.edu.tw"
      },
      {
        "name": "Yie-Tarng Chen",
        "affiliation": "Department of Electronic and Computer Engineering, National Taiwan University of Science and Technology",
        "email": "ytchen@mail.ntust.edu.tw"
      },
      {
        "name": "Wen-Hsien Fang",
        "affiliation": "Department of Electronic and Computer Engineering, National Taiwan University of Science and Technology",
        "email": "whf@mail.ntust.edu.tw"
      }
    ]
  },
  {
    "title": "Deep Correlation for Matching Images and Text\n---AUTHOR---\nFei Yan\nKrystian Mikolajczyk",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Yan_Deep_Correlation_for_2015_CVPR_paper.pdf",
    "id": "Yan_Deep_Correlation_for_2015_CVPR_paper",
    "abstract": "This paper addresses the problem of matching images and captions in a joint latent space learnt with deep canonical correlation analysis (DCCA). The image and caption data are represented by the outputs of the vision and text based deep neural networks. The high dimensionality of the features presents a great challenge in terms of memory and speed complexity when used in DCCA framework. We address these problems by a GPU implementation and propose methods to deal with overﬁtting. This makes it possible to evaluate DCCA approach on popular caption-image matching benchmarks. We compare our approach to other recently proposed techniques and present state of the art results on three datasets.",
    "topics": [
      "Image-Text Matching",
      "Deep Canonical Correlation Analysis (DCCA)",
      "GPU Implementation",
      "Overfitting Mitigation",
      "Cross-Modal Retrieval"
    ],
    "references": [],
    "author_details": [
      {
        "name": "Fei Yan",
        "affiliation": "Centre for Vision, Speech and Signal Processing, University of Surrey",
        "email": "f.yan@surrey.ac.uk"
      },
      {
        "name": "Krystian Mikolajczyk",
        "affiliation": "Centre for Vision, Speech and Signal Processing, University of Surrey",
        "email": "k.mikolajczyk@surrey.ac.uk"
      }
    ]
  },
  {
    "title": "Illumination and Reﬂectance Spectra Separation of a Hyperspectral Image Meets Low-Rank Matrix Factorization\n---AUTHOR---\nYinqiang Zheng\nImari Sato\nYoichi Sato",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Zheng_Illumination_and_Reflectance_2015_CVPR_supplemental.pdf",
    "id": "Zheng_Illumination_and_Reflectance_2015_CVPR_supplemental",
    "abstract": "In the supplementary material, we detail the similarities and discrepancies between the illumination and reﬂectance spectra separation (IRSS) problem and the well-known non-rigid structure-from-motion (NRSfM) problem. The IRSS problem is formulated as a low-rank matrix factorization, drawing parallels to the factorization of the NRSfM problem. We highlight the connections and differences between these two problems, aiming to inspire collaborative efforts in photometric and geometric computer vision.",
    "topics": [
      "Illumination and Reflectance Spectra Separation (IRSS)",
      "Non-Rigid Structure-from-Motion (NRSfM)",
      "Low-Rank Matrix Factorization",
      "Computer Vision",
      "Geometric Vision"
    ],
    "references": [],
    "author_details": [
      {
        "name": "Yinqiang Zheng",
        "affiliation": "National Institute of Informatics",
        "email": "yqzheng@nii.ac.jp"
      },
      {
        "name": "Imari Sato",
        "affiliation": "National Institute of Informatics",
        "email": "imarik@nii.ac.jp"
      },
      {
        "name": "Yoichi Sato",
        "affiliation": "The University of Tokyo",
        "email": "ysato@iis.u-tokyo.ac.jp"
      }
    ]
  },
  {
    "title": "Image Denoising via Adaptive Soft-Thresholding Based on Non-Local Samples\n---AUTHOR---\nHangfan Liu\nRuiqin Xiong\nJian Zhang\nWen Gao",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Liu_Image_Denoising_via_2015_CVPR_paper.pdf",
    "id": "Liu_Image_Denoising_via_2015_CVPR_paper",
    "abstract": "This paper proposes a new image denoising approach using adaptive signal modeling and adaptive soft-thresholding. It improves image quality by regularizing all patches in the image based on distribution modeling in the transform domain. Instead of using a global model for all patches, it employs content adaptive models to address the non-stationarity of image signals. The distribution model of each patch is estimated individually, allowing for non-zero expectation, and utilizes non-local correlation of image patches as data samples to estimate parameters. Experimental results demonstrate that the proposed scheme outperforms state-of-the-art denoising methods like BM3D and CSR in both PSNR and perceptual quality.\n\n---TOPIC---\nImage Denoising\nAdaptive Soft-Thresholding\nNon-Local Correlation\nDistribution Modeling\nTransform Domain",
    "topics": [],
    "references": [
      {
        "citation": "[Tomasi, C., & Manduchi, R. (1998). Bilateral filtering for gray and color images. *IEEE International Conference on Computer Vision*, 839–846.]"
      },
      {
        "citation": "[Dabov, K., Foi, A., Katkovnik, V., & Egiazharian, K. (2007). Image denoising by sparse 3-d transform-domain collaborative filtering. *IEEE Transactions on Image Processing*, *16*(8), 2080–2095.]"
      },
      {
        "citation": "[Elad, M., & Aharon, M. (2006). Image denoising via sparse and redundant representations over learned dictionaries. *IEEE Transactions on Image Processing*, *15*(12), 12, 3736–3745.]"
      },
      {
        "citation": "[Foi, A., Katkovnik, V., & Egiazharian, K. (2007). Pointwise shape-adaptive dct for high-quality denoising and deblocking of grayscale and color images. *IEEE Transactions on Image Processing*, *16*(5), 1395–1411.]"
      },
      {
        "citation": "[Donoho, D.L. (1995). De-noising by soft-thresholding. *IEEE Transactions on Information Theory*, *41*(3), 613–627.]"
      },
      {
        "citation": "[Rudin, L.I., Osher, S., & Fatemi, E. (1992). Nonlinear total variation based noise removal algorithms. *Physica D: Nonlinear Phenomena*, *60*(1), 259–268.]"
      },
      {
        "citation": "[Chatterjee, P., & Milanfar, P. (2009). Clustering-based denoising with locally learned dictionaries. *IEEE Transactions on Image Processing*, *18*(7), 1438–1451.]"
      },
      {
        "citation": "[Zhang, L., Dong, W., Zhang, D., & Shi, G. (2010). Two-stage image denoising by principal component analysis with local pixel grouping. *Pattern Recognition*, *43*(4), 1531–1549.]"
      },
      {
        "citation": "[Wang, Z., Bovik, A.C., Sheikh, H.R., & Simoncelli, E.P. (2004). Image quality assessment: from error visibility to structural similarity. *IEEE Transactions on Image Processing*, *13*(4), 600–612.]"
      },
      {
        "citation": "[Beck, A., & Teboulle, M. (2009). Fast gradient-based algorithms for constrained total variation image denoising and deblurring problems. *IEEE Transactions on Image Processing*, *18*(11), 2419–2434.]"
      }
    ],
    "author_details": [
      {
        "name": "Hangfan Liu",
        "affiliation": "Institute of Digital Media, Peking University",
        "email": "liuhf@pku.edu.cn"
      },
      {
        "name": "Ruiqin Xiong",
        "affiliation": "Institute of Digital Media, Peking University",
        "email": "rqxiong@pku.edu.cn"
      },
      {
        "name": "Jian Zhang",
        "affiliation": "Institute of Digital Media, Peking University",
        "email": "jian.zhang@pku.edu.cn"
      },
      {
        "name": "Wen Gao",
        "affiliation": "Institute of Digital Media, Peking University",
        "email": "wgao@pku.edu.cn"
      }
    ]
  },
  {
    "title": "Texture Representations for Image and Video Synthesis\n---AUTHOR---\nGeorgios Georgiadis\nAlessandro Chiuso\nStefano Soatto",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Georgiadis_Texture_Representations_for_2015_CVPR_paper.pdf",
    "id": "Georgiadis_Texture_Representations_for_2015_CVPR_paper",
    "abstract": "In texture synthesis and classification, algorithms typically require a small texture as input, assumed to be representative of a larger region. This paper addresses the challenge of automatically characterizing and retrieving these textures. The authors propose a new representation that compactly summarizes a texture, enabling texture compression and synthesis. They demonstrate its use in a video texture synthesis algorithm for generating novel textures and video hole-filling, and introduce a novel criterion for measuring dissimilarity between textures. The representation avoids external information and automatically determines local neighborhood scales, mitigating synthesis biases.\n\n---TOPICs---\nTexture Synthesis\nTexture Representation\nVideo Processing\nTexture Dissimilarity\nImage Compression",
    "topics": [],
    "references": [
      {
        "citation": "[Z. Bar-Joseph, R. El-Yaniv, D. Lischinski, and M. Werman, Texture mixing and texture movie synthesis using statistical learning, TVCG, 2001]"
      },
      {
        "citation": "[Lucas, B., & Kanade, T. An iterative image registration technique with an application to stereo vision. IJCAI, 1981]"
      },
      {
        "citation": "[Malik, J., & Perona, P. Preattentive texture discrimination with early vision mechanisms. JOSAA, 1990]"
      },
      {
        "citation": "[Bertalmio, M., Vese, L., Sapiro, G., & Osher, S. Simultaneous structure and texture image inpainting. TIP, 2003]"
      },
      {
        "citation": "[Bradley, P. S., Mangasarian, O. L., & Street, W. N. Clustering via concave minimization. NIPS, 1997]"
      },
      {
        "citation": "[Sheikh, H., & Bovik, A. Image information and visual quality. IEEE Transactions on Image Processing, 2006]"
      },
      {
        "citation": "[Cimpoi, M., Maji, S., Kokkinos, I., Mohamed, S., & Vedaldi, A. Describing textures in the wild. CVPR, 2014]"
      },
      {
        "citation": "[Sifre, L., & Mallat, S. Rotation, scaling and deformation invariant scattering for texture discrimination. CVPR, 2013]"
      },
      {
        "citation": "[Haralick, R. M. Statistical and structural approaches to texture. Proceedings of the IEEE, 2005]"
      },
      {
        "citation": "[Freeman, W., & Liu, C. Markov random fields for super-resolution and texture synthesis. Advances in Markov Random Fields for Vision and Image Processing, 2011]"
      }
    ],
    "author_details": [
      {
        "name": "Georgios Georgiadis",
        "affiliation": "UCLA Vision Lab, University of California, Los Angeles, CA 90095",
        "email": "giorgos@ucla.edu"
      },
      {
        "name": "Alessandro Chiuso",
        "affiliation": "Dept. of Information Eng., University of Padova, Padova 35131, Italy",
        "email": "chiuso@dei.unipd.it"
      },
      {
        "name": "Stefano Soatto",
        "affiliation": "UCLA Vision Lab, University of California, Los Angeles, CA 90095",
        "email": "soatto@ucla.edu"
      }
    ]
  },
  {
    "title": "Traditional Saliency Reloaded: A Good Old Model in New Shape\n---AUTHOR---\nSimone Frintrop\nThomas Werner\nGerm´an M. Garc´ıa",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Frintrop_Traditional_Saliency_Reloaded_2015_CVPR_supplemental.pdf",
    "id": "Frintrop_Traditional_Saliency_Reloaded_2015_CVPR_supplemental",
    "abstract": "This paper presents a re-evaluation of a traditional salience model, \"Traditional Saliency Reloaded,\" and demonstrates its effectiveness through comparative results on several benchmark datasets. The study investigates the influence of key parameters within the system and compares its performance against existing salience methods, highlighting the benefits of integrating segmentation techniques for improved results. The paper emphasizes the system's robustness and potential for real-time applications while acknowledging limitations and future directions.",
    "topics": [
      "Saliency Detection",
      "Parameter Optimization",
      "Segmentation Techniques",
      "Benchmark Datasets",
      "Comparative Analysis"
    ],
    "references": [
      {
        "citation": "[Ittti, L., Koch, C., & Niebur, E. (1998). A model of salience-based visual attention for rapid scene analysis. *TPAMI*, *20*(11).] - This is a foundational paper introducing a key model for visual attention."
      },
      {
        "citation": "[Li, Y., Hou, X., Koch, C., Rehg, J. M., & Yuille, A. L. (2014). The secrets of salient object segmentation. In *CVPR*.] - This paper appears to be a comprehensive overview and exploration of salient object segmentation."
      },
      {
        "citation": "[Jiang, H., Wang, J., Yuan, Z., Wu, Y., Zheng, N., & Li, S. (2013). Salient object detection: A discriminative regional feature integration approach. In *CVPR*.] - This paper focuses on a specific approach to salient object detection using regional features."
      },
      {
        "citation": "[Bruce, N. D. B., & Tsotzos, J. K. (2009). Salience, attention, and visual search: An information theoretic approach. *J. of Vision*, *9*(3).] - This paper provides a theoretical framework for understanding salience using information theory."
      },
      {
        "citation": "[Cheng, M.-M., Mitra, N. J., Huang, X., Torr, P. H. S., & Hu, S.-M. (2015). Global contrast based salient region detection. *TPAMI*, *37*(3):569–582.] - This paper presents a specific method for salient region detection based on global contrast."
      },
      {
        "citation": "[Achanta, R., & S¨usstrunk, S. (2010). Saliency Detection using Maximum Symmetric Surround. In *ICIP*.] - This paper details a specific method for salience detection."
      },
      {
        "citation": "[Hou, X., & Zhang, L. (2008). Dynamic visual attention: Searching for coding length increments. In *Advances in Neural Information Processing Systems*.] - This paper explores visual attention using a coding length increment approach."
      },
      {
        "citation": "[Yang, C., Zhang, L., Lu, H., Ruan, X., & Yang, M.-H. (2013). Saliency Detection via Graph-based Manifold Ranking. In *CVPR*.] - This paper presents a method for salience detection using graph-based manifold ranking."
      },
      {
        "citation": "[Alpert, S., Galun, M., Basri, R., & Brandt, A. (2007). Image segmentation by probabilistic bottom-up aggregation and cue integration. In *CVPR*.] - This paper details an image segmentation approach that is relevant to salient object detection."
      },
      {
        "citation": "[Margolin, R., Zelnik-Manor, L., & Tal, A. (2014). How to evaluate foreground maps? ] - This paper addresses the crucial aspect of evaluating the results of salient object detection methods."
      }
    ],
    "author_details": [
      {
        "name": "Simone Frintrop",
        "affiliation": "Institute of Computer Science III, Rheinische Friedrich-Wilhelms-Universität Bonn, Germany",
        "email": "frintrop@iai.uni-bonn.de"
      },
      {
        "name": "Thomas Werner",
        "affiliation": "Institute of Computer Science III, Rheinische Friedrich-Wilhelms-Universität Bonn, Germany",
        "email": "*Not available*"
      },
      {
        "name": "Germ´an M. Garc´ıa",
        "affiliation": "Institute of Computer Science III, Rheinische Friedrich-Wilhelms-Universität Bonn, Germany",
        "email": "*Not available*"
      }
    ]
  },
  {
    "title": "Dataset Fingerprints: Exploring Image Collections Through Data Mining\n---AUTHOR---\nKonstantinos Rematas\nBasura Fernando\nFrank Dellaert\nTinne Tuytelaars",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Rematas_Dataset_Fingerprints_Exploring_2015_CVPR_paper.pdf",
    "id": "Rematas_Dataset_Fingerprints_Exploring_2015_CVPR_paper",
    "abstract": "As the amount of visual data increases, so does the need for summarization tools that can be used to explore large image collections and to quickly get familiar with their content. In this paper, we propose dataset fingerprints, a new and powerful method based on data mining that extracts meaningful patterns from a set of images. The discovered patterns are compositions of discriminative mid-level features that co-occur in several images. Compared to earlier work, ours stands out because i) it’s fully unsu- pervised, ii) discovered patterns cover large parts of the images, often corresponding to full objects or meaningful parts thereof, and iii) different patterns are connected based on co-occurrence, allowing a user to “browse” the images from one pattern to the next and to group patterns in a semantically meaningful manner.\n\n---TOPIC---\nData Mining\nImage Collection Summarization\nDataset Fingerprints\nMid-level Features\nUnsupervised Pattern Discovery",
    "topics": [],
    "references": [
      {
        "citation": "[Cao, S., & Snavely, N. (2013). Graph-based discriminative learning for location recognition. In CVPR.]"
      },
      {
        "citation": "[Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., & Fei-Fei, L. (2009). ImageNet: A Large-Scale Hierarchical Image Database. In CVPR.]"
      },
      {
        "citation": "[Doersch, C., Gupta, A., & Efroos, A. A. (2014). Context as supervisory signal: Discovering objects with predictable context. In ECCV.]"
      },
      {
        "citation": "[Doersch, S., Singh, A., Gupta, A., Sivic, J., & Efroos, A. A. (2012). What makes paris look like paris? ACM Trans. Graph., 31(101).]"
      },
      {
        "citation": "[Everingham, M., Van Gool, L., Williams, C. K., Winn, J., & Zisserman, A. (2010). The pascal visual object classes (voc) challenge. International journal of computer vision, 88(2), 303–338.]"
      },
      {
        "citation": "[Fernando, B., Fromont, E., & Tuytelaars, T. (2012). Effective use of frequent itemset mining for image classification. In ECCV.]"
      },
      {
        "citation": "[Faktor, A., & Iran, M. (2012). Clustering by composition unsupervised discovery of image categories. In ECCV.]"
      },
      {
        "citation": "[Heath, K., Gelfand, N., Ovsjanikov, M., Aanjaneya, M., & Guibas, L. J. (2010). Image webs: Computing and exploiting connectivity in image collections. In CVPR.]"
      },
      {
        "citation": "[Kim, G., Faloutsos, C., & Hebert, M. (2008). Unsupervised Modeling of Object Categories Using Link Analysis Techniques. In CVPR.]"
      },
      {
        "citation": "[Singh, S., Gupta, A., & Efroos, A. A. (2012). Unsupervised discovery of mid-level discriminative patches. In ECCV.]"
      }
    ],
    "author_details": [
      {
        "name": "Konstantinos Rematas",
        "affiliation": "KU Leuven, ESAT-PSI, iMinds",
        "email": "krematas@esat.kuleuven.be (available through project website)"
      },
      {
        "name": "Basura Fernando",
        "affiliation": "KU Leuven, ESAT-PSI, iMinds",
        "email": "Not available"
      },
      {
        "name": "Frank Dellaert",
        "affiliation": "Georgia Tech",
        "email": "Not available"
      },
      {
        "name": "Tinne Tuytelaars",
        "affiliation": "KU Leuven, ESAT-PSI, iMinds",
        "email": "Not available"
      }
    ]
  },
  {
    "title": "The k-Support Norm and Convex Envelopes of Cardinality and Rank\n---AUTHOR---\nAnders Eriksson\nTrung Thanh Pham\nTat-Jun Chin\nIan Reid",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Eriksson_The_k-Support_Norm_2015_CVPR_paper.pdf",
    "id": "Eriksson_The_k-Support_Norm_2015_CVPR_paper",
    "abstract": "This paper revisits the k-support norm, a convex relaxation of cardinality that provides the tightest bound on cardinality over the Euclidean norm unit ball. The authors present a re-derivation of this norm, aiming to clarify its connection to convex envelopes and the rank operator. They establish a connection between cardinality, the rank operator, and the nuclear norm. Furthermore, they propose a novel and computationally efficient algorithm for solving optimization problems involving the k-support norm, demonstrating significant speedups compared to existing methods through empirical validation. The paper aims to improve understanding of the k-support norm and provide a practical, faster solution for related optimization problems.",
    "topics": [
      "k-support norm",
      "Convex relaxation",
      "Cardinality",
      "Rank operator",
      "Optimization algorithms"
    ],
    "references": [
      {
        "citation": "[Argyriou, A., Foygel, R., and Srebro, N. Sparse prediction with the k-support norm. Advances in Neural Information Processing Systems 25, 2012.] - *This paper introduces the k-support norm, likely relevant to the paper's core methodology.*"
      },
      {
        "citation": "[Bhatia, R. Matrix Analysis. Springer, 1997.] - *A foundational text on matrix analysis, likely providing mathematical background.*"
      },
      {
        "citation": "[Cai, J.-F., Cand`es, E. J., and Shen, Z. A singular value thresholding algorithm for matrix completion. SIAM J. on Optimization, 20(4), 2010.] - *Matrix completion is a common technique, and this paper presents a specific algorithm.*"
      },
      {
        "citation": "[Recht, B., Fazel, M., and Parillo, P. A. Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization. SIAM Rev., 52(3), 2010.] - *This is a key reference on nuclear norm minimization, a common regularization technique for matrix factorization and related problems.*"
      },
      {
        "citation": "[Lai, H., Pan, Y., Lu, C., Tang, Y., and Yan, S. Efﬁcient k-support matrix pursuit. European Conference on Computer Vision, 8, 2014.] - *This paper likely builds upon the k-support norm introduced earlier and focuses on efficient computation.*"
      },
      {
        "citation": "[Parih, N., and Boyd, S. Proximal Algorithms. Foundations and Trends in Optimization, 1(3), 2014.] - *Provides a comprehensive overview of proximal algorithms, a likely core technique.*"
      },
      {
        "citation": "[Fazel, M. Matrix Rank Minimization with Applications. PhD thesis, Stanford University.] - *A thesis focused on matrix rank minimization, a related concept.*"
      },
      {
        "citation": "[Combettes, P. L., and Pesquet, J.-C. Proximal Splitting Methods in Signal Processing. Fixed-Point Algorithms for Inverse Problems in Science and Engineering, 2011.] - *Provides context and background on proximal splitting methods.*"
      },
      {
        "citation": "[Hu, Y., Zhang, D., Ye, J., Li, X., and He, X. Fast and accurate matrix completion via truncated nuclear norm regularization. IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(9), 2013.] - *Another paper on matrix completion, focusing on speed and accuracy.*"
      },
      {
        "citation": "[Moreau, J. J. Fonctions convexes duales et points proximaux dans un espace hilbertien. Comptes Rendus de l’Acad´emie des Sciences (Paris), 255, 1962.] - *A seminal work on proximal points, providing theoretical foundations.*"
      }
    ],
    "author_details": [
      {
        "name": "Anders Eriksson",
        "affiliation": "School of Electrical Engineering and Computer Science, Queensland University of Technology",
        "email": "anders.eriksson@qut.edu.au"
      },
      {
        "name": "Trung Thanh Pham",
        "affiliation": "School of Computer Science, The University of Adelaide",
        "email": "trung.pham@adelaide.edu.au"
      },
      {
        "name": "Tat-Jun Chin",
        "affiliation": "School of Computer Science, The University of Adelaide",
        "email": "tat-jun.chin@adelaide.edu.au"
      },
      {
        "name": "Ian Reid",
        "affiliation": "School of Computer Science, The University of Adelaide",
        "email": "ian.reid@adelaide.edu.au"
      }
    ]
  },
  {
    "title": "Adaptive Region Pooling for Object Detection\n---AUTHOR---\nYi-Hsuan Tsai\nOnur C. Hamsici\nMing-Hsuan Yang",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Tsai_Adaptive_Region_Pooling_2015_CVPR_paper.pdf",
    "id": "Tsai_Adaptive_Region_Pooling_2015_CVPR_paper",
    "abstract": "Learning models for object detection is a challenging problem due to the large intra-class variability of objects in appearance, viewpoints, and rigidity. We address this variability by a novel feature pooling method that is adaptive to segmented regions. The proposed detection algorithm automatically discovers a diverse set of exemplars and their distinctive parts which are used to encode the region structure by the proposed feature pooling method. Based on each exemplar and its parts, a regression model is learned with samples selected by a coarse region matching scheme. The proposed algorithm performs favorably on the PASCAL VOC 2007 dataset against existing algorithms. We demonstrate the beneﬁts of our feature pooling method when compared to conventional spatial pyramid pooling features. We also show that object information can be transferred through exemplars for detected objects.\n\n---TOPIICS---\nObject Detection\nFeature Pooling\nRegion Structure\nExemplars\nAdaptive Regression",
    "topics": [],
    "references": [
      {
        "citation": "[Alexe, B., Deselaers, T., & Ferrari, V. What is an object? In CVPR, 2010.]"
      },
      {
        "citation": "[Arbeláez, P., Hariharan, B., Gu, C., Gupta, S., Bourdev, L., & Malik, J. Semantic segmentation using regions and parts. In CVPR, 2012.]"
      },
      {
        "citation": "[Aubry, M., Maturana, D., Efroos, A., Russell, B., & Sivic, J. Seeing 3d chairs: exemplar part-based 2d-3d alignment using a large dataset of cad models. In CVPR, 2014.]"
      },
      {
        "citation": "[Belkin, M., & Niyogi, P. Laplacian eigenmaps and spectral techniques for embedding and clustering. In NIPS, 2001.]"
      },
      {
        "citation": "[Bourdev, L., & Malik, J. Poselets: Body part detectors trained using 3d human pose annotations. In ICCV, 2009.]"
      },
      {
        "citation": "[Brox, T., Bourdev, L., Maji, S., & Malik, J. Object segmentation by alignment of poselet activations to image contours. In CVPR, 2011.]"
      },
      {
        "citation": "[Carreira, J., & Sminchisescu, C. Constrained parametric min-cuts for automatic object segmentation. In CVPR, 2010.]"
      },
      {
        "citation": "[Carreira, J., Batista, J., Caseiro, R., & Sminchisescu, C. Semantic segmentation with second-order pooling. In ECCV, 2012.]"
      },
      {
        "citation": "[Chen, X., Mottaghi, R., Liu, X., Fidler, S., Urtaşun, R., & Yuille, A. Detect what you can: Detecting and representing objects using holistic models and body parts. In CVPR, 2014.]"
      },
      {
        "citation": "[Divvala, S. K., Efroos, A. A., & Hebert, M. How important are deformable parts in the deformable parts model? In ECCV, 2012.]"
      }
    ],
    "author_details": [
      {
        "name": "Yi-Hsuan Tsai",
        "affiliation": "UC Merced",
        "email": "ytsai2@ucmerced.edu"
      },
      {
        "name": "Onur C. Hamsici",
        "affiliation": "Qualcomm Research, San Diego",
        "email": "ohamsici@qti.qualcomm.com"
      },
      {
        "name": "Ming-Hsuan Yang",
        "affiliation": "UC Merced",
        "email": "mhyang@ucmerced.edu"
      }
    ]
  },
  {
    "title": "Intra-Frame Deblurring by Leveraging Inter-Frame Camera Motion\n---AUTHOR---\nHaichao Zhang\nJianchao Yang",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Zhang_Intra-Frame_Deblurring_by_2015_CVPR_paper.pdf",
    "id": "Zhang_Intra-Frame_Deblurring_by_2015_CVPR_paper",
    "abstract": "Camera motion introduces motion blur, degrading the quality of video. This paper proposes a video deblurring method based on the observations that camera motion within the capture of each individual frame leads to motion blur, and camera motion between frames yields inter-frame mis-alignment that can be exploited for blur removal. The method effectively leverages information distributed across multiple video frames due to camera motion, jointly estimating the motion between consecutive frames and blur within each frame. This joint analysis is crucial for achieving effective restoration by leveraging temporal information. Experiments on synthetic and real-world blurry videos demonstrate the method's effectiveness.",
    "topics": [
      "Video Deblurring",
      "Camera Motion Estimation",
      "Inter-Frame Analysis",
      "Motion Blur Removal",
      "Joint Estimation (Motion and Blur)"
    ],
    "references": [
      {
        "citation": "[Fergus, R., Singh, B., Hertzmann, A., Roweis, S. T., & Freeman, W. T. Removing camera shake from a single photograph. In SIGGRAPH, 2006. 2, 6]"
      },
      {
        "citation": "[Agrawal, A. K., & Xu, Y. Coded exposure deblurring: Optimized codes for PSF estimation and invertibility. In CVPR, 2009. 8]"
      },
      {
        "citation": "[Ben-Ezra, M., & Nayar, S. Motion-based Motion Deblurring. IEEE Trans. Pattern Anal. Mach. Intell., 26(6):689–698, Jun 2004. 3]"
      },
      {
        "citation": "[Shechtman, E., Caspi, Y., & Irani, M. Space-time super-resolution. IEEE Trans. Pattern Anal. Mach. Intell., 27(4):531–545, 2005. 8]"
      },
      {
        "citation": "[Levin, A., Fergus, R., Durand, F., & Freeman, W. T. Deconvolution using natural image priors. Technical report, MIT, 2007. 4, 5]"
      },
      {
        "citation": "[Xu, L., & Jia, J. Two-phase kernel estimation for robust motion deblurring. In ECCV, 2010. 2, 5, 6, 7]"
      },
      {
        "citation": "[Zhang, H., & Carin, L. Multi-shot imaging: Joint alignment, deblurring and resolution enhancement. In CVPR, 2014. 2]"
      },
      {
        "citation": "[Levin, A., Weiss, Y., Durand, F., & Freeman, W. T. Efficient marginal likelihood optimization in blind deconvolu-tion. In CVPR, 2011. 2, 3, 4, 5]"
      },
      {
        "citation": "[Zhang, H., & Wipf, D. P. Non-uniform camera shake removal using a spatially adaptive sparse penalty. In NIPS, 2013. 2, 4, 5, 8]"
      },
      {
        "citation": "[Li, Y., Kang, S. B., Joshi, N., Seitz, S. M., & Huttenlocher, D. P. Generating sharp panoramas from motion-blurred videos. In CVPR, 2010. 2]"
      }
    ],
    "author_details": [
      {
        "name": "Haichao Zhang",
        "affiliation": "Duke University, NC",
        "email": "hczhang1@gmail.com"
      },
      {
        "name": "Jianchao Yang",
        "affiliation": "Adobe Research, CA",
        "email": "jiayang@adobe.com"
      }
    ]
  },
  {
    "title": "Illumination and Reﬂectance Spectra Separation of a Hyperspectral Image Meets Low-Rank Matrix Factorization\n---AUTHOR---\nYinqiang Zheng\nImari Sato\nYoichi Sato",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Zheng_Illumination_and_Reflectance_2015_CVPR_paper.pdf",
    "id": "Zheng_Illumination_and_Reflectance_2015_CVPR_paper",
    "abstract": "This paper addresses the illumination and reflectance spectra separation (IRSS) problem of a hyperspectral image captured under general spectral illumination. The huge amount of pixels in a hypersepctral image poses tremendous challenges on computational efficiency, yet in turn offers greater color variety that might be utilized to improve separation accuracy and relax the restrictive subspace illumination assumption in existing works. We show that this IRSS problem can be modeled into a low-rank matrix factorization problem, and prove that the separation is unique up to an unknown scale under the standard low-dimensionality assumption of reflectance. We also develop a scalable algorithm for this separation task that works in the presence of model error and image noise. Experiments on both synthetic data and real images have demonstrated that our separation results are sufficiently accurate, and can benefit some important applications, such as spectra relighting and illumination swapping.",
    "topics": [
      "Illumination and Reflectance Spectra Separation (IRSS)",
      "Hyperspectral Images",
      "Low-Rank Matrix Factorization",
      "Singular Value Decomposition (SVD)",
      "Scalable Algorithms"
    ],
    "references": [
      {
        "citation": "[Akhter, I., Sheikh, Y., Khan, S., & Kanade, T. (2011). Trajectory space: A dual representation for nonrigid structure from motion. *IEEE Transactions on Pattern Analysis and Machine Intelligence*, *33*(7), 1442–1456.]"
      },
      {
        "citation": "[Candès, E., Li, X., Ma, Y., & Wright, J. (2011). Robust principal component analysis. *Journal of the ACM*, *58*(3), 1–37.]"
      },
      {
        "citation": "[Candès, E., & Recht, B. (2009). Exact matrix completion via convex optimization. *Foundations of Computational Mathematics*, *9*(6), 717–772.]"
      },
      {
        "citation": "[Bregler, C., Hertzmann, A., & Biermann, H. (2000). Recovering non-rigid 3D shape from image streams. *Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition*, 690–696.]"
      },
      {
        "citation": "[Maloney, L. (1986). Evaluation of linear models of surface spectral reflectance with small numbers of parameters. *Journal of the Optical Society of America A*, *3*(10), 1673–1683.]"
      },
      {
        "citation": "[Ebner, M. (2007). *Color constancy*. John Wiley & Sons.]"
      },
      {
        "citation": "[Gijsenij, A., & Gevers, T. (2011). Color constancy using natural image statistics and scene semantics. *IEEE Transactions on Pattern Analysis and Machine Intelligence*, *33*(4), 687–698.]"
      },
      {
        "citation": "[Ho, J., Funt, B., & Drew, M. (1990). Separating a color signal into illumination and surface reflectance components: theory and applications. *IEEE Transactions on Pattern Analysis and Machine Intelligence*, *12*(10), 966–977.]"
      },
      {
        "citation": "[Huynh, C., & Robles-Kelly, A. (2010). A solution of the dichromatic model for multispectral photometric invariance. *International Journal of Computer Vision*, *90*(1), 1–27.]"
      },
      {
        "citation": "[Ikari, A., Kawakami, R., Tan, R., & Ikeuchi, K. (2008). Separating illumination and surface spectra from multiple color signals. In *Digitally Archiving Cultural Objects*, 297–321.]"
      }
    ],
    "author_details": [
      {
        "name": "Yinqiang Zheng",
        "affiliation": "National Institute of Informatics",
        "email": "yqzheng@nii.ac.jp"
      },
      {
        "name": "Imari Sato",
        "affiliation": "National Institute of Informatics",
        "email": "imarik@nii.ac.jp"
      },
      {
        "name": "Yoichi Sato",
        "affiliation": "The University of Tokyo",
        "email": "ysato@iis.u-tokyo.ac.jp"
      }
    ]
  },
  {
    "title": "Real-time part-based visual tracking via adaptive correlation filters\n---AUTHORs---\nTing Liu\nGang Wang\nQingxiong Yang",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Liu_Real-Time_Part-Based_Visual_2015_CVPR_paper.pdf",
    "id": "Liu_Real-Time_Part-Based_Visual_2015_CVPR_paper",
    "abstract": "Robust object tracking is a challenging task in computer vision. Part-based methods are widely used to better solve the partial occlusion issue, but most cannot run in real-time due to complicated online training and updating processes. This paper proposes a novel, real-time part-based visual object tracker that utilizes multiple correlation filters to track objects. The tracker incorporates a Bayesian inference framework and a structural constraint mask to handle appearance changes and partial occlusions. The key idea is to adopt correlation filters as part classifiers to achieve fast part evaluation and develop new criteria to measure the performance of different parts, assigning proper weights to them. Experiments demonstrate the effectiveness of the proposed method.\n\n---TOPICICS---\nReal-time object tracking\nPart-based tracking\nCorrelation filters\nBayesian inference\nPartial occlusion handling",
    "topics": [],
    "references": [],
    "author_details": [
      {
        "name": "Ting Liu",
        "affiliation": "School of Electrical and Electronic Engineering, Nanyang Technological University.",
        "email": "liut0016@ntu.edu.sg"
      },
      {
        "name": "Gang Wang",
        "affiliation": "School of Electrical and Electronic Engineering, Nanyang Technological University.",
        "email": "wanggang@ntu.edu.sg"
      },
      {
        "name": "Qingxiong Yang",
        "affiliation": "Department of Computer Science, City University of Hong Kong.",
        "email": "qiyang@cityu.edu.hk"
      }
    ]
  },
  {
    "title": "Learning to Segment Moving Objects in Videos\n---AUTHOR---\nPablo Arbeláez\nKaterina Fragkiadaki\nPanna Felsen\nJitendra Malik",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Fragkiadaki_Learning_to_Segment_2015_CVPR_paper.pdf",
    "id": "Fragkiadaki_Learning_to_Segment_2015_CVPR_paper",
    "abstract": "We propose a method that segments moving objects in monocular uncalibrated videos by object proposal generation from multiple segmentations on motion boundaries and ranking with a “moving objectness” detector. In each frame, we extract motion boundaries by applying a learning based boundary detector on the magnitude of optical ﬂow. The extracted motion boundaries establish pixel afﬁnities for multiple ﬁgure-ground segmentations that generate a pool of segment proposals, which we call per frame Moving Object Proposals (MOPs). MOPs increase the object detection rate by 7% over state-of-the-art static segment proposals and demonstrate the value of motion for object detection in videos. We extend per frame MOPs and static proposals into space-time tubes using constrained segmentation on dense point trajectories. The set of proposals is ranked with a “Moving Objectness” Convolutional Neural Network Detector (MOD) trained from image and optical flow fields to detect moving objects and discard over/under segmentations and static parts of the scene. This ranking ensures good object coverage even with a very small number of proposals.\n\n---TOPICICS---\nMoving Object Segmentation\nOptical Flow\nSpatiotemporal Proposal Generation\nConvolutional Neural Networks (CNNs)\nVideo Object Detection",
    "topics": [],
    "references": [
      {
        "citation": "[Alexe, B., Deselaers, T., & Ferrari, V. What is an object? In CVPR, 2010.]"
      },
      {
        "citation": "[Krizhevsky, A., Sutskever, I., & Hinton, G. E. ImageNet classification with deep convolutional neural networks. In NIPS, 2012.]"
      },
      {
        "citation": "[Brox, T., & Malik, J. Large displacement optical ﬂow: Descriptor matching in variational motion estimation. TPAMI, 2010.]"
      },
      {
        "citation": "[LeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W., & Jackel, L. D. Backpropagation applied to handwritten zip code recognition. Neural Comput., 1(4), 1989.]"
      },
      {
        "citation": "[Dollár, P., & Zitnick, C. L. Structured forests for fast edge detection. In ICCV, 2013.]"
      },
      {
        "citation": "[Girshick, R. B., Donahue, J., Darrell, T., & Malik, J. Rich feature hierarchies for accurate object detection and semantic segmentation. In CVPR, 2014.]"
      },
      {
        "citation": "[Brox, T., & Malik, J. Object segmentation by long term analysis of point trajectories. In ECCV. 2010.]"
      },
      {
        "citation": "[Sundaram, N., Brox, T., & Keutzer, K. Dense point trajectories by GPU-accelerated large displacement optical ﬂow. In ECCV, 2010.]"
      },
      {
        "citation": "[Martin, D., Fowlkes, C., Tal, D., & Malik, J. A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In ICCV, 2001.]"
      },
      {
        "citation": "[Ochs, P., & Brox, T. Object segmentation in video: a hierarchical variational approach for turning point trajectories into dense regions. In ICCV, 2011.]"
      }
    ],
    "author_details": [
      {
        "name": "Pablo Arbeláez",
        "affiliation": "Universidad de los Andes, Colombia",
        "email": "pa.arbelaez@uniandes.edu.co"
      },
      {
        "name": "Katerina Fragkiadaki",
        "affiliation": "University of California, Berkeley",
        "email": "katef@berkeley.edu"
      },
      {
        "name": "Panna Felsen",
        "affiliation": "University of California, Berkeley",
        "email": "panna@eecs.berkeley.edu"
      },
      {
        "name": "Jitendra Malik",
        "affiliation": "University of California, Berkeley",
        "email": "malik@eecs.berkeley.edu"
      }
    ]
  },
  {
    "title": "Learning from Massive Noisy Labeled Data for Image Classiﬁcation\n---AUTHORISTS---\nTong Xiao\nTian Xia\nYi Yang\nChang Huang\nXiaogang Wang",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Xiao_Learning_From_Massive_2015_CVPR_paper.pdf",
    "id": "Xiao_Learning_From_Massive_2015_CVPR_paper",
    "abstract": "Large-scale supervised datasets are crucial to train convolutional neural networks (CNNs) for various computer vision problems. However, obtaining a massive amount of well-labeled data is usually very expensive and time-consuming. In this paper, we introduce a general framework to train CNNs with only a limited number of clean labels and millions of easily obtained noisy labels. We model the relationships between images, class labels and label noises with a probabilistic graphical model and further integrate it into an end-to-end deep learning system. To demonstrate the effectiveness of our approach, we collect a large-scale real-world clothing classification dataset with both noisy and clean labels. Experiments on this dataset indicate that our approach can better correct the noisy labels and improves the performance of trained CNNs.\n\n---TOPIICS---\nConvolutional Neural Networks (CNNs)\nNoisy Labels\nProbabilistic Graphical Models\nDeep Learning\nImage Classification",
    "topics": [],
    "references": [
      {
        "citation": "[Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). ImageNet classification with deep convolutional neural networks. In *NIPS*, 2012. 1, 3, 6]"
      },
      {
        "citation": "[Sukhbaatar, S., & Fergus, R. (2014). Learning from noisy labels with deep neural networks. *arXiv*, 1406.2080. 2, 3, 4, 6, 7, 8]"
      },
      {
        "citation": "[Barandela, R., & Gasca, E. (2000). Decontamination of training samples for supervised pattern recognition methods. In *ICAPR*, 2000. 1, 2]"
      },
      {
        "citation": "[Fr´enay, B., & Verleysen, M. (2014). Classification in the presence of label noise: a survey. *TNNLS*, *25*(5), 2014. 2]"
      },
      {
        "citation": "[Matic, N., Guyon, I., Bottou, L., Denker, J., & Vapnik, V. (1992). Computer aided cleaning of large databases for character recognition. In *IAPR*, 1992. 2]"
      },
      {
        "citation": "[Lee, D.-H. (2013). Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks. In *ICML Workshop*, 2013. 6, 8]"
      },
      {
        "citation": "[Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., & Fei-Fei, L. (2009). ImageNet: A large-scale hierarchical image database. In *CVPR*, 2009. 1]"
      },
      {
        "citation": "[Weston, J., Ratle, F., Mobahi, H., & Collobert, R. (2012). Deep learning via semi-supervised embedding. In *Neural Networks: Tricks of the Trade*, 2012. 3]"
      },
      {
        "citation": "[Simonyan, K., & Zisserman, A. (2014). Very deep convolutional networks for large-scale image recognition. *arXiv*, 1409.1556. 1]"
      },
      {
        "citation": "[Zhu, X., & Ghahramani, Z. (2002). Learning from labeled and unlabeled data with label propagation. *Technical Report CMU-CALD-02-107*, Carnegie Mellon University. 2, 6]"
      }
    ],
    "author_details": [
      {
        "name": "Tong Xiao",
        "affiliation": "The Chinese University of Hong Kong",
        "email": "Not available"
      },
      {
        "name": "Tian Xia",
        "affiliation": "Baidu Research",
        "email": "Not available"
      },
      {
        "name": "Yi Yang",
        "affiliation": "Baidu Research",
        "email": "Not available"
      },
      {
        "name": "Chang Huang",
        "affiliation": "Baidu Research",
        "email": "Not available"
      },
      {
        "name": "Xiaogang Wang",
        "affiliation": "The Chinese University of Hong Kong",
        "email": "Not available"
      }
    ]
  },
  {
    "title": "Random Tree Walk toward Instantaneous 3D Human Pose Estimation\n---AUTHOR---\nHo Yub Jung\nSoochahn Lee\nYong Seok Heo\nIl Dong Yun",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Jung_Random_Tree_Walk_2015_CVPR_paper.pdf",
    "id": "Jung_Random_Tree_Walk_2015_CVPR_paper",
    "abstract": "This paper introduces a 1000 frames per second pose estimation method on a single core CPU. A large computation gain is achieved by random walk sub-sampling. Instead of training trees for pixel-wise classification, a regression tree is trained to estimate the probability distribution to the direction toward the particular joint, relative to the current position. At test time, the direction for the random walk is randomly chosen from a set of representative directions. The new position is found by a constant step toward the direction, and the distribution for next direction is found at the new position. The continual random walk through 3D space will eventually produce an expectation of step positions, which we estimate as the joint position. A regression tree is built separately for each joint. The number of random walk steps can be assigned for each joint so that the computation time is consistent regardless of the size of body segmentation. The experiments show that even with large computation gain, the accuracy is higher or comparable to the state-of-the-art pose estimation methods.\n\n---TOPIC---\nHuman Pose Estimation\nRandom Tree Walks (RTW)\nRegression Trees\nDepth Cameras\nReal-time Algorithms",
    "topics": [],
    "references": [
      {
        "citation": "[L. Breiman. Random forest. Machine Learning, 45:5–32, 3014.] - This paper introduces the Random Forest algorithm, a key technique often used in machine learning and likely relevant to the paper's methodology."
      },
      {
        "citation": "[T. F. Cootes, M. C. Ionita, C. Lindner, and P. Sauer. Robust and accurate shape model fitting using random forest regression voting. In Computer Vision–ECCV 2012, pages 278–291. Springer, 2012.] - This reference builds on the Random Forest method and applies it to shape model fitting, suggesting a specific application of the technique."
      },
      {
        "citation": "[S. Geman and D. Geman. Stochastic relaxation, gibbs distribution, and the bayesian restoration of images. IEEE Trans. Pattern Analysis and Machine Intelligence, 6:721–741, 1984.] - This paper describes foundational concepts related to image restoration and probabilistic modeling, potentially informing the underlying principles of the paper."
      },
      {
        "citation": "[A. Agarwal and B. Triggs. Recovering 3d human pose from monocular images. IEEE Trans. Pattern Analysis and Machine Intelligence, 2006.] - This paper addresses a core problem in the field: 3D human pose recovery, which is likely a central focus of the paper."
      },
      {
        "citation": "[V. Ferrari, M. Marin-Jimenez, and A. Zisserman. Progressive search space reduction for human pose estimation. Proc. Conf. Computer Vision and Pattern Recognition, 2008.] - This paper deals with a common challenge in pose estimation: efficiently searching for the correct pose configuration."
      },
      {
        "citation": "[L. Bourdev and J. Malik. Body part detectors trained using 3d human pose annotations. Proc. Int’l Conf. Computer Vision, 2009.] - This reference highlights the use of 3D annotations for training body part detectors, a common approach in pose estimation."
      },
      {
        "citation": "[A. Baak, M. Muller, G. Bharaj, H.-P. Seidel, and C. Theobalt. A data-driven approach for real-time full body pose reconstruction from a depth camera. Proc. Int’l Conf. Computer Vision, 2011.] - This paper focuses on real-time pose reconstruction using depth cameras, a relevant technique for many applications."
      },
      {
        "citation": "[J. Gall, C. Stoll, E. de Auiar, C. Theobalt, B. Rosenhahn, and H.-P. Seidel. Motion capture using joint skeleton tracking and surface estimation. Proc. Conf. Computer Vision and Pattern Recognition, 2009.] - This paper explores motion capture techniques using skeleton tracking and surface estimation, a related area."
      },
      {
        "citation": "[V. Ganapathi, C. Plagemann, D. Koller, and S. Thrun. Real time motion capture using a single time-of-ﬂight camera. Proc. Conf. Computer Vision and Pattern Recognition, 2010.] - This paper focuses on real-time motion capture using a single time-of-flight camera, a specific hardware setup."
      },
      {
        "citation": "[D. M. Gavrila and L. Davis. 3-d model-based tracking of humans in action: a multi-view approach. Proc. Conf. Computer Vision and Pattern Recognition, 1996.] - This paper presents an early approach to 3D human tracking, providing historical context for the field."
      }
    ],
    "author_details": [
      {
        "name": "Ho Yub Jung",
        "affiliation": "Ajou University",
        "email": "jung.ho.yub@gmail.com"
      },
      {
        "name": "Soochahn Lee",
        "affiliation": "Soonchunghyang U.",
        "email": "sclsch@sch.ac.kr"
      },
      {
        "name": "Yong Seok Heo",
        "affiliation": "Hankuk U. of Foreign Studies",
        "email": "ysheo@ajou.ac.kr"
      },
      {
        "name": "Il Dong Yun",
        "affiliation": "Hankuk U. of Foreign Studies",
        "email": "yun@hufs.ac.kr"
      }
    ]
  },
  {
    "title": "Robust Multi-Image Based Blind Face Hallucinatiion\n---AUTHOR---\nYonggang Jin\nChris-Savvas Bouganis",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Jin_Robust_Multi-Image_Based_2015_CVPR_paper.pdf",
    "id": "Jin_Robust_Multi-Image_Based_2015_CVPR_paper",
    "abstract": "This paper proposes a robust multi-image based blind face hallucinaton framework to super-resolve low-resolution (LR) faces in image sequences. The proposed method first estimates both blurring kernel and transformations of multiple LR faces by robust deblurring and registration in PCA subspace using PCA prior. A patch-wise mixture of probabilistic PCA (MPPCA) prior is then incorporated for face SR. Previous work on face SR using PCA prior can be viewed as special cases and it can be shown that FS-MAP[3]/Soft-Constraint[15] reduces to one iteration step in the framework. Experimental results in both simulated and real LR sequences demonstrate very promising performance of the proposed method.\n\n---TOPICCS---\nFace Super-Resolution\nBlind Face Hallucination\nPCA Subspace\nRobust Deblurring and Registration\nMulti-Image Processing",
    "topics": [],
    "references": [
      {
        "citation": "[S. Baker and T. Kanade. Limits on super-resolution and how to break them. IEEE TPAMI, 24(9):1167–1183, 2002.] - This paper establishes fundamental limits on super-resolution and proposes methods to overcome them, a key starting point for the field."
      },
      {
        "citation": "[C. Liu and D. Sun. A Bayesian approach to adaptive video super resolution. In Proc. IEEE CVPR, pages 209–216, 2011.] - Focuses on video super-resolution using a Bayesian framework, relevant to the paper's potential application."
      },
      {
        "citation": "[M. E. Tipping and C. M. Bishop. Bayesian imge super-resolution. In NIPS, pages 1279–1286, 2003.] - Introduces a Bayesian approach to image super-resolution, a common technique in the field."
      },
      {
        "citation": "[C. Liu, H. Y. Shum, and W. T. Freeman. Face hallucinaton: Theory and practice. IJCV, 75(1):115–134, 2007.] - Directly addresses face hallucination, a core topic."
      },
      {
        "citation": "[X. Wang and X. Tang. Hallucinating face by eigentransfor-"
      },
      {
        "citation": "mation. IEEE TSMC(C), 35(3):425–434, 2005.] - Presents a specific method for face hallucination using eigen-transformations."
      },
      {
        "citation": "[Y. Hu, K.-M. Lam, G. Qiu, and T. Shen. From local pixel"
      },
      {
        "citation": "structure to global image super-resolution: A new face hal-"
      },
      {
        "citation": "lucinaton framework. IEEE TIP, 20(2):433–445, Feb. 2011.] - Proposes a framework for face super-resolution that connects local pixel structure to global image quality."
      },
      {
        "citation": "[W. Liu, D. Lin, and X. Tang. Hallucinating faces: Tensor-"
      },
      {
        "citation": "Patch super-resolution and coupled residue compensation. In"
      },
      {
        "citation": "Proc. IEEE CVPR, pages 478–484, 2005.] - Introduces a tensor-patch based approach for face hallucination."
      },
      {
        "citation": "[C. Liu and D. Sun. A Bayesian approach to adaptive video"
      },
      {
        "citation": "super resolution. In Proc. IEEE CVPR, pages 209–216,"
      },
      {
        "citation": "] - This paper presents a Bayesian approach to adaptive video super-resolution, which is relevant to the paper's potential application."
      },
      {
        "citation": "[M. E. Tipping and C. M. Bishop. Mixtures of probabilis-"
      },
      {
        "citation": "tic principal component analysers. Neural Computation,"
      },
      {
        "citation": "11(2):443–482, 1999.] - Introduces a technique (Mixtures of probabilistic principal component analysers) that could be relevant to the underlying methodology."
      },
      {
        "citation": "[S. Baker and I. Matthews. Lucas-Kanade 20 years on: A"
      },
      {
        "citation": "unifying framework. IJCV, 56(3):221 – 255, 2004.] - Provides context and a framework for understanding image registration techniques, which are often used in super-resolution."
      },
      {
        "citation": "---"
      }
    ],
    "author_details": [
      {
        "name": "Yonggang Jin",
        "affiliation": "University of Bristol",
        "email": "[Email not available in the provided text]"
      },
      {
        "name": "Chris-Savvas Bouganis",
        "affiliation": "Imperial College London",
        "email": "[Email not available in the provided text]"
      }
    ]
  },
  {
    "title": "Joint Inference of Groups, Events and Human Roles in Aerial Videos\n---AUTHORs---\nTianmin Shu\nDan Xie\nBrandon Rothrock\nSinisia Todorovic\nSong-Chun Zhu",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Shu_Joint_Inference_of_2015_CVPR_paper.pdf",
    "id": "Shu_Joint_Inference_of_2015_CVPR_paper",
    "abstract": "With the advent of drones, aerial video analysis becomes increasingly important; yet, it has received scant attention in the literature. This paper addresses a new problem of parsing low-resolution aerial videos of large spatial areas, in terms of 1) grouping, 2) recognizing events and 3) assigning roles to people engaged in events. We propose a novel framework aimed at conducting joint inference of the above tasks, as reasoning about each in isolation typically fails in our setting. Given noisy tracklets of people and detections of large objects and scene surfaces (e.g., building, grass), we use a spatiotemporal AND-OR graph to drive our joint inference, using Markov Chain Monte Carlo and dynamic programming. We also introduce a new formalism of spatiotemporal templates characterizing latent sub-events. For evaluation, we have collected and released a new aerial videos dataset using a hex-rotor flying over picnic areas rich with group events. Our results demonstrate that we successfully address above inference tasks under challenging conditions.",
    "topics": [
      "Aerial video analysis",
      "Event recognition",
      "Human role assignment",
      "Joint inference",
      "Low-resolution video"
    ],
    "references": [
      {
        "citation": "[R. Achanta, A. Shaji, K. Smith, A. Lucchi, P. Fua, and S. S¨usstrunk, Slic superpixels compared to state-of-the-art superpixel methods, IEEE TPAMI, 34(11):2274–2282, 2012]"
      },
      {
        "citation": "[S. Oh et al., A large-scale benchmark dataset for event recognition in surveillance video, CVPR, 2011]"
      },
      {
        "citation": "[R. Mehran, O. Oreifej, and M. Shah, Human identity recognition in aerial images, CVPR, 2010]"
      },
      {
        "citation": "[S. Ali, V. Reilly, and M. Shah, Motion and appearance contexts for tracking and re-acquiring targets in aerial videos, CVPR, 2007]"
      },
      {
        "citation": "[M. R. Amer, D. Xie, M. Zhao, S. Todorovic, and S.-C. Zhu, Cost-sensitive top-down/bottom-up inference for multiscale activity recognition, CVIU, 117(10):1369–1383, 2013]"
      },
      {
        "citation": "[B. Rothrock, S. Park, and S.-C. Zhu, Integrating grammar and segmentation for human pose estimation, CVPR, 2013]"
      },
      {
        "citation": "[W. Choi and S. Savarese, Understanding collective activities of people from videos, IEEE TPAMI, 36(6):1242–1257, 2014]"
      },
      {
        "citation": "[J. Porway, K. Wang, and S.-C. Zhu, A hierarchical and contextual model for aerial image parsing, IJCV, 88(2):254–283, 2010]"
      },
      {
        "citation": "[V. Ramananthan, B. Yao, and L. Fei-Fei, Social role discovery in human events, CVPR, 2013]"
      },
      {
        "citation": "[M. S. Ryoo and J. K. Aggarwal, Stochastic representation and recognition of high-level group activities, IJCV, 93(2):183–200, 2011]"
      }
    ],
    "author_details": [
      {
        "name": "Tianmin Shu",
        "affiliation": "Center for Vision, Cognition, Learning and Art, University of California, Los Angeles",
        "email": "stm512@g.ucla.edu"
      },
      {
        "name": "Dan Xie",
        "affiliation": "Center for Vision, Cognition, Learning and Art, University of California, Los Angeles",
        "email": "xiedan@g.ucla.edu"
      },
      {
        "name": "Brandon Rothrock",
        "affiliation": "Jet Propulsion Laboratory, California Institute of Technology",
        "email": "brandon.rothrock@jpl.nasa.gov"
      },
      {
        "name": "Sinisia Todorovic",
        "affiliation": "School of Electrical Engineering and Computer Science, Oregon State University",
        "email": "sinisa@onid.orst.edu"
      },
      {
        "name": "Song-Chun Zhu",
        "affiliation": "Center for Vision, Cognition, Learning and Art, University of California, Los Angeles",
        "email": "sczhu@stat.ucla.edu"
      }
    ]
  },
  {
    "title": "Automatic Construction Of Robust Sphererical Harmonic Subspaces\n---AUTHOR---\nPatrick Snape\nYannis Panagakis\nStefanos Zafeiriou",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Snape_Automatic_Construction_Of_2015_CVPR_paper.pdf",
    "id": "Snape_Automatic_Construction_Of_2015_CVPR_paper",
    "abstract": "In this paper we propose a method to automatically recover a class specific low dimensional spherical harmonic basis from a set of in-the-wild facial images. We combine existing techniques for uncalibrated photometric stereo and low rank matrix decompositions in order to robustly recover a combined model of shape and identity. We build this basis without aid from a 3D model and show how it can be combined with recent eﬃcient sparse facial feature localisation techniques to recover dense 3D facial shape. Unlike previous works in the area, our method is very eﬃcient and is an order of magnitude faster to train, taking only a few minutes to build a model with over 2000 images. Furthermore, it can be used for real-time recovery of facial shape.\n\n---TOPIC---\nSpherical Harmonics\nFacial Shape Recovery\nLow-Rank Matrix Decomposition\nPhotometric Stereo\nAutomatic Model Construction",
    "topics": [],
    "references": [],
    "author_details": [
      {
        "name": "Patrick Snape",
        "affiliation": "Imperial College London",
        "email": "p.snape@imperial.ac.uk"
      },
      {
        "name": "Yannis Panagakis",
        "affiliation": "Imperial College London",
        "email": "i.panagakis@imperial.ac.uk"
      },
      {
        "name": "Stefanos Zafeiriou",
        "affiliation": "Imperial College London",
        "email": "s.zafeiriou@imperial.ac.uk"
      }
    ]
  },
  {
    "title": "Supplementary Material for “Learning to Generate Chairs with Convolutional Neural Networks”\n---AUTHORs---\nAlexey Dosovitskiy\nJost Tobias Springenberg\nThomas Brox",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Dosovitskiy_Learning_to_Generate_2015_CVPR_supplemental.pdf",
    "id": "Dosovitskiy_Learning_to_Generate_2015_CVPR_supplemental",
    "abstract": "This paper presents supplementary experiments related to the analysis of a generative network designed to generate chairs. The analysis focuses on understanding the behavior of individual neurons and groups of neurons within the network, particularly within the fully connected (FC-1) layer. Experiments explore the effect of varying neuron activation strengths and activating multiple neurons simultaneously. Furthermore, the paper examines feature maps from different layers and analyzes the robustness of the hidden representation by selectively zeroing out neuron activations. The findings provide insights into how the network represents and generates chair variations.\n\n---TOPSICS---\nGenerative Neural Networks\nChair Generation\nNeuron Activation Analysis\nHidden Layer Feature Maps\nRobustness of Hidden Representation",
    "topics": [],
    "references": [],
    "author_details": [
      {
        "name": "Alexey Dosovitskiy",
        "email": "dosovits@cs.uni-freiburg.de"
      },
      {
        "name": "Jost Tobias Springenberg",
        "affiliation": "Department of Computer Science, University of Freiburg",
        "email": "springj@cs.uni-freiburg.de"
      },
      {
        "name": "Thomas Brox",
        "affiliation": "Department of Computer Science, University of Freiburg",
        "email": "brox@cs.uni-freiburg.de"
      }
    ]
  },
  {
    "title": "Fast and Accurate Image Upscaling with Super-Resolution Forests\n---AUTHOR---\nSamuel Schulter\nChristian Leistner\nHorst Bischof",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Schulter_Fast_and_Accurate_2015_CVPR_paper.pdf",
    "id": "Schulter_Fast_and_Accurate_2015_CVPR_paper",
    "abstract": "The paper addresses the problem of single image super-resolution (SISR), aiming to reconstruct a high-resolution image from a single low-resolution input. The authors propose a novel approach called Super-Resolution Forests (SRF), which utilizes random forests to directly map low-resolution patches to high-resolution ones. They demonstrate a connection between their method and locally linear regression, and introduce a novel regularized objective function that optimizes both the output and input spaces. The approach achieves state-of-the-art results with fast training and evaluation times.",
    "topics": [
      "Single Image Super-Resolution (SISR)",
      "Random Forests",
      "Locally Linear Regression",
      "Regularized Objective Functions",
      "Image Upscaling"
    ],
    "references": [
      {
        "citation": "[M. Aharon, M. Elad, and A. Bruckstein. K-SVD: An Algorithm for Designing Overcomplete Dictionaries for Sparse Representation. TSP, 54(11):4311–4322, 2006.]"
      },
      {
        "citation": "[Y. Amit and D. Geman. Shape Quantization and Recognition with Randomized Trees. NECO, 9(7):1545–1588, 1997.]"
      },
      {
        "citation": "[P. Arbeláez, M. Maire, C. Fowlkes, and J. Malik. Contour Detection and Hierarchical Image Segmentation. PAMI, 33(5):898–916, 2011.]"
      },
      {
        "citation": "[M. Bishop. Pattern Recognition and Machine Learning. Springer, 2007.]"
      },
      {
        "citation": "[H. Chang, D.-Y. Yeung, and Y. Xiong. Super-Resolution Through Neighbor Embedding. In CVPR, 2004.]"
      },
      {
        "citation": "[C. Dong, C. Change Loy, K. He, and X. Tang. Learning a deep convolutional network for image super-resolution. In ECCV, 2014.]"
      },
      {
        "citation": "[C. E. Duchon. Lanczos Filtering in One and Two Dimensions. JAM, 18(8):1016–1022, 1979.]"
      },
      {
        "citation": "[S. Fanello, C. Keskin, P. Kohli, J. Izadi, Shahram Shotton, A. Criminis, U. Pataccini, and T. Paek. Filter Forests for Learning Data-Dependent Convolutional Kernels. In CVPR, 2014.]"
      },
      {
        "citation": "[R. Fattál. Upsampling via Imposed Edges Statistics. TOG, 26(3):95, 2007.]"
      },
      {
        "citation": "[W. T. Freeman, T. R. Jones, and E. C. Pasztor. Example-Based Super-Resolution. CGA, 22(2):56–65, 2002.]"
      }
    ],
    "author_details": [
      {
        "name": "Samuel Schulter",
        "affiliation": "Graz University of Technology",
        "email": "schulter@icg.tugraz.at"
      },
      {
        "name": "Christian Leistner",
        "affiliation": "Microsoft Photogrammetry",
        "email": "christian.leistner@microsoft.com"
      },
      {
        "name": "Horst Bischof",
        "affiliation": "Graz University of Technology",
        "email": "bischof@icg.tugraz.at"
      }
    ]
  },
  {
    "title": "Combining Local Appearance and Holistic View: Dual-Source Deep Neural Networks for Human Pose Estimation\n---AUTHOR---\nXiaochuan Fan\n---AUTHOR---\nKang Zheng\n---AUTHOR---\nYuewei Lin\n---AUTHOR---\nSong Wang",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Fan_Combining_Local_Appearance_2015_CVPR_paper.pdf",
    "id": "Fan_Combining_Local_Appearance_2015_CVPR_paper",
    "abstract": "We propose a new learning-based method for estimating 2D human pose from a single image, using Dual-Source Deep Convolutional Neural Networks (DS-CNN). The proposed DS-CNN takes a set of image patches as input and learns the appearance of each local part by considering their holistic views in the full body. It achieves both joint detection (determining if a patch contains a joint) and joint localization (finding the joint's location). An algorithm then combines these results from all patches to estimate the human pose. Experimental results demonstrate the method's effectiveness compared to state-of-the-art approaches.\n\n---TOPIC---\nHuman Pose Estimation\nDeep Convolutional Neural Networks (CNNs)\nDual-Source Networks (DS-CNN)\nJoint Detection and Localization\nPart-based Models",
    "topics": [],
    "references": [],
    "author_details": [
      {
        "name": "Xiaochuan Fan",
        "affiliation": "Department of Computer Science & Engineering, University of South Carolina",
        "email": "fan23@email.sc.edu"
      },
      {
        "name": "Kang Zheng",
        "affiliation": "Department of Computer Science & Engineering, University of South Carolina",
        "email": "zheng37@email.sc.edu"
      },
      {
        "name": "Yuewei Lin",
        "affiliation": "Department of Computer Science & Engineering, University of South Carolina",
        "email": "lin59@email.sc.edu"
      },
      {
        "name": "Song Wang",
        "affiliation": "Department of Computer Science & Engineering, University of South Carolina",
        "email": "songwang@cec.sc.edu"
      }
    ]
  },
  {
    "title": "Image Retrieval using Scene Graphs\n---AUTHOR---\nJustin Johnson\nRanjay Krishna\nMichael Stark\nLi-Jia Li\nDavid A. Shamma\nMichael S. Bernstein\nLi Fei-Fei",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Johnson_Image_Retrieval_Using_2015_CVPR_paper.pdf",
    "id": "Johnson_Image_Retrieval_Using_2015_CVPR_paper",
    "abstract": "This paper develops a novel framework for semantic image retrieval based on the notion of a scene graph. Our scene graphs represent objects (“man”, “boat”), attributes of objects (“boat is white”) and relationships between objects (“man standing on boat”). We use these scene graphs as queries to retrieve semantically related images. To this end, we design a conditional random ﬁeld model that reasons about possible groundings of scene graphs to test images. The likelihoods of these groundings are used as ranking scores for retrieval. We introduce a novel dataset of 5,000 human-generated scene graphs grounded to images and use this dataset to evaluate our method for image retrieval. We show that our method outperforms retrieval methods that use only objects or low-level image features, and that our full model can be used to improve object localization.\n\n---TOPICCS---\nScene Graphs\nImage Retrieval\nConditional Random Fields (CRFs)\nSemantic Image Understanding\nObject Relationships & Attributes",
    "topics": [],
    "references": [],
    "author_details": [
      {
        "name": "Justin Johnson",
        "affiliation": "Stanford University",
        "email": "[Email not available]"
      },
      {
        "name": "Ranjay Krishna",
        "affiliation": "Stanford University",
        "email": "[Email not available]"
      },
      {
        "name": "Michael Stark",
        "affiliation": "Max Planck Institute for Informatics",
        "email": "[Email not available]"
      },
      {
        "name": "Li-Jia Li",
        "affiliation": "Yahoo Labs, Snapchat",
        "email": "[Email not available]"
      },
      {
        "name": "David A. Shamma",
        "affiliation": "Yahoo Labs",
        "email": "[Email not available]"
      },
      {
        "name": "Michael S. Bernstein",
        "affiliation": "Stanford University",
        "email": "[Email not available]"
      },
      {
        "name": "Li Fei-Fei",
        "affiliation": "Stanford University",
        "email": "[Email not available]"
      }
    ]
  },
  {
    "title": "Line Drawing Interpretation in a Multi-View Context (Supplementary Material)\n---AUTHOR---\nJean-Dominique FAVREAU\nFlorent LAFARGE\nAdrien Bousseau",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Favreau_Line_Drawing_Interpretation_2015_CVPR_supplemental.pdf",
    "id": "Favreau_Line_Drawing_Interpretation_2015_CVPR_supplemental",
    "abstract": "This supplementary material details the computation of M(l), a key component for line drawing interpretation in a multi-view context. Specifically, it focuses on resolving the quadratic minimization problem under linear constraints. The paper provides a detailed mathematical formulation and derivation of the optimization problem, including the expression of the cost function (ǫ) and the constraints, ultimately leading to a matrix representation suitable for numerical solution. The derivation involves rewriting the problem in terms of homogeneous coordinates and utilizing Lagrange multipliers.",
    "topics": [
      "Quadratic Minimization",
      "Linear Constraints",
      "Homogeneous Coordinates",
      "Line Drawing Interpretation",
      "Multi-View Geometry"
    ],
    "references": [],
    "author_details": [
      {
        "name": "Jean-Dominique FAVREAU",
        "affiliation": "INRIA Sophia-Antipolis, France",
        "email": "firstname.lastname@inria.fr"
      },
      {
        "name": "Florent LAFARGE",
        "affiliation": "INRIA Sophia-Antipolis, France",
        "email": "firstname.lastname@inria.fr"
      },
      {
        "name": "Adrien Bousseau",
        "affiliation": "INRIA Sophia-Antipolis, France",
        "email": "firstname.lastname@inria.fr"
      }
    ]
  },
  {
    "title": "Salient Object Detection via Bootstrap Learning\n---AUTHORISTS---\nNa Tong\nHuchuan Lu\nXiang Ruan\nMing-Hsuan Yang",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Tong_Salient_Object_Detection_2015_CVPR_supplemental.pdf",
    "id": "Tong_Salient_Object_Detection_2015_CVPR_supplemental",
    "abstract": "We present a salient object detection method based on bootstrap learning. The method utilizes RGB and CIELab features to generate salience maps, demonstrating that combining these features, even if correlated, can yield better results than using either alone. The proposed bootstrap learning algorithm leverages both weak and strong salience models, contributing to the final results, with the strong model playing a more significant role. Experiments on five datasets show that the proposed method effectively detects salient objects and backgrounds, exhibiting robustness compared to state-of-the-art approaches.\n\n---TOPICCS---\nSalient Object Detection\nBootstrap Learning\nRGB and CIELab Features\nMultiscale Integration\nWeak and Strong Salience Models",
    "topics": [],
    "references": [
      {
        "citation": "[M. Cheng, N. J. Mitra, X. Huang, P. H. S. Torr, and S. Hu. Global contrast based salient region detection. PAMI, 37(3):569–582, 2015.] - This paper is cited multiple times and focuses on a contrast-based approach, suggesting it's a core reference."
      },
      {
        "citation": "[M.-M. Cheng, J. Warrell, W.-Y. Lin, S. Zheng, V. Vineet, and N. Crook. Efficient salient region detection with soft image abstraction. In ICCV, 2013.] - Cited multiple times, indicating its relevance to the paper's methodology."
      },
      {
        "citation": "[F. Perazzi, P. Kr¨ahenb¨uhl, Y. Pritch, and A. Hornung. Saliency filters: Contrast based filtering for salient region detection. In CVPR, 2012.] - Frequently cited, highlighting the importance of contrast-based filtering."
      },
      {
        "citation": "[X. Shen and Y. Wu. A unified approach to salient object detection via low rank matrix recovery. In CVPR, 2012.] - Cited multiple times, suggesting a significant contribution."
      },
      {
        "citation": "[Y. Wei, F. Wen, W. Zhu, and J. Sun. Geodesic saliency using background priors. In ECCV, 2012.] - Cited multiple times, indicating its relevance to the paper's methodology."
      },
      {
        "citation": "[H. Jiang, J. Wang, Z. Yuan, Y. Wu, N. Zheng, and S. Li. Salient object detection: A discriminative regional feature integration approach. In CVPR, 2013.] - Cited multiple times, suggesting a significant contribution."
      },
      {
        "citation": "[Y. Li, X. Hou, C. Koch, J. Rehg, and A. Yuille. The secrets of salient object segmentation. In CVPR, 2014.] - Cited multiple times, suggesting a significant contribution."
      },
      {
        "citation": "[Q. Yan, L. Xu, J. Shi, and J. Jia. Hierarchical saliency detection. In CVPR, 2013.] - Cited multiple times, indicating its relevance to the paper's methodology."
      },
      {
        "citation": "[C. Yang, L. Zhang, H. Lu, X. Ruan, and M.-H. Yang. Saliency detection via graph-based manifold ranking. In CVPR, 2013.] - Cited multiple times, indicating its relevance to the paper's methodology."
      },
      {
        "citation": "[W. Zhu, S. Liang, Y. Wei, and J. Sun. Saliency optimization from robust background detection. In CVPR, 2014.] - Cited multiple times, indicating its relevance to the paper's methodology."
      }
    ],
    "author_details": [
      {
        "name": "Na Tong",
        "affiliation": "Dalian University of Technology",
        "email": "[Email not available]"
      },
      {
        "name": "Huchuan Lu",
        "affiliation": "Dalian University of Technology",
        "email": "[Email not available]"
      },
      {
        "name": "Xiang Ruan",
        "affiliation": "OMRON Corporation",
        "email": "[Email not available]"
      },
      {
        "name": "Ming-Hsuan Yang",
        "affiliation": "University of California at Merced",
        "email": "[Email not available]"
      }
    ]
  },
  {
    "title": "Constrained Planar Cuts - Object Partitioning for Point Clouds\n---AUTHOR---\nMarkus Schoeler\n---AUTHOR---\nJeremie Papon\n---AUTHOR---\nFlorentin Wörgötter",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Schoeler_Constrained_Planar_Cuts_2015_CVPR_paper.pdf",
    "id": "Schoeler_Constrained_Planar_Cuts_2015_CVPR_paper",
    "abstract": "Humans can easily separate unknown objects into meaningful parts, but recent segmentation methods struggle to achieve similar results without training on human-annotated data. This paper introduces a bottom-up method for segmenting 3D point clouds into functional parts that doesn't require supervision and achieves comparable results. The method utilizes local concavities as indicators for inter-part boundaries, demonstrating efficiency and generalization across object classes. It employs a locally constrained geometrical boundary model proposing greedy cuts through a local concavity graph, considering only planar cuts evaluated using a cost function that rewards cuts orthogonal to concave edges. A local clustering constraint ensures partitioning affects only relevant concave regions. The algorithm is evaluated on RGB-D camera recordings and the Princeton Segmentation Benchmark, outperforming existing bottom-up methods and achieving scores similar to data-driven approaches.",
    "topics": [
      "3D Point Cloud Segmentation",
      "Bottom-up Object Partitioning",
      "Local Concavity Analysis",
      "Planar Cut Algorithms",
      "Unsupervised Learning"
    ],
    "references": [
      {
        "citation": "[P. Arbeláez, B. Hariharan, C. Gu, S. Gupta, L. Bourdev, and J. Malik. Semantic segmentation using regions and parts. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 3378–3385, 2012.] - Referenced 1 time. Relevant to the broader field of segmentation."
      },
      {
        "citation": "[L. Bourdev and J. Malik. Poselets: Body part detectors trained using 3D human pose annotations. In ICCV, pages 1365–1372, 2009.] - Referenced 1 time. Relevant to human pose estimation and body part detection."
      },
      {
        "citation": "[X. Chen, A. Golovinskiy, and T. Funkhouser. A benchmark for 3d mesh segmentation. In ACM Transactions on Graphics (TOG), volume 28, page 73. ACM, 2009.] - Referenced 3 times. Provides a benchmark for 3D mesh segmentation, a key area of focus."
      },
      {
        "citation": "[E. Kalogerakis, A. Hertzmann, and K. Singh. Learning 3d mesh segmentation and labeling. ACM Transactions on Graphics, 29(4):1, July 2010.] - Referenced 2 times. Focuses on learning-based 3D mesh segmentation."
      },
      {
        "citation": "[S. Stein, M. Schoeler, J. Papon, and F. Wrgtter. Object partitioning using local convexity. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2014.] - Referenced 2 times. Directly related to the method's use of convexity."
      },
      {
        "citation": "[Y. Yang and D. Ramanan. Articulated pose estimation with flexible mixtures-of-parts. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1385–1392, 2011.] - Referenced 1 time. Relevant to pose estimation, a related area."
      },
      {
        "citation": "[H. Pirsiavash and D. Ramanan. Detecting activities of daily living in first-person camera views. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 2847–2854. IEEE, 2012.] - Referenced 1 time. Related to activity recognition, a potential application."
      },
      {
        "citation": "[Y. Zheng, C.-L. Tai, E. Zhang, and P. Xu. Pairwise harmonics for shape analysis. IEEE Transactions on Visualization and Computer Graphics, 19(7):1172–1184, July 2013.] - Referenced 2 times. Relevant to shape analysis."
      },
      {
        "citation": "[J. Papon, A. Abramov, M. Schoeler, and F. Wrg¨otter. Voxel cloud connectivity segmentation - supervoxels for point clouds. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 2027–2034, 2013.] - Referenced 1 time. Relevant to point cloud segmentation."
      },
      {
        "citation": "[A. G. Golovinskiy and T. Funkhouser. Randomized cuts for 3D mesh analysis. ACM Transactions on Graphics (Proc. SIGGRAPH ASIA), 27(5), Dec. 2008.] - Referenced 1 time. Relevant to 3D mesh analysis."
      }
    ],
    "author_details": [
      {
        "name": "Markus Schoeler",
        "affiliation": "Bernstein Center for Computational Neuroscience (BCCN), III Physikalishes Institut - Biophysis, Georg-August University of G¨ottingen",
        "email": "mshoeler@gwdg.de"
      },
      {
        "name": "Jeremie Papon",
        "affiliation": "Bernstein Center for Computational Neuroscience (BCCN), III Physikalishes Institut - Biophysis, Georg-August University of G¨ottingen",
        "email": "jpapon@gwdg.de"
      },
      {
        "name": "Florentin Wörgötter",
        "affiliation": "Bernstein Center for Computational Neuroscience (BCCN), III Physikalishes Institut - Biophysis, Georg-August University of G¨ottingen",
        "email": "worgott@gwdg.de"
      }
    ]
  },
  {
    "title": "Long-term Recurrent Convolutional Networks for Visual Recognition and Description\n---AUTHOR---\n[Authors not explicitly listed in provided text]",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Donahue_Long-Term_Recurrent_Convolutional_2015_CVPR_supplemental.pdf",
    "id": "Donahue_Long-Term_Recurrent_Convolutional_2015_CVPR_supplemental",
    "abstract": "This supplemental material provides additional examples and a more detailed description and analysis of our approach to activity recognition, image description, and video description. For the latter one we also provide results in the form of videos with subcaptions in the subfolder video/. We report more detailed results on activity recognition on the UCF-101 dataset, analyzing which classes the LRCN improves the most on for split-1 and reporting results on all three splits and comparing our system to other deep activity recognition models. The results demonstrate that the LRCN model, particularly when incorporating flow information, significantly improves activity recognition performance, especially in scenarios requiring long-term dynamic reasoning.\n\n---TOPICCS---\nActivity Recognition\nLong-Term Recurrent Convolutional Networks (LRCN)\nVideo Description\nUCF-101 Dataset\nFlow-based Activity Recognition",
    "topics": [],
    "references": [],
    "author_details": []
  },
  {
    "title": "Separating Objects and Clutter in Indoor Scenes\n---AUTHOR---\nS. H. Khan\nXuming He\nM. Bannamoun\nF. Sohel\nR. Togneri",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Khan_Separating_Objects_and_2015_CVPR_paper.pdf",
    "id": "Khan_Separating_Objects_and_2015_CVPR_paper",
    "abstract": "Objects’ spatial layout estimation and clutter identification are two important tasks to understand indoor scenes. We propose to solve both of these problems in a joint framework using RGBD images of indoor scenes. In contrast to recent approaches which focus on either one of these two problems, we perform ‘fine grained structure categorization’ by predicting all the major objects and simultaneously labeling the cluttered regions. A conditional random field model is proposed to incorporate a rich set of local appearance, geometric features and interactions between the scene elements. We take a structural learning approach with a loss of 3D localization to estimate the model parameters from a large annotated RGBD dataset, and a mixed integer linear programming formulation for inference. We demonstrate that our approach is able to detect cuboids and estimate cluttered regions across many different object and scene categories in the presence of occlusion, illumination and appearance variations.",
    "topics": [
      "RGBD images",
      "Indoor scene understanding",
      "Object detection (cuboids)",
      "Clutter identification",
      "Conditional Random Fields (CRFs)"
    ],
    "references": [
      {
        "citation": "[Arbeláez, P., Maire, M., et al. Contour detection and hierarchical image segmentation. TPAM, 2011.] - This paper likely provides foundational work on image segmentation, a core component of many computer vision tasks."
      },
      {
        "citation": "[Carreira, J., & Sminchisescu, C. CPMC: Automatic object segmentation using constrained parametric min-cuts. TRAMI, 2012.] - CPMC is a significant method for object segmentation, and its inclusion suggests relevance to the paper's focus."
      },
      {
        "citation": "[Felzenszwalb, P. F., Girshick, R. B., et al. Object detection with discriminatively trained part-based models. TRAMI, 2010.] - This reference likely provides a key approach to object detection, which is often a precursor to scene understanding."
      },
      {
        "citation": "[Geiger, A., Wojek, C., & Urtasun, R. Joint 3D estimation of objects and scene layout. NIPS, 2011.] - This paper addresses a related problem of 3D scene understanding, which is a likely area of interest."
      },
      {
        "citation": "[Koppula, H. S., Anand, A., et al. Semantic labeling of 3D point clouds for indoor scenes. NIPS, 2011.] - Semantic labeling of 3D point clouds is a crucial step in indoor scene understanding, making this a relevant reference."
      },
      {
        "citation": "[Hayat, M., Bennamoun, M., & An, S. Deep reconstruction models for image set classification. TRAMI, 2015.] - Given the year of the paper, a reference to deep learning methods is expected, and this one is a good example."
      },
      {
        "citation": "[Ester, M., Kriegel, H.-P., et al. A density-based algorithm for discovering clusters in large spatial databases with noise. KDD, 1996.] - DBSCAN is a widely used clustering algorithm, and its inclusion suggests the paper may use clustering techniques."
      },
      {
        "citation": "[Land, A. H., & Doig, A. G. An automatic method of solving discrete programming problems. Econometrica, 1960.] - This reference is likely relevant if the paper uses optimization techniques, particularly those related to discrete programming."
      },
      {
        "citation": "[Khan, S. H., Bennamoun, M., Sohel, F., & Togneri, R. Automatic feature learning for robust shadow detection. CVPR, 2014.] - Shadow detection is a common challenge in scene understanding, and this reference provides a relevant technique."
      },
      {
        "citation": "[Jiang, H., & Xiao, J. A linear approach to matching cuboids in rgbd images. CVPR, 2013.] - Given the focus on 3D scene understanding, a reference to cuboid matching is likely relevant."
      }
    ],
    "author_details": [
      {
        "name": "S. H. Khan",
        "affiliation": "School of CSEE UWA",
        "email": "salman.khan@uwa.edu.au"
      },
      {
        "name": "Xuming He",
        "affiliation": "NICTA and ANU",
        "email": "xuming.he@nicta.com.au"
      },
      {
        "name": "M. Bannamoun",
        "affiliation": "School of CSEE UWA",
        "email": "mohammed.bennamoun@uwa.edu.au"
      },
      {
        "name": "F. Sohel",
        "affiliation": "School of CSEE UWA",
        "email": "ferdous.sohel@uwa.edu.au"
      },
      {
        "name": "R. Togneri",
        "affiliation": "School of EECE UWA",
        "email": "roberto.togneri@uwa.edu.au"
      }
    ]
  },
  {
    "title": "Shape and Light Directions from Shading and Polarization\n---AUTHOR---\nTrung Thanh Ngo\nHajime Nagahara\nRin-ichiro Taniguchi",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Thanh_Shape_and_Light_2015_CVPR_paper.pdf",
    "id": "Thanh_Shape_and_Light_2015_CVPR_paper",
    "abstract": "The paper introduces a novel method for recovering the 3D shape of a smooth dielectric object from polarization images taken with a light source from different directions. The method integrates constraints on shading and polarization, leveraging the complementary abilities of photometric stereo and polarization-based methods. Polarization-based methods provide cues for surface orientation and refractive index, while photometric stereo disambiguates surface orientation and provides a relationship between surface normals and light directions. The proposed method recovers surface normals for both small and large zenith angles, light directions, and refractive indexes, and is evaluated through simulation and real-world experiments.\n\n---TOPICICS---\n3D Shape Reconstruction\nPolarization Imaging\nPhotometric Stereo\nRefractive Index Estimation\nLight Direction Recovery",
    "topics": [],
    "references": [],
    "author_details": [
      {
        "name": "Trung Thanh Ngo",
        "affiliation": "Faculty of Information Science and Electrical Engineering, Kyushu University",
        "email": "[Email not available in the provided text]"
      },
      {
        "name": "Hajime Nagahara",
        "affiliation": "Faculty of Information Science and Electrical Engineering, Kyushu University",
        "email": "[Email not available in the provided text]"
      },
      {
        "name": "Rin-ichiro Taniguchi",
        "affiliation": "Faculty of Information Science and Electrical Engineering, Kyushu University",
        "email": "[Email not available in the provided text]"
      }
    ]
  },
  {
    "title": "Privacy Preseving Optics for Miniature Vision Sensors\n---AUTHOR---\nFrancesco Pittaluga\n---AUTHOR---\nSanjeev J. Koppal",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Pittaluga_Privacy_Preserving_Optics_2015_CVPR_paper.pdf",
    "id": "Pittaluga_Privacy_Preserving_Optics_2015_CVPR_paper",
    "abstract": "We build novel optical designs that filter incident illumination from the scene before image capture, enabling both power efficiency and privacy preservation for vision on small devices. This allows us to attenuate sensitive information while capturing exactly the portion of the signal that is relevant to a particular vision task. We demonstrate privacy preserving optics that enable accurate depth sensing, full-body motion tracking, multiple people tracking, blob detection and face recognition. Our optical designs filter light before image capture and represent a new axis of privacy vision research that complements existing “post image capture” hardware and software based approaches to privacy preservation. We also demonstrate how to select a defocus blur that provides a certain level of privacy over a working region, and implement scale space analysis using an optical array.\n\n---TOPIC---\nPrivacy-preserving optics\nMiniature vision sensors\nOptical filtering\nComputer vision\nScale space analysis",
    "topics": [],
    "references": [],
    "author_details": [
      {
        "name": "Francesco Pittaluga",
        "affiliation": "University of Florida, Electrical and Computer Engineering Dept.",
        "email": "f.pittaluga@ufl.edu"
      },
      {
        "name": "Sanjeev J. Koppal",
        "affiliation": "University of Florida, Electrical and Computer Engineering Dept.",
        "email": "sjkoppal@ece.ufl.edu"
      }
    ]
  },
  {
    "title": "SUN RGB-D: A RGB-D Scene Understanding Benchmark Suite\n---AUTHORs---\nShuran Song\nSamuel P. Lichtenberg\nJianxiong Xiao",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Song_SUN_RGB-D_A_2015_CVPR_paper.pdf",
    "id": "Song_SUN_RGB-D_A_2015_CVPR_paper",
    "abstract": "Although RGB-D sensors have enabled major breakthroughs for several vision tasks, such as 3D reconstruction, we have not attained the same level of success in high-level scene understanding. Perhaps one of the main reasons is the lack of a large-scale benchmark with 3D annotations and 3D evaluation metrics. In this paper, we introduce an RGB-D benchmark suite for the goal of advancing the state-of-the-arts in all major scene understanding tasks. Our dataset is captured by four different sensors and contains 10,335 RGB-D images, at a similar scale as PASCAL VOC. The whole dataset is densely annotated and includes 146,617 2D polygons and 64,595 3D bounding boxes with accurate object orientations, as well as a 3D room layout and scene category for each image. This dataset enables us to train data-hungry algorithms for scene-understanding tasks, evaluate them using meaningful 3D metrics, avoid overﬁtting to a small testing set, and study cross-sensor bias.\n\n---TOPICICS---\nRGB-D Scene Understanding\n3D Annotations\nBenchmark Dataset\nScene Recognition\nCross-Sensor Bias",
    "topics": [],
    "references": [
      {
        "citation": "[A. Aldoma, F. Tombari, L. Di Stefano, and M. Vincze. A global hypotheses veriﬁcation method for 3d object recognition. In ECCV. 2012.]"
      },
      {
        "citation": "[A. Anand, H. S. Koppula, T. Joachims, and A. Saxena. Contextually guided semantic labeling and search for three-dimensional point clouds. IJRR, 2012.]"
      },
      {
        "citation": "[B. I. Barbosa, M. Cristan, A. Del Bue, L. Bazzani, and V. Murino. Re-identiﬁcation with rgb-d sensors. In First International Workshop on Re-Identiﬁcation, 2012.]"
      },
      {
        "citation": "[J. T. Barron and J. Malik. Intrinsic scene properties from a single rgb-d image. CVPR, 2013.]"
      },
      {
        "citation": "[J.-Y. Bouguet. Camera calibration toolbox for matlab. 2004.]"
      },
      {
        "citation": "[C. S. Catalin Ionescu, Fuxin Li. Latent structured models for human pose estimation. In ICCV, 2011.]"
      },
      {
        "citation": "[J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imaginet: A large-scale hierarchical image database. In CVPR, 2009.]"
      },
      {
        "citation": "[N. Erdogmus and S. Marcel. Spoofing in 2d face recognition with 3d masks and anti-spoofing with Kinect. In BTAS, 2013.]"
      },
      {
        "citation": "[M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The pascal visual object classes (VOC) challenge. IJCV, 2010.]"
      },
      {
        "citation": "[G. Fanelli, M. Dantone, J. Gall, A. Fossati, and L. Van Gool. Random forests for real time 3d face analysis. IJCV, 2013.]"
      }
    ],
    "author_details": [
      {
        "name": "Shuran Song",
        "affiliation": "Princeton University",
        "email": "Not available"
      },
      {
        "name": "Samuel P. Lichtenberg",
        "affiliation": "Princeton University",
        "email": "Not available"
      },
      {
        "name": "Jianxiong Xiao",
        "affiliation": "Princeton University",
        "email": "Not available"
      }
    ]
  },
  {
    "title": "Unknown Title",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Ahmed_An_Improved_Deep_2015_CVPR_supplemental.pdf",
    "id": "Ahmed_An_Improved_Deep_2015_CVPR_supplemental",
    "abstract": "This paper presents a supplementary material for an improved deep learning architecture for person re-identification. It details variations of the proposed architecture, specifically focusing on disparity-wise convolution. The supplementary material provides more in-depth explanations of the disparity-wise convolution architecture, including the rearrangement of cross-input neighborhood differences into disparity-wise groups and the application of convolution on each group separately. It also includes performance results on the CUHK03 dataset as a function of mini-batch iterations and qualitative results on CUHK03, CUHK01, and VIPeR datasets. Furthermore, the impact of dropout rates on validation set performance is analyzed.\n\n---TOPICCS---\nPerson Re-Identification\nDeep Learning Architecture\nDisparity-wise Convolution\nCross-Input Neighborhood Differences\nMini-batch Iterations",
    "topics": [],
    "references": [],
    "author_details": [
      {
        "name": "Ejaz Ahmed",
        "affiliation": "University of Maryland",
        "email": "ejaz@umd.edu"
      },
      {
        "name": "Michael Jones",
        "affiliation": "Mitsubishi Electric Research Labs",
        "email": "mjones@merl.com"
      },
      {
        "name": "Tim K. Marks",
        "affiliation": "Mitsubishi Electric Research Labs",
        "email": "tmarks@merl.com"
      }
    ]
  },
  {
    "title": "3D Shape Estimation from 2D Landmarks: A Convex Relaxation Approach\n---AUTHOR---\nXiaowei Zhou\nSpyridon Leonardos\nXiaoyan Hu\nKostas Daniilidis",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Zhou_3D_Shape_Estimation_2015_CVPR_paper.pdf",
    "id": "Zhou_3D_Shape_Estimation_2015_CVPR_paper",
    "abstract": "The problem of estimating the 3D shape of an object from 2D landmarks in a single image is addressed. Existing methods, which confine the unknown 3D shape within a shape space, often rely on alternating minimization schemes for joint shape and camera-pose parameter estimation, leading to sensitivity to initialization and potential entrapment in local optima. This paper proposes a convex relaxation approach to convert the problem into a spectral-norm regularized linear inverse problem, which is a convex program. The paper demonstrates the exact recovery property of the proposed method, its merits compared to alternative methods, and its applicability in human pose and car shape estimation.\n\n---TOPIC---\n3D Shape Estimation\n---TOPI---\nConvex Relaxation\n---TOPI---\nNon-Convex Optimization\n---TOPI---\nShape Space Models\n---TOPI---\nComputer Vision",
    "topics": [],
    "references": [],
    "author_details": [
      {
        "name": "Xiaowei Zhou",
        "affiliation": "University of Pennsylvania",
        "email": "xiaowz@cis.upenn.edu"
      },
      {
        "name": "Spyridon Leonardos",
        "affiliation": "University of Pennsylvania",
        "email": "spyridon@cis.upenn.edu"
      },
      {
        "name": "Xiaoyan Hu",
        "affiliation": "Beijing Normal University",
        "email": "huxy@bnu.edu.cn"
      },
      {
        "name": "Kostas Daniilidis",
        "affiliation": "University of Pennsylvania",
        "email": "kostas@cis.upenn.edu"
      }
    ]
  },
  {
    "title": "Supplement: Scene Classiﬁcation with Semantic Fisher Vectors\n---AUTHOR---\nNot specified in the provided text.",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Dixit_Scene_Classification_With_2015_CVPR_supplemental.pdf",
    "id": "Dixit_Scene_Classification_With_2015_CVPR_supplemental",
    "abstract": "The paper details a method for scene classification utilizing Semantic Fisher Vectors (SFVs). Specifically, it focuses on the direct implementation of a Fisher Information matrix for a Dirichlet Mixture Model (DMM). The approach simplifies the Fisher Information matrix calculation by leveraging assumptions about large mixture distributions, ultimately leading to a Dirichlet mixture Fisher vector (DMM FV) representation for images. The authors argue that this method provides a computationally efficient and effective way to incorporate semantic information into image classification.\n\n---TOPSICS---\nScene Classification\nSemantic Fisher Vectors\nDirichlet Mixture Models\nFisher Information Matrix\nImage Representation",
    "topics": [],
    "references": [
      {
        "citation": "[Perronnin, F., Sánchez, J., & Mensink, T. Improving the Fisher kernel for large-scale image classification. In Proceedings of the 11th European conference on Computer vision: Part IV, ECCV’10, pages 143–156, Berlin, Heidelberg, 2010. Springer-Verlag.]"
      },
      {
        "citation": "[Sánchez, J., Perronnin, F., Mensink, T., & Verbeek, J. J. Image classification with the Fisher vector: Theory and practice. International Journal of Computer Vision, 105(3):222–245, 2013.]"
      }
    ],
    "author_details": [
      {
        "name": "F. Perronnin",
        "affiliation": "Not specified in the provided text.",
        "email": "Not available"
      },
      {
        "name": "J. S´anchez",
        "affiliation": "Not specified in the provided text.",
        "email": "Not available"
      },
      {
        "name": "T. Mensink",
        "affiliation": "Not specified in the provided text.",
        "email": "Not available"
      },
      {
        "name": "J. J. Verbeek",
        "affiliation": "Not specified in the provided text.",
        "email": "Not available"
      }
    ]
  },
  {
    "title": "Large-scale and Drift-Free Surface Reconstruction Using Online Subvolume Registration\n---AUTHOR---\n[Authors not explicitly listed in the provided text]",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Fioraio_Large-Scale_and_Drift-Free_2015_CVPR_supplemental.pdf",
    "id": "Fioraio_Large-Scale_and_Drift-Free_2015_CVPR_supplemental",
    "abstract": "This document provides supplementary results for a paper on large-scale surface reconstruction using online subvolume registration. The results highlight the strengths and weaknesses of the proposed approach, focusing on key steps like subvolume creation, registration, and volume blending. Comparisons are made against moving volume approaches and Zhou and Koltun's method, demonstrating superior performance, particularly in challenging scenarios like reconstructing a room in complete darkness where other state-of-the-art methods fail. The supplementary material includes high-resolution images, detailed comparisons with and without pose optimization, and expanded views of reconstructions from various sequences.\n\n---TOPICCS---\nSurface Reconstruction\nSubvolume Registration\nDrift Error Correction\nOnline SLAM\nDark Environment Reconstruction",
    "topics": [],
    "references": [],
    "author_details": []
  },
  {
    "title": "High-Fidelity Pose and Expression Normalization for Face Recognition in the Wild\n---AUTHOR---\nXiangyu Zhu\nZhen Lei\nJunjie Yan\nDong Yi\nStan Z. Li",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Zhu_High-Fidelity_Pose_and_2015_CVPR_supplemental.pdf",
    "id": "Zhu_High-Fidelity_Pose_and_2015_CVPR_supplemental",
    "abstract": "This paper provides supplemental material for the work \"High-Fidelity Pose and Expression Normalization for Face Recognition in the Wild.\" The supplemental material demonstrates normalization results on the LFW and MultiPIE datasets, showcasing the system's ability to handle pose, expression, and illumination variations. It also includes examples of failure cases due to inaccurate landmark detection and occlusion, providing insights into the limitations of the normalization process.\n\n---TOPICAS---\nFace Recognition\nPose Normalization\nExpression Normalization\n3D Morphable Model (3DMM)\nLandmark Detection",
    "topics": [],
    "references": [],
    "author_details": [
      {
        "name": "Xiangyu Zhu",
        "affiliation": "Center for Biometrics and Security Research & National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences",
        "email": "xiangyu.zhu@nlpr.ia.ac.cn"
      },
      {
        "name": "Zhen Lei",
        "affiliation": "Center for Biometrics and Security Research & National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences",
        "email": "zlei@nlpr.ia.ac.cn"
      },
      {
        "name": "Junjie Yan",
        "affiliation": "Center for Biometrics and Security Research & National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences",
        "email": "jjyan@nlpr.ia.ac.cn"
      },
      {
        "name": "Dong Yi",
        "affiliation": "Center for Biometrics and Security Research & National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences",
        "email": "dony.yi@nlpr.ia.ac.cn"
      },
      {
        "name": "Stan Z. Li",
        "affiliation": "Center for Biometrics and Security Research & National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences",
        "email": "szli@nlpr.ia.ac.cn"
      }
    ]
  },
  {
    "title": "Landmarks-based Kernelized Subspace Alignment for Unsupervised Domain Adaptation\n---AUTHOR---\nRahaf Aljundi\nR´emi Emonet\nDamien Muselet\nMarc Sebban",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Aljundi_Landmarks-Based_Kernelized_Subspace_2015_CVPR_paper.pdf",
    "id": "Aljundi_Landmarks-Based_Kernelized_Subspace_2015_CVPR_paper",
    "abstract": "Domain adaptation (DA) has gained a lot of success in the recent years in computer vision to deal with situations where the learning process has to transfer knowledge from a source to a target domain. In this paper, we introduce a novel unsupervised DA approach based on both subspace alignment and selection of landmarks similarly distributed between the two domains. Those landmarks are selected so as to reduce the discrepancy between the domains and then are used to non linearly project the data in the same space where an efﬁcient subspace alignment (in closed-form) is performed. We carry out a large experimental comparison in visual domain adaptation showing that our new method outperforms the most recent unsupervised DA approaches.\n\n---TOPIC---\nDomain Adaptation\nSubspace Alignment\nLandmark Selection\nUnsupervised Learning\nComputer Vision",
    "topics": [],
    "references": [],
    "author_details": [
      {
        "name": "Rahaf Aljundi",
        "affiliation": "CNRS, UMR 5516, Laboratoire Hubert Curien, F-42000, Saint-´Etienne, France",
        "email": "rahaf.aljundi@gmail.com"
      },
      {
        "name": "R´emi Emonet",
        "affiliation": "CNRS, UMR 5516, Laboratoire Hubert Curien, F-42000, Saint-´Etienne, France",
        "email": "remi.emonet@univ-st-etienne.fr"
      },
      {
        "name": "Damien Muselet",
        "affiliation": "CNRS, UMR 5516, Laboratoire Hubert Curien, F-42000, Saint-´Etienne, France",
        "email": "damien.muselet@univ-st-etienne.fr"
      },
      {
        "name": "Marc Sebban",
        "affiliation": "CNRS, UMR 5516, Laboratoire Hubert Curien, F-42000, Saint-´Etienne, France",
        "email": "marc.sebban@univ-st-etienne.fr"
      }
    ]
  },
  {
    "title": "PAIGE: PAirwise Image Geometry Encoding for Improved Efﬁciency in Structure-from-Motion\n---AUTHOR---\nJohannes L. Schönberger\n---AUTHOR---\nAlexander C. Berg\n---AUTHOR---\nJan-Michael Frahm",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Schonberger_PAIGE_PAirwise_Image_2015_CVPR_paper.pdf",
    "id": "Schonberger_PAIGE_PAirwise_Image_2015_CVPR_paper",
    "abstract": "Large-scale Structure-from-Motion (SfM) systems are computationally expensive, particularly in the pairwise image matching and geometric verification stages. This paper evaluates existing techniques for reducing the cost of these stages, highlighting the trade-off between computational efficiency and sufficient image connectivity. The authors propose a novel approach, PAirwise Image Geometry Encoding (PAIGE), which efficiently identifies image pairs with scene overlap without exhaustive matching or geometric verification. PAIGE utilizes approximate feature correspondences to infer location and orientation properties, leveraging a classification strategy to prioritize matching and verification for overlapping image pairs. Experiments demonstrate that PAIGE achieves state-of-the-art performance and improves the speed of large-scale SfM pipelines.\n---TOPIC---\nStructure-from-Motion (SfM)\n---TOPI---\nPairwise Image Matching\n---TOPI---\nGeometric Verification\n---TOPI---\nPAIGE (Pairwise Image Geometry Encoding)\n---TOPI---\nComputational Efficiency",
    "topics": [],
    "references": [],
    "author_details": [
      {
        "name": "Johannes L. Schönberger",
        "affiliation": "Department of Computer Science, The University of North Carolina at Chapel Hill",
        "email": "jsch@cs.unc.edu"
      },
      {
        "name": "Alexander C. Berg",
        "affiliation": "Department of Computer Science, The University of North Carolina at Chapel Hill",
        "email": "aberg@cs.unc.edu"
      },
      {
        "name": "Jan-Michael Frahm",
        "affiliation": "Department of Computer Science, The University of North Carolina at Chapel Hill",
        "email": "jmf@cs.unc.edu"
      }
    ]
  },
  {
    "title": "Transformation of Markov Random Fields for Marginal Distribution Estimation\n---AUTHOR---\nMasaki Saito\nTakayuki Okatani",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Saito_Transformation_of_Markov_2015_CVPR_paper.pdf",
    "id": "Saito_Transformation_of_Markov_2015_CVPR_paper",
    "abstract": "Abstract not found",
    "topics": [],
    "references": [
      {
        "citation": "[Y. Boykov, O. Veksler, and R. Zabih. Fast Approximate Energy Minimization via Graph Cuts. PAMI, 23(11), 2001. 8] - A foundational paper on graph cuts for energy minimization."
      },
      {
        "citation": "[D. Koller and N. Friedman. Probablistic Graphical Models: Principles and Techniques. The MIT Press, 2009. 1] - A comprehensive textbook on probabilistic graphical models."
      },
      {
        "citation": "[R. Szeliski. Computer Vision: Algorithms and Applications. Springer, 2011. 1] - A widely used textbook covering various computer vision algorithms."
      },
      {
        "citation": "[S. Roth and M. J. Black. Fields of Experts. IJCV, 82(2), 2009. 1, 2] - Introduces the concept of Fields of Experts, a popular approach in computer vision."
      },
      {
        "citation": "[P. Kohli, L. Ladicky, and P. H. S. Torr. Robust Higher Order Potentials for Enforcing Label Consistency. IJCV, 82(3), 2009. 1, 2] - Addresses label consistency in graphical models."
      },
      {
        "citation": "[V. Kolmogorov. Convergent Tree-Reweighted Message Passing for Energy Minimization. PAMI, 28(10):1568–1583, 2006. 1] - Introduces a specific message passing algorithm for energy minimization."
      },
      {
        "citation": "[P. Kr¨ahenb¨uhl and V. Koltun. Efﬁcient Inference in Fully Connected CRFs with Gaussian Edge Potentials. In NIPS. 2011. 1, 7, 8] - Focuses on efficient inference in CRFs, a common model in computer vision."
      },
      {
        "citation": "[B. Andres, T. Beier, and J. H. Kappes. OpenGM: A C++ Library for Discrete Graphical Models. CoRR, abs/1206.0, 2012. 7, 8] - A practical resource for working with discrete graphical models."
      },
      {
        "citation": "[N. Komodakis and G. Tziritas. Approximate Labeling via Graph Cuts Based on Linear Programming. PAMI, 29(8):1436–1453, 2007. 1] - Explores graph cuts and linear programming for labeling."
      },
      {
        "citation": "[M. Saito, T. Okatani, and K. Deguchi. Discrete MRF Inference of Marginal Densities for Non-uniformly Discretized Variable Space. In CVPR, 2013. 2, 4, 7] - Deals with inference in MRFs with non-uniform discretization."
      }
    ],
    "author_details": [
      {
        "name": "Masaki Saito",
        "affiliation": "Tohoku University, Japan",
        "email": "msaito@vision.is.tohoku.ac.jp"
      },
      {
        "name": "Takayuki Okatani",
        "affiliation": "Tohoku University, Japan",
        "email": "okatani@vision.is.tohoku.ac.jp"
      }
    ]
  },
  {
    "title": "Making Better Use of Edges via Perceptual Grouping\n---AUTHORs---\nYonggang Qi\nYi-Zhe Song\nTao Xiang\nHonggang Zhang\nTimothy Hospedales\nYi Li\nJun Guo",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Qi_Making_Better_Use_2015_CVPR_paper.pdf",
    "id": "Qi_Making_Better_Use_2015_CVPR_paper",
    "abstract": "Abstract not found",
    "topics": [],
    "references": [
      {
        "citation": "[N. Adlurru, L. J. Latecki, R. Lak¨amper, T. Young, X. Bai, and A. D. Gross. Contour grouping based on local symmetry. In ICCV 2007.]"
      },
      {
        "citation": "[P. Arbeláez, M. Maire, C. Fowlkes, and J. Malik. Contour detection and hierarchical image segmentation. TPAMI 2011.]"
      },
      {
        "citation": "[P. A. Arbel´aez, J. Pont-Tuset, J. T. Barron, F. Marqu´es, and J. Malik. Multiscale combinatorial grouping. In CVPR, 2014.]"
      },
      {
        "citation": "[Y. Boykov, O. Vekler, and R. Zabih. Fast approximate energy minimization via graph cuts. TPAMI 2001.]"
      },
      {
        "citation": "[J. Canny. A computational approach to edge detection. TPA MI, 1986.]"
      },
      {
        "citation": "[M. Cheng, Z. Zhang, W. Lin, and P. H. S. Torr. BING: binarized normed gradients for objectness estimation at 300fps. In CVPR, 2014.]"
      },
      {
        "citation": "[J. Carreira and C. Sminchisescu. CPMC: automatic object segmentation using constrained parametric min-cuts. TPA MI, 2012.]"
      },
      {
        "citation": "[R. Herbrich, T. Graeppel, and K. Obermayer. Large margin rank boundaries for ordinal regression. Advances in neural information processing systems, 1999.]"
      },
      {
        "citation": "[M. Eitz, J. Hays, and M. Alexa. How do humans sketch objects? SIGGRAPH 2012.]"
      },
      {
        "citation": "[J. H. Elder and R. M. Goldberg. Ecological statistics of gestalt laws for the perceptual organization of contours. Journal of Vision.]"
      }
    ],
    "author_details": [
      {
        "name": "Yonggang Qi",
        "affiliation": "Beijing University of Posts and Telecommunications",
        "email": "qiyg@bupt.edu.cn"
      },
      {
        "name": "Yi-Zhe Song",
        "affiliation": "Queen Mary University of London",
        "email": "yizhe.song@qmul.ac.uk"
      },
      {
        "name": "Tao Xiang",
        "affiliation": "Queen Mary University of London",
        "email": "t.xiang@qmul.ac.uk"
      },
      {
        "name": "Honggang Zhang",
        "affiliation": "Beijing University of Posts and Telecommunications",
        "email": "zhhg@bupt.edu.cn"
      },
      {
        "name": "Timothy Hospedales",
        "affiliation": "Queen Mary University of London",
        "email": "t.hospedales@qmul.ac.uk"
      },
      {
        "name": "Yi Li",
        "affiliation": "Queen Mary University of London",
        "email": "yi.li@qmul.ac.uk"
      },
      {
        "name": "Jun Guo",
        "affiliation": "Beijing University of Posts and Telecommunications",
        "email": "guojun@bupt.edu.cn"
      }
    ]
  },
  {
    "title": "Blur Kernel Estimation using Normalized Color-Line Priors\n---AUTHOR---\nWei-Sheng Lai\nJian-Jiun Ding\nYen-Yu Lin\nYung-Yu Chuang",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Lai_Blur_Kernel_Estimation_2015_CVPR_supplemental.pdf",
    "id": "Lai_Blur_Kernel_Estimation_2015_CVPR_supplemental",
    "abstract": "Unfortunately, the provided text does not contain an abstract. It appears to be a results section and supplementary material from a research paper.\n\n---TOPIPS---\nBlur Kernel Estimation\nColor-Line Priors\nNon-Blind Deconvolution\nImage Restoration\nQuantitative Evaluation",
    "topics": [],
    "references": [],
    "author_details": [
      {
        "name": "Wei-Sheng Lai",
        "affiliation": "Not available in the provided text.",
        "email": "Not available in the provided text."
      },
      {
        "name": "Jian-Jiun Ding",
        "affiliation": "Not available in the provided text.",
        "email": "Not available in the provided text."
      },
      {
        "name": "Yen-Yu Lin",
        "affiliation": "Not available in the provided text.",
        "email": "Not available in the provided text."
      },
      {
        "name": "Yung-Yu Chuang",
        "affiliation": "Not available in the provided text.",
        "email": "Not available in the provided text."
      }
    ]
  },
  {
    "title": "Unconstrained Realtime Facial Performance Capture\n---AUTHOR---\nPei-Lun Hsieh\nChongyang Ma\nJihun Yu\nHao Li",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Hsieh_Unconstrained_Realtime_Facial_2015_CVPR_paper.pdf",
    "id": "Hsieh_Unconstrained_Realtime_Facial_2015_CVPR_paper",
    "abstract": "Facial performance capture is well-established in the film and game industries, but unconstrained facial performance capture, which allows for more natural and dynamic scenarios, has the potential to impact consumer AR applications and surveillance. This paper introduces a realtime facial tracking system specifically designed for performance capture in unconstrained settings using a consumer-level RGB-D sensor. The framework provides uninterrupted 3D facial tracking, even in the presence of extreme occlusions such as those caused by hair, hand-to-face gestures, and wearable accessories. It allows for instant user switching without an extra calibration step. The system explicitly segments face regions from any occluding parts by detecting outliers in the shape and appearance input using an exponentially smoothed and user-adaptive tracking model. It synthesizes plausible face textures in the occluded regions and progressively personalizes the tracking model to the current user.\n\n---TOPICICS---\nRealtime Facial Tracking\nOcclusion Handling\nUser-Adaptive Tracking\nRGB-D Sensors\nPerformance Capture",
    "topics": [],
    "references": [
      {
        "citation": "[R. Achanta, A. Shaji, K. Smith, A. Lucchi, P. Fua, and S. Ssstrunk. SLIC Superpixels Compared to State-of-the-art Superpixel Methods. TPAMI, 34(11):2274 – 2282, 2012.]"
      },
      {
        "citation": "[T. Beeler, F. Hahn, D. Bradley, B. Bickel, P. Beardsley, C. Gotsman, R. W. Sumner, and M. Gross. High-quality passive facial performance capture using anchor frames. ACM Trans. Graph., 30:75:1–75:10, 2011.]"
      },
      {
        "citation": "[C. Bregler and S. Omohundro. Surface learning with applications to lipreading. Advances in neural information processing systems, pages 43–43, 1994.]"
      },
      {
        "citation": "[X. Burgos-Artizzu, P. Perona, and P. Doll´ar. Robust face landmark estimation under occlusion. In ICCV, 2013.]"
      },
      {
        "citation": "[C. Cao, Q. Hou, and K. Zhou. Displaced dynamic expression regression for real-time facial tracking and animation. ACM Trans. Graph., 33(4):43:1–43:10, 2014.]"
      },
      {
        "citation": "[C. Cao, Y. Weng, S. Lin, and K. Zhou. 3D shape regression for real-time facial animation. ACM Trans. Graph., 32(4):41:1–41:10, 2013.]"
      },
      {
        "citation": "[C. Cao, Y. Weng, S. Zhou, Y. Tong, and K. Zhou. FaceWare-house: A 3D facial expression database for visual computing. TVCG, 20(3):413–425, 2014.]"
      },
      {
        "citation": "[J. Chai, J. Xiao, and J. Hodgins. Vision-based control of 3D facial animation. In SCA ’03, pages 193–206, 2003.]"
      },
      {
        "citation": "[E. Chuang and C. Bregler. Performance driven facial animation using blendshape interpolation. Technical report, Stanford University, 2002.]"
      },
      {
        "citation": "[T. F. Cootes, G. J. Edwards, and C. J. Taylor. Active appearance models. TPAMI, 23(6):681–685, 2001.]"
      }
    ],
    "author_details": [
      {
        "name": "Pei-Lun Hsieh",
        "affiliation": "University of Southern California",
        "email": "*Not Available*"
      },
      {
        "name": "Chongyang Ma",
        "affiliation": "University of Southern California",
        "email": "*Not Available*"
      },
      {
        "name": "Jihun Yu",
        "affiliation": "Industrial Light & Magic",
        "email": "*Not Available*"
      },
      {
        "name": "Hao Li",
        "affiliation": "University of Southern California",
        "email": "*Not Available*"
      }
    ]
  },
  {
    "title": "UniHIST: A Uniﬁed Framework for Image Restoration With Marginal Histogram Constraints\n---AUTHOR---\nXing Mei\nWeiming Dong\nBao-Gang Hu\nSiwei Lyu",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Mei_UniHIST_A_Unified_2015_CVPR_paper.pdf",
    "id": "Mei_UniHIST_A_Unified_2015_CVPR_paper",
    "abstract": "Marginal histograms provide valuable information for various computer vision problems. However, current image restoration methods do not fully exploit the potential of marginal histograms, in particular, their role as ensemble constraints on the marginal statistics of the restored image. In this paper, we introduce a new framework, UniHIST, to incorporate marginal histogram constraints into image restoration. The key idea of UniHIST is to minimize the discrepancy between the marginal histograms of the restored image and the reference histograms in pixel or gradient domains using the quadratic Wasserstein (W2) distance. The W2 distance can be computed directly from data without resorting to density estimation. It provides a differentiable metric between marginal histograms and allows easy integration with existing image restoration methods. The authors demonstrate the effectiveness of UniHIST through denoising of pattern images and non-blind deconvolution of natural images, showing enhanced restoration performance and improvements over existing methods.",
    "topics": [
      "Marginal Histograms",
      "Image Restoration",
      "Wasserstein Distance (W2)",
      "Ensemble Constraints",
      "Optimization Framework"
    ],
    "references": [
      {
        "citation": "[Banham, M. R., & Katsaggelos, A. K. (1997). Digital image restoration. IEEE Signal Process. Mag., 14(2):24–41.]"
      },
      {
        "citation": "[Bertsekas, D. P. (1996). Constrained optimization and Lagrange multiplier methods. Athena Scientiﬁc.]"
      },
      {
        "citation": "[Dabov, K., Foi, A., Katkovnik, V., & Egiazaarian, K. (2007). Image denoising by sparse 3-d transform-domain collaborative filtering. TIP, 16(8):2080–2095.]"
      },
      {
        "citation": "[Fergus, R., Singh, B., Hertzmann, A., Roweis, S. T., & Freeman, W. T. (2006). Removing camera shake from a single photograph. TOG, 25(3):787–794.]"
      },
      {
        "citation": "[Gonzalez, R. C., & Woods, R. E. (2006). Digital Image Processing (3rd Edition). Prentice-Hall, Inc.]"
      },
      {
        "citation": "[Heeger, D. J., & Bergen, J. R. (1995). Pyramid-based texture analysis/synthesis. In ACM SIGGRAPH, pages 229–238.]"
      },
      {
        "citation": "[Lowe, D. G. (2004). Distinctive image features from scale-invariant keypoints. IJCV, 60(2):91–110.]"
      },
      {
        "citation": "[Piti´e, F., Kokaram, A. C., & Dahyot, R. (2005). N-dimensional probability density function transfer and its application to color transfer. In ICCV, pages 1434–1439.]"
      },
      {
        "citation": "[Roth, S., & Black, M. (2009). Field of experts. IJCV, 82(2):205–229.]"
      },
      {
        "citation": "[Villani, C. (2009). Optimal Transport: Old and New, volume 338. Grundlehren der mathematischen Wissenschaften.]"
      }
    ],
    "author_details": [
      {
        "name": "Xing Mei",
        "affiliation": "Computer Science Department, University at Albany, SUNY",
        "email": "xmei@albany.edu"
      },
      {
        "name": "Weiming Dong",
        "affiliation": "NLPR, Institute of Automation, Chinese Academy of Sciences, Beijing, China",
        "email": "wmdong@nlpr.ia.ac.cn"
      },
      {
        "name": "Bao-Gang Hu",
        "affiliation": "NLPR, Institute of Automation, Chinese Academy of Sciences, Beijing, China",
        "email": "hubg@nlpr.ia.ac.cn"
      },
      {
        "name": "Siwei Lyu",
        "affiliation": "Computer Science Department, University at Albany, SUNY",
        "email": "slyu@albany.edu"
      }
    ]
  },
  {
    "title": "Unsupervised Visual Alignment with Similarity Graphs\n---AUTHORs---\nFatemeh Shokrollahi Yancheshmeh\nKe Chen\nJoni-Kristian Kamäräinen",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Yancheshmeh_Unsupervised_Visual_Alignment_2015_CVPR_paper.pdf",
    "id": "Yancheshmeh_Unsupervised_Visual_Alignment_2015_CVPR_paper",
    "abstract": "Alignment of semantically meaningful visual patterns is crucial for object detection and image categorization. Supervised alignment methods require significant manual annotation, making unsupervised techniques more desirable for large-scale problems. This work addresses the limitations of existing unsupervised methods, such as the need for good initialization and manual seed image selection, particularly when dealing with large viewpoint changes and visual sub-classes. The proposed approach defines visual similarity under the generalized assignment problem, constructs an image graph, and uses a novel centrality measure to automatically find a suitable seed. The method outperforms state-of-the-art techniques on benchmark datasets.\n\n---TOPICCS---\nUnsupervised Visual Alignment\nImage Similarity Graphs\nGeneralized Assignment Problem\nNon-linear Optimization\nSeed Image Selection",
    "topics": [],
    "references": [
      {
        "citation": "[H. Azizpour and I. Laptev, Object detection using strongly-supervised deformable part models, ECCV, 2012]"
      },
      {
        "citation": "[S. Bagon, O. Brostovski, M. Galun, and M. Irani, Detecting and sketching the common, CVPR, 2010]"
      },
      {
        "citation": "[A. Berg, T. Berg, and J. Malik, Shape matching and object recognition using low distortion correspondences, CVPR, 2005]"
      },
      {
        "citation": "[M. Brown and D. G. Lowe, Automatic panoramic image stitching using invariant features, Int J Comput Vis, 2007]"
      },
      {
        "citation": "[F. Brunet, V. Gay-Bellile, A. Bartoli, N. Navab, and R. Malgouyres, Feature-driven direct non-rigid image registration, Int J Comput Vis, 2011]"
      },
      {
        "citation": "[R. Cohen, L. Katzir, and D. Raz, An efﬁcient approximation for the generalized assignment problem, Information Processing Letters, 2006]"
      },
      {
        "citation": "[T. F. Cootes, C. J. Twining, V. S. Petrovi, K. O. Babalola, and C. J. Taylor, Computing accurate correspondences across groups of images, PAMI, 2010]"
      },
      {
        "citation": "[T. H. Cormen, C. E. Leiserson, R. L. Rives, and C. Stein, Introduction to Algorithms, MIT Press, 2009]"
      },
      {
        "citation": "[M. Cox, S. Sridharan, S. Lucey, and J. Cohn, Least squares congealing for large number of images, CVPR, 2009]"
      },
      {
        "citation": "[P. Felzenszwalb, D. McAllester, and D. Ramanan, A discriminatively trained, multiscale, deformable part model, CVPR, 2008]"
      }
    ],
    "author_details": [
      {
        "name": "Fatemeh Shokrollahi Yancheshmeh",
        "affiliation": "Department of Signal Processing, Tampere University of Technology, Finland",
        "email": "fatemeh.shokrollahiyancheshmeh@tut.fi"
      },
      {
        "name": "Ke Chen",
        "affiliation": "Department of Signal Processing, Tampere University of Technology, Finland",
        "email": "ke.chen@tut.fi"
      },
      {
        "name": "Joni-Kristian Kamäräinen",
        "affiliation": "Department of Signal Processing, Tampere University of Technology, Finland",
        "email": "joni.kamarainen@tut.fi"
      }
    ]
  },
  {
    "title": "3D Model-Based Continuous Emotion Recognition\n---AUTHOR---\nHui Chen\nJiangdong Li\nFengjun Zhang\nYang Li\nHongan Wang",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Chen_3D_Model-Based_Continuous_2015_CVPR_paper.pdf",
    "id": "Chen_3D_Model-Based_Continuous_2015_CVPR_paper",
    "abstract": "Visual signals are crucial for emotion recognition. Emotions in natural communications are subtle, often incomplete, and vary greatly between individuals. These characteristics, along with large head rotations, fast motions, and occlusions, pose challenges for accurate emotion estimation. To address these issues, this paper proposes a real-time 3D model-based method for continuous emotion recognition. The approach utilizes a 3D facial model for robustness, incorporates user-specific temporal features and user-independent emotion presentations, and employs a novel random forest-based framework that simultaneously performs 3D facial tracking and continuous emotion estimation. Experimental results demonstrate state-of-the-art performance with a high Pearson’s correlation coefficient.\n\n---TOPICICS---\n3D Model-Based Emotion Recognition\nContinuous Emotion Estimation\nRandom Forest Regression\nFacial Tracking\nUser-Specific Features",
    "topics": [],
    "references": [
      {
        "citation": "[Ashraf, A. B., Lucey, S., Cohn, J. F., Chen, T., Ambadar, Z., Prkachin, K. M., & Solomon, P. E. The painful face]"
      },
      {
        "citation": "[Kapoor, A., Burleson, W., & Picard, R. W. Automatic prediction of frustration. International Journal of Human-Computer Studies]"
      },
      {
        "citation": "[Baltrusaitis, T., Robinson, P., & Morency, L. P. Constrained local neural fields for robust facial landmark detection in the wild. IEEE International Conference on Computer Vision Workshops]"
      },
      {
        "citation": "[Breiman, L. Random forests. Machine learning]"
      },
      {
        "citation": "[Cao, C., Weng, Y., Lin, S., & Zhou, K. 3d shape regression for real-time facial animation. ACM Transactions on Graphics]"
      },
      {
        "citation": "[Cao, C., Weng, Y., Zhou, S., Tong, Y., & Zhou, K. Facewarehouse: a 3d facial expression database for visual computing. IEEE Transactions on Visualization and Computer Graphics]"
      },
      {
        "citation": "[Fasel, B., & Luettin, J. Automatic facial expression analysis: a survey. Pattern Recognition]"
      },
      {
        "citation": "[Gunes, H., & Pantic, M. Automatic, dimensional and continuous emotion recognition. International Journal of Synthetic Emotions]"
      },
      {
        "citation": "[Fanelli, G., Dantone, M., Gall, J., Fossati, A., & Gool, L. V. Random forests for real time 3d face analysis. International Journal of Computer Vision]"
      },
      {
        "citation": "[Gunes, H., & Schuller, B. Categorical and dimensional affect analysis in continuous input: Current trends and future directions. Image and Vision Computing]"
      }
    ],
    "author_details": [
      {
        "name": "Hui Chen",
        "affiliation": "Beijing Key Lab of Human-computer Interaction, Institute of Software, Chinese Academy of Sciences",
        "email": "[Not available in the provided text]"
      },
      {
        "name": "Jiangdong Li",
        "affiliation": "University of Chinese Academy of Sciences",
        "email": "[Not available in the provided text]"
      },
      {
        "name": "Fengjun Zhang",
        "affiliation": "State Key Lab of Computer Science, Institute of Software, Chinese Academy of Sciences",
        "email": "[Not available in the provided text]"
      },
      {
        "name": "Yang Li",
        "affiliation": "Beijing Key Lab of Human-computer Interaction, Institute of Software, Chinese Academy of Sciences",
        "email": "[Not available in the provided text]"
      },
      {
        "name": "Hongan Wang",
        "affiliation": "University of Chinese Academy of Sciences",
        "email": "[Not available in the provided text]"
      }
    ]
  },
  {
    "title": "Unifying Holistic and Parts-Based Deformable Model Fitting - Supplementary Material -\n---AUTHOR---\nJoan Alabort-i-Medina\nStefanos Zafeiriou",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Alabort-i-Medina_Unifying_Holistic_and_2015_CVPR_supplemental.pdf",
    "id": "Alabort-i-Medina_Unifying_Holistic_and_2015_CVPR_supplemental",
    "abstract": "This supplementary material extends the experimental section of the main paper with two additional experiments. The first experiment reports the fitting accuracy of five algorithms (PIC and AIC for AAMs, RLMS for CLMs, and PIC-RLMS and AIC-RLMS for the unified approach) given different amounts of training data. The second experiment reports the accuracy of the same five algorithms on the challenging problem of deformable face tracking in-the-wild. The results demonstrate the consistent accuracy and robustness of the proposed AIC-RLMS algorithm, particularly with larger training datasets and in challenging tracking scenarios.",
    "topics": [
      "Deformable face models",
      "Active appearance models (AAMs)",
      "Constrained Local Models (CLMs)",
      "Face tracking",
      "Training data impact"
    ],
    "references": [
      {
        "citation": "Belhumeur, P. N., Jacobs, D. W., Kriegman, D. J., & Kumar, N. (2011). Localizing parts of faces using a consensus of exemplars. In *Conference on Computer Vision and Pattern Recognition (CVPR)*."
      },
      {
        "citation": "Kim, M., Kumar, S., Pavlovic, V., & Rowley, H. A. (2008). Face tracking and recognition with visual constraints in real-world videos. In *Conference on Computer Vision and Pattern Recognition (CVPR)*."
      },
      {
        "citation": "Le, V., Jonathan, B., Lin, Z., Boudev, L., & Huang, T. S. (2012). Interactive facial feature localization. In *European Conference on Computer Vision (ECCV)*."
      },
      {
        "citation": "Zhu, X., & Ramanan, D. (2012). Face detection, pose estimation, and landmark localization in the wild. In *Conference on Computer Vision and Pattern Recognition (CVPR)*."
      },
      {
        "citation": "http://ibug.doc.ic.ac.uk/resources/300-W/. (Year not specified, accessed from the paper)"
      },
      {
        "citation": "Sandler, A. (Sequence used in Figure 5)"
      },
      {
        "citation": "Willis, B. (Sequence used in Figure 6)"
      },
      {
        "citation": "Stallone, S. (Sequence used in Figure 7)"
      },
      {
        "citation": "Jolie, A. (Sequences used in Figures 3 and 4)"
      },
      {
        "citation": "Rowley, H. A., Krishnan, A., & Jackowski, E. (Year not specified, likely referenced for the 300-W dataset)"
      }
    ],
    "author_details": [
      {
        "name": "Joan Alabort-i-Medina",
        "affiliation": "Department of Computing, Imperial College London, United Kingdom",
        "email": "ja310@imperial.ac.uk"
      },
      {
        "name": "Stefanos Zafeiriou",
        "affiliation": "Department of Computing, Imperial College London, United Kingdom",
        "email": "s.zafeiriou@imperial.ac.uk"
      }
    ]
  },
  {
    "title": "What do 15,000 object categories tell us about classifying and localizing actions?\n---AUTHOR---\nMihir Jain\nJan C. van Gemert\nCees G. M. Snoek",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Jain_What_do_15000_2015_CVPR_paper.pdf",
    "id": "Jain_What_do_15000_2015_CVPR_paper",
    "abstract": "This paper investigates the role of objects in automatic human action classification and localization in video. Unlike traditional approaches that focus primarily on motion, this study assesses the benefits of incorporating object categories into the video representation. The authors conduct an empirical study using 15,000 object categories across 6 datasets (totaling over 200 hours of video and 180 action classes). Key findings include demonstrating the importance of objects for actions, establishing that actions exhibit object preferences, revealing generic object-action relationships, and showing that combining object encodings with motion improves state-of-the-art performance in both action classification and localization.",
    "topics": [
      "Action Classification",
      "Action Localization",
      "Object Recognition",
      "Human Action Recognition",
      "Object-Action Relationships"
    ],
    "references": [
      {
        "citation": "[Cai, Z., Wang, L., Peng, X., & Qiao, Y. Multi-view super vector for action recognition. CVPR, 2014.]"
      },
      {
        "citation": "[Lan, T., Wang, Y., & Mori, G. Discrimiative figure-centric models for joint action localization and recognition. ICCV, 2011.]"
      },
      {
        "citation": "[Laptev, I., & Lindeberg, T. Space-time interest points. ICCV, 2003.]"
      },
      {
        "citation": "[Laptev, I., Marzalek, M., Schmid, C., & Rozenfeld, B. Learning realistic human actions from movies. CVPR, 2008.]"
      },
      {
        "citation": "[Liu, J., Kuipers, B., & Savarese, S. Recognizing human actions by attributes. CVPR, 2011.]"
      },
      {
        "citation": "[Marszalek, M., Laptev, I., & Schmid, C. Actions in context. CVPR, 2009.]"
      },
      {
        "citation": "[Oneata, D., Verbeek, J., & Schmid, C. Action and Event Recognition with Fisher Vectors on a Compact Feature Set. ICCV, 2013.]"
      },
      {
        "citation": "[Fernando, B., Gavves, E., Oramas, J., Ghodrati, A., & Tuytelaars, T. Modeling video evolution for action recognition. CVPR, 2015.]"
      },
      {
        "citation": "[Wang, H., & Schmid, C. Action Recognition with Improved Trajectories. ICCV, 2013.]"
      },
      {
        "citation": "[Jain, M., J´egou, H., & Bouthemy, P. Better exploiting motion for better action recognition. CVPR, 2013.]"
      }
    ],
    "author_details": [
      {
        "name": "Mihir Jain",
        "affiliation": "University of Amsterdam",
        "email": "[Email not available in the provided text]"
      },
      {
        "name": "Jan C. van Gemert",
        "affiliation": "University of Amsterdam",
        "email": "[Email not available in the provided text]"
      },
      {
        "name": "Cees G. M. Snoek",
        "affiliation": "University of Amsterdam, Qualcomm Research Netherlands",
        "email": "[Email not available in the provided text]"
      }
    ]
  },
  {
    "title": "A Stochastic Optimization Framework for Cryo-Electron Microscopy Density Estimation\n---AUTHOR---\nSigwo",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Brubaker_Building_Proteins_in_2015_CVPR_paper.pdf",
    "id": "Brubaker_Building_Proteins_in_2015_CVPR_paper",
    "abstract": "We introduce a framework for Cryo-EM density estimation, formulated as one of stochastic optimization to perform maximum-a-posteriori (MAP) estimation in a probabilistic model. The approach is remarkably efficient, providing useful low resolution density estimates in an hour. We show that our stochastic optimization technique is insensitive to initialization, allowing the use of random initializations. We further introduce a novel importance sampling scheme that dramatically reduces the computational costs associated with high resolution reconstruction, leading to speedups of 100,000-fold or more. The proposed framework is flexible, allowing parts of the model to be changed and improved without impacting the estimation. We demonstrate our method on two real datasets and one synthetic dataset.\n\n---TOPIC---\nCryo-EM Density Estimation\n---TOPIC---\nStochastic Optimization\n---TOPIC---\nImportance Sampling\n---TOPIC---\nProbabilistic Modeling\n---TOPIC---\nComputational Efficiency",
    "topics": [],
    "references": [],
    "author_details": [
      {
        "name": "Sigwo",
        "affiliation": "Not available in the provided text.",
        "email": "Not available in the provided text."
      }
    ]
  },
  {
    "title": "Event-Driven Stereo Matching for Real-Time 3D Panoramic Vision\n---AUTHOR---\nStephan Schraml\n---AUTHOR---\nAhmed Nabil Belbachir\n---AUTHOR---\nHorst Bischof",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Schraml_Event-Driven_Stereo_Matching_2015_CVPR_paper.pdf",
    "id": "Schraml_Event-Driven_Stereo_Matching_2015_CVPR_paper",
    "abstract": "This paper presents a stereo matching approach for a novel multi-perspective panoramic stereo vision system, making use of asynchronous and non-simultaneous stereo imaging towards real-time 3D 360◦ vision. The method is designed for events representing the scenes visual contrast as a sparse visual code allowing the stereo reconstruction of high resolution panoramic views. We propose a novel cost measure for the stereo matching, which makes use of a similarity measure based on event distributions. Thus, the robustness to variations in event occurrences was increased. An evaluation of the proposed stereo method is presented using distance estimation of panoramic stereo views and ground truth data. Furthermore, our approach is compared to standard stereo methods applied on event-data. Results show that we obtain 3D reconstructions of 1024 × 3600 round views and outperform depth reconstruction accuracy of state-of-the-art methods on event data.",
    "topics": [
      "Event-driven vision",
      "Stereo matching",
      "Panoramic vision",
      "3D reconstruction",
      "Asynchronous imaging"
    ],
    "references": [
      {
        "citation": "[Alahi, A., Ortiz, R., & Vandergheynst, P. (2012). Freak: Fast retina keypoint. IEEE Conference on Computer Vision and Pattern Recognition, 510–517.]"
      },
      {
        "citation": "[Belbachir, A., Hofstätter, M., Litzenberger, M., & Schön, P. (2011). High-speed embedded-object analysis using a dual-line timed-address-event temporal-contrast vision sensor. IEEE Transactions on Industrial Electronics, 58(3), 770–783.]"
      },
      {
        "citation": "[Belbachir, A., Schraml, S., Mayerhofer, M., & Hofstätter, M. (2014). A novel hdr depth camera for real-time 3d 360-degree panoramic vision. IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, 419–426.]"
      },
      {
        "citation": "[Geiger, A., Roser, M., & Urtasun, R. (2010). Efﬁcient large-scale stereo matching. Asian Conference on Computer Vision (ACCV).]"
      },
      {
        "citation": "[Hirschmüller, H. (2008). Stereo processing by semiglobal matching and mutual information. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 30(2), 328–341.]"
      },
      {
        "citation": "[Peleg, S., Pritch, Y., & Ben-Ezra, M. (2000). Cameras for stereo panoramic imaging. IEEE Conference on Computer Vision and Pattern Recognition, 1, 208–214.]"
      },
      {
        "citation": "[Piatkowska, E., Belbachir, A., Schraml, S., & Gelautz, M. (2012). Spatiotemporal multiple persons tracking using dynamic vision sensor. IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, 35–40.]"
      },
      {
        "citation": "[Rogister, P., Benosman, R., Sio-Hoi, I., & Lichtsteiner, P. (2012). Asynchronous event-based binocular stereo matching. IEEE Transactions on Networks and Learning Systems, 23(2).]"
      },
      {
        "citation": "[Scharstein, D., & Szeliski, R. (2002). A taxonomy and evaluation of dense two-frame stereo correspondence algorithms. International Journal of Computer Vision, 47(1), 74.]"
      },
      {
        "citation": "[Schraml, S., Belbachir, A., Milosevic, N., & Schön, P. (2010). Dynamic stereo vision system for real-time tracking. IEEE International Symposium on Circuits and Systems, 1409–1412.]"
      }
    ],
    "author_details": [
      {
        "name": "Stephan Schraml",
        "affiliation": "AIT Austrian Institute of Technology",
        "email": "{stephan.schraml}@ait.ac.at"
      },
      {
        "name": "Ahmed Nabil Belbachir",
        "affiliation": "AIT Austrian Institute of Technology",
        "email": "{nabili.belbachir}@ait.ac.at"
      },
      {
        "name": "Horst Bischof",
        "affiliation": "Graz University of Technology",
        "email": "bischof@icg.tugraz.at"
      }
    ]
  },
  {
    "title": "Similarity Learning on an Explicit Polynomial Kernel Feature Map for Person Re-Identification\n---AUTHORs---\nDapeneng Chen\nZejian Yuan\nGang Hua\nNanning Zheng\nJingdong Wang",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Chen_Similarity_Learning_on_2015_CVPR_paper.pdf",
    "id": "Chen_Similarity_Learning_on_2015_CVPR_paper",
    "abstract": "This paper addresses the person re-identification problem by learning a similarity function to suppress inter-camera variations. The approach follows the learning-to-rank methodology and maximizes the difference between the similarity scores of matched and unmatched images for the same person. The key contributions include: (1) an explicit polynomial kernel feature map for soft-patch-matching, (2) a mixture of linear similarity functions to discover various soft-patch-matching patterns, and (3) negative semi-deﬁnite regularization and sparsity constraints to avoid overfitting. Experimental results on three public benchmarks demonstrate the superiority of the proposed approach.",
    "topics": [
      "Person Re-identification",
      "Similarity Learning",
      "Polynomial Kernel Feature Map",
      "Learning to Rank",
      "Soft-Patch Matching"
    ],
    "references": [
      {
        "citation": "[S. Bak, E. Corv´ee, F. Br´emond, and M. Thonnat. Person re-identiﬁcation using spatial covariance regions of human body part-s. In International Conference on Advanced Video and Signal Based Surveillance, 2010.]"
      },
      {
        "citation": "[L. Bazzani, M. Cristani, A. Perina, and V. Murino. Multiple-shot person re-identiﬁcation by chromatic and epitomic analyses. Pattern Recognition Letters, 33(7):898–903, 2012.]"
      },
      {
        "citation": "[M. Farenzena, L. Bazzani, A. Perina, V. Murino, and M. Cristani. Person re-identiﬁcation by symmetry-driven accumulation of local features. In Computer Vision and Pattern Recognition, 2010.]"
      },
      {
        "citation": "[D. S. Cheng, M. Cristani, M. Stoppa, L. Bazzani, and V. Murino. Custom pictorial structures for re-identiﬁcation. In British Machine Vision Conference, 2011.]"
      },
      {
        "citation": "[P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and D. Ramanan. Object detection with discriminatively trained part-based models. IEEE Trans. Pattern Anal. Mach. Intell, 32(9):1627–1645, 2010.]"
      },
      {
        "citation": "[G. B. Huang, M. Ramesh, T. Berg, and E. Learned-Miller. Labeled faces in the wild: A database for studying face recognition in uncon-strained environments. Technical Report 07-49, University of Massachusetts, Amherst, October 2007.]"
      },
      {
        "citation": "[M. K¨ostinger, M. Hirzer, P. Wohlhart, P. M. Roth, and H. Bischof. Large scale metric learning from equivalence constraints. In Computer Vision and Pattern Recognition, 2012.]"
      },
      {
        "citation": "[H. J´egou and O. Chum. Negative evidences and co-occurences in image retrieval: The beneﬁt of PCA and whitening. In European Conference on Computer Vision, 2012.]"
      },
      {
        "citation": "[M. Hirzer, P. M. Roth, M. K¨ostinger, and H. Bischof. Relaxed pair-wise learned metric for person re-identiﬁcation. In European Con-ference on Computer Vision, 2012.]"
      },
      {
        "citation": "[R. Zhao, W. Ouyang, and X. Wang. Person re-identiﬁcation by salience matching. In International Conference on Computer Vision, 2013.]"
      }
    ],
    "author_details": [
      {
        "name": "Dapeng Chen",
        "affiliation": "Xi’an Jiaotong University",
        "email": "[Not available in paper]"
      },
      {
        "name": "Zejian Yuan",
        "affiliation": "Xi’an Jiaotong University",
        "email": "[Not available in paper]"
      },
      {
        "name": "Gang Hua",
        "affiliation": "Stevens Institute of Technology",
        "email": "[Not available in paper]"
      },
      {
        "name": "Nanning Zheng",
        "affiliation": "Xi’an Jiaotong University",
        "email": "[Not available in paper]"
      },
      {
        "name": "Jingdong Wang",
        "affiliation": "Microsoft Research",
        "email": "[Not available in paper]"
      }
    ]
  },
  {
    "title": "Metric imitation by manifold transfer for efﬁcient vision applications\n---AUTHOR---\nDengxin Dai\nTill Kroeger\nRadu Timofte\nLuc Van Gool",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Dai_Metric_Imitation_by_2015_CVPR_paper.pdf",
    "id": "Dai_Metric_Imitation_by_2015_CVPR_paper",
    "abstract": "This paper proposes a novel approach, Metric Imitation (MI), to learn good metrics for cheap features without human annotation. MI draws inspiration from transfer learning, taking state-of-the-art, off-the-shelf features (SFs) as source features and cheap features (TFs) as target features. MI learns good metrics for the TFs by imitating the metrics computed over the SFs, transferring view-independent property manifold structures. The method is useful when TFs are computationally efficient but SFs contain privileged information unavailable at testing time. Experiments demonstrate MI's ability to provide good metrics without data labeling and achieve state-of-the-art performance in image super-resolution.",
    "topics": [
      "Metric Learning",
      "Transfer Learning",
      "Manifold Geometry",
      "Image Super-Resolution",
      "Unsupervised Learning"
    ],
    "references": [
      {
        "citation": "[M. S. Baghshah and S. B. Shouraki, Semi-supervised metric learning using pairwise constraints, IJCAI, 2009]"
      },
      {
        "citation": "[P. N. Belhumeur, J. a. P. Hespanha, and D. J. Kriegman, Eigenfaces vs. Fisherfaces: Recognition using class specific linear projection, TPAMI, 1997]"
      },
      {
        "citation": "[M. Belkin and P. Niyogi, Laplacian eigenmaps and spectral techniques for embedding and clustering, NIPS, 2001]"
      },
      {
        "citation": "[A. Bosch, A. Zisserman, and X. Muoz, Image classification using random forests and ferns, ICCV, 2007]"
      },
      {
        "citation": "[H. Chang, D.-Y. Yeung, and Y. Xiong, Super-resolution through neighbor embedding, CVPR, 2004]"
      },
      {
        "citation": "[K. Chatﬁeld, K. Simonyan, A. Vedaldi, and A. Zisser-man, Return of the devil in the details: Delving deep into convolutional nets, BMVC, 2014]"
      },
      {
        "citation": "[M. A. A. Cox and T. F. Cox, Multidimensional scaling, Handbook of Data Visualization, 2008]"
      },
      {
        "citation": "[D. Dai, M. Prasad, C. Leistner, and L. V. Gool, Ensemble partitioning for unsupervised image categorization, ECCV, 2012]"
      },
      {
        "citation": "[D. Dai, R. Timofte, and L. Van Gool, Jointly optimized regressor for image super-resolution, Eurographics, 2015]"
      },
      {
        "citation": "[D. Dai, T. Wu, and S. C. Zhu, Discovering scene categories by information projection and cluster sampling, CVPR, 2010]"
      }
    ],
    "author_details": [
      {
        "name": "Dengxin Dai",
        "affiliation": "Computer Vision Lab, ETH Zurich",
        "email": "dai@vision.ee.ethz.ch"
      },
      {
        "name": "Till Kroeger",
        "affiliation": "Computer Vision Lab, ETH Zurich",
        "email": "kroegert@vision.ee.ethz.ch"
      },
      {
        "name": "Radu Timofte",
        "affiliation": "Computer Vision Lab, ETH Zurich",
        "email": "timofter@vision.ee.ethz.ch"
      },
      {
        "name": "Luc Van Gool",
        "affiliation": "Computer Vision Lab, ETH Zurich; VISICS, ESAT/PSI, KU Leuven",
        "email": "vangool@vision.ee.ethz.ch"
      }
    ]
  },
  {
    "title": "Efficient ConvNet-based Marker-less Motion Capture in General Scenes with a Low Number of Cameras\n---AUTHOR---\nE. de Aguiar\nA. Elhayek\nA. Jain\nJ. Tompson\nL. Pishchuin\nC. Bregler\nM. Andriluka\nB. Schiele\nC. Theobalt",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Elhayek_Efficient_ConvNet-Based_Marker-Less_2015_CVPR_paper.pdf",
    "id": "Elhayek_Efficient_ConvNet-Based_Marker-Less_2015_CVPR_paper",
    "abstract": "We present a novel method for accurate marker-less capture of articulated skeleton motion of several subjects in general scenes, indoors and outdoors, even from input filmed with as few as two cameras. Our approach unites a discriminative image-based joint detection method with a model-based generative motion tracking algorithm through a combined pose optimization energy. The discriminative part-based pose detection method, implemented using Convolutional Networks (ConvNet), estimates unary potentials for each joint of a kinematic skeleton model. These unary potentials are used to probabilistically extract pose constraints for tracking by using weighted sampling from a pose posterior guided by the model. In the final energy, these constraints are combined with an appearance-based model-to-image similarity term. Poses can be computed very efficiently using iterative local optimization, as ConvNet detection is fast, and our formulation yields a combined pose estimation energy with analytic derivatives. In combination, this enables to track full articulated joint angles at state-of-the-art accuracy and temporal stability with a very low number of cameras.",
    "topics": [
      "Marker-less Motion Capture",
      "Convolutional Networks (ConvNet)",
      "Pose Estimation",
      "Multi-View Geometry",
      "Articulated Skeleton Tracking"
    ],
    "references": [
      {
        "citation": "[Amin, S., Andriluka, M., Rohrbach, M., & Schiele, B. (2013). Multi-view pictorial structures for 3D human pose estimation. In BMVC.]"
      },
      {
        "citation": "[Andriluka, M., Pishchulin, L., Gehler, P., & Schiele, B. (2014). 2D human pose estimation: New benchmark and state of the art analysis. In IEEE CVPR.]"
      },
      {
        "citation": "[Andriluka, M., Roth, S., & Schiele, B. (2009). Pictoiral structures revisited: People detection and articulated pose estimation. In CVPR.]"
      },
      {
        "citation": "[Baak, A., M¨uller, M., Bharaj, G., Seidel, H.-P., & Theobalt, C. (2011). A data-driven approach for real-time full body pose reconstruction from a depth camera. In Proc. ICCV.]"
      },
      {
        "citation": "[Belagiannis, V., Amin, S., Andriluka, M., Schiele, B., Navab, N., & Ilic, S. (2014). 3D pictorial structures for multiple human pose estimation. CVPR, IEEE.]"
      },
      {
        "citation": "[Bo, L., & Sminchisescu, C. (2010). Twin gaussian processes for structured prediction. IJCV, 28(1):28–52.]"
      },
      {
        "citation": "[Bourdev, L., & Malik, J. (2009). Poselets: Body part detectors trained using 3d human pose annotations. In ICCV.]"
      },
      {
        "citation": "[Bray, M., Koller-Meier, E., & Gool, L. V. (2007). Smart particle filtering for high-dimensional tracking. CVIU, 106(1):116–129.]"
      },
      {
        "citation": "[Brox, T., Rosenhahn, B., Gall, J., & Cremers, D. (2010). Combined region and motion-based 3d tracking of rigid and articulated objects. TPAM, 32:402–415.]"
      },
      {
        "citation": "[Burenius, M., Sullivan, J., & Carlsson, S. (2013). 3D pictorial structures for multiple view articulated pose estimation. In CVPR.]"
      },
      {
        "citation": "---"
      }
    ],
    "author_details": [
      {
        "name": "E. de Aguiar",
        "affiliation": "MPI Informatics",
        "email": "Not available"
      },
      {
        "name": "A. Elhayek",
        "affiliation": "MPI Informatics",
        "email": "Not available"
      },
      {
        "name": "A. Jain",
        "affiliation": "MPI Informatics",
        "email": "Not available"
      },
      {
        "name": "J. Tompson",
        "affiliation": "New York University",
        "email": "Not available"
      },
      {
        "name": "L. Pishchulin",
        "affiliation": "New York University",
        "email": "Not available"
      },
      {
        "name": "C. Bregler",
        "affiliation": "Stanford University",
        "email": "Not available"
      },
      {
        "name": "M. Andriluka",
        "affiliation": "Not available (likely associated with MPI Informatics or Stanford University based on context)",
        "email": "Not available"
      },
      {
        "name": "B. Schiele",
        "affiliation": "New York University",
        "email": "Not available"
      },
      {
        "name": "C. Theobalt",
        "affiliation": "MPI Informatics",
        "email": "Not available"
      }
    ]
  },
  {
    "title": "Sense Discovery via Co-Clustering on Images and Text\n---AUTHOR---\nXinlei Chen\nAlan Ritter\nAbhinav Gupta\nTom Mitchell",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Chen_Sense_Discovery_via_2015_CVPR_paper.pdf",
    "id": "Chen_Sense_Discovery_via_2015_CVPR_paper",
    "abstract": "We present a co-clustering framework that can be used to discover multiple semantic and visual senses of a given Noun Phrase (NP). Unlike traditional clustering approaches which assume a one-to-one mapping between the clusters in the text-based feature space and the visual space, we adopt a one-to-many mapping between the two spaces. This is primarily because each semantic sense (concept) can correspond to different visual senses due to viewpoint and appearance variations. Our structure-EM style optimization not only extracts the multiple senses in both semantic and visual feature space, but also discovers the mapping between the senses. We introduce a challenging dataset (CMU Polysemy-30) for this problem consisting of 30 NPs (⇠5600 labeled instances out of ⇠22K total instances). We have also conducted a large-scale experiment that performs sense disambiguation for ⇠2000 NPs.",
    "topics": [
      "Polyseny",
      "Co-clustering",
      "Sense Discovery",
      "Noun Phrases (NPs)",
      "Image and Text Feature Spaces"
    ],
    "references": [
      {
        "citation": "[M. Banko, M. J. Cafarella, S. Soderland, M. Broadhead, and O. Etzioni. Open information extraction for the web. In IJCAI, 2007.]"
      },
      {
        "citation": "[J. Krishnamurthy and T. M. Mitchell. Which noun phrases denote which concepts? In ACL, 2011.]"
      },
      {
        "citation": "[M. Lesk. Automatic sense disambiguation using machine readable dictionaries: how to tell a pine cone from an ice cream cone. In Proceedings of the 5th annual international conference on Systems documentation, 1986.]"
      },
      {
        "citation": "[D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent dirichlet allocation. JMLR, 2003.]"
      },
      {
        "citation": "[K. Barnard and M. Johnson. Word sense disambiguation with pictures. In AI, 2005.]"
      },
      {
        "citation": "[C. D. Manning, P. Raghavan, and H. Sch¨utze. Introduction to Information Retrieval. Cambridge University Press, New York, NY, USA, 2008.]"
      },
      {
        "citation": "[X. Chen, A. Shrivastava, and A. Gupta. Neil: Extracting visual knowledge from web data. In ICCV, 2013.]"
      },
      {
        "citation": "[A. Ritter, L. Zettlemoyer, Mausam, and O. Etzioni. Modeling missing data in distant supervision for information extraction. TACL, 2013.]"
      },
      {
        "citation": "[G. A. Miller. Wordnet: a lexical database for english. Communications of the ACM, 38(11):39–41, 1995.]"
      },
      {
        "citation": "[H. Sch¨utze. Automatic word sense discrimination. Computational linguistics, 24(1):97–123, 1998.]"
      }
    ],
    "author_details": [
      {
        "name": "Xinlei Chen",
        "affiliation": "Carnegie Mellon University",
        "email": "xinleic@cs.cmu.edu"
      },
      {
        "name": "Alan Ritter",
        "affiliation": "Ohio State University",
        "email": "ritter.1492@osu.edu"
      },
      {
        "name": "Abhinav Gupta",
        "affiliation": "Carnegie Mellon University",
        "email": "abhinavg@cs.cmu.edu"
      },
      {
        "name": "Tom Mitchell",
        "affiliation": "Carnegie Mellon University",
        "email": "tom.mitchell@cs.cmu.edu"
      }
    ]
  },
  {
    "title": "Multi-Objective Convolutional Learning for Face Labeling\n---AUTHOR---\nSifei Liu\nJimei Yang\nChang Huang\nMing-Hsuan Yang",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Liu_Multi-Objective_Convolutional_Learning_2015_CVPR_paper.pdf",
    "id": "Liu_Multi-Objective_Convolutional_Learning_2015_CVPR_paper",
    "abstract": "This paper formulates face labeling as a conditional random field with unary and pairwise classifiers. We develop a novel multi-objective learning method that optimizes a single unified deep convolutional network with two distinct non-structured loss functions: one encoding the unary label likelihoods and the other encoding the pairwise label dependencies. Moreover, we regularize the network by using a nonparametric prior as new input channels in addition to the RGB image, and show that significant performance improvements can be achieved with a much smaller network size. Experiments on both the LFW and Helen datasets demonstrate state-of-the-art results of the proposed algorithm, and accurate labeling results on challenging images can be obtained by the proposed algorithm for real-world applications.",
    "topics": [
      "Face Labeling",
      "Conditional Random Fields (CRFs)",
      "Deep Convolutional Networks (CNNs)",
      "Multi-Objective Learning",
      "Nonparametric Prior Regularization"
    ],
    "references": [],
    "author_details": [
      {
        "name": "Sifei Liu",
        "affiliation": "UC Merced",
        "email": "[Email not available]"
      },
      {
        "name": "Jimei Yang",
        "affiliation": "UC Merced",
        "email": "[Email not Available]"
      },
      {
        "name": "Chang Huang",
        "affiliation": "Baidu Research",
        "email": "[Email not available]"
      },
      {
        "name": "Ming-Hsuan Yang",
        "affiliation": "UC Merced",
        "email": "[Email not available]"
      }
    ]
  },
  {
    "title": "Approximate Nearest Neighbor Fields in Video\n---AUTHOR---\nNir Ben-Zrihem\n---AUTHOR---\nLihi Zelnik-Manor",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Ben-Zrihem_Approximate_Nearest_Neighbor_2015_CVPR_paper.pdf",
    "id": "Ben-Zrihem_Approximate_Nearest_Neighbor_2015_CVPR_paper",
    "abstract": "We introduce RIANN (Ring Intersection Approximate Nearest Neighbor search), an algorithm for matching patches of a video to a set of reference patches in real-time. For each query, RIANN finds potential matches by intersecting rings around key points in appearance space. Its search complexity is inversely correlated to the amount of temporal change, making it a good fit for videos, where typically most patches change slowly with time. Experiments show that RIANN is up to two orders of magnitude faster than previous ANN methods, and is the only solution that operates in real-time. We further demonstrate how RIANN can be used for real-time video processing and provide examples for a range of real-time video applications, including colorization, denoising, and several artistic effects.",
    "topics": [
      "Approximate Nearest Neighbor Search (ANN)",
      "Real-time Video Processing",
      "Ring Intersection Search (RIANN)",
      "Patch Matching",
      "Query-Sensitive Hashing"
    ],
    "references": [],
    "author_details": [
      {
        "name": "Nir Ben-Zrihem",
        "affiliation": "Technion, Israel",
        "email": "bentzinir@gmail.com"
      },
      {
        "name": "Lihi Zelnik-Manor",
        "affiliation": "Technion, Israel",
        "email": "lihi@ee.technion.ac.il"
      }
    ]
  },
  {
    "title": "Salient Object Subitizing: Supplementary Material\n---AUTHOR---\nThe authors are not explicitly listed in this supplementary material.",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Zhang_Salient_Object_Subitizing_2015_CVPR_supplemental.pdf",
    "id": "Zhang_Salient_Object_Subitizing_2015_CVPR_supplemental",
    "abstract": "This paper presents supplementary material for a study on salient object subitizing using convolutional neural networks (CNNs). The materials include visualization results of the CNN-based subitizing classifiers, sample prediction results illustrating true positives, false positives, and false negatives, visualizations using a saliency map method, and 2D embeddings of the fc7 CNN feature before and after fine-tuning on a subitizing dataset. These visualizations aim to provide insight into the learned representations within the CNN models and to understand the impact of fine-tuning.\n\n---TOPICCS---\nConvolutional Neural Networks (CNNs)\nSubitizing\nVisualization Techniques\nFeature Embeddings (fc7)\nFine-tuning",
    "topics": [],
    "references": [
      {
        "citation": "Karpathy, A. (n.d.). t-SNE visualization of CNN. http://cs.stanford.edu/people/karpathy/cnnembed/."
      },
      {
        "citation": "Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). ImageNet classification with deep convolutional neural networks. *Advances in neural information processing systems*."
      },
      {
        "citation": "Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A. C., & Fei-Fei, L. (2014). ImageNet large scale visual recognition challenge, 2014."
      },
      {
        "citation": "Simonyan, K., Vedaldi, A., & Zisserman, A. (2014). Deep inside convolutional networks: Visualising image classification models and saliency maps. *International Conference on Learning Representations*."
      }
    ],
    "author_details": [
      {
        "name": "A. Karpathy",
        "affiliation": "Stanford University (based on reference [1])",
        "email": "Not available"
      },
      {
        "name": "A. Krizhevsky",
        "affiliation": "Not available (based on reference [2])",
        "email": "Not available"
      },
      {
        "name": "I. Sutskever",
        "affiliation": "Not available (based on reference [2])",
        "email": "Not available"
      },
      {
        "name": "G. E. Hinton",
        "affiliation": "Not available (based on reference [2])",
        "email": "Not available"
      },
      {
        "name": "O. Russakovsky",
        "affiliation": "Not available (based on reference [3])",
        "email": "Not available"
      },
      {
        "name": "J. Deng",
        "affiliation": "Not available (based on reference [3])",
        "email": "Not available"
      },
      {
        "name": "H. Su",
        "affiliation": "Not available (based on reference [3])",
        "email": "Not available"
      },
      {
        "name": "J. Krause",
        "affiliation": "Not available (based on reference [3])",
        "email": "Not available"
      },
      {
        "name": "S. Satheesh",
        "affiliation": "Not available (based on reference [3])",
        "email": "Not available"
      },
      {
        "name": "S. Ma",
        "affiliation": "Not available (based on reference [3])",
        "email": "Not available"
      },
      {
        "name": "Z. Huang",
        "affiliation": "Not available (based on reference [3])",
        "email": "Not available"
      },
      {
        "name": "A. Khosla",
        "affiliation": "Not available (based on reference [3])",
        "email": "Not available"
      },
      {
        "name": "M. Bernstein",
        "affiliation": "Not available (based on reference [3])",
        "email": "Not available"
      },
      {
        "name": "A. C. Berg",
        "affiliation": "Not available (based on reference [3])",
        "email": "Not available"
      },
      {
        "name": "L. Fei-Fei",
        "affiliation": "Not available (based on reference [3])",
        "email": "Not available"
      },
      {
        "name": "K. Simonyan",
        "affiliation": "Not available (based on reference [4])",
        "email": "Not available"
      },
      {
        "name": "A. Vedaldi",
        "affiliation": "Not available (based on reference [4])",
        "email": "Not available"
      },
      {
        "name": "A. Zisserman",
        "affiliation": "Not available (based on reference [4])",
        "email": "Not available"
      }
    ]
  },
  {
    "title": "Virtual View Networks for Object Reconstruction\n---AUTHORISTS---\nJo˜ao Carreira\nAbhishek Kar\nShubham Tulsiani\nJitendra Malik",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Carreira_Virtual_View_Networks_2015_CVPR_paper.pdf",
    "id": "Carreira_Virtual_View_Networks_2015_CVPR_paper",
    "abstract": "All that structure from motion algorithms “see” are sets of 2D points. We show that these impoverished views of the world can be faked for the purpose of reconstructing objects in challenging settings, such as from a single image, or from a few ones far apart, by recognizing the object and getting help from a collection of images of other objects from the same class. We synthesize virtual views by computing geodesics on networks connecting objects with similar viewpoints, and introduce techniques to increase the specificity and robustness of factorization-based object reconstruction in this setting. We report accurate object shape reconstruction from a single image on challenging PASCAL VOC data, which suggests that the current domain of applications of rigid structure-from-motion techniques may be significantly extended.\n\n---TOPICICS---\nStructure from Motion (SfM)\nObject Reconstruction\nVirtual View Networks (VVN)\nGeodesics\nFactorization-based methods",
    "topics": [],
    "references": [],
    "author_details": [
      {
        "name": "João Carreira",
        "affiliation": "University of California, Berkeley",
        "email": "carreira@eecs.berkeley.edu"
      },
      {
        "name": "Abhishek Kar",
        "affiliation": "University of California, Berkeley",
        "email": "akar@eecs.berkeley.edu"
      },
      {
        "name": "Shubham Tulsiani",
        "affiliation": "University of California, Berkeley",
        "email": "shubtuls@eecs.berkeley.edu"
      },
      {
        "name": "Jitendra Malik",
        "affiliation": "University of California, Berkeley",
        "email": "malik@eecs.berkeley.edu"
      }
    ]
  },
  {
    "title": "Efﬁcient Minimal-Surface Regularization of Perspective Depth Maps in Variational Stereo\n---AUTHORs---\nGottfried Graber\nJonathan Balzer\nStefano Soatto\nThomas Pock",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Graber_Efficient_Minimal-Surface_Regularization_2015_CVPR_paper.pdf",
    "id": "Graber_Efficient_Minimal-Surface_Regularization_2015_CVPR_paper",
    "abstract": "We propose a method for dense three-dimensional surface reconstruction that leverages the strengths of shape-based approaches, by imposing regularization that respects the geometry of the surface, and the strength of depth-map-based stereo, by avoiding costly computation of surface topology. The result is a near real-time variational reconstruction algorithm free of the staircasing artifacts that affect depth-map and plane-sweeping approaches. This is made possible by exploiting the gauge ambiguity to design a novel representation of the regularizer that is linear in the parameters and hence amenable to be optimized with state-of-the-art primal-dual numerical schemes.\n\n---TOPICCS---\nVariational Stereo\nSurface Reconstruction\nDepth Maps\nRegularization\nComputational Efficiency",
    "topics": [],
    "references": [
      {
        "citation": "[Bailer, C., Finckh, M., & Lensch, H. P. A. (2012). Scale Robust Multi View Stereo. European Conference on Computer Vision, 1:398–411.]"
      },
      {
        "citation": "[Chambolle, A., & Pock, T. (2011). A First-Order Primal-Dual Algorithm for Convex Problems with Applications to Imaging. Journal of Mathematical Imaging and Vision, 40(1):120–145.]"
      },
      {
        "citation": "[Gallup, D. (2014). 3D Reconstruction from Accidental Motion. IEEE Conference on Computer Vision and Pattern Recognition.]"
      },
      {
        "citation": "[Furukawa, Y., & Ponce, J. (2010). Accurate, dense, and robust multi-view stereopsis. IEEE Transactions on Pattern Analysis and Machine Intelligence, 32(8):1362–1376.]"
      },
      {
        "citation": "[Goesele, M., Curless, B., & Seitz, S. (2006). Multi-View Stereo Revisited. IEEE Conference on Computer Vision and Pattern Recognition, 2:2402–2409.]"
      },
      {
        "citation": "[Heise, P., Klose, S., Jensen, B., & Knoll, A. (2013). PM-Huber: PatchMatch with Huber Regularization for Stereo Matching. IEEE International Conference on Computer Vision, pages 2360–2367.]"
      },
      {
        "citation": "[Liu, Y., Cao, X., Dai, Q., & Xu, W. (2009). Continuous depth estimation for multi-view stereo. pages 621–628.]"
      },
      {
        "citation": "[Pock, T., & Chambolle, A. (2011). Diagonal Preconditioning for First Order Prim-dual Algorithms in Convex Optimization. In IEEE International Conference on Computer Vision, pages 1762–1769.]"
      },
      {
        "citation": "[Ranftl, R., Gehring, S., Pock, T., & Bischof, H. (2012). Pushing the limits of stereo using variational stereo estimation. In Intelligent Vehicles Symposium, pages 401–407.]"
      },
      {
        "citation": "[St¨uhmer, J., Gumhold, S., & Cremers, D. (2010). Real-Time Dense Geometry from a Handheld Camera. In Pattern Recognition (Proc. DAGM), pages 11–20.]"
      }
    ],
    "author_details": [
      {
        "name": "Gottfried Graber",
        "affiliation": "Graz University of Technology",
        "email": "graber@icg.tugraz.at"
      },
      {
        "name": "Jonathan Balzer",
        "affiliation": "UCLA, Vathos GmbH",
        "email": "jbalzer@ucla.edu"
      },
      {
        "name": "Stefano Soatto",
        "affiliation": "UCLA",
        "email": "soatto@ucla.edu"
      },
      {
        "name": "Thomas Pock",
        "affiliation": "Graz University of Technology, AIT Austrian Institute of Technology",
        "email": "pock@icg.tugraz.at"
      }
    ]
  },
  {
    "title": "Learning Coarse-to-Fine Sparselets for Efficient Object Detection and Scene Classification\n---AUTHOR---\nGong Cheng\nJunwei Han\nLei Guo\nTianming Liu",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Cheng_Learning_Coarse-to-Fine_Sparselets_2015_CVPR_paper.pdf",
    "id": "Cheng_Learning_Coarse-to-Fine_Sparselets_2015_CVPR_paper",
    "abstract": "Part model-based methods have achieved state-of-the-art results in object detection and scene classification. Inspired by sparselets, which serve as a universal set of shared basis learned from part detectors, this paper proposes a novel coarse-to-fine framework to train more effective sparselets. The approach uses an unsupervised auto-encoder to train coarse sparselets and a supervised neural network to simultaneously train fine sparselets and activation vectors. A new discriminative objective function with L0-norm sparsity constraint is introduced to explore discriminative information and achieve sparsity. The proposed framework achieves promising results on PASPascal VOC 2007, MIT Scene-67, and UC Merced Land Use datasets.",
    "topics": [
      "Object Detection",
      "Scene Classification",
      "Sparselets",
      "Coarse-to-Fine Learning",
      "Sparse Coding"
    ],
    "references": [
      {
        "citation": "[H. O. Song, S. Zickler, T. Althoff, R. Girshick, M. Fritz, C. Geyer, P. Felzenszwalb, and T. Darrell. Sparselet models for efficient multiclass object detection. In ECCV, 2012.]"
      },
      {
        "citation": "[R. Girshick, H. O. Song, and T. Darrell. Discriminatively activated sparselets. In ICML, 2013.]"
      },
      {
        "citation": "[P. F. Felzenszwalb and D. P. Huttenlocher. Pictorial structures for object recognition. IJCV, 61(1): 55-79, 2005.]"
      },
      {
        "citation": "[P. Felzenszwalb, R. B. Girshick, D. McAllester, and D. Ramanan. Object detection with discriminatively trained part-based models. TPAMI, 32(9): 1627-1645, 2010.]"
      },
      {
        "citation": "[P. F. Felzenszwalb, D. Mcallester, and D. Ramanan. A discriminatively trained, multiscale, deformable part model. In CVPR, 2008.]"
      },
      {
        "citation": "[G. E. Hinton and R. R. Salakhutdinov. Reducing the dimensionality of data with neural networks. Science, 313(5786): 504-507, 2006.]"
      },
      {
        "citation": "[P. F. Felzenszwalb, R. B. Girshick, and D. McAllester. Cascade object detection with deformable part models. In CVPR, 2010.]"
      },
      {
        "citation": "[J. Wu and J. M. Rehg. CENTRIST: A visual descriptor for scene categorization. TPAMI, 33(8): 1489-1501, 2011.]"
      },
      {
        "citation": "[P. F. Felzenszwalb, R. B. Girshick, and D. McAllester. Discriminatively trained deformable part models, release 4. http://people.cs.uchicago.edu/~pff/latent-release4/.]"
      },
      {
        "citation": "[P. F. Felzenszwalb, R. B. Girshick, and D. McAllester. Discriminatively trained deformable part models. In ECCV, 2012.]"
      }
    ],
    "author_details": [
      {
        "name": "Gong Cheng",
        "affiliation": "School of Automation, Northwestern Polytechnical University",
        "email": "gcheng@nwpu.edu.cn"
      },
      {
        "name": "Junwei Han",
        "affiliation": "School of Automation, Northwestern Polytechnical University",
        "email": "jhan@nwpu.edu.cn"
      },
      {
        "name": "Lei Guo",
        "affiliation": "School of Automation, Northwestern Polytechnical University",
        "email": "lguo@nwpu.edu.cn"
      },
      {
        "name": "Tianming Liu",
        "affiliation": "Department of Computer Science, The University of Georgia",
        "email": "tliu@cs.uga.edu"
      }
    ]
  },
  {
    "title": "Multi-Task Deep Visual-Semantic Embedding for Video Thumbnail Selection\n---AUTHOR---\nWu Liu\nTao Mei\nYongdong Zhang\nCherry Che\nJiebo Luo",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Liu_Multi-Task_Deep_Visual-Semantic_2015_CVPR_paper.pdf",
    "id": "Liu_Multi-Task_Deep_Visual-Semantic_2015_CVPR_paper",
    "abstract": "The increasing volume of online videos necessitates effective video thumbnail selection to enhance user browsing and searching experiences. Conventional methods often fail to produce satisfactory results by ignoring associated side semantic information (e.g., title, description, and query). This paper introduces a multi-task deep visual-semantic embedding model that automatically selects query-dependent video thumbnails based on both visual and side information. The model maps query and thumbnails into a common latent semantic space, enabling matching even for unseen query-thumbnail pairs. Trained on large-scale click-through data and employing a multi-task learning strategy, the approach fuses visual representativeness and query relevance scores to select final thumbnails. Evaluations on a dataset labeled by Amazon Mechanical Turk demonstrate the effectiveness of the proposed method.\n\n---TOPIC---\nVideo Thumbnail Selection\nMulti-Task Deep Learning\nVisual-Semantic Embedding\nSide Information/Semantic Information\nQuery-Dependent Retrieval",
    "topics": [],
    "references": [
      {
        "citation": "[L. Ballan, M. Bertini, A. D. Bimba, and G. Serra. Enriching and localizing semantic tags in internet videos. ACM Multimedia, 2011.] - Relevant for video understanding and semantic tagging."
      },
      {
        "citation": "[A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks. NIPS, 2012.] - A seminal work on deep convolutional neural networks, likely foundational for many later methods."
      },
      {
        "citation": "[Y. Gong, Q. Ke, M. Isard, and S. Lazebnik. A multi-view embedding space for modeling internet images, tags, and their semantics. International Journal of Computer Vision, 2014.] - Addresses the core problem of relating images, tags, and semantics."
      },
      {
        "citation": "[T. Mei, Y. Rui, S. Li, and Q. Tian. Multimedia search reranking: A literature survey. ACM Computing Surveys, 2014.] - Provides context and background on multimedia search techniques."
      },
      {
        "citation": "[T. Mikolov, I. Sutskever, K. Chen, G. Corrado, and J. Dean. Distributed representations of words and phrases and their compositionality. NIPS, 2013.] - Introduces word embeddings, a crucial component for semantic understanding."
      },
      {
        "citation": "[Y. Pan, T. Yao, T. Mei, H. Li, C.-W. Ngo, and Y. Rui. Click-through-based cross-view learning for image search. SIGIR, 2014.] - Directly relevant to the paper's use of click-through data."
      },
      {
        "citation": "[J. Wan, D. Wang, S. C. H. Hoi, P. Wu, J. Zhu, Y. Zhang, and J. Li. Deep learning for content-based image retrieval: A comprehensive study. ACM Multimedia, 2014.] - Provides a broader context for deep learning approaches in image retrieval."
      },
      {
        "citation": "[T. Zhao and E. P. Xing. Quasi real-time summarization for consumer videos. CVPR, 2014.] - Relevant to the video summarization aspect."
      },
      {
        "citation": "[M. Wang, R. Hong, G. Li, Z. Zha, S. Yan, and T. Chua. Event driven web video summarization by tag localization and key-shot identiﬁcation. IEEE Transactions on Multimedia, 2012.] - Addresses video summarization using tags."
      },
      {
        "citation": "[Y. Lee, J. Ghosh, and K. Grauman. Discovering important people and objects for egocentric video summarization. CVPR, 2012.] - Relevant to identifying key elements within videos."
      }
    ],
    "author_details": [
      {
        "name": "Wu Liu",
        "affiliation": "Institute of Computing Technology, Chinese Academy of Sciences, Beijing 100190, China; University of Chinese Academy of Sciences, Beijing 100049, China",
        "email": "liuwu@live.cn"
      },
      {
        "name": "Tao Mei",
        "affiliation": "Microsoft Research, Beijing 100080, China",
        "email": "tmei@microsoft.com"
      },
      {
        "name": "Yongdong Zhang",
        "affiliation": "Institute of Computing Technology, Chinese Academy of Sciences, Beijing 100190, China",
        "email": "zhyd@ict.ac.cn"
      },
      {
        "name": "Cherry Che",
        "affiliation": "Microsoft Research, Beijing 100080, China",
        "email": "cherc@microsoft.com"
      },
      {
        "name": "Jiebo Luo",
        "affiliation": "University of Rochester, Rochester, NY 14627, USA",
        "email": "jluo@cs.rochester.edu"
      }
    ]
  },
  {
    "title": "Weakly Supervised Object Detection with Convex Clustering\n---AUTHOR---\nHakan Bilen\nMarco Pedersoli\nTinne Tuytelaars",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Bilen_Weakly_Supervised_Object_2015_CVPR_paper.pdf",
    "id": "Bilen_Weakly_Supervised_Object_2015_CVPR_paper",
    "abstract": "Weakly supervised object detection is a challenging task where both model appearance and object location must be learned simultaneously. Classical approaches treat object location as a latent variable, leading to a non-convex optimization prone to getting stuck in local minima. This paper proposes a method to improve optimization by enforcing a \"soft\" similarity between possible object locations and a reduced set of \"exemplars\" or clusters learned with a convex formulation. This approach leverages a smooth source of information, independent of the main task, improving a strong baseline based on convolutional neural network features by more than 4 points without additional testing time.\n\n---TOPSICS---\nWeakly Supervised Object Detection\nConvex Clustering\nObject Exemplars/Clusters\nNon-Convex Optimization\nConvolutional Neural Networks (CNNs)",
    "topics": [],
    "references": [
      {
        "citation": "[R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In CVPR, pages 580–587. IEEE, 2014.] - This paper introduces Rich Feature Hierarchies, a significant advancement in object detection and semantic segmentation."
      },
      {
        "citation": "[P. Felzenszwalb, R. Girshick, D. McAllesster, and D. Ramanan. Object detection with discriminatively trained part-based models. TPAMI, 32(9):1627–1645, 2010.] - This paper presents a part-based model for object detection, a foundational technique."
      },
      {
        "citation": "[H. Bilen, M. Pedersoli, V. Namboodiri, T. Tuytelaars, and L. Van Gool. Object classification with adaptable regions. ] - This paper focuses on object classification using adaptable regions, a key aspect of object recognition."
      },
      {
        "citation": "[Z. Shi, T. M. Hospedales, and T. Xiang. Bayesian joint topic modelling for weakly supervised object localisation. In ICCV, pages 2984–2991. IEEE, 2013.] - This paper explores weakly supervised object localization using Bayesian joint topic modeling."
      },
      {
        "citation": "[H. O. Song, Y. J. Lee, S. Jegelka, and T. Darrell. Weakly-supervised discovery of visual pattern conﬁgurations. In Advances in Neural Information Processing Systems, pages 1637–1645, 2014.] - This paper addresses weakly supervised learning of visual patterns."
      },
      {
        "citation": "[M. Everingham, A. Zisserman, C. K. I. Williams, and L. Van Gool. The PASCAL Visual Object Classes Challenge 2007 (VOC2007) Results.] - This reference details the results of a significant benchmark challenge in object recognition."
      },
      {
        "citation": "[J. Uijlings, K. van de Sande, T. Gevers, and A. Smeulders. Selective search for object recognition. International journal of computer vision, 104(2):154–171, 2013.] - This paper introduces Selective Search, a widely used technique for object proposal generation."
      },
      {
        "citation": "[T. Hastie, R. Tibshirani, and J. J. H. Friedman. The elements of statistical learning, volume 1. 2001.] - A foundational text on statistical learning, providing background for many techniques used in the field."
      },
      {
        "citation": "[H. O. Song, R. Girshick, S. Jegelka, J. Mairal, Z. Harchaoui, and T. Darrell. On learning to localize objects with minimal supervision. In Proceedings of the 31st International Conference on Machine Learning (ICML-14), pages 1611–1619, 2014.] - This paper focuses on object localization with minimal supervision, a crucial aspect of weakly supervised learning."
      },
      {
        "citation": "[A. Joulin and F. R. Bach. A convex relaxation for weakly supervised classiﬁers. In Proceedings of the 29th International Conference on Machine Learning (ICML-12), pages 1279–1286, 2012.] - This paper presents a convex relaxation approach for weakly supervised classifiers."
      }
    ],
    "author_details": [
      {
        "name": "Hakan Bilen",
        "affiliation": "KU Leuven, Belgium / VGG, Dept. of Eng. Sci.",
        "email": "firstname.lastname@esat.kuleuven.be"
      },
      {
        "name": "Marco Pedersoli",
        "affiliation": "KU Leuven, Belgium",
        "email": "firstname.lastname@esat.kuleuven.be"
      },
      {
        "name": "Tinne Tuytelaars",
        "affiliation": "KU Leuven, Belgium",
        "email": "firstname.lastname@esat.kuleuven.be"
      }
    ]
  },
  {
    "title": "Watch and Learn: Semi-Supervised Learning of Object Detectors from Videos\n---AUTHOR---\nIshan Misra\nAbhinav Shrivastava\nMartial Hebert",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Misra_Watch_and_Learn_2015_CVPR_paper.pdf",
    "id": "Misra_Watch_and_Learn_2015_CVPR_paper",
    "abstract": "We present a semi-supervised approach that localizes multiple unknown object instances in long videos. We start with a handful of labeled boxes and iteratively learn and label hundreds of thousands of object instances. We propose criteria for reliable object detection and tracking for constraining the semi-supervised learning process and minimizing semantic drift. Our approach does not assume exhaustive labeling of each object instance in any single frame, or any explicit annotation of negative data. Working in such a generic setting allow us to tackle multiple object instances in video, many of which are static. The experiments demonstrate the effectiveness of our approach by evaluating the automatically labeled data on a variety of metrics like quality, coverage (re-call), diversity, and relevance to training an object detector.\n\n---TOPICICS---\nSemi-supervised learning\nObject detection\nVideo analysis\nTracking\nSparse labels",
    "topics": [],
    "references": [
      {
        "citation": "[Alexe, B., Deselaers, T., & Ferrari, V. (2010). What is an object?. In CVPR. ]"
      },
      {
        "citation": "[Avidan, S. (2005). Ensemble tracking. In CVPR.]"
      },
      {
        "citation": "[Berclaz, J., Fleuret, F., Turetken, E., & Fua, P. (2011). Multiple object tracking using k-shortest paths optimization. TPAMI.]"
      },
      {
        "citation": "[Bosch, A., Zisserman, A., & Munoz, X. (2007). Representing shape with a spatial pyramid kernel. In CVPR.]"
      },
      {
        "citation": "[Dai, Q., & Hoiem, D. (2012). Learning to localize detected objects. In CVPR.]"
      },
      {
        "citation": "[Deng, J., Dong, W., Socher, R., Li, L. jia, Li, K., & Fei-Fei, L. (2009). Imaginet: A large-scale hierarchical image database. In CVPR.]"
      },
      {
        "citation": "[Fergus, R., Weiss, Y., & Torralba, A. (2009). Semi-supervised learning in gigantic image collections. In NIPS.]"
      },
      {
        "citation": "[Felzenszwalb, P., Girshick, R., McAllester, D., & Ramanan, D. (2010). Object detection with discriminatively trained part-based models. PAMI.]"
      },
      {
        "citation": "[Girshick, R., Donahue, J., Darrell, T., & Malik, J. (2014). Rich feature hierarchies for accurate object detection and semantic segmentation. In CVPR.]"
      },
      {
        "citation": "[Hare, S., Saffari, A., & Torr, P. (2011). Struck: Structured output tracking with kernels. In ICCV.]"
      }
    ],
    "author_details": [
      {
        "name": "Ishan Misra",
        "affiliation": "Robotics Institute, Carnegie Mellon University",
        "email": "imisra@cs.cmu.edu"
      },
      {
        "name": "Abhinav Shrivastava",
        "affiliation": "Robotics Institute, Carnegie Mellon University",
        "email": "ashrivas@cs.cmu.edu"
      },
      {
        "name": "Martial Hebert",
        "affiliation": "Robotics Institute, Carnegie Mellon University",
        "email": "hebert@cs.cmu.edu"
      }
    ]
  },
  {
    "title": "Learning Multiple Visual Tasks while Discovering their Structure\n---AUTHOR---\nCarlo Ciliberto\nLorenzo Rosasco\nSilvia Villa",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Ciliberto_Learning_Multiple_Visual_2015_CVPR_paper.pdf",
    "id": "Ciliberto_Learning_Multiple_Visual_2015_CVPR_paper",
    "abstract": "Multi-task learning is a natural approach for computer vision applications requiring the simultaneous solution of several distinct but related problems. This paper proposes and studies a novel sparse, non-parametric approach exploiting the theory of Reproducing Kernel Hilbert Spaces for vector-valued functions. A suitable regularization framework is developed, formulated as a convex optimization problem, and provably solvable using an alternating minimization approach. Empirical tests show that the proposed method compares favorably to state of the art techniques and further allows to recover interpretable structures.",
    "topics": [
      "Multi-task learning",
      "Kernel methods",
      "Reproducing Kernel Hilbert Spaces",
      "Sparse regularization",
      "Task structure discovery"
    ],
    "references": [
      {
        "citation": "[Alvarez, M., Lawrence, N., & Rosasco, L. (2012). Kernels for vector-valued functions: a review. Foundations and Trends in Machine Learning, 4(3), 195–266.]"
      },
      {
        "citation": "[Argyriou, A., Evgeniou, T., & Pontil, M. (2008). Convex multi-task feature learning. Machine Learning, 73.]"
      },
      {
        "citation": "[Micchelli, C. A., & Pontil, M. (2004). Kernel methods for multi-task learning. Advances in Neural Information Processing Systems.]"
      },
      {
        "citation": "[Minh, H. Q., & Sindhwani, V. (2011). Vector-valued manifold regularization. International Conference on Machine Learning.]"
      },
      {
        "citation": "[Boyd, S. P., & Vandenberghe, L. (2004). Convex optimization. Cambridge university press.]"
      },
      {
        "citation": "[Dinuzzo, F., Ong, C. S., Gehler, P., & Pillonetto, G. (2011). Learning output kernels with block coordinate descent. International Conference on Machine Learning.]"
      },
      {
        "citation": "[Evgeniou, T., Micchelli, C. A., & Pontil, M. (2005). Learning multiple tasks with kernel methods. Journal of Machine Learning Research.]"
      },
      {
        "citation": "[Fergus, R., Bernal, H., Weiss, Y., & Torralba, A. (2010). Semantic label sharing for learning with many categories. European Conference on Computer Vision.]"
      },
      {
        "citation": "[Hwang, S. J., Sha, F., & Grauman, K. (2011). Sharing features between objects and their attributes. Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on.]"
      },
      {
        "citation": "[Jacob, L., Bach, F., & Vert, J.-P. (2008). Clustered multi-task learning: a convex formulation. Advances in Neural Information Processing Systems.]"
      }
    ],
    "author_details": [
      {
        "name": "Carlo Ciliberto",
        "affiliation": "Poggio Lab, Massachusetts Institute of Technology, Cambridge, MA, USA",
        "email": "cciliber@mit.edu"
      },
      {
        "name": "Lorenzo Rosasco",
        "affiliation": "Laboratory for Computational and Statistical Learning, Istituto Italiano di Tecnologia, Genova, Italy",
        "email": "lrosasco@mit.edu"
      },
      {
        "name": "Silvia Villa",
        "affiliation": "Laboratory for Computational and Statistical Learning, Istituto Italiano di Tecnologia, Genova, Italy",
        "email": "silvia.villa@iit.it"
      }
    ]
  },
  {
    "title": "Curriculum Learning of Multiple Tasks\n---AUTHOR---\nAnastasia Pentina\nViktoriiia Sharmanska\nChris Christoph Lampert",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Pentina_Curriculum_Learning_of_2015_CVPR_supplemental.pdf",
    "id": "Pentina_Curriculum_Learning_of_2015_CVPR_supplemental",
    "abstract": "This paper presents a generalization bound for sequential task solving within a curriculum learning setting. By applying PAC-Bayesian theory, a bound is derived that relates the difference between the expected error and the empirical error across a sequence of tasks. The bound incorporates Kullback-Leibler divergence between posterior distributions and provides insights into the impact of task order and sample sizes. The results are extended to hold uniformly for all task orders using a union bound and are further instantiated for binary classification with linear predictors.\n\n---TOPSICS---\nCurriculum Learning\nPAC-Bayesian Theory\nGeneralization Bounds\nSequential Task Solving\nKullback-Leibler Divergence",
    "topics": [],
    "references": [
      {
        "citation": "Catoni, O. (2007). PAC-Bayesian Supervised Classiﬁcation (The Thermodynamics of Statistical Learning). Institute of Mathematical Statistics."
      },
      {
        "citation": "Germain, P., Lacasse, A., Laviolette, F., & Marchand, M. (2009). PAC-Bayesian learning of linear classiﬁers. In *International Conference on Machine Learning (ICML)*."
      },
      {
        "citation": "Herbrich, R., & Graeppel, T. (2001). A PAC-Bayesian margin bound for linear classiﬁers: Why SVMs work. In *Conference on Neural Information Processing Systems (NIPS)*."
      },
      {
        "citation": "Hoeffding, W. (1963). Probability inequalities for sums of bounded random variables. *Journal of the American Statistical Association*."
      },
      {
        "citation": "Kovashka, A., Parikh, D., & Grauman, K. (2012). Whittlesearch: Image Search with Relative Attribute Feedback. In *Computer Vision and Pattern Recognition (CVPR)*."
      }
    ],
    "author_details": [
      {
        "name": "Anastasia Pentina",
        "affiliation": "IST Austria",
        "email": "apenitina@ist.ac.at"
      },
      {
        "name": "Viktoriiia Sharmanska",
        "affiliation": "IST Austria",
        "email": "vsharman@ist.ac.at"
      },
      {
        "name": "Chris Christoph Lampert",
        "affiliation": "IST Austria",
        "email": "chl@ist.ac.at"
      }
    ]
  },
  {
    "title": "Ego-Surfing First Person Videos\n---AUTHOR---\nRyo Yonetani\nKris M. Kitani\nYoichi Sato",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Yonetani_Ego-Surfing_First-Person_Videos_2015_CVPR_paper.pdf",
    "id": "Yonetani_Ego-Surfing_First-Person_Videos_2015_CVPR_paper",
    "abstract": "We envision a future where wearable cameras record first-person point-of-view (POV) videos of everyday life. While these cameras offer potential for assistive technologies and research, they also raise privacy concerns due to the passive recording of individuals without consent. To address this, we develop a self-search technique tailored to first-person POV videos. Our key observation is that the egocentric head motions of a target person are observed in both the target's POV video and observer videos. We leverage this motion correlation to uniquely identify instances of the self, incorporating it into an approach that computes motion correlation over supervoxel hierarchies to localize target instances. Our approach significantly improves self-search performance compared to face detectors and recognizers, enabling applications like privacy filtering, automated video collection, and social group discovery.\n\n---TOPIC---\nFirst-Person Point-of-View (POV) Videos\nSelf-Search/Person Localization\nMotion Correlation\nPrivacy Concerns/Privacy Filtering\nSupervoxel Hierarchies",
    "topics": [],
    "references": [
      {
        "citation": "[T. Ahonen, A. Hadid, and M. Pietikainen. Face description with local binary patterns: Application to face recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), 28(12):2037–2041, 2006.]"
      },
      {
        "citation": "[S. Gong, M. Crisstanini, C. Loy, and T. Hospedales. The re-identiﬁcation challenge. In Person Re-Identiﬁcation, Advances in Computer Vision and Pattern Recognition, pages 1–20. Springer London, 2014.]"
      },
      {
        "citation": "[J. Shi and C. Tomasi. Good features to track. In Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 593 – 600, 600, 1994.]"
      },
      {
        "citation": "[P. Viola and M. Jones. Robust real-time object detection. International Journal of Computer Vision (IJCV), 57(2):137–154, 2004.]"
      },
      {
        "citation": "[H. Bay, A. Ess, T. Tuytelaars, and L. Van Gool. Speeded-up robust features. Computer Vision and Image Understanding (CVIU), 110(3):346–359, 2008.]"
      },
      {
        "citation": "[P. F. Felzenszwalb and D. P. Huttenlocher. Efﬁcient graph-based image segmentation. International Journal of Computer Vision (IJCV), 59(2):167–181, 2004.]"
      },
      {
        "citation": "[J. Hesch and S. Roumeliotis. Consistency analysis and improvement for single-camera localization. In Proc. IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), pages 15 – 22, 2012.]"
      },
      {
        "citation": "[Y. J. Lee, J. Ghosh, and K. Grauman. Discovering important people and objects for egocentric video summarization. In Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1346 – 1353, 2012.]"
      },
      {
        "citation": "[R. Vezzani, D. Baltieri, and R. Cucchiarra. People reidentiﬁcation in surveillance and forensics: A survey. ACM Compututing Surveys, 56(2):29:1–29:37, 2013.]"
      },
      {
        "citation": "[H. S. Park, E. Jain, and Y. Sheikh. 3D Social Saliency from Head-mounted Cameras. In Advances in Neural Information Processing Systems (NIPS), pages 1–9, 2012.]"
      }
    ],
    "author_details": [
      {
        "name": "Ryo Yonetani",
        "affiliation": "The University of Tokyo",
        "email": "yonetani@iis.u-tokyo.ac.jp"
      },
      {
        "name": "Kris M. Kitan",
        "affiliation": "Carnegie Mellon University",
        "email": "kkitani@cs.cmu.edu"
      },
      {
        "name": "Yoichi Sato",
        "affiliation": "The University of Tokyo",
        "email": "ysato@iis.u-tokyo.ac.jp"
      }
    ]
  },
  {
    "title": "On the Minimal Problems of Low-Rank Matrix Factorization\n---AUTHOR---\nFangyuan Jiang\nMagnus Oskarsson\nKalle ˚Astr¨om",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Jiang_On_the_Minimal_2015_CVPR_paper.pdf",
    "id": "Jiang_On_the_Minimal_2015_CVPR_paper",
    "abstract": "Low-rank matrix factorization is a fundamental problem with applications in computer vision. This paper investigates minimal cases for this problem, focusing on scenarios with outliers or sparse observation matrices. The authors utilize Laman graph theory to generate minimal problems and propose a building-block scheme to solve them. The method is tested on synthetic and real data, demonstrating its ability to handle cases where conventional iterative methods fail.\n\n---TOPIC---\nLow-rank matrix factorization\nMinimal problems\nLaman graph theory\nOutliers and missing data\nBuilding-block scheme",
    "topics": [],
    "references": [
      {
        "citation": "[G. Laman, \"On graphs and rigidity of plane skeletal structures,\" Journal of Engineering mathematics, 4(4):331–340, 1970.]"
      },
      {
        "citation": "[H. Aanaes, R. Fisker, K. ˚Astr¨om, and J. Carstensen, \"Robust factorization,\" Trans. Pattern Analysis and Machine Intelligence, 2002.]"
      },
      {
        "citation": "[R. Basri, D. Jacobs, and I. Kemelmacher, \"Photometric stereo with general, unknown lighting,\" International Journal of Computer Vision, 72(3):239–257, 2007.]"
      },
      {
        "citation": "[C. Bregler, A. Hertzmann, and H. Biermann, \"Recovering non-rigid 3d shape from image streams,\" Conf. Computer Vision and Pattern Recognition, 2000.]"
      },
      {
        "citation": "[Z. Lin, M. Chen, L. Wu, and Y. Ma, \"The augmented lagrange multiplier method for exact recovery of a corrupted low-rank matrices,\" CoRR, abs/1009.5055, 2009.]"
      },
      {
        "citation": "[B. D. Lucas and T. Kanade, \"An iterative image registration technique with an application to stereo vision,\" Proceedings of the 7th International Joint Conference on Artificial Intelligence - Volume 2, IJCAI’81, pages 674–679, San Francisco, CA, USA, 1981.]"
      },
      {
        "citation": "[L. W. Mackey, M. I. Jordan, and A. Talwalkar, \"Divide-and-conquer matrix factorization,\" Advances in Neural Information Processing Systems, pages 1134–1142, 2011.]"
      },
      {
        "citation": "[S. I. Olsen and A. Bartoli, \"Implicit non-rigid structure-from-motion with priors,\" Journal of Mathematical Imaging and Vision, 31(2-3):233–244, 2008.]"
      },
      {
        "citation": "[M. Oskarsson, K. Astrom, and N. C. Overgaard, \"Classifying and solving minimal structure and motion problems with missing data,\" Computer Vision, ICCV 2001, volume 1, pages 628–634, 2001.]"
      },
      {
        "citation": "[E. J. Cand`es and B. Recht, \"Exact matrix completion via convex optimization,\" CoRR, abs/0805.4471, 2008.]"
      }
    ],
    "author_details": [
      {
        "name": "Fangyuan Jiang",
        "affiliation": "Lund University, Sweden",
        "email": "fangyuan@maths.lth.se"
      },
      {
        "name": "Magnus Oskarsson",
        "affiliation": "Lund University, Sweden",
        "email": "magnuso@maths.lth.se"
      },
      {
        "name": "Kalle ˚Astr¨om",
        "affiliation": "Lund University, Sweden",
        "email": "kalle@maths.lth.se"
      }
    ]
  },
  {
    "title": "Non-rigid Registration of Images with Geometric and Photometric Deformation by Using Local Afﬁne Fourier-Moment Matching\n---AUTHOR---\nHong-Ren Su\nShang-Hong Lai",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Su_Non-Rigid_Registration_of_2015_CVPR_paper.pdf",
    "id": "Su_Non-Rigid_Registration_of_2015_CVPR_paper",
    "abstract": "Registration between images taken with different cameras, from different viewpoints or under different lighting conditions is a challenging problem. It needs to solve not only the geometric registration problem but also the photometric matching problem. In this paper, we propose to estimate the integrated geometric and photometric transformations between two images based on a local afﬁne Fourier-moment matching framework, which is developed to achieve deformable registration. We combine the local Fourier moment constraints with the smoothness constraints to determine the local afﬁne transforms in a hierarchal block model. Our experimental results on registering some real images related by large color and geometric transformations show that the proposed registration algorithm provides superior image registration results compared to the state-of-the-art image registration methods.\n\n---TOPIC---\nImage Registration\nFourier-Moment Matching\nPhotometric Transformation\nDeformable Registration\nLocal Affine Transforms",
    "topics": [],
    "references": [
      {
        "citation": "[S. G. E. Miller and K. Tieu. Color eigenflows: Statistical modeling of joint color changes. In ICCV, 2001.]"
      },
      {
        "citation": "[D. G. Lowe. Distinctive image features from scale-invariant keypoints. IJCV, 2004.]"
      },
      {
        "citation": "[A. E. Abdel-Hakim and A. A. Farag. Csift: A sift descriptor with color invariant characteristics. In CVPR, 2006.]"
      },
      {
        "citation": "[H. Bay, T. Tuytelaars, and L. V. Gool. Surf: Speeded up robust features. In ECCV, 2006.]"
      },
      {
        "citation": "[N. Dalal and B. Triggs. Histograms of oriented gradients for human detection. In CVPR, 2005.]"
      },
      {
        "citation": "[S. Kagarlitsky, Y. Moses, and Y. Hel-Or. Piecewise-consistent color mappings of images acquired under various condition. In CVPR, 2009.]"
      },
      {
        "citation": "[J. Morel and G. Y. Asift. A new framework for fully afﬁne invariant image comparison. SIAM Journal on Imaging Sciences, 2009.]"
      },
      {
        "citation": "[P. W. Josien, J. B. Pluim, M. Antoine, and A. V. Max. Mutual information based registration of medical images: a survey. IEEE Trans Med Imaging, 22(8):986–1004, 2003.]"
      },
      {
        "citation": "[L. G. Brown. A survey of image registration techniques. ACM Computing Survey, 24(4):325–376, 1992.]"
      },
      {
        "citation": "[B. Zitova and J. Flusser. Image registration methods: a survey. Image Vision Computing, 21(11):977–1000, 2003.]"
      }
    ],
    "author_details": [
      {
        "name": "Hong-Ren Su",
        "affiliation": "Institute of Information Systems and Applications, National Tsing Hua University",
        "email": "suhongren@gmail.com"
      },
      {
        "name": "Shang-Hong Lai",
        "affiliation": "Department of Computer Science, National Tsing Hua University",
        "email": "lai@cs.nthu.edu.tw"
      }
    ]
  },
  {
    "title": "Visual Vibrometry: Estimating Material Properties from Small Motions in Video\n---AUTHORs---\nAbe Davis\nKatherine L. Bouman\nJustin G. Chen\nMichael Rubinstein\nFrédо Durand\nWilliam T. Freeman",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Davis_Visual_Vibrometry_Estimating_2015_CVPR_paper.pdf",
    "id": "Davis_Visual_Vibrometry_Estimating_2015_CVPR_paper",
    "abstract": "Understanding a scene involves more than just recognizing object categories or 3D shape. The physical properties of objects, such as the way they move and bend, can be critical for applications that involve assessing or interacting with the world. This paper connects fundamentals of vibration mechanics with computer vision techniques in order to infer material properties from small, often imperceptible motion in video. Objects tend to vibrate in a set of preferred modes. The shapes and frequencies of these modes depend on the structure and material properties of an object. Focusing on the case where geometry is known or fixed, we show how information about an object’s modes of vibration can be extracted from video and used to make inferences about that object’s material properties. We demonstrate our approach by estimating material properties for a variety of rods and fabrics by passively observing their motion in high-speed and regular framerate video.\n\n---TOPIICS---\nMaterial Properties Estimation\nVibration Analysis\nComputer Vision\nModal Vibrations\nNon-Destructive Testing (NDT)",
    "topics": [],
    "references": [
      {
        "citation": "[Haupt, R. W., & Rolt, K. D. (2005). Stand-off acoustic laser technique to locate buried land mines. Lincoln Laboratory Journal, 36(1), 3–22.]"
      },
      {
        "citation": "[Aranchuk, V., Lal, A., Sabatier, J. M., & Hess, C. (2006). Multi-beam laser doppler vibrometer for landmine detection. Optical Engineering, 45(10), 777777–777777.]"
      },
      {
        "citation": "[Ho, Y.-x., Landy, M. S., & Maloney, L. T. (2006). How direction of illumination affects visually perceived surface roughness. Journal of Vision.]"
      },
      {
        "citation": "[Bhat, K. S., Twigg, C. D., Hodgins, J. K., Kholas, P. K., Popovi´c, Z., & Seitz, S. M. (2003). Estimating cloth simulation parameters from video. In Proceedings of the 2003 ACM SIGGRAPH/Eurographics Symposium on Computer Animation (pp. 37–51). Eurographics Association.]"
      },
      {
        "citation": "[Jojic, N., & Huang, T. S. (1997). Estimating cloth draping parameters from range data. In In International Workshop on Synthetic-Natural Hybrid Coding and 3-D Imaging (pp. 73–76).]"
      },
      {
        "citation": "[Bouman, K. L., Xiao, B., Battaglia, P., & Freeman, W. T. (2013). Estimating the material properties of fabric from video. In IEEE International Conference on Computer Vision (pp. 1984–1991).]"
      },
      {
        "citation": "[Kawabata, S., & Niwa, M. (1989). Fabric performance in clothing and clothing manufacture. Journal of the Textile Institute.]"
      },
      {
        "citation": "[Liu, C., Sharan, L., Adelson, E., & Rosenholtz, R. (2010). Exploring features in a bayesian framework for material recognition.]"
      },
      {
        "citation": "[Portilla, J., & Simoncelli, E. P. (2000). A parametric texture model based on joint statistics of complex wavelet coefficients. International Journal of Computer Vision, 40(1), 49–70.]"
      },
      {
        "citation": "[Rubinstein, M. (2014). Analysis and Visualization of Temporal Variations in Video. PhD thesis, Massachusetts Institute of Technology.]"
      }
    ],
    "author_details": [
      {
        "name": "Abe Davis",
        "affiliation": "Massachusetts Institute of Technology",
        "email": "abedavis@mit.edu"
      },
      {
        "name": "Katherine L. Bouman",
        "affiliation": "Massachusetts Institute of Technology",
        "email": "klbouman@mit.edu"
      },
      {
        "name": "Justin G. Chen",
        "affiliation": "Massachusetts Institute of Technology",
        "email": "ju21743@mit.edu"
      },
      {
        "name": "Michael Rubinstein",
        "affiliation": "Google Research",
        "email": "mrub@google.com"
      },
      {
        "name": "Frédо Durand",
        "affiliation": "Massachusetts Institute of Technology",
        "email": "fredo@mit.edu"
      },
      {
        "name": "William T. Freeman",
        "affiliation": "Massachusetts Institute of Technology",
        "email": "billf@mit.edu"
      }
    ]
  },
  {
    "title": "Hardware Compliant Approximate Image Codes",
    "authors": [
      "Da Kuang",
      "Alex Gittens",
      "Raffay Hamid"
    ],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Kuang_Hardware_Compliant_Approximate_2015_CVPR_paper.pdf",
    "id": "Kuang_Hardware_Compliant_Approximate_2015_CVPR_paper",
    "abstract": "In recent years, several feature encoding schemes for the bags-of-visual-words model have been proposed. While most of these schemes produce impressive results, they all share an important limitation: their high computational complexity makes it challenging to use them for large-scale problems. In this work, we propose an approximate locality-constrained encoding scheme that offers significantly better computational efﬁciency (∼ 40×) than its exact counterpart, with comparable classiﬁcation accuracy. Using the perturbation analysis of least-squares problems, we present a formal approximation error analysis of our approach, which helps distill the intuition behind the robustness of our method. We present a thorough set of empirical analyses on multiple standard data-sets, to assess the capability of our encoding scheme for its representational as well as discriminative accuracy.\n\n---TOPIC---\nImage Encoding\nLocality-Constrained Encoding\nApproximate Algorithms\nComputational Efficiency\nLeast-Squares Problems",
    "topics": [],
    "references": [
      {
        "citation": "[Aranandjelovic, R., & Zisserma, A. All about VLAD. In Proc. of the 2013 IEEE Conf. on Computer Vision and Pattern Recognition, CVPR ’13, pages 1578–1585, 2013.]"
      },
      {
        "citation": "[Chatfield, K., Lemitsky, V., Vedaldi, A., & Zisserma, A. The devil is in the details: an evaluation of recent feature encoding methods. pages 76.1–76.12, 2011.]"
      },
      {
        "citation": "[Fei-Fei, L., Fergus, R., & Perona, P. Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories. Comput. Vis. Image Underst., 106(1):59–70, 2007.]"
      },
      {
        "citation": "[Lowe, D. G. Object recognition from local scale-invariant features. In Proc. of the Int. Conf. on Computer Vision - Volume 2, ICCV ’99, pages 1150–1157, 1999.]"
      },
      {
        "citation": "[Peronnin, F., Liu, Y., S´anchez, J., & Poirier, H. Large-scale image retrieval with compressed Fisher vectors. In Proc. of the 2010 IEEE Conf. on Computer Vision and Pattern Recognition, CVPR ’10, pages 3384–3391, 2010.]"
      },
      {
        "citation": "[Ge, T., He, K., & Sun, J. Product sparse coding. In Proc. of the 2014 IEEE Conf. on Computer Vision and Pattern Recognition, CVPR ’14, pages 939–946, 2014.]"
      },
      {
        "citation": "[S´anchez, J., Perronnin, F., Mensink, T., & Verbeek, J. Image classification with the Fisher vector: Theory and practice. Int. J. Computer Vision, 105(3):222–245, 2013.]"
      },
      {
        "citation": "[Wang, J., Yang, J., Yu, K., Lv, F., Huang, T., & Gong, Y. Locality-constrained linear coding for image classification. In Proc. of the 2010 IEEE Conf. on Computer Vision and Pattern Recognition, CVPR ’10, pages 3360–3367, 2010.]"
      },
      {
        "citation": "[H. Lee, A. Battle, R. Raina, and A. Y. Ng. Efﬁcient sparse coding algorithms. In Advances in Neural Information Processing Systems 19, NIPS ’07, pages 801–808, 2007.]"
      },
      {
        "citation": "[Yang, J., Yu, K., & Gong, Y. Nonlinear learning using local coordinate coding. In Advances in Neural Information Processing Systems 22, NIPS ’09, pages 2223–2231, 2009.]"
      }
    ],
    "author_details": [
      {
        "name": "Da Kuang",
        "affiliation": "Georgia Institute of Technology",
        "email": "da.kuang@cc.gatech.edu"
      },
      {
        "name": "Alex Gittens",
        "affiliation": "University of California, Berkeley",
        "email": "gittens@alumni.caltech.edu"
      },
      {
        "name": "Raffay Hamid",
        "affiliation": "DigitalGlobe Inc",
        "email": "raffay@cc.gatech.edu"
      }
    ]
  },
  {
    "title": "PatchCut: Data-Driven Object Segmentation via Local Shape Transfer\n---AUTHOR---\nJimei Yang\nBrian Price\nScott Cohen\nZhe Lin\nMing-Hsuan Yang",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Yang_PatchCut_Data-Driven_Object_2015_CVPR_paper.pdf",
    "id": "Yang_PatchCut_Data-Driven_Object_2015_CVPR_paper",
    "abstract": "Recent methods have shown that object segmentation can be solved efficiently with carefully prepared bounding boxes and user inputs. However, these algorithms often lack consideration of object shape, leading to poor performance in cluttered images. This paper proposes PatchCut, a data-driven algorithm that uses examples to overcome these limitations. It matches query image patches with example images in multiple scales to enable local shape transfer, creating a patch-level segmentation solution space. A novel cascade algorithm iteratively refines the estimated segmentation using color models, demonstrating effectiveness and robustness across various datasets.\n\n---TOPICCS---\nObject Segmentation\nData-Driven Algorithms\nLocal Shape Transfer\nPatchMatch Algorithm\nCoarse-to-Fine Segmentation",
    "topics": [],
    "references": [
      {
        "citation": "[Ahmed, E., Cohen, S., and Price, B. Semantic object selection. In CVPR, 2014.] - Focuses on object selection, a core component of many segmentation tasks."
      },
      {
        "citation": "[Barnes, C., Shechtman, E., Goldman, D. B., and Finkelstein, A. The generalized PatchMatch correspondence algorithm. In ECCV, 2010.] - PatchMatch is a foundational algorithm for correspondence and initialization in segmentation."
      },
      {
        "citation": "[Bertelli, L., Yu, T., Vu, D., and Gokturk, B. Kernelized structural svm learning for supervised object segmentation. In CVPR, 2011.] - Explores a structural SVM approach, relevant for learning segmentation models."
      },
      {
        "citation": "[Carreira, J., and Sminchisescu, C. Constrained parametric min-cuts for automatic object segmentation. In CVPR, 2010.] - Introduces a min-cut approach for object segmentation, a common technique."
      },
      {
        "citation": "[Dollár, P., and Zitnick, C. Structured forests for fast edge detection. In ICCV, 2013.] - Edge detection is often a precursor to segmentation."
      },
      {
        "citation": "[Harel, J., Koch, C., and Perona, P. Graph-based visual salience. In NIPS, 2006.] - Visual salience is a key concept for identifying regions of interest, often used in segmentation."
      },
      {
        "citation": "[Kim, J., and Grauman, K. Shape sharing for object segmentation. In ECCV, 2012.] - Shape information is crucial for object recognition and segmentation."
      },
      {
        "citation": "[Krizhevsky, A., Sutskever, I., and Hinton, G. E. ImageNet classification with deep convolutional neural networks. In NIPS, 2012.] - This paper introduced AlexNet, a pivotal moment in deep learning and significantly impacted image segmentation."
      },
      {
        "citation": "[Kumar, M. P., Torr, P., and Zisserman, A. Obj cut. In CVPR, 2005.] - A foundational interactive segmentation method."
      },
      {
        "citation": "[Liu, C., Yuen, J., and Torralba, A. Nonparametric scene parsing via label transfer. PAMI, 33(12):2368–2382, 2011.] - Scene parsing and understanding context are important for accurate segmentation."
      }
    ],
    "author_details": [
      {
        "name": "Jimei Yang",
        "affiliation": "UC Merced",
        "email": "Not available"
      },
      {
        "name": "Brian Price",
        "affiliation": "Adobe Research",
        "email": "Not available"
      },
      {
        "name": "Scott Cohen",
        "affiliation": "Adobe Research",
        "email": "Not available"
      },
      {
        "name": "Zhe Lin",
        "affiliation": "Adobe Research",
        "email": "Not available"
      },
      {
        "name": "Ming-Hsuan Yang",
        "affiliation": "UC Merced",
        "email": "Not available"
      }
    ]
  },
  {
    "title": "Recovering Inner Slices of Translucnt Objects by Multi-frequency Illumination\n---AUTHOR---\nKenichiro Tanaka\nYasuhhiro Mukaigawa\nHiroyuki Kubo\nYasuyuki Matsushita\nYasushi Yagi",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Tanaka_Recovering_Inner_Slices_2015_CVPR_paper.pdf",
    "id": "Tanaka_Recovering_Inner_Slices_2015_CVPR_paper",
    "abstract": "This paper describes a method for recovering the appearance of inner slices of translucent objects. The outer appearance of translucent objects is a summation of the appearance of slices at all depths, where each slice is blurred by depth-dependent point spread functions (PSFs). By exploiting the difference of low-pass characteristics of depth-dependent PSFs, the authors develop a multi-frequency illumination method for obtaining the appearance of individual inner slices using a coaxial projector-camera setup. Specifically, by measuring the target object with varying the spatial frequency of checker patterns emitted from a projector, their method recovers inner slices via a simple linear solution method. They quantitatively evaluate the accuracy of the proposed method by simulations and show qualitative recovery results using real-world scenes.\n\n---TOPICCS---\nTranslucent Objects\nMulti-frequency Illumination\nPoint Spread Functions (PSFs)\nImage Recovery\nSubsurface Scattering",
    "topics": [],
    "references": [
      {
        "citation": "[Achar, S., & Narasimhan, S. G. Multi Focus Structured Light for Recovering Scene Shape and Global Illumination. Nature methods, 6(5):339–42, May 2009.]"
      },
      {
        "citation": "[Achar, S., Nuske, S. T., & Narasimhan, S. G. Compensating for Motion during Direct-Global Separation. IEEE International Conference on Computer Vision (ICCV), 2013.]"
      },
      {
        "citation": "[Lamond, B., Peers, P., & Debevec, P. Fast image-based separation of diffuse and specular reflections. ACM SIGGRAPH sketches, 2007.]"
      },
      {
        "citation": "[Chandrasekhar, S. Radiative Transfer. Dover Publications, Inc., 1960.]"
      },
      {
        "citation": "[Narasimhan, S. G., & Nayar, S. K. Shedding light on the weather. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 665–672, 2003.]"
      },
      {
        "citation": "[Mukagawa, Y., Raskar, R., & Yagi, Y. Analysis of light transport in scattering media. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 153–160, 2010.]"
      },
      {
        "citation": "[Narasimhan, S. G., Gupta, M., Donner, C., Ramamoorthi, R., Nayar, S. K., & Jensen, H. W. Acquiring scattering properties of participating media by dilution. ACM Transactions on Graphics, 25, 2006.]"
      },
      {
        "citation": "[Gupta, M., Tian, Y., Narasimhan, S. G., & Zhang, L. A Combined Theory of Defocused Illumination and Global Light Transport. International Journal of Computer Vision, 98(2):146–167, Oct. 2011.]"
      },
      {
        "citation": "[Gu, J., Nayar, S. K., Gringspun, E., Belhumeur, P., & Ramamoorthi, R. Compressive Structured Light for Recovering Inhomogeneous Participating Media. IEEE Transactions on Pattern Analysis and Machine Intelligence(TPAMI), 35(3):555–567, 2013.]"
      },
      {
        "citation": "[Nayar, S. K., Krishnan, G., Grossberg, M. D., & Raskar, R. Fast separation of direct and global components of a scene using high frequency illumination. Proceeding ACM SIGGRAPH, pages 935–944, 2006.]"
      }
    ],
    "author_details": [
      {
        "name": "Kenichiro Tanaka",
        "affiliation": "Osaka University",
        "email": "tanaka@am.sanken.osaka-u.ac.jp (inferred from the project webpage)"
      },
      {
        "name": "Yasuhiro Mukaigawa",
        "affiliation": "Nara Institute of Science and Technology",
        "email": "Not available"
      },
      {
        "name": "Hiroyuki Kubo",
        "affiliation": "Nara Institute of Science and Technology",
        "email": "Not available"
      },
      {
        "name": "Yasuyuki Matsushita",
        "affiliation": "Osaka University",
        "email": "Not available"
      },
      {
        "name": "Yasushi Yagi",
        "affiliation": "Osaka University",
        "email": "Not available"
      }
    ]
  },
  {
    "title": "Solving Multiple Square Jigsaw Puzzles with Missing Pieces\n---AUTHOR---\nGenady Paikin\nAyellet Tal",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Paikin_Solving_Multiple_Square_2015_CVPR_paper.pdf",
    "id": "Paikin_Solving_Multiple_Square_2015_CVPR_paper",
    "abstract": "Jigsaw-puzzle solving is necessary in many applications, including biology, archaeology, and every-day life. In this paper we consider the square jigsaw puzzle problem, where the goal is to reconstruct the image from a set of non-overlapping, unordered, square puzzle parts. Our key contribution is a fast, fully-automatic, and general solver, which assumes no prior knowledge about the original image. It is general in the sense that it can handle puzzles of unknown size, with pieces of unknown orientation, and even puzzles with missing pieces. Moreover, it can handle all the above, given pieces from multiple puzzles. Through an extensive evaluation we show that our approach outperforms state-of-the-art methods on commonly-used datasets.",
    "topics": [
      "Square jigsaw puzzles",
      "Image reconstruction",
      "Greedy algorithms",
      "Multiple puzzle solving",
      "Missing pieces"
    ],
    "references": [
      {
        "citation": "[T. Altman. Solving the jigsaw puzzle problem in linear time. Applied Artiﬁcial Intelligence, 3(4):453–462, 1990.]"
      },
      {
        "citation": "[B. J. Brown, C. Toler-Franklin, D. Nehab, M. Burns, D. Dobkin, A. Vlachopoulos, C. Doumas, S. Rusinkiewicz, and T. Weyrich. A system for high-volume acquisition and matching of fresco fragments: Reassembling theran wall paintings. ACM Transactions on Graphics, 27(3), 2008.]"
      },
      {
        "citation": "[S. Cao, H. Liu, and S. Yan. Automated assembly of shredded pieces from multiple photos. In IEEE Int. Conf. on Multime-dia and Expo, pages 358–363, 2010.]"
      },
      {
        "citation": "[T. Cho, S. Avidan, and W. Freeman. The patch transform. PAMI, 32(8):1489–1501, 2010.]"
      },
      {
        "citation": "[T. Cho, S. Avidan, and W. Freeman. A probabilistic image jigsaw puzzle solver. In CVPR, 2010.]"
      },
      {
        "citation": "[M. G. Chung, M. M. Fleck, and D. A. Forsyth. Jigsaw puzzle solver using shape and color. In ICSP, 1998.]"
      },
      {
        "citation": "[A. Gallagher. Jigsaw puzzles with pieces of unknown orien-tation. In CVPR, 2012.]"
      },
      {
        "citation": "[D. Pomeranz, M. Shemesh, and O. Ben-Shahar. A fully au-tomated greedy square jigsaw puzzle solver. In CVPR, 2011.]"
      },
      {
        "citation": "[D. Goldberg, C. Malon, and M. Bern. A global approach to solution of jigsaw puzzles. In Symposium on Computational Geometry, 2002.]"
      },
      {
        "citation": "[D. Koller and M. Levoy. Computer-aided reconstruction and new matches in the forma urbis romae. Bullettino Della Commissione Archeologica Comunale di Roma, 15:103–125, 2006.]"
      }
    ],
    "author_details": [
      {
        "name": "Genady Paikin",
        "affiliation": "Technion",
        "email": "genadyp28@gmail.com"
      },
      {
        "name": "Ayellett Tal",
        "affiliation": "Technion",
        "email": "ayellet@ee.technion.ac.il"
      }
    ]
  },
  {
    "title": "One-day outdoor photometric stereo via skylight estimation\n---AUTHOR---\nJiyoung Jung\nJoon-Young Lee\nIn So Kweon",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Jung_One-Day_Outdoor_Photometric_2015_CVPR_paper.pdf",
    "id": "Jung_One-Day_Outdoor_Photometric_2015_CVPR_paper",
    "abstract": "We present an outdoor photometric stereo method using images captured in a single day. We simulate a sky hemisphere for each image according to its GPS and timestamp, and parameterize the obtained sky hemisphere into a quadratic skylight and a Gaussian sunlight distribution. Unlike previous works which usually model outdoor illumination as a sum of constant ambient light and a distant point light, our method models natural illumination according to a popular sky model and thus provides sufficient constraints for shape reconstruction from one day images. We generate pixel profiles of uniformly sampled unit vectors for the corresponding time of captures and evaluate them using correlation with the actual pixel profiles. The estimated surface normal is refined by MRF optimization. We have tested our method to recover objects and scenes of various sizes in real-world outdoor daylight.\n\n---TOPICCS---\nPhotometric Stereo\nOutdoor Illumination\nSkylight Modeling\nSurface Normal Estimation\nShape Reconstruction",
    "topics": [],
    "references": [],
    "author_details": [
      {
        "name": "Jiyoung Jung",
        "affiliation": "Robotics and Computer Vision Lab, KAIST",
        "email": "jyjung@rcv.kaist.ac.kr"
      },
      {
        "name": "Joon-Young Lee",
        "affiliation": "Robotics and Computer Vision Lab, KAIST",
        "email": "jylee@rcv.kaist.ac.kr"
      },
      {
        "name": "In So Kweon",
        "affiliation": "Robotics and Computer Vision Lab, KAIST",
        "email": "iskweon77@kaist.ac.kr"
      }
    ]
  },
  {
    "title": "Bayesian Adaptive Matrix Factorization with Automatic Model Selection\n---AUTHORISTS---\nPeixian Chen\nNaiyan Wang\nNevin L. Zhang\nDit-Yan Yeung",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Chen_Bayesian_Adaptive_Matrix_2015_CVPR_paper.pdf",
    "id": "Chen_Bayesian_Adaptive_Matrix_2015_CVPR_paper",
    "abstract": "Low-rank matrix factorization is fundamental in many computer vision applications. However, existing methods often struggle with model selection issues, specifically choosing the noise model and determining the model capacity (rank). This paper addresses these issues by proposing a robust non-parametric Bayesian adaptive matrix factorization (AMF) model. AMF utilizes a Dirichlet process Gaussian mixture model (DP-GMM) for noise modeling, offering flexibility in component number selection and fitting various noise types. It also employs an automatic relevance determination (ARD) prior on the factor matrices to automatically determine the rank. An efficient variational method is developed for model inference, and experimental results demonstrate AMF's superior or comparable performance compared to state-of-the-art methods.",
    "topics": [
      "Matrix Factorization",
      "Bayesian Methods",
      "Noise Modeling (DP-GMM)",
      "Automatic Relevance Determination (ARD)",
      "Variational Inference"
    ],
    "references": [
      {
        "citation": "[S. Babacan, M. Luessi, R. Molina, and A. Katsaggelos. Sparse Bayesian methods for low-rank matrix estimation. IEEE Transactions on Signal Processing, 60(8):3964–3977, 2012.]"
      },
      {
        "citation": "[M. Blei and I. Jordan. Variational inference for Dirichlet process mixtures. Bayesian analysis, 1(1):121–143, 2006.]"
      },
      {
        "citation": "[S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein. Distributed optimization and statistical learning via the alternating direction method of multipliers. Foundations and Trends R⃝ in Machine Learning, 3(1):1–122, 2011.]"
      },
      {
        "citation": "[E. Candes, X. Li, Y. Ma, and J. Wright. Robust principal component analysis? Journal of the Association for Computing Machinery, 58(3), 2011.]"
      },
      {
        "citation": "[L. Carin, X. Ding, and L. He. Bayesian robust principal component analysis. IEEE Transactions on Image Processing, 20(12):3419–3430, 2011.]"
      },
      {
        "citation": "[Q. Ke and T. Kanade. Robust l1 norm factorization in the presence of outliers and missing data by alternative convex programming. In CVPR, pages 739–746, 2005.]"
      },
      {
        "citation": "[H. Ishwaran and L. James. Gibbs sampling methods for stick-breaking priors. Journal of the American Statistical Association, 96(453), 2001.]"
      },
      {
        "citation": "[D. J. MacKay. Bayesian interpolation. Neural computation, 4(3):415–447, 1992.]"
      },
      {
        "citation": "[D. J. MacKay. Probable networks and plausible predictions–a review of practical Bayesian methods for supervised neu-ral networks. Network: Computation in Neural Systems, 6(3):469–505, 1995.]"
      },
      {
        "citation": "[D. Meng and F. De la Torre. Robust matrix factorization with unknown noise. In ICCV, 2013.]"
      }
    ],
    "author_details": [
      {
        "name": "Peixian Chen",
        "affiliation": "The Hong Kong University of Science and Technology",
        "email": "pchenac@cse.ust.hk"
      },
      {
        "name": "Naiyan Wang",
        "affiliation": "The Hong Kong University of Science and Technology",
        "email": "winsty@gmail.com"
      },
      {
        "name": "Nevin L. Zhang",
        "affiliation": "The Hong Kong University of Science and Technology",
        "email": "lzhang@cse.ust.hk"
      },
      {
        "name": "Dit- Yan Yeung",
        "affiliation": "The Hong Kong University of Science and Technology",
        "email": "dyyeung@cse.ust.hk"
      }
    ]
  },
  {
    "title": "The S-HOCK Dataset: Analyzing Crowds at the Stadium\n---AUTHOR---\nDavide Coniglianro\nPaolo Rota\nFrancesco Setti\nChiara Bassetti\nNicola Conci\nNicu Sebe\nMarco Crisitan",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Conigliaro_The_S-Hock_Dataset_2015_CVPR_paper.pdf",
    "id": "Conigliaro_The_S-Hock_Dataset_2015_CVPR_paper",
    "abstract": "The paper addresses the challenge of crowd modeling in computer vision, noting that existing approaches often assume a generic crowd typology. It introduces the Spectators Hockey (S-HOCK) dataset, focusing on spectator crowds at hockey matches. This dataset contains over 100 million annotations at varying levels of detail, from team support and social connections to pose information and fine-grained actions. The dataset’s multidimensional nature, including views of both the crowd and the game, enables a wide range of applications, including spectator categorization and analysis of crowd reactions to game events. The paper also discusses the limitations of standard crowd analysis methods when applied to this specialized dataset and provides baseline results and experimental protocols to encourage further research.\n\n---TOPIC---\nCrowd Modeling\nSpectator Analysis\nHockey Matches\nDataset Creation (S-HOCK)\nComputer Vision Applications",
    "topics": [],
    "references": [
      {
        "citation": "[Aggarwal, J., & Ryoo, M. (2011). Human activity analysis: A review. ACM Computing Surveys (CSUR), 43(3), 16:1–16:43.]"
      },
      {
        "citation": "[Ali, S., & Shah, M. (2007). A lagrangian particle dynamics approach for crowd flow segmentation and stability analysis. In CVPR.]"
      },
      {
        "citation": "[Chan, A. B., & Vasconcelos, N. (2009). Bayesian poisson regression for crowd counting. In ICCV.]"
      },
      {
        "citation": "[Dalal, N., & Triggs, B. (2005). Histograms of oriented gradients for human detection. In CVPR.]"
      },
      {
        "citation": "[Viola, M. J. P. (2001). Rapid object detection using a boosted cascade of simple features. In CVPR.]"
      },
      {
        "citation": "[Dollar, P., Appel, R., Belongie, S., & Perona, P. (2014). Fast feature pyramids for object detection. PAMI, 56(8), 1532–1545.]"
      },
      {
        "citation": "[Rodriguez, M., Laptev, I., Sivic, J., & Audibert, J.-Y. (2011). Density-aware person detection and tracking in crowds. In ICCV.]"
      },
      {
        "citation": "[Zhou, B., X. Tang, and H. Zhang. (2014). Understanding collective crowd behaviors: Learning a mixture model of dynamic pedestrian-agents. In CVPR.]"
      },
      {
        "citation": "[Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). ImageNet classification with deep convolutional neural networks. In NIPS.]"
      },
      {
        "citation": "[Mahadevan, V., Li, W., Bhalodia, V., & Vasconcelos, N. (2010). Anomaly detection in crowded scenes. In CVPR.]"
      }
    ],
    "author_details": [
      {
        "name": "Davide Coniglianro",
        "affiliation": "University of Verona",
        "email": "Not available"
      },
      {
        "name": "Paolo Rota",
        "affiliation": "Vienna University of Technology",
        "email": "Not available"
      },
      {
        "name": "Francesco Setti",
        "affiliation": "ISTC–CNR (Trento)",
        "email": "Not available"
      },
      {
        "name": "Chiara Bassetti",
        "affiliation": "ISTC–CNR (Trento)",
        "email": "Not available"
      },
      {
        "name": "Nicola Conci",
        "affiliation": "University of Trento",
        "email": "Not available"
      },
      {
        "name": "Nicu Sebe",
        "affiliation": "University of Trento",
        "email": "Not available"
      },
      {
        "name": "Marco Crisitan",
        "affiliation": "University of Verona",
        "email": "Not available"
      }
    ]
  },
  {
    "title": "Simplified Mirror-Based Camera Pose Computation via Rotation Averaging\n---AUTHOR---\nGucan Long\n---AUTHOR---\nLaurent Kneip\n---AUTHOR---\nXin Li\n---AUTHOR---\nXiaohu Zhang\n---AUTHOR---\nQifeng Yu",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Long_Simplified_Mirror-Based_Camera_2015_CVPR_paper.pdf",
    "id": "Long_Simplified_Mirror-Based_Camera_2015_CVPR_paper",
    "abstract": "We propose a novel approach to compute the camera pose with respect to a reference object given only mirrored views. The latter originate from a planar mirror at different unknown poses. This problem is highly relevant in several extrinsic camera calibration scenarios, where the camera cannot see the reference object directly. In contrast to numerous existing methods, our approach does not employ the fixed axis rotation constraint, but represents a more elegant formulation as a rotation averaging problem. Our theoretical contribution extends the applicability of rotation averaging to a more general case, and enables mirror-based pose estimation in closed-form under the chordal L2-metric, or in an outlier-robust way by employing iterative L1-norm averaging. We demonstrate the advantages of our approach on both synthetic and real data, and show how the method can be applied to calibrate the non-overlapping pair of cameras of a common smart phone.",
    "topics": [
      "Mirror-based camera pose estimation",
      "Rotation averaging",
      "Extrinsic camera calibration",
      "Non-overlapping camera pairs",
      "Outlier-robust pose estimation"
    ],
    "references": [],
    "author_details": [
      {
        "name": "Gucan Long",
        "affiliation": "College of Aerospace Science and Engineering, National University of Defence Technology, P.R. China",
        "email": "[Not available in provided text]"
      },
      {
        "name": "Laurent Kneip",
        "affiliation": "Research School of Engineering, Australian National University",
        "email": "[Not available in provided text]"
      },
      {
        "name": "Xin Li",
        "affiliation": "College of Aerospace Science and Engineering, National University of Defence Technology, P.R. China",
        "email": "[Not available in provided text]"
      },
      {
        "name": "Xiaohu Zhang",
        "affiliation": "College of Aerospace Science and Engineering, National University of Defence Technology, P.R. China",
        "email": "[Not available in provided text]"
      },
      {
        "name": "Qifeng Yu",
        "affiliation": "College of Aerospace Science and Engineering, National University of Defence Technology, P.R. China",
        "email": "[Not available in provided text]"
      }
    ]
  },
  {
    "title": "Associating Neural Word Embeddings with Deep Image Representations using Fisher Vectors\n---AUTHOR---\nBenjamin Klein\nGuy Lev\nGil Sadeh\nLior Wolf",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Klein_Associating_Neural_Word_2015_CVPR_paper.pdf",
    "id": "Klein_Associating_Neural_Word_2015_CVPR_paper",
    "abstract": "This paper presents new variants of Fisher Vectors based on the Laplacian distribution and a hybrid Gaussian-Laplacian distribution. The work addresses the limitations of traditional Fisher Vectors, which often rely on Gaussian distributions that may not adequately capture the distribution of image descriptors. By deriving Expectation-Maximization (EM) equations and Fisher Vector definitions for Laplacian Mixture Models (LMM) and Hybrid Gaussian-Laplacian Mixture Models (HGLMM), the authors aim to improve sentence representation for image annotation and image search tasks. The proposed methods achieve state-of-the-art results on four benchmarks (Pascal1K, Flickr8K, Flickr30K, and COCO) by using the new Fisher Vectors derived from HGLMMs to represent sentences.",
    "topics": [
      "Fisher Vector",
      "Laplacian Distribution",
      "Image Annotation",
      "Sentence Representation",
      "Hybrid Gaussian-Laplacian Mixture Model"
    ],
    "references": [
      {
        "citation": "[Bengio, Y., Ducharme, R., Vincent, P., & Janvin, C. (2003). A neural probabilistic language model. *Journal of Machine Learning Research*, *3*, 1137–1155.] - This paper introduces a neural probabilistic language model, a foundational work in language modeling."
      },
      {
        "citation": "[Girshick, R., Donahue, J., Darrell, T., & Malik, J. (2013). Rich feature hierarchies for accurate object detection and semantic segmentation. *arXiv preprint arXiv:1311.2524*.] - This paper introduces a rich feature hierarchy approach for object detection and semantic segmentation, a significant contribution to computer vision."
      },
      {
        "citation": "[Hinton, G. E., Srivastava, N., Krizhevsky, A., Sutskever, I., & Salakhutdov, R. (2012). Improving neural networks by preventing co-adaptation of feature detectors. *CoRR, abs/1207.0580*.] - This paper proposes techniques to prevent co-adaptation of feature detectors in neural networks, a crucial step in improving network performance."
      },
      {
        "citation": "[Socher, R., & Fei-Fei, L. (2010). Connecting modalities: Semi-supervised segmentation and annotation of images using unaligned text corpora. *In Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on, pages 2010*.] - This paper explores connecting visual and textual information for image understanding."
      },
      {
        "citation": "[Karpathy, A., & Fei-Fei, L. (2014). Deep visual-semantic alignments for generating image descriptions. *Technical report, Computer Science Department, Stanford University*.] - This paper focuses on aligning visual and semantic information for image description generation."
      },
      {
        "citation": "[Socher, R., Lin, C. C., Ng, A. Y., & Manning, C. D. (2011). Parsing Natural Scenes and Natural Language with Recursive Neural Networks. *In Proceedings of the 26th International Conference on Machine Learning (ICML)*.] - This paper introduces a recursive neural network approach for parsing natural scenes and language."
      },
      {
        "citation": "[Vinyals, O., Toshev, A., Bengio, S., & Erhan, D. (2014). Show and tell: A neural image caption generator. *arXiv preprint arXiv:1411.4555*.] - This paper presents a neural image caption generator, a significant advancement in image understanding and generation."
      },
      {
        "citation": "[Simonyan, K., Parkhi, O. M., Vedaldi, A., & Zisserman, A. (2013). Fisher vector faces in the wild. *In Proc. BMVC, volume 1, page 7*.] - This paper applies Fisher vectors to face recognition, demonstrating their effectiveness in a challenging setting."
      },
      {
        "citation": "[Mnih, A., & Hinton, G. (2007). Three new graphical models for statistical language modelling. *In Proceedings of the 24th International Conference on Machine Learning (ICML), pages 2007*.] - This paper introduces new graphical models for statistical language modeling."
      },
      {
        "citation": "[Rashtchian, C., Young, P., Hodosh, M., & Hockenmaier, J. (2010). Collecting image annotations using amazon’s mechanical turk. *In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk, pages 2010*.] - This paper details a method for collecting image annotations using Amazon's Mechanical Turk, a common practice in image understanding research."
      }
    ],
    "author_details": [
      {
        "name": "Benjamin Klein",
        "affiliation": "Tel Aviv University",
        "email": "beni.klein@gmail.com"
      },
      {
        "name": "Guy Lev",
        "affiliation": "Tel Aviv University",
        "email": "levguy@gmail.com"
      },
      {
        "name": "Gil Sadeh",
        "affiliation": "Tel Aviv University",
        "email": "gilsadeh@gmail.com"
      },
      {
        "name": "Lior Wolf",
        "affiliation": "Tel Aviv University",
        "email": "wolf@cs.tau.ac.il"
      }
    ]
  },
  {
    "title": "Interleaved Text/Image Deep Mining on a Large-Scale Radiology Database\n---AUTHOR---\nHoo-Chang Shin\nLe Lu\nLauren Kim\nAri Seff\nJianhua Yao\nRonald M. Summers",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Shin_Interleaved_TextImage_Deep_2015_CVPR_supplemental.pdf",
    "id": "Shin_Interleaved_TextImage_Deep_2015_CVPR_supplemental",
    "abstract": "This paper presents a novel approach to \"Interleaved Text/Image Deep Mining\" on a large-scale radiology database. The method leverages deep convolutional neural networks (CNNs) to classify images into topics based on associated radiology reports. Experiments explore various levels of topic modeling (document, sentence) and architectures (AlexNet, VGGNet), demonstrating the benefits of fine-tuning CNNs from the ImageNet dataset and transferring knowledge between related tasks. The study also includes a radiologist's analysis of the generated topics, providing valuable insights into the clinical relevance of the findings. This work represents a large-scale study of deep CNNs' cross-modality image generality.\n\n---TOPICICS---\nDeep Convolutional Neural Networks (CNNs)\nRadiology Reports and Image Mining\nTopic Modeling (Document-level, Sentence-level)\nCross-Modality Image Generality\nImage Classification and Fine-tuning",
    "topics": [],
    "references": [],
    "author_details": [
      {
        "name": "Hoo-Chang Shin",
        "affiliation": "Not explicitly mentioned in the provided text.",
        "email": "Not available."
      },
      {
        "name": "Le Lu",
        "affiliation": "Not explicitly mentioned in the provided text.",
        "email": "Not available."
      },
      {
        "name": "Lauren Kim",
        "affiliation": "Not explicitly mentioned in the provided text.",
        "email": "Not available."
      },
      {
        "name": "Ari Seff",
        "affiliation": "Not explicitly mentioned in the provided text.",
        "email": "Not available."
      },
      {
        "name": "Jianhua Yao",
        "affiliation": "Not explicitly mentioned in the provided text.",
        "email": "Not available."
      },
      {
        "name": "Ronald M. Summers",
        "affiliation": "Not explicitly mentioned in the provided text.",
        "email": "Not available."
      }
    ]
  },
  {
    "title": "Finding Action Tubes\n---AUTHORSHIP---\nGeorgia Gkioxari\nJitendra Malik",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Gkioxari_Finding_Action_Tubes_2015_CVPR_paper.pdf",
    "id": "Gkioxari_Finding_Action_Tubes_2015_CVPR_paper",
    "abstract": "We address the problem of action detection in videos. Driven by the latest progress in object detection from 2D images, we build action models using rich feature hierarchies derived from shape and kinematic cues. We incorporate appearance and motion in two ways. First, starting from image region proposals we select those that are motion salient and thus are more likely to contain the action. This leads to a significant reduction in the number of regions being processed and allows for faster computations. Second, we extract spatio-temporal feature representations to build strong classifiers using Convolutional Neural Networks. We link our predictions to produce detections consistent in time, which we call action tubes. We show that our approach outperforms other techniques in the task of action detection. Our detection pipeline is inspired by the human vision system and, in particular, the two-streams hypothesis. We use convolutional neural networks to computationally simulate the two pathways. We show results on the task of action detection on two publicly available datasets, that contain actions in real world scenarios, UCF Sports and J-HMDB.\n\n---TOPICCS---\nAction Detection\nConvolutional Neural Networks (CNNs)\nTwo-Stream Networks\nMotion Analysis\nRegion Proposals",
    "topics": [],
    "references": [
      {
        "citation": "[Aggarwal, J., & Ryoo, M. (2011). Human activity analysis: A review. ACM Computing Surveys.]"
      },
      {
        "citation": "[Arbeláez, P., Pont-Tuset, J., Barron, J., Marques, F., & Malik, J. (2014). Multiscale combinatorial grouping. In CVPR.]"
      },
      {
        "citation": "[Blank, M., Gorelick, L., Shechtman, E., Irani, M., & Basri, R. (2005). Actions as space-time shapes. In ICCV.]"
      },
      {
        "citation": "[Brox, T., Bruhn, A., Papenberg, N., & Weickert, J. (2004). High accuracy optical flow estimation based on a theory for warping. In ECCV.]"
      },
      {
        "citation": "[Dalal, N., & Triggs, B. (2005). Histograms of oriented gradients for human detection. In CVPR.]"
      },
      {
        "citation": "[Deng, J., Berg, A., Satheesh, S., Su, H., Khosla, A., & Fei-Fei, L. (2012). ImageNet Large Scale Visual Recognition Competition 2012 (ILSVRC2012). http://www.image-net.org/challenges/LSVRC/2012/.]"
      },
      {
        "citation": "[Deng, J., Dong, W., Socher, R., Li-Jia, L., Li, K., & Fei-Fei, L. (2009). ImageNet: A large-scale hierarchical image database. In CVPR.]"
      },
      {
        "citation": "[Efroos, A. A., Berg, A. C., & Malik, J. (2003). Recognizing action at a distance. In ICCV.]"
      },
      {
        "citation": "[Everingham, M., Van Gool, L., Williams, C. K. I., Winn, J., & Zisserman, A. (2010). The PASCAL Visual Object Classes (VOC) Challenge. IJCV.]"
      },
      {
        "citation": "[Felzenszwalb, P., Girshick, R., McAllester, D., & Ramanan, D. (2010). Object detection with discriminatively trained part based models. TPAMI.]"
      }
    ],
    "author_details": [
      {
        "name": "Georgia Gkioxari",
        "affiliation": "UC Berkeley",
        "email": "gkioxari@eecs.berkeley.edu"
      },
      {
        "name": "Jitendra Malik",
        "affiliation": "UC Berkeley",
        "email": "malik@eecs.berkeley.edu"
      }
    ]
  },
  {
    "title": "Active Sample Selection and Correction Propagation on a Gradually-Augmented Graph\n---AUTHOR---\nHang Su\nZhaozheng Yin\nTakeo Kanade\nSeungil Huh",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Su_Active_Sample_Selection_2015_CVPR_paper.pdf",
    "id": "Su_Active_Sample_Selection_2015_CVPR_paper",
    "abstract": "Since the seminal work of Zhu et al., a lot of effort has been made on combining active and semi-supervised learning. To address the issue of complex data structures or evolving data characteristics, this paper proposes a sample selection criterion used for active query of informative samples by minimizing the expected prediction error, and an efficient correction propagation method that propagates human correction on selected samples over a gradually-augmented graph to unlabeled samples without rebuilding the afﬁnity graph. Experimental results validate that the proposed algorithm quickly reaches high-quality classification results with minimal human interventions. The paper also addresses the challenge of training and testing data exhibiting differences in statistics and characteristics of samples gradually changing over time, which often leads to error-free label propagation.\n\n---TOPICICS---\nActive Learning\nSemi-Supervised Learning\nGraph-Based Learning\nCorrection Propagation\nSample Selection",
    "topics": [],
    "references": [
      {
        "citation": "[Settles, B. Active learning literature survey. Computer Sciences Technical Report 1648, University of Wisconsin–Madison, 2009.] - Provides a comprehensive overview of active learning techniques."
      },
      {
        "citation": "[Guillory, A., & Bilmes, J. A. Label selection on graphs. In Advances in Neural Information Processing Systems, pages 691–699. Curran Associates, Inc., 2009.] - Focuses on label selection specifically within graph-based active learning."
      },
      {
        "citation": "[Settles, B., Craven, M., & Ray, S. Multiple-instance active learning. In Advances in Neural Information Processing Systems 20, pages 1289–1296. 2008.] - Explores active learning in the multiple-instance learning setting."
      },
      {
        "citation": "[Settles, B., & Craven, M. An analysis of active learning strategies for sequence labeling tasks. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, October 2008.] - Examines active learning strategies for sequence labeling."
      },
      {
        "citation": "[Guillory, A., & Bilmes, J. A. Active semi-supervised learning using submodular functions. In Uncertainty in Artificial Intelligence (UAI), July 2011.] - Introduces a submodular function approach to active semi-supervised learning."
      },
      {
        "citation": "[El-Yaniv, R., & Pechyony, D. Transductive rademacher complexity and its applications. Journal of Artificial Intelligence Research, 35:193–234, 2009.] - Discusses transductive learning and its connection to active learning."
      },
      {
        "citation": "[Settles, B. Active learning literature survey. Computer Sciences Technical Report 1648, University of Wisconsin–Madison, 2009.] - Provides a broad overview of the field."
      },
      {
        "citation": "[Jain, P., & Kapoor, A. Active learning for large multi-class problems. In Computer Vision and Pattern Recognition. IEEE Conference on, pages 762–769, June 2009.] - Addresses active learning challenges in multi-class classification."
      },
      {
        "citation": "[Zhu, X., Lafferty, J., & Ghahramani, Z. Combining active learning and semi-supervised learning using gaussian fields and harmonic functions. In ICML 2003 workshop on The Continuum from Labeled to Unlabeled Data in Machine Learning and Data Mining, pages 58–65, 2003.] - Explores a combined approach to active and semi-supervised learning."
      },
      {
        "citation": "[Guo, Y., & Greiner, R. Optimistic active learning using mutual information. In Proceedings of the 20th International Joint Conference on Artificial Intelligence, pages 823–829, 2007.] - Proposes an optimistic active learning approach based on mutual information."
      }
    ],
    "author_details": [
      {
        "name": "Hang Su",
        "affiliation": "Department of Computer Science and Technology, Tsinghua University",
        "email": "[Email not available in the provided text]"
      },
      {
        "name": "Zhaozheng Yin",
        "affiliation": "Department of Computer Science, Missouri University of Science and Technology",
        "email": "[Email not available in the provided text]"
      },
      {
        "name": "Takeo Kanade",
        "affiliation": "Robotics Institute, Carnegie Mellon University",
        "email": "[Email not available in the provided text]"
      },
      {
        "name": "Seungil Huh",
        "affiliation": "Google",
        "email": "[Email not available in the provided text]"
      }
    ]
  },
  {
    "title": "Deformable Part Models are Convolutional Neural Networks\n---AUTHORISTS---\nRoss Girshick\nForrest Iandola\nTrevor Darrell\nJitendra Malik",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Girshick_Deformable_Part_Models_2015_CVPR_paper.pdf",
    "id": "Girshick_Deformable_Part_Models_2015_CVPR_paper",
    "abstract": "This paper demonstrates that deformable part models (DPMs) can be formulated as convolutional neural networks (CNNs), bridging the gap between these traditionally distinct approaches to visual recognition. The authors construct a \"DeepPyramid DPM\" by unrolling the DPM inference algorithm and mapping each step to an equivalent CNN layer, replacing standard image features with learned features from a fully-convolutional network. Experimental results on the PASCAL VOC dataset show that DeepPyramid DPMs significantly outperform HOG-based DPMs and slightly outperform a comparable R-CNN detector, while running significantly faster. The paper also highlights the complementary nature of region-based and sliding-window detection methods.",
    "topics": [
      "Deformable Part Models (DPMs)",
      "Convolutional Neural Networks (CNNs)",
      "Object Detection",
      "DeepPyramid DPM",
      "Feature Representation (HOG vs. Learned Features)"
    ],
    "references": [
      {
        "citation": "[Christian Szegedy, Alexander Toshev, Deep neural networks for object detection. In NIPS, 2013.]"
      },
      {
        "citation": "[Y.-L. Boureau, J. Ponce, and Y. LeCun. A theoretical analysis of feature pooling in visual recognition. In ICML, 2010.]"
      },
      {
        "citation": "[R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. CoRR, abs/1311.2524, 2013.]"
      },
      {
        "citation": "[K. He, X. Zhang, S. Ren, and J. Sun. Spatial pyramid pooling in deep convolutional networks for visual recognition. In ECCV, 2014.]"
      },
      {
        "citation": "[J. Donahue, Y. Jia, O. Vinyals, J. Hoffman, N. Zhang, E. Tzeng, and T. Darrell. DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition. In ICML, 2014.]"
      },
      {
        "citation": "[R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In CVPR, 2014.]"
      },
      {
        "citation": "[A. Krizhevsky, I. Sutskever, and G. Hinton. ImageNet classification with deep convolutional neural networks. In NIPS, 2012.]"
      },
      {
        "citation": "[P. Felzenszwalb, R. Girshick, D. McAllester, and D. Ramanan. Object detection with discriminatively trained part based models. TPAMI, 2010.]"
      },
      {
        "citation": "[Y. Jia, C. Huang, and T. Darrell. Beyond spatial pyramids: Receptive field learning for pooled image features. In CVPR, 2012.]"
      },
      {
        "citation": "[J. Long, N. Zhang, and T. Darrell. Do convnets learn correspondence? In NIPS (to appear), 2014.]"
      }
    ],
    "author_details": [
      {
        "name": "Ross Girshick",
        "affiliation": "Microsoft Research",
        "email": "rbg@microsoft.com"
      },
      {
        "name": "Forrest Iandola",
        "affiliation": "UC Berkeley",
        "email": "{forresti}@eecs.berkeley.edu"
      },
      {
        "name": "Trevor Darrell",
        "affiliation": "UC Berkeley",
        "email": "{trevor}@eecs.berkeley.edu"
      },
      {
        "name": "Jitendra Malik",
        "affiliation": "UC Berkeley",
        "email": "{malik}@eecs.berkeley.edu"
      }
    ]
  },
  {
    "title": "Mid-level Deep Pattern Mining\n---AUTHOR---\nYao Li\nLingqiao Liu\nChunhua Shen\nAnton van den Hengel",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Li_Mid-Level_Deep_Pattern_2015_CVPR_paper.pdf",
    "id": "Li_Mid-Level_Deep_Pattern_2015_CVPR_paper",
    "abstract": "Mid-level visual element discovery aims to find clusters of image patches that are both representative and discriminative. This work studies this problem from the perspective of pattern mining, leveraging Convolutional Neural Networks (CNNs). The authors find that activations extracted from the first fully-connected layer of a CNN have properties that enable seamless integration with pattern mining via association rule mining. The discovered patterns result in visually and semantically consistent image patches. The approach outperforms previous methods and matches or exceeds recent CNN-based approaches on scene and object classification tasks, utilizing fewer elements.\n\n---TOPIC---\nMid-level Visual Element Discovery\nConvolutional Neural Networks (CNNs)\nAssociation Rule Mining\nPattern Mining\nImage Classification",
    "topics": [],
    "references": [],
    "author_details": [
      {
        "name": "Yao Li",
        "affiliation": "The University of Adelaide, NICTA",
        "email": "Not available"
      },
      {
        "name": "Lingqiao Liu",
        "affiliation": "The University of Adelaide",
        "email": "Not available"
      },
      {
        "name": "Chunhua Shen",
        "affiliation": "The University of Adelaide, Australian Centre for Robotic Vision",
        "email": "Not available"
      },
      {
        "name": "Anton van den Hengel",
        "affiliation": "The University of Adelaide, Australian Centre for Robotic Vision",
        "email": "Not available"
      }
    ]
  },
  {
    "title": "Handling Motion Blur in Multi-Frame Super-Resolution\n---AUTHOR---\nZiayang Ma\nRenjie Liao\nXin Tao\nLi Xu\nJiaya Jia\nEnhua Wu",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Ma_Handling_Motion_Blur_2015_CVPR_paper.pdf",
    "id": "Ma_Handling_Motion_Blur_2015_CVPR_paper",
    "abstract": "Ubiquitous motion blur easily fails multi-frame super-resolution (MFSR). Our method proposed in this paper tackles this issue by optimally searching least blurred pixels in MFSR. An EM framework is proposed to guide residual blur estimation and high-resolution image reconstruction. To suppress noise, we employ a family of sparse penalties as natural image priors, along with an effective solver. Theoretical analysis is performed on how and when our method works. The relationship between estimation errors of motion blur and the quality of input images is discussed. Our method produces sharp and higher-resolution results given input of challenging low-resolution noisy and blurred sequences.",
    "topics": [
      "Multi-frame Super-Resolution (MFSR)",
      "Motion Blur Estimation",
      "Expectation-Maximization (EM) Framework",
      "Image Deblurring",
      "Sparse Image Priors"
    ],
    "references": [
      {
        "citation": "[S. Cho and S. Lee. Fast motion deblurring. ACM Transactions on Graphics (TOG), 28(5):145, 2009.] - This paper likely forms a core contribution to the field of motion deblurring, given its focus on a fundamental technique."
      },
      {
        "citation": "[S. Cho, J. Wang, and S. Lee. Video deblurring for hand-held cameras using patch-based synthesis. ACM Transactions on Graphics (TOG), 31(4):64, 2012.] - Addresses a practical application of deblurring, specifically for common scenarios like hand-held cameras."
      },
      {
        "citation": "[S. Cho, H. Cho, Y.-W. Tai, and S. Lee. Registration based non-uniform motion deblurring. Computer Graphics Forum, 31(7):2183–2192, 2012.] - Deals with a more complex scenario of non-uniform motion, which is common in real-world video."
      },
      {
        "citation": "[J. Shi and C. Tomasi. Good features to Track. In CVPR, pages 593–593, 1994.] - This is a foundational paper on feature detection, which is often used in image registration and motion estimation, crucial steps in deblurring."
      },
      {
        "citation": "[M. Irani and S. Peleg. Improving resolution by image registration. CVGIP: Graphical models and image processing, 53(3):231–239, 1991.] - A seminal work on image registration, a key component in many deblurring and super-resolution techniques."
      },
      {
        "citation": "[S. Cho, J. Wang, and S. Lee. Handling outliers in non-blind image deconconvolution. In ICCV, pages 495–502, 2011.] - Addresses a common problem in deconvolution, making the techniques more robust."
      },
      {
        "citation": "[H. Bay, T. Tuytelaars, and L. Van Gool. Surf: Speeded up robust features. In ECCV, pages 404–417, 2006.] - SURF is a popular feature detection algorithm often used in image registration and related tasks."
      },
      {
        "citation": "[S. C. Park, M. K. Park, and M. G. Kang. Super-resolution image reconstruction: a technical overview. Signal Processing Magazine, IEEE, 20(3):21–36, 2003.] - Provides a broader context for understanding super-resolution, a related technique."
      },
      {
        "citation": "[T. Michaeli and M. Irani. Nonparametric blind super-resolution. In ICCV, pages 945–952, 2013.] -  Addresses blind super-resolution, a more challenging problem."
      },
      {
        "citation": "[C. Liu et al. Beyond pixels: exploring new representation-s and applications for motion analysis. PhD thesis, Massachusetts Institute of Technology, 2009.] - A thesis likely covering a wide range of motion analysis techniques, potentially including deblurring."
      }
    ],
    "author_details": [
      {
        "name": "Ziayang Ma",
        "affiliation": "University of Chinese Academy of Sciences & State Key Lab. of Computer Science, Inst. of Software, CAS",
        "email": "[Not available in the provided text]"
      },
      {
        "name": "Renjie Liao",
        "affiliation": "The Chinese University of Hong Kong",
        "email": "[Not available in the provided text]"
      },
      {
        "name": "Xin Tao",
        "affiliation": "The Chinese University of Hong Kong",
        "email": "[Not available in the provided text]"
      },
      {
        "name": "Li Xu",
        "affiliation": "The Chinese University of Hong Kong",
        "email": "[Not available in the provided text]"
      },
      {
        "name": "Jiaya Jia",
        "affiliation": "The Chinese University of Hong Kong",
        "email": "[Not available in the provided text]"
      },
      {
        "name": "Enhua Wu",
        "affiliation": "University of Chinese Academy of Sciences & State Key Lab. of Computer Science, Inst. of Software, CAS, FST, University of Macau",
        "email": "[Not available in the provided text]"
      }
    ]
  },
  {
    "title": "From Captions to Visual Concepts and Back\n---AUTHORs---\nHao Fang\nSaurabh Gupta\nForrest Iandola\nRupesh K. Srivastava\nLi Deng\nPiotr Dollár\nJianfeng Gao\nXiaodong He\nMargaret Mitchell\nJohn C. Platt\nC. Lawrence Zitnick\nGeoffrey Zweig",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Fang_From_Captions_to_2015_CVPR_paper.pdf",
    "id": "Fang_From_Captions_to_2015_CVPR_paper",
    "abstract": "This paper presents a novel approach for automatically generating image descriptions: visual detectors, language models, and multimodal similarity models learnt directly from a dataset of image captions. We use multiple instance learning to train visual detectors for words that commonly occur in captions, including many different parts of speech such as nouns, verbs, and adjectives. The word detector outputs serve as conditional inputs to a maximum-entropy language model. The language model learns from a set of over 400,000 image descriptions to capture the statistics of word usage. We capture global semantics by re-ranking caption candidates using sentence-level features and a deep multimodal similarity model. Our system is state-of-the-art on the ofﬁcial Microsoft COCO benchmark, producing a BLEU-4 score of 29.1%. When human judges compare the system captions to ones written by other people on our held-out test set, the system captions have equal or better quality 34% of the time.",
    "topics": [
      "Image captioning",
      "Visual detectors",
      "Language models",
      "Multimodal similarity",
      "Weakly-supervised learning"
    ],
    "references": [
      {
        "citation": "[Banerjee, S. and Lavie, A. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, 2005.]"
      },
      {
        "citation": "[Berger, A. L., Pietra, S. A. D., and Pietra, V. J. D. A maximum entropy approach to natural language processing. Computational Linguistics, 1996.]"
      },
      {
        "citation": "[Kiros, R., Zemel, R., and Salakhutdinov, R. Multimodal neural language models. In NIPS Deep Learning Workshop, 2013.]"
      },
      {
        "citation": "[Krizhevsky, A., Sutskever, I., and Hinton, G. E. ImageNet classification with deep convolutional neural networks. In NIPS, 2012.]"
      },
      {
        "citation": "[Kulkarni, G., Premraj, V., Dhar, S., Li, S., Choi, Y., Berg, A. C., and Berg, T. L. Baby talk: Understanding and generating simple image descriptions. In CVPR, 2011.]"
      },
      {
        "citation": "[Chen, X., Shrivastava, A., and Gupta, A. Neil: Extracting visual knowledge from web data. In ICCV, 2013.]"
      },
      {
        "citation": "[Chen, X. and Zitnick, C. L. Mind’s eye: A recurrent visual representation for image caption generation. CVPR, 2015.]"
      },
      {
        "citation": "[Deng, J., Dong, W., Socher, R., Li-J., Li, K., and Fei-Fei, L. ImageNet: A large-scale hierarchical image database. In CVPR, 2009.]"
      },
      {
        "citation": "[Divvala, S., Farhad, A., and Guestrin, C. Learning everything about anything: Webly-supervised visual concept learning. In CVPR, 2014.]"
      },
      {
        "citation": "[Elliott, D. and Keller, F. Comparing automatic evaluation measures for image description. In ACL, 2014.]"
      }
    ],
    "author_details": [
      {
        "name": "Hao Fang",
        "affiliation": "University of Washington",
        "email": "Not available"
      },
      {
        "name": "Saurabh Gupta",
        "affiliation": "University of California at Berkeley",
        "email": "Not available"
      },
      {
        "name": "Forrest Iandola",
        "affiliation": "University of California at Berkeley",
        "email": "Not available"
      },
      {
        "name": "Rupesh K. Srivastava",
        "affiliation": "IDSIA",
        "email": "Not available"
      },
      {
        "name": "Li Deng",
        "affiliation": "Microsoft Research",
        "email": "Not available"
      },
      {
        "name": "Piotr Dollár",
        "affiliation": "Microsoft Research",
        "email": "Not available"
      },
      {
        "name": "Jianfeng Gao",
        "affiliation": "Microsoft Research",
        "email": "Not available"
      },
      {
        "name": "Xiaodong He",
        "affiliation": "Microsoft Research",
        "email": "Not available"
      },
      {
        "name": "Margaret Mitchell",
        "affiliation": "Microsoft Research",
        "email": "Not available"
      },
      {
        "name": "John C. Platt",
        "affiliation": "Microsoft Research",
        "email": "Not available"
      },
      {
        "name": "C. Lawrence Zitnick",
        "affiliation": "Microsoft Research",
        "email": "Not available"
      },
      {
        "name": "Geoffrey Zweig",
        "affiliation": "Microsoft Research",
        "email": "Not available"
      }
    ]
  },
  {
    "title": "Tree Quantization for Large-Scale Similarity Search and Classification\n---AUTHOR---\nArtem Babenko\nVictor Lempitsky",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Babenko_Tree_Quantization_for_2015_CVPR_paper.pdf",
    "id": "Babenko_Tree_Quantization_for_2015_CVPR_paper",
    "abstract": "This paper introduces Tree Quantization (TQ), a novel vector encoding scheme designed for large-scale similarity search and classification. TQ, similar to Product Quantization (PQ) and Additive Quantization (AQ), utilizes a set of codebooks and encodes vectors as a sum of codewords. However, unlike AQ, TQ employs a tree-based dynamic programming approach to jointly optimize the coding tree structure and codebooks, minimizing compression error. Experiments demonstrate that TQ achieves a favorable balance between fast encoding and state-of-the-art accuracy in terms of compression error, retrieval performance, and image classification error, outperforming both PQ and AQ in encoding time while maintaining comparable coding accuracy.",
    "topics": [
      "Vector Quantization",
      "Product Quantization",
      "Additive Quantization",
      "Tree Quantization",
      "Large-Scale Similarity Search"
    ],
    "references": [
      {
        "citation": "[Gurobi optimizer. http://www.gurobi.com/, 2013.] - This is likely used for optimization within the paper."
      },
      {
        "citation": "[A. Babenko and V. Lempitsky. The inverted multi-index. In CVPR, 2012.] - Relevant given the focus on quantization and coding."
      },
      {
        "citation": "[A. Bergamo and L. Torresani. Meta-class features for large-scale object categorization on a budget. In CVPR, pages 3085–3092, 2012.] - Provides context for object categorization and potentially related techniques."
      },
      {
        "citation": "[A. Bergamo, L. Torresani, and A. W. Fitzgibbon. Picodes: Learning a compact code for novel-category recognition. In NIPS, pages 2088–2096, 2011.] - Relates to compact coding and novel category recognition, a key theme."
      },
      {
        "citation": "[T. Ge, K. He, Q. Ke, and J. Sun. Optimized product quantization for approximate nearest neighbor search. In CVPR, 2013.] - This is a direct comparison point for the proposed method (Tree Quantization)."
      },
      {
        "citation": "[H. J´egou, M. Douze, and C. Schmid. Product quantization for nearest neighbor search. TPAMI, 33(1), 2011.] - Foundational work on product quantization, a core technique."
      },
      {
        "citation": "[Y. Gong and S. Lazebnik. Iterative quantization: A procrustean approach to learning binary codes. In CVPR, 2011.] - Another relevant technique for learning binary codes."
      },
      {
        "citation": "[A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks. In NIPS, pages 1106–1114, 2012.] - Provides context for large-scale image classification and the use of deep learning."
      },
      {
        "citation": "[D. G. Lowe. Distinctive image features from scale-invariant keypoints. IJCV, 60(2), 2004.] -  SIFT is a common feature descriptor, and this is the original paper."
      },
      {
        "citation": "[Y. Kalantidis and Y. Avrithis. Locally optimized product quantization for approximate nearest neighbor search. In CVPR. IEEE, 2014.] - Another variation of product quantization."
      }
    ],
    "author_details": [
      {
        "name": "Artem Babenko",
        "affiliation": "Yandex",
        "email": "arbabenko@yandex-team.ru"
      },
      {
        "name": "Victor Lempitsky",
        "affiliation": "Skolkovo Institute of Science and Technology (Skoltech)",
        "email": "lempitsky@skoltech.ru"
      }
    ]
  },
  {
    "title": "FAemb: a function approximation-based embedding method for image retrieval\n---AUTHOR---\nThanh-Toan Do\nQuang D. Tran\nNgai-Man Cheung",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Do_FAemb_A_Function_2015_CVPR_paper.pdf",
    "id": "Do_FAemb_A_Function_2015_CVPR_paper",
    "abstract": "This paper investigates the relationship between linear approximation of nonlinear functions and state-of-the-art feature representation used in image retrieval. It reveals that VLAD is a simplified version of TLCC, leading to a new embedding method that generalizes both. The proposed method, FAemb, achieves a performance boost over existing methods on standard image retrieval benchmarks by focusing on the embedding step, which maps local descriptors to a higher-dimensional representation. The paper also introduces coordinate coding and its relation to TLCC.",
    "topics": [
      "Image Retrieval",
      "Function Approximation",
      "Embedding Methods",
      "Coordinate Coding",
      "VLAD/TLCC"
    ],
    "references": [
      {
        "citation": "[R. Aranandjelovic and A. Zisserman, Three things everyone should know to improve object retrieval, CVPR, 2012]"
      },
      {
        "citation": "[R. Aranandjelovic and A. Zisserman, All about VLAD, CVPR, 2013]"
      },
      {
        "citation": "[H. Jégou, M. Douze, and C. Schmid, Improving bag-of-features for large scale image search, IJCV, 2010]"
      },
      {
        "citation": "[H. Jégou, F. Perronnin, M. Douze, J. Sánchez, P. Pérez, and C. Schmid, Aggregating local images descriptors into compact codes, PAMI, 2012]"
      },
      {
        "citation": "[S. Lazebnik, C. Schmid, and J. Ponce, Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories, CVPR, 2006]"
      },
      {
        "citation": "[J. Wang, J. Yang, K. Yu, F. Lv, T. S. Huang, and Y. Gong, Locality-constrained linear coding for image classification, CVPR, 2010]"
      },
      {
        "citation": "[J. Sivic and A. Zisserman, Video Google: A text retrieval approach to object matching in videos, ICCV, 2003]"
      },
      {
        "citation": "[H. Jégou and O. Chum, Negative evidences and co-ocurencies in image retrieval: The benefit of PCA and whitening, ECCV, 2012]"
      },
      {
        "citation": "[J. Philbin, O. Chum, M. Isard, J. Sivic, and A. Zisser-man, Object retrieval with large vocabularies and fast spatial matching, CVPR, 2007]"
      },
      {
        "citation": "[D. Picard and P. H. Gosselin, Improving image similarity with vectors of locally aggregated tensors, ICIP, 2011]"
      }
    ],
    "author_details": [
      {
        "name": "Thanh-Toan Do",
        "affiliation": "Singapore University of Technology and Design",
        "email": "thanhtoan.do@sutd.edu.sg"
      },
      {
        "name": "Quang D. Tran",
        "affiliation": "Singapore University of Technology and Design",
        "email": "quang.tran@sutd.edu.sg"
      },
      {
        "name": "Ngai-Man Cheung",
        "affiliation": "Singapore University of Technology and Design",
        "email": "ngaiman.cheung@sutd.edu.sg"
      }
    ]
  },
  {
    "title": "Dual Domain Filters Based Texture and Structure Preerving Image Non-blind Deconvolution\n---AUTHOR---\nHang Yang\nMing Zhu\nYan Niu\nYujing Guan\nZhongbo Zhang",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Yang_Dual_Domain_Filters_2015_CVPR_paper.pdf",
    "id": "Yang_Dual_Domain_Filters_2015_CVPR_paper",
    "abstract": "Image deconvolution aims to recover a sharp image from a blurry one. A key challenge is preserving fine-scale texture structures while removing blur and noise. This paper proposes a novel approach for efficient image deconvolution based on dual domain filters. The method utilizes a novel rolling guidance filter to ensure proper texture/structure separation and a short-time Fourier transform to recover textures while removing noise. Experimental results demonstrate that the proposed algorithm outperforms state-of-the-art deconvolutions in both quantitative measures and visual perception quality.\n\n---TOPIC---\nImage Deconvolution\nDual Domain Filters\nTexture Preservation\nShort-Time Fourier Transform (STFT)\nNon-Blind Deconvolution",
    "topics": [],
    "references": [
      {
        "citation": "[Hansen, P. C. Rank-Deﬁcient and Discrete Ill-Posed Problems: Numerical Aspects of Linear Inversion. Philadelphia, PA: SIAM, 1998.]"
      },
      {
        "citation": "[Neelamani, R., Choi, H., & Baraniuk, R. G. ForWaRD: Fourier-wavetelet regularized deconvolution for ill-conditioned systems. IEEE Trans. Signal Process., 52(2):418-433, 2004.]"
      },
      {
        "citation": "[Guerrero-Colon, J. A., Mancera, L., & Portilla, J. Image restoration using space-variant Gaussian scale mixtures in overcomplete pyramids. IEEE Trans. Image Process., 17(1):27-41, 2007.]"
      },
      {
        "citation": "[Foi, A., Dabov, K., Katkovnik, V., & Egiazarian, K. Shape-adaptive DCT for denoising and image reconstruction. Soc. Photo-Optical Instrumentation Engineers, 6064:203-214, 2006.]"
      },
      {
        "citation": "[Dabove, K., Foi, A., Katkovnik, V., & Egiazarian, K. Image restoration by sparse 3D transform-domain collaborative filtering. Soc. Photo-Optical Instrumentation Engineers, 6812:6, 2008.]"
      },
      {
        "citation": "[Rudin, L., Osher, S., & Fatemi, E. Nonlinear total variation based noise removal algorithms. Physica D, 60:259-268, 1992.]"
      },
      {
        "citation": "[Portilla, J. Image restoration through l0 analysis-based sparse optimization in tight frames. IEEE Int. Conf Image Processing, 3909-3912, 2009.]"
      },
      {
        "citation": "[Xue, F., Luisier, F., & Blu, T. Multi-Wiener SURE-LET Deconvolution. IEEE Trans. Image Process., 22(5):1954-1968, 2013.]"
      },
      {
        "citation": "[Jia, C., & Evans, B. Patch based image deconvolution via joint modeling of sparse priors. IEEE Int. Conf Image Processing, 681-684, 2011.]"
      },
      {
        "citation": "[Yuan, L., Sun, J., Quan, L., & Shum, H.-Y. Image deblurring with blurred/noisy image pairs. ACM Trans. on Graph., 26:3, 2007.]"
      }
    ],
    "author_details": [
      {
        "name": "Hang Yang",
        "affiliation": "Changchun Institute of Optics, Fine Mechanics and Physics, Chinese Academy of Science",
        "email": "yanghang09@mails.jlu.edu.cn"
      },
      {
        "name": "Ming Zhu",
        "affiliation": "Changchun Institute of Optics, Fine Mechanics and Physics, Chinese Academy of Science",
        "email": "zhumingca@163.com"
      },
      {
        "name": "Yan Niu",
        "email": "niuyan@jlu.edu.cn"
      },
      {
        "name": "Yujing Guan",
        "affiliation": "Jilin University",
        "email": "guanyj@jlu.edu.cn"
      },
      {
        "name": "Zhongbo Zhang",
        "affiliation": "Jilin University",
        "email": "zhongbozhang@jlu.edu.cn"
      }
    ]
  },
  {
    "title": "Large-Scale and Drift-Free Surface Reconstruction Using Online Subvolume Registration\n---AUTHOR---\nNicola Fioraio\nJonathan Taylor\nAndrew Fitzgibbon\nLuigi Di Stefano\nShahram Izadi",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Fioraio_Large-Scale_and_Drift-Free_2015_CVPR_paper.pdf",
    "id": "Fioraio_Large-Scale_and_Drift-Free_2015_CVPR_paper",
    "abstract": "Depth cameras have enabled the commoditization of 3D digitization, but large-scale scans suffer from distortions and drift due to accumulated pose estimation errors. Existing techniques require expensive offline optimization, multiple passes of depth data, RGB data, or explicit user-defined loop closures. This paper presents a method that addresses these issues, supporting online model correction without reprocessing or storing depth data. The method corrects large 3D models in minutes rather than hours, doesn't require explicit loop closures, and relies solely on depth data. The authors demonstrate the method's effectiveness on large-scale scenes, highlighting the lack of error and drift, and compare it to state-of-the-art techniques, showcasing its ability to perform dense surface reconstruction \"in the dark.\"",
    "topics": [
      "Large-scale 3D reconstruction",
      "Online model correction",
      "Depth camera registration",
      "Drift-free surface reconstruction",
      "Subvolume registration"
    ],
    "references": [
      {
        "citation": "[S. Agarwal, K. Mierle, and Others. Ceres solver. http://ceres-solver.org.] - This is a widely used optimization library, crucial for many SLAM and 3D reconstruction pipelines."
      },
      {
        "citation": "[P. J. Besl and H. D. McKay. A method for registration of 3-d shapes. Pattern Analysis and Machine Intelligence (PAMI), IEEE Trans. on, 14(2):239–256, 1992.] - A foundational paper on Iterative Closest Point (ICP), a core algorithm for point cloud registration."
      },
      {
        "citation": "[J. Chen, D. Bautembach, and S. Izadi. Scalable real-time volumetric surface reconstruction. ACM Transaction on Graphics (TOG), 32(4), July 2013.] - Presents a scalable method for real-time volumetric surface reconstruction, a key technique for 3D mapping."
      },
      {
        "citation": "[B. Curless and M. Levoy. A volumetric method for building complex models from range images. In SIGGRAPH Intl. Conf., New Orleans, LA, USA, 1996.] - Introduces a pioneering volumetric approach to 3D modeling from range images."
      },
      {
        "citation": "[S. Izadi, D. Kim, O. Hilliges, D. Molyneaux, R. Newcombe, P. Kohli, J. Shotton, S. Hodges, D. Freeman, A. Davison, and A. Fitzgibbon. KinectFusion: Real-time 3d reconstruction and interaction using a moving depth camera. In ACM Symposium on User Interface Software and Technology, Santa Barbara, CA, USA, October 2011.] - Introduces KinectFusion, a significant advancement in real-time 3D reconstruction."
      },
      {
        "citation": "[K. Levenberg. A method for the solution of certain non-linear problems in least squares. Quarterly Journal of Applied Mathmatics, II(2):164–168, 1944.] - Introduces the Levenberg-Marquardt algorithm, a fundamental optimization technique used in many of the referenced papers."
      },
      {
        "citation": "[W. Lorensen and H. Cline. Marching cubes: A high resolution 3d surface construction algorithm. SIGGRAPH Intl. Conf., 21(4):163–170, July 1987.] - Describes the Marching Cubes algorithm, a standard method for generating 3D surfaces from volumetric data."
      },
      {
        "citation": "[R. A. Newcombe, S. Izadi, O. Hilliges, D. Molyneaux, D. Kim, A. J. Davison, P. Kohli, J. Shotton, S. Hodges, and A. Fitzgibbon. KinectFusion: Real-time dense surface mapping and tracking. In Mixed and Augmented Reality (ISMAR), IEEE and ACM Int’l Symp. on, pages 127–136, Washington, DC, USA, 2011.] - A more detailed description of KinectFusion."
      },
      {
        "citation": "[H. Strasdat, A. J. Davison, J. Montiel, and K. Konolige. Double window optimisation for constant time visual SLAM. In Computer Vision (ICCV), IEEE Int’l Conf. on, pages 2352–2359, Los Alamitos, CA, USA, 2011.] - Addresses optimization techniques for visual Simultaneous Localization and Mapping (SLAM)."
      },
      {
        "citation": "[M. Niessner, M. Zollhofer, S. Izadi, and M. Stamminger. Real-time 3d reconstruction at scale using voxel hashing. ACM Trans. Graph., 32(6), November 2013.] - Presents a method for real-time 3D reconstruction at scale using voxel hashing."
      }
    ],
    "author_details": [
      {
        "name": "Nicola Fioraio",
        "affiliation": "CVLab - CSE, University of Bologna",
        "email": "nicola.fioraio@unibo.it"
      },
      {
        "name": "Jonathan Taylor",
        "affiliation": "Microsoft Research",
        "email": "jota@microsoft.com"
      },
      {
        "name": "Andrew Fitzgibbon",
        "affiliation": "Microsoft Research",
        "email": "awf@microsoft.com"
      },
      {
        "name": "Luigi Di Stefano",
        "affiliation": "CVLab - CSE, University of Bologna",
        "email": "luigi.distefano@unibo.it"
      },
      {
        "name": "Shahram Izadi",
        "affiliation": "Microsoft Research",
        "email": "shahrami@microsoft.com"
      }
    ]
  },
  {
    "title": "Image Speciﬁcity\n---AUTHOR---\nMainak Jas\n---AUTHOR---\nDevi Parikh",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Jas_Image_Specificity_2015_CVPR_supplemental.pdf",
    "id": "Jas_Image_Specificity_2015_CVPR_supplemental",
    "abstract": "This paper investigates the concept of \"image specificity,\" which refers to how well an image's visual content aligns with its associated captions. The authors explore the correlation between human-measured and automated specificity scores, analyze image properties that influence specificity, and demonstrate the feasibility of predicting specificity from image features alone. They train a ν-SVR model on various datasets (MEM-5S, PASPAL-50S, ABSTRACT-50S) and show that semantic features are more effective for predicting specificity than low-level features, particularly in the ABSTRACT-50S dataset. The study also details the automated computation of sentence similarity used to estimate image specificity.\n\n---TOPIC---\nImage Specificity\nAutomated Sentence Similarity\nν-SVR Prediction\nDataset Analysis (MEM-5S, PASPAL-50S, ABSTRACT-50S)\nImage Feature Correlation",
    "topics": [],
    "references": [],
    "author_details": [
      {
        "name": "Mainak Jas",
        "affiliation": "Aalto University",
        "email": "mainak.jas@aalto.fi"
      },
      {
        "name": "Devi Parikh",
        "affiliation": "Virginia Tech",
        "email": "parikh@vt.edu"
      }
    ]
  },
  {
    "title": "Understanding Tools: Task-Oriented Object Modeling, Learning and Recognition\n---AUTHOR---\nYixin Zhu\n---AUTHOR---\nYibiao Zhao\n---AUTHOR---\nSong-Chun Zhu",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Zhu_Understanding_Tools_Task-Oriented_2015_CVPR_paper.pdf",
    "id": "Zhu_Understanding_Tools_Task-Oriented_2015_CVPR_paper",
    "abstract": "This paper presents a new framework for understanding objects as \"tools\" used to accomplish tasks. The framework, termed task-oriented modeling, learning, and recognition, aims to understand the underlying functions, physics, and causality involved in using objects. Each object is represented by a generative spatio-temporal representation including affordance, functional basis, imagined actions, and physical concepts. The algorithm learns from a single RGB-D video of a human performing a task and then infers the best object choice and associated parameters for new tasks and objects, enabling generalization beyond typical object categories. The approach frames object recognition as reasoning about physical mechanisms rather than simply memorizing appearance examples.\n\n---TOPICCS---\nTask-oriented object recognition\nGenerative spatio-temporal representation\nPhysical reasoning\nTool use\nRGB-D video learning",
    "topics": [],
    "references": [],
    "author_details": [
      {
        "name": "Yixin Zhu",
        "affiliation": "University of California, Los Angeles",
        "email": "yixin.zhu@ucla.edu"
      },
      {
        "name": "Yibiao Zhao",
        "affiliation": "University of California, Los Angeles",
        "email": "ybzhao@ucla.edu"
      },
      {
        "name": "Song-Chun Zhu",
        "affiliation": "University of California, Los Angeles",
        "email": "sczhu@stat.ucla.edu"
      }
    ]
  },
  {
    "title": "Temporally Coherent Interpretations for Long Videos Using Pattern Theory\n---AUTHORSS---\nFillipe Souza\nSudeep Sarkar\nAnuj Srivastava\nJingyong Su",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Souza_Temporally_Coherent_Interpretations_2015_CVPR_paper.pdf",
    "id": "Souza_Temporally_Coherent_Interpretations_2015_CVPR_paper",
    "abstract": "This paper presents an extension of a pattern-theoretic approach for interpreting long videos, building upon previous work that uses flexible graphs to represent interactions between actors and objects. The core limitation of prior methods was their rigidity in graph structure. This extension incorporates temporal bonds across individual actions to enable semantic interpretations of longer videos, improving scene understanding by favoring globally superior solutions over locally optimal ones. The approach was verified on the YouCook dataset, demonstrating performance increases of approximately 70% and improved robustness to classification errors in complex video scenarios.",
    "topics": [
      "Long video understanding",
      "Pattern theory",
      "Graph-theoretical methods",
      "Temporal modeling",
      "Semantic interpretation"
    ],
    "references": [
      {
        "citation": "[U. Grenander and M. I. Miller, *Pattern theory: from representation to inference*, 2007.] - This appears to be a foundational text for the work, frequently cited and likely provides the theoretical basis for many approaches."
      },
      {
        "citation": "[H. Pirsiavash and D. Ramanan, “Detecting activities of daily living in ﬁrst-person camera views,” *IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, 2012.] - Frequently cited and relevant to activity recognition from a specific viewpoint."
      },
      {
        "citation": "[M. R. Amer and S. Todorovic, “Sum-product networks for modeling activities with stochastic structure,” *IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, 2012.] -  Cited multiple times, suggesting a significant contribution to modeling activities."
      },
      {
        "citation": "[A. Fathi and J. M. Rehg, “Modeling actions through state changes,” *IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, 2013.] - Cited multiple times, indicating its relevance to action modeling."
      },
      {
        "citation": "[S. Bhattacharya, M. M. Kalayeh, R. Sukthankar, and M. Shah, “Recognition of complex events: Exploiting temporal dynamics between underlying concepts,” *IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, 2014.] - Cited multiple times, suggesting a significant contribution to complex event recognition."
      },
      {
        "citation": "[M. S. Ryoo and J. K. Aggarwal, “Semantic representation and recognition of continued and recursive human activities,” *International Journal of Computer Vision (IJCV)*, vol. 82, no. 1, pp. 1–24, 2009.] - Cited multiple times, suggesting a significant contribution to understanding complex activities."
      },
      {
        "citation": "[P. Bojanowski, R. Lajugie, F. Bach, I. Laptev, J. Ponce, C. Schmid, and J. Sivic, “Weakly supervised action labeling in videos under ordering constraints,” *European Conference on Computer Vision (ECCV)*, 2014.] - Cited multiple times, suggesting its relevance to action labeling."
      },
      {
        "citation": "[K. Hilde, A. Arslan, and T. Serre, “The language of actions: Recovering the syntax and semantics of goal-directed human activities,” *IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, 2014.] - Cited multiple times, suggesting its relevance to understanding the structure of actions."
      },
      {
        "citation": "[I. Laptev, M. Marszalek, C. Schmid, and B. Rojenfeld, “Learning realistic human actions from movies,” *IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, 2008.] - Cited multiple times, suggesting its relevance to learning actions from video data."
      },
      {
        "citation": "[B. Laxton, J. Lim, and D. Kriegman, “Leveraging temporal, contextual and ordering constraints for recognizing complex activities in video,” *IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, 2007.] - Cited multiple times, suggesting its relevance to complex activity recognition."
      }
    ],
    "author_details": [
      {
        "name": "Fillipe Souza",
        "affiliation": "University of South Florida",
        "email": "fillipe@mail.usf.edu"
      },
      {
        "name": "Suddeep Sarkar",
        "affiliation": "University of South Florida",
        "email": "sarkar@cse.usf.edu"
      },
      {
        "name": "Anuj Srivastava",
        "affiliation": "Florida State University",
        "email": "anuj@stat.fsu.edu"
      },
      {
        "name": "Jingyong Su",
        "affiliation": "Texas Tech University",
        "email": "jingyong.su@ttu.edu"
      }
    ]
  },
  {
    "title": "Computing Similarity Transformations from Only Image Correspondences\n---AUTHOR---\nChris Sweeney\nLaurent Kneip\nTobias H¨ollerer\nMatthew Turk",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Sweeney_Computing_Similarity_Transformations_2015_CVPR_paper.pdf",
    "id": "Sweeney_Computing_Similarity_Transformations_2015_CVPR_paper",
    "abstract": "We propose a novel solution for computing the relative pose between two generalized cameras that includes reconciling the internal scale of the generalized cameras. This approach can be used to compute a similarity transformation between two coordinate systems, making it useful for loop closure in visual odometry and registering multiple structure from motion reconstructions together. In contrast to alternative similarity transformation methods, our approach uses 2D-2D image correspondences thus is not subject to the depth uncertainty that often arises with 3D points. We utilize a known vertical direction (which may be easily obtained from IMU data or vertical vanishing point detection) of the generalized cameras to solve the generalized relative pose and scale problem as an efficient Quadratic Eigenvalue Problem. To our knowledge, this is the first method for computing similarity transformations that does not require any 3D information. Our experiments on synthetic and real data demonstrate that this leads to improved performance compared to methods that use 3D-3D or 2D-3D correspondences, especially as the depth of the scene increases.",
    "topics": [
      "Generalized cameras",
      "Similarity transformation",
      "Visual odometry",
      "Structure from Motion",
      "Quadratic Eigenvalue Problem"
    ],
    "references": [
      {
        "citation": "[Besl, P. J., & McKay, N. D. (1992). A method for registration of 3-d shapes. *IEEE Transactions on Pattern Analysis and Machine Intelligence*, *14*(2), 239–256.]"
      },
      {
        "citation": "[Courchay, J., Dalalyan, A., Keriven, R., & Sturm, P. (2010). Exploiting loops in the graph of trifocal tensors for calibrating a network of cameras. In *European Conference on Computer Vision* (pp. 85–99). Springer.]"
      },
      {
        "citation": "[Davison, A. J., Reid, I. D., Molton, N. D., & Stasse, O. (2007). Monoslam: Real-time single camera slam. *IEEE Transactions on Pattern Analysis and Machine Intelligence*, *29*(6), 1052–1067.]"
      },
      {
        "citation": "[Eade, E., & Drummond, T. (2008). Unified loop closing and recovery for real time monocular slam. In *Proc. British Machine Vision Conference* (Vol. 13, pp. 136). Citeseer.]"
      },
      {
        "citation": "[Fraundorfer, F., Tanskanen, P., & Pollefeys, M. (2010). A minimal case solution to the calibrated relative pose problem for the case of two known orientation angles. In *Proc. of the European Conference on Computer Vision* (pp. 269–282). Springer.]"
      },
      {
        "citation": "[Grossberg, M. D., & Nayar, S. K. (2001). A general imaging model and a method for finding its parameters. In *Proc. of IEEE Intn’l. Conf. on Computer Vision*.]"
      },
      {
        "citation": "[Kim, J.-H., Li, H., & Hartley, R. (2010). Motion estimation for nonoverlapping multica-mera rigs: Linear algebraic and l ge-ometric solutions. *IEEE Transactions on Pattern Analysis and Machine Intelligence*, *32*(6), 2010.]"
      },
      {
        "citation": "[Klopschitz, M., Zach, C., Irschara, A., & Schmalstieg, D. (2008). Generalized detection and merging of loop closures for video sequences. In *Proc. 3D Data Processing, Visualization, and Transmission*.]"
      },
      {
        "citation": "[Kneip, L., & Li, H. (2014). Efﬁcient computation of relative pose for multi-camera systems. In *Proc. of IEEE Conference on Computer Vision and Pattern Recognition* (pp. 1–8). IEEE.]"
      },
      {
        "citation": "[Kneip, L., Scaramuzza, D., & Siegwart, R. (2011). A novel parametrization of the perspective-three-point problem for a direct computation of absolute camera position and orientation. In *Proc. IEEE Conference on Computer Vision and Pattern Recognition* (pp. 2969–2976). IEEE.]"
      }
    ],
    "author_details": [
      {
        "name": "Chris Sweeney",
        "affiliation": "University of California Santa Barbara",
        "email": "cmsweeney@cs.ucsb.edu"
      },
      {
        "name": "Laurent Kneip",
        "affiliation": "Research School of Engineering, Australian National University",
        "email": "laurent.kneip@anu.edu.au"
      },
      {
        "name": "Tobias H¨ollerer",
        "affiliation": "University of California Santa Barbara",
        "email": "holl@cs.ucsb.edu"
      },
      {
        "name": "Matthew Turk",
        "affiliation": "University of California Santa Barbara",
        "email": "mturk@cs.ucsb.edu"
      }
    ]
  },
  {
    "title": "Superpixel Meshes for Fast Edge-Preserving Surface Reconstruction\n---AUTHORs---\nAndr´as B´odis-Szomor´u\nHayko Riemenschneider\nLuc Van Gool",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Bodis-Szomoru_Superpixel_Meshes_for_2015_CVPR_paper.pdf",
    "id": "Bodis-Szomoru_Superpixel_Meshes_for_2015_CVPR_paper",
    "abstract": "Multi-View-Stereo (MVS) methods aim for high detail but often require less. This work proposes a novel surface reconstruction method based on image edges, superpixels, and second-order smoothness constraints, producing meshes comparable to classic MVS surfaces in quality but orders of magnitudes faster. The method performs per-view dense depth optimization directly over sparse 3D Ground Control Points (GCPs), removing the need for view pairing, image rectiﬁcation, and stereo depth estimation, and allowing for full per-image paralleliziation. The resulting meshes are compact and inherently edge-aligned with image gradients, enabling good-quality lightweight per-face flat renderings. Experiments demonstrate superiority in speed and competitive surface quality.\n\n---TOPICCS---\nSurface Reconstruction\nMulti-View Stereo (MVS)\nSuperpixels\nEdge-Preserving Meshes\nStructure-from-Motion (SfM)",
    "topics": [],
    "references": [
      {
        "citation": "[R. Achanta, A. Shaji, K. Smith, A. Lucchi, P. Fua, and S. S¨usstrunk, SLIC superpixels compared to state-of-the-art superpixel methods, PAMI, 34(11):2274–2282, 2012]"
      },
      {
        "citation": "[S. Agarwal, N. Snavely, I. Simon, S. M. Seitz, and R. Szeliski, Building rome in a day, ICCV, 2009]"
      },
      {
        "citation": "[M. Bergh, X. Boix, G. Roig, B. Capitani, and L. Van Gool, SEEDS: Superpixels extracted via energy-driven sampling, ECCV, 2012]"
      },
      {
        "citation": "[A. B´odis-Szomor´u, H. Riemenschneider, and L. Van Gool, Fast, Approximate Piecewise-Planar Modeling Based on Sparse Structure-from-Motion and Superpixels, CVPR, 2014]"
      },
      {
        "citation": "[Y. Boykov and V. Kolmogorov, An experimental comparison of min-cut/max-ﬂow algorithms for energy minimization in vision, PAMI, 26(9):124–1137, 2004]"
      },
      {
        "citation": "[Y. Boykov, O. Veksler, and R. Zabih, Fast approximate energy minimization via graph cuts, PAMI, 23(11):1222–1239, 2001]"
      },
      {
        "citation": "[J. Canny, A computational approach to edge detection, PAMI, 8(6):679–698, 1986]"
      },
      {
        "citation": "[J. C. Carr, R. K. Beatson, J. B. Cherrie, T. J. Mitchell, W. R. Fright, B. C. McCallum, and T. R. Evans, Reconstruction and representation of 3d objects with radial basis functions, SIGGRAPH, 2001]"
      },
      {
        "citation": "[A. Chauve, P. Labatut, and J. Pons, Robust piecewise-planar 3d reconstruction and completion from large-scale unstruc-tured point data, CVPR, 2010]"
      },
      {
        "citation": "[N. Cornelis, K. Cornelis, and L. Van Gool, Fast compact city modeling for navigation pre-visualization, CVPR, 2006]"
      }
    ],
    "author_details": [
      {
        "name": "Andr´as B´odis-Szomor´u",
        "affiliation": "Computer Vision Lab, ETH Zurich, Switzerland",
        "email": "bodis@vision.ee.ethz.ch"
      },
      {
        "name": "Hayko Riemenschneider",
        "affiliation": "Computer Vision Lab, ETH Zurich, Switzerland",
        "email": "hayko@vision.ee.ethz.ch"
      },
      {
        "name": "Luc Van Gool",
        "affiliation": "Computer Vision Lab, ETH Zurich, Switzerland; PSI-VISICS, KU Leuven, Belgium",
        "email": "vangool@vision.ee.ethz.ch"
      }
    ]
  },
  {
    "title": "Projection Metric Learning on Grassmann Manifold with Application to Video based Face Recognition\n---AUTHOR---\nZhiwu Huang\nRuiping Wang\nShiguang Shan\nXilin Chen",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Huang_Projection_Metric_Learning_2015_CVPR_paper.pdf",
    "id": "Huang_Projection_Metric_Learning_2015_CVPR_paper",
    "abstract": "This paper addresses the problem of video-based face recognition by representing videos as linear subspaces residing on a Grassmann manifold. While representing videos as subspaces offers advantages, it introduces challenges related to handling the unique geometric structure of the Grassmann manifold. Existing methods embed the Grassmann manifold into a Hilbert space using a projection metric to leverage Euclidean space algorithms. However, these methods suffer from drawbacks like implicit mappings and high computational costs. To overcome these limitations, the authors propose a novel method that directly learns the projection metric on the Grassmann manifold. This approach performs geometry-aware dimensionality reduction, enabling more favorable classification. Experiments on real-world datasets demonstrate competitive performance.\n\n---TOPIC---\nGrassmann Manifold\nProjection Metric Learning\nVideo-Based Face Recognition\nDimensionality Reduction\nRiemannian Geometry",
    "topics": [],
    "references": [
      {
        "citation": "[Absil, P.-A., Mahony, R., & Sepulchre, R. (2008). Optimization algorithms on matrix manifolds. Princeton University Press.]"
      },
      {
        "citation": "[Jain, S., & Govindu, V. (2013). Efﬁcient higher-order clustering on the grasmannn manifold. In ICCV.]"
      },
      {
        "citation": "[Baudat, G., & Anouar, F. (2000). Generalized discriminant analysis using a kernel approach. Neural computation, 12(10), 2385–2404.]"
      },
      {
        "citation": "[Hamm, J., & Lee, D. D. (2008). Extended grasmannn kernels for subspace-based learning. In NIPS.]"
      },
      {
        "citation": "[Hamm, J., & Lee, D. D. (2008). Grassmann discriminant analysis: a unifying view on subspace-based learning. In ICML.]"
      },
      {
        "citation": "[Harandi, M. T., Salzmann, M., & Hartley, R. (2014). From manifold to manifold: Geometry-aware dimensionality reduction for spd matrices. In ECCV.]"
      },
      {
        "citation": "[Harandi, M. T., Sanderson, M., Shirazi, S., & Lovell, B. C. (2011). Graph embedding discriminant analysis on grasmannnian manifolds for improved image set matching. In CVPR.]"
      },
      {
        "citation": "[Wang, R., & Chen, X. (2009). Manifold discriminant analysis. In CVPR.]"
      },
      {
        "citation": "[Turaga, P., Veeraraghavan, A., Srivastava, A., & Chellappa, R. (2011). Statistical computations on grasmannn and stiefel manifolds for image and video-based recognition. IEEE T-PAMI, 33(11), 2273–2286.]"
      },
      {
        "citation": "[Wang, R., Shan, S., Chen, X., Dai, Q., & Gao, W. (2012). Manifold-Manifold distance and its application to face recognition with image sets. IEEE T-IP, 21(10), 4466–4479.]"
      }
    ],
    "author_details": [
      {
        "name": "Zhiwu Huang",
        "affiliation": "Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing, 100190, China",
        "email": "zhiwu.huang@vcipl.ict.ac.cn"
      },
      {
        "name": "Ruiping Wang",
        "affiliation": "Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing, 100190, China",
        "email": "wangruiping@ict.ac.cn"
      },
      {
        "name": "Shiguang Shan",
        "affiliation": "Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing, 100190, China",
        "email": "sgshan@ict.ac.cn"
      },
      {
        "name": "Xilin Chen",
        "affiliation": "Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing, 100190, China",
        "email": "xlchen@ict.ac.cn"
      }
    ]
  },
  {
    "title": "Blind Optical Aberration Correction by Exploring Geometric and Visual Priors\n---AUTHOR---\nTao Yue\nJinli Suo\nJue Wang\nXun Cao\nQionghai Dai",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Yue_Blind_Optical_Aberration_2015_CVPR_paper.pdf",
    "id": "Yue_Blind_Optical_Aberration_2015_CVPR_paper",
    "abstract": "Optical aberration widely exists in optical imaging systems, especially in consumer-level cameras. In contrast to previous solutions using hardware compensation or pre-calibration, we propose a computational approach for blind aberration removal from a single image, by exploring various geometric and visual priors. The global rotational symmetry allows us to transform the non-uniform degeneration into several uniform ones by the proposed radial splitting and warping technique. Locally, two types of symmetry constraints, i.e. central symmetry and reflection symmetry are defined as geometric priors in central and surrounding regions, respectively. Furthermore, by investigating the visual artifacts of aberration degenrated images captured by consumer-level cameras, the non-uniform distribution of sharpness across color channels and the image lattice is exploited as visual priors, resulting in a novel strategy to utilize the guidance from the sharpest channel and local image regions to improve the overall performance and robustness. Extensive evaluation on both real and synthetic data suggests that the proposed method outperforms the state-of-the-art techniques.\n\n---TOPIC---\nOptical Aberration\nBlind Deconvolution\nGeometric Priors\nVisual Priors\nImage Restoration",
    "topics": [],
    "references": [
      {
        "citation": "[Krishnan, D., & Fergus, R. (2009). Dark ﬂash photography. ACM Transactions on Graphics, SIGGRAPH 2009 Conference Proceedings, page To appear.] - This paper likely introduces a fundamental technique related to image enhancement or computational photography."
      },
      {
        "citation": "[Krishnan, D., & Fergus, R. (2009). Fast Image Deconvolution using Hyper-Laplacian Priors. In NIPS.] - This paper likely presents a key algorithm for image restoration."
      },
      {
        "citation": "[Krishnan, D., Tay, T., & Fergus, R. (2011). Blind deconvolution using a normalized sparsity measure. In CVPR.] - This paper likely addresses a common problem in image processing: removing blur without knowing the blurring kernel."
      },
      {
        "citation": "[Heide, F., Rouf, M., Hullin, M. B., Labitzke, B., Heidrich, W., & Kolb, A. (2013). High-quality computational imaging through simple lenses. ACM Trans. Graph., 32(5):149:1–149:14.] - This paper likely explores computational techniques to improve image quality using simple optics."
      },
      {
        "citation": "[Schuler, C., Hirsch, M., Harmeling, S., & Sch¨olkopf, B. (2012). Blind correction of optical aberrations. In ECCV, pages 200.] - This paper likely presents a method for correcting lens aberrations without prior knowledge."
      },
      {
        "citation": "[Tang, H., & Kutulakos, K. N. (2013). What does an aberated photo tell us about the lens and the scene? In ICCP, pages 1–10. IEEE.] - This paper likely investigates the relationship between lens aberrations and scene properties."
      },
      {
        "citation": "[Wang, Y., Yang, J., Yin, W., & Zhang, Y. (2008). A new alternating minimization algorithm for total variation image reconstruction. SIAM J. Imaging Sci., pages 248–272.] - This paper likely introduces a key algorithm for image restoration."
      },
      {
        "citation": "[Logean, E., Dalimier, E., & Dainty, C. (2008). Measured double-pass intensity point-spread function after adaptive optics correction of ocular aberrations. Opt. Express, 16(22):17348–17357.] - This paper likely focuses on characterizing the effect of adaptive optics."
      },
      {
        "citation": "[Mugnier, L., Sauvage, J.-F., Fusco, T., Cornia, A., & Dandy, S. (2008). On-line long-exposure phase diversity: a powerful tool for sensing quasi-static aberrations of extreme adaptive optics imaging systems. Opt. Express, 16(22):18406–18416.] - This paper likely describes a technique for measuring aberrations in real-time."
      },
      {
        "citation": "[Schuler, C., Hirsch, M., Harmeling, S., & Scholkopf, B. (2011). Non-stationary correction of optical aberrations. In ICCV, pages 659–666, Nov 2011.] - This paper likely addresses a more complex scenario where aberrations change over time."
      }
    ],
    "author_details": [
      {
        "name": "Tao Yue",
        "affiliation": "Department of Automation, Tsinghua University",
        "email": "[Email unavailable in provided text]"
      },
      {
        "name": "Jinli Suo",
        "affiliation": "Department of Automation, Tsinghua University",
        "email": "[Email unavailable in provided text]"
      },
      {
        "name": "Jue Wang",
        "affiliation": "Adobe Research",
        "email": "[Email unavailable in provided text]"
      },
      {
        "name": "Xun Cao",
        "affiliation": "School of Electronic Science and Engineering, Nanjing University",
        "email": "[Email unavailable in provided text]"
      },
      {
        "name": "Qionghai Dai",
        "affiliation": "Department of Automation, Tsinghua University",
        "email": "[Email unavailable in provided text]"
      }
    ]
  },
  {
    "title": "Deep Networks for Saliency Detection via Local Estimation and Global Search\n---AUTHOR---\nLijun Wang\nHuchuan Lu\nXiang Ruan\nMing-Hsuan Yang",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Wang_Deep_Networks_for_2015_CVPR_supplemental.pdf",
    "id": "Wang_Deep_Networks_for_2015_CVPR_supplemental",
    "abstract": "This paper introduces a novel approach for salient object detection (SOD) called Deep Networks for Salience Detection via Local Estimation and Global Search (LEGS). LEGS combines local and global estimation strategies within a deep network framework. The local estimation component identifies potential salient regions, while the global search component refines these regions based on contextual information. The method utilizes a supervised learning scheme to integrate multiple global features, overcoming the limitations of heuristic combinations. Experiments on four benchmark datasets demonstrate that LEGS achieves state-of-the-art performance, outperforming ten existing methods. Furthermore, the paper provides a detailed feature analysis, revealing the challenges in effectively combining global features for SOD.",
    "topics": [
      "Salient Object Detection (SOD)",
      "Deep Networks",
      "Local Estimation",
      "Global Search",
      "Feature Analysis"
    ],
    "references": [],
    "author_details": [
      {
        "name": "Lijun Wang",
        "affiliation": "Dalian University of Technology",
        "email": "[Not available in the provided text]"
      },
      {
        "name": "Huchuan Lu",
        "affiliation": "Dalian University of Technology",
        "email": "[Not available in the provided text]"
      },
      {
        "name": "Xiang Ruan",
        "affiliation": "OMRON Corporation",
        "email": "[Not available in the provided text]"
      },
      {
        "name": "Ming-Hsuan Yang",
        "affiliation": "University of California at Merced",
        "email": "[Not available in the provided text]"
      }
    ]
  },
  {
    "title": "Real-time 3D Head Pose and Facial Landmark Estimation from Depth Images Using Triangular Surface Patch Features\n---AUTHOR---\nChavdar Papazov\nTim K. Marks\nMichael Jones",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Papazov_Real-Time_3D_Head_2015_CVPR_paper.pdf",
    "id": "Papazov_Real-Time_3D_Head_2015_CVPR_paper",
    "abstract": "We present an algorithm for 3D head pose estimation and facial landmark localization using a commodity depth sensor such as Microsoft’s Kinect. The proposed method consists of an offline training and an online testing phase. Both phases are based on a new triangular surface patch (TSP) descriptor. Our method is robust to noise, is rotation and translation invariant, and runs on a frame-by-frame basis without the need for initialization. It can process 10 to 25 frames per second, depending on the desired accuracy, on a single CPU core, without using parallel hardware such as GPUs. The proposed descriptor is viewpoint invariant, and it is robust to noise and to variations in the data resolution. Using a fast nearest neighbor lookup, TSP descriptors from an input depth map are matched to the most similar ones that were computed from synthetic head models in a training phase. The matched triangular surface patches in the training set are used to compute estimates of the 3D head pose and facial landmark positions in the input depth map. By sampling many TSP descriptors, many votes for pose and landmark positions are generated which together yield robust final estimates. Our results show a significant improvement in the accuracy of both pose and landmark location estimates while maintaining real-time speed.",
    "topics": [
      "3D Head Pose Estimation",
      "Facial Landmark Localization",
      "Triangular Surface Patch (TSP) Descriptor",
      "Real-time Processing",
      "Depth Sensor Data"
    ],
    "references": [
      {
        "citation": "[Asthana, A., Marks, T., Jones, M., Tieu, K., & M.V. R. (2011). Fully automatic pose-invariant face recognition via 3d pose normalization. In ICCV, pages 937–944.]"
      },
      {
        "citation": "[Baltrusaitis, T., Robinson, P., & L.-P. Morency. (2012). 3d constrained local model for rigid and non-rigid facial tracking. In CVPR, 2012.]"
      },
      {
        "citation": "[Besl, P., & N. D. McKay. (1992). A method for registration of 3-d shapes. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 14(2).]"
      },
      {
        "citation": "[Bradley, D., Heidrich, W., Popa, T., & A. Sheffer. (2010). High resolution passive facial performance capture. In ACM Trans. on Graphics (SIGGGRAPH).]"
      },
      {
        "citation": "[Bouaziz, S., Wang, Y., & M. Pauly. (2013). Online modeling for realtime facial animation. In ACM Trans. on Graphics (SIGGRAPH).]"
      },
      {
        "citation": "[Crisanacce, D., & T. Cootes. (2006). Feature detection and tracking with constrained local models. In BMVC.]"
      },
      {
        "citation": "[Fanelli, G., Dantone, M., Gall, J., Fossati, A., & L. V. Gool. (2013). Random forests for real time 3d face analysis. Int. J. of Computer Vision, 101:437–458.]"
      },
      {
        "citation": "[Fanelli, G., Gall, J., & L. V. Gool. (2011). Real time head pose estimation with random regression forests. In CVPR.]"
      },
      {
        "citation": "[Frome, A., Huber, D., Kolluri, R., Bulow, T., & J. Malik. (2004). Recognizing Objects in Range Data Using Regional Point Descriptors. In ECCV.]"
      },
      {
        "citation": "[Hartley, R., Trumpf, J., Dai, Y., & H. Li. (2013). Rotation averaging. International Journal of Computer Vision, 103(3):267–305.]"
      }
    ],
    "author_details": [
      {
        "name": "Chavdar Papazov",
        "affiliation": "Mitsubishi Electric Research Laboratories (MERL)",
        "email": "chavdar.papazov@gmail.com"
      },
      {
        "name": "Tim K. Marks",
        "affiliation": "Mitsubishi Electric Research Laboratories (MERL)",
        "email": "tmarks@merl.com"
      },
      {
        "name": "Michael Jones",
        "affiliation": "Mitsubishi Electric Research Laboratories (MERL)",
        "email": "mjones@merl.com"
      }
    ]
  },
  {
    "title": "Adopting an Unconstrained Ray Model in Light-ﬁeld Cameras for 3D Shape Reconstruction\n---AUTHORs---\nFilippo Bergamasco\nAndrea Albarelli\nLuca Cosmo\nAndrea Torsello\nEmanuele Rodolà\nDaniel Cremers",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Bergamasco_Adopting_an_Unconstrained_2015_CVPR_paper.pdf",
    "id": "Bergamasco_Adopting_an_Unconstrained_2015_CVPR_paper",
    "abstract": "Given the raising interest in light-ﬁeld technology and the increasing availability of professional devices, a feasible and accurate calibration method is paramount to unleash practical applications. In this paper we propose to embrace a fully non-parametric model for the imaging and we show that it can be properly calibrated with little effort using a dense active target. This process produces a dense set of independent rays that cannot be traditionally used to produce a conventional image. However, they are an ideal tool for 3D reconstruction tasks, since they are highly redundant, very accurate and they cover a wide range of different baselines. The feasibility and convenience of the process and the accuracy of the obtained calibration are comprehensively evaluated through several experiments.\n\n---TOPIC---\nLight-field cameras\n3D reconstruction\nNon-parametric imaging models\nCalibration methods\nPlenoptic function",
    "topics": [],
    "references": [
      {
        "citation": "[Adelson, E. H., & Bergen, J. R. (1991). The plenoptic function and the elements of early vision. *Computational Models of Visual Processing*, 3–20.]"
      },
      {
        "citation": "[Hartley, R., & Zisserma, A. (2003). *Multiple View Geometry in Computer Vision*. Cambridge University Press.]"
      },
      {
        "citation": "[Bishop, T., & Favaro, P. (2009). Plenoptic depth estimation from multiple alias views. *Computer Vision Workshops (ICCV Workshops)*, 1622–1629.]"
      },
      {
        "citation": "[Bishop, T., & Favaro, P. (2012). The light field camera: Extended depth of field, aliasing, and superresolution. *IEEE Transactions on Pattern Analysis and Machine Intelligence*, *34*(5), 972–986.]"
      },
      {
        "citation": "[Levin, A., & Durand, F. (2010). Linear view synthesis using a dimensionality gap light field prior. *Computer Vision and Pattern Recognition (CVPR)*, 1831–1838.]"
      },
      {
        "citation": "[Levoy, M., & Hanrahan, P. (1996). Light field rendering. *Proceedings of the 23rd Annual Conference on Computer Graphics and Interactive Techniques*, 31–42.]"
      },
      {
        "citation": "[Ng, R., Levoy, M., Brédif, M., Duval, G., Horowitz, M., & Hanrahan, P. (2005). Light field photography with a handheld plenoptic camera. *Computer Science Technical Report CSTR*, 2(11).]"
      },
      {
        "citation": "[Goldluecke, B. (2012). Globally consistent depth labeling of 4d light fields. *Proceedings of the 2012 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, 608–621.]"
      },
      {
        "citation": "[Cho, D., Lee, M., Kim, S., & Tai, Y.-W. (2013). Modeling the calibration pipeline of the lytro camera for high quality light-field image reconstruction. *Computer Vision (ICCV)*, 3280–3287.]"
      },
      {
        "citation": "[Wanner, S., & Goldluecke, B. (2012). Spatial and angular variational super-resolution of 4d light fields. *Computer Vision - ECCV 2012*, 608–621.]"
      },
      {
        "citation": "---"
      }
    ],
    "author_details": [
      {
        "name": "Filippo Bergamasco",
        "affiliation": "Universit`a Ca’ Foscari Venezia, Venice Italy",
        "email": "[Email not available in the provided text]"
      },
      {
        "name": "Andrea Albarelli",
        "affiliation": "Universit`a Ca’ Foscari Venezia, Venice Italy",
        "email": "[Email not available in the provided text]"
      },
      {
        "name": "Luca Cosmo",
        "affiliation": "Universit`a Ca’ Foscari Venezia, Venice Italy",
        "email": "[Email not available in the provided text]"
      },
      {
        "name": "Andrea Torsello",
        "affiliation": "Universit`a Ca’ Foscari Venezia, Venice Italy",
        "email": "[Email not available in the provided text]"
      },
      {
        "name": "Emanuele Rodolà",
        "affiliation": "Technische Universit¨at M¨unchen, Garching, Germany",
        "email": "[Email not available in the provided text]"
      },
      {
        "name": "Daniel Cremers",
        "affiliation": "Technische Universit¨at M¨unchen, Garching, Germany",
        "email": "[Email not available in the provided text]"
      }
    ]
  },
  {
    "title": "Hierarchical Sparse Coding With Geometric Prior For Visual Geo-location\n---AUTHOR---\nRaghuraman Gopalan",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Gopalan_Hierarchical_Sparse_Coding_2015_CVPR_paper.pdf",
    "id": "Gopalan_Hierarchical_Sparse_Coding_2015_CVPR_paper",
    "abstract": "We address the problem of estimating location information of an image using principles from automated representation learning. We pursue a hierarchical sparse coding approach that learns features useful in discriminating images across locations, by initializing it with a geometric prior corresponding to transformations between image appearance space and their corresponding location grouping space using the notion of parallel transport on manifolds. We then extend this approach to account for the availability of heterogeneous data modalities such as geo-tags and videos pertaining to different locations, and also study a relatively under-addressed problem of transferring knowledge available from certain locations to infer the grouping of data from novel locations. We evaluate our approach on several standard datasets such as im2gps, San Francisco and MediaEval2010, and obtain state-of-the-art results.\n\n---TOPIC---\nVisual Geo-location\n---TOPI---\nHierarchical Sparse Coding\n---TOPI---\nGeometric Prior\n---TOPI---\nManifold Learning\n---TOPI---\nRepresentation Learning",
    "topics": [],
    "references": [
      {
        "citation": "[P.-A. Absil, R. Mahony, and R. Sepulchre, Riemannian geometry of grasmannn manifolds with a view on algorithmic computation, Acta Applicandae Mathematica, 2004]"
      },
      {
        "citation": "[M. Bansal and K. Daniilidis, Geometric urban geo-localization, IEEE Conference on Computer Vision and Pattern Recognition, 2014]"
      },
      {
        "citation": "[Y. Bengio, Learning deep architectures for ai, Foundations and trends R⃝ in Machine Learning, 2009]"
      },
      {
        "citation": "[A. Bergamo, S. N. Sinha, and L. Torresani, Leveraging structure from motion to learn discriminative codebooks for scalable landmark classification, IEEE Conference on Computer Vision and Pattern Recognition, 2013]"
      },
      {
        "citation": "[M. A. Brubaker, A. Geiger, and R. Urtasun, Lost! leveraging the crowd for probabilistic visual self-localization, CVPR, 2013]"
      },
      {
        "citation": "[S. Cao and N. Snavely, Graph-based discriminative learning for location recognition, IEEE Conference on Computer Vision and Pattern Recognition, 2013]"
      },
      {
        "citation": "[D. M. Chen, G. Baatz, K. Koser, S. S. Tsai, R. Vedantham, T. Pylvanainen, K. Roimela, X. Chen, J. Bach, M. Pollefeys, et al., City-scale landmark identiﬁcation on mobile devices, IEEE Conference on Computer Vision and Pattern Recognition, 2011]"
      },
      {
        "citation": "[K. Crammer and Y. Singer, On the algorithmic implementation of multiclass kernel-based vector machines, The Journal of Machine Learning Research, 2002]"
      },
      {
        "citation": "[M. Elad, Sparse and redundant representations, Springer, 2010]"
      },
      {
        "citation": "[J. Hays and A. A. Efro, Im2gps: estimating geographic information from a single image, IEEE Conference on Computer Vision and Pattern Recognition, 2008]"
      }
    ],
    "author_details": [
      {
        "name": "Raghuraman Gopalan",
        "affiliation": "AT&T Labs-Research, Dept. of Video and Multimedia Technologies Research",
        "email": "raghuram@research.att.com"
      }
    ]
  },
  {
    "title": "Image partitioning into convex polygons (Supplementary Materials)\n---AUTHOR---\nLiuyun DUAN\nFlorent LAFARGE",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Duan_Image_Partitioning_Into_2015_CVPR_supplemental.pdf",
    "id": "Duan_Image_Partitioning_Into_2015_CVPR_supplemental",
    "abstract": "This paper presents a method for image partitioning into convex polygons. The authors evaluate their method against existing superpixel techniques (SLIC, SEEDS, and ERS) using the Achievable Segmentation Accuracy (ASA) metric and precision-recall on the Berkeley dataset. They also demonstrate the applicability of their algorithm through experiments in corner detection and object polygonalization, specifically extracting roof contours from aerial images and detecting L- and Y-junctions. The results show comparable performance to specialized algorithms in these applications.\n\n---TOPSICS---\nImage partitioning\nConvex polygons\nObject polygonalization\nCorner detection\nSuperpixel methods",
    "topics": [],
    "references": [
      {
        "citation": "[3] X. Sun, M. Christoudias, and P. Fua. Free-shape polygonal object localization. In ECCV, 2014. (Frequently cited and directly compared to in the paper's visual comparisons.)"
      },
      {
        "citation": "[5] D. J. Xia, G.-S. and Y. Gousseau. Accurate junction detection and characterization in natural images. volume 106, 2014. (Used for corner detection comparison.)"
      },
      {
        "citation": "[1] R. Achanta, A. Shaji, K. Smith, A. Lucchi, P. Fua, and S. Susstrunk. Slic superpixels compared to state-of-the-art superpixel methods. PAM I, 34(11), 2012. (Likely relevant given the paper's focus on polygonal object localization and superpixels.)"
      },
      {
        "citation": "[2] M.-Y. Liu, O. Tuzel, S. Ramalingam, and R. Chellappa. Entropy rate superpixel segmentation. In CVPR, 2011. (Related to the superpixel segmentation aspect.)"
      },
      {
        "citation": "[4] M. Van den Bergh, X. Boix, G. Roig, B. De Capitani, and L. Van Gool. SEEDS: Superpixels extracted via energy-driven sampling. In ECCV, 2012. (Another superpixel method, likely for context.)"
      }
    ],
    "author_details": [
      {
        "name": "Liuyun DUAN",
        "affiliation": "INRIA Sophia Antipolis, France",
        "email": "firstname.lastname@inria.fr"
      },
      {
        "name": "Florent LAFARGE",
        "affiliation": "INRIA Sophia Antipolis, France",
        "email": "firstname.lastname@inria.fr"
      }
    ]
  },
  {
    "title": "Unconstrained 3D Face Reconstruction\n---AUTHOR---\nJoseph Roth\n---AUTHOR---\nYiying Tong\n---AUTHOR---\nXiaoming Liu",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Roth_Unconstrained_3D_Face_2015_CVPR_paper.pdf",
    "id": "Roth_Unconstrained_3D_Face_2015_CVPR_paper",
    "abstract": "This paper presents an algorithm for unconstrained 3D face reconstruction. The input is a collection of face images captured under diverse poses, expressions, and illuminations, without metadata about cameras or timing. The output is a watertight triangulated surface model with albedo data. The algorithm leverages photometric stereo-based methods, utilizing images from all possible poses, including profiles, and combines landmark constraints with normal field-based Laplace editing. The effectiveness of the algorithm is demonstrated qualitatively and quantitatively using a photo collection of Tom Hanks.",
    "topics": [
      "3D Face Reconstruction",
      "Photometric Stereo",
      "Landmark Estimation",
      "Surface Modeling",
      "Matrix Completion"
    ],
    "references": [
      {
        "citation": "[Alexa, M., Behr, J., Cohen-Or, D., Fleishman, S., Levin, D., & Silva, C. T. (2003). Computing and rendering point set surfaces. IEEE Transactions on Visualization and Computer Graphics, 9(1), 3-15.]"
      },
      {
        "citation": "[Basri, R., & Jacobs, D. (2003). Lambertian reflectance and linear subspaces. IEEE Transactions on Pattern Analysis and Machine Intelligence, 25(2), 218-233.]"
      },
      {
        "citation": "[Basri, R., Jacobs, D., & Kemelmacher, I. (2007). Photometric stereo with general, unknown lighting. International Journal of Computer Vision, 72(3), 239-257.]"
      },
      {
        "citation": "[Beeler, T., Bickel, B., Beardsley, P., Sumner, R., & Gross, M. (2010). High-quality single-shot capture of facial geometry. ACM Transactions on Graphics, 29(3).]"
      },
      {
        "citation": "[Berger, M., Tagliasacchi, A., Seversky, L., Alliez, P., Levine, J., Sharf, A., & Silva, C. (2014). State of the Art in Surface Reconstruction from Point Clouds. EUROGAPHICS star reports, 1(1), 161-185.]"
      },
      {
        "citation": "[Cao, C., Hou, Q., & Zhou, K. (2014). Displaced dynamic expression regression for real-time facial tracking and animation. ACM Transactions on Graphics, 33(4), 43.]"
      },
      {
        "citation": "[Curless, B., & LeVoy, M. (1996). A volumetric method for building complex models from range images. ACM Transactions on Graphics, 15(1), 303-312.]"
      },
      {
        "citation": "[Garrido, P., Valgaerts, L., Wu, C., & Theobalt, C. (2013). Reconstructing detailed dynamic face geometry from monocular video. ACM Transactions on Graphics, 32(6), 158.]"
      },
      {
        "citation": "[Hoppe, H., DeRose, T., Duchamp, T., McDonald, J., & Stuetzle, W. (1992). Surface reconstruction from unorganized points. SIGGRAPH, 71-78.]"
      },
      {
        "citation": "[Hassner, T. (2013). Viewing real-world faces in 3D. ICCV, 3607-3614.]"
      }
    ],
    "author_details": [
      {
        "name": "Joseph Roth",
        "affiliation": "Department of Computer Science and Engineering, Michigan State University",
        "email": "rothjos1@msu.edu"
      },
      {
        "name": "Yiying Tong",
        "affiliation": "Department of Computer Science and Engineering, Michigan State University",
        "email": "ytong@msu.edu"
      },
      {
        "name": "Xiaoming Liu",
        "affiliation": "Department of Computer Science and Engineering, Michigan State University",
        "email": "liuxm@msu.edu"
      }
    ]
  },
  {
    "title": "Efﬁcient Illuminant Estimation for Color Constancy Using Grey Pixels\n---AUTHOR---\nKai-Fu Yang\nShao-Bing Gao\nYong-Jie Li",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Yang_Efficient_Illuminant_Estimation_2015_CVPR_paper.pdf",
    "id": "Yang_Efficient_Illuminant_Estimation_2015_CVPR_paper",
    "abstract": "Illuminant estimation is a key step for computational color constancy. This paper proposes a novel method for illuminant estimation by using the information of grey pixels detected in a given color-biased image. The underlying hypothesis is that most natural images include detectable grey pixels that can be reliably utilized for illuminant estimation. The method involves validating this assumption, developing a grey pixel detection method based on an illuminant-invariant measure, and then estimating the light source color from the detected grey pixels. Experimental results on benchmark datasets demonstrate that the proposed method outperforms state-of-the-art approaches with low computational cost.\n\n---TOPIC---\nColor Constancy\n---TOPIC---\nIlluminant Estimation\n---TOPIC---\nGrey Pixels\n---TOPIC---\nImage Processing\n---TOPIC---\nComputational Vision",
    "topics": [],
    "references": [
      {
        "citation": "[R. Achanta, S. Hemami, F. Estrada, and S. Susstrunk, Frequency-tuned salient region detection, IEEE Conference on Computer Vision and Pattern Recognition, 2009.]"
      },
      {
        "citation": "[G. D. Finlayson and E. Trezzi, Shades of gray and colour constancy, Color and Imaging Conference, 2004.]"
      },
      {
        "citation": "[K. Barnard, Improvements to gamut mapping colour constancy algorithms, European Conference on Computer Vision, 2000.]"
      },
      {
        "citation": "[D. A. Forsyth, A novel algorithm for color constancy, International Journal of Computer Vision, 1990.]"
      },
      {
        "citation": "[D. H. Foster, Color constancy, Vision research, 2011.]"
      },
      {
        "citation": "[B. Funt and M. Mosny, Removing outliers in illumination estimation, Color and Imaging Conference, 2012.]"
      },
      {
        "citation": "[B. V. Funt and G. D. Finlayson, Color constant color indexing, Pattern Analysis and Machine Intelligence, IEEE Transactions on, 1995.]"
      },
      {
        "citation": "[S. Gao, W. Han, K. Yang, C. Li, and Y. Li, Efﬁcient color constancy with local surface reﬂectance statistics, European Conference on Computer Vision, 2014.]"
      },
      {
        "citation": "[A. Chakrabarti, K. Hirakawa, and T. Zickler, Color constancy with spati-spectral statistics, Pattern Analysis and Machine Intelligence, IEEE Transactions on, 2012.]"
      },
      {
        "citation": "[D. Cheng, D. K. Prasad, and M. S. Brown, Illuminant estimation for color constancy, Journal of Optical Society of America A: Optics, Image Science, and Vision, 2014.]"
      }
    ],
    "author_details": [
      {
        "name": "Kai-Fu Yang",
        "affiliation": "University of Electronic Science and Technology of China",
        "email": "yang kf@163.com"
      },
      {
        "name": "Shao-Bing Gao",
        "affiliation": "University of Electronic Science and Technology of China",
        "email": "gao shaobing@163.com"
      },
      {
        "name": "Yong-Jie Li",
        "affiliation": "University of Electronic Science and Technology of China",
        "email": "liyj@uestc.edu.cn"
      }
    ]
  },
  {
    "title": "Deep Visual-Semantic Alignments for Generating Image Descriptions\n---AUTHORs---\nAndrej Karpathy\nLi Fei-Fei",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Karpathy_Deep_Visual-Semantic_Alignments_2015_CVPR_paper.pdf",
    "id": "Karpathy_Deep_Visual-Semantic_Alignments_2015_CVPR_paper",
    "abstract": "We present a model that generates natural language descriptions of images and their regions. Our approach leverages datasets of images and their sentence descriptions to learn about the inter-modal correspondences between language and visual data. Our alignment model is based on a novel combination of Convolutional Neural Networks over image regions, bidirectional Recurrent Neural Networks over sentences, and a structured objective that aligns the two modalities through a multimodal embedding. We then describe a Multimodal Recurrent Neural Network architecture that uses the inferred alignments to learn to generate novel descriptions of image regions. We demonstrate that our alignment model produces state of the art results in retrieval experiments on Flickr8K, Flickr30K and MSCOCO datasets. We then show that the generated descriptions significantly outperform retrieval baselines on both full images and on a new dataset of region-level annotations.\n\n---TOPICCS---\nImage Description Generation\nDeep Neural Networks\nMultimodal Alignment\nConvolutional Neural Networks (CNNs)\nRecurrent Neural Networks (RNNs)",
    "topics": [],
    "references": [
      {
        "citation": "[Barbu, A., Bridge, A., Burchill, Z., Coroian, D., Dickinson, S., Fidler, S., Michaux, A., Mussman, S., Narayanaswamy, S., Salvi, D., et al. Video in sentences out. arXiv preprint arXiv:1204.2742, 2012.]"
      },
      {
        "citation": "[Bengio, Y., Schwenk, H., Sen´ecal, J.-S., Morin, F., and Gauvain, J.-L. Neural probabilistic language models. In Innovations in Machine Learning. Springer, 306. 2006.]"
      },
      {
        "citation": "[Chen, X., and Zitnick, C. L. Learning a recurrent visual representation for image caption generation. CoRR, abs/1411.5654, 2014.]"
      },
      {
        "citation": "[Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imaginet: A large-scale hierarchical image database. In CVPR, 2009.]"
      },
      {
        "citation": "[Elliott, D., and Keller, F. Image description using visual dependency representations. In EMNLP, pages 1292–1302, 2013.]"
      },
      {
        "citation": "[Everingham, M., Van Gool, L., Williams, C. K. I., Winn, J., and Zisserman, A. The pascal visual object classes (voc) challenge. International Journal of Computer Vision, 88(2):303–338, June 2010.]"
      },
      {
        "citation": "[Fei-Fei, L., Iyer, A., Koch, C., and Perona, P. What do we perceive in a glance of a real-world scene? Journal of vision, 7(1):10, 2007.]"
      },
      {
        "citation": "[Fidler, S., Sharma, A., and Urtaşun, R. A sentence is worth a thousand pixels. In CVPR, 2013.]"
      },
      {
        "citation": "[Frome, A., Corrado, G. S., Shlens, J., Bengio, S., Dean, J., Mikolov, T., et al. Devise: A deep visual-semantic embedding model. In NIPS, 2013.]"
      },
      {
        "citation": "[Gould, S., Fulton, R., and Koller, D. Decomposin a scene into geometric and semantically consistent regions. In Computer Vision, 2009 IEEE 12th International Conference on, pages 1–8. IEEE, 2009.]"
      }
    ],
    "author_details": [
      {
        "name": "Andrej Karpathy",
        "affiliation": "Department of Computer Science, Stanford University",
        "email": "karpathy@cs.stanford.edu"
      },
      {
        "name": "Li Fei-Fei",
        "affiliation": "Department of Computer Science, Stanford University",
        "email": "feifeili@cs.stanford.edu"
      }
    ]
  },
  {
    "title": "A Stable Multi-Scale Kernel for Topological Machine Learning\n---AUTHOR---\nJan Reininghaus\nStefan Huber\nUlrich Bauer\nRoland Kwitt",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Reininghaus_A_Stable_Multi-Scale_2015_CVPR_paper.pdf",
    "id": "Reininghaus_A_Stable_Multi-Scale_2015_CVPR_paper",
    "abstract": "This paper addresses the lack of a theoretically sound connection between topological data analysis (TDA) and popular kernel-based learning techniques like kernel SVMs or kernel PCA. It introduces a multi-scale kernel for persistence diagrams, a stable summary representation of topological features in data. The key contribution is demonstrating that this kernel is positive definite and stable with respect to the 1-Wasserstein distance. Experiments on shape classification/retrieval and texture recognition benchmarks show performance gains compared to alternative approaches.\n\n---TOPICs---\nPersistent homology\nKernel methods\nTopological data analysis\nMulti-scale kernel\nStability (in TDA)",
    "topics": [],
    "references": [
      {
        "citation": "[Cohen-Steiner, D., Edelbrunner, H., & Haerer, J. (2007). Stability of persistence diagrams. *Discrete Comput. Geom*, *37*(1), 71-102.]"
      },
      {
        "citation": "[Carlsson, G. (2009). Topology and data. *Bull. Amer. Math. Soc*, *46*(2), 255–308.]"
      },
      {
        "citation": "[Schölkopf, B., & Smolka, A. J. (2001). Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond. MIT Press.]"
      },
      {
        "citation": "[Chang, C.-C., & Lin, C.-J. (2011). LIBSVM: A library for support vector machines. *ACM TIST*, *2*(3), 1–27.]"
      },
      {
        "citation": "[Gao, M., Chen, C., Zhang, S., Qian, Z., Metaxas, D., & Axel, L. (2013). Segmenting the papillary muscles and the trabeculae from high resolution cardiac CT through restoration of topological handles. In *Topological Methods in Data Analysis and Visualization II*. Springer, Berlin Heidelberg.]"
      },
      {
        "citation": "[Cohen-Steiner, D., Edelbrunner, H., Haerer, J., & Mileyko, Y. (2010). Lipschitz functions have Lp-stable persistence. *Found. Comput. Math*, *10*(2), 127–139.]"
      },
      {
        "citation": "[Bubeník, P. (2012). Statistical topological data analysis using persistence landscapes. arXiv preprint arXiv:1207.6437.]"
      },
      {
        "citation": "[Gonen, M., & Alpaydin, E. (2011). Multiple kernel learning algorithms. *J. Mach. Learn. Res*, *12*, 2211–2268.]"
      },
      {
        "citation": "[Wagner, H., Chen, C., & Vučinić, H. (2012). Efficient computation of persistent homology for cubical data. In *Topological Methods in Data Analysis and Visualization II*. Springer, Berlin Heidelberg.]"
      },
      {
        "citation": "[Chazal, F., Guibas, L., Oudot, S., & Skiba, P. (2011). Persistence-based clustering in Riemannian manifolds. In *SoSG*.]"
      }
    ],
    "author_details": [
      {
        "name": "Jan Reininghaus",
        "affiliation": "IST Austria",
        "email": "[Email not available in provided text]"
      },
      {
        "name": "Stefan Huber",
        "affiliation": "IST Austria",
        "email": "[Email not available in provided text]"
      },
      {
        "name": "Ulrich Bauer",
        "affiliation": "IST Austria, TU M¨unchen",
        "email": "[Email not available in provided text]"
      },
      {
        "name": "Roland Kwitt",
        "affiliation": "University of Salzburg, Austria",
        "email": "[Email not available in provided text]"
      }
    ]
  },
  {
    "title": "Pooled Motion Features for First-Person Videos\n---AUTHOR---\nM. S. Ryoo\nBrandon Rothrock\nLarry Matthies",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Ryoo_Pooled_Motion_Features_2015_CVPR_supplemental.pdf",
    "id": "Ryoo_Pooled_Motion_Features_2015_CVPR_supplemental",
    "abstract": "This appendix provides detailed experimental results of our pooled time series (PoT) representations. We show activity recognition performances of our PoT per each different temporal pooling operator, as mentioned in Section 3.2 of the paper: sum pooling, max pooling, and two types of our new ‘histogram of time series gradients’ pooling. Results with individual pooling operators as well as possible combinations of them are reported. Both the Dog-Centric activity dataset and the UEC Park dataset were used. The results suggest that using multiple different types of temporal pooling operators helps overall recognition and that our PoT is a good representation to combine different types of feature descriptors.\n\n---TOPIC---\nPooled Time Series (PoT)\nTemporal Pooling Operators\nActivity Recognition\nDogCentric Dataset\nUEC Park Dataset",
    "topics": [],
    "references": [],
    "author_details": [
      {
        "name": "M. S. Ryoo",
        "affiliation": "Jet Propulsion Laboratory, California Institute of Technology, Pasadena, CA",
        "email": "mryoo@jpl.nasa.gov"
      },
      {
        "name": "Brandon Rothrock",
        "affiliation": "Jet Propulsion Laboratory, California Institute of Technology, Pasadena, CA",
        "email": "Not available"
      },
      {
        "name": "Larry Matthies",
        "affiliation": "Jet Propulsion Laboratory, California Institute of Technology, Pasadena, CA",
        "email": "Not available"
      }
    ]
  },
  {
    "title": "Beyond Spatial Pooling: Fine-Grained Representation Learning in Multiple Domains\n---AUTHOR---\nChi Li\nAustin Reiter\nGregory D. Hager",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Li_Beyond_Spatial_Pooling_2015_CVPR_paper.pdf",
    "id": "Li_Beyond_Spatial_Pooling_2015_CVPR_paper",
    "abstract": "Object recognition systems have shown great progress, but creating object representations robust to viewpoint changes while capturing local visual details remains a challenge. Recent convolutional architectures employ spatial pooling for scale and shift invariance, but are still sensitive to out-of-plane rotations. This paper formulates a probabilistic framework for analyzing pooling performance, suggesting improvements through multiple scales of filters, different pooling granularities, and the use of color as an additional pooling domain to reduce sensitivity to spatial deformations. The algorithm is evaluated on object instance recognition tasks using RGB-D datasets, demonstrating significant improvements over the state-of-the-art. A new dataset for industrial objects is also presented to further validate the approach.\n\n---TOPICCS---\nObject Recognition\nConvolutional Architectures\nSpatial Pooling\nRGB-D Data\nFine-Grained Representation Learning",
    "topics": [],
    "references": [
      {
        "citation": "[Goodfellow, I., Lee, H., Le, Q. V., Saxe, A., & Ng, A. Y. Measuring invariances in deep networks. In NIPS, 2009.]"
      },
      {
        "citation": "[Aldoma, A., Tombari, F., Rusu, R. B., & Vincze, M. OUR-CVFH–Oriented, Unique and Repeatable Clustered Viewpoint Feature Histogram for Object Recognition and 6DOF Pose Estimation. 2012.]"
      },
      {
        "citation": "[He, K., Zhang, X., Ren, S., & Sun, J. Spatial pyramid pooling in deep convolutional networks for visual recognition. In ECCV, 2014.]"
      },
      {
        "citation": "[Jia, Y., Huang, C., & Darrell, T. Beyond spatial pyramids: Receptive ﬁeld learning for pooled image features. In CVPR. IEEE, 2012.]"
      },
      {
        "citation": "[Bengio, Y., & LeCun, Y. Scaling learning algorithms towards ai. Large-scale kernel machines, 2007.]"
      },
      {
        "citation": "[Kavukcuoglu, K., Ranzato, M., Fergus, R., & LeCun, Y. Learning invariant features through topographic filter maps. In CVPR. IEEE, 2009.]"
      },
      {
        "citation": "[Krizhevsky, A., Sutskever, I., & Hinton, G. E. ImageNet classification with deep convolutional neural networks. In NIPS, 2012.]"
      },
      {
        "citation": "[Lai, K., Bo, L., Ren, X., & Fox, D. A large-scale hierarchical multi-view rgb-d object dataset. In ICRA, 2011.]"
      },
      {
        "citation": "[Lazebnik, S., Schmid, C., & Ponce, J. Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories. In CVPR. IEEE, 2006.]"
      },
      {
        "citation": "[LeCun, Y., Huang, F. J., & Bottou, L. Learning methods for generic object recognition with invariance to pose and lighting. In CVPR. IEEE, 2004.]"
      }
    ],
    "author_details": [
      {
        "name": "Chi Li",
        "affiliation": "Department of Computer Science, Johns Hopkins University",
        "email": "chi li@jhu.edu"
      },
      {
        "name": "Austin Reiter",
        "affiliation": "Department of Computer Science, Johns Hopkins University",
        "email": "areiter@cs.jhu.edu"
      },
      {
        "name": "Gregory D. Hager",
        "affiliation": "Department of Computer Science, Johns Hopkins University",
        "email": "hager@cs.jhu.edu"
      }
    ]
  },
  {
    "title": "Graph-based Simplex Method for Pairwise Energy Minimization with Binary Variables\n---AUTHOR---\nDaniel Pr˚uˇsa",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Prusa_Graph-Based_Simplex_Method_2015_CVPR_paper.pdf",
    "id": "Prusa_Graph-Based_Simplex_Method_2015_CVPR_paper",
    "abstract": "This paper introduces a novel graph-based simplex method for solving pairwise energy minimization problems with binary variables. Unlike existing approaches that rely on translating the problem to a max-flow/min-cut formulation, this method directly applies the simplex algorithm to the linear programming (LP) relaxation of the problem. The authors identify a special structure within the simplex algorithm's iterations, allowing for combinatorial processing over the energy minimization graph. Experimental results demonstrate that this new solver outperforms max-flow based methods for certain computer vision instances and offers potential for generalization to multi-label problems and applications beyond energy minimization.\n\n---TOPICNS---\nEnergy Minimization\nSimplex Algorithm\nLinear Programming Relaxation\nBinary Variables\nGraph-Based Methods",
    "topics": [],
    "references": [
      {
        "citation": "[E. Boros and P. L. Hammer, \"Pseudo-Boolean optimization,\" *Discrete Applied Mathematics*, 123(1-3):155–225, 2002.]"
      },
      {
        "citation": "[Y. Boykov and V. Kolmogorov, \"An experimental comparison of min-cut/max-ﬂow algorithms for energy minimization in vision,\" *IEEE Trans. on Pattern Analysis and Machine Intelligence*, 26(9):1124–1137, Sept. 2004.]"
      },
      {
        "citation": "[Y. Boykov, O. Veksler, and R. Zabih, \"Fast approximate energy minimization via graph cuts,\" *IEEE Trans. on Pattern Analysis and Machine Intelligence*, 23(11):1222–1239, Nov. 2001.]"
      },
      {
        "citation": "[G. Dantzig and M. Thapa, *Linear Programming 1: Introduction*, Springer, 1997.]"
      },
      {
        "citation": "[D. Goldfarb and J. Hao, \"A primal simplex algorithm that solves the maximum ﬂow problem in at most nm pivots and O(n2m) time,\" *Mathematical Programming*, 47(1-3):353–365, 1990.]"
      },
      {
        "citation": "[H. Ishikawa, \"Exact optimization for Markov random ﬁelds with convex priors,\" *IEEE Trans. on Pattern Analysis and Machine Intelligence*, 25(10):1333–1336, Oct. 2003.]"
      },
      {
        "citation": "[V. Kolmogorov, \"Convergent tree-reweighted message passing for energy minimization,\" *IEEE Trans. on Pattern Analysis and Machine Intelligence*, 28(10):1568–1583, 2006.]"
      },
      {
        "citation": "[V. Kolmogorov and C. Rother, \"Minimizing nonsubmodular functions with graph cuts-a review,\" *IEEE Trans. on Pattern Analysis and Machine Intelligence*, 29(7):1274–1279, 2007.]"
      },
      {
        "citation": "[C. Rother, V. Kolmogorov, V. S. Lempitsky, and M. Szummer, \"Optimizing binary MRFs via extended roof duality,\" *In IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)*, 2007.]"
      },
      {
        "citation": "[T. Verma and D. Batra, \"MaxFlow Revisited,\" http://ttic.uchicago.edu/˜dbatra/research/mfcomp/, 2007.]"
      }
    ],
    "author_details": [
      {
        "name": "Daniel Pr˚uˇsa",
        "affiliation": "Center for Machine Perception, Faculty of Electrical Engineering, Czech Technical University",
        "email": "prusapa1@fel.cvut.cz"
      }
    ]
  },
  {
    "title": "Dynamically Encoded Actions based on Spacetime Saliency\n---AUTHOR---\nChrisoph Feichtenhofer\nAxel Pinz\nRichard P. Wildes",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Feichtenhofer_Dynamically_Encoded_Actions_2015_CVPR_paper.pdf",
    "id": "Feichtenhofer_Dynamically_Encoded_Actions_2015_CVPR_paper",
    "abstract": "Human actions typically occur over a well localized extent in both space and time. Similarly, as typically captured in video, human actions have small spatio-temporal support in image space. This paper capitalizes on these observations by weighting feature pooling for action recognition over those areas within a video where actions are most likely to occur. To enable this operation, we deﬁne a novel measure of spacetime saliency. The measure relies on two observations regarding foreground motion of human actors: They typically exhibit motion that contrasts with that of their surrounding region and they are spatially compact. By using the resulting deﬁnition of saliency during feature pooling we show that action recognition performance achieves state-of-the-art levels on three widely considered action recognition datasets. Our saliency weighted pooling can be applied to essentially any locally deﬁned features and encodings thereof. Additionally, we demonstrate that inclusion of locally aggregated spatio-temporal energy features, which efﬁciently result as a by-product of the saliency computation, further boosts performance over reliance on standard action recognition features alone.\n\n---TOPICICS---\nSpatiotemporal Salience\nAction Recognition\nFeature Pooling\nVideo Analysis\nBag-of-Visual-Words (BoW)",
    "topics": [],
    "references": [
      {
        "citation": "[R. Achanta, A. Shaji, K. Smith, A. Lucchi, P. Fua, and S. Susstrunk. Slic superpixels compared to state-of-the-art superpixel methods. PAMI, 34(11):2274–2282, 2012.]"
      },
      {
        "citation": "[A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks. In Proc. NIPS, 2012.]"
      },
      {
        "citation": "[N. Ballas, Y. Yang, Z.-Z. Lan, B. Delezoide, F. Preteux, and A. Hauptmann. Space-time robust representation for action recognition. In Proc. ICCV, 2013.]"
      },
      {
        "citation": "[H. Kuehne, H. Jhuang, E. Garrote, T. Poggio, and T. Serre. HMDB: a large video database for human motion recognition. In Proc. ICCV, 2011.]"
      },
      {
        "citation": "[I. Laptev, M. Marszalek, C. Schmid, and B. Rozenfeld. Learning realistic human actions from movies. In Proc. CVPR, 2008.]"
      },
      {
        "citation": "[K. Chatfield, K. Simonyan, A. Vedaldi, and A. Zisserman. Return of the devil in the details: Delving deep into convolutional nets. In Proc. BMVC, 2014.]"
      },
      {
        "citation": "[R. G. Cinbis, J. Verbeek, and C. Schmid. Segmentation driven object detection with Fisher vectors. In Proc. ICCV, 2013.]"
      },
      {
        "citation": "[C. Cortes and V. Vapnik. Support-vector networks. Machine Learning, 20(3):273–297, 1995.]"
      },
      {
        "citation": "[N. Dalal. Finding people in images and videos. PhD thesis, Institut National Polytechnique de Grenoble, 2006.]"
      },
      {
        "citation": "[N. Dalal and B. Triggs. Histograms of oriented gradients for human detection. In Proc. CVPR, 2005.]"
      }
    ],
    "author_details": [
      {
        "name": "Christoph Feichtenhofer",
        "affiliation": "Institute of Electrical Measurement and Measurement Signal Processing, TU Graz, Austria",
        "email": "feichtenhofer@tugraz.at"
      },
      {
        "name": "Axel Pinz",
        "affiliation": "Institute of Electrical Measurement and Measurement Signal Processing, TU Graz, Austria",
        "email": "axel.pinz@tugraz.at"
      },
      {
        "name": "Richard P. Wildes",
        "affiliation": "Department of Electrical Engineering and Computer Science, York University, Toronto, Canada",
        "email": "wildes@cse.yorku.ca"
      }
    ]
  },
  {
    "title": "On Pairwise Costs for Network Flow Multi-Object Tracking\n---AUTHOR---\nVisesh Chari\nSimon Lacoste-Julien\nIvan Laptev\nJosef Sivic",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Chari_On_Pairwise_Costs_2015_CVPR_paper.pdf",
    "id": "Chari_On_Pairwise_Costs_2015_CVPR_paper",
    "abstract": "Multi-object tracking has recently benefited from min-cost network flow optimization techniques, enabling the modeling of dependencies among tracks. This paper addresses the challenge of object detector failures (due to occlusions and clutter) within this framework. The authors propose adding pairwise costs to the min-cost network flow objective function. While this introduces NP-hardness, they design a convex relaxation solution with an efficient rounding heuristic that empirically provides certificates of small suboptimality. They evaluate two types of pairwise costs and demonstrate improvements over existing methods in real-world video sequences.\n\n---TOPSICS---\nMulti-object Tracking\nMin-Cost Network Flow\nPairwise Costs\nConvex Relaxation\nTracking-by-Detection",
    "topics": [],
    "references": [
      {
        "citation": "[M. Andriluka, S. Roth, and B. Schiele. People-tracking-by-detection and people-detection-by-tracking. In CVPR, 2008.] - Frequently cited (appears 6 times) and foundational work on the topic."
      },
      {
        "citation": "[R. K. Ahuja, T. L. Magnanti, and J. B. Orlin. Network ﬂows: theory, algorithms, and applications. Prentice-Hall, Inc., 1993.] - Referenced for network flow concepts, a common technique in tracking."
      },
      {
        "citation": "[B. Yang and R. Nevatia. An online learned CRF model for multi-target tracking. In CVPR, 2012.] - Referenced multiple times, indicating its relevance to the paper's methodology."
      },
      {
        "citation": "[M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The Pascal visual object classes (VOC) challenge. IJCV, 2010.] - Referenced for dataset information, likely used for evaluation."
      },
      {
        "citation": "[D. Bertsimas and J. N. Tsitskiklis. Introduction to Linear Optimization. Athena Scientiﬁc, 1997.] - Referenced for optimization techniques."
      },
      {
        "citation": "[A. Yilmaz, O. Javed, and M. Shah. Object tracking: A Survey. ACM Comput. Surv., 38(4), 2006.] - A survey paper providing context and background."
      },
      {
        "citation": "[S. Pellegrini, A. Ess, K. Schindler, and L. van Gool. You’ll never walk alone: Modeling social behavior for multi-target tracking. In CVPR, 2009.] - Referenced for modeling social behavior."
      },
      {
        "citation": "[B. Yang and R. Nevatia. An online learned CRF model for multi-target tracking. In CVPR, 2012.] - Referenced multiple times, indicating its relevance to the paper's methodology."
      },
      {
        "citation": "[L. Zhang, Y. Li, and R. Nevatia. Global data association for multi-object tracking using network ﬂows. In CVPR, 2008.] - Referenced for network flow techniques."
      },
      {
        "citation": "[A. Butt and R. T. Collins. Multi-target tracking by Lagrangian relaxation to min-cost network ﬂow. In CVPR, 2013.] - Referenced for network flow techniques."
      }
    ],
    "author_details": [
      {
        "name": "Visesh Chari",
        "affiliation": "INRIA and Ecole Normale Sup´erieure, Paris, France",
        "email": "Not available"
      },
      {
        "name": "Simon Lacoste-Julien",
        "affiliation": "INRIA and Ecole Normale Sup´erieure, Paris, France",
        "email": "Not available"
      },
      {
        "name": "Ivan Laptev",
        "affiliation": "INRIA and Ecole Normale Sup´erieure, Paris, France",
        "email": "Not available"
      },
      {
        "name": "Josef Sivic",
        "affiliation": "INRIA and Ecole Normale Sup´erieure, Paris, France",
        "email": "Not available"
      }
    ]
  },
  {
    "title": "Discrete Optimization of Ray Potentials for Semantic 3D Reconstruction\n---AUTHOR---\nNikolay Savinov\n---AUTHOR---\nL’ubor Ladický\n---AUTHOR---\nChristian Häne\n---AUTHOR---\nMarc Pollefeys",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Savinov_Discrete_Optimization_of_2015_CVPR_paper.pdf",
    "id": "Savinov_Discrete_Optimization_of_2015_CVPR_paper",
    "abstract": "Dense semantic 3D reconstruction is typically formulated as a discrete optimization problem combining semantic and depth likelihoods. However, modelling likelihoods as unary potentials leads to visibility artifacts. This paper proposes a new optimization problem that directly optimizes the reprojection error of the 3D model with respect to image estimates, optimizing over rays and considering the semantic class and depth of the first occupied voxel. The approach utilizes QPBO relaxation for graph representation and graph cut for solving the 2-label problem, and α-expansion for the multi-label problem. The method achieves comparable speed to existing methods while avoiding ray potential approximation artifacts.\n\n---TOPICCS---\nDense 3D Reconstruction\nSemantic Segmentation\nRay Potentials\nGraph Cut Optimization\nQPBO Relaxation",
    "topics": [],
    "references": [
      {
        "citation": "[Besag, J. On the statistical analysis of dirty pictures. Journal of the Royal Statistical Society, 1986.]"
      },
      {
        "citation": "[Brostow, G. J., Shotton, J., Fauqueur, J., & Cipolla, R. Segmentation and recognition using structure from motion point clouds. In European Conference on Computer Vision, 2008.]"
      },
      {
        "citation": "[Boykov, Y., Vekslar, O., & Zabih, R. Fast approximate energy minimization via graph cuts. Transactions on Pattern Analysis and Machine Intelligence, 2001.]"
      },
      {
        "citation": "[Hane, C., Zach, C., Cohen, A., Angst, R., & Pollefeys, M. Joint 3D scene reconstruction and class segmentation. In Conference on Computer Vision and Pattern Recognition, 2013.]"
      },
      {
        "citation": "[Ladicky, L., Russell, C., Kohli, P., & Torr, P. H. S. Associative hierarchical CRFs for object class image segmentation. In International Conference on Computer Vision, 2009.]"
      },
      {
        "citation": "[Kolmogorov, V., & Boykov, Y. What metrics can be approximated by geo-cuts, or global optimization of length/area and flux. In International Conference on Computer Vision, 2005.]"
      },
      {
        "citation": "[Ramalingam, S., Russell, C., Ladicky, L., & Torr, P. H. Efficient minimization of higher order submodular functions using monotonic boolean functions. Arxiv preprint arXiv:1109.2304, 2011.]"
      },
      {
        "citation": "[Seitz, S. M., Curless, B., Diebel, J., Scharstein, D., & Szeliski, R. A comparison and evaluation of multi-view stereo reconstruction algorithms. In Conference on Computer Vision and Pattern Recognition, 2006.]"
      },
      {
        "citation": "[Shotton, J., Winn, J., Rother, C., & Criminis, A. Texton-Boost: Joint appearance, shape and context modeling for multi-class object recognition and segmentation. In European Conference on Computer Vision, 2006.]"
      },
      {
        "citation": "[Strecha, C., von Hansen, W., Van Gool, L., Fua, P., & Thoennessen, U. On benchmarking camera calibration and multi-view stereo for high resolution imagery. In Conference on Computer Vision and Pattern Recognition, 2008.]"
      }
    ],
    "author_details": [
      {
        "name": "Nikolay Savinov",
        "affiliation": "ETH Z¨urich",
        "email": "nikolay.savinov@inf.ethz.ch"
      },
      {
        "name": "L’ubor Ladick´y",
        "affiliation": "ETH Z¨urich",
        "email": "lubor.ladicky@inf.ethz.ch"
      },
      {
        "name": "Christian H¨ane",
        "affiliation": "ETH Z¨urich",
        "email": "christian.haene@inf.ethz.ch"
      },
      {
        "name": "Marc Pollefeys",
        "affiliation": "ETH Z¨urich",
        "email": "marc.pollefeys@inf.ethz.ch"
      }
    ]
  },
  {
    "title": "Show and Tell: A Neural Image Caption Generator\n---AUTHOR---\nOriol Vinyals\nAlexander Toshev\nSamy Bengio\nDumitru Erhan",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Vinyals_Show_and_Tell_2015_CVPR_paper.pdf",
    "id": "Vinyals_Show_and_Tell_2015_CVPR_paper",
    "abstract": "This paper introduces a generative model based on a deep recurrent architecture for automatically generating natural language descriptions of images. The model combines recent advances in computer vision and machine translation, training to maximize the likelihood of a target description sentence given an image. Experiments on several datasets demonstrate the model's accuracy and fluency, achieving state-of-the-art results on benchmarks like Pascal, Flickr30k, SBU, and COCO, often approaching human performance. The model utilizes a deep convolutional neural network (CNN) for image encoding and a recurrent neural network (RNN) for language generation, forming a \"Neural Image Caption\" (NIC) system.",
    "topics": [
      "Neural Image Caption (NIC)",
      "Deep Convolutional Neural Networks (CNNs)",
      "Recurrent Neural Networks (RNNs)",
      "Machine Translation",
      "Image Captioning"
    ],
    "references": [],
    "author_details": [
      {
        "name": "Oriol Vinyals",
        "affiliation": "Google",
        "email": "vinylas@google.com"
      },
      {
        "name": "Alexander Toshev",
        "affiliation": "Google",
        "email": "toshev@google.com"
      },
      {
        "name": "Samy Bengio",
        "affiliation": "Google",
        "email": "bengio@google.com"
      },
      {
        "name": "Dumitru Erhan",
        "affiliation": "Google",
        "email": "dumitru@google.com"
      }
    ]
  },
  {
    "title": "Multi-Manifold Deep Metric Learning for Image Set Classiﬁcation\n---AUTHOR---\nJiwen Lu\n---AUTHOR---\nGang Wang\n---AUTHOR---\nWeihong Deng\n---AUTHOR---\nPierre Moulin\n---AUTHOR---\nJie Zhou",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Lu_Multi-Manifold_Deep_Metric_2015_CVPR_paper.pdf",
    "id": "Lu_Multi-Manifold_Deep_Metric_2015_CVPR_paper",
    "abstract": "In this paper, we propose a multi-manifold deep metric learning (MMDML) method for image set classification, which aims to recognize an object of interest from a set of image instances captured from varying viewpoints or under varying illuminations. Motivated by the fact that manifold can be effectively used to model the nonlinearity of samples in each image set and deep learning has demonstrated superb capability to model the nonlinearity of samples, we propose a MMDML method to learn multiple sets of nonlinear transformations, one set for each object class, to nonlinearly map multiple sets of image instances into a shared feature subspace, under which the manifold margin of different class is maximized, so that both discriminative and class-speciﬁc information can be exploited, simultaneously. Our method achieves the state-of-the-art performance on ﬁve widely used datasets.",
    "topics": [
      "Image Set Classification",
      "Deep Metric Learning",
      "Manifold Learning",
      "Multi-Manifold Approach",
      "Nonlinear Transformations"
    ],
    "references": [],
    "author_details": [
      {
        "name": "Jiwen Lu",
        "affiliation": "Advanced Digital Sciences Center, Singapore",
        "email": "jiwen.lu@adsc.com.sg"
      },
      {
        "name": "Gang Wang",
        "affiliation": "School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore",
        "email": "wanggang@ntu.edu.sg"
      },
      {
        "name": "Weihong Deng",
        "affiliation": "School of ICE, Beijing University of Posts and Telecommunications, Beijing, China",
        "email": "whdeng@bupt.edu.cn"
      },
      {
        "name": "Pierre Moulin",
        "affiliation": "Department of ECE, University of Illinois at Urbana-Champaign, Urbana, IL, USA",
        "email": "moulin@ifp.uiuc.edu"
      },
      {
        "name": "Jie Zhou",
        "affiliation": "Department of Automation, Tsinghua University, Beijing, China",
        "email": "jzhou@tsinghua.edu.cn"
      }
    ]
  },
  {
    "title": "Semantic Part Segmentation using Compositional Model combining Shape and Appearance\n---AUTHOR---\nJianyu Wang\n---AUTHOR---\nAlan Yuille",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Wang_Semantic_Part_Segmentation_2015_CVPR_paper.pdf",
    "id": "Wang_Semantic_Part_Segmentation_2015_CVPR_paper",
    "abstract": "In this paper, we study the problem of semantic part segmentation for animals. This is more challenging than standard object detection, object segmentation and pose estimation tasks because semantic parts of animals often have similar appearance and highly varying shapes. To tackle these challenges, we build a mixture of compositional models to represent the object boundary and the boundaries of semantic parts. And we incorporate edge, appearance, and semantic part cues into the compositional model. Given part-level segmentation annotation, we develop a novel algorithm to learn a mixture of compositional models under various poses and viewpoints for certain animal classes. Furthermore, a linear complexity algorithm is offered for efficient inference of the compositional model using dynamic programming. We evaluate our method for horse and cow using a newly annotated dataset on Pascal VOC 2010 which has pixelwise part labels. Experimental results demonstrate the effectiveness of our method.",
    "topics": [
      "Semantic Part Segmentation",
      "Compositional Models",
      "Animal Pose Estimation",
      "Dynamic Programming",
      "Mixture Models"
    ],
    "references": [
      {
        "citation": "[Arbeláez, P., Hariharan, B., Gu, C., Gupta, S., Bourdev, L., & Malik, J. (2012). Semantic segmentation using regions and parts. CVPR.]"
      },
      {
        "citation": "[Fidler, S., Mottaghi, R., Yuille, A. L., & Urbas, J. (2013). Bottom-up segmentation for top-down detection. CVPR.]"
      },
      {
        "citation": "[Felzenszwalb, P. F., Girshick, R. B., McAllester, D., & Ramanan, D. (2010). Object detection with discriminatively trained part-based models. PAMI.]"
      },
      {
        "citation": "[Arbeláez, P., Maire, M., Fowlkes, C., & Malik, J. (2011). Contour detection and hierarchical image segmentation. PAMI.]"
      },
      {
        "citation": "[Mottaghi, R., Chen, X., Liu, X., Cho, N.-G., Lee, S.-W., Fidler, S., Urbas, J., & Yuille, A. L. (2014). The role of context for object detection and semantic segmentation in the wild. CVPR.]"
      },
      {
        "citation": "[Dong, J., Chen, Q., Shen, X., Yang, J., & Yan, S. (2014). Towards unified human parsing and pose estimation. CVPR.]"
      },
      {
        "citation": "[Felzenszwalb, P. F., & Huttenlocher, D. P. (2005). Pictorial structures for object recognition. IJCV.]"
      },
      {
        "citation": "[Chen, L.-C., Papandreou, G., & Yuille, A. L. (2013). Learning a dictionary of shape epitomes with applications to image labeling. ICCV.]"
      },
      {
        "citation": "[Hariharan, B., Arbeláez, P., Girshick, R., & Malik, J. (2014). Simultaneous detection and segmentation. ECCV.]"
      },
      {
        "citation": "[Dong, J., Chen, Q., Xia, W., Huang, Z., & Yan, S. (2013). A deformable mixture parsing model with parselets. ICCV.]"
      }
    ],
    "author_details": [
      {
        "name": "Jianyu Wang",
        "affiliation": "University of California, Los Angeles",
        "email": "wjyouch@ucla.edu"
      },
      {
        "name": "Alan Yuille",
        "affiliation": "University of California, Los Angeles",
        "email": "yuille@stat.ucla.edu"
      }
    ]
  },
  {
    "title": "Efﬁcient Sparse-to-Dense Optical Flow Estimation using a Learned Basis and Layers\n---AUTHOR---\nJonas Wulff\nMichael J. Black",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Wulff_Efficient_Sparse-to-Dense_Optical_2015_CVPR_paper.pdf",
    "id": "Wulff_Efficient_Sparse-to-Dense_Optical_2015_CVPR_paper",
    "abstract": "We address the elusive goal of estimating optical ﬂow both accurately and efﬁciently by adopting a sparse-to-dense approach. Given a set of sparse matches, we regress to dense optical ﬂow using a learned set of full-frame basis ﬂow ﬁelds. We learn the principal components of natural ﬂow ﬁelds using ﬂow computed from four Hollywood movies. Optical ﬂow ﬁelds are then compactly approximated as a weighted sum of the basis ﬂow ﬁelds. Our novel PCA-Flow algorithm robustly estimates these weights from sparse feature matches. The method runs in under 200ms/frame on the MPI-Sintel dataset using a single CPU and is more accurate and significantly faster than popular methods such as LDOF and Classic+NL. For some applications, however, the results are too smooth. Consequently, we develop a novel sparse layered ﬂow method in which each layer is represented by PCA-Flow. Unlike existing layered methods, estimation is fast because it uses only sparse matches. We combine information from different layers into a dense ﬂow ﬁeld using an image-aware MRF. The resulting PCA-Layers method runs in 3.2s/frame, is significantly more accurate than PCA-Flow, and achieves state-of-the-art performance in occluded regions on MPI-Sintel.",
    "topics": [
      "Optical Flow Estimation",
      "Sparse-to-Dense Methods",
      "Principal Component Analysis (PCA)",
      "Layered Flow Models",
      "Markov Random Fields (MRF)"
    ],
    "references": [],
    "author_details": [
      {
        "name": "Jonas Wulff",
        "affiliation": "Max Planck Institute for Intelligent Systems, T¨ubingen, Germany",
        "email": "jonas.wulff@tue.mpg.de"
      },
      {
        "name": "Michael J. Black",
        "affiliation": "Max Planck Institute for Intelligent Systems, T¨ubingen, Germany",
        "email": "black@tue.mpg.de"
      }
    ]
  },
  {
    "title": "Active Learning for Structured Probabilistic Models with Histogram Approximation\n---AUTHOR---\n[Authors not provided in the provided text]",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Sun_Active_Learning_for_2015_CVPR_supplemental.pdf",
    "id": "Sun_Active_Learning_for_2015_CVPR_supplemental",
    "abstract": "This supplementary material presents proofs for Lemma 1 and Lemma 2, which underpin the active learning approach for structured probabilistic models using histogram approximation. The core idea is to strategically select data points that maximize the model's learning progress. The proofs detail how to optimize the approximation parameters to minimize the KL divergence between the approximate and true distributions, ultimately leading to a normalized histogram over the top M most probable locations in the distribution. Qualitative results demonstrate the effectiveness of the proposed approach compared to Gibbs sampling, highlighting its ability to identify genuinely uncertain images for model refinement.\n\n---TOPIICS---\nActive Learning\nStructured Probabilistic Models\nHistogram Approximation\nKL Divergence\nLagrangian Multipliers",
    "topics": [],
    "references": [],
    "author_details": []
  },
  {
    "title": "Human Action Segmentation with Hierarchical Supervoxel Consistency\n---AUTHOR---\nJiasen Lu\nRan Xu\nJason J. Corso",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Lu_Human_Action_Segmentation_2015_CVPR_paper.pdf",
    "id": "Lu_Human_Action_Segmentation_2015_CVPR_paper",
    "abstract": "Detailed analysis of human action, such as action classification, detection and localization has received increasing attention. However, detailed automatic segmentation of human action has comparatively been unexplored. This paper addresses this gap by proposing a hierarchical MRF model to bridge low-level video fragments with high-level human motion and appearance. The model connects different levels of the supervoxel hierarchy to enforce the consistency of the human segmentation. Experimental results demonstrate that the proposed model significantly outperforms state-of-the-art methods on actionness and improves upon baseline models in action segmentation.\n\n---TOPICCS---\nHuman Action Segmentation\nSupervoxel Hierarchy\nMarkov Random Fields (MRF)\nAction Localization\nComputer Vision",
    "topics": [],
    "references": [
      {
        "citation": "[Boykov, Y., Vekslar, O., & Zabih, R. (2001). Fast Approximate Energy Minimization via Graph Cuts. IEEE Transactions on Pattern Analysis and Machine Intelligence, 23(11), 1222–1239.]"
      },
      {
        "citation": "[Karpathy, A., Toderici, G., Shetty, S., Leung, T., Sukthankar, R., & Fei-Fei, L. (2014). Large-scale video classification with convolutional neural networks. IEEE Conference on Computer Vision and Pattern Recognition.]"
      },
      {
        "citation": "[Sheikh, Y., Javed, O., & Kanade, T. (2009). Background subtraction for freely moving cameras. IEEE International Conference on Computer Vision.]"
      },
      {
        "citation": "[Felzenszwalb, P. F., Girshick, R. B., McAllester, D., & Ramanan, D. (2010). Object detection with discriminatively trained part-based models. IEEE transactions on pattern analysis and machine intelligence, 32(9), 1627–45.]"
      },
      {
        "citation": "[Kuehne, H., Jhuang, H., Garrote, E., Poggio, T., & Serre, T. (2011). HMDB: A large video database for human motion recognition. IEEE International Conference on Computer Vision.]"
      },
      {
        "citation": "[Gorelick, L., Blank, M., Shechtman, E., Irani, M., & Basri, R. (2007). Actions as space-time shapes. IEEE Transactions on Pattern Analysis and Machine Intelligence, 29(12), 2247–2253.]"
      },
      {
        "citation": "[Jain, M., Jegou, H., & Bouthemy, P. (2013). Better exploiting motion for better action recognition. IEEE Conference on Computer Vision and Pattern Recognition.]"
      },
      {
        "citation": "[Lan, T., Wang, Y., & Mori, G. (2011). Discriminative figure-centric models for joint action localization and recognition. IEEE International Conference on Computer Vision.]"
      },
      {
        "citation": "[Soomro, K., Zamir, A. R., & Shah, M. (2012). A dataset of 101 human action classes from videos in the wild. Technical report, University of Central Florida, Center for Research in Computer Vision.]"
      },
      {
        "citation": "[Wang, H., Kläser, A., Schmid, C., & Liu, C.-L. (2011). Action Recognition by Dense Trajectories. IEEE Conference on Computer Vision and Pattern Recognition.]"
      }
    ],
    "author_details": [
      {
        "name": "Jiasen Lu",
        "affiliation": "Computer Science and Engineering, SUNY at Buffalo",
        "email": "jiasenlu@buffalo.edu"
      },
      {
        "name": "Ran Xu",
        "affiliation": "Computer Science and Engineering, SUNY at Buffalo",
        "email": "rxu2@buffalo.edu"
      },
      {
        "name": "Jason J. Corso",
        "affiliation": "Electrical Engineering and Computer Science, University of Michigan",
        "email": "jjcorso@eecs.umich.edu"
      }
    ]
  },
  {
    "title": "Transport-Based Single Frame Super Resolution of Very Low Resolution Face Images",
    "authors": [
      "Soheil Kolouri",
      "Gustavo K. Rohde"
    ],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Kolouri_Transport-Based_Single_Frame_2015_CVPR_paper.pdf",
    "id": "Kolouri_Transport-Based_Single_Frame_2015_CVPR_paper",
    "abstract": "Extracting high-resolution information from highly degraded facial images is an important problem with several applications in science and technology. Here we describe a single frame super resolution technique that uses a transport-based formulation of the problem. The method consists of a training and a testing phase. In the training phase, a nonlinear Lagrangian model of high-resolution facial appearance is constructed fully automatically. In the testing phase, the resolution of a degraded image is enhanced by finding the model parameters that best fit the given low resolution data. We test the approach on two face datasets, namely the extended Yale Face Database B and the AR face datasets, and compare it to state of the art methods. The proposed method outperforms existing solutions in problems related to enhancing images of very low resolution.",
    "topics": [
      "Super-resolution (SR)",
      "Single-frame Super Resolution (SFSR)",
      "Optimal Transport",
      "Lagrangian modeling",
      "Face image reconstruction"
    ],
    "references": [
      {
        "citation": "[L. Ambrosio. Optimal transport maps in monge-kantorovich problem. arXiv preprint math/0304389, 2003.]"
      },
      {
        "citation": "[S. Baker and T. Kanade. Hallucinating faces. In Automatic Face and Gesture Recognition, 2000. Proceedings. Fourth IEEE International Conference on, pages 83–88. IEEE, 2000.]"
      },
      {
        "citation": "[S. Basu, S. Kolouri, and G. K. Rohde. Detecting and visualizing cell phenotype differences from microscopy images using transport-based morphometry. Proceedings of the National Academy of Sciences, 6(9):3448–3453, 2014.]"
      },
      {
        "citation": "[S. Borman and R. L. Stevenson. Super-resolution from image sequences-a review. In Circuits and Systems, Midwest Symposium on, pages 374–374. IEEE Computer Society, 1998.]"
      },
      {
        "citation": "[J. Wright, A. Y. Yang, A. Ganesh, S. S. Sastry, and Y. Ma. Robust face recognition via sparse representation. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 31(2):210–227, 2009.]"
      },
      {
        "citation": "[A. M. Martinez. The AR face database. CVC Technical Report, 24, 1998.]"
      },
      {
        "citation": "[J. Yang, J. Wright, T. S. Huang, and Y. Ma. Image super-resolution via sparse representation. Image Processing, IEEE Transactions on, 19(11):2861–2873, 2010.]"
      },
      {
        "citation": "[W. W. Zou and P. C. Yuen. Very low resolution face recognition problem. Image Processing, IEEE Transactions on, 21(1):327–340, 2012.]"
      },
      {
        "citation": "[K. Nasrollahi and T. B. Moeslund. Super-resolution: A comprehensive survey. Machine Vision & Applications, 2014.]"
      },
      {
        "citation": "[A. Georghiadies, P. Belhumeur, and D. Kriegman. From few to many: Illumination cone models for face recognition un-der variable lighting and pose. IEEE Trans. Pattern Anal. Mach. Intelligence, 23(6):643–660, 2001.]"
      }
    ],
    "author_details": [
      {
        "name": "Soheil Kolouri",
        "affiliation": "Carnegie Mellon University",
        "email": "skolouri@andrew.cmu.edu"
      },
      {
        "name": "Gustavo K. Rohde",
        "affiliation": "Carnegie Mellon University",
        "email": "gustavor@cmu.edu"
      }
    ]
  },
  {
    "title": "Learning with Dataset Bias in Latent Subcategory Models\n---AUTHOR---\nDimitris Stamos\nSamuele Martelli\nMoin Nabi\nAndrew McDonald\nVittorio Murino\nMassimiliano Pontil",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Stamos_Learning_With_Dataset_2015_CVPR_paper.pdf",
    "id": "Stamos_Learning_With_Dataset_2015_CVPR_paper",
    "abstract": "Latent subcategory models (LSMs) offer improvements over linear SVMs, but training them is challenging due to optimization difficulties and the need for large training sets. Combining datasets from different sources can address the training set size issue, but standard machine learning methods often fail to account for dataset bias, leading to decreased performance. This paper presents a model that jointly learns an LSM for each dataset and a compound LSM, leveraging multiple biased datasets to tackle a common classification task. The method aims to borrow statistical strength while reducing inherent bias, demonstrating significant improvements over existing approaches in experiments on PASSCAL, LabelMe, Caltech101, and SUN09 datasets.\n\n---TOPICCS---\nLatent Subcategory Models (LSMs)\nDataset Bias\nMultitask Learning\nObject Recognition\nDeformable Part-based Models",
    "topics": [],
    "references": [
      {
        "citation": "[Malisiewicz, T., Gupta, A., & Efroos, A. (2011). Ensemble of exemplar-svms for object detection and beyond. ICCV.]"
      },
      {
        "citation": "[Aiolli, F., & Sperduti, A. (2005). Multiclass classification with multi-prototype support vector machines. Journal of Machine Learning Research.]"
      },
      {
        "citation": "[Maurer, A., Pontil, M., & Romera-Paredes, B. (2013). Sparse coding for multitask and transfer learning. Proceedings of the 30th International Conference on Machine Learning.]"
      },
      {
        "citation": "[Boyd, S., & Vandenberghe, L. (2009). Convex optimization. Cambridge University Press.]"
      },
      {
        "citation": "[Torralba, A., & Efroos, A. (2011). Unbiased look at dataset bias. CVPR.]"
      },
      {
        "citation": "[Choi, M., Lim, J. J., Torralba, A., & Willsky, A. (2010). Exploiting hierarchical context on a large database of object categories. CVPR.]"
      },
      {
        "citation": "[Felzenszwalb, P. F., Girshick, R. B., McAlleser, D., & Ramanan, D. (2010). Object detection with discriminatively trained part-based models. IEEE Transactions on Pattern Analysis and Machine Intelligence.]"
      },
      {
        "citation": "[Girshick, R., & Malik, J. (2013). Training deformable part models with decorrelated features. ICCV.]"
      },
      {
        "citation": "[Khosla, A., Zhou, T., Malisiewicz, T., Efroos, A., & Torralba, A. (2012). Undoing the damage of dataset bias. ECCV.]"
      },
      {
        "citation": "[Torralba, A., Russell, B. C., Murphy, K. P., & Freeman, W. T. (2008). Labelme: a database and web-based tool for image annotation. International Journal of Computer Vision.]"
      }
    ],
    "author_details": [
      {
        "name": "Dimmitris Stamos",
        "affiliation": "Department of Computer Science, University College London, UK",
        "email": "d.stamos@cs.ucl.ac.uk"
      },
      {
        "name": "Samuele Martelli",
        "affiliation": "Pattern Analysis & Computer Vision, Istituto Italiano di Tecnologia, Italy",
        "email": "samuele.marтелli@iit.it"
      },
      {
        "name": "Moin Nabi",
        "affiliation": "Pattern Analysis & Computer Vision, Istituto Italiano di Tecnologia, Italy",
        "email": "moin.nabi@iit.it"
      },
      {
        "name": "Andrew McDonald",
        "affiliation": "Department of Computer Science, University College London, UK",
        "email": "a.mcdoanlds@cs.ucl.ac.uk"
      },
      {
        "name": "Vittorio Murino",
        "affiliation": "Pattern Analysis & Computer Vision, Istituto Italiano di Tecnologia, Italy",
        "email": "vittorio.murino@iit.it"
      },
      {
        "name": "Massimiliano Pontil",
        "affiliation": "Department of Computer Science, University College London, UK",
        "email": "m.pontil@cs.ucl.ac.uk"
      }
    ]
  },
  {
    "title": "Structured Sparse Subspace Clustering: A Uniﬁed Optimization Framework\n---AUTHORs---\nRen´e Vidal\nChun-Guang Li",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Li_Structured_Sparse_Subspace_2015_CVPR_paper.pdf",
    "id": "Li_Structured_Sparse_Subspace_2015_CVPR_paper",
    "abstract": "This paper addresses the subspace clustering problem by proposing a unified optimization framework. Existing approaches typically follow a two-stage process: learning an affinity matrix followed by spectral clustering. This paper overcomes the limitations of this approach by jointly optimizing the affinity matrix and the segmentation. The proposed framework is based on expressing each data point as a structured sparse linear combination of other data points, where the structure is induced by a norm dependent on the unknown segmentation. The authors demonstrate that the segmentation and structured sparse representation can be found via an alternating direction method of multipliers combined with spectral clustering, and validate the approach with experiments on synthetic and real-world datasets.\n\n---TOPSICS---\nSubspace Clustering\nSparse Representation\nSpectral Clustering\nOptimization Framework\nStructured Norms",
    "topics": [],
    "references": [
      {
        "citation": "[Agarwal, P., & Mustafa, N. (2004). k-means projective clustering. ACM Symposium on Principles of database systems.]"
      },
      {
        "citation": "[Candès, E., Wakin, M., & Boyd, S. (2008). Enhancing sparsity by reweighted ℓ1 minimization. Journal of Fourier Analysis and Applications, 14(5), 877–905.]"
      },
      {
        "citation": "[Elhamifar, E., & Vidal, R. (2009). Sparse subspace clustering. IEEE Conference on Computer Vision and Pattern Recognition.]"
      },
      {
        "citation": "[Boyd, S., Parikh, N., Chu, E., Peleato, B., & Eckstein, J. (2010). Distributed optimization and statistical learning via the alternating direction method of multipliers. Foundations and Trends in Machine Learning, 3(1), 1–122.]"
      },
      {
        "citation": "[Vidal, R., & Elhamifar, E. (2013). Sparse subspace clustering: Algorithm, theory, and applications. IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(11), 2765–2781.]"
      },
      {
        "citation": "[Ma, Y., Derksen, H., Hong, W., & Wright, J. (2007). Segmentation of multivariate mixed data via lossy coding and compression. IEEE Transactions on Pattern Analysis and Machine Intelligence, 29(9), 1546–1562.]"
      },
      {
        "citation": "[Hu, Z., Lin, M., Feng, J., & Zhou, J. (2014). Smooth representation clustering. CVPR.]"
      },
      {
        "citation": "[Liu, G., Lin, Z., Yan, S., Sun, J., & Ma, Y. (2013). Robust recovery of subspace structures by low-rank representation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(1), 171–184.]"
      },
      {
        "citation": "[Tron, R., & Vidal, R. (2014). Low rank subspace clustering (LRSC). Pattern Recognition Letters, 43, 47–61.]"
      },
      {
        "citation": "[Lin, Z., Chen, M., Wu, L., & Ma, Y. (2011). The augmented Lagrange multiplier method for exact recovery of corrupted low-rank matrices. arXiv:1009.5055v2.]"
      }
    ],
    "author_details": [
      {
        "name": "René Vidal",
        "affiliation": "Center for Imaging Science, Johns Hopkins University",
        "email": "[Email not available in provided text]"
      },
      {
        "name": "Chun-Guang Li",
        "affiliation": "SICe, Beijing University of Posts and Telecommunications",
        "email": "[Email not available in provided text]"
      }
    ]
  },
  {
    "title": "Layered RGBD Scene Flow Estimation\n---AUTHOR---\nDeqing Sun\nErik B. Sudderth\nHanspeter Pﬁster",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Sun_Layered_RGBD_Scene_2015_CVPR_paper.pdf",
    "id": "Sun_Layered_RGBD_Scene_2015_CVPR_paper",
    "abstract": "Estimating scene ﬂow from RGBD sequences has received increasing attention as consumer depth sensors become widely available. Although depth information allows the recovery of 3D motion from a single view, it poses new challenges, particularly due to misalignment between depth and RGB image edges and errors in occlusion regions. To better utilize depth for occlusion reasoning, this paper proposes a layered RGBD scene ﬂow method that jointly solves for scene segmentation and motion. The key observation is that noisy depth is sufficient to decide the depth ordering of layers, avoiding a computational bottleneck for RGB layered methods. Furthermore, depth enables estimation of per-layer 3D rigid motion to constrain layer movement. Experimental results demonstrate the effectiveness of the layered approach for RGBD scene ﬂow estimation.",
    "topics": [
      "RGBD Scene Flow Estimation",
      "Layered Models",
      "Depth Ordering",
      "3D Rigid Motion",
      "Occlusion Reasoning"
    ],
    "references": [
      {
        "citation": "[Amit, Y., & Geman, D. (1997). Shape quantization and recognition with randomized trees. Neural computation, 9(7), 1545–1588.]"
      },
      {
        "citation": "[Ayer, S., & Sawhney, H. S. (1995). Layered representation of motion video using robust maximum-likelihood estimation of mixture models and MDL encoding. In IEEE International Conference on Computer Vision.]"
      },
      {
        "citation": "[Baker, S., Scharstein, D., Lewis, J. P., Roth, S., Black, M. J., & Szeliski, R. (2011). A database and evaluation methodology for optical ﬂow. IJCV, 92(1), 1–31.]"
      },
      {
        "citation": "[Barron, J. T., & Malik, J. (2013). Intrinsic scene properties from a single rgb-d image. IEEE Conference on Computer Vision and Pattern Recognition.]"
      },
      {
        "citation": "[Brox, T., Bruhn, A., Papenberg, N., & Weickert, J. (2004). High accuracy optical ﬂow estimation based on a theory for warping. In European Conference on Computer Vision, pages 25–36.]"
      },
      {
        "citation": "[Darrell, T., & Pentland, A. (1995). Cooperative robust estimation using layers of support. IEEE TPAM, 17(5), 474–487.]"
      },
      {
        "citation": "[Geiger, A., Lenz, P., & Urtasun, R. (2012). Are we ready for autonomous driving? the kitti vision benchmark suite. In IEEE Conference on Computer Vision and Pattern Recognition.]"
      },
      {
        "citation": "[Horn, B. (1981). Closed-form solution of absolute orientation using unit quaternions. JOSA A, 4(4), 629–642.]"
      },
      {
        "citation": "[Irani, M., Anandan, P., & Weinshall, D. (1998). From reference frames to reference planes: Multi-view parallax geometry and applications. In European Conference on Computer Vision.]"
      },
      {
        "citation": "[Quirosga, J., Brox, T., Devernay, F., et al. (2014). Dense semi-rigid scene ﬂow estimation from rgbd images. In ICIP-IEEE International Confer-ence on Image Processing.]"
      }
    ],
    "author_details": [
      {
        "name": "Deqing Sun",
        "affiliation": "Harvard University",
        "email": "[Email not available in provided text]"
      },
      {
        "name": "Erik B. Sudderth",
        "affiliation": "Brown University",
        "email": "[Email not available in provided text]"
      },
      {
        "name": "Hanspeter Pﬁster",
        "affiliation": "Harvard University",
        "email": "[Email not available in provided text]"
      }
    ]
  },
  {
    "title": "Learning to Compare Image Patches via Convolutional Neural Networks\n---AUTHORs---\nSergey Zagoruyko\nNikos Komodakis",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Zagoruyko_Learning_to_Compare_2015_CVPR_paper.pdf",
    "id": "Zagoruyko_Learning_to_Compare_2015_CVPR_paper",
    "abstract": "In this paper, we demonstrate a method for learning a general similarity function for image patches directly from image data, eliminating the need for manually designed features. We utilize convolutional neural networks (CNNs) to encode this function, exploring various architectures adapted for the task. The approach significantly outperforms state-of-the-art methods on several problems and benchmark datasets, implicitly accounting for various transformations and effects. The paper also proposes and evaluates different neural network models and highlights architectures that offer improved performance.",
    "topics": [
      "Convolutional Neural Networks (CNNs)",
      "Image Patch Similarity",
      "Deep Learning",
      "Computer Vision",
      "Feature Descriptors"
    ],
    "references": [],
    "author_details": []
  },
  {
    "title": "Flying Objects Detection from a Single Moving Camera\n---AUTHORs---\nArtem Rozantsev\nVincent Lepetit\nPascal Fua",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Rozantsev_Flying_Objects_Detection_2015_CVPR_paper.pdf",
    "id": "Rozantsev_Flying_Objects_Detection_2015_CVPR_paper",
    "abstract": "We propose an approach to detect ﬂying objects such as UAVs and aircrafts when they occupy a small portion of the ﬁeld of view, possibly moving against complex backgrounds, and are ﬁlmed by a camera that itself moves. Solving such a difﬁcult problem requires combining both appearance and motion cues. To this end we propose a regression-based approach to motion stabilization of local image patches that allows us to achieve effective classiﬁcation on spatio-temporal image cubes and outperform state-of-the-art techniques. As the problem is relatively new, we collected two challenging datasets for UAVs and Aircrafts, which can be used as benchmarks for ﬂying objects detection and vision-guided collision avoidance.\n\n---TOPIC---\nFlying Object Detection\nMotion Stabilization\nSpatio-Temporal Image Cubes\nVision-Guided Collision Avoidance\nMoving Camera",
    "topics": [],
    "references": [
      {
        "citation": "[A. Bosch, A. Zisserman, and X. Munoz, Image Classification Using Random Forests and Ferns, International Conference on Computer Vision, 2007]"
      },
      {
        "citation": "[L. Breiman, Random Forests, Machine Learning, 2001]"
      },
      {
        "citation": "[T. Brox and J. Malik, Large Displacement Optical Flow: Descriptor Matching in Variational Motion Estimation, IEEE Transactions on Pattern Analysis and Machine Intelligence, 2011]"
      },
      {
        "citation": "[G. Conte and P. Doherty, An Integrated UAV Navigation System Based on Aerial Image Matching, IEEE Aerospace Conference, 2008]"
      },
      {
        "citation": "[P. Dollar, Piotr’s Computer Vision Matlab Toolbox (PMT), http://vision.ucsd.edu/~pdollar/toolbox/doc/index.html, n.d.]"
      },
      {
        "citation": "[P. Dollar, V. Rabaud, G. Cottrell, and S. Belongie, Behavior Recognition via Sparse Spatio-Temporal Features, VS-PETS, 2005]"
      },
      {
        "citation": "[P. Dollar, Z. Tu, P. Perona, and S. Belongie, Integral Channel Features, British Machine Vision Conference, 2009]"
      },
      {
        "citation": "[P. Felzenszwalb, R. Girshick, D. McAllester, and D. Ramanan, Object Detection with Discriminatively Trained Part Based Models, IEEE Transactions on Pattern Analysis and Machine Intelligence, 2010]"
      },
      {
        "citation": "[C. Forster, M. Pizzolli, and D. Scaramuzza, SVO: Fast Semi-Direct Monocular Visual Odometry, International Conference on Robotics and Automation, 2014]"
      },
      {
        "citation": "[C. Hane, C. Zach, J. Lim, A. Ranganathan, and M. Pollefeys, Stereo Depth Map Fusion for Robot Navigation, Proceedings of International Conference on Intelligent Robots and Systems, 2011]"
      }
    ],
    "author_details": [
      {
        "name": "Artem Rozantsev",
        "affiliation": "Computer Vision Laboratory, ´Ecole Polytechnique F´ed´erale de Lausanne (EPFL)",
        "email": "artem.rozantsev@epfl.ch"
      },
      {
        "name": "Vincent Lepetit",
        "affiliation": "Computer Vision Laboratory, ´Ecole Polytechnique F´ed´erale de Lausanne (EPFL)",
        "email": "lepetit@icg.tugraz.at"
      },
      {
        "name": "Pascal Fua",
        "affiliation": "Computer Vision Laboratory, ´Ecole Polytechnique F´ed´erale de Lausanne (EPFL)",
        "email": "pascal.fua@epfl.ch"
      }
    ]
  },
  {
    "title": "Articulated Motion Discovery using Pairs of Trajectories\n---AUTHOR---\nLuca Del Pero\nSusanna Ricco\nRahul Sukthankar\nVittorio Ferrari",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Pero_Articulated_Motion_Discovery_2015_CVPR_paper.pdf",
    "id": "Pero_Articulated_Motion_Discovery_2015_CVPR_paper",
    "abstract": "We propose an unsupervised approach for discovering characteristic motion patterns in videos of highly articulated objects performing natural, unscriped behaviors, such as tigers in the wild. We discover consistent patterns in a bottom-up manner by analyzing the relative displacements of large numbers of ordered trajectory pairs through time, such that each trajectory is attached to a different moving part on the object. The pairs of trajectories descriptor relies entirely on motion and is more discriminative than state-of-the-art features that employ single trajecto-ries. Our method generates temporal video intervals, each automatically trimmed to one instance of the discovered behavior, and clusters them by type (e.g., running, turning head, drinking water). We present experiments on two datasets: dogs from YouTube-Objects and a new dataset of National Geographic tiger videos. Results confirm that our proposed descriptor outperforms existing appearance- and trajectory-based descriptors (e.g., HOG and DTFs) on both datasets and enables us to segment unconstrained animal video into intervals containing single behaviors.\n\n---TOPICCS---\nArticulated Motion Discovery\nTrajectory Analysis\nUnsupervised Learning\nBehavior Segmentation\nMotion Patterns",
    "topics": [],
    "references": [],
    "author_details": [
      {
        "name": "Luca Del Pero",
        "affiliation": "University of Edinburgh",
        "email": "ldelper@inf.ed.ac.uk"
      },
      {
        "name": "Susanna Ricco",
        "affiliation": "Google Research",
        "email": "ricco@google.com"
      },
      {
        "name": "Rahul Sukthankar",
        "affiliation": "Google Research",
        "email": "sukthankar@google.com"
      },
      {
        "name": "Vittorio Ferrari",
        "affiliation": "University of Edinburgh",
        "email": "ferrari@inf.ed.ac.uk"
      }
    ]
  },
  {
    "title": "Latent Trees for Estimating Intensity of Facial Action Units\n---AUTHOR---\nSebastian Kaltwang\nSinisia Todorovic\nMaja Pantic",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Kaltwang_Latent_Trees_for_2015_CVPR_paper.pdf",
    "id": "Kaltwang_Latent_Trees_for_2015_CVPR_paper",
    "abstract": "This paper addresses the problem of estimating intensity levels of Facial Action Units (FAUs) in videos, a crucial step towards interpreting facial expressions. The authors propose a generative latent tree (LT) model to handle the uncertainty of input facial landmark points. Their structure learning iteratively builds the LT, and they derive closed-form expressions for posterior marginals for efficient inference. Evaluations on benchmark datasets demonstrate superior performance compared to state-of-the-art methods, even with noisy landmark data. The effectiveness of the structure learning is further validated through probabilistic sampling of facial expressions.\n\n---TOPICCS---\nFacial Action Units (FAUs)\nLatent Tree Models\nFacial Expression Analysis\nGenerative Bayesian Framework\nFacial Landmark Detection",
    "topics": [],
    "references": [
      {
        "citation": "[Ekman, P., Friesen, W. V., & Hager, J. C. (2002). Facial action coding system. A Human Face.] - This is a foundational reference for facial action coding, crucial for understanding the methodology likely used in the paper."
      },
      {
        "citation": "[Ekman, P., & Rosenberg, E. L. (2005). What the face reveals: Basic and applied studies of spontaneous expression using the Facial Action Coding System. Oxford Univ. Press, USA.] - Provides context and a deeper understanding of facial expression analysis."
      },
      {
        "citation": "[Koller, D., & Friedman, N. (2009). Probablistic graphical models: principles and techniques. MIT press.] - A core reference for understanding the probabilistic modeling techniques likely employed."
      },
      {
        "citation": "[Friedman, N. (1998). The Bayesian Structural EM Algorithm. Proc. 14th Conf. Uncertain. Artif. Intell., UAI’98, pages 129–138.] - Provides a specific algorithm likely used in the paper."
      },
      {
        "citation": "[Chang, C.-C., & Lin, C.-J. (2011). LIBSVM: A library for sup-port vector machines. ACM Trans. Intell. Syst. Technol., 2(3):27:1–27:27.] - Indicates the use of SVMs, a common machine learning technique."
      },
      {
        "citation": "[Mavadati, S., Mahoor, M., Bartlett, K., TrinH, P., & Cohn, J. F. (2013). DISFA: A Spontaneous Facial Action Intensity Database. IEEE Trans. Affect. Comput., 4(2):151–160.] - This is likely a dataset used in the paper, making it a very important reference."
      },
      {
        "citation": "[Ojala, T., Pietikainen, M., & Maenpaa, T. (2002). Multiresolution gray-scale and rotation invariant texture classiﬁcation with local binary patterns. IEEE Trans. Pattern Anal. Mach. Intell., 24(7):971–987.] - Suggests the use of texture analysis techniques."
      },
      {
        "citation": "[Rudovic, O., Pavlovic, V., & Pantic, M. (2015). Context-sensitive Dynamic Ordinal Regression for Intensity Estimation of Facial Action Units. IEEE Trans. Pattern Anal. Mach. Intell., 37(5):944–958.] - This reference likely describes a specific method for intensity estimation."
      },
      {
        "citation": "[Zeng, Z., Pantic, M., Roisman, G. I., & Huang, T. S. (2009). A Survey of Affect Recognition Methods: Audio, Visual, and Spontaneous Expressions. IEEE Trans. Pattern Anal. Mach. Intel., 31(1):39–58.] - Provides context and a broader perspective on the field."
      },
      {
        "citation": "[Scherer, K. R., & Ellgring, H. (2007). Are facial expressions of emotion produced by categorical affect programs or dynamically driven by appraisal? Emotion, 7(1):113–130.] - Provides theoretical background on the generation of facial expressions."
      }
    ],
    "author_details": [
      {
        "name": "Sebastian Kaltwang",
        "affiliation": "Imperial College London",
        "email": "sk2608@imperial.ac.uk"
      },
      {
        "name": "Sinisa Todorovic",
        "affiliation": "Oregon State University",
        "email": "sinisa@eecs.oregonstate.edu"
      },
      {
        "name": "Maja Pantic",
        "affiliation": "Imperial College London",
        "email": "m.pantic@imperial.ac.uk"
      }
    ]
  },
  {
    "title": "Robust Video Segment Proposals with Painless Occlusion Handling\n---AUTHOR---\nZhengyang Wu\n---AUTHOR---\nFuxin Li\n---AUTHOR---\nRahul Sukthankar\n---AUTHOR---\nJames M. Rehg",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Wu_Robust_Video_Segment_2015_CVPR_paper.pdf",
    "id": "Wu_Robust_Video_Segment_2015_CVPR_paper",
    "abstract": "Abstract not found",
    "topics": [],
    "references": [
      {
        "citation": "[Arbeláez, P., Pont-Tuset, J., Barron, J. T., Marques, F., & Malik, J. (2014). Multiscale combinatorial grouping. In *IEEE Conference on Computer Vision and Pattern Recognition*.]"
      },
      {
        "citation": "[Carreira, J., & Sminchisescu, C. (2010). CPMC: Automatic Object Segmentation Using Constrained Parametric Min-Cuts. *IEEE Transaction on Pattern Analysis and Machine Intelligence*, 2012.]"
      },
      {
        "citation": "[Carreira, J., & Sminchisescu, C. (2010). Object Recognition by Sequential Figure-Ground Ranking. *International Journal of Computer Vision*, 2012.]"
      },
      {
        "citation": "[Galasso, F., Keuper, M., Brox, T., & Schiele, B. (2014). Spectral graph reduction for efﬁcient image and streaming video seg-mentation. In *IEEE Conf. on Computer Vision and Pattern Recognition*.]"
      },
      {
        "citation": "[Huang, Y., & Essa, I. (2005). Tracking multiple objects through occlusions. In *Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on*, volume 2, pages 1051–1058. IEEE.]"
      },
      {
        "citation": "[Krahenbuhl, P., & Koltun, V. (2014). Geodesic object proposals. In *European Conference on Computer Vision*.]"
      },
      {
        "citation": "[Li, F., Carreira, J., & Sminchisescu, C. (2010). Object recognition as ranking holistic ﬁgure-ground hypotheses. In *IEEE Conference on Computer Vision and Pattern Recognition*.]"
      },
      {
        "citation": "[Ochs, P., Malik, J., & Brox, T. (2014). Segmentation of moving objects by long term video analysis. *PAMI*.]"
      },
      {
        "citation": "[Wang, L., Hua, G., Sukthankar, R., Xue, J., & Zheng, N. (2014). Video object discovery and co-segmentation with extremely weak supervision. In *Computer Vision–ECCV 2014*, pages 596–611. Springer.]"
      },
      {
        "citation": "[Zhang, D., Javed, O., & Shah, M. (2013). Video object segmentation through spatially accurate and temporally dense extraction of primary object regions. In *CVPR*.]"
      }
    ],
    "author_details": [
      {
        "name": "Zhengyang Wu",
        "affiliation": "Computational Perception Lab - Georgia Tech",
        "email": "zwu66@cc.gatech.edu"
      },
      {
        "name": "Fuxin Li",
        "affiliation": "Computational Perception Lab - Georgia Tech",
        "email": "ﬂi@cc.gatech.edu"
      },
      {
        "name": "Rahul Sukthankar",
        "affiliation": "Google Research",
        "email": "sukthankar@google.com"
      },
      {
        "name": "James M. Rehg",
        "affiliation": "Computational Perception Lab - Georgia Tech",
        "email": "rehg@cc.gatech.edu"
      }
    ]
  },
  {
    "title": "Revisiting Kernelized Locality-Sensitive Hashing for Improved Large-Scale Image Retrieval\n---AUTHOR---\nKe Jiang\n---AUTHOR---\nQichao Que\n---AUTHOR---\nBrian Kulis",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Jiang_Revisiting_Kernelized_Locality-Sensitive_2015_CVPR_paper.pdf",
    "id": "Jiang_Revisiting_Kernelized_Locality-Sensitive_2015_CVPR_paper",
    "abstract": "This paper revisits kernelized locality-sensitive hashing (KLSH), a popular method for approximate nearest-neighbor searches in reproducing kernel Hilbert spaces. The authors present a new interpretation of KLSH that resolves conceptual difficulties in the existing motivation and provides the first formal retrieval performance bounds. Their analysis reveals two techniques for boosting the empirical performance of KLSH, leading to at least a 12% improvement in recall performance on benchmark image retrieval datasets.",
    "topics": [
      "Kernelized Locality-Sensitive Hashing (KLSH)",
      "Approximate Nearest Neighbor Search",
      "Image Retrieval",
      "Reproducing Kernel Hilbert Space (RKHS)",
      "Performance Bounds"
    ],
    "references": [
      {
        "citation": "[A. Bourrier, F. Perronnin, R. Gribonval, P. Perez, and H. Je-gou. Explicit embeddings for nearest neighbor search with Mercer kernels. Journal of Mathematical Imaging and Vision, 2015.] - Appears frequently, likely central to the work."
      },
      {
        "citation": "[M. Charikar. Similarity estimation techniques for rounding algorithms. In ACM Symposium on Theory of Computing, 2002.] - Referenced for similarity estimation, a key concept."
      },
      {
        "citation": "[M. Datar, N. Immorlica, P. Indyk, and V. Mirrokni. Locality-sensitive hashing scheme based on p-stable distributions. In Proceedings of the Symposium on Computational Geometry, 2004.] - A foundational paper on LSH."
      },
      {
        "citation": "[P. Drineas and M. Mahoney. On the Nystr¨om method for approximating a gram matrix for improved kernel-based learn-ing. Journal of Machine Learning Research, 6:2153–2175, 2005.] - Nyström method is used for kernel approximation."
      },
      {
        "citation": "[A. Gionis, P. Indyk, and R. Motwani. Similarity search in high dimension via hashing. In Proceedings of the International Conference on Very Large DataBases, 1999.] - Early work on hashing for similarity search."
      },
      {
        "citation": "[H. Jegou, M. Douze, and C. Schmid. Product quantization for nearest neighbor search. IEEE Transactions on Pattern Analysis and Machine Intelligence, 33:117–128, 2011.] - Product quantization is a common technique."
      },
      {
        "citation": "[B. Kulis and K. Grauman. Kernelized locality-sensitive hashing for scalable image search. In IEEE International Conference on Computer Vision and Pattern Recognition, 2009.] - A key paper on kernelized LSH."
      },
      {
        "citation": "[B. Kulis and K. Grauman. Kernelized locality-sensitive hashing. IEEE Transactions on Pattern Analysis and Machine Intelligence, 34(6):1092–1104, 2012.] - Follow-up work on kernelized LSH."
      },
      {
        "citation": "[Y. Mu, J. Shen, and S. Yan. Weakly-supervised hashing in kernel space. In IEEE International Conference on Computer Vision and Pattern Recognition, 2010.] - Addresses hashing in a kernel space."
      },
      {
        "citation": "[A. Rahimi and B. Recht. Random features for large-scale kernel machines. In Advances in Neural Information Processing Systems, 2007.] - Random features are a common technique for kernel approximation."
      }
    ],
    "author_details": [
      {
        "name": "Ke Jiang",
        "affiliation": "The Ohio State University",
        "email": "jiangk@cse.ohio-state.edu"
      },
      {
        "name": "Qichao Que",
        "email": "que@cse.ohio-state.edu"
      },
      {
        "name": "Brian Kulis",
        "affiliation": "The Ohio State University",
        "email": "kulis@cse.ohio-state.edu"
      }
    ]
  },
  {
    "title": "Hierarchically-Constrained Optical Flow\n---AUTHOR---\nRyan Kennedy\n---AUTHOR---\nCamillo J. Taylor",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Kennedy_Hierarchically-Constrained_Optical_Flow_2015_CVPR_paper.pdf",
    "id": "Kennedy_Hierarchically-Constrained_Optical_Flow_2015_CVPR_paper",
    "abstract": "This paper presents a novel approach to solving optical flow problems using a discrete, tree-structured MRF derived from a hierarchical segmentation of the image. The method finds globally-optimal matching solutions even for problems involving very large motions. Experiments demonstrate that the approach is competitive on the MPI-Sintel dataset and significantly outperforms existing methods on problems involving large motions. The hierarchical approach balances representational power and optimization complexity, allowing for the efficient solution of large optical flow problems.\n\n---TOPICCS---\nOptical flow\nHierarchical segmentation\nMarkov Random Fields (MRF)\nDiscrete optimization\nLarge motion estimation",
    "topics": [],
    "references": [
      {
        "citation": "[Arbeláez, P. Boundary extraction in natural images using ultrametric contour maps. In CVPR, pages 182–182. IEEE, 2006.]"
      },
      {
        "citation": "[Arbeláez, P., Maire, M., Fowlkes, C., & Malik, J. Contour detection and hierarchical image segmentation. PAMI, 33(5):898–916, 2011.]"
      },
      {
        "citation": "[Felzenszwalb, P. F., & Huttenlocher, D. P. Pictorial structures for object recognition. IJCV, 61(1), 2005.]"
      },
      {
        "citation": "[Felzenszwalb, P. F., & Huttenlocher, D. P. Distance transforms of sampled functions. Technical report, Cornell University, 2004.]"
      },
      {
        "citation": "[Goldstein, T., Bresson, X., Osher, S., & Chambolle, A. GLOBAL MINIMIZATION OF MARKOV RANDOM FIELDS WITH APPLICATIONS TO OPTICAL FLOW. Inverse Problems & Imaging, 6(4), 2012.]"
      },
      {
        "citation": "[Kolmogorov, V. Convergent tree-reweighted message passing for energy minimization. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 28(10):1568–1583, 2006.]"
      },
      {
        "citation": "[Kolmogorov, V., & Zabih, R. Computing visual correspondence with occlusions using graph cuts. In Computer Vision, 2001. ICCV 2001. Proceedings. Eighth IEEE International Conference on, volume 2, pages 508–515. IEEE, 2001.]"
      },
      {
        "citation": "[Martin, D., Fowlkes, C., Tal, D., & Malik, J. A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In Computer Vision, 2001. ICCV 2001. Proceedings. Eighth IEEE International Conference on, volume 2, pages 416–423. IEEE, 2001.]"
      },
      {
        "citation": "[Pearl, J. Probablistic reasoning in intelligent systems: networks of plausible inference. Morgan Kaufmann, 1988.]"
      },
      {
        "citation": "[Sun, D., Roth, S., & Black, M. J. Secrets of optical ﬂow estimation and their principles. CVPR, 2010.]"
      }
    ],
    "author_details": [
      {
        "name": "Ryan Kennedy",
        "affiliation": "University of Pennsylvania",
        "email": "kenry@cis.upenn.edu"
      },
      {
        "name": "Camillo J. Taylor",
        "affiliation": "University of Pennsylvania",
        "email": "cjtaylor@cis.upenn.edu"
      }
    ]
  },
  {
    "title": "Saliency Propagation from Simple to Difﬁcult\n---AUTHOR---\nChen Gong\nDacheng Tao\nWei Liu\nS.J. Maybank\nMeng Fang\nKeren Fu\nJie Yang",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Gong_Saliency_Propagation_From_2015_CVPR_paper.pdf",
    "id": "Gong_Saliency_Propagation_From_2015_CVPR_paper",
    "abstract": "Saliency propagation has been widely adopted for identifying the most attractive object in an image. However, existing methods rely on spatial relationships, which can lead to incorrect propagations for inhomogeneous regions. This paper introduces a novel propagation algorithm inspired by educational psychology's teaching-to-learn and learning-to-teach strategies. The algorithm arranges regions from simple to difficult, improving propagation quality and reducing uncertainty in difficult regions, ultimately yielding more accurate salient object detection with optimized background suppression. Experimental results on benchmark datasets demonstrate the algorithm's superiority over existing methods.\n\n---TOPICCS---\nSaliency Detection\nPropagation Algorithms\nTeaching-to-Learn Strategies\nImage Processing\nSuperpixel Segmentation",
    "topics": [],
    "references": [],
    "author_details": [
      {
        "name": "Chen Gong",
        "affiliation": "Institute of Image Processing and Pattern Recognition, Shanghai Jiao Tong University",
        "email": "Not available"
      },
      {
        "name": "Dacheng Tao",
        "affiliation": "The Centre for Quantum Computation & Intelligent Systems, University of Technology, Sydney",
        "email": "dacheng.tao@gmail.com"
      },
      {
        "name": "Wei Liu",
        "affiliation": "IBM T. J. Watson Research Center",
        "email": "Not available"
      },
      {
        "name": "S.J. Maybank",
        "affiliation": "Birkbeck College, London",
        "email": "Not available"
      },
      {
        "name": "Meng Fang",
        "affiliation": "The Centre for Quantum Computation & Intelligent Systems, University of Technology, Sydney",
        "email": "Not available"
      },
      {
        "name": "Keren Fu",
        "affiliation": "Institute of Image Processing and Pattern Recognition, Shanghai Jiao Tong University",
        "email": "Not available"
      },
      {
        "name": "Jie Yang",
        "affiliation": "Institute of Image Processing and Pattern Recognition, Shanghai Jiao Tong University",
        "email": "jieyang@sjtu.edu.cn"
      }
    ]
  },
  {
    "title": "VisKE: Visual Knowledge Extraction and Question Answering by Visual Verification of Relation Phrases\n---AUTHOR---\nFereshteh Sadeghi\n---AUTHOR---\nSantosh K. Divvala\n---AUTHOR---\nAli Farhadi",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Sadeghi_VisKE_Visual_Knowledge_2015_CVPR_paper.pdf",
    "id": "Sadeghi_VisKE_Visual_Knowledge_2015_CVPR_paper",
    "abstract": "Extracting knowledge about the world has been a long-standing research focus in AI, yet most previous research has focused on reasoning primarily using language and text. This disparity stems from easier access to text data and well-performing text-based methods. However, with the proliferation of images online, the time is ripe for extracting knowledge by reasoning with images. In this work, we introduce the problem of visual verification of relation phrases and develop a Visual Knowledge Extraction system called VisKE. Given a verb-based relation phrase between common nouns, our approach assesses its validity by jointly analyzing text and images and reasoning about the spatial consistency of the relative configurations of the entities and the relation involved. Our approach involves no explicit human supervision and has been used to enrich existing textual knowledge bases and augment open-domain question-answer reasoning.\n\n---TOPIC---\nVisual Knowledge Extraction\nRelation Phrase Verification\nSpatial Reasoning\nKnowledge Base Enrichment\nWeakly-Supervised Learning",
    "topics": [],
    "references": [
      {
        "citation": "[A. Akbik and T. Michael. The weltmodell: A data-driven commonsensen knowledge base. In LREC, 2014.]"
      },
      {
        "citation": "[M. Banko et al. Open information extraction from the web. In IJCAI, 2007.]"
      },
      {
        "citation": "[J. Berant, A. Chou, R. Frostig, and P. Liang. Semantic parsing on freebase from question-answer pairs. In EMNLP, 2013.]"
      },
      {
        "citation": "[A. Carlson, J. Betteridge, E. R. Hruschka, Jr., and T. M. Mitchell. Coupling semi-supervised learning of categories and relations. In NAACL, 2009.]"
      },
      {
        "citation": "[A. Carlson et al. Toward an architecture for never-ending language learning. In AAAI, 2010.]"
      },
      {
        "citation": "[X. Chen, A. Shrivastava, and A. Gupta. NEIL: Extracting visual knowledge from web data. In ICCV, 2013.]"
      },
      {
        "citation": "[P. Clark, P. Harrison, and N. Balasubramanian. A study of the knowledge base requirements for passing an elementary science test. In AKBC, 2013.]"
      },
      {
        "citation": "[J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imaginet: A large-scale hierarchical image database. In CVPR, 2009.]"
      },
      {
        "citation": "[S. Divvala, A. Farhadi, and C. Guestrin. Learning everything about anything: Webly-supervised visual concept learning. In CVPR, 2014.]"
      },
      {
        "citation": "[A. Fader, S. Soderland, and O. Etzioni. Identifying relations for open information extraction. In EMNLP, 2011.]"
      }
    ],
    "author_details": [
      {
        "name": "Fereshteh Sadeghi",
        "affiliation": "University of Washington",
        "email": "fsadeghi@cs.washington.edu"
      },
      {
        "name": "Santosh K. Divvala",
        "affiliation": "The Allen Institute for AI",
        "email": "santoshd@allenai.org"
      },
      {
        "name": "Ali Farhadi",
        "affiliation": "University of Washington",
        "email": "ali@cs.washington.edu"
      }
    ]
  },
  {
    "title": "Descriptor Free Visual Indoor Localization with Line Segments\n---AUTHOR---\nBranislav Micusik\nHorst Wildenauer",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Micusik_Descriptor_Free_Visual_2015_CVPR_paper.pdf",
    "id": "Micusik_Descriptor_Free_Visual_2015_CVPR_paper",
    "abstract": "We present a novel approach to indoor visual localization that avoids the use of interest points and associated descriptors, which are fundamental to most standard methods. Instead, localization is framed as an alignment problem between the edges of a query image and a 3D model consisting of line segments. This strategy is effective in low-textured indoor environments and with large view-point changes, overcoming the limitations of image descriptors. The method utilizes line segments as the primary features and employs a Chamfer distance-based alignment cost computed through integral contour images, incorporated into a first-best-search strategy. Experimental results confirm the method's effectiveness in terms of both accuracy and computational complexity.\n\n---TOPICKS---\nIndoor Visual Localization\nLine Segment Matching\n3D Model Alignment\nChamfer Distance\nFirst-Best Search",
    "topics": [],
    "references": [
      {
        "citation": "[H. G. Barrow, J. M. Tenenbaum, R. C. Bolles, and H. C. Wolf. Parametric correspondence and chamfer matching: Two new techniques for image matching. In Proc. International Joint Conference on Artificial Intelligence (IJCAI, pages 659–663, 2006.] - This appears to be a foundational paper on image matching techniques."
      },
      {
        "citation": "[H. Bay, V. Ferrari, and L. Van Gool. Wide-baseline stereo matching with line segments. In Proc. CVPR, 2005.] - Relevant given the focus on line-based methods."
      },
      {
        "citation": "[R. Hartley and A. Zisserman. Multiple View Geometry in Computer Vision. Cambridge University Press, 2004.] - A key reference for understanding the geometric principles underlying many of the techniques."
      },
      {
        "citation": "[A. Jain, C. Kurz, T. Thormählen, and H.-P. Seidel. Exploiting global connectivity constraints for reconstruction of 3D line segments from images. In Proc. CVPR, 2010.] - Directly related to the reconstruction of 3D line segments."
      },
      {
        "citation": "[J. Shotton, A. Blake, and R. Cipolla. Multiscale categorical object recognition using contour fragments. PAM I, 30(7):1270–1281, July 2008.] - Important for understanding contour-based approaches."
      },
      {
        "citation": "[T. Sattler, B. Leibe, and L. Kobbelt. Fast image-based localization using direct 2D-to-3D matching. In Proc. ICCV, 2011.] - Relevant for localization tasks."
      },
      {
        "citation": "[P. Viola and M. Jones. Rapid object detection using a boosted cascade of simple features. In Proc. CVPR, 2001.] - A classic paper on object detection, potentially relevant for feature extraction or related tasks."
      },
      {
        "citation": "[B. Stenger, A. Thayananthan, P. Torr, and R. Cipolla. Model-based hand tracking using a hierarchical Bayesian filter. PAM I, 28(9):1372–1384, 2006.] - Demonstrates a Bayesian filtering approach, potentially useful for tracking."
      },
      {
        "citation": "[H. Lim, S. N. Sinha, M. Cohen, and M. Uytendaele. Real-time image-based 6-dof localization in large-scale environments. In Proc. CVPR, pages 1043–1050, 2012.] - Addresses a key application area: localization."
      },
      {
        "citation": "[B. Micusik and H. Wildenauer. Structure from motion with line segments under relaxed endpoint constraints. In Proc. 3DV, 2014.] - A more recent paper on structure from motion with line segments."
      }
    ],
    "author_details": [
      {
        "name": "Branislav Micusik",
        "affiliation": "AIT Austrian Institute of Technology",
        "email": "branislav.micusik@ait.ac.at"
      },
      {
        "name": "Horst Wildenauer",
        "affiliation": "Zeno Track GmbH, Austria",
        "email": "horst.wildenauer@zenotrack.com"
      }
    ]
  },
  {
    "title": "3D ShapeNets: A Deep Representation for Volumetric Shapes\n---AUTHOR---\nZhirong Wu\nShuran Song\nAditya Khosla\nFisher Yu\nLinguang Zhang\nXiaoou Tang\nJianxiong Xiao",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Wu_3D_ShapeNets_A_2015_CVPR_paper.pdf",
    "id": "Wu_3D_ShapeNets_A_2015_CVPR_paper",
    "abstract": "3D shape is a crucial but underutilized cue in today’s computer vision systems, mostly due to the lack of a good generic shape representation. With the recent availability of inexpensive 2.5D depth sensors, it is becoming increasingly important to have a powerful 3D shape representation. To this end, we propose to represent a geometric 3D shape as a probability distribution of binary variables on a 3D voxel grid, using a Convolutional Deep Belief Network. Our model, 3D ShapeNets, learns the distribution of complex 3D shapes across different object categories and arbitrary poses from raw CAD data, and discovers hierarchical compositional part representation automatically. It naturally supports joint object recognition and shape completion from 2.5D depth maps, and it enables active object recognition through view planning. We construct ModelNet – a large-scale 3D CAD model dataset to train our model, and experiments show significant performance improvement over the state-of-the-arts.\n\n---TOPICCS---\n3D Shape Representation\nConvolutional Deep Belief Networks\nShape Completion\nObject Recognition\nModelNet Dataset",
    "topics": [],
    "references": [
      {
        "citation": "[N. Athanasov, B. Sankaran, J. Le Ny, T. Koletschka, G. J. Pappas, and K. Daniilidis. Hypothesis testing framework for active object detection. In ICRA, 2013.]"
      },
      {
        "citation": "[J. L. Mundy. Object recognition in the geometric era: A retrospective. In Toward category-level object recognition. 2006.]"
      },
      {
        "citation": "[N. Athanasov, B. Sankaran, J. L. Ny, G. J. Pappas, and K. Daniilidis. Nonmyopic view planning for active object detection. arXiv preprint arXiv:1309.5401, 2013.]"
      },
      {
        "citation": "[P. K. Nathan Silberman, Derek Hoiem and R. Fergus. Indoor segmentation and support inference from rgbd images. In ECCV, 2012.]"
      },
      {
        "citation": "[F. Rothganger, S. Lazebnik, C. Schmid, and J. Ponce. 3d object modeling and recognition using local afﬁne-invariant image descriptors and multi-view spatial constraints. IJCV, 2006.]"
      },
      {
        "citation": "[P. Besl and N. D. McKay. Method for registration of 3-d shapes. In PAMI, 1992.]"
      },
      {
        "citation": "[I. Biederman. Recognition-by-components: a theory of human image understanding. Psychological review, 1987.]"
      },
      {
        "citation": "[S. Scott, G. Roth, and J.-F. Rivest. View planning for automated 3d object reconstruction inspection. ACM Computing Surveys, 2003.]"
      },
      {
        "citation": "[S. Shalom, A. Shamir, H. Zhang, and D. Cohen-Or. Cone carving for surface reconstruction. In ACM Transactions on Graphics (TOG), 2010.]"
      },
      {
        "citation": "[S. Gupta, R. Girshick, P. Arbel´aez, and J. Malik. Learning rich features from rgb-d images for object detection and segmentation. In ECCV. 2014.]"
      }
    ],
    "author_details": [
      {
        "name": "Zhirong Wu",
        "affiliation": "Princeton University",
        "email": "[Email not available in the provided text]"
      },
      {
        "name": "Shuran Song",
        "affiliation": "Princeton University",
        "email": "[Email not available in the provided text]"
      },
      {
        "name": "Aditya Khosla",
        "affiliation": "Massachusetts Institute of Technology",
        "email": "[Email not available in the provided text]"
      },
      {
        "name": "Fisher Yu",
        "affiliation": "Princeton University",
        "email": "[Email not available in the provided text]"
      },
      {
        "name": "Linguang Zhang",
        "affiliation": "Princeton University",
        "email": "[Email not available in the provided text]"
      },
      {
        "name": "Xiaoou Tang",
        "affiliation": "Chinese University of Hong Kong",
        "email": "[Email not available in the provided text]"
      },
      {
        "name": "Jianxiong Xiao",
        "affiliation": "Princeton University",
        "email": "[Email not available in the provided text]"
      }
    ]
  },
  {
    "title": "Rolling Shutter Motion Deblurring\n---AUTHOR---\nShuochen Su\nWolfgang Heidrich",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Su_Rolling_Shutter_Motion_2015_CVPR_paper.pdf",
    "id": "Su_Rolling_Shutter_Motion_2015_CVPR_paper",
    "abstract": "Although motion blur and rolling shutter deformations are closely coupled artifacts in images taken with CMOS image sensors, the two phenomena have so far mostly been treated separately, with deblurring algorithms being unable to handle rolling shutter wobble, and rolling shutter algorithms being incapable of dealing with motion blur. We propose an approach that delivers sharp and undistorted output given a single rolling shutter motion blurred image. The key to achieving this is a global modeling of the camera motion trajectory, which enables each scanline of the image to be deblurred with the corresponding motion segment. We show the results of the proposed framework through experiments on synthetic and real data.\n\n---TOPIC---\nRolling Shutter Motion Deblurring\nBlind Deblurring\nCamera Motion Estimation\nRolling Shutter Sensors\nGlobal Motion Modeling",
    "topics": [],
    "references": [
      {
        "citation": "[S. Baker, E. Bennett, S. B. Kang, and R. Szeliski. Removing rolling shutter wobble. In Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on, pages 2392–2399. IEEE, 2010.] - Appears relevant due to focus on rolling shutter."
      },
      {
        "citation": "[S. Baker and I. Matthews. Lucas-kanade 20 years on: A unifying framework. International journal of computer vision, 56(3):221–255, 2004.] - Lucas-Kanade is a core technique mentioned."
      },
      {
        "citation": "[S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein. Distributed optimization and statistical learning via the alternating direction method of multipliers. Foundations and Trends R⃝ in Machine Learning, 3(1):1–122, 2011.] -  ADMM is a key optimization technique."
      },
      {
        "citation": "[S. Cho and S. Lee. Fast motion deblurring. In ACM Transactions on Graphics (TOG), volume 28, page 145. ACM, 2009.] - A direct focus on motion deblurring."
      },
      {
        "citation": "[A. Gupta, N. Joshi, C. L. Zitnick, M. Cohen, and B. Curless. Single image deblurring using motion density functions. In Computer Vision–ECCV 2010, pages 171–184. Springer, 2010.] -  Motion density functions are a significant approach."
      },
      {
        "citation": "[Z. Hu and M.-H. Yang. Fast non-uniform deblurring using constrained camera pose subspace. In BMVC, pages 1–11, 2012.] - Addresses non-uniform deblurring."
      },
      {
        "citation": "[R. K¨ohler, M. Hirsch, B. Mohler, B. Sch¨olkopf, and S. Harmeling. Recording and playback of camera shake: Benchmarking blind deconvolution with a real-world database. In Computer Vision–ECCV 2012, pages 27–40. Springer, 2012.] - Provides context and benchmarking."
      },
      {
        "citation": "[M. Meilland, T. Drummond, and A. I. Comport. A unified rolling shutter and motion blur model for 3d visual registration. In Computer Vision (ICCV), 2013 IEEE International Conference on, pages 2016–2023. IEEE, 2013.] - Unified model of rolling shutter and motion blur."
      },
      {
        "citation": "[S. H. Park and M. Levoy. Gyro-based multi-image deconvolution for removing handshake blur. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2014.] - Addresses handshake blur."
      },
      {
        "citation": "[D. Perrone and P. Favaro. Total variation blind deconvolution: The devil is in the details. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2014.] - Total variation blind deconvolution is a relevant technique."
      }
    ],
    "author_details": [
      {
        "name": "Shuochen Su",
        "affiliation": "University of British Columbia",
        "email": "Not available"
      },
      {
        "name": "Wolfgang Heidrich",
        "affiliation": "KAUST",
        "email": "Not available"
      }
    ]
  },
  {
    "title": "EpicFlow: Edge-Preerving Interpolation of Correspondences for Optical Flow\n---AUTHOR---\nJerome Revaud\nPhilippe Weinzaepfel\nZaid Harchaouia\nCordelia Schmid",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Revaud_EpicFlow_Edge-Preserving_Interpolation_2015_CVPR_paper.pdf",
    "id": "Revaud_EpicFlow_Edge-Preserving_Interpolation_2015_CVPR_paper",
    "abstract": "We propose a novel approach for optical ﬂow estimation, targeted at large displacements with signiﬁcant occlusions. It consists of two steps: i) dense matching by edge-preserving interpolation from a sparse set of matches; ii) variational energy minimization initialized with the dense matches. The sparse-to-dense interpolation relies on an appropriate choice of the distance, namely an edge-aware geodesic distance. This distance is tailored to handle occlusions and motion boundaries – two common and difficult issues for optical ﬂow computation. We also propose an approximation scheme for the geodesic distance to allow fast computation without loss of performance. Subsequent to the dense interpolation step, standard one-level variational energy minimization is carried out on the dense matches to obtain the final ﬂow estimation. The proposed approach, called Edge-Preserving Interpolation of Correspondences (EpicFlow) is fast and robust to large displacements. It signiﬁcantly outperforms the state of the art on MPI-Sintel and performs on par on Kitti and Middlebury.",
    "topics": [
      "Optical Flow Estimation",
      "Edge-Preserving Interpolation",
      "Variational Energy Minimization",
      "Occlusion Handling",
      "Motion Boundaries"
    ],
    "references": [
      {
        "citation": "[R. Achanta, A. Shaji, K. Smith, A. Lucchi, P. Fua, and S. Susstrunk. Slic superpixels compared to state-of-the-art superpixel methods. IEEE Trans. PAMI, 2012.]"
      },
      {
        "citation": "[L. Alvarez, J. Weickert, and J. S´anchez. Reliable estimation of dense optical ﬂow ﬁelds with large displacements. IJCV, 2000.]"
      },
      {
        "citation": "[P. Arbeláez, M. Maire, C. Fowlkes, and J. Malik. Contour detection and hierarchical image segmentation. IEEE Trans. PAMI, 2011.]"
      },
      {
        "citation": "[S. Baker, D. Scharstein, J. P. Lewis, S. Roth, M. J. Black, and R. Szeliski. A database and evaluation methodology for optical ﬂow. IJCV, 2011.]"
      },
      {
        "citation": "[C. Barnes, E. Shechtman, D. B. Goldman, and A. Finkelshtein. The generalized PatchMatch correspondence algorithm. In ECCV, 2010.]"
      },
      {
        "citation": "[J. Braux-Zin, R. Dupont, and A. Bartoli. A general dense image matching framework combining direct and feature-based costs. In ICCV, 2013.]"
      },
      {
        "citation": "[T. Brox and J. Malik. Large displacement optical ﬂow: descriptor matching in variational motion estimation. IEEE Trans. PAMI, 2011.]"
      },
      {
        "citation": "[T. Brox, A. Bruhn, N. Papenberg, and J. Weickert. High accuracy optical ﬂow estimation based on a theory for warping. In ECCV, 2004.]"
      },
      {
        "citation": "[D. J. Butler, J. Wulff, G. B. Stanley, and M. J. Black. A naturalistic open source movie for optical ﬂow evaluation. In ECCV, 2012.]"
      },
      {
        "citation": "[J. Canny. A computational approach to edge detection. IEEE Trans. PAMI, 1986.]"
      }
    ],
    "author_details": [
      {
        "name": "Jerome Revaud",
        "affiliation": "Inria∗",
        "email": "firstname.lastname@inria.fr"
      },
      {
        "name": "Philippe Weinzaepfel",
        "affiliation": "Inria∗",
        "email": "firstname.lastname@inria.fr"
      },
      {
        "name": "Zaid Harchaouia",
        "affiliation": "NYU",
        "email": "firstname.lastname@inria.fr"
      },
      {
        "name": "Cordelia Schmid",
        "affiliation": "Inria∗",
        "email": "firstname.lastname@inria.fr"
      }
    ]
  },
  {
    "title": "Supplementary Material — Beyond Spatial Pooling: Fine-Grained Representation Learning in Multiple Domains\n---AUTHOR---\nChi Li\n---AUTHOR---\nAustin Reiter\n---AUTHOR---\nGregory D. Hager",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Li_Beyond_Spatial_Pooling_2015_CVPR_supplemental.pdf",
    "id": "Li_Beyond_Spatial_Pooling_2015_CVPR_supplemental",
    "abstract": "This supplementary material provides detailed proofs and numerical results to support the claims made in the main paper. Specifically, it presents proofs for Theorems 1 and 2, which relate to the derivation of Eq. 4 using max and average pooling operators, respectively. It also substantiates variance statements associated with Eq. 6 and Eq. 7. Furthermore, it includes numerical details for figures 4 and 5 from the paper and provides examples of object instances from the JHUIT-50 dataset, illustrating the experimental setup.\n\n---TOPIC---\nFine-Grained Representation Learning\nPooling Operators (Max and Average)\nVariance Analysis\nJHUIT-50 Dataset\nTheoretical Proofs",
    "topics": [],
    "references": [],
    "author_details": [
      {
        "name": "Chi Li",
        "affiliation": "Department of Computer Science, Johns Hopkins University",
        "email": "chi li@jhu.edu"
      },
      {
        "name": "Austin Reiter",
        "affiliation": "Department of Computer Science, Johns Hopkins University",
        "email": "areiter@cs.jhu.edu"
      },
      {
        "name": "Gregory D. Hager",
        "affiliation": "Department of Computer Science, Johns Hopkins University",
        "email": "hager@cs.jhu.edu"
      }
    ]
  },
  {
    "title": "Burst Deblurring: Removing Camera Shake Through Fourier Burst Accumulation\n---AUTHOR---\nMauricio Delbracio\nGuillermo Sapiro",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Delbracio_Burst_Deblurring_Removing_2015_CVPR_paper.pdf",
    "id": "Delbracio_Burst_Deblurring_Removing_2015_CVPR_paper",
    "abstract": "Numerous recent approaches attempt to remove image blur due to camera shake, either with one or multiple input images, by explicitly solving an inverse and inherently ill-posed deconvolution problem. If the photographer takes a burst of images, a modality available in virtually all modern digital cameras, we show that it is possible to combine them to get a clean sharp version. This is done without explicitly solving any blur estimation and subsequent inverse problem. The proposed algorithm is strikingly simple: it performs a weighted average in the Fourier domain, with weights depending on the Fourier spectrum magnitude. The method’s rationale is that camera shake has a random nature and therefore each image in the burst is generally blurred differently. Experiments with real camera data show that the proposed Fourier Burst Accumulation algorithm achieves state-of-the-art results an order of magnitude faster, with simplicity for on-board implementation on camera phones.",
    "topics": [
      "Image Deblurring",
      "Camera Shake",
      "Fourier Analysis",
      "Inverse Problems",
      "Computational Photography"
    ],
    "references": [
      {
        "citation": "[Fergus, R., Singh, B., Hertzmann, A., Roweis, S. T., & Freeman, W. T. Removing camera shake from a single photograph. ACM Trans. Graph., 25(3), 2006.]"
      },
      {
        "citation": "[Levin, A., Weiss, Y., Durand, F., & Freeman, W. T. Understanding and evaluating blind deconvolution algorithms. In CVPR, 2009.]"
      },
      {
        "citation": "[Boracchi, G., & Foi, A. Modeling the performance of image restoration from motion blur. IEEE Trans. Image Process., 21(8), 2012.]"
      },
      {
        "citation": "[Michaeli, T., & Irani, M. Blind deblurring using internal patch recurrence. In ECCV, 2014.]"
      },
      {
        "citation": "[Moisan, L., Moulon, P., & Monasse, P. Automatic Homographic Registration of a Pair of Images, with A Contrario Elimination of Outliers. IPOL, 2, 2012.]"
      },
      {
        "citation": "[Sroubek, F., & Milanfar, P. Robust multichannel blind deconvolution via fast alternating minimization. IEEE Trans. Image Process., 21(4), 2012.]"
      },
      {
        "citation": "[Park, S. H., & Levoy, M. Gyro-based multi-image deconvolution for removing handshake blur. CVPR, 2014.]"
      },
      {
        "citation": "[Whyte, O., Sivic, J., Zisserman, A., & Ponce, J. Non-uniform deblurring for shaken images. Int. J. Comput. Vision, 98(2), 2012.]"
      },
      {
        "citation": "[Krishnan, D., Tay, T., & Fergus, R. Blind deconvolution using a normalized sparsity measure. In CVPR, 2011.]"
      },
      {
        "citation": "[Zhang, H., Wipf, D., & Zhang, Y. Multi-image blind deblurring using a coupled adaptive sparse prior. In CVPR, 2013.]"
      }
    ],
    "author_details": [
      {
        "name": "Mauricio Delbracio",
        "affiliation": "Duke University",
        "email": "mauricio.delbracio@duke.edu"
      },
      {
        "name": "Guillermo Sapiro",
        "affiliation": "Duke University",
        "email": "guillermo.sapiro@duke.edu"
      }
    ]
  },
  {
    "title": "Ranking and Retrieval of Image Sequences from Multiple Paragraph Queries\n---AUTHOR---\nGunhee Kim\nSeungwhan Moon\nLeonid Sigal",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Kim_Ranking_and_Retrieval_2015_CVPR_paper.pdf",
    "id": "Kim_Ranking_and_Retrieval_2015_CVPR_paper",
    "abstract": "We propose a method to rank and retrieve image sequences from a natural language text query, consisting of multiple sentences or paragraphs. One of the method’s key applications is to visualize visitors’ text-only reviews on TRIPADVISOR or YELP, by automatically retrieving the most illustrative image sequences. While most previous work has dealt with the relations between a natural language sentence and an image or a video, our work extends to the relations between paragraphs and image sequences. Our approach leverages the vast user-generated resource of blog posts and photo streams on the Web. We use blog posts as text-image parallel training data that co-locate informative text with representative images that are carefully selected by users. We exploit large-scale photo streams to augment the image samples for retrieval. We design a latent structural SVM framework to learn the semantic relevance relations between text and image sequences. We present both quantitative and qualitative results on the newly created DISNEYLAND dataset.\n\n---TOPIICS---\nImage sequence retrieval\nNatural language processing\nLatent structural SVM\nText-image relevance\nTourism applications",
    "topics": [],
    "references": [
      {
        "citation": "[A. Aizawa. An Information-Theoretic Perspective of TF-IDF Measures. Info. Proc. Manag., 39(1):45–65, 3]"
      },
      {
        "citation": "[D. M. Blei and J. D. Lafferty. Dynamic Topic Models. In ICML, pages 113–120, 2006.]"
      },
      {
        "citation": "[D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent Dirichlet Allocation. JMLR, 3:993–1022, 2003.]"
      },
      {
        "citation": "[G. Salton, E. A. Fox, and H. Wu. Extended Boolean Information Retrieval. CACM, 26(11):1022–1036, 1983.]"
      },
      {
        "citation": "[T. Joachims. Training Linear SVMs in Linear Time. In KDD, 2006.]"
      },
      {
        "citation": "[G. Erkan and D. R. Radev. LexRank: Graph-based Lexical Centrality As Salience in Text Summarization. JAIR, 22(1):457–479, 2004.]"
      },
      {
        "citation": "[Y. Gong, L. Wang, M. Hodosh, J. Hockenmaier, and S. Lazebnik. Improving Image-Sentence Embeddings Using Large Weakly Annotated Photo Collections. In ECCV, 2014.]"
      },
      {
        "citation": "[M. Hodosh, P. Young, and J. Hockenmaier. Framing Image Description as a Ranking Task: Data, Models and Evaluation Metrics. JAIR, 47:853–899, 2013.]"
      },
      {
        "citation": "[T. Joachims, T. Finley, and C. N. J. Yu. Cutting-plane Training of Structural SVMs. Mach. Learn., 77:27–59, 2009.]"
      },
      {
        "citation": "[K. Kireyev. Using Latent Semantic Analysis for Extractive Summarization. In TAC, 2008.]"
      }
    ],
    "author_details": [
      {
        "name": "Gunhee Kim",
        "affiliation": "Seoul National University",
        "email": "gunhee@cs.cmu.edu"
      },
      {
        "name": "Seungwhan Moon",
        "affiliation": "Carnegie Mellon University",
        "email": "seungwhm@cs.cmu.edu"
      },
      {
        "name": "Leonid Sigal",
        "affiliation": "Disney Research Pittsburgh",
        "email": "lsigal@disneyresearch.com"
      }
    ]
  },
  {
    "title": "How Do We Use Our Hands? Discovering a Diverse Set of Common Grasps (Supplementary Material)\n---AUTHORISTS---\nDe-An Huang\nMinghuang Ma\nWei-Chiu Ma\nKris M. Kitani",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Huang_How_Do_We_2015_CVPR_supplemental.pdf",
    "id": "Huang_How_Do_We_2015_CVPR_supplemental",
    "abstract": "This supplementary material details the methods used to discover dominant modes of hand-object interactions from first-person videos. It compares different hand region harvesting techniques and feature representations, providing additional clustering results and pseudocodes of baselines. The paper evaluates six harvesting methods (random, center focus, objectness, gaze fixation, hand contour, and hand-fixation) and various feature representations (Masked HOG descriptor) to identify key frames and bounding boxes capturing important hand-object regions. The results confirm the importance of hand location for learning about hand-object interactions, and the paper uses hand locations to harvest hand-object regions.\n\n---TOPICCS---\nHand-Object Interaction\nHarvesting Methods\nFeature Representation\nComputer Vision\nHuman-Computer Interaction",
    "topics": [],
    "references": [
      {
        "citation": "[B. Alexe, T. Deselaers, and V. Ferrari, \"What is an object?\", CVPR, 2010.] - Likely foundational for the paper's object recognition/understanding focus."
      },
      {
        "citation": "[L. Cheng and K. M. Kitani, \"Pixel-level hand detection in ego-centric videos.\", CVPR, 2013.] - Directly relevant to hand detection, a key aspect of the paper."
      },
      {
        "citation": "[P. Doll´ar, V. Rabaud, G. Cottrell, and S. Belongie, \"Behavior recognition via sparse spatio-temporal features.\", VSPETS, 2005.] - Relevant to action/behavior recognition, potentially informing the context of the work."
      },
      {
        "citation": "[A. Fathi, Y. Li, and J. M. Rehg, \"Learning to recognize daily actions using gaze.\", ECCV, 2012.] - Related to action recognition, a common task in the field."
      },
      {
        "citation": "[H. Kang, M. Hebert, and T. Kanade, \"Discovering object instances from scenes of daily living.\", ICCV, 2011.] - Addresses object discovery in daily living scenarios, aligning with the paper's potential scope."
      },
      {
        "citation": "[J. R. Napier, \"The prehensile movements of the human hand.\", Journal of bone and Joint surgery, 38(4):902–913, 1956.] - Provides a foundational understanding of hand movements, likely informing the biomechanical aspects of the work."
      },
      {
        "citation": "[J. Shi and C. Tomasi, \"Good features to track.\", CVPR, 1994.] - A classic paper on feature detection, potentially used for tracking or feature extraction."
      }
    ],
    "author_details": [
      {
        "name": "De-An Huang",
        "affiliation": "Carnegie Mellon University",
        "email": "deanh@andrew.cmu.edu"
      },
      {
        "name": "Minghuang Ma",
        "affiliation": "Carnegie Mellon University",
        "email": "minghuam@andrew.cmu.edu"
      },
      {
        "name": "Wei-Chiu Ma",
        "affiliation": "Carnegie Mellon University",
        "email": "weichium@andrew.cmu.edu"
      },
      {
        "name": "Kris M. Kitani",
        "affiliation": "Carnegie Mellon University",
        "email": "kkitani@cs.cmu.edu"
      }
    ]
  },
  {
    "title": "Neuroaesthetics in Fashion: Modeling the Perception of Fashionability\n---AUTHOR---\nEdgar Simo-Serra\nSanja Fidler\nFrancesc Moreno-Noguer\nRaquel Urtasun",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Simo-Serra_Neuroaesthetics_in_Fashion_2015_CVPR_paper.pdf",
    "id": "Simo-Serra_Neuroaesthetics_in_Fashion_2015_CVPR_paper",
    "abstract": "In this paper, we analyze the fashion of clothing from a large social website. Our goal is to learn and predict how fashionable a person looks on a photograph and suggest subtle improvements the user could make to improve her/his appeal. We propose a Conditional Random Field model that jointly reasons about several fashionability factors such as the type of outﬁt and garments the user is wearing, the type of the user, the photograph’s setting (e.g., the scenery behind the user), and the fashionability score. Our model is able to give rich feedback back to the user, conveying which garments or even scenery she/he should change in order to improve fashionability. We demonstrate that our joint approach significantly outperforms a variety of intelligent baselines. We additionally collected a novel heterogeneous dataset with 144,169 user posts containing diverse image, textual and meta information which can be exploited for our task. We also provide a detailed analysis of the data, showing different outﬁt trends and fashionability scores across the globe and across a span of 6 years.\n\n---TOPICCS---\nFashionability Prediction\nConditional Random Fields\nComputer Vision\nSocial Media Analysis\nClothing Recognition",
    "topics": [],
    "references": [
      {
        "citation": "[Bossard, Leistner, Dantone, Wengert, and Gool. Apparel classifcation with style. In ACCV, 2012.]"
      },
      {
        "citation": "[Bourdev, Maji, and Malik. Describing people: A poselet-based approach to attribute classiﬁcation. In ICCV, 2011.]"
      },
      {
        "citation": "[Chen, Gallagher, and Girod. Describing clothing by semantic attributes. In ECCV, 2012.]"
      },
      {
        "citation": "[Gallagher and Chen. Clothing cosegmentation for recognizing people. In CVPR, 2008.]"
      },
      {
        "citation": "[Hazan and Urtasun. A primal-dual message-passing algorithm for approximated large scale structured prediction. In NIPS, 2010.]"
      },
      {
        "citation": "[Khosla, Torralba, and Oliva. Modifying the memorability of face photographs. In ICCV, 2013.]"
      },
      {
        "citation": "[Liu, Feng, Song, Zhang, Lu, Changsheng, and Yan. Hi, magic closet, tell me what to wear! In ACMMM, 2012.]"
      },
      {
        "citation": "[Simo-Serra, Fidler, Moreno-Noguer, and Urtasun. A High Performance CRF Model for Clothes Parsing. In ACCV, 2014.]"
      },
      {
        "citation": "[Socher, Perelygin, Wu, Chuang, Manning, Ng, and Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In EMNLP, 2013.]"
      },
      {
        "citation": "[Xiao, Hays, Ehinger, Oliva, and Torralba. Sun database: Large-scale scene recognition from abbey to zoo. In CVPR, 2010.]"
      }
    ],
    "author_details": [
      {
        "name": "Edgar Simo-S Serra",
        "affiliation": "Institut de Rob`otica i Inform`atica Industrial (CSI-UPC)",
        "email": "[Email not available]"
      },
      {
        "name": "Sanja Fidler",
        "affiliation": "University of Toronto",
        "email": "[Email not available]"
      },
      {
        "name": "Francesc Moreno-Noguer",
        "affiliation": "Institut de Rob`otica i Inform`atica Industrial (CSI-UPC)",
        "email": "[Email not available]"
      },
      {
        "name": "Raquel Urtasun",
        "affiliation": "University of Toronto",
        "email": "[Email not available]"
      }
    ]
  },
  {
    "title": "Indoor Scene Structure Analysis for Single Image Depth Estimation\n---AUTHOR---\nWei Zhuo\nMathieu Salzmann\nXuming He\nMiaomiao Liu",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Zhuo_Indoor_Scene_Structure_2015_CVPR_paper.pdf",
    "id": "Zhuo_Indoor_Scene_Structure_2015_CVPR_paper",
    "abstract": "We tackle the problem of single image depth estimation, which, without additional knowledge, suffers from many ambiguities. Unlike previous approaches that only reason locally, we propose to exploit the global structure of the scene to estimate its depth. To this end, we introduce a hierarchical representation of the scene, which models local depth jointly with mid-level and global scene structures. We formulate single image depth estimation as inference in a graphical model whose edges let us encode the interactions within and across the different layers of our hierarchy. Our method therefore still produces detailed depth estimates, but also leverages higher-level information about the scene. We demonstrate the benefits of our approach over local depth estimation methods on standard indoor datasets.\n\n---TOPICCS---\nSingle Image Depth Estimation\nScene Structure Analysis\nHierarchical Scene Representation\nGraphical Models\nIndoor Scene Understanding",
    "topics": [],
    "references": [
      {
        "citation": "[Silberman, N., Hoiem, D., Kohli, P., & Fergus, R. (2012). Indoor segmentation and support inference from rgbd images. In ECCV. ]"
      },
      {
        "citation": "[Saxena, A., Chung, S. H., & Ng, A. Y. (2007). 3-d depth reconstruction from a single still image. IJCV.]"
      },
      {
        "citation": "[Saxena, A., Sun, M., & Ng, A. Y. (2009). Make3d: Learning 3d scene structure from a single still image. PAMI.]"
      },
      {
        "citation": "[Liu, B., Gould, S., & Koller, D. (2010). Single image depth estimation from predicted semantic labels. In CVPR.]"
      },
      {
        "citation": "[Eigen, D., Puhrsch, C., & Fergus, R. (2014). Depth map prediction from a single image using a multi-scale deep network. arXiv preprint arXiv:1406.2283.]"
      },
      {
        "citation": "[Torralba, A., & Oliva, A. (2002). Depth estimation from image structure. Pattern Analysis and Machine Intelligence, IEEE Transactions on.]"
      },
      {
        "citation": "[Konrad, J., Wang, M., & Ishwar, P. (2012). 2d-to-3d image conversion by learning depth from examples. In 3DCINE.]"
      },
      {
        "citation": "[Schwing, A., Fidler, S., Pollefeys, M., & Urtasun, R. (2013). Box in the box: Joint 3d layout and object reasoning from single images. In ICCV.]"
      },
      {
        "citation": "[Gupta, A., Girshick, R., Arbeláez, P., & Malik, J. (2014). Learning rich features from RGB-D images for object detection and segmentation. In ECCV.]"
      },
      {
        "citation": "[Hoiem, D., A. N. Stein, A. A. Efrofs, and M. Hebert. (2007). Recovering occlusion boundaries from a single image. In Computer Vision, 2007. ICCV 2007. IEEE 11th International Conference on, pages 1–8. IEEE, 2007.]"
      }
    ],
    "author_details": [
      {
        "name": "Wei Zhuo",
        "affiliation": "Australian National University, NICTA",
        "email": "wei.zhuo@nicta.com.au"
      },
      {
        "name": "Mathieu Salzmann",
        "affiliation": "Australian National University, NICTA",
        "email": "mathieu.salzmann@nicta.com.au"
      },
      {
        "name": "Xuming He",
        "affiliation": "Australian National University, NICTA",
        "email": "xuming.he@nicta.com.au"
      },
      {
        "name": "Miaomiao Liu",
        "affiliation": "Australian National University, NICTA",
        "email": "miaomiao.liu@nicta.com.au"
      }
    ]
  },
  {
    "title": "Photometric Reﬁnement of Depth Maps for Multi-albedo Objects\n---AUTHORs---\nAvishek Chatterjee\nVenu Madhav Govindu",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Chatterjee_Photometric_Refinement_of_2015_CVPR_paper.pdf",
    "id": "Chatterjee_Photometric_Refinement_of_2015_CVPR_paper",
    "abstract": "This paper proposes a novel uncalibrated photometric method for refining depth maps of multi-albedo objects obtained from consumer depth cameras like Kinect. Existing uncalibrated photometric methods either assume that the object has constant albedo or rely on segmenting images into constant albedo regions. The proposed method does not require the constant albedo assumption and is the first to handle objects with arbitrary albedo under uncalibrated illumination. The method robustly estimates a rank 3 approximation of the observed brightness matrix, factorizes it into lighting, albedo, and surface normal components, and presents a direct radiometric calibration technique for the infra-red camera of the structured-light stereo depth scanner. Experimental results demonstrate highly accurate three-dimensional reconstructions of a wide variety of objects.\n\n---TOPIC---\nPhotometric Stereo\n---TOPIC---\nDepth Map Refinement\n---TOPIC---\nMulti-albedo Objects\n---TOPIC---\nRadiometric Calibration\n---TOPIC---\nSurface Normal Estimation",
    "topics": [],
    "references": [
      {
        "citation": "[R. Basri, D. Jacobs, and I. Kemelmacher. Photometric stereo with general unknown lighting. International Journal of Computer Vision, 72(3):239–257, 2007.] - A foundational paper on photometric stereo."
      },
      {
        "citation": "[B. K. P. Horn. Robot vision. MIT press, 1986.] - A classic and comprehensive text on robot vision, likely relevant to shape from shading."
      },
      {
        "citation": "[B. K. P. Horn. Obtaining Shape from Shading Information. MIT Press, 1989.] - A key work specifically focused on shape from shading."
      },
      {
        "citation": "[R. Szeliski. Computer Vision: Algorithms and Applications. Springer-Verlag, 2010.] - A widely used and comprehensive textbook on computer vision."
      },
      {
        "citation": "[P. E. Debevec and J. Malik. Recovering high dynamic range radiance maps from photographs. In Proceedings of the 24th Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH ’97, pages 369–378, 1997.] - Important for understanding HDR imaging, often used in conjunction with photometric stereo."
      },
      {
        "citation": "[J.-D. Durou, M. Falcone, and M. Sagona. Numerical methods for shape-from-shading: A new survey with benchmarks. Computer Vision and Image Understanding, 109(1):22–43, 2008.] - A survey paper providing a good overview of numerical methods for shape from shading."
      },
      {
        "citation": "[A. Ecker and A. D. Jepson. Polynomial shape from shading. In Computer Vision and Pattern Recognition (CVPR), pages 145–152. IEEE, 2010.] - Introduces a polynomial approach to shape from shading."
      },
      {
        "citation": "[Y. Han, J. Lee, and I. Kweon. High quality shape from a single rgb-d image under uncalibrated natural illumination. In International Conference on Computer Vision (ICCV), pages 1617–1624. IEEE, 2013.] - Addresses shape recovery from a single RGB-D image, a common modern application."
      },
      {
        "citation": "[S. M. Haque, A. Chatterjee, and V. M. Govindu. High quality photometric reconstruction using a depth camera. In Computer Vision and Pattern Recognition (CVPR), pages 2283–2290. IEEE, 2014.] - Focuses on photometric reconstruction using depth cameras."
      },
      {
        "citation": "[K. Wilson and N. Snavely. Robust global translations with 1dsfm. In Computer Vision–ECCV 2014, pages 61–75. Springer, 2014.] - Relevant if the paper uses structure from motion or related techniques."
      }
    ],
    "author_details": [
      {
        "name": "Avishek Chatterjee",
        "affiliation": "Indian Institute of Science",
        "email": "avishek@ee.iisc.ernet.in"
      },
      {
        "name": "Venu Madhav Govindu",
        "affiliation": "Indian Institute of Science",
        "email": "venu@ee.iisc.ernet.in"
      }
    ]
  },
  {
    "title": "FPA-CS: Focal Plane Array-based Compressive Imaging in Short-wave Infrared\n---AUTHOR---\nHuaijin Chen\nM. Salman Asif\nAswin C. Sankaranarayanan\nAshok Veeraraghavan",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Chen_FPA-CS_Focal_Plane_2015_CVPR_paper.pdf",
    "id": "Chen_FPA-CS_Focal_Plane_2015_CVPR_paper",
    "abstract": "Cameras for imaging in the short-wave infrared (SWIR) spectrum are significantly more expensive than visible imaging cameras, limiting their accessibility. Compressive sensing (CS) offers a potential solution for inexpensive SWIR cameras, but single-pixel cameras (SPCs) suffer from low measurement rates. This paper presents a focal plane array-based compressive sensing (FPA-CS) architecture that utilizes an array of SPCs to increase the measurement rate and achieve high spatial and temporal resolutions. A proof-of-concept prototype using a 64×64 SWIR sensor array demonstrates a 4096× increase in measurement rate compared to a single SPC, enabling megapixels resolution at video rate using CS techniques.\n\n---TOPIC---\nShort-wave Infrared (SWIR) Imaging\nCompressive Sensing (CS)\nFocal Plane Array (FPA)\nSingle-Pixel Camera (SPC)\nDigital Micro-mirror Device (DMD)",
    "topics": [],
    "references": [
      {
        "citation": "[Asif, M., Fernandes, F., & Romberg, J. Low-complexity video compression and compressive sensing. Asilomar Conference on Signals, Systems and Computers, 2013.]"
      },
      {
        "citation": "[Beck, A., & Teboulle, M. Fast gradient-based algorithms for constrained total variation image denoising and deblurring problems. Computational Optimization and Applications, 2013.]"
      },
      {
        "citation": "[Candès, E. J. The restricted isometry property and its implications for compressed sensing. Comptes rendus-Math´ematique, 2008.]"
      },
      {
        "citation": "[Candès, E. J., Romberg, J., & Tao, T. Robust uncertainty principles: Exact signal reconstruction from highly incomplete frequency information. IEEE Transactions on Information Theory, 2006.]"
      },
      {
        "citation": "[Cevher, V., Sankaranarayanan, A. C., Duarte, M. F., Reddy, D., Baraniuk, R. G., & Chellappa, R. Compressive sensing for background subtraction. European Conference on Computer Vision, 2008.]"
      },
      {
        "citation": "[Chambolle, A. An algorithm for total variation minimization and applications. Journal of Mathematical Imaging and Vision, 2004.]"
      },
      {
        "citation": "[Donoho, D. L. Compressed sensing. IEEE Transactions on Information Theory, 2006.]"
      },
      {
        "citation": "[Duarte, M. F., Davenport, M. A., Takhar, D., Lask, J. N., Sun, T., Kelly, K. F., & Baraniuk, R. G. Single-pixel imaging via compressive sampling. IEEE Signal Processing Magazine, 2008.]"
      },
      {
        "citation": "[Fowler, J. E., Mun, S., Tramel, E. W., Gupta, M. R., Chen, Y., Wiegand, T., & Schwarz, H. Block-based compressed sensing of images and video. Foundations and Trends in Signal Processing, 2010.]"
      },
      {
        "citation": "[Glasner, D., Bagon, S., & Irani, M. Super-resolution from a single image. IEEE Conference on Computer Vision and Pattern Recognition, 2009.]"
      },
      {
        "citation": "---"
      }
    ],
    "author_details": [
      {
        "name": "Huaijin Chen",
        "affiliation": "ECE Department, Rice University, Houston, TX",
        "email": "[Email not available in provided text]"
      },
      {
        "name": "M. Salman Asif",
        "affiliation": "ECE Department, Rice University, Houston, TX",
        "email": "[Email not available in provided text]"
      },
      {
        "name": "Aswin C. Sankaranarayanan",
        "affiliation": "ECE Department, Carnegie Mellon University, Pittsburgh, PA",
        "email": "[Email not available in provided text]"
      },
      {
        "name": "Ashok Veeraraghavan",
        "affiliation": "ECE Department, Rice University, Houston, TX",
        "email": "[Email not available in provided text]"
      }
    ]
  },
  {
    "title": "Real-time Joint Estimation of Camera Orientation and Vanishing Points\n---AUTHOR---\nJeong-Kyun Lee\n---AUTHOR---\nKuk-Jin Yoon",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Lee_Real-Time_Joint_Estimation_2015_CVPR_paper.pdf",
    "id": "Lee_Real-Time_Joint_Estimation_2015_CVPR_paper",
    "abstract": "A widely-used approach for estimating camera orientation utilizes vanishing points (VPs). While enforcing the Manhattan world constraint between VPs allows for drift-free estimation, it suffers from limitations in practical applications due to spurious parallel lines or non-Manhattan world scenes. This paper proposes a novel method that jointly estimates VPs and camera orientation based on sequential Bayesian filtering, eliminating the Manhattan world assumption and enabling highly accurate, real-time camera orientation estimation. A feature management technique is also introduced to remove false positives and classify detected lines, enhancing robustness. The method's superiority is demonstrated through extensive evaluation using synthetic and real datasets.\n\n---TOPIC---\nCamera Orientation Estimation\nVanishing Points (VPs)\nBayesian Filtering\nFeature Management\nManhattan World Constraint",
    "topics": [],
    "references": [
      {
        "citation": "[S. N. Sinha, D. Steedly, R. Szeliski, M. Agrawala, and M. Pollefeys. Interactive 3d architectural modeling from un-ordered photo collections. ACM Transactions on Graphics, 27(5), 2008.] - Focuses on 3D architectural modeling, a key application area."
      },
      {
        "citation": "[J. Sturm, N. Engelhard, F. Endres, W. Burgard, and D. Cremers. A benchmark for the evaluation of rgb-d slam systems. In IROS, 2012.] - Provides a benchmark for systems relevant to the paper's likely use case."
      },
      {
        "citation": "[J.-C. Bazin, C. Demonceaux, P. Vasseur, and I. Kweon. Rotation estimation and vanishing point extraction by omnidirectional vision in urban environment. IJRR, 31(1):63–81, 2012.] - Addresses a core problem: vanishing point extraction in urban environments."
      },
      {
        "citation": "[R. G. Von Gioi, J. Jakubowicz, J.-M. Morel, and G. Randall. Lsd: A fast line segment detector with a false detection control. TPAMI, 32(4):722–732, 2010.] - Introduces LSD, a significant line segment detector."
      },
      {
        "citation": "[J.-C. Bazin, Y. Seo, C. Demonceaux, P. Vasseur, K. Ikeuchi, I. Kweon, and M. Pollefeys. Globally optimal line clustering and vanishing point estimation in manhattan world. In CVPR, 2012.] - Focuses on line clustering and vanishing point estimation, a central theme."
      },
      {
        "citation": "[Z. Wang, F. Wu, and Z. Hu. Msld: A robust descriptor for line matching. Pattern Recognition, 42(5):941–953, 2009.] - Presents a descriptor for line matching, important for feature correspondence."
      },
      {
        "citation": "[A. Geiger, P. Lenz, and R. Urtasun. Are we ready for autonomous driving? the kitti vision benchmark suite. In CVPR, 2012.] - Provides context and a benchmark related to real-world applications."
      },
      {
        "citation": "[Y. Xu, C. Park, and S. Oh. A minimum error vanishing point detection approach for uncalibrated monocular images of man-made environments. In CVPR, 2013.] - Presents a specific approach to vanishing point detection."
      },
      {
        "citation": "[L. Zhang and R. Koch. An efﬁcient and robust line segment matching approach based on lbd descriptor and pairwise geometric consistency. Journal of Visual Communication and Image Representation, 24(7):794–805, 2013.] - Focuses on line segment matching, a crucial step."
      },
      {
        "citation": "[J. Lezama, R. Grompone von Gioi, G. Randall, and J.-M. Morel. Finding vanishing points via point alignments in image primal and dual domains. In CVPR, 2014.] - Presents a method for finding vanishing points."
      }
    ],
    "author_details": [
      {
        "name": "Jeong-Kyun Lee",
        "affiliation": "Gwangju Institute of Technology",
        "email": "leejk@gist.ac.kr"
      },
      {
        "name": "Kuk-Jin Yoon",
        "affiliation": "Gwangju Institute of Science and Technology",
        "email": "kjyoon@gist.ac.kr"
      }
    ]
  },
  {
    "title": "FaLRR: A Fast Low Rank Representation Solver\n---AUTHORs---\nShijie Xiao\nWen Li\nDong Xu\nDacheng Tao",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Xiao_FaLRR_A_Fast_2015_CVPR_paper.pdf",
    "id": "Xiao_FaLRR_A_Fast_2015_CVPR_paper",
    "abstract": "This paper addresses the challenge of solving the low-rank representation (LRR) problem, a spectral clustering-based approach used in computer vision. Existing LRR algorithms rely on a two-variable formulation involving the original data matrix, which can be computationally expensive. To overcome this limitation, the authors introduce FaLRR, a fast LRR solver that reformulates the problem based on factorized data obtained through skinny SVD. This new formulation benefits both optimization and theoretical analysis, leading to an efficient algorithm guaranteed to find a globally optimal solution. The proposed method achieves significant speedups compared to existing solvers and can be readily integrated into distributed frameworks for further acceleration.",
    "topics": [
      "Low-Rank Representation (LRR)",
      "Spectral Clustering",
      "Singular Value Decomposition (SVD)",
      "Optimization Algorithms (Alternating Direction Method - ADM)",
      "Distributed Computing"
    ],
    "references": [
      {
        "citation": "[S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein. Distributed optimization and statistical learning via the alternating direction method of multipliers. Foundations and Trends in Machine Learning, 3(1):1–122, 2011.] - This paper introduces the alternating direction method of multipliers, a key optimization technique likely used in several related works."
      },
      {
        "citation": "[R. Vidal, Y. Ma, and S. Sastry. Generalized principal component analysis (GPCA). T-PAMI, 27(12):1945–1959, 2005.] - A foundational paper on Generalized Principal Component Analysis, a core technique for dimensionality reduction and feature extraction."
      },
      {
        "citation": "[U. Von Luxburg. A tutorial on spectral clustering. Statistics and computing, 17(4):395–416, 2007.] - Provides a tutorial on spectral clustering, a common clustering algorithm."
      },
      {
        "citation": "[E. Elhamifar and R. Vidal. Sparse subspace clustering: Algorithm, theory, and applications. T-PAMI, 35(11):2765–2781, 2013.] - Introduces sparse subspace clustering, a significant technique for clustering data based on subspace representation."
      },
      {
        "citation": "[J. Cai, C. Emmanuel, and Z. Shen. A singular value thresholding algorithm for matrix completion. SIAM Journal on Optimization, 20(4):1956–1982, 2010.] - This paper presents an algorithm for matrix completion, a technique relevant to handling missing data and reconstructing matrices."
      },
      {
        "citation": "[G. Liu, Z. Lin, S. Yan, J. Sun, Y. Yu, and Y. Ma. Robust recovery of subspace structures by low-rank representation. T-PAMI, 33(1):171–184, 2013.] - This paper focuses on recovering subspace structures using low-rank representation, a core concept in many of the other cited works."
      },
      {
        "citation": "[G. Liu, Z. Lin, and Y. Yu. Robust subspace segmentation by low-rank representation. In ICML, pages 663–670, 2010.] - An earlier work by the same authors, further developing the low-rank representation approach for subspace segmentation."
      },
      {
        "citation": "[A. Y. Ng, M. I. Jordan, Y. Weiss, et al. On spectral clustering: Analysis and an algorithm. In NIPS, pages 849–856, 2002.] - A seminal paper on spectral clustering, providing theoretical analysis and an algorithm."
      },
      {
        "citation": "[H. Fan, Z. Cao, Y. Jiang, Q. Yin, and C. Doudou. Learning deep face representation. arXiv preprint arXiv:1403.2802, 2014.] - Introduces a method for learning deep face representations, a key development in face recognition."
      },
      {
        "citation": "[G. B. Huang, M. Ramesh, T. Berg, and E. Learned-Miller. Labeled faces in the wild: A database for studying face recognition in unconstrained environments. Technical Report 07-49, University of Massachusetts, Amherst, October 2007.] - Describes the Labeled Faces in the Wild (LFW) dataset, a standard benchmark for face recognition algorithms."
      }
    ],
    "author_details": [
      {
        "name": "Shijie Xiao",
        "affiliation": "School of Computer Engineering, Nanyang Technological University, Singapore",
        "email": "XIAO0050@ntu.edu.sg"
      },
      {
        "name": "Wen Li",
        "affiliation": "School of Computer Engineering, Nanyang Technological University, Singapore",
        "email": "WLI1@ntu.edu.sg"
      },
      {
        "name": "Dong Xu",
        "affiliation": "School of Computer Engineering, Nanyang Technological University, Singapore",
        "email": "DongXu@ntu.edu.sg"
      },
      {
        "name": "Dacheng Tao",
        "affiliation": "Centre for Quantum Computation & Intelligent Systems and the Faculty of Engineering and Information Technology, University of Technology, Sydney, Australia",
        "email": "Dacheng.Tao@uts.edu.au"
      }
    ]
  },
  {
    "title": "Modeling Object Appearance using Context-Conditioned Component Analysis\n---AUTHOR---\nDaniyar Turmukhambetov\nNeill D.F. Campbell\nSimon J.D. Prince\nJan Kautz",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Turmukhambetov_Modeling_Object_Appearance_2015_CVPR_paper.pdf",
    "id": "Turmukhambetov_Modeling_Object_Appearance_2015_CVPR_paper",
    "abstract": "Subspace models are effective for modeling object appearance when images are aligned. However, they struggle with objects exhibiting deformations, occlusions, or varying structures, as these scenarios violate the assumption of a one-to-one mapping between image parts. This paper introduces a novel approach that removes these alignment limitations by conditioning the models on a shape-dependent context. This allows for the capture of complex, non-linear appearance structures, enabling the modeling of non-rigid objects, occlusions, and varying numbers of parts. The effectiveness of the model is demonstrated through structured inpainting and appearance transfer examples.",
    "topics": [
      "Subspace Models",
      "Object Appearance Modeling",
      "Context-Conditioned Models",
      "Deformable Objects",
      "Occlusion Handling"
    ],
    "references": [
      {
        "citation": "[Belhumeur, P. N., Hespanha, J. P., and Kriegman, D. J. Eigenfaces vs. Fisherfaces: Recognition using class specific linear projection. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 19(7):711–720, 1997.]"
      },
      {
        "citation": "[Leung, T., and Malik, J. Representing and recognizing the visual appearance of materials using three-dimensional textons. International Journal of Computer Vision, 43(1):29–44, 2001.]"
      },
      {
        "citation": "[Liu, C., Shum, H.-Y., and Freeman, W. T. Face hallucination: Theory and practice. International Journal of Computer Vision, 75(1):115–134, 2007.]"
      },
      {
        "citation": "[Liu, C., Shum, H.-Y., and Zhang, C.-S. A two-step approach to hallucinating faces: global parametric model and local nonparametric model. In International Conference on Computer Vision and Pattern Recognition (CVPR), volume 1, pages I–192. IEEE, 2001.]"
      },
      {
        "citation": "[Liu, C., Yuen, J., Torralba, A., Sivic, J., and Freeman, W. T. SIFT flow: Dense correspondence across different scenes. In Computer Vision – ECCV 2008, pages 28–42. Springer, 2008.]"
      },
      {
        "citation": "[Mobahi, H., Liu, C., and Freeman, W. T. A compositional model for low-dimensional image set representation. In International Conference on Computer Vision and Pattern Recognition (CVPR). IEEE Computer Society, 2014.]"
      },
      {
        "citation": "[Mohammed, U., Prince, S. J. D., and Kautz, J. Visio-lization: generating novel facial images. ACM Trans. Graph., 28(3):57:1–57:8, July 2009.]"
      },
      {
        "citation": "[Prince, S. J. D. Computer Vision: Models, Learning, and Inference. Cambridge University Press, 2012.]"
      },
      {
        "citation": "[Rasmussen, C. E., and Williams, C. Gaussian Processes for Machine Learning. MIT Press, 2006.]"
      },
      {
        "citation": "[S´anchez-Lozano, E., De la Torre, F., and Gonz´alez-Jim´enez, D. Continuous regression for non-rigid image alignment. In Computer Vision – ECCV 2012, pages 250–263. Springer, 2012.]"
      }
    ],
    "author_details": [
      {
        "name": "Daniyar Turmukhambetov",
        "affiliation": "University College London, UK",
        "email": "[Email not available]"
      },
      {
        "name": "Neill D.F. Campbell",
        "affiliation": "University College London, UK",
        "email": "[Email not available]"
      },
      {
        "name": "Simon J.D. Prince",
        "affiliation": "University College London, UK",
        "email": "[Email not available]"
      },
      {
        "name": "Jan Kautz",
        "affiliation": "University College London, UK",
        "email": "[Email not available]"
      }
    ]
  },
  {
    "title": "Simultaneous Video Defogging and Stereo Reconstruction\n---AUTHOR---\nZhuwen Li\nPing Tan\nRobby T. Tan\nDanping Zou\nSteven Zhiying Zhou\nLoong-Fah Cheong",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Li_Simultaneous_Video_Defogging_2015_CVPR_paper.pdf",
    "id": "Li_Simultaneous_Video_Defogging_2015_CVPR_paper",
    "abstract": "This paper presents a novel method for simultaneously estimating scene depth and recovering clear latent images from a foggy video sequence. The approach leverages the complementary nature of stereo depth cues and fog information, resulting in superior performance compared to conventional stereo or defogging algorithms. Key innovations include an improved photo-consistency term that models fog scattering effects, a matting Laplacian constraint for detail-preserving depth smoothness, and ordering consistency between depth and fog transmission. The method is formulated within a Markov Random Field (MRF) framework and optimized iteratively. Experiments on real videos demonstrate the effectiveness of the proposed technique, particularly in scenes with thick fog or significant camera movement.\n\n---TOPIPS---\nSimultaneous Depth Estimation\nVideo Defogging\nMarkov Random Fields (MRF)\nPhoto-Consistency Modeling\nFog Transmission Estimation",
    "topics": [],
    "references": [
      {
        "citation": "[D. Lowe, Distinctive image features from scale-invariant keypoints, International Journal of Computer Vision, 60(2):91–110, 2004.]"
      },
      {
        "citation": "[C. Ancuti, C. O. Ancuti, T. Haber, and P. Bekaert, Enhancing underwater images and videos by fusion, Proc. CVPR, 2012.]"
      },
      {
        "citation": "[A. F. Bobick and S. S. Intille, Large occlusion stereo, International Journal of Computer Vision, 33(3):181–200, 1999.]"
      },
      {
        "citation": "[Y. Boykov, O. Veksler, and R. Zabih, Fast approximate energy minimization via graph cuts, IEEE Trans. Pattern Anal. Mach. Intell., 23(11):1222–1239, 2001.]"
      },
      {
        "citation": "[D. Bradley, T. Boubekeur, and W. Heidrich, Accurate multi-view reconstruction using robust binocular stereo and surface meshing, Proc. CVPR, 2008.]"
      },
      {
        "citation": "[N. D. F. Campbell, G. Vogiatzis, C. Hernandez, and R. Cipolla, Using multiple hypotheses to improve depth-maps for multi-view stereo, Proc. ECCV, 2008.]"
      },
      {
        "citation": "[L. Caraffa and J.-P. Tarel, Stereo reconstruction and contrast restoration in daytime fog, Proc. ACCV, 2012.]"
      },
      {
        "citation": "[F. G. Cozman and E. Krotkov, Depth from scattering, Proc. CVPR, 1997.]"
      },
      {
        "citation": "[P. F. Felzenszwalb and D. P. Huttenlocher, Efﬁcient belief propagation for early vision, International Journal of Computer Vision, 70(1):41–54, 2006.]"
      },
      {
        "citation": "[Y. Furukawa and J. Ponce, Accurate, dense, and robust multiview stereopsis, IEEE Trans. Pattern Anal. Mach. Intell., 32(8):1362–1376, 2009.]"
      }
    ],
    "author_details": [
      {
        "name": "Zhuwen Li",
        "affiliation": "National University of Singapore",
        "email": "[Email not available]"
      },
      {
        "name": "Ping Tan",
        "affiliation": "Simon Fraser University",
        "email": "[Email not available]"
      },
      {
        "name": "Robby T. Tan",
        "affiliation": "SIM University",
        "email": "[Email not available]"
      },
      {
        "name": "Danping Zou",
        "affiliation": "Shanghai Jiao Tong University",
        "email": "[Email not available]"
      },
      {
        "name": "Steven Zhiying Zhou",
        "affiliation": "National University of Singapore (Suzhou) Research Institute",
        "email": "[Email not available]"
      },
      {
        "name": "Loong-Fah Cheong",
        "affiliation": "National University of Singapore",
        "email": "[Email not available]"
      }
    ]
  },
  {
    "title": "Self Scaled Regularized Robust Regression\n---AUTHOR---\nYin Wang\nCaglayan Dicle\nMario Sznaier\nOctavia Camps",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Wang_Self_Scaled_Regularized_2015_CVPR_paper.pdf",
    "id": "Wang_Self_Scaled_Regularized_2015_CVPR_paper",
    "abstract": "Linear Robust Regression (LRR) aims to find linear mapping parameters from noisy data corrupted by outliers, maximizing the number of inliers. While NP-hard, tractable relaxations exist with recovery guarantees, they struggle with large outlier errors and lack the ability to incorporate prior information. This paper proposes a new sparsity-based formulation that handles gross outliers and a priori information, demonstrating its equivalence to a \"self-scaled\" ℓ1 regularized robust regression problem. This self-scaling property improves performance with gross outliers while maintaining theoretical recovery properties. The proposed method, termed \"Self-Scaled Regularized Robust Regression\" (S2R3), is illustrated with application examples using synthetic and real data.",
    "topics": [
      "Robust Regression",
      "Linear Regression",
      "Sparsity-based Methods",
      "Outlier Detection",
      "Self-Scaled Regularization"
    ],
    "references": [
      {
        "citation": "[M. Fischler and R. C. Bolles. Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography. Communications of the ACM, 24(6):381–395, 1981.] - This is the original RANSAC paper, foundational to the work."
      },
      {
        "citation": "[E. J. Cand`es, X. Li, Y. Ma, and J. Wright. Robust principal component analysis? Journal of the ACM (JACM), 58(3):11, 2011.] - This paper directly addresses robust regression, a core component of the work."
      },
      {
        "citation": "[P. Huber. Robust Statistics. Wiley, 1981.] - A classic text on robust statistics, providing background and context."
      },
      {
        "citation": "[E. J. Candes and T. Tao. Decoding by linear programming. IEEE Trans. Inf. Theory, 51(12):4203–4215, 2005.] - Relevant due to its connection to sparse signal recovery and optimization techniques."
      },
      {
        "citation": "[O. Chum, J. Matas, and J. Kittler. Locally optimized ransac. In Pattern Recognition, pages 236–243. Springer, 2003.] - A significant extension of the original RANSAC."
      },
      {
        "citation": "[R. Hartley and A. Zisserman. Multiple View Geometry. Cambridge University Press, 2003.] - Provides the geometric background for many computer vision problems."
      },
      {
        "citation": "[D. L. Donoho. Compressed sensing. IEEE Trans. Inf. Theory, 52(4):1289–1306, 2006.] - Provides context for sparse recovery techniques."
      },
      {
        "citation": "[K. Mitra, A. Veeraraghavan, and R. Chellappa. Analysis of sparse regularization based robust regression algorithms. IEEE Trans. Signal Processing, 61(5):1249–1257, 2013.] - Directly related to the use of sparse regularization in robust regression."
      },
      {
        "citation": "[P. J. Rousseeuw. Least median of squares regression. Journal of the American statistical association, 79(388):871–880, 1984.] - Another important method in robust statistics."
      },
      {
        "citation": "[H. Xu, C. Caramanis, and S. Sanghavi. Robust pca via outlier pursuit. In J. Lafferty, C. Williams, J. Shawe-Taylor, R. Zemel, and A. Culotta, editors, Advances in Neural Information Processing Systems 23, pages 2496–2504. Curran Associates, Inc., 2010.] - Explores robust PCA, a related problem."
      }
    ],
    "author_details": [
      {
        "name": "Yin Wang",
        "affiliation": "Northeastern University, Boston, MA 02115",
        "email": "wang.yin@husky.neu.edu"
      },
      {
        "name": "Caglayan Dicle",
        "affiliation": "Northeastern University, Boston, MA 02115",
        "email": "cdicle@gmail.com"
      },
      {
        "name": "Mario Sznaier",
        "affiliation": "Northeastern University, Boston, MA 02115",
        "email": "msznaier@coe.neu.edu"
      },
      {
        "name": "Octavia Camps",
        "affiliation": "Northeastern University, Boston, MA 02115",
        "email": "camps@coe.neu.edu"
      }
    ]
  },
  {
    "title": "Beyond Mahalanobis Metric: Cayley-Kin Metric Learning\n---AUTHOR---\nYanhong Bi\nBin Fan\nFuchao Wu",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Bi_Beyond_Mahalanobis_Metric_2015_CVPR_paper.pdf",
    "id": "Bi_Beyond_Mahalanobis_Metric_2015_CVPR_paper",
    "abstract": "This paper introduces the Cayley-Klein metric, a non-Euclidean metric suitable for projective space, into the computer vision community as an alternative to the widely studied Mahalanobis metric. The authors demonstrate that it is a generalization of Mahalanobis metric in specific cases and present two Cayley-Klein metric learning methods (MMC and LMNN). Experimental results show the superiority of the Cayley-Klein metric over Mahalanobis metrics and the effectiveness of the proposed learning methods.\n\n---TOPIC---\nCayley-Klein metric\nMetric learning\nMahalanobis metric\nProjective space\nComputer vision",
    "topics": [],
    "references": [],
    "author_details": [
      {
        "name": "Yanhong Bi",
        "affiliation": "Institute of Automation, Chinese Academy of Sciences",
        "email": "yanhong.bi@nlpr.ia.ac.cn"
      },
      {
        "name": "Bin Fan",
        "affiliation": "Institute of Automation, Chinese Academy of Sciences",
        "email": "bfan@nlpr.ia.ac.cn"
      },
      {
        "name": "Fuchao Wu",
        "affiliation": "Institute of Automation, Chinese Academy of Sciences",
        "email": "fcwu@nlpr.ia.ac.cn"
      }
    ]
  },
  {
    "title": "Learning semantic relationships for better action retrieval in images\n---AUTHOR---\nVignesh Ramanathan\nCongcong Li\nJia Deng\nWei Han\nZhen Li\nKunlong Gu\nYang Song\nSamy Bengio\nChuck Rossenberg\nLi Fei-Fei",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Ramanathan_Learning_Semantic_Relationships_2015_CVPR_paper.pdf",
    "id": "Ramanathan_Learning_Semantic_Relationships_2015_CVPR_paper",
    "abstract": "Human actions capture a wide variety of interactions between people and objects, leading to a vast and sparsely supervised action space. To address this sparsity, the authors propose a novel neural network framework that jointly extracts relationships between actions and uses them for training better action retrieval models. The model incorporates linguistic, visual, and logical consistency cues to identify these relationships and demonstrates a significant improvement in mean AP compared to baseline methods.\n\n---TOPICCS---\nAction Recognition\nSemantic Relationships\nNeural Networks\nAction Retrieval\nFew-Shot Learning",
    "topics": [],
    "references": [],
    "author_details": [
      {
        "name": "Vignesh Ramanathan",
        "affiliation": "Stanford University",
        "email": "vigneshr@cs.stanford.edu"
      },
      {
        "name": "Congcong Li",
        "affiliation": "Google",
        "email": "congcongli@google.com"
      },
      {
        "name": "Jia Deng",
        "affiliation": "University of Michigan",
        "email": "jia deng@umich.edu"
      },
      {
        "name": "Wei Han",
        "affiliation": "Google",
        "email": "weihan@google.com"
      },
      {
        "name": "Zhen Li",
        "affiliation": "Google",
        "email": "zhenli@google.com"
      },
      {
        "name": "Kunlong Gu",
        "affiliation": "Google",
        "email": "kunlonggu@google.com"
      },
      {
        "name": "Yang Song",
        "affiliation": "Google",
        "email": "yangsong@google.com"
      },
      {
        "name": "Samy Bengio",
        "affiliation": "Google",
        "email": "bengio@google.com"
      },
      {
        "name": "Chuck Rossenberg",
        "affiliation": "Google",
        "email": "chuck@google.com"
      },
      {
        "name": "Li Fei-Fei",
        "affiliation": "Stanford University",
        "email": "feifeili@cs.stanford.edu"
      }
    ]
  },
  {
    "title": "Long-term Correlation Tracking\n---AUTHOR---\nChao Ma\n---AUTHOR---\nXiaokang Yang\n---AUTHOR---\nChongyang Zhang\n---AUTHOR---\nMing-Hsuan Yang",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Ma_Long-Term_Correlation_Tracking_2015_CVPR_paper.pdf",
    "id": "Ma_Long-Term_Correlation_Tracking_2015_CVPR_paper",
    "abstract": "In this paper, we address the problem of long-term visual tracking where the target objects undergo significant appearance variation due to deformation, abrupt motion, heavy occlusion and out-of-view. In this setting, we decompose the task of tracking into translation and scale estimation of objects. We show that the correlation between temporal context considerably improves the accuracy and reliability for translation estimation, and it is effective to learn discriminative correlation filters from the most confident frames to estimate the scale change. In addition, we train an online random fern classifier to re-detect objects in case of tracking failure. Extensive experimental results on large-scale benchmark datasets show that the proposed algorithm performs favorably against state-of-the-art methods in terms of efﬁciency, accuracy, and robustness.\n\n---TOPICICS---\nLong-term visual tracking\nCorrelation filters\nScale and translation estimation\nOnline random fern classifier\nTemporal context modeling",
    "topics": [],
    "references": [
      {
        "citation": "[Murphy, K. P. Machine Learning: A Probabilistic Perspective. The MIT Press, 2012.] - Provides a foundational understanding of machine learning principles, likely relevant to many tracking approaches."
      },
      {
        "citation": "[Babenko, B., Yang, M.-H., and Belongie, S. Robust object tracking with online multiple instance learning. IEEE Transactions on Pattern Analysis and Machine Intelligence, 33(8), 2011.] - Introduces a specific tracking method using multiple instance learning."
      },
      {
        "citation": "[Baker, S., and Matthews, I. Lucas-kanade 20 years on: A unifying framework. International Journal of Computer Vision, 56(3):221–255, 2004.] - Discusses a widely used optical flow algorithm, likely a core component of many trackers."
      },
      {
        "citation": "[Dalal, N., and Triggs, B. Histograms of oriented gradients for human detection. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, 2005.] - Introduces HOG features, a common feature descriptor used in object detection and tracking."
      },
      {
        "citation": "[Viola, P. A., and Jones, M. J. Rapid object detection using a boosted cascade of simple features. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, 2001.] - Describes the Viola-Jones object detection framework, a significant contribution to real-time object detection."
      },
      {
        "citation": "[Santner, J., Leistner, C., Saffari, A., Pock, T., and Bischof, H. PROST: parallel robust online simple tracking. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, 2010.] - Presents a specific tracking algorithm (PROST)."
      },
      {
        "citation": "[Smeulders, A. W. M., Chu, D. M., Cucchiara, R., Calderara, S., Dehghan, A., and Shah, M. Visual tracking: An experimental survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 36(7):1442–1468, 2014.] - Provides a comprehensive overview and benchmark of visual tracking methods."
      },
      {
        "citation": "[Yilmaz, A., Javed, O., and Shah, M. Object tracking: A survey. ACM Computing Surveys, 38(4), 2006.] - Offers a broad survey of object tracking techniques."
      },
      {
        "citation": "[Henriques, J. F., Caseiro, R., Martins, P., and Batista, J. Exploiting the circulant structure of tracking-by-detection with kernels. In Proceedings of the European Conference on Computer Vision, 2012.] - Introduces a tracking method based on circulant structure and kernel methods."
      },
      {
        "citation": "[Danelljan, M., Hager, G., Khan, F. S., and Felsberg, M. Accurate scale estimation for robust visual tracking. In Proceedings of British Machine Vision Conference, 2014.] - Focuses on a critical aspect of tracking: scale estimation."
      }
    ],
    "author_details": [
      {
        "name": "Chao Ma",
        "affiliation": "Shanghai Jiao Tong University",
        "email": "chaoma@sjtu.edu.cn"
      },
      {
        "name": "Xiaokang Yang",
        "affiliation": "Shanghai Jiao Tong University",
        "email": "xkyang@sjtu.edu.cn"
      },
      {
        "name": "Chongyang Zhang",
        "affiliation": "Shanghai Jiao Tong University",
        "email": "sunny zhang@sjtu.edu.cn"
      },
      {
        "name": "Ming-Hsuan Yang",
        "affiliation": "University of California at Merced",
        "email": "mhyang@ucmerced.edu"
      }
    ]
  },
  {
    "title": "A Coarse-to-Fine Model for 3D Pose Estimation and Sub-category Recognition\n---AUTHOR---\nRoozbeh Mottaghi\nYu Xiang\nSilvio Savarese",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Mottaghi_A_Coarse-to-Fine_Model_2015_CVPR_paper.pdf",
    "id": "Mottaghi_A_Coarse-to-Fine_Model_2015_CVPR_paper",
    "abstract": "Despite the correlation between object detection, 3D pose estimation, and sub-category recognition, they are typically addressed independently due to the complexity of modeling them jointly. This paper proposes a coarse-to-fine hierarchical representation to jointly model these tasks, preventing performance loss caused by increased parameters. The hierarchical representation leverages the PASCAL3D+ dataset, augmented with annotations, and demonstrates the effectiveness of the approach in joint modeling of object detection, 3D pose estimation, and sub-category recognition.\n\n---TOPIC---\n3D Pose Estimation\n---TOPI---\nSub-category Recognition\n---TOPI---\nObject Detection\n---TOPI---\nHierarchical Representation\n---TOPI---\nCoarse-to-Fine Modeling",
    "topics": [],
    "references": [
      {
        "citation": "[M. Arie-Nachimson and R. Basri. Constructing implicit 3d shape models for pose estimation. In ICCV, 2009.]"
      },
      {
        "citation": "[M. Aubry, D. Maturana, A. A. Efros, B. C. Russell, and J. Sivic. Seeing 3d chairs: exemplar part-based 2d-3d align-ment using a large dataset of cad models. In CVPR, 2014.]"
      },
      {
        "citation": "[T. Berg and P. N. Belhumeur. Poof: Part-based one-vs-one features for ﬁne-grained categorization, face veriﬁcation, and attribute estimation. In CVPR, 2013.]"
      },
      {
        "citation": "[R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In CVPR, 2014.]"
      },
      {
        "citation": "[S. Fidler and A. Leonardis. Towards scalable representations of object categories: Learning a hierarchy of parts. In CVPR, 2007.]"
      },
      {
        "citation": "[J. Krause, M. Stark, J. Deng, and L. Fei-Fei. 3d object representations for ﬁne-grained categorization. In 3dRR Workshop, 2013.]"
      },
      {
        "citation": "[B. Hariharan, P. Arbeláez, L. Bourdev, S. Maji, and J. Malik. Semantic contours from inverse detectors. In ICCV, 2011.]"
      },
      {
        "citation": "[R. Viola and M. Jones. Rapid object detection using a boosted cascade of simple features. In CVPR, 2001.]"
      },
      {
        "citation": "[J. R. R. Uijlings, K. E. A. van de Sande, T. Gevers, and A. W. M. Smeulder. Selective search for object recognition. IJCV, 2013.]"
      },
      {
        "citation": "[B. Pepik, P. Gehler, M. Stark, and B. Schiele. 3d2pm - 3d deformable part models. In ECCV, 2012.]"
      }
    ],
    "author_details": [
      {
        "name": "Roozbeh Mottaghi",
        "affiliation": "Allen Institute for AI",
        "email": "Not available"
      },
      {
        "name": "Yu Xiang",
        "affiliation": "University of Michigan-Ann Arbor",
        "email": "Not available"
      },
      {
        "name": "Silvio Savarese",
        "affiliation": "Stanford University",
        "email": "Not available"
      }
    ]
  },
  {
    "title": "Predicting the Future Behavior of a Time-Varying Probability Distribution\n---AUTHOR---\nChris Christoph Lampert",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Lampert_Predicting_the_Future_2015_CVPR_paper.pdf",
    "id": "Lampert_Predicting_the_Future_2015_CVPR_paper",
    "abstract": "We study the problem of predicting the future, though only in the probabilistic sense of estimating a future state of a time-varying probability distribution. This is not only an interesting academic problem, but solving this extrapolation problem also has many practical applications, e.g. for training classifiers that have to operate under time-varying conditions. Our main contribution is a method for predicting the next step of the time-varying distribution from a given sequence of sample sets from earlier time steps. For this we rely on two recent machine learning techniques: embedding probability distributions into a reproducing kernel Hilbert space, and learning operators by vector-valued regression. We illustrate the working principles and the practical usefulness of our method by experiments on synthetic and real data. We also highlight an exemplary application: training a classifier in a domain adaptation setting without having access to examples from the test time distribution at training time.",
    "topics": [
      "Time-varying probability distributions",
      "Extrapolation",
      "Reproducing Kernel Hilbert Spaces (RKHS)",
      "Vector-valued regression",
      "Domain adaptation"
    ],
    "references": [
      {
        "citation": "[Smola, A., Gretton, A., Song, L., & Sch¨olkopf, B. A Hilbert space embedding for distributions. In International Conference on Algorithmic Learning Theory (ALT), 2007.]"
      },
      {
        "citation": "[Altun, Y., & Smola, A. Unifying divergence minimization and statistical inference via convex duality. In Workshop on Computational Learning Theory (COLT), 2006.]"
      },
      {
        "citation": "[Song, L. Learning via Hilbert space embedding of distributions. PhD thesis, University of Sydney, 2008.]"
      },
      {
        "citation": "[Bach, F., Lacoste-Julien, S., & Obozinski, G. On the equivalence between herding and conditional gradient algorithms. In International Conference on Machine Learning (ICML), 2012.]"
      },
      {
        "citation": "[Song, L., Huang, J., Smola, A., & Fukumizu, K. Hilbert space embeddings of conditional distributions with applications to dynamical systems. In International Conference on Machine Learning (ICML), 2009.]"
      },
      {
        "citation": "[Sriperumbudur, B. K., Fukumizu, K., & Lanckriet, G. Universality, characteristic kernels and RKHS embedding of measures. Journal of Machine Learning Research (JMLR), 12:2389–2410, 2011.]"
      },
      {
        "citation": "[Chen, Y., Welling, M., & Smola, A. J. Super-samples from kernel herding. In Uncertainty in Artificial Intelligence (UAI), 2010.]"
      },
      {
        "citation": "[Koppula, H. S., & Saxena, A. Anticipating human activities using object affordances for reactive robotic response. In Robotics: Science and Systems, 2013.]"
      },
      {
        "citation": "[Kalman, R. E. A new approach to linear filtering and prediction problems. Journal of Basic Engineering, 82:35–45, 1960.]"
      },
      {
        "citation": "[Lampert, C. H. Kernel methods in computer vision. Foundations and Trends in Computer Graphics and Vision, 4(3):193–285, 2009.]"
      }
    ],
    "author_details": [
      {
        "name": "Chris Christoph Lampert",
        "affiliation": "IST Austria",
        "email": "chl@ist.ac.at"
      }
    ]
  },
  {
    "title": "Diversity-induced Multi-view Subspace Clustering\n---AUTHOR---\nXiaochun Cao\nChangqing Zhang\nHuazhu Fu\nSi Liu\nHua Zhang",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Cao_Diversity-Induced_Multi-View_Subspace_2015_CVPR_paper.pdf",
    "id": "Cao_Diversity-Induced_Multi-View_Subspace_2015_CVPR_paper",
    "abstract": "In this paper, we focus on how to boost the multi-view clustering by exploring the complementary information among multi-view features. A multi-view clustering framework, called Diversity-induced Multi-view Subspace Clustering (DiMSC), is proposed for this task. In our method, we extend the existing subspace clustering into the multi-view domain, and utilize the Hilbert Schmidt Independence Criterion (HSIC) as a diversity term to explore the complementarity of multi-view representations, which could be solved efficiently by using the alternating minimizing optimization. Compared to other multi-view clustering methods, the enhanced complementarity reduces the redundancy between the multi-view representations, and improves the accuracy of the clustering results. Experiments on both image and video face clustering well demonstrate that the proposed method outperforms the state-of-the-art methods.",
    "topics": [
      "Multi-view Clustering",
      "Subspace Clustering",
      "Hilbert Schmidt Independence Criterion (HSIC)",
      "Diversity-induced Learning",
      "Complementarity of Views"
    ],
    "references": [
      {
        "citation": "[M.-R. Amini and C. Goutte. A co-classiﬁcation approach to learning from multilingual corpora. Machine learning, 79(1):105–121, 2010.] - This paper introduces a co-classiﬁcation approach, which is relevant to multi-view learning contexts."
      },
      {
        "citation": "[R. H. Bartelts and G. W. Stewart. Solution of the matrix equation AX + XB = C. Communications of the ACM, 15(9):820–826, 1972.] - This is a foundational paper on solving matrix equations, which can be relevant to the underlying mathematical formulations used in many multi-view learning algorithms."
      },
      {
        "citation": "[M. Blaschko and C. H. Lampert. Correlational spectral clustering. In CVPR, pages 1–8, 2008.] - A seminal work on spectral clustering, a core technique in many multi-view learning approaches."
      },
      {
        "citation": "[A. Blum and T. Mitchell. Combining labeled and unlabeled data with co-training. In COLT, pages 92–100. ACM, 1998.] - Introduces the co-training method, a key concept for leveraging unlabeled data, often used in conjunction with multi-view learning."
      },
      {
        "citation": "[K. Chaudhuri, S. M. Kakade, K. Livescu, and K. Sridharan. Multi-view clustering via canonical correlation analysis. In ICML, pages 129–136, 2009.] - Directly addresses multi-view clustering using canonical correlation analysis."
      },
      {
        "citation": "[E. Elhamifar and R. Vidal. Sparse subspace clustering: Algorithm, theory, and applications. TPAMI, 35(11):2765–2781, 2013.] - A key paper on sparse subspace clustering, a widely used technique in multi-view learning."
      },
      {
        "citation": "[N. Chen, J. Zhu, and E. Xing. Predictive subspace learning for multi-view data: A large margin approach. In NIPS, pages 361–369, 2010.] - Presents a predictive subspace learning approach for multi-view data."
      },
      {
        "citation": "[C. Xu, D. Tao, and C. Xu. A survey on multi-view learning. arXiv preprint arXiv:1304.5634, 2013.] - Provides a comprehensive overview of the field of multi-view learning."
      },
      {
        "citation": "[H. Fu, D. Xu, S. Lin, D. W. K. Wong, and J. Liu. Automatic optic disc detection in oct slices via low-rank reconstruction. TBME, 62(4):1151–1158, 2015.] - Demonstrates the application of low-rank reconstruction, a common technique in multi-view learning, to a specific problem."
      },
      {
        "citation": "[A. Kumar and H. Daum´e III. A co-training approach for multi-view spectral clustering. In ICML, pages 393–400, 2011.] - Combines co-training with spectral clustering for multi-view data."
      }
    ],
    "author_details": [
      {
        "name": "Xiaochun Cao",
        "affiliation": "State Key Laboratory of Information Security, IIE, Chinese Academy of Sciences",
        "email": "caoxiaochun@iie.ac.cn"
      },
      {
        "name": "Changqing Zhang",
        "affiliation": "School of Computer Science and Technology, Tianjin University",
        "email": "zhangchangqing@tju.edu.cn"
      },
      {
        "name": "Huazhu Fu",
        "affiliation": "School of Computer Engineering, Nanyang Technological University",
        "email": "hzfu@ntu.edu.sg"
      },
      {
        "name": "Si Liu",
        "affiliation": "State Key Laboratory of Information Security, IIE, Chinese Academy of Sciences",
        "email": "fifthzombiesi@gmail.com"
      },
      {
        "name": "Hua Zhang",
        "affiliation": "School of Computer Science and Technology, Tianjin University",
        "email": "huazhang@tju.edu.cn"
      }
    ]
  },
  {
    "title": "An appealing feature of our alignment model is that it learns to modulate the importance of words and regions by scaling the magnitude of their corresponding embedding vectors.\n---AUTHORs---\nAndrej Karpathy\nLi Fei-Fei",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Karpathy_Deep_Visual-Semantic_Alignments_2015_CVPR_supplemental.pdf",
    "id": "Karpathy_Deep_Visual-Semantic_Alignments_2015_CVPR_supplemental",
    "abstract": "The paper introduces an alignment model that learns to modulate the importance of words and regions by scaling the magnitude of their corresponding embedding vectors. This allows the model to assign higher magnitudes to visually discriminative words and regions, increasing their influence on image-sentence similarity scores. Conversely, stop words are mapped near the origin, reducing their influence. The model also learns visual appearances of text snippets from raw data, demonstrating sensitivity to compound words and modifiers. The authors also discuss and ultimately abandon an approach using Natural Language Processing toolkits for sentence processing due to performance and signal limitations.\n\n---TOPIC---\nMagnitude Modulation\nImage-Sentence Alignment\nVisual Salience\nText Snippet Appearance\nNeural Network Architectures",
    "topics": [],
    "references": [
      {
        "citation": "Frome, A., Corrado, G. S., Shlens, J., Bengio, S., Dean, J., & Mikolov, T. (2013). Devise: A deep visual-semantic embedding model. *NIPS*."
      },
      {
        "citation": "Karpathy, A., Joulin, A., & Fei-Fei, L. (2014). Deep fragment embeddings for bidirectional image sentence mapping. *arXiv preprint arXiv:1406.5679*."
      },
      {
        "citation": "Kiros, R., Salakhutdinov, R., & Zemel, R. S. (2014). Unifying visual-semantic embeddings with multimodal neural language models. *arXiv preprint arXiv:1411.2539*."
      },
      {
        "citation": "Mao, J., Xu, W., Yang, Y., Wang, J., & Yuille, A. L. (2014). Explain images with multimodal recurrent neural networks. *arXiv preprint arXiv:1410.1090*."
      },
      {
        "citation": "Socher, R., Karpathy, A., Le, Q. V., Manning, C. D., & Ng, A. Y. (2014). Grounded compositional semantics for finding and describing images with sentences. *TACL*."
      }
    ],
    "author_details": [
      {
        "name": "Andrej Karpathy",
        "affiliation": "Department of Computer Science, Stanford University",
        "email": "karpathy@cs.stanford.edu"
      },
      {
        "name": "Li Fei-Fei",
        "affiliation": "Department of Computer Science, Stanford University",
        "email": "feifeili@cs.stanford.edu"
      }
    ]
  },
  {
    "title": "From Dictionary of Visual Words to Subspaces: Locality-constrained Afﬁne Subspace Coding\n---AUTHOR---\nPeihua Li\n---AUTHOR---\nXiaoxiao Lu\n---AUTHOR---\nQilong Wang",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Li_From_Dictionary_of_2015_CVPR_paper.pdf",
    "id": "Li_From_Dictionary_of_2015_CVPR_paper",
    "abstract": "The locality-constrained linear coding (LLC) is a successful feature coding method, but it crudely approximates the data manifold. To address this, the paper proposes a novel feature coding method called locality-constrained afﬁne subspace coding (LASC). LASC characterizes the data manifold using an ensemble of subspaces attached to representative points (afﬁne subspaces), providing a piecewise linear approximation. The method also incorporates a second-order LASC vector based on the Fisher information metric for further performance improvement. Experiments on challenging benchmarks demonstrate the competitiveness of the LASC method.",
    "topics": [
      "Bag of Visual Words (BoVW)",
      "Locality-constrained Linear Coding (LLC)",
      "Locality-constrained Afﬁne Subspace Coding (LASC)",
      "Feature Coding Methods",
      "Data Manifold Approximation"
    ],
    "references": [
      {
        "citation": "[S. Lazebnik, C. Schmid, and J. Ponce. Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories. In CVPR, 2006.]"
      },
      {
        "citation": "[S. Amarri. Natural gradient works efﬁciently in learning. Neural Comput., 10(2):251–276, 1998.]"
      },
      {
        "citation": "[G. D. Canas, T. Poggio, and L. Rosasco. Learning manifolds with k-means and k-ﬂats. NIPS, 2012.]"
      },
      {
        "citation": "[D. G. Lowe. Distinctive image features from scale-invariant keypoints. IJCV, 60(2):91–110, 2004.]"
      },
      {
        "citation": "[J. Carreira, R. Caseiro, J. Batista, and C. Sminchisescu. Semantic Segmentation with Second-Order Pooling. In ECCV, 2012.]"
      },
      {
        "citation": "[G. Csurka, C. Dance, L. Fan, J. Willamowski, and C. Bray. Visual categorization with bags of keypoints. In ECCV Workshop on Statistical Learning in Computer Vision, 2004.]"
      },
      {
        "citation": "[E. Elhamifar and R. Vidal. Sparse subspace clustering. In CVPR, 2009.]"
      },
      {
        "citation": "[P. Favaro, R. Vidal, and A. Ravichandran. A closed form solution to robust subspace estimation and clustering. In CVPR, 2011.]"
      },
      {
        "citation": "[M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zisserman. The PASCAL Visual Object Classes (VOC) Challenge. IJCV, 88(2):303–338, 2010.]"
      },
      {
        "citation": "[T. Hastie, R. Tibshirani, and J. Friedman. The Elements of Statistial Learning. Springer, 2009.]"
      }
    ],
    "author_details": [
      {
        "name": "Peihua Li",
        "affiliation": "School of Information and Communication Engineering, Dalian University of Technology",
        "email": "peihuali@dlut.edu.cn"
      },
      {
        "name": "Xiaoxiao Lu",
        "affiliation": "School of Information and Communication Engineering, Dalian University of Technology",
        "email": "shaw@mail.dlut.edu.cn"
      },
      {
        "name": "Qilong Wang",
        "affiliation": "School of Information and Communication Engineering, Dalian University of Technology",
        "email": "qlwang@mail.dlut.edu.cn"
      }
    ]
  },
  {
    "title": "Deep Filter Banks for Texture Recognition and Segmentation\n---AUTHORs---\nSubhransu Maji\nAndrea Vedaldi\nMircea Cimpoi",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Cimpoi_Deep_Filter_Banks_2015_CVPR_paper.pdf",
    "id": "Cimpoi_Deep_Filter_Banks_2015_CVPR_paper",
    "abstract": "Research in texture recognition often concentrates on the problem of material recognition in uncluttered conditions, an assumption rarely met by applications. In this work we conduct a first study of material and describable texture attributes recognition in clutter, using a new dataset derived from the OpenSurface texture repository. We propose a new texture descriptor, FV-CNN, obtained by Fisher Vector pooling of a Convolutional Neural Network (CNN) filter bank. FV-CNN substantially improves the state-of-the-art in texture, material and scene recognition, achieving 79.8% accuracy on Flickr material dataset and 81% accuracy on MIT indoor scenes. FV-CNN easily transfers across domains and can seamlessly incorporate multi-scale information and describe regions of arbitrary shapes and sizes. The approach is particularly suited at localizing “stuff” categories and obtains state-of-the-art results on MSRC segmentation dataset.",
    "topics": [
      "Texture recognition",
      "Convolutional Neural Networks (CNNs)",
      "Fisher Vector pooling",
      "Material recognition",
      "Image segmentation"
    ],
    "references": [
      {
        "citation": "[E. H. Adelson, \"On seeing stuff: The perception of materials by humans and machines,\" SPIE, 2001.]"
      },
      {
        "citation": "[D. G. Lowe, \"Object recognition from local scale-invariant features,\" Proc. ICCV, 1999.]"
      },
      {
        "citation": "[P. Arbeláez, J. Pont-Tuset, J. T. Barron, F. Marques, and J. Malik, \"Multiscale combinatorial grouping,\" CVPR, 2014.]"
      },
      {
        "citation": "[S. Bell, P. Upchurch, N. Snavely, and K. Bala, \"Opensurfaces: A richly annotated catalog of surface appearance,\" Proc. SIGGRAPH, 2013.]"
      },
      {
        "citation": "[M. Oquab, L. Bottou, I. Laptev, and J. Sivic, \"Learning and Transferring Mid-Level Image Representations using Convolutional Neural Networks,\" Proc. CVPR, 2014.]"
      },
      {
        "citation": "[K. Chatfield, K. Simonyan, A. Vedaldi, and A. Zisserman, \"Return of the devil in the details: Delving deep into convolutional nets,\" Proc. BMVC, 2014.]"
      },
      {
        "citation": "[M. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed, and A. Vedaldi, \"Describing textures in the wild,\" Proc. CVPR, 2014.]"
      },
      {
        "citation": "[K. J. Dana, B. van Ginneken, S. K. Nayar, and J. J. Koenderink, \"Reflectance and texture of real world surfaces,\" ACM Transactions on Graphics, 1999.]"
      },
      {
        "citation": "[A. Efroos and T. Leung, \"Texture synthesis by non-parametric sampling,\" CVPR, 1999.]"
      },
      {
        "citation": "[M. Everingham, A. Zisserman, C. Williams, and L. V. Gool, \"The PASUAL visual object classes challenge 2007 (VOC2007) results,\" Technical report, Pascal Challenge, 2007.]"
      }
    ],
    "author_details": [
      {
        "name": "Subhransu Maji",
        "affiliation": "University of Massachusetts, Amherst",
        "email": "smaji@cs.umass.edu"
      },
      {
        "name": "Andrea Vedaldi",
        "affiliation": "University of Oxford",
        "email": "vedaldi@robots.ox.ac.uk"
      },
      {
        "name": "Mircea Cimpoi",
        "affiliation": "University of Oxford",
        "email": "mircea@robots.ox.ac.uk"
      }
    ]
  },
  {
    "title": "More About VLAD: A Leap from Euclidean to Riemannian Manifolds\n---AUTHOR---\nMasoud Faraki\nMehrtasht T. Harandi\nFatih Porikli",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Faraki_More_About_VLAD_2015_CVPR_paper.pdf",
    "id": "Faraki_More_About_VLAD_2015_CVPR_paper",
    "abstract": "This paper extends the Vector of Locally Aggregated Descriptors (VLAD) to general curved Riemannian manifolds, introducing a novel approach called R-VLAD. It provides a mathematical framework for aggregating manifold data, specifically focusing on Region Covariance Descriptors and linear subspaces residing on the manifold of Symmetric Positive Deﬁnite matrices and the Grasmanian manifolds, respectively. The paper develops computationally efficient versions of R-VLAD using Stein and Jeffrey divergences on SPD matrices and projection distance on Grasmanian manifolds. Experimental validation demonstrates the superior performance of R-VLAD on tasks including video-based face recognition, dynamic scene recognition, and head pose classification.",
    "topics": [
      "Riemannian Manifolds",
      "Vector of Locally Aggregated Descriptors (VLAD)",
      "Region Covariance Descriptors",
      "Grasmannian Manifolds",
      "Computer Vision"
    ],
    "references": [],
    "author_details": [
      {
        "name": "Masoud Faraki",
        "affiliation": "College of Engineering and Computer Science, Australian National University, Australia; NICTA, Canberra Research Laboratory, Australia",
        "email": "masoud.faraki@nicta.com.au"
      },
      {
        "name": "Mehrtasht T. Harandi",
        "affiliation": "College of Engineering and Computer Science, Australian National University, Australia; NICTA, Canberra Research Laboratory, Australia",
        "email": "mehrtasht.harandi@nicta.com.au"
      },
      {
        "name": "Fatih Porikli",
        "affiliation": "College of Engineering and Computer Science, Australian National University, Australia; NICTA, Canberra Research Laboratory, Australia",
        "email": "fatih.porikli@nicta.com.au"
      }
    ]
  },
  {
    "title": "Discovering States and Transformations in Image Collections\n---AUTHOR---\nPhillip Isola\nJoseph J. Lim\nEdward H. Adelson",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Isola_Discovering_States_and_2015_CVPR_paper.pdf",
    "id": "Isola_Discovering_States_and_2015_CVPR_paper",
    "abstract": "This paper introduces a system that discovers states and transformations within collections of images. The system generalizes across object classes, enabling it to interpret image collections for entirely new object categories without prior training on those specific objects. By analyzing collections of images, the system infers the states an object can exist in (e.g., ripe, unripe, moldy) and the transformations between antonymic pairs of states (e.g., raw <-> cooked). The system explains image collections in terms of both unary states and transformations, and demonstrates how these transformations can be used to extract a continuum of images depicting the full range of the transformation.",
    "topics": [
      "Image collections",
      "Object states",
      "Image transformations",
      "Generalization across object classes",
      "Computer vision"
    ],
    "references": [],
    "author_details": [
      {
        "name": "Phillip Isola",
        "affiliation": "Massachusetts Institute of Technology",
        "email": "phillipi@mit.edu"
      },
      {
        "name": "Joseph J. Lim",
        "affiliation": "Massachusetts Institute of Technology",
        "email": "lim@cail.mit.edu"
      },
      {
        "name": "Edward H. Adelson",
        "affiliation": "Massachusetts Institute of Technology",
        "email": "adelson@cail.mit.edu"
      }
    ]
  },
  {
    "title": "Reﬂectance Hashing for Material Recognition\n---AUTHOR---\nHang Zhang\n---AUTHOR---\nKristin Dana\n---AUTHOR---\nKo Nishino",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Zhang_Reflectance_Hashing_for_2015_CVPR_paper.pdf",
    "id": "Zhang_Reflectance_Hashing_for_2015_CVPR_paper",
    "abstract": "We introduce a novel method for using reflectance to identify materials. Reflectance offers a unique signature of the material but is challenging to measure and use for recognizing materials due to its high-dimensionality. In this work, one-shot reflectance of a material surface which we refer to as a reflectance disk is capturing using a unique optical camera. The pixel coordinates of these reflectance disks correspond to the surface viewing angles. The reflectance has class-specific structure and angular gradients computed in this reflectance space reveal the material class. These reflectance disks encode discriminative information for efficient and accurate material recognition. We introduce a framework called reflectance hashing that models the reflectance disks with dictionary learning and binary hashing. We demonstrate the effectiveness of reflectance hashing for material recognition with a number of real-world materials.",
    "topics": [
      "Material Recognition",
      "Reflectance Hashing",
      "Optical Camera",
      "Dictionary Learning",
      "Binary Hashing"
    ],
    "references": [
      {
        "citation": "[Adelson, E. H. On seeing stuff: the perception of materials by humans and machines. In Photonics West 7001-Electronic Imaging, pages 1–12. International Society for Optics and Photonics, 2001.]"
      },
      {
        "citation": "[Bergamo, A., Torresani, L., & Fitzgibbon, A. Picodes: Learning a compact code for novel-category recognition. Advances in Neural Information Processing Systems 24, pages 2088–2096, 2011.]"
      },
      {
        "citation": "[Cula, O. G., & Dana, K. J. Recognition methods for 3d textured surfaces. Proceedings of SPIE Conference on Human Vision and Electronic Imaging VI, 4299:209–220, January 2001.]"
      },
      {
        "citation": "[Dana, K. J., & Wang, J. Device for convenient measurement of spatially varying bidirectional reﬂectance. Journal of the Optical Society of America A, 21:pp. 1–12, January 4004.]"
      },
      {
        "citation": "[Gong, Y., Kumar, S., Rowley, H. A., & Lazebnik, S. Learning binary codes for high-dimensional data using bilinear projections. IEEE Conference on Computer Vision and Pattern Recognition, pages 484–491, 2013.]"
      },
      {
        "citation": "[Ladicky, L., Shi, J., & Pollefeys, M. Pulling things out of perspective. In Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on, pages 89–96, June 2014.]"
      },
      {
        "citation": "[Leung, T., & Malik, J. Representing and recognizing the visual appearance of materials using three-dimensional tex-tons. International Journal of Computer Vision, 43(1):29–44, 2001.]"
      },
      {
        "citation": "[Liu, C., & Gu, J. Discrimative illumination: Per-pixel classification of raw materials based on optimal projections of spectral brdf. In Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on, pages 797–804. IEEE, 2012.]"
      },
      {
        "citation": "[Matusik, W., Pﬁster, H., Brand, M., & McMillan, L. A Data-Driven Reﬂectance Model. ACM Trans. on Graphics, 22(3):759–769, 2003.]"
      },
      {
        "citation": "[Torralba, A., Fergus, R., & Weiss, Y. Small codes and large image databases for recognition. In Computer Vision and Pattern Recognition, 2008. CVPR 2008. IEEE Conference on, pages 1–8, 2008.]"
      }
    ],
    "author_details": [
      {
        "name": "Hang Zhang",
        "affiliation": "Department of Electrical and Computer Engineering, Rutgers University, Piscataway, NJ 08854",
        "email": "zhang.hang@rutgers.edu"
      },
      {
        "name": "Kristin Dana",
        "affiliation": "Department of Electrical and Computer Engineering, Rutgers University, Piscataway, NJ 08854",
        "email": "kdana@ece.rutgers.edu"
      },
      {
        "name": "Ko Nishino",
        "affiliation": "Department of Computer Science, Drexel University, Philadelphia, PA 19104",
        "email": "kon@drexel.edu"
      }
    ]
  },
  {
    "title": "Learning Scene-Specific Pedestrian Detectors without Real Data\n---AUTHOR---\nHironori Hat tori\nVishnu Nare sh Boddeti\nKris Kitani\nTakeo Kanade",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Hattori_Learning_Scene-Specific_Pedestrian_2015_CVPR_paper.pdf",
    "id": "Hattori_Learning_Scene-Specific_Pedestrian_2015_CVPR_paper",
    "abstract": "We consider the problem of designing a scene-specific pedestrian detector in a scenario where we have zero instances of real pedestrian data. This scenario may arise when a new surveillance system is installed in a novel location and a scene-specific pedestrian detector must be trained prior to any observations of pedestrians. The key idea of our approach is to infer the potential appearance of pedestrians using geometric scene data and a customizable database of virtual simulations of pedestrian motion. We propose an efficient discriminative learning method that generates a spatially-varying pedestrian appearance model that takes into the account the perspective geometry of the scene. As a result, our method is able to learn a unique pedestrian classifier customized for every possible location in the scene. Our experimental results show that our proposed approach outperforms classical pedestrian detection models and hybrid synthetic-real models. Our results also yield a surprising result, that our method using purely synthetic data is able to outperform models trained on real scene-specific data when data is limited.\n\n---TOPICCS---\nScene-specific pedestrian detection\nSynthetic data generation\nData-free learning\nPerspective geometry\nAppearance model learning",
    "topics": [],
    "references": [
      {
        "citation": "[Agarwal, A., & Triggs, B. (2006). A local basis representation for estimating human pose from cluttered images. In ACCV.]"
      },
      {
        "citation": "[Athitsos, V., Wang, H., & Stefan, A. (2010). A database-based frame-work for gesture recognition. Personal and Ubiquitous Computing, 14(6):511–526.]"
      },
      {
        "citation": "[Benfold, B., & Reid, I. (2011). Stable multi-target tracking in real-time surveillance video. In CVPR.]"
      },
      {
        "citation": "[Bose, B., & Grimson, E. (2004). Improving object classiﬁcation in far-ﬁeld video. In CVPR.]"
      },
      {
        "citation": "[Dalal, N., & Triggs, B. (2005). Histograms of oriented gradients for human detection. In CVPR.]"
      },
      {
        "citation": "[Dollar, P., Wojek, C., Schiele, B., & Perona, P. (2012). Pedestrian detection: An evaluation of the state of the art. PAMI, 34(4):743–761.]"
      },
      {
        "citation": "[Felzenszwalb, P. F., Girshick, R. B., McAllester, D., & Ramanan, D. (2010). Object detection with discriminatively trained part based models. PAMI, 32(9):1627–1645.]"
      },
      {
        "citation": "[Girshick, R. B., Felzenszwalb, P. F., & McAllester, D. (n.d.). Discriminatively trained deformable part models, release 5. http://people.cs.uchicago.edu/˜rbg/latent-release5/]"
      },
      {
        "citation": "[Grauman, K., Shakhnarovich, G., & Darrell, T. (2003). Inferring 3d structure with a statistical image-based shape model. In ICCV.]"
      },
      {
        "citation": "[Henriques, J. F., Carreira, J., Caseiro, R., & Batista, J. (2013). Beyond hard negative mining: Efﬁcient detector learning via block-circulant decomposition. In ICCV.]"
      }
    ],
    "author_details": [
      {
        "name": "Hironori Hattori",
        "affiliation": "Sony Corporation",
        "email": "Hironori.Hat tori@jp.sony.com"
      },
      {
        "name": "Vishnu Nare sh Boddeti",
        "affiliation": "The Robotics Institute, Carnegie Mellon University",
        "email": "naresh@cmu.edu"
      },
      {
        "name": "Kris Kitani",
        "affiliation": "The Robotics Institute, Carnegie Mellon University",
        "email": "kkitani@cs.cmu.edu"
      },
      {
        "name": "Takeo Kanade",
        "affiliation": "The Robotics Institute, Carnegie Mellon University",
        "email": "tk@cs.cmu.edu"
      }
    ]
  },
  {
    "title": "Understanding Image Virality\n---AUTHOR---\nArturo Deza\nDevi Parikh",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Deza_Understanding_Image_Virality_2015_CVPR_paper.pdf",
    "id": "Deza_Understanding_Image_Virality_2015_CVPR_paper",
    "abstract": "This paper investigates the virality of online images, specifically from a computer vision perspective. The authors introduce three new image datasets from Reddit and define a virality score using Reddit metadata. They train classifiers to predict image virality, relative virality, and dominant topic, comparing computer performance to human performance. The study identifies key visual attributes correlating with virality and develops an attribute-based characterization for predicting relative viral-ity. Furthermore, it examines how human prediction of image virality is influenced by context. The datasets and annotations will be publicly available.\n\n---TOPICCS---\nImage Virality\nComputer Vision\nReddit Datasets\nVisual Attributes\nHuman-Computer Comparison",
    "topics": [],
    "references": [
      {
        "citation": "[Berger, J. (2011). Arousal increases social transmission of information. *Psychological science*, *22*(3), 339-342.]"
      },
      {
        "citation": "[Berger, J., & Milkman, K. L. (2012). What makes online content viral?. *Journal of Marketing Research*, *49*(2), 220-232.]"
      },
      {
        "citation": "[Berger, J., & Schwartz, E. M. (2011). What drives immediate and ongoing word of mouth?. *Journal of Marketing Research*, *48*(1), 23-37.]"
      },
      {
        "citation": "[Berg, A., Berg, T., Daume, H., Dodge, J., Goyal, A., Han, X., Mensch, A., Mitchell, M., Sood, A., Stratos, K., et al. (2012). Understanding and predicting importance in images. In *CVPR*.]"
      },
      {
        "citation": "[Isola, P., Parikh, D., Torralba, A., & Oliva, A. (2011). Understanding the intrinsic memorability of images. In *NIPS*.]"
      },
      {
        "citation": "[Guerini, M., Staiano, J., & Albanese, D. (2013). Exploring image virality in google plus. In *Social Computing (SocialCom), 2013 International Conference on*, pages 671–678. IEEE.]"
      },
      {
        "citation": "[Torralba, A., & Efroos, A. A. (2011). Unbiased look at dataset bias. In *Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on*, pages 1521–1528. IEEE.]"
      },
      {
        "citation": "[Torralba, A., Fergus, R., & Freeman, W. T. (2008). 80 million tiny images: A large data set for nonparametric object and scene recognition. *TPAMI*, *30*(11), 2011-2020.]"
      },
      {
        "citation": "[Oliva, A., & Torralba, A. (2001). Modeling the shape of the scene: A holistic representation of the spatial envelope. *IJCV*, *42*(3), 147-161.]"
      },
      {
        "citation": "[Xiao, J., Hays, J., Ehinger, K. A., Oliva, A., & Torralba, A. (2010). Sun database: Large-scale scene recognition from abbey to zoo. In *CVPR*.]"
      }
    ],
    "author_details": [
      {
        "name": "Arturo Deza",
        "affiliation": "UC Santa Barbara",
        "email": "deza@dyns.ucsb.edu"
      },
      {
        "name": "Devi Parikh",
        "affiliation": "Virginia Tech",
        "email": "parikh@vt.edu"
      }
    ]
  },
  {
    "title": "Semantics-Preserving Hashing for Cross-View Retrieval\n---AUTHOR---\nZijia Lin\nGuiguang Ding\nMingqing Hu\nJianmin Wang",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Lin_Semantics-Preserving_Hashing_for_2015_CVPR_paper.pdf",
    "id": "Lin_Semantics-Preserving_Hashing_for_2015_CVPR_paper",
    "abstract": "In this paper, we study the problem of cross-view retrieval for data with multiple views. We propose a supervised Semantics-Preserving Hashing method termed SePH. Given the semantic afﬁnities of training data as the supervised information, the proposed SePH first transforms them into a probability distribution and approximates it with to-be-learnt hash codes in Hamming space via minimizing the Kullback-Leibler divergence. Kernel logistic regression with a sampling strategy is utilized to learn the non-linear projections from features in each view to the learnt hash codes. For any unseen instance, predicted hash codes and their corresponding output probabilities from observed views are utilized to determine its uniﬁed hash code using a novel probabilistic approach. Extensive experiments conducted on three benchmark datasets well demonstrate the effectiveness and reasonableness of SePH.",
    "topics": [
      "Cross-view retrieval",
      "Hashing methods",
      "Semantics-preserving hashing",
      "Kullback-Leibler divergence",
      "Kernel logistic regression"
    ],
    "references": [
      {
        "citation": "[M. Bronstein, A. Bronstein, F. Michel, and N. Paragios. Data fusion through cross-modality metric learning using similarity-sensitive hashing. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2010.] - This paper introduces a data fusion technique using metric learning and similarity-sensitive hashing, a core concept in the paper's approach."
      },
      {
        "citation": "[T. Chua, J. Tang, R. Hong, H. Li, Z. Luo, and Y. Zheng. Nus-wide: A real-world web image database from national university of singapore. In Proceedings of the ACM International Conference on Image and Video Retrieval, 2009.] - This reference describes a relevant dataset (NUS-WIDE) often used for evaluating image retrieval methods, providing context for the paper's experimental setup."
      },
      {
        "citation": "[G. Ding, Y. Guo, and J. Zhou. Collective matrix factorization hashing for multimodal data. In 2014 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2014.] - This paper presents a related hashing technique using collective matrix factorization, highlighting a different approach to the problem."
      },
      {
        "citation": "[Y. Gong and S. Lazebnik. Iterative quantization: A procrustean approach to learning binary codes. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2011.] - This paper introduces a method for learning binary codes, a fundamental component of hashing techniques."
      },
      {
        "citation": "[K. He, F. Wen, and J. Sun. K-means hashing: An afinity-preserving quantization method for learning binary compact codes. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2013.] - This paper presents a specific hashing method (K-Means Hashing) that is relevant to the techniques explored in the paper."
      },
      {
        "citation": "[S. Kumar and R. Udupa. Learning hash functions for cross-view similarity search. In Proceedings of the Twenty-Second International Joint Conference on Artificial Intelligence, 2011.] - This paper addresses a related problem (cross-view similarity search) using hashing, providing a broader context."
      },
      {
        "citation": "[G. Lin, C. Shen, Q. Shi, A. Hengel, and D. Suter. Fast supervised hashing with decision trees for high-dimensional data. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2014.] - This paper introduces a supervised hashing method using decision trees, a relevant technique for comparison."
      },
      {
        "citation": "[W. Liu, J. Wang, R. Ji, Y. Jiang, and S. Chang. Supervised hashing with kernels. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2012.] - This paper explores supervised hashing using kernels, a related approach to the paper's methodology."
      },
      {
        "citation": "[J. Pereira, E. Coviello, G. Doyle, N. Rasiwasia, G. Lanckriet, R. Levy, and N. Vasconcelos. On the role of correlation and abstraction in cross-modal multimedia retrieval. Transactions of Pattern Analysis and Machine Intelligence, 36(3):521–535, 2014.] - This paper discusses the theoretical foundations of cross-modal retrieval, providing a broader context for the paper's work."
      },
      {
        "citation": "[F. Shen, C. Shen, Q. Shi, A. Hengel, and Z. Tang. Inductive hashing on manifolds. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2013.] - This paper presents a hashing method that operates on manifolds, a relevant technique for handling complex data structures."
      }
    ],
    "author_details": [
      {
        "name": "Zijia Lin",
        "affiliation": "Department of Computer Science and Technology, Tsinghua University, Beijing, P.R.China",
        "email": "[Email not available]"
      },
      {
        "name": "Guiguang Ding",
        "affiliation": "School of Software, Tsinghua University, Beijing, P.R.China",
        "email": "[Email not available]"
      },
      {
        "name": "Mingqing Hu",
        "affiliation": "Institute of Computing Technology, Chinese Academy of Sciences, Beijing, P.R.China",
        "email": "[Email not available]"
      },
      {
        "name": "Jianmin Wang",
        "affiliation": "School of Software, Tsinghua University, Beijing, P.R.China",
        "email": "[Email not available]"
      }
    ]
  },
  {
    "title": "A Dynamic Convolutional Layer for Short Range Weather Prediction\n---AUTHOR---\nBenjamin Klein\nLior Wolf\nYehuda Afek",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Klein_A_Dynamic_Convolutional_2015_CVPR_paper.pdf",
    "id": "Klein_A_Dynamic_Convolutional_2015_CVPR_paper",
    "abstract": "We present a new deep network layer called the “Dynamic Convolutional Layer” which is a generalization of the convolutional layer. The conventional convolutional layer uses filters that are learned during training and are held constant during testing. In contrast, the dynamic convolutional layer uses filters that will vary from input to input during testing. This is achieved by learning a function that maps the input to the filters. We apply the dynamic convolutional layer to the application of short range weather prediction and show performance improvements compared to other baselines.\n\n---TOPIC---\nDynamic Convolutional Layer\nConvolutional Neural Networks (CNNs)\nShort-Range Weather Prediction\nRadar Image Processing\nDeep Learning",
    "topics": [],
    "references": [
      {
        "citation": "[Alain, D., & Olivier, S. (2013). Gated autoencoders with tied input weights. *Proceedings of the 30th International Conference on Machine Learning (ICML-13)*, 154–162.]"
      },
      {
        "citation": "[Alvarez, J. M., LeCun, Y., Gevers, T., & Lopez, A. M. (2012). Semantic road segmentation via multi-scale ensembles of learned features. *Proceedings of the 12th International Conference on Computer Vision - Volume 2*, 586–595.]"
      },
      {
        "citation": "[Dong, C., Loy, C., He, K., & Tang, X. (2014). Learning a deep convolutional network for image super-resolution. In *Computer Vision ECCV 2014*, 184–199.]"
      },
      {
        "citation": "[Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). ImageNet classification with deep convolutional neural networks. *Advances in neural information processing systems*, 1097–1105.]"
      },
      {
        "citation": "[LeCun, Y., Bottou, L., Bengio, Y., & Haffner, P. (1998). Gradient-based learning applied to document recognition. *Proceedings of the IEEE*, 86(11), 2278–2324.]"
      },
      {
        "citation": "[Rumelhart, D. E., Hinton, G. E., & Wilson, R. J. (1986). Learning representations by back-propagating errors. *Parallel distributed processing: Explorations in the microstructure of cognition*, 533–536.]"
      },
      {
        "citation": "[Schuler, C. J., Hirsch, M., Harmeling, S., & Schölkopf, B. (2014). Learning to deblur. *arXiv preprint arXiv:1406.7444*.]"
      },
      {
        "citation": "[Sermanet, P., Eigen, D., Zhang, X., Mathieu, M., Fergus, R., & LeCun, Y. (2013). Overfeat: Integrated recognition, localization and detection using convolutional networks. *arXiv preprint arXiv:1312.6229*.]"
      },
      {
        "citation": "[Taigman, Y., Yang, M., Ranzato, M., & Wolf, L. (2014). Deepface: Closing the gap to human-level performance in face verification. *Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on*, 1701–1708.]"
      },
      {
        "citation": "[Glorot, X., & Bengio, Y. (2010). Understanding the difficulty of training deep feedforward neural networks. *International Conference on Artificial Intelligence and Statistics*, 249–256.]"
      }
    ],
    "author_details": [
      {
        "name": "Benjamin Klein",
        "affiliation": "Tel Aviv University",
        "email": "beni.klein@gmail.com"
      },
      {
        "name": "Lior Wolf",
        "affiliation": "Tel Aviv University",
        "email": "wolf@cs.tau.ac.il"
      },
      {
        "name": "Yehuda Afek",
        "affiliation": "Tel Aviv University",
        "email": "afek@post.tau.ac.il"
      }
    ]
  },
  {
    "title": "Robust Manhattan Frame Estimation from a Single RGB-D Image\n---AUTHOR---\nBernard Ghanem\nAli Thabet\nJuan Carlos Niebles\nFabian Caba Heilbron",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Ghanem_Robust_Manhattan_Frame_2015_CVPR_paper.pdf",
    "id": "Ghanem_Robust_Manhattan_Frame_2015_CVPR_paper",
    "abstract": "This paper proposes a new framework for estimating the Manhattan Frame (MF) of an indoor scene from a single RGB-D image. The method formulates the problem as estimating a rotation matrix that best aligns the normals of the captured scene to a canonical world axes. By introducing sparsity constraints, the method simultaneously estimates the scene MF, the surfaces aligned to coordinate axes, and outlier surfaces. A new benchmark with ground truth MFs for the NYUv2 dataset is introduced for evaluation. Experimental results demonstrate that the proposed method is more accurate, faster, more reliable, and more robust than existing techniques, and can be used to improve RGB-D Simultaneous Localization and Mapping (SLAM) performance. A framework for assessing MF estimation methods is also presented, along with publicly available code and results.\n\n---TOPIC---\nManhattan Frame Estimation\nRGB-D Image Processing\nRotation Matrix Optimization\nSparsity Constraints\nRGB-D SLAM",
    "topics": [],
    "references": [
      {
        "citation": "[15] L. D. Pero, J. Guan, E. Brau, J. Schlect, and K. Barnard. Sampling bedrooms. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, 2011. (Focuses on a specific room type, potentially relevant for scene understanding)"
      },
      {
        "citation": "[17] A. G. Schwing, T. Hazan, M. Pollefeys, and R. Urtasun. Efficient structured prediction for 3d indoor scene understanding. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, 2012. (Addresses the core problem of 3D scene understanding)"
      },
      {
        "citation": "[18] A. G. Schwing and R. Urtrasun. Efficient exact inference for 3d indoor scene understanding. In Proceedings of European Conference on Computer Vision, 2012. (Related to the previous reference, focusing on inference methods)"
      },
      {
        "citation": "[25] J. Zhang, C. Kan, A. G. Schwing, and R. Urtrasun. Estimating the 3d layout of indoor scenes and its clutter from depth sensors. In Proceedings of the IEEE International Conference on Computer Vision, 2013. (Directly addresses 3D layout estimation)"
      },
      {
        "citation": "[4] J. M. Coughlan and A. L. Yuille. Manhattan world: Compass direction from a single image by Bayesian inference. In Proceedings of the IEEE International Conference on Computer Vision, pages 941–947, 1999. (A foundational work on understanding room orientation)"
      },
      {
        "citation": "[9] V. Hedau, D. Hoiem, and D. A. Forsyth. Recovering the spatial layout of cluttered rooms. In Proceedings of the IEEE International Conference on Computer Vision, 2009. (Addresses the challenge of cluttered scenes)"
      },
      {
        "citation": "[10] V. Hedau, D. Hoiem, and D. A. Forsyth. Thinking inside the box: Using appearance models and context based on room geometry. In Proceedings of European Conference on Computer Vision, 2010. (Builds on the previous reference, incorporating appearance models)"
      },
      {
        "citation": "[5] F. Endres, J. Hess, N. Engelhard, J. Sturm, D. Cremers, and W. Burgard. An evaluation of the rgb-d slam system. In Proceedings of the IEEE International Conference on Robotics and Automation, 2012. (Provides context and evaluation of RGB-D systems, crucial for many scene understanding approaches)"
      },
      {
        "citation": "[12] D. C. Lee, M. Hebert, and T. Kanade. Geometric reasoning for single image structure recovery. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, 2009. (Focuses on geometric reasoning, a key aspect of scene understanding)"
      },
      {
        "citation": "[24] J. Yang, K. Yu, Y. Gong, and T. Huang. Linear spatial pyramid matching using sparse coding for image classification. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, 2009. (Sparse coding is a common technique used in many vision tasks, including scene understanding)"
      }
    ],
    "author_details": [
      {
        "name": "Bernard Ghanem",
        "affiliation": "King Abdullah University of Science and Technology (KAUST), Saudi Arabia",
        "email": "[Email not available in the provided text]"
      },
      {
        "name": "Ali Thabet",
        "affiliation": "King Abdullah University of Science and Technology (KAUST), Saudi Arabia",
        "email": "[Email not available in the provided text]"
      },
      {
        "name": "Juan Carlos Niebles",
        "affiliation": "Universidad del Norte, Colombia",
        "email": "[Email not available in the provided text]"
      },
      {
        "name": "Fabian Caba Heilbron",
        "affiliation": "King Abdullah University of Science and Technology (KAUST), Saudi Arabia",
        "email": "[Email not available in the provided text]"
      }
    ]
  },
  {
    "title": "Learning Descriptors for Object Recognition and 3D Pose Estimation\n---AUTHOR---\nPaul Wohlhart\n---AUTHOR---\nVincent Lepetit",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Wohlhart_Learning_Descriptors_for_2015_CVPR_supplemental.pdf",
    "id": "Wohlhart_Learning_Descriptors_for_2015_CVPR_supplemental",
    "abstract": "This paper focuses on learning descriptors for object recognition and 3D pose estimation. The authors present a novel approach that utilizes a triplet cost function designed to be scale-invariant, allowing for the definition of triplets over arbitrary ranges within the template dome. They contrast their definition with existing methods, highlighting the advantages of their scale-invariant approach. The paper includes supplementary material showcasing retrieved templates and a detailed comparison of different triplet cost function definitions.\n\n---TOPICKS---\nObject Recognition\n3D Pose Estimation\nDescriptor Learning\nTriplet Cost Function\nScale-Invariant Learning",
    "topics": [],
    "references": [],
    "author_details": [
      {
        "name": "Paul Wohlhart",
        "affiliation": "Institute for Computer Vision and Graphics, Graz University of Technology, Austria",
        "email": "wohlhart@icg.tugraz.at"
      },
      {
        "name": "Vincent Lepetit",
        "affiliation": "Institute for Computer Vision and Graphics, Graz University of Technology, Austria",
        "email": "lepetit@icg.tugraz.at"
      }
    ]
  },
  {
    "title": "An Approximate Shading Model for Object Relighting\n---AUTHOR---\nKevin Karsch\nDavid Forsyth\nZicheng Liao",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Liao_An_Approximate_Shading_2015_CVPR_paper.pdf",
    "id": "Liao_An_Approximate_Shading_2015_CVPR_paper",
    "abstract": "We propose an approximate shading model for image-based object modeling and insertion. Our approach is a hybrid of 3D rendering and image-based composition. It avoids the difficulties of physically accurate shape estimation from a single image, and allows for more flexible image composition than pure image-based methods. The model decomposes the shading field into (a) a rough shape term that can be reshaded, (b) a parametric shading detail that encodes missing features from the first term, and (c) a geometric detail term that captures fine-scale material properties. With this object model, we build an object relighting system that allows an artist to select an object from an image and insert it into a 3D scene. Through simple interactions, the system can adjust illumination on the inserted object so that it appears more naturally in the scene. Our quantitative evaluation and extensive user study suggest our method is a promising alternative to existing methods of object insertion.",
    "topics": [
      "Approximate Shading Model",
      "Image-Based Object Modeling",
      "Hybrid Rendering Techniques",
      "Object Relighting",
      "Shape Estimation"
    ],
    "references": [
      {
        "citation": "[A. Agarwala, M. Dontcheva, M. Agrawala, S. Drucker, A. Colburn, B. Curless, D. Salesin, and M. Cohen. Interactive digital photomontage. ACM Trans. Graph., 23(3):294–302, 2004.]"
      },
      {
        "citation": "[J. T. Barron and J. Malik. Color constancy, intrinsic images, and shape estimation. ECCV, 2012.]"
      },
      {
        "citation": "[H. Barrow and J. Tenenbaum. Recovering intrinsic scene characteristics from images. In Comp. Vision Sys., pages 3–26, 1978.]"
      },
      {
        "citation": "[R. Basri and D. Jacobs. Lambertian reflectance and linear subspaces. PAMI, 2003.]"
      },
      {
        "citation": "[P. N. Belhumeur and D. J. Kriegman. What is the set of images of an object under all possible illumination conditions? IJCV, 1998.]"
      },
      {
        "citation": "[P. Burt and E. Adelson. The laplacian pyramid as a compact image code. Communications, IEEE Transactions on, 31(4):532–540, 1983.]"
      },
      {
        "citation": "[P. Cavanagh. The artist as neuroscientist. Nature, pages 301–307, 2005.]"
      },
      {
        "citation": "[T. Chen, M.-M. Cheng, P. Tan, A. Shamir, and S.-M. Hu. Sketch2photo: internet image montage. ACM Trans. Graph., 28(5):124:1–124:10, Dec. 2009.]"
      },
      {
        "citation": "[P. Debevec. Rendering synthetic objects into real scenes: bridging traditional and image-based graphics with global illumination and high dynamic range photography. In SIGGRAPH’98, pages 189–198. ACM, 1998.]"
      },
      {
        "citation": "[J.-D. Durou, M. Falcone, and M. Sagona. Numerical methods for shape-from-shading: A new survey with benchmarks. Comput. Vis. Image Underst., 109(1):22–43, 2008.]"
      },
      {
        "citation": "---"
      }
    ],
    "author_details": [
      {
        "name": "Kevin Karsch",
        "affiliation": "Zhejiang University",
        "email": "Not available in the provided text."
      },
      {
        "name": "David Forsyth",
        "affiliation": "University of Illinois at Urbana-Champaign",
        "email": "Not available in the provided text."
      },
      {
        "name": "Zicheng Liao",
        "affiliation": "Zhejiang University, University of Illinois at Urbana-Champaign",
        "email": "Not available in the provided text."
      }
    ]
  },
  {
    "title": "Clique-graph Matching by Preserving Global & Local Structure\n---AUTHOR---\nWei-Zhi Nie\nAn-An Liu\nZan Gao\nYu-Ting Su",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Nie_Clique-Graph_Matching_by_2015_CVPR_paper.pdf",
    "id": "Nie_Clique-Graph_Matching_by_2015_CVPR_paper",
    "abstract": "Previous approaches to clique-graph matching primarily focus on node-to-node mapping using first and second-order attributes, which struggle to preserve both global and local structure. This paper introduces a novel clique-graph matching method that addresses this limitation by preserving both global and local structures. The method formulates an objective function with respect to latent variables representing clique information and pairwise clique correspondence. Due to the non-convex nature of the objective function, it is decomposed into two consecutive steps: clique-to-clique similarity measure preserving local unary and pairwise correspondences, and graph-to-graph similarity measure preserving global clique-to-clique correspondence. Experiments on synthetic and real-world data demonstrate the method's superior performance, especially in the presence of noise and outliers. The paper also introduces the concept of a \"clique-graph\" itself, replacing individual nodes with cliques of nearest neighbors to capture local structural attributes.\n\n---TOPICICS---\nClique-graph Matching\nLocal and Global Structure Preservation\nLatent Variable Optimization\nGraph/Hypergraph Matching\nComputer Vision",
    "topics": [],
    "references": [
      {
        "citation": "[Barducci, A., & Marinai, S. Object recognition in ﬂoor plans by graphs of white connected components. ICPR, 2012.]"
      },
      {
        "citation": "[Cho, M., Lee, J., & Lee, K. M. Reweighted random walks for graph matching. ECCV, 2010.]"
      },
      {
        "citation": "[Cour, T., Srinivasan, P., & Shi, J. Balanced graph matching. NIPS, 2006.]"
      },
      {
        "citation": "[Duchenne, O., Bach, F., Kweon, I., & Ponce, J. A tensor-based algorithm for high-order graph matching. IEEE Trans. Pattern Anal. Mach. Intell., 2011.]"
      },
      {
        "citation": "[Gao, Y., & Dai, Q. View-based 3d object retrieval: Challenges and approaches. IEEE MultiMedia, 2014.]"
      },
      {
        "citation": "[Gao, Y., Wang, M., Tao, D., Ji, R., & Dai, Q. 3-d object retrieval and recognition with hypergraph analysis. TIP, 2012.]"
      },
      {
        "citation": "[Lee, J., Cho, M., & Lee, K. M. Hyper-graph matching via reweighted random walks. CVPR, 2011.]"
      },
      {
        "citation": "[Leordeanu, M., & Hebert, M. A spectral technique for correspondence problems using pairwise constraints. ICCV, 2005.]"
      },
      {
        "citation": "[Wang, C., Wang, L., & Liu, L. Improving graph matching via density maximization. ICCV, 2013.]"
      },
      {
        "citation": "[Zass, R., & Shashua, A. Probablistic graph and hypergraph matching. CVPR, 2008.]"
      }
    ],
    "author_details": [
      {
        "name": "Wei-Zhi Nie",
        "affiliation": "School of Electronics Information Engineering, Tianjin University, China",
        "email": "Not available"
      },
      {
        "name": "An-An Liu",
        "affiliation": "School of Electronics Information Engineering, Tianjin University, China",
        "email": "anan0422@gmail.com"
      },
      {
        "name": "Zan Gao",
        "affiliation": "School of Computer & Communication Engineering, Tianjin University of Technology, China",
        "email": "Not available"
      },
      {
        "name": "Yu-Ting Su",
        "affiliation": "School of Electronics Information Engineering, Tianjin University, China",
        "email": "Not available"
      }
    ]
  },
  {
    "title": "Building a bird recognition app and large scale dataset with citizen scientists: The ﬁne print in fine-grained dataset collection\n---AUTHOR---\nGrant Van Horn\nSteve Branson\nRyan Farrell\nScott Haber\nJessie Barry\nPanos Ipeirotis\nPietro Perona\nSerge Belongie",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Horn_Building_a_Bird_2015_CVPR_paper.pdf",
    "id": "Horn_Building_a_Bird_2015_CVPR_paper",
    "abstract": "This paper introduces tools and methodologies to collect high-quality, large-scale fine-grained computer vision datasets using citizen scientists – crowd annotators who are passionate and knowledgeable about specific domains. The authors introduce NABirds, a new dataset containing 48,562 images of North American birds with 555 categories, part annotations, and bounding boxes. They find that citizen scientists are significantly more accurate than Mechanical Turkers at zero cost. The paper also analyzes the quality of existing datasets and demonstrates the utility of a high-quality test set like NABirds for accurately measuring the performance of fine-grained computer vision systems, culminating in a publicly available bird recognition service.\n\n---TOPICICS---\nCitizen Science\nFine-Grained Image Classification\nDataset Creation\nBird Recognition\nAnnotation Quality",
    "topics": [],
    "references": [
      {
        "citation": "[Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). ImageNet classification with deep convolutional neural networks. *NIPS*.]"
      },
      {
        "citation": "[Fei-Fei, L., Fergus, R., & Perona, P. (2006). One-shot learning of object categories. *Pattern Analysis and Machine Intelligence, IEEE Transactions on, 28*(4), 594–611.]"
      },
      {
        "citation": "[Deng, J., Krause, J., & Fei-Fei, L. (2013). Fine-grained crowd sourcing for fine-grained recognition. *Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on*, 580–587.]"
      },
      {
        "citation": "[Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., & Fei-Fei, L. (2009). ImageNet: A Large-Scale Hierarchical Image Database. *CVPR09*.]"
      },
      {
        "citation": "[Berg, T., Liu, J., Lee, S. W., Alexander, M. L., Jacobs, D. W., & Belhumeur, P. N. (2014). Birdsnap: Large-Scale Fine-Grained Visual Categorization of Birds. *2014 IEEE Conference on Computer Vision and Pattern Recognition*, 2019–2026.]"
      },
      {
        "citation": "[Girshick, R., Donahue, J., Darrell, T., & Malik, J. (2013). Rich feature hierarchies for accurate object detection and semantic segmentation. *arXiv preprint arXiv:1311.2524*.]"
      },
      {
        "citation": "[Ipeirotis, P. G., & Gabrilovich, E. (2014). Quizz: targeted crowdsourcing with a billion (potential) users. *pages 143–154, Apr.*]"
      },
      {
        "citation": "[Khosla, A., Jayadevaprakash, N., Yao, B., & Fei-Fei, L. (2011). Novel dataset for fine-grained image categorization. *First Workshop on Fine-Grained Visual Categorization, IEEE Conference on Computer Vision and Pattern Recognition*, Colorado Springs, CO.]"
      },
      {
        "citation": "[Maji, S., Kannala, J., Rahtu, E., Blaschko, M., & Vedaldi, A. (2013). Fine-grained visual classification of aircraft. *Technical report*.]"
      },
      {
        "citation": "[Von Ahn, L. (2006). Games with a purpose. *Computer, 39*(6), 92–94.]"
      }
    ],
    "author_details": [
      {
        "name": "Grant Van Horn",
        "affiliation": "Caltech",
        "email": "Not available"
      },
      {
        "name": "Steve Branson",
        "affiliation": "Caltech",
        "email": "Not available"
      },
      {
        "name": "Ryan Farrell",
        "affiliation": "BYU",
        "email": "Not available"
      },
      {
        "name": "Scott Haber",
        "affiliation": "Cornell Lab of Ornithology",
        "email": "Not available"
      },
      {
        "name": "Jessie Barry",
        "affiliation": "Cornell Lab of Ornithology",
        "email": "Not available"
      },
      {
        "name": "Panos Ipeirotis",
        "affiliation": "NYU",
        "email": "Not available"
      },
      {
        "name": "Pietro Perona",
        "affiliation": "Caltech",
        "email": "Not available"
      },
      {
        "name": "Serge Belongie",
        "affiliation": "Cornell Tech",
        "email": "Not available"
      }
    ]
  },
  {
    "title": "Saliency Detection via Cellular Automata\n---AUTHOR---\nYao Qin\nHuchuan Lu\nYiqun Xu\nHe Wang",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Qin_Saliency_Detection_via_2015_CVPR_paper.pdf",
    "id": "Qin_Saliency_Detection_via_2015_CVPR_paper",
    "abstract": "This paper introduces a novel approach to salient object detection using Cellular Automata (CA). The method addresses limitations of existing techniques that often rely on image boundaries as background seeds, which can lead to inaccuracies when objects appear on the boundary. The proposed algorithm constructs a background-based map, utilizes a novel CA-based propagation mechanism to exploit regional similarity, and balances influential power through impact and coherence matrices. A Bayesian framework integrates multiple salience maps. Experiments on public datasets demonstrate superior performance compared to state-of-the-art methods. Key contributions include an efficient algorithm for integrating global distance matrices, the use of single-layer CA to optimize existing methods, and a multi-layer CA for integrating multiple salience maps.\n\n---TOPICCS---\nSalience Detection\nCellular Automata\nImage Processing\nBayesian Framework\nPropagation Mechanism",
    "topics": [],
    "references": [
      {
        "citation": "[Achanta, R., Hemami, S., Estrada, F., & Susstrunk, S. (2009). Frequency-tuned salient region detection. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on (pp. 1597–1604). IEEE.]"
      },
      {
        "citation": "[Alexe, B., Deselaers, T., & Ferrari, V. (2010). What is an object?. In Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on (pp. 73–80). IEEE.]"
      },
      {
        "citation": "[Batty, M. (2007). Cities and complexity: understanding cities with cellular automata, agent-based models, and fractals. The MIT press.]"
      },
      {
        "citation": "[Cheng, M.-M., Warrell, J., Lin, W.-Y., Zheng, S., Vineet, V., & Crook, N. (2013). Efficient salient region detection with soft image abstraction. In Computer Vision (ICCV), 2013 IEEE International Conference on (pp. 1529–1536). IEEE.]"
      },
      {
        "citation": "[Cheng, M.-M., Zhang, G.-X., Mitra, N. J., Huang, X., & Hu, S.-M. (2011). Global contrast based salient region detection. In Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on (pp. 409–416). IEEE.]"
      },
      {
        "citation": "[Chopard, B., & Droz, M. (1998). Cellular automata modeling of physical systems. Cambridge University Press Cambridge.]"
      },
      {
        "citation": "[Ding, Y., Xiao, J., & Yu, J. (2011). Importance filtering for image retargeting. In Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on (pp. 89–96). IEEE.]"
      },
      {
        "citation": "[Everingham, M., Van Gool, L., Williams, C. K., Winn, J., & Zisserman, A. (2010). The pascal visual object classes (voc) challenge. International journal of computer vision, 88(2), 303–338.]"
      },
      {
        "citation": "[Goferman, S., & Tal, A. L. (2010). Context-aware saliency detection. Computer, 2010.]"
      },
      {
        "citation": "[Goferman, S., Zelnik-Manor, L., & Tal, A. (2012). Context-aware saliency detection. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 34(10), 1915–1926.]"
      }
    ],
    "author_details": [
      {
        "name": "Yao Qin",
        "affiliation": "Dalian University of Technology",
        "email": "*Not available in the provided text*"
      },
      {
        "name": "Huchuan Lu",
        "affiliation": "Dalian University of Technology",
        "email": "*Not available in the provided text*"
      },
      {
        "name": "Yiqun Xu",
        "affiliation": "Dalian University of Technology",
        "email": "*Not available in the provided text*"
      },
      {
        "name": "He Wang",
        "affiliation": "Dalian University of Technology",
        "email": "*Not available in the provided text*"
      }
    ]
  },
  {
    "title": "FaceNet: A Uniﬁed Embedding for Face Recognition and Clustering\n---AUTHOR---\nFlorian Schroff\nDmitry Kalenichenko\nJames Philbin",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Schroff_FaceNet_A_Unified_2015_CVPR_paper.pdf",
    "id": "Schroff_FaceNet_A_Unified_2015_CVPR_paper",
    "abstract": "Despite significant recent advances in the field of face recognition, implementing face verification and recognition efficiently at scale presents serious challenges to current approaches. In this paper we present a system, called FaceNet, that directly learns a mapping from face images to a compact Euclidean space where distances directly correspond to a measure of face similarity. Once this space has been produced, tasks such as face recognition, verification and clustering can be easily implemented using standard techniques with FaceNet embeddings as feature vectors. Our method uses a deep convolutional network trained to directly optimize the embedding itself, rather than an intermediate bottleneck layer as in previous deep learning approaches. We use triplets of roughly aligned matching / non-matching face patches generated using a novel online triplet mining method. The benefit of our approach is much greater representational efﬁcienicy: we achieve state-of-the-art face recognition performance using only 128-bytes per face. On the widely used Labeled Faces in the Wild (LFW) dataset, our system achieves a new record accuracy of 99.63%. On YouTube Faces DB it achieves 95.12%. Our system cuts the error rate in comparison to the best published result by 30% on both datasets.\n\n---TOPIC---\nFace Recognition\nFace Verification\nDeep Convolutional Networks\nTriplet Loss\nEmbedding Space",
    "topics": [],
    "references": [
      {
        "citation": "[Y. Bengio, J. Louradour, R. Collobert, and J. Weston. Curriculum learning. In Proc. of ICML, 2009.] - Seems foundational to a learning approach."
      },
      {
        "citation": "[D. E. Rumelhart, G. E. Hinton, and R. J. Williams. Learning representations by back-propagating errors. Nature, 1986.] - A seminal paper on backpropagation, a core concept in deep learning."
      },
      {
        "citation": "[Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel. Backpropagation applied to handwritten zip code recognition. Neural Computation, 1989.] - Early application of backpropagation."
      },
      {
        "citation": "[M. Schultz and T. Joachims. Learning a distance metric from relative comparisons. In NIPS, 2004.] - Relevant to face verification and representation learning."
      },
      {
        "citation": "[G. B. Huang, M. Ramesh, T. Berg, and E. Learned-Miller. Labeled faces in the wild. Technical Report 07-49, University of Massachusetts, Amherst, 2007.] - A key dataset for face recognition."
      },
      {
        "citation": "[J. Dean, G. Corrado, R. Monga, K. Chen, M. Devin, M. Mao, M. Ranzato, A. Senior, P. Tucker, K. Yang, Q. V. Le, and A. Y. Ng. Large scale distributed deep networks. In NIPS, 2012.] - Important for understanding the scaling of deep learning."
      },
      {
        "citation": "[M. D. Zeiler and R. Fergus. Visualizing and understanding convolutional networks. CoRR, 2013.] - Crucial for interpreting and debugging deep learning models."
      },
      {
        "citation": "[Y. Sun, X. Wang, and X. Tang. Deep learning face representation by joint identiﬁcation-veriﬁcation. CoRR, 2014.] - Directly relevant to the paper's focus."
      },
      {
        "citation": "[C. Lu and X. Tang. Surpassing human-level face verification performance on LFW with gausssianface. CoRR, 2014.] - Demonstrates state-of-the-art results."
      },
      {
        "citation": "[M. Lin, Q. Chen, and S. Yan. Network in network. CoRR, 2013.] - Introduces a specific network architecture."
      }
    ],
    "author_details": [
      {
        "name": "Florian Schroff",
        "affiliation": "Google Inc.",
        "email": "fschroff@google.com"
      },
      {
        "name": "Dmitry Kalenichenko",
        "affiliation": "Google Inc.",
        "email": "dkalenichenko@google.com"
      },
      {
        "name": "James Philbin",
        "affiliation": "Google Inc.",
        "email": "jphilbin@google.com"
      }
    ]
  },
  {
    "title": "Global Refinement of Random Forest\n---AUTHORs---\nShaoqing Ren\nXudong Cao\nYichen Wei\nJian Sun",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Ren_Global_Refinement_of_2015_CVPR_paper.pdf",
    "id": "Ren_Global_Refinement_of_2015_CVPR_paper",
    "abstract": "Random forest is a well-known and successful learning method, but it suffers from drawbacks such as a heuristic learning rule that doesn't effectively minimize global training loss and a large model size. This paper proposes two techniques, global refinement and global pruning, to improve a pre-trained random forest. Global refinement jointly relearns the leaf nodes of all trees under a global objective function to exploit complementary information, significantly enhancing fitting power. Global pruning reduces model size and overfitting risk. Experiments demonstrate improved performance and reduced storage cost with the refined model.",
    "topics": [
      "Random Forest",
      "Global Refinement",
      "Global Pruning",
      "Model Optimization",
      "Leaf Node Relearning"
    ],
    "references": [
      {
        "citation": "[Breiman, L. (2001). Random forests. *Machine learning*, *45*(1), 5–32.] - This is a foundational paper on Random Forests, frequently referenced and crucial for understanding the context of the paper."
      },
      {
        "citation": "[Bernard, S., Adam, S., & Heutte, S. (2012). Dynamic random forest-s. *Pattern Recognition Letters*, *33*(12), 1580–1586.] - Directly addresses the dynamic aspects of Random Forests, a key focus of the paper."
      },
      {
        "citation": "[Fanelli, G., Gall, J., & Van Gool, L. (2011). Real time head pose estimation with random regression forests. *Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on*, 617–624.] - Demonstrates a practical application of Random Forests in a real-time scenario."
      },
      {
        "citation": "[Schulter, S., Leistner, C., Wohlhart, P., Roth, P. M., & Bischof, H. (2013). Alternating regression forests for object detection and pose estimation. *Proc. International Conference on Computer Vision*,] - Explores a specific variation of Random Forests for object detection and pose estimation."
      },
      {
        "citation": "[Friedman, J. H. (2001). Greedy function approximation: a gradient boosting machine. *Annals of Statistics*, 1189–1232.] - Provides background on related boosting techniques."
      },
      {
        "citation": "[Kontschieder, P., Kohli, P., Shotton, J., & Criminisi, A. (2013). Geof: Geodesic forests for learning coupled predictors. *Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on*, 65–72.] - Introduces a specific forest architecture."
      },
      {
        "citation": "[Quinlan, J. R. (1986). Induction of decision trees. *Machine learning*, *1*(1), 81–106.] - A seminal work on decision trees, providing context for Random Forests."
      },
      {
        "citation": "[Vapnik, V. (2000). *The nature of statistical learning theory*. Springer.] - Provides theoretical background on statistical learning."
      },
      {
        "citation": "[Robnik-ˇSikonja, M. (2004). Improving random forests. *Machine Learning*, 359–370.] - Discusses methods for improving Random Forest performance."
      },
      {
        "citation": "[Bernard, S., Heutte, L., & Adam, S. (2008). Forest-rk: A new random forest induction method. In *Advanced Intelligent Computing Theories and Applications. With Aspects of Artificial Intelligence*, 430–437.] - Introduces a specific Random Forest induction method."
      }
    ],
    "author_details": [
      {
        "name": "Shaoqing Ren",
        "affiliation": "University of Science and Technology of China",
        "email": "sqren@mail.ustc.edu.cn"
      },
      {
        "name": "Xudong Cao",
        "affiliation": "Microsoft Research",
        "email": "xudongca@microsoft.com"
      },
      {
        "name": "Yichen Wei",
        "affiliation": "Microsoft Research",
        "email": "yichenw@microsoft.com"
      },
      {
        "name": "Jian Sun",
        "affiliation": "Microsoft Research",
        "email": "jiansun@microsoft.com"
      }
    ]
  },
  {
    "title": "Separating Objects and Clutter in Indoor Scenes\n---AUTHOR---\nS. H. Khan\nXuming He\nM. Bannamoun\nF. Sohel\nR. Togneri",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Khan_Separating_Objects_and_2015_CVPR_supplemental.pdf",
    "id": "Khan_Separating_Objects_and_2015_CVPR_supplemental",
    "abstract": "This document comprises supplementary material accompanying the paper \"Separating objects and clutter in indoor scenes\" [1]. It details the formulation of the problem as a Mixed Integer Linear Program (MILP) for inference and outlines a Structured SVM formulation for parameter learning. The paper describes an algorithm for parameter learning that efficiently incorporates low-energy labelings as active constraints and updates parameters to minimize a re-scaled margin energy function. The parameter learning process utilizes an IOU loss function and benefits from reasonable initial parameter estimates to avoid local minima.\n\n---TOPICCS---\nMixed Integer Linear Programming (MILP)\nStructured SVM\nParameter Learning\nIndoor Scene Understanding\nObject Segmentation",
    "topics": [],
    "references": [
      {
        "citation": "[Khan, S. H., He, X., Bennamoun, M., Sohel, F., & Togneri, R. Separating objects and clutter in indoor scenes. In CVPR. IEEE, 2015.] - Likely relevant due to the focus on object separation, a core task in scene understanding."
      },
      {
        "citation": "[Shotton, J., Winn, J., Rother, C., & Criminis, A. Textonboost for image understanding: Multi-class object recognition and segmentation by jointly modeling texture, layout, and context. IJCV, 2009.] - This reference addresses object recognition and segmentation, key components of scene understanding."
      },
      {
        "citation": "[Taskar, B., Chatalbashev, V., & Koller, D. Learning associative markov networks. In ICML, page 102. ACM, 2004.] - This is a foundational reference on a specific type of graphical model, potentially used as a methodological underpinning."
      }
    ],
    "author_details": [
      {
        "name": "S. H. Khan",
        "affiliation": "School of C SSE UWA",
        "email": "salman.khan@uwa.edu.au"
      },
      {
        "name": "Xuming He",
        "affiliation": "NICTA and ANU",
        "email": "xuming.he@nicta.com.au"
      },
      {
        "name": "M. Bannamoun",
        "affiliation": "School of C SSE UWA",
        "email": "mohammed.bennamoun@uwa.edu.au"
      },
      {
        "name": "F. Sohel",
        "affiliation": "School of C SSE UWA",
        "email": "ferdous.sohel@uwa.edu.au"
      },
      {
        "name": "R. Togneri",
        "affiliation": "School of EECE UWA",
        "email": "roberto.togneri@uwa.edu.au"
      }
    ]
  },
  {
    "title": "Accurate Depth Map Estimation from a Lenslet Light Field Camera\n---AUTHOR---\nHae-Gon Jeon\nJaesik Park\nGyeongmin Choe\nJinsun Park\nYunsu Bok\nYu-Wing Tai\nIn So Kweon",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Jeon_Accurate_Depth_Map_2015_CVPR_paper.pdf",
    "id": "Jeon_Accurate_Depth_Map_2015_CVPR_paper",
    "abstract": "This paper introduces an algorithm that accurately estimates depth maps using a lenslet light field camera. The proposed algorithm estimates the multi-view stereo correspondences with sub-pixel accuracy using the cost volume. The foundation for constructing accurate costs involves displacing sub-aperature images using the phase shift theorem, adaptively aggregating gradient costs based on angular coordinates, and using feature correspondences as constraints. A multi-label optimization propagates and corrects the depth map in weak texture regions, followed by iterative refinement using local quadratic function fitting to estimate a non-discrete depth map. A method is also proposed to correct distortions caused by micro-lenses. The algorithm's effectiveness is demonstrated through real-world examples and comparisons with advanced depth estimation algorithms.\n\n---TOPIC---\nLenslet Light Field Camera\nDepth Map Estimation\nSub-pixel Accuracy\nPhase Shift Theorem\nMulti-label Optimization",
    "topics": [],
    "references": [],
    "author_details": [
      {
        "name": "Hae-Gon Jeon",
        "affiliation": "Korea Advanced Institute of Science and Technology (KAIST)",
        "email": "hgjeon@rcv.kaist.ac.kr"
      },
      {
        "name": "Jaesik Park",
        "affiliation": "Korea Advanced Institute of Science and Technology (KAIST)",
        "email": "jspark@rcv.kaist.ac.kr"
      },
      {
        "name": "Gyeongmin Choe",
        "affiliation": "Korea Advanced Institute of Science and Technology (KAIST)",
        "email": "gmchoe@rcv.kaist.ac.kr"
      },
      {
        "name": "Jinsun Park",
        "affiliation": "Korea Advanced Institute of Science and Technology (KAIST)",
        "email": "zzangjinsun@gmail.com"
      },
      {
        "name": "Yunsu Bok",
        "affiliation": "Korea Advanced Institute of Science and Technology (KAIST)",
        "email": "ysbok@rcv.kaist.ac.kr"
      },
      {
        "name": "Yu-Wing Tai",
        "affiliation": "Korea Advanced Institute of Science and Technology (KAIST)",
        "email": "yuwing@gmail.com"
      },
      {
        "name": "In So Kweon",
        "affiliation": "Korea Advanced Institute of Science and Technology (KAIST)",
        "email": "iskweon@kaist.ac.kr"
      }
    ]
  },
  {
    "title": "Efﬁcient Parallel Optimization for Potts Energy with Hierarchical Fusion\n---AUTHOR---\nOlga Veksler",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Veksler_Efficient_Parallel_Optimization_2015_CVPR_paper.pdf",
    "id": "Veksler_Efficient_Parallel_Optimization_2015_CVPR_paper",
    "abstract": "Potts energy frequently occurs in computer vision applications. We present an efficient parallel method for optimizing Potts energy based on the extension of hierarchical fusion algorithm. Unlike previous parallel graph-cut based optimization algorithms, our approach has optimality bounds even after a single iteration over all labels, i.e. after solving only k-1 max-ﬂow problems, where k is the number of labels. This is perhaps the minimum number of max-ﬂow problems one has to solve to obtain a solution with optimality guarantees. Our approximation factor is O(log2 k). Although this is not as good as the factor of 2 approximation of the well known expansion algorithm, we achieve very good results in practice. In particular, we found that the results of our algorithm after one iteration are always better than the results after one iteration of the expansion algorithm. We demonstrate experimentally the computational advantages of our parallel implementation on the problem of stereo correspondence, achieving a factor of 1.5 to 2.6 speedup compared to the serial implementation. These results were obtained with a small number of processors. The expected speedups with a larger number of processors are greater.\n\n---TOPIC---\nPotts Energy Optimization\nHierarchical Fusion Algorithm\nParallel Graph Cuts\nStereo Correspondence\nApproximation Algorithms",
    "topics": [],
    "references": [
      {
        "citation": "[Boykov, Y., Vekler, O., & Zabih, R. (2001). Optimizing multi-label MRFs by move making algorithms. In Advances in Markov Random Fields for Vision and Image Processing, edited by A. Blake, P. Kohli and C. Rother, pages 51–65.]"
      },
      {
        "citation": "[Boykov, Y., & Kolmogorov, V. (2004). An Experimental Comparison of Min-Cut/Max-Flow Algorithms for Energy Minimization in Vision. IEEE Transactions on Pattern Recognition and Machine Intelligence (TPAMI), 29(9), 1124–1137.]"
      },
      {
        "citation": "[Kolmogorov, V., & Rother, C. (2004). What Energy Functions Can Be Optimized via Graph Cuts. IEEE Transactions on Pattern Recognition and Machine Intelligence (TPAMI), 26(2), 147–159.]"
      },
      {
        "citation": "[Kolmogorov, V., & Zabih, R. (2006). Convergent Tree-Reweighted Message Passing for Energy Minimization. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 28(10), 1568–1583.]"
      },
      {
        "citation": "[Alahari, K., Kohli, P., & Torr, P. H. (2010). Dynamic Hybrid Algorithms for MAP Inference in Discrete MRFs. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 32, 1846–1857.]"
      },
      {
        "citation": "[Boykov, Y., Vekler, O., & Zabih, R. (2001). Fast Approximate Energy Minimization via Graph Cuts. IEEE Transactions on Pattern Recognition and Machine Intelligence (TPAMI), 23(11), 1222–1239.]"
      },
      {
        "citation": "[Kumar, M. P., & Koller, D. (2009). MAP estimation of semi-metric MRFs via hierarchical graph cuts. In Conference on Uncertainty in Artificial Intelligence, pages 51–65.]"
      },
      {
        "citation": "[Rother, C., Kolmogorov, V., Lempitsky, V., & Szummer, M. (2007). Optimizing Binary MRFs via Extended Roof Duality. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 589–596.]"
      },
      {
        "citation": "[Szeliski, R., Hartley, R., Mashford, J., & Burn, Y. (2011). Superpixels via pseudo-boolean optimization. In Computer Vision (ICCV), 1387–1394.]"
      },
      {
        "citation": "[Alahari, K., Kohli, P., & Torr, P. H. (2008). Reduce, reuse & recycle: Efficiently solving multi-label mrfs. In Computer Vision and Pattern Recognition, 2008. CVPR 2008. IEEE Conference on, pages 1–8.]"
      }
    ],
    "author_details": [
      {
        "name": "Olga Veksler",
        "affiliation": "University of Western Ontario",
        "email": "olga@csd.uwo.ca"
      }
    ]
  },
  {
    "title": "Jointly Learning Heterogeneous Features for RGB-D Activity Recognition\n---AUTHOR---\nJian-Fang Hu\n---AUTHOR---\nWei-Shi Zheng\n---AUTHOR---\nJianhuang Lai\n---AUTHOR---\nJianguo Zhang",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Hu_Jointly_Learning_Heterogeneous_2015_CVPR_paper.pdf",
    "id": "Hu_Jointly_Learning_Heterogeneous_2015_CVPR_paper",
    "abstract": "In this paper, we focus on heterogeneous feature learning for RGB-D activity recognition. Considering that features from different channels could share some similar hidden structures, we propose a joint learning model to simultaneously explore the shared and feature-specific components as an instance of heterogenous multi-task learning. The proposed model in a unified framework is capable of: 1) jointly mining a set of subspaces with the same dimensionality to enable the multi-task classifier learning, and 2) meanwhile, quantifying the shared and feature-specific components of features in the subspaces. To efficiently train the joint model, a three-step iterative optimization algorithm is proposed, followed by two inference models. Extensive results on three activity datasets have demonstrated the efficacy of the proposed method. In addition, a novel RGB-D activity dataset focusing on human-object interaction is collected for evaluating the proposed method, which will be made available to the community for RGB-D activity benchmarking and analysis.\n\n---TOPIC---\nRGB-D Activity Recognition\nHeterogeneous Feature Learning\nMulti-Task Learning\nJoint Subspace Mining\nHuman-Object Interaction",
    "topics": [],
    "references": [
      {
        "citation": "[Y. Amit, M. Fink, N. Srebro, and S. Ullman. Uncovering shared structures in multiclass classification. In ICML, 2007.]"
      },
      {
        "citation": "[R. K. Ando and T. Zhang. A framework for learning predictive structures from multiple tasks and unlabeled data. JMLR, (6):1817–1853, 2005.]"
      },
      {
        "citation": "[J. Chen, L. Tang, J. Liu, and J. Ye. A convex formulation for learning a shared predictive structure from multiple tasks. TPAMI, 35(5):1025–1038, 2013.]"
      },
      {
        "citation": "[P. Doll´ar. Piotr’s Computer Vision Matlab Toolbox (PMT). http://vision.ucsd.edu/∼pdollar/toolbox/doc/index.html. 3]"
      },
      {
        "citation": "[S. Han, X. Liao, and L. Carin. Cross-domain multitask learning with latent probit models. In ICML, 2012.]"
      },
      {
        "citation": "[J. Hu, W. Zheng, J. Lai, S. Gong, and T. Xiang. Exemplar-based recognition of human-object interactions. TCSVT, PP(99):1–1, 2015.]"
      },
      {
        "citation": "[H. S. Koppula, R. Gupta, and A. Saxena. Learning human activities and object affordances from rgb-d videos. IJRR, 32(8):951–970, 2013.]"
      },
      {
        "citation": "[J. Lei, X. Ren, and D. Fox. Fine-grained kitchen activity recognition using rgb-d. In Ubicomp, 2012.]"
      },
      {
        "citation": "[W. Li, Z. Zhang, and Z. Liu. Action recognition based on a bag of 3d points. In CVPRW, 2010.]"
      },
      {
        "citation": "[I. Lillo, A. Soto, and J. C. Niebles. Discriminative hierarchical modeling of spatiotemporally composable human activities. In CVPR, 2014.]"
      }
    ],
    "author_details": [
      {
        "name": "Jian-Fang Hu",
        "affiliation": "School of Mathematics and Computational Science, Sun Yat-sen University, China",
        "email": "hujianf@mail2.sysu.edu.cn"
      },
      {
        "name": "Wei-Shi Zheng",
        "affiliation": "School of Information Science and Technology, Sun Yat-sen University, China",
        "email": "wszheng@ieee.org"
      },
      {
        "name": "Jianhuang Lai",
        "affiliation": "School of Information Science and Technology, Sun Yat-sen University, China",
        "email": "stsljh@mail.sysu.edu.cn"
      },
      {
        "name": "Jianguo Zhang",
        "affiliation": "School of Computing, University of Dundee, United Kingdom",
        "email": "jgzhang@computing.dundee.ac.uk"
      }
    ]
  },
  {
    "title": "Classifier Learning with Hidden Information\n---AUTHOR---\nZiheng Wang\nQiang Ji",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Wang_Classifier_Learning_With_2015_CVPR_paper.pdf",
    "id": "Wang_Classifier_Learning_With_2015_CVPR_paper",
    "abstract": "Traditional data-driven classifier learning approaches become limited when the training data is inadequate. To address this, the paper proposes combining hidden information (available during training but not testing) and data to enhance classifier learning. They propose two general approaches to exploit different types of hidden information to improve different classifiers, extending the methods to deal with incomplete hidden information. Experimental results demonstrate the effectiveness of the proposed methods and their superior performance compared to existing methods. The core problem addressed is how to effectively encode hidden information into the classifier structure or parameters during training to improve classification performance.",
    "topics": [
      "Hidden information",
      "Classifier learning",
      "Data-driven approaches",
      "Machine learning",
      "Human action recognition"
    ],
    "references": [],
    "author_details": [
      {
        "name": "Ziheng Wang",
        "affiliation": "ECSE, Rensselaer Polytechnic Institute",
        "email": "wangz10@rpi.edu"
      },
      {
        "name": "Qiang Ji",
        "affiliation": "ECSE, Rensselaer Polytechnic Institute",
        "email": "jiq@rpi.edu"
      }
    ]
  },
  {
    "title": "Embedded Phase Shifting: Robust Phase Shifting with Embedded Signals\n---AUTHOR---\nDaniel Moreno\nKilho Son\nGabriel Taubin",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Moreno_Embedded_Phase_Shifting_2015_CVPR_paper.pdf",
    "id": "Moreno_Embedded_Phase_Shifting_2015_CVPR_paper",
    "abstract": "We introduce Embedded PS, a new robust and accurate phase shifting algorithm for 3D scanning. The method projects only high frequency sinusoidal patterns in order to reduce errors due to global illumination effects, such as subsurface scattering and interreﬂections. The frequency set for the projected patterns is specially designed so that our algorithm can extract a set of embedded low frequency sinusoidals with simple math. All the signals, patterns high and embedded low frequencies, are used with temporal phase unwraping to compute absolute phase values in closed-form, without quantization or approximation via LUT, resulting in fast computation. The absolute phases provide correspondences from projector to camera pixels which enable to recover 3D points using optical triangulation. The algorithm estimates multiple absolute phase values per pixel which are combined to reduce measurement noise while preserving fine details. We prove that embedded periodic signals can be recovered from any periodic signal, not just sinusoidal signals, which may result in further improvements for other 3D imaging methods. Several experiments are presented showing that our algorithm produces more robust and accurate 3D scanning results than state-of-the-art methods for challenging surface materials, with an equal or smaller number of projected patterns and at lower computational cost.\n\n---TOPICICS---\n3D Scanning\nPhase Shifting Algorithms\nStructured Light\nOptical Triangulation\nGlobal Illumination",
    "topics": [],
    "references": [
      {
        "citation": "[Chen, T., Seidel, H.-P., and Lensch, H. Modulated phase-shifting for 3D scanning. CVPR 2008.]"
      },
      {
        "citation": "[Gupta, M., Agarwal, A., Veeraraghavan, A., and Narasimhan, S. Structured light 3D scanning in the presence of global illumination. CVPR 2011.]"
      },
      {
        "citation": "[Gupta, M., and Nayar, S. Micro Phase Shifting. CVPR, 2012.]"
      },
      {
        "citation": "[Huang, P. S., Zhang, S., and Chiang, F.-P. Trapezoidal phase-shifting method for 3D shape measurement. 2004.]"
      },
      {
        "citation": "[Karpinsky, N., Hoke, M., Chen, V., and Zhang, S. High-resolution, real-time three-dimensional shape measurement on graphics processing unit. Optical Engineering, 2014.]"
      },
      {
        "citation": "[Liu, K., Wang, Y., Lau, D. L., Hao, Q., and Hassebrook, L. G. Dual-frequency pattern scheme for high-speed 3D shape measurement. Optics express, 2010.]"
      },
      {
        "citation": "[Moreno, D., and Taubin, G. Simple, accurate, and robust projector-camera calibration. 3DIMPVT 2012.]"
      },
      {
        "citation": "[Nayar, S., Krishnan, G., Grossberg, M. D., and Raskar, R. Fast Separation of Direct and Global Components of a Scene using High Frequency Illumination. ACM Trans. on Graphics, 2006.]"
      },
      {
        "citation": "[Pribanic, T., D´zapo, H., and Salvi, J. Efﬁcient and low-cost 3D structured light system based on a modiﬁed number-theoretic approach. EURASIP J Adv Sign Proc, 2010.]"
      },
      {
        "citation": "[Salvi, J., Fernandez, S., Pribanic, T., and Llado, X. A state of the art in structured light patterns for surface proﬁlometry. Pattern Recognition, 2010.]"
      }
    ],
    "author_details": [
      {
        "name": "Daniel Moreno",
        "affiliation": "Brown University",
        "email": "daniel moreno@brown.edu"
      },
      {
        "name": "Kilho Son",
        "affiliation": "Brown University",
        "email": "kilho son@brown.edu"
      },
      {
        "name": "Gabriel Taubin",
        "affiliation": "Brown University",
        "email": "gabriel t aubin@brown.edu"
      }
    ]
  },
  {
    "title": "A Geodesic-Preserving Method for Image Warping\n---AUTHORs---\nDongping Li\nKaiming He\nJian Sun\nKun Zhou",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Li_A_Geodesic-Preserving_Method_2015_CVPR_paper.pdf",
    "id": "Li_A_Geodesic-Preserving_Method_2015_CVPR_paper",
    "abstract": "The manipulation of panoramic/wide-angle images is usually achieved via image warping. Though various techniques have been developed for preserving shapes and straight lines for warping, these are not sufficient for panoramic/wide-angle images. The image projections will turn the straight lines into curved “geodesic lines”, and it is fundamentally impossible to keep all these lines straight. In this work, we propose a geodesic-preserving method for content-aware image warping. An energy term is introduced to preserve the geodesic appearance of the geodesic lines, and can be used with shape-preserving terms. Our method is demonstrated in various applications, including rectangling panoramas, resizing panoramic/wide-angle images, and wide-angle image manipulation. An extension to ellipse preservation for general images is also presented.",
    "topics": [
      "Geodesic lines",
      "Panoramic/wide-angle images",
      "Image warping",
      "Content-aware image processing",
      "Non-local geometric entities"
    ],
    "references": [
      {
        "citation": "[R. Szeliski. Image alignment and stitching: A tutorial. Foundations and Trends in Computer Graphics and Vision, 2006.] - This appears to be a key foundational work on the topic."
      },
      {
        "citation": "[R. Szeliski and H.-Y. Shum. Creating full view panoramic image mosaics and environment maps. In ACM SIGGRAPH ’97, 1997.] - An early work on panoramic image creation."
      },
      {
        "citation": "[J. Kopf, D. Lischinski, O. Deussen, D. Cohen-Or, and M. Cohen. Locally adapted projections to reduce panorama distortions. In Computer Graphics Forum, pages 1083–1089. Wiley Online Library, 2009.] - Addresses a core problem in panorama creation: distortion reduction."
      },
      {
        "citation": "[A. Agarwala, M. Agrawala, M. Cohen, D. Salesin, and R. Szeliski. Photographing long scenes with multi-viewpoint panoramas. In ACM SIGGRAPH ’06, pages 853–861, 2006.] - Explores techniques for creating panoramas from multiple viewpoints."
      },
      {
        "citation": "[R. Szeliski. Image alignment and stitching: A tutorial. Foundations and Trends in Computer Graphics and Vision, 2006.] - Provides a comprehensive overview of the field."
      },
      {
        "citation": "[J. Zaragoza, T.-J. Chin, M. S. Brown, and D. Suter. As-projective-as-possible image stitching with moving dlt. In International Conference on Computer Vision and Pattern Recognition (CVPR), 2013.] - A more recent work on image stitching."
      },
      {
        "citation": "[T. Igarashi, T. Moscovich, and J. F. Hughes. As-rigid-as-possible shape manipulation. In ACM SIGGRAPH ’05, 2005.] - Relevant due to the concept of \"as-rigid-as-possible\" being used in other papers."
      },
      {
        "citation": "[S. Avidan and A. Shamir. Seam carving for content-aware image resizing. In ACM SIGGRAPH ’07, 2007.] - Introduces a content-aware image resizing technique."
      },
      {
        "citation": "[R. von Gioi, J. Jakubowicz, J. Morel, and G. Randall. Lsd: A fast line segment detector with a false detection control. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), pages 722–732, 2010.] - A line segment detector, potentially useful for feature extraction."
      },
      {
        "citation": "[Y.-S. Wang, H.-C. Lin, O. Sorkine, and T.-Y. Lee. Motion-based video retargeting with optimized crop-and-warp. In ACM SIGGRAPH ’10, 2010.] - Addresses related concepts of warping and resizing in video."
      }
    ],
    "author_details": [
      {
        "name": "Dongping Li",
        "affiliation": "Zhejiang University",
        "email": "Not available"
      },
      {
        "name": "Kaiming He",
        "affiliation": "Microsoft Research",
        "email": "Not available"
      },
      {
        "name": "Jian Sun",
        "affiliation": "Microsoft Research",
        "email": "Not available"
      },
      {
        "name": "Kun Zhou",
        "affiliation": "Zhejiang University",
        "email": "Not available"
      }
    ]
  },
  {
    "title": "Is object localization for free? – Weakly-supervised learning with convolutional neural networks\n---AUTHORs---\nMaxime Oquab\nLéon Bottou\nIvan Laptev\nJosef Sivic",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Oquab_Is_Object_Localization_2015_CVPR_paper.pdf",
    "id": "Oquab_Is_Object_Localization_2015_CVPR_paper",
    "abstract": "We describe a weakly supervised convolutional neural network (CNN) for object classification that relies only on image-level labels, yet can learn from cluttered scenes containing multiple objects. We quantify its object classification and object location prediction performance on the Pascal VOC 2012 (20 object classes) and the much larger Microsoft COCO (80 object classes) datasets. We find that the network (i) outputs accurate image-level labels, (ii) predicts approximate locations (but not extent) of objects, and (iii) performs comparably to its fully-supervised counterparts using object bounding box annotation for training.\n\n---TOPIPS---\nConvolutional Neural Networks (CNNs)\nWeakly Supervised Learning\nObject Localization\nImage Classification\nCluttered Scenes",
    "topics": [],
    "references": [
      {
        "citation": "[R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In CVPR, 2014.]"
      },
      {
        "citation": "[H. Arora, N. Loeff, D. Forsyth, and N. Ahuja. Unsupervised segmentation of objects using efﬁcient learning. In CVPR, 2007.]"
      },
      {
        "citation": "[A. Bergamo, L. Bazzani, D. Anguelov, and L. Torresani. Self-taught object localization with deep networks. CoRR, abs/1409.3964, 2014.]"
      },
      {
        "citation": "[M. Hejrati and D. Ramanan. Analyzing 3d objects in cluttered images. In NIPS, 2012.]"
      },
      {
        "citation": "[A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imaginet classification with deep convolutional neural networks. In NIPS, 2012.]"
      },
      {
        "citation": "[Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R.E. Howard, W. Hubbard, and L.D. Jackel. Backpropagation applied to handwritten zip code recognition. Neural Computation, 1(4):541–551, Winter 1989.]"
      },
      {
        "citation": "[R. Fergus, P. Perona, and A. Zisserman. Object class recognition by unsupervised scale-invariant learning. In CVPR, 2003.]"
      },
      {
        "citation": "[M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The pascal visual object classes (VOC) challenge. IJCV, 88(2):303–338, Jun 2010.]"
      },
      {
        "citation": "[P. Felzenszwalb, R. Girshick, D. McAllester, and D. Ramanan. Object detection with discriminatively trained part based models. IEEE PAMI, 32(9):1627–1645, 2010.]"
      },
      {
        "citation": "[J. Foulds and E. Frank. A review of multi-instance learning assumptions. The Knowledge Engineering Review, 25(01):1–25, 2010.]"
      }
    ],
    "author_details": [
      {
        "name": "Maxime Oquab",
        "affiliation": "INRIA Paris, France",
        "email": "Not available"
      },
      {
        "name": "Léon Bottou",
        "affiliation": "MSR, New York, USA",
        "email": "Not available"
      },
      {
        "name": "Ivan Laptev",
        "affiliation": "INRIA, Paris, France",
        "email": "Not available"
      },
      {
        "name": "Josef Sivic",
        "affiliation": "INRIA, Paris, France",
        "email": "Not available"
      }
    ]
  },
  {
    "title": "The Treasure beneath Convolutional Layers: Cross-convolutional-layer Pooling for Image Classiﬁcation\n---AUTHOR---\nLingqiao Liu\nChunhua Shen\nAnton van den Hengel",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Liu_The_Treasure_Beneath_2015_CVPR_paper.pdf",
    "id": "Liu_The_Treasure_Beneath_2015_CVPR_paper",
    "abstract": "Recent studies have shown that Deep Convolutional Neural Networks (DCNNs) pretrained on large datasets can be used as universal image descriptors, leading to impressive performance in image classification. However, most approaches utilize fully-connected layer activations, while convolutional layer activations are often considered less discriminative. This paper challenges this view, advocating that convolutional layer activations can be a powerful image representation when used appropriately. The authors propose a new technique called cross-convolutional layer pooling (or cross layer pooling) which extracts subarrays of feature maps from one convolutional layer as local features and pools them using activations from a successive convolutional layer. This method avoids the image style mismatching issue encountered in existing methods and is simpler to implement. Experiments on four visual classification tasks demonstrate comparable or significantly better performance compared to existing fully-connected layer-based approaches.",
    "topics": [
      "Deep Convolutional Neural Networks (DCNNs)",
      "Cross-convolutional layer pooling",
      "Image representation learning",
      "Transfer learning",
      "Image classification"
    ],
    "references": [
      {
        "citation": "[Krizhevsky, A., Sutskever, I., & Hinton, G. (2012). ImageNet classification with deep convolutional neural networks. *Advances in Neural Information Processing Systems*, *25*.]"
      },
      {
        "citation": "[Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., & Fei-Fei, L. (2009). ImageNet: A large-scale hierarchical image database. *IEEE Conference on Computer Vision and Pattern Recognition*.]"
      },
      {
        "citation": "[Razavian, A. S., Azizpour, H., Sullivan, J., & Carlsson, S. (2014). CNN features off-the-shelf: an astounding baseline for recognition. *Workshop of IEEE Conference on Computer Vision and Pattern Recognition*.]"
      },
      {
        "citation": "[Azizpour, H., Razavian, A. S., Sullivan, J., Maki, A., & Carlsson, S. (2014). From generic to specific deep representations for visual recognition. arXiv preprint arXiv:1406.5774.]"
      },
      {
        "citation": "[Gong, Y., Wang, L., Guo, R., & Lazebnik, S. (2014). Multi-scale orderless pooling of deep convolutional activation features. *European Conference on Computer Vision*.]"
      },
      {
        "citation": "[Liu, L., Shen, C., Wang, L., van den Hengel, A., & Wang, C. (2014). Encoding high dimensional local features by sparse coding based Fisher vectors. *Advances in Neural Information Processing Systems*.]"
      },
      {
        "citation": "[Zeiler, M., & Fergus, R. (2014). Visualizing and understanding convolutional networks. *European Conference on Computer Vision*.]"
      },
      {
        "citation": "[Agraval, P., Girshick, R., & Malik, J. (2014). Analyzing the performance of multilayer neural networks for object recognition. *European Conference on Computer Vision*.]"
      },
      {
        "citation": "[Li, Y., Liu, L., Shen, C., & van den Hengel, A. (2015). Mid-level deep pattern mining. *IEEE Conference on Computer Vision and Pattern Recognition*.]"
      },
      {
        "citation": "[He, K., Zhang, X., Ren, S., & Sun, J. (2014). Spatial pyramid pooling in deep convolutional networks for visual recognition. *European Conference on Computer Vision*.]"
      }
    ],
    "author_details": [
      {
        "name": "Lingqiao Liu",
        "affiliation": "The University of Adelaide",
        "email": "Lingqiao.Liu@adelaide.edu.au"
      },
      {
        "name": "Chunhua Shen",
        "affiliation": "The University of Adelaide, The Australian Centre for Robotic Vision",
        "email": "Chunhua.Shen@adelaide.edu.au (inferred from context, not explicitly provided)"
      },
      {
        "name": "Anton van den Hengel",
        "affiliation": "The University of Adelaide, The Australian Centre for Robotic Vision",
        "email": "Anton.vanDenHengel@adelaide.edu.au (inferred from context, not explicitly provided)"
      }
    ]
  },
  {
    "title": "Fast Action Proposals for Human Action Detection and Search\n---AUTHOR---\nGang Yu\n---AUTHOR---\nJunsong Yuan",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Yu_Fast_Action_Proposals_2015_CVPR_paper.pdf",
    "id": "Yu_Fast_Action_Proposals_2015_CVPR_paper",
    "abstract": "In this paper, we target generating generic action proposals in unconstrained videos. Each action proposal corresponds to a temporal series of spatial bounding boxes (a spatiotemporal video tube) with the potential to locate a human action. We utilize both appearance and motion cues to measure the actionness of these video tubes. Action proposal generation is formulated as a maximum set coverage problem, solved using a greedy search to maximize the overall actionness score. Our approach avoids video segmentation and achieves near real-time performance. Experimental results on MSRII and UCF 101 datasets demonstrate superior performance in action proposals and competitive results in action detection and search.\n\n---TOPICKS---\nHuman Action Detection\nAction Proposals\nMaximum Set Coverage\nSpatio-Temporal Video Analysis\nGreedy Search Algorithm",
    "topics": [],
    "references": [],
    "author_details": [
      {
        "name": "Gang Yu",
        "affiliation": "School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore",
        "email": "iskicy@gmail.com"
      },
      {
        "name": "Junsong Yuan",
        "affiliation": "School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore",
        "email": "jsyuan@ntu.edu.sg"
      }
    ]
  },
  {
    "title": "Visual Recognition by Counting Instances: A Multi-Instance Cardinality Potential Kernel\n---AUTHOR---\nHossein Hajimirsadeghi\nWang Yan\nArash Vahdat\nGreg Mori",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Hajimirsadeghi_Visual_Recognition_by_2015_CVPR_paper.pdf",
    "id": "Hajimirsadeghi_Visual_Recognition_by_2015_CVPR_paper",
    "abstract": "Many visual recognition problems can be approached by counting instances. To determine whether an event is present in a long internet video, one could count how many frames seem to contain the activity. Classifying the activity of a group of people can be done by counting the actions of individual people. Encoding these cardinality relation-ships can reduce sensitivity to clutter, in the form of irrelevant frames or individuals not involved in a group activity. Learned parameters can encode how many instances tend to occur in a class of interest. To this end, this paper develops a powerful and ﬂexible framework to infer any cardinal-ity relation between latent labels in a multi-instance model. Hard or soft cardinality relations can be encoded to tackle diverse levels of ambiguity. Experiments on tasks such as human activity recognition, video event detection, and video summarization demonstrate the effectiveness of using cardi-nality relations for improving recognition results.\n\n---TOPIC---\nCardinality relations\nMulti-instance learning\nVisual recognition\nLatent structured models\nKernel methods",
    "topics": [],
    "references": [
      {
        "citation": "[Amer, M. R., Lei, P., & Todorovic, S. Hirf: Hierarchical random field for collective activity recognition in videos. European Conference on Computer Vision (ECCV), 572–585. Springer, 2014.]"
      },
      {
        "citation": "[Kwok, J. T., & Cheung, P.-M. Marginalized multi-instance kernels. International Joint Conferences on Artificial Intelligence (IJCAI), 901–906, 2007.]"
      },
      {
        "citation": "[Lai, K.-T., Yu, F. X., Chen, M.-S., & Chang, S.-F. Video event detection by inferring temporal instance labels. Computer Vision and Pattern Recognition (CVPR), 2014.]"
      },
      {
        "citation": "[Lan, T., Wang, Y., Yang, W., Robinovitch, S. N., & Mori, G. Discriminative latent models for recognizing contextual group activities. IEEE Trans. Pattern Analysis and Machine Intelligence (T-PAMI), 34(8):1549–1562, 2012.]"
      },
      {
        "citation": "[Choi, W., & Savarese, S. A unified framework for multi-target tracking and collective activity recognition. European Conference on Computer Vision (ECCV), 215–230. 2012.]"
      },
      {
        "citation": "[Gartner, T., Flach, P., Kowalczyk, A., & Smolka, A. Multi-instance kernels. International Conference on Machine Learning (ICML), 179–186, 2002.]"
      },
      {
        "citation": "[Gupta, A., Srinivasan, P., Shi, J., & Davis, L. S. Understanding videos, constructing plots: Learning a visually grounded storyline model from annotated videos. Computer Vision and Pattern Recognition (CVPR), 2009.]"
      },
      {
        "citation": "[Gigli, M., Grabner, H., Riemenschneider, H., & Van Gool, L. Creating summaries from user videos. European Conference on Computer Vision (ECCV), 505–520. Springer, 2014.]"
      },
      {
        "citation": "[Hajimirsadeghi, H., Li, J., Mori, G., Zaki, M., & Sayed, T. Multiple instance learning by discriminative training of markov networks. Uncertainty in Artificial Intelligence (UAI), 2013.]"
      },
      {
        "citation": "[Khosla, A., Hamid, R., Lin, C., & Sundarese, N. Large-scale video summarization using web-image priors. Computer Vision and Pattern Recognition (CVPR), 2013.]"
      }
    ],
    "author_details": [
      {
        "name": "Hossein Hajimirsadeghi",
        "affiliation": "School of Computing Science, Simon Fraser University, Canada",
        "email": "hosseinh@sfu.ca"
      },
      {
        "name": "Wang Yan",
        "affiliation": "School of Computing Science, Simon Fraser University, Canada",
        "email": "wyan@sfu.ca"
      },
      {
        "name": "Arash Vahdat",
        "affiliation": "School of Computing Science, Simon Fraser University, Canada",
        "email": "avahdat@sfu.ca"
      },
      {
        "name": "Greg Mori",
        "affiliation": "School of Computing Science, Simon Fraser University, Canada",
        "email": "mori@cs.sfu.ca"
      }
    ]
  },
  {
    "title": "Feature-Independent Context Estimation for Automatic Image Annotation\n---AUTHOR---\nAmara Tariq\nHassan Foroosh",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Tariq_Feature-Independent_Context_Estimation_2015_CVPR_paper.pdf",
    "id": "Tariq_Feature-Independent_Context_Estimation_2015_CVPR_paper",
    "abstract": "Automatic image annotation aims to bridge the gap between low-level image representations and textual descriptions. This paper proposes a feature-independent and unsupervised context estimation process to aid this task. The method utilizes Tucker decomposition of tensors, transforming images into suitable tensors to provide context information without relying on additional metadata or visual features. The estimated context is incorporated as prior knowledge in the automatic image annotation process, and evaluation on two datasets demonstrates the effectiveness of the approach.\n\n---TOPICs---\nAutomatic Image Annotation\nContext Estimation\nTensor Decomposition\nFeature-Independent Methods\nSemantic Gap",
    "topics": [],
    "references": [],
    "author_details": [
      {
        "name": "Amara Tariq",
        "affiliation": "University of Central Florida",
        "email": "[Email not available in the provided text]"
      },
      {
        "name": "Hassan Foroosh",
        "affiliation": "University of Central Florida",
        "email": "[Email not available in the provided text]"
      }
    ]
  },
  {
    "title": "HC-Search for Structured Prediction in Computer Vision\n---AUTHOR---\nMichael Lam\nJanardhan Rao Doppa\nSinisia Todorovic\nThomas G. Dietterich",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Lam_HC-Search_for_Structured_2015_CVPR_paper.pdf",
    "id": "Lam_HC-Search_for_Structured_2015_CVPR_paper",
    "abstract": "This paper investigates the applicability of HC-Search, a structured prediction method successful in natural language processing, to basic computer vision tasks like object detection, scene labeling, and monocular depth estimation. The mainstream approach in vision relies on energy minimization, but search-based methods like HC-Search offer an alternative with the potential for higher performance. The paper introduces a search operator tailored for vision, along with the DAGGER algorithm for training the search heuristic, to improve HC-Search's performance. The results demonstrate significant performance boosts, achieving state-of-the-art results in scene labeling and depth estimation, suggesting HC-Search is a suitable tool for learning and inference in vision.",
    "topics": [
      "HC-Search",
      "Computer Vision",
      "Structured Prediction",
      "Search-based Methods",
      "Energy Minimization"
    ],
    "references": [
      {
        "citation": "[Arbeláez, P. Boundary extraction in natural images using ultrametric contour maps. In IEEE Workshop Perceptual Organization (POCV), page 182, 2006.]"
      },
      {
        "citation": "[Arbeláez, P., Hariharan, B., Gu, C., Gupta, S., & Malik, J. Semantic Segmentation using Regions and Parts. In CVPR, 2012.]"
      },
      {
        "citation": "[Felzenszwalb, P. F., & McAllester, D. The generalized A* architecture. Journal of Artificial Intelligence Research (JAIR), 29:153–190, 2007.]"
      },
      {
        "citation": "[Boykov, Y., Vekslér, O., & Zabih, R. Fast approximate energy minimization via graph cuts. IEEE TPAM, 23(11):1222–1239, 2001.]"
      },
      {
        "citation": "[Doppa, J. R., Fern, A., & Tadepalli, P. HC-search: A learning framework for search-based structured prediction. J. Artificial Intell. Res. (JAIR), 50:369–407, 2014.]"
      },
      {
        "citation": "[Gould, S., Fulton, R., & Koller, D. Decomposing a scene into geometric and semantically consistent regions. In ICCV, 2009.]"
      },
      {
        "citation": "[Saxena, A., Chung, S. H., & Ng, A. Y. 3-d depth reconstruction from a single still image. ICCV, 2007.]"
      },
      {
        "citation": "[Weiss, D., & Taskar, B. Structured prediction cascades. Journal of Machine Learning Research - Proceedings Track, 9:916–923, 2010.]"
      },
      {
        "citation": "[Kohli, P., Ladický, L., & Torr, P. H. Robust Higher Order Potentials for Enforcing Label Consistency. Int. J. Comput. Vision, 82(3):302–324, 2009.]"
      },
      {
        "citation": "[Arbeláez, P., Maire, M., Fowlkes, C., & Malik, J. Contour detection and hierarchical image segmentation. IEEE Trans. Pattern Anal. Mach. Intell., 33(5):898–916, May 2011.]"
      }
    ],
    "author_details": [
      {
        "name": "Michael Lam",
        "affiliation": "Oregon State University",
        "email": "lamm@eecs.oregonstate.edu"
      },
      {
        "name": "Janardhan Rao Doppa",
        "affiliation": "Washington State University",
        "email": "jana@eecs.wsu.edu"
      },
      {
        "name": "Sinisia Todorovic",
        "affiliation": "Oregon State University",
        "email": "sinisa@eecs.oregonstate.edu"
      },
      {
        "name": "Thomas G. Dietterich",
        "affiliation": "Oregon State University",
        "email": "tgd@eecs.oregonstate.edu"
      }
    ]
  },
  {
    "title": "Propagated Image Filtering\n---AUTHORISTS---\nYu-Chiang Frank Wang\nJen-Hao Rick Chang",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Chang_Propagated_Image_Filtering_2015_CVPR_supplemental.pdf",
    "id": "Chang_Propagated_Image_Filtering_2015_CVPR_supplemental",
    "abstract": "There is no abstract provided in the provided text. The provided text appears to be supplemental material containing experimental results and figures related to image filtering.\n\n---TOPSICS---\nImage Denoising\nImage Smoothing\nPropagation Filtering\nPSNR (Peak Signal-to-Noise Ratio)\nSSIM (Structural Similarity Index Measure)",
    "topics": [],
    "references": [],
    "author_details": [
      {
        "name": "Yu-Chiang Frank Wang",
        "affiliation": "Research Center for IT Innovation, Academia Sinica",
        "email": "ycwang@citi.sinica.edu.tw"
      },
      {
        "name": "Jen-Hao Rick Chang",
        "affiliation": "Dept. Electrical and Computer Engineering, Carnegie Mellon University",
        "email": "rickchang@cmu.edu"
      }
    ]
  },
  {
    "title": "Joint Vanish Point Extraction and Tracking (Supplementary Material)\n---AUTHOR---\nTill Kroeger\nDengxin Dai\nLuc Van Gool",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Kroeger_Joint_Vanishing_Point_2015_CVPR_supplemental.pdf",
    "id": "Kroeger_Joint_Vanishing_Point_2015_CVPR_supplemental",
    "abstract": "This paper addresses the lack of datasets for vanishing point (VP) detection and tracking over time. To overcome this, the authors adapt an existing dataset of street-view videos captured from a van-mounted camera system, which provides ground truth camera poses. They describe a semi-automatic procedure for VP annotation, leveraging the known camera orientations and line segment detection. The resulting dataset enables the evaluation of VP extraction and tracking algorithms, particularly those incorporating pose knowledge. The annotation process involves detecting line segments, transforming them into world coordinates, computing VP exemplars, clustering interpretation planes, and manually correcting occasional errors. The approach builds upon single-frame VP detection but extends it to a multi-view, time-dependent setting.\n\n---TOPICHS---\nVanishing Point Detection\nCamera Pose Estimation\nSemi-Automatic Annotation\nMulti-View Geometry\nLine Segment Detection",
    "topics": [],
    "references": [
      {
        "citation": "Kroeger, T., & Van Gool, L. (2014). Video Registration to SfM Models. *ECCV*."
      },
      {
        "citation": "Tardif, J.-P. (2009). Non-Iterative Approach for Fast and Accurate Vanishing Point Detection. *ICCV*."
      },
      {
        "citation": "von Gioi, R. G., Jakubowicz, J., Morel, J.-M., & Randall, G. (2012). LSD: a Line Segment Detector. *IPOL*."
      }
    ],
    "author_details": [
      {
        "name": "Till Kroeger",
        "affiliation": "Computer Vision Laboratory, D-ITET, ETH Zurich",
        "email": "kroegert@vision.ee.ethz.ch"
      },
      {
        "name": "Dengxin Dai",
        "affiliation": "Computer Vision Laboratory, D-ITET, ETH Zurich",
        "email": "N/A"
      },
      {
        "name": "Luc Van Gool",
        "affiliation": "Computer Vision Laboratory, D-ITET, ETH Zurich; VISICS, ESAT/PSI, KU Leuven",
        "email": "vangool@vision.ee.ethz.ch"
      }
    ]
  },
  {
    "title": "A Light Transport Model for Mitigating Multipath Interference in Time-of-ﬂight Sensors\n---AUTHORs---\nNikhil Naik\nAchuta Kadambi\nChristoph Rhemann\nShahram Izadi\nRamesh Raskar\nSing Bing Kang",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Naik_A_Light_Transport_2015_CVPR_supplemental.pdf",
    "id": "Naik_A_Light_Transport_2015_CVPR_supplemental",
    "abstract": "This supplementary material details the implementation of a light transport model designed to mitigate multipath interference (MPI) in time-of-flight (ToF) sensors. It elaborates on the capture process, radiometric calibration, post-filtering results, and the approximate global illumination model used for MPI correction. Specific attention is given to numerical stability considerations. The supplementary content provides a comprehensive understanding of the methodology employed to address MPI in ToF sensing, including the separation of direct and global light components, radiometric calibration techniques, and the application of a global illumination model.\n\n---TOPICCS---\nMultipath Interference (MPI)\nTime-of-Flight (ToF) Sensing\nRadiometric Calibration\nGlobal Illumination\nLight Transport Models",
    "topics": [],
    "references": [
      {
        "citation": "[Kadambi, A., Bhandari, A., & Raskar, R. 3d depth cameras in vision: Benefits and limitations of the hardware. *Computer Vision and Machine Learning with RGB-D Sensors*. Springer, 2014.] - This paper provides a foundational overview of 3D depth cameras, which is crucial context for any work utilizing them."
      },
      {
        "citation": "[Nayar, S. K., Krishnan, G., Grossberg, M. D., & Raskar, R. Fast separation of direct and global components of a scene using high frequency illumination. *ACM Transactions on Graphics (TOG)*, 2006.] - This is a seminal work by Raskar and colleagues, introducing a key technique for scene understanding that likely informs the paper's methodology."
      },
      {
        "citation": "[Trefethen, L. N., & Bau III, D. *Numerical Linear Algebra*. Siam, 1997.] - This reference suggests the paper utilizes numerical linear algebra techniques, making it a potentially important methodological underpinning."
      }
    ],
    "author_details": [
      {
        "name": "Nikhil Naik",
        "affiliation": "MIT Media Lab",
        "email": "naik@mit.edu"
      },
      {
        "name": "Achuta Kadambi",
        "affiliation": "MIT Media Lab",
        "email": "kadambi@mit.edu"
      },
      {
        "name": "Christoph Rhemann",
        "affiliation": "Microsoft Research",
        "email": "chrheman@microsoft.com"
      },
      {
        "name": "Shahram Izadi",
        "affiliation": "Microsoft Research",
        "email": "shahrami@microsoft.com"
      },
      {
        "name": "Ramesh Raskar",
        "affiliation": "MIT Media Lab",
        "email": "raskar@mit.edu"
      },
      {
        "name": "Sing Bing Kang",
        "affiliation": "Microsoft Research",
        "email": "sbkang@microsoft.com"
      }
    ]
  },
  {
    "title": "Person Count Localization in Videos from Noisy Foreground and Detection\n---AUTHOR---\nSheng Chen\n---AUTHOR---\nAlan Fern\n---AUTHOR---\nSinisia Todorovic",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Chen_Person_Count_Localization_2015_CVPR_paper.pdf",
    "id": "Chen_Person_Count_Localization_2015_CVPR_paper",
    "abstract": "In many domains, detecting people in video frames is a standard initial step for activity recognition and tracking. However, reliably detecting every individual in crowded scenes with severe occlusion remains a challenge. This paper introduces a new problem, person count localization, which aims to find a middle ground between frame-level person counting (which lacks localization) and perfect person detection (which is unreliable in crowded scenes). The goal is to output, for each frame, a set of detections covering both isolated individuals and groups of people, along with counts assigned to each detection. The authors propose a novel framework based on iterative error-driven revisions of a flow graph derived from noisy input of person detections and foreground segmentation. They also introduce a new metric to evaluate their approach on American football and pedestrian videos, measuring both count precision and localization.\n\n---TOPIC---\nPerson Count Localization\nCrowded Scene Analysis\nFlow Graph Optimization\nInteger Programming\nVideo Understanding",
    "topics": [],
    "references": [
      {
        "citation": "[Felzenszwalb, P. F., Girshick, R. B., McAllester, D., & Ramanan, D. (2010). Object detection with discriminatively trained part-based models. *PAMI*, *32*(9), 1627–1645.] - This is a foundational paper on part-based models, relevant to many computer vision tasks including those likely addressed in the original paper."
      },
      {
        "citation": "[Aggarwal, J., & Ryoo, M. (2011). Human activity analysis: A review. *ACM Comput. Surv*, *2011*.] - Provides a broad overview of the field, useful for context."
      },
      {
        "citation": "[Andriluka, M., Roth, S., & Schiele, B. (2008). People-tracking-by-detection and people-detection-by-tracking. *CVPR*, 2008.] - A key paper on the interplay between detection and tracking, a common problem in this area."
      },
      {
        "citation": "[Chan, A. B., & Vasconcelos, N. (2012). Counting people with low-level features and bayesian regression. *TIP*, *2012*.] - Directly addresses crowd counting using Bayesian regression, a core technique."
      },
      {
        "citation": "[Kong, D., Gray, D., & Tao, H. (2012). A viewpoint invariant approach for crowd counting. *ICPR*, 2012.] - Focuses on a specific challenge in crowd counting: viewpoint invariance."
      },
      {
        "citation": "[Ma, Z., & Chan, A. B. (2013). Crossing the line: Crowd counting by integer programming with local features. *CVPR*, 2013.] - Introduces an integer programming approach for crowd counting."
      },
      {
        "citation": "[Shu, G., Dehghan, A., Oreifej, O., Hand, E., & Shah, M. (2012). Part-based multiple-person tracking with partial occlusion handling. *CVPR*, 2012.] - Addresses a critical problem in multi-person tracking: occlusion."
      },
      {
        "citation": "[Ryan, D., Denman, S., Fookes, C., & Sridharan, S. (2009). Crowd counting using multiple local features. *DICTA*, 2009.] - Explores the use of multiple local features for crowd counting."
      },
      {
        "citation": "[Lempitsky, V., & Zisserma, A. (2010). Learning to count objects in images. *NIPS*, 2010.] - Presents a learning-based approach to object counting."
      },
      {
        "citation": "[Ge, W., Collins, T. R., & Ruback, B. (2012). Vision-based analysis of small groups in pedestrian crowds. *IEEE TPAM*, 2012.] - Focuses on analyzing small groups within crowds."
      }
    ],
    "author_details": [
      {
        "name": "Sheng Chen",
        "affiliation": "Oregon State University",
        "email": "chenshen@eecs.oregonstate.edu"
      },
      {
        "name": "Alan Fern",
        "affiliation": "Oregon State University",
        "email": "afern@eecs.oregonstate.edu"
      },
      {
        "name": "Sinisia Todorovic",
        "affiliation": "Oregon State University",
        "email": "sinisa@eecs.oregonstate.edu"
      }
    ]
  },
  {
    "title": "Symmetry-Based Text Line Detection in Natural Scenes\n---AUTHOR---\nZheng Zhang\n---AUTHOR---\nWei Shen\n---AUTHOR---\nCong Yao\n---AUTHOR---\nXiang Bai",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Zhang_Symmetry-Based_Text_Line_2015_CVPR_paper.pdf",
    "id": "Zhang_Symmetry-Based_Text_Line_2015_CVPR_paper",
    "abstract": "This paper addresses the problem of scene text detection, which involves discovering and localizing texts from natural scene images. Unlike traditional methods that focus on individual characters or strokes, the proposed algorithm exploits the symmetry property of character groups to directly extract text lines from natural images. Experiments on standard benchmarks demonstrate state-of-the-art performance and stronger adaptability to challenging scenarios.\n\n---TOPICCS---\nScene text detection\nText line extraction\nSymmetry property\nAlgorithm design\nComputer vision",
    "topics": [],
    "references": [
      {
        "citation": "[21] S. M. Lucas, A. Panaretos, L. Sosa, A. Tang, S. Wong, and R. Young. ICDAR 2003 robust reading competitions. In Proc. of ICDAR, 2003."
      },
      {
        "citation": "[5] X. Chen and A. Yuille. Detecting and reading text in natural scenes. In Proc. of CVPR, 2004."
      },
      {
        "citation": "[16] Y. LeCun, B. Boser, J. S. Denker, R. E. Howard, W. Habbard, L. D. Jackel, and D. Henderson. Handwritten digit recognition with a back-propagation network. In Proc. of NIPS, 1990."
      },
      {
        "citation": "[45] C. Yi and Y. Tian. Text detection in natural scene images by stroke gabor words. In Proc. of ICDAR, 2011."
      },
      {
        "citation": "[33] C. Shi, C. Wang, B. Xiao, Y. Zhang, and S. Gao. Scene text detection using graph model built upon maximally stable extremal regions. Pattern Recognition Letters, 34(2):107–116, 2013."
      },
      {
        "citation": "[40] C. Yao, X. Bai, W. Liu, and L. J. Latecki. Human detection using learned part alphabet and pose dictionary. In Proc. of ECCV, 2014."
      },
      {
        "citation": "[1] ICDAR 2013 robust reading competition challenge 2 results. http://dag.cvc.uab.es/icdar2013competition, 2014. [Online; accessed 11-November-2014]."
      },
      {
        "citation": "[3] A. Bissacco, M. Cummins, Y. Netzer, and H. Neven. PhotoOCR: Reading text in uncontrolled conditions. In Proc. of ICCV, 2013."
      },
      {
        "citation": "[4] L. Breiman. Random forests. Machine Learning, 45(1):5–32, 2001."
      },
      {
        "citation": "[2] P. Arbeláez, M. Maire, C. Fowlkes, and J. Malik. Contour detection and hierarchical image segmentation. IEEE Trans. PAM I, 33(5):898–916, 2011."
      }
    ],
    "author_details": [
      {
        "name": "Zheng Zhang",
        "affiliation": "School of Electronic Information and Communications, Huazhong University of Science and Technology",
        "email": "*Not available in the provided text*"
      },
      {
        "name": "Wei Shen",
        "affiliation": "Key Lab of Specialty Fiber Optics and Optical Access Networks, Shanghai University",
        "email": "*Not available in the provided text*"
      },
      {
        "name": "Cong Yao",
        "affiliation": "School of Electronic Information and Communications, Huazhong University of Science and Technology",
        "email": "*Not available in the provided text*"
      },
      {
        "name": "Xiang Bai",
        "affiliation": "School of Electronic Information and Communications, Huazhong University of Science and Technology",
        "email": "*Not available in the provided text*"
      }
    ]
  },
  {
    "title": "Finding Distractors In Images\n---AUTHORISTS---\nOhad Fried\nEli Shechtman\nDan B Goldman\nAdam Finkelstein",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Fried_Finding_Distractors_In_2015_CVPR_paper.pdf",
    "id": "Fried_Finding_Distractors_In_2015_CVPR_paper",
    "abstract": "This paper introduces a new computer vision task called \"distractor prediction,\" addressing the challenge of automatically identifying and removing distracting regions in images. The authors define distractor prediction, collect a large-scale dataset with annotations of distractor regions, train a prediction model to generate distractor maps for arbitrary images, and use this model to automatically remove distractor regions. The work aims to improve image composition and reduce the manual effort required for image editing, particularly for casual photographers. The authors make the annotated dataset and code publicly available.\n\n---TOPIC---\nDistractor Prediction\nImage Enhancement\nComputer Vision\nDataset Creation\nSemi-Automatic Image Editing",
    "topics": [],
    "references": [
      {
        "citation": "[Kokaram, A. C. On missing data treatment for degraded video and film archives: A survey and a new bayesian approach. IEEE Trans. on Image Processing, 13(3):397–415, Mar. 2004.]"
      },
      {
        "citation": "[Alers, H., Liu, H., Redi, J., and Heynderickx, I. Studying the effect of optimizing the image quality in saliency regions at the expense of background content. Proc. SPIE, 7529, 2010.]"
      },
      {
        "citation": "[Liu, H., and Heynderickx, I. Studying the added value of visual attention in objective image quality metrics based on eye movement data. IEEE International Conf. on Image Processing (ICIP), Nov. 2009.]"
      },
      {
        "citation": "[Arbeláez, P., Pont-Tuset, J., Barron, J., Marques, F., and Malik, J. Multiscale combinatorial grouping. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2014.]"
      },
      {
        "citation": "[Avidan, S., and Shamir, A. Seam carving for content-aware image resizing. ACM Trans. on Graphics, 2007.]"
      },
      {
        "citation": "[Mairon, R., and Ben-Shahar, O. A closer look at context: From coxels to the contextual emergence of object saliency. European Conf. on Computer Vision (ECCV), 2014.]"
      },
      {
        "citation": "[Neumann, L., and Matas, J. Real-time scene text localization and recognition. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2012.]"
      },
      {
        "citation": "[Oliva, A., and Torralba, A. Modeling the shape of the scene: A holistic representation of the spatial envelope. International journal of computer vision, 42(3):145–175, 2001.]"
      },
      {
        "citation": "[Felzenszwalb, P. F., Girshick, R. B., McAllester, D., and Ramanan, D. Object Detection with Discriminatively Trained Part Based Models. IEEE Trans. on Pattern Analysis and Machine Intelligence (PAM), 32(9):1627–1645, 2010.]"
      },
      {
        "citation": "[Harel, J., Koch, C., and Perona, P. Graph-based visual saliency. Advances in Neural Information Processing Systems, 2007.]"
      }
    ],
    "author_details": [
      {
        "name": "Ohad Fried",
        "affiliation": "Princeton University",
        "email": "ohad@cs.princenton.edu"
      },
      {
        "name": "Eli Shechtman",
        "affiliation": "Adobe Research",
        "email": "elishe@adobe.com"
      },
      {
        "name": "Dan B Goldman",
        "affiliation": "Adobe Research",
        "email": "dgoldman@adobe.com"
      },
      {
        "name": "Adam Finkelstein",
        "affiliation": "Princeton University",
        "email": "af@cs.princenton.edu"
      }
    ]
  },
  {
    "title": "Supplementary Materials for Paper 1915\n---AUTHOR---\n(The paper does not list authors in the provided text)",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Tan_Learning_Graph_Structure_2015_CVPR_supplemental.pdf",
    "id": "Tan_Learning_Graph_Structure_2015_CVPR_supplemental",
    "abstract": "This supplementary material provides detailed information about Algorithms 1 and 2 used in the main paper, along with derivations of the main conclusions. Specifically, it elaborates on the stopping conditions for each algorithm, addresses inequality constraint handling within the subproblem optimization, and provides proofs for key theoretical results (Theorem 1 and Proposition 1). The document also includes a derivation of the Lagrangian dual for a specific problem.",
    "topics": [
      "Stochastic Optimization",
      "Support Vector Machines (SSVM)",
      "Lagrangian Duality",
      "Inequality Constraints",
      "Algorithm Design"
    ],
    "references": [],
    "author_details": []
  },
  {
    "title": "Adaptive As-Natural-As-Possible Image Stitching\n---AUTHOR---\nChung-Ching Lin\n---AUTHOR---\nSharathchandra U. Pankanti\n---AUTHOR---\nKarthikeyan Natesan Ramamurthy\n---AUTHOR---\nAleksandr Y. Aravkin",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Lin_Adaptive_As-Natural-As-Possible_Image_2015_CVPR_paper.pdf",
    "id": "Lin_Adaptive_As-Natural-As-Possible_Image_2015_CVPR_paper",
    "abstract": "The goal of image stitching is to create natural-looking mosaics free of artifacts. We propose a novel stitching method that uses a smooth stitching field over the entire target image, while accounting for all the local transformation variations. Computing the warp is fully automated and uses a combination of local homography and global similarity transformations, both of which are estimated with respect to the target. We mitigate the perspective distortion in the non-overlapping regions by linearizing the homography and slowly changing it to the global similarity. The proposed method is easily generalized to multiple images, and allows one to automatically obtain the best perspective in the panorama. It is also more robust to parameter selection, and hence more automated compared with state-of-the-art methods.\n\n---TOPIC---\nImage Stitching\nHomography\nSimilarity Transform\nPerspective Distortion\nPanorama Generation",
    "topics": [],
    "references": [],
    "author_details": [
      {
        "name": "Chung-Ching Lin",
        "affiliation": "IBM Thomas J. Watson Research Center",
        "email": "cclin@us.ibm.com"
      },
      {
        "name": "Sharathchandra U. Pankanti",
        "affiliation": "IBM Thomas J. Watson Research Center",
        "email": "sharath@us.ibm.com"
      },
      {
        "name": "Karthikeyan Natesan Ramamurthy",
        "affiliation": "IBM Thomas J. Watson Research Center",
        "email": "knatesa@us.ibm.com"
      },
      {
        "name": "Aleksandr Y. Aravkin",
        "affiliation": "IBM Thomas J. Watson Research Center",
        "email": "saravkin@us.ibm.com"
      }
    ]
  },
  {
    "title": "Visual Saliency Based on Multiscale Deep Features\n---AUTHOR---\nGuanbin Li\nYizhou Yu",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Li_Visual_Saliency_Based_2015_CVPR_paper.pdf",
    "id": "Li_Visual_Saliency_Based_2015_CVPR_paper",
    "abstract": "Visual saliency is a fundamental problem in cognitive and computational sciences. This paper proposes a novel computational model for visual saliency using multiscale deep features extracted by convolutional neural networks (CNNs). The model incorporates fully connected layers on top of CNNs at three different scales, a refinement method to enhance spatial coherence, and aggregation of salience maps from different image segmentation levels. A new large database of 4447 challenging images and pixelwise salience annotations (HKU-IS) is also introduced. Experimental results demonstrate state-of-the-art performance on public benchmarks, achieving significant improvements in F-Measure and mean absolute error.\n\n---TOPIC---\nVisual Salience\n---TOPIC---\nDeep Convolutional Neural Networks (CNNs)\n---TOPIC---\nMultiscale Feature Extraction\n---TOPIC---\nImage Segmentation\n---TOPIC---\nComputational Vision Models",
    "topics": [],
    "references": [],
    "author_details": [
      {
        "name": "Guanbin Li",
        "email": "Not available in the provided text."
      },
      {
        "name": "Yizhou Yu",
        "email": "Not available in the provided text."
      }
    ]
  },
  {
    "title": "Mind’s Eye: A Recurrent Visual Representation for Image Caption Generation\n---AUTHOR---\nXinlei Chen\nC. Lawrence Zitnick",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Chen_Minds_Eye_A_2015_CVPR_paper.pdf",
    "id": "Chen_Minds_Eye_A_2015_CVPR_paper",
    "abstract": "In this paper, we explore the bi-directional mapping between images and their sentence-based descriptions. Our approach utilizes a recurrent neural network to dynamically build a visual representation of the scene as a caption is being generated or read, allowing it to remember long-term visual concepts. The model can generate novel captions from images and reconstruct visual features from image descriptions. We evaluate our approach on several tasks, including sentence generation, sentence retrieval, and image retrieval, achieving state-of-the-art results in novel image description generation. The automatically generated captions are often preferred by humans, and the results are competitive with state-of-the-art methods on image and sentence retrieval tasks.\n\n---TOPICCS---\nImage Captioning\nRecurrent Neural Networks (RNNs)\nVisual Representation Learning\nBi-directional Mapping (Image-Text)\nLong-Term Memory in Neural Networks",
    "topics": [],
    "references": [],
    "author_details": [
      {
        "name": "Xinlei Chen",
        "affiliation": "Carnegie Mellon University",
        "email": "xinleic@cs.cmu.edu"
      },
      {
        "name": "C. Lawrence Zitnick",
        "affiliation": "Microsoft Research, Redmond",
        "email": "larryz@microsoft.com"
      }
    ]
  },
  {
    "title": "Understanding Image Structure via Hierarchical Shape Parsing\n---AUTHOR---\nXianming Liu\nRongrong Ji\nChanghu Wang\nWei Liu\nBineng Zhong\nThomas S. Huang",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Liu_Understanding_Image_Structure_2015_CVPR_paper.pdf",
    "id": "Liu_Understanding_Image_Structure_2015_CVPR_paper",
    "abstract": "Exploring image structure is a long-standing research subject in computer vision. This paper proposes a hierarchical shape parsing strategy to partition and organize image components into a hierarchical structure in the scale space. Image appearances are bundled into hierarchical parsing trees, and image descriptions are constructed via structural pooling, facilitating efficient matching between the parsing trees. The proposed method is applied to edge scale refinement and unsupervised “objectness” detection, demonstrating competitive performance with fewer proposals than state-of-the-art methods.\n\n---TOPICCS---\nHierarchical Shape Parsing\nImage Structure Understanding\nScale Space Representation\nUnsupervised Object Detection\nStructural Pooling",
    "topics": [],
    "references": []
  },
  {
    "title": "Ambient Occlusion via Compressive Visibility Estimation\n---AUTHOR---\nWei Yang\nYu Ji\nHaiting Lin\nYang Yang\nSing Bing Kang\nJingyi Yu",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Yang_Ambient_Occlusion_via_2015_CVPR_paper.pdf",
    "id": "Yang_Ambient_Occlusion_via_2015_CVPR_paper",
    "abstract": "This paper presents a novel computational imaging solution for recovering the ambient occlusion (AO) map of an object. AO measures the occlusion of ambient light caused by local surface geometry. Previous approaches require accurate surface geometry or a large number of images. This work adopts a compressive sensing framework that captures the object under strategically coded lighting directions, leveraging the unique properties of the incident illumination field (binary ray contributions and sparse distribution) to iteratively recover surface normals, albedo, and the visibility function from a small number of images. The approach is physically implemented using a light field probe and validated on synthetic and real data, achieving comparable accuracy to existing methods with significantly fewer images.\n\n---TOPICs---\nAmbient Occlusion (AO)\nCompressive Sensing\nComputational Imaging\nVisibility Function Recovery\nLight Field Probe",
    "topics": []
  },
  {
    "title": "Matching Bags of Regions in RGBD images\n---AUTHOR---\nHao Jiang",
    "authors": [],
    "file_path": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Jiang_Matching_Bags_of_2015_CVPR_paper.pdf",
    "id": "Jiang_Matching_Bags_of_2015_CVPR_paper"
  }
]