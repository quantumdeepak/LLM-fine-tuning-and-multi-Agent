[
  {
    "title": "Hollywood 3D: Recognizing Actions in 3D Natural Scenes",
    "authors": [
      "Centre for Vision",
      "Speech",
      "Signal Processing"
    ],
    "abstract": "Action recognition in unconstrained situations is a difﬁ-\ncult task, suffering from massive intra-class variations. It\nis made even more challenging when complex 3D actions\nare projected down to the image plane, losing a great deal\nof information. The recent emergence of 3D data, both in\nbroadcast content, and commercial depth sensors, provides\nthe possibility to overcome this issue. This paper presents\na new dataset, for benchmarking action recognition algo-\nrithms in natural environments, while making use of 3D in-\nformation. The dataset contains around 650 video clips,\nacross 14 classes.\nIn addition, two state of the art action recognition algo-\nrithms are extended to make use of the 3D data, and ﬁve new\ninterest point detection strategies are also proposed, that ex-\ntend to the 3D data. Our evaluation compares all 4 feature\ndescriptors, using 7 different types of interest point, over\na variety of threshold levels, for the Hollywood3D dataset.\nWe make the dataset including stereo video, estimated depth\nmaps and all code required to reproduce the benchmark re-\nsults, available to the wider community.\n1. Introduction\nThis paper presents a new 3D dataset for Action Recog-\nnition in the Wild. The detection and recognition of actions\nin natural settings is useful in a number of applications, in-\ncluding automatic video indexing and search, surveillance\nand assisted living. Benchmark datasets such as KTH [21]\nor Weizmann [2] have been invaluable in providing compar-\native benchmarks for competing approaches. However, high\nperformance rates are routinely reported on these staged\ndatasets and this suggests that they are reaching the end\nof their service to the community.\nMore recent datasets\nsuch as Hollywood [14] and Hollywood2 [16] attempt to\nprovide a more challenging problem and consist of actions\n“in the wild” consisting of video clips taken from a vari-\nety of Hollywood feature ﬁlms. These datasets presented a\nnew level of complexity to the recognition community, aris-\ning from the natural within-class variation of unconstrained\ndata, including unknown camera motion, viewpoint, light-\ning, background and actors, and variations in action scale,\nduration, style and number of participants. While this nat-\nural variability is one of the strengths of the data, the lack\nof structure or constraints make classiﬁcation an extremely\nchallenging task.\nFigure 1: Example frames of various action sequences from the\ndataset, showing the left viewpoint and depth streams. Darker re-\ngions of the depth image are closer to the camera. From top to\nbottom the actions are Eat, Hug, Kick, Kiss and Drive.\nIn this work, a new natural action dataset is introduced\ntermed Hollywood3D (see ﬁgure 1), it builds on the spirit\n2013 IEEE Conference on Computer Vision and Pattern Recognition\n1063-6919/13 $26.00 © 2013 IEEE\nDOI 10.1109/CVPR.2013.436\n3396\n2013 IEEE Conference on Computer Vision and Pattern Recognition\n1063-6919/13 $26.00 © 2013 IEEE\nDOI 10.1109/CVPR.2013.436\n3396\n2013 IEEE Conference on Computer Vision and Pattern Recognition\n1063-6919/13 $26.00 © 2013 IEEE\nDOI 10.1109/CVPR.2013.436\n3398\nof the existing Hollywood datasets but includes 3D infor-\nmation. This 3D information gives additional visual cue’s\nwhich can be used to help simplify the within-class vari-\nation of actions. Lighting variations are generally not ex-\npressed in depth data, and actor appearance differences are\neliminated (although differences in body shape remain).\nAdditionally, depth provides useful cues for background\nsegmentation, and occlusion detection.\nThe recent introduction of affordable 3D capture de-\nvices such as the Microsoft Kinect, has resulted in an ex-\nplosion of techniques based on 3D data. Furthermore, the\nuptake of 3D display technology in the home and Cin-\nema, has prompted television networks to begin broadcast\nof 3D programming, and ﬁlm studios to produce commer-\ncially available 3D ﬁlms. In this work, data is extracted\nfrom the latter commercially available sources, providing a\nnumber of advantages over self-captured data with a depth\nsensor[15, 5, 4]. The type of actions that you get in movies\nor “in the wild” is substantially different from the more con-\ntrived set-ups that exist in “lab-setting” datasets. Methods\ndesigned/trained on the latter rarely work on the former.\nCommercial data offers a richer range of actors, locations\nand lighting conditions than could be easily achieved in a\nlab and is one of the strengths of the Hollywood [14] and\nHollywood2 [16] datasets. Additionally, active depth sen-\nsors are often unable to function in direct sunlight, severely\nlimiting possible applications. Finally, 3D information pro-\nduced by active depth sensors tends to be much lower ﬁ-\ndelity than that available commercially, and is limited in\nterms of operational range. In addition to the release of\na new dataset, which incorporates both original video and\ndepth estimates, this paper provides baseline performance\nusing both depth and appearance, and the software neces-\nsary to reproduce these results.\nPrevious work on action recognition, has focused on the\nuse of feature points which can either be sampled densely or\nsparsely within the video. Sparse sampling reduces the im-\npact of large background regions while dense sampling can\ncapture context. However, all approaches sample from the\nspatio-temporal domain using visual appearance in x, y, and\nt. In this work, the additional dimension z is employed, and\nwe show how this depth information can be incorporated\nboth at the descriptor level, and while detecting regions of\ninterest, extending common Spatio-temporal Interest Point\ntechniques.\nThe remainder of the paper is structured as follows. The\nstate of the art in natural 2D action recognition is ﬁrst dis-\ncussed in section 2, followed by section 3 covering the data\nextraction process, with details of the dataset. Section 4 pro-\nvides a general overview of the action recognition method-\nology employed. Section 5 details the depth-aware spatio-\ntemporal interest point detection schemes, followed by ex-\ntensions for two state of the art feature descriptors in sec-\ntion 7. Results are provided with different combinations of\ninterest point and recognition schemes in sections 8. Finally\nsection 9 draws conclusions about the beneﬁts of depth data\nin natural action tasks, and the relative merits of the pre-\nsented approaches.\n2. Related Work\nThe majority of existing approaches to action recogni-\ntion focus on collections of local feature descriptors. These\ndescriptors can be applied sparsely, i.e. at areas detected as\nbeing “interest points”, or densely, using a regular sampling\nscheme.\nHowever, for reasons of scalability, the sparse\nsampling scheme is often favored.\nThese interest points\ndetect salient image locations, for example using separa-\nble linear ﬁlters [7] or spatio-temporal Harris corners [13].\nDescriptors are generated around these interest points in\na number of ways, including SIFT and SURF approaches\n[26, 22, 12], pixel gradients [7], Jet descriptors [21] or de-\ntection distributions and strengths [19, 9]. Focusing on in-\nterest points allows a sparse representation, for fast compu-\ntation, and reduces contamination of background regions.\nHowever, in unconstrained scenarios, the presence of dy-\nnamic backgrounds or signiﬁcant camera motion can lead\nto overwhelming numbers of uninformative detections.\nThese background detections can contribute in terms of\ncontext and some authors [10, 16] take advantage of this\nfact, modeling context directly. By performing a separate\nscene classiﬁcation stage, combined with prior knowledge\nof probable action contexts (for example the “Get Out Car”\naction is unlikely to occur indoors) recognition rates can be\nimproved. The approach of Wang et al. [25] demonstrated\nthat dense sampling of features provides combined action\nand context information, and generally outperforms sparse\ninterest points.\nGenerally the local features are accumulated over the\nsequence to form a histogram descriptor for the entire se-\nquence, which is then classiﬁed (often using an SVM).\nThis accumulation provides invariance to spatial and tem-\nporal translations, and changes in speed. An alternative ap-\nproach sometimes used, is to consider each frame in iso-\nlation, then to classify the video based on the sequence of\nframes. An example of this is assigning each frame to a\nstate in a Hidden Markov Model (HMM), then determin-\ning the most probable action for the observed sequence of\nstates [3]. This has the advantage that it accounts for the\ntemporal ordering of the features, but it can be difﬁcult to\ndetermine the correct structure of the HMM. For additional\ninformation on the current state of the art, refer to [18, 17].\n3. Extracting 3D Actions from Movies\nWith the emergence of High Deﬁnition DVD such as\nBluRayTMand the introduction of 3D displays into the con-\n3397\n3397\n3399\nsumer market, there has been a sharp rise in commercially\navailable 3D content. However, the subset that is useful for\ngenerating an action recognition dataset is still limited. A\ngreat deal of initially available 3D ﬁlms were constructed\nfrom the original 2D data, via post-processing techniques\nsuch as rotoscoping. Depth data extracted from these ﬁlms\nis less rich, lacking depth variations within objects, resem-\nbling a collection of card board cut-outs, and is fundamen-\ntally artiﬁcial, created for effect only. Additionally, ﬁlms\ngenerated entirely through CGI, such as “Monsters Inc.” are\nunlikely to provide transferable information on human ac-\ntions. For this dataset, we have focused on content captured\nusing commercial camera rigs such as James Cameron’s\nFusion Camera SystemTMor products from 3ality Technica.\nThese technologies produce 3D consumer content from real\nstereo cameras which can be used to reconstruct accurate\n3D depth maps.\nThe dataset was compiled from 14 ﬁlms 1 and is avail-\nable2. It contains over 650 manually labeled video clips\nacross 13 action classes, plus a further 78 clips represent-\ning the “NoAction”. Most 3D ﬁlms are too recent to have\npublicly available transcriptions, and subtitles alone rarely\noffer action cues, so automatic extraction techniques such\nas those employed by Marszalek et al. [16] are currently not\npossible. For this reason manual labeling was used which\nensures that all examples are well segmented from the car-\nrier movies. In addition to the action sequences, a collection\nof sequences containing no actions was also automatically\nextracted as negative data, while ensuring no overlap with\npositive classes.\nActions are temporally localized to the frame level, en-\nsuring non-discriminative data at the start and end of se-\nquences does not confuse training, and improving separa-\ntion of the NoAction class. The data is provided from both\nleft and right viewpoints at 1920 by 1080 resolution, at 24\nframes per second. In addition, reconstructed depth is pro-\nvided for all clips, at the same resolution and frame rate.\nDepth is reconstructed using the bilateral grid ﬁltering ap-\nproach described in [20]. If the right appearance stream is\nremoved from the dataset, it is possible to simulate the in-\nput data that would be provided by hybrid sensors like the\nKinect, albeit at a higher spatial, and lower depth resolution.\nArtifacts introduced by post processing are not considered,\nhowever it may be useful in future work to examine the be-\nhavior and consequences of such artifacts, with regards to\naction recognition.\nThe 14 ﬁlms comprising the dataset were split between\n1Avatar, Pirates Of the Caribbean: On Stranger Tides, Sanctum, Drive\nAngry, Spy Kids: All The Time In The World, Step Up 3D, Resident\nEvil: Afterlife, Fright Night, My Bloody Valentine, Tron: Legacy, A Very\nHarold and Kumar Christmas, The Three Musketeers, Final Destination 5\nand Underworld: Awakening\n2personal.ee.surrey.ac.uk/Personal/S.Hadfield/\nhollywood3d",
    "id": "80afe5177df7ffd5dda32e0ac833defa",
    "file_path": "/mnt/DATA/Glucoma/LLM/cvf_papers/CVPR/2013/Hadfield_Hollywood_3D_Recognizing_2013_CVPR_paper.pdf",
    "cited_papers": []
  },
  {
    "title": "Recovering Stereo Pairs from Anaglyphs",
    "authors": [
      "the matches",
      "selects the good matches (which deﬁnes the"
    ],
    "abstract": "An anaglyph is a single image created by selecting com-\nplementary colors from a stereo color pair; the user can\nperceive depth by viewing it through color-ﬁltered glasses.\nWe propose a technique to reconstruct the original color\nstereo pair given such an anaglyph. We modiﬁed SIFT-Flow\nand use it to initially match the different color channels\nacross the two views. Our technique then iteratively reﬁnes\nthe matches, selects the good matches (which deﬁnes the\n“anchor” colors), and propagates the anchor colors. We\nuse a diffusion-based technique for the color propagation,\nand added a step to suppress unwanted colors. Results on\na variety of inputs demonstrate the robustness of our tech-\nnique. We also extended our method to anaglyph videos by\nusing optic ﬂow between time frames.\n1. Introduction\nArguably, the ﬁrst 3D motion picture was shown in\n1889 by William Friese-Green, who used the separation of\ncolors from the stereo pair to generate color composites.\nGlasses with appropriate color ﬁlters are used to perceive\nthe depth effect. Such images generated by color separation\nare called anaglyphs. An example is shown in top-left part\nof Figure 1, where the color separation is red-cyan. Here,\nanaglyph glasses comprising red-cyan ﬁlters are used to per-\nceive depth. There is much legacy anaglyph content avail-\nable with no original color pairs.\nIn this paper, we show how we can reliably reconstruct\na good approximation of the original color stereo pair given\nits anaglyph. We assume the left-right color separation is\nred-cyan, which is the most common; the principles of our\ntechnique can be applied to other color separation schemes.\nGiven an anaglyph, recovering the stereo color pair is\nequivalent to transferring the intensities in the red channel\nto the right image and the intensities in the blue and green\nchannels to the left one. Unfortunately, the intensity distri-\nbutions between the three channels are usually very differ-\n1WILLOW project-team, Laboratoire d’Informatique de l’Ecole Nor-\nmale Sup´erieure, ENS/INRIA UMR 8548.\nent, with mappings of intensities typically being many-to-\nmany. This complicates the use of stereo and standard color\ntransfer methods. Since the input is essentially a stereo pair,\nwe have to handle occlusions as well.\nOur technical contributions are as follow:\n• A completely automatic technique for reconstructing\nthe stereo pair from an anaglyph (image or video).\n• Non-trivial extensions to SIFT-Flow [14] to handle\nregistration across color channels.\n• Non-trivial extensions to Levin et al.’s [11] coloriza-\ntion technique to handle diffusion of only speciﬁc\ncolor channels per view, and use of longer range inﬂu-\nence. We also added the step of suppressing visually\ninappropriate colors at disocclusions.\nWe made the following assumptions:\n• There are no new colors in the disoccluded regions.\n• The information embedded in the anaglyph is enough\nto reconstruct full rgb2.\n• The images are roughly rectiﬁed (with epipolar devia-\ntion of up to ±25 pixels).\nAs we demonstrate through a variety of results, our tech-\nnique is robust to inadvertent asymmetric occlusion in one\ncamera (e.g., part of a ﬁnger in one view but not in the other)\nas well as blurry and low-light scenes.\n2. Related work\nAs far as we know, there are two approaches for generat-\ning stereo pairs from anaglyphs. The ﬁrst is a tool called\nDeanaglyph3.\nHowever, it requires as input not just the\nanaglyph, but also one color image. Dietz [3] proposed a\nmethod to recover the original images from anaglyphs cap-\ntured using a modiﬁed camera with color ﬁlters. There is\nunfortunately insufﬁcient information in the paper on how\nhis technique works; however, his results show that the col-\nors often do not match across the reconstructed stereo pair.\n2Certain types of anaglyphs, such as the “optimized” anaglyph, throws\naway the red channel; it fakes the red channel of the left image by combin-\ning green and blue channels.\n3http://www.3dtv.at/Knowhow/DeAnaglyph_en.aspx\n2013 IEEE Conference on Computer Vision and Pattern Recognition\n1063-6919/13 $26.00 © 2013 IEEE\nDOI 10.1109/CVPR.2013.44\n289\n2013 IEEE Conference on Computer Vision and Pattern Recognition\n1063-6919/13 $26.00 © 2013 IEEE\nDOI 10.1109/CVPR.2013.44\n289\n2013 IEEE Conference on Computer Vision and Pattern Recognition\n1063-6919/13 $26.00 © 2013 IEEE\nDOI 10.1109/CVPR.2013.44\n289\nFigure 1. Our pipeline: Given an anaglyph, we iteratively ﬁnd cor-\nrespondences between the left and right images and recolorize the\ntwo images based on these correspondences.\nAnother paper which is closely related is that of Bando\net al. [1]. They use an RGB color ﬁlter on the camera lens\naperture to obtain three known shifted views. They propose\nan algorithm to match the corresponding pixels across the\nthree channels by exploiting the tendency of colors in nat-\nural images to form elongated clusters in the RGB space\n(color lines model of [16]), with the assumption of small\ndisparities (between -5 to 10 pixels only).\nWe now brieﬂy survey topics relevant to our approach:\ncolorization, extraction of dense correspondences, and color\ntransfer.\n2.1. Colorization\nIn our approach, we use a diffusion-based colorization\nmethod to propagate the colors of reliable correspondences\nto other pixels. Colorization methods assume that the color\nof some input pixels is known and infer the color of the\nother pixels [11, 15, 22, 12, 20, 9]. Most colorization meth-\nods also assume, explicitly or implicitly, that the greyscale\nvalues are known and used as part of the optimization. For\nanaglyphs, the luminance values are not available.\nOur method is based on diffusion of reliably transferred\ncolors using a graph construct (e.g., diffusing reliable red\nintensities in the left view). Our approach is related to [11].\nThey assume that the input colors given by a user are ap-\nproximate and may be reﬁned depending on the image con-\ntent. In our work, this assumption is not required. Un-\nlike [11], since we work in rgb space directly, we must pre-\nserve edges while diffusing the color (especially over shad-\nowed regions). We use longer-range information (for each\npixel, within a 21 × 21 centered window) to diffuse colors.\nAnother popular approach to colorize an image is to use\nthe geodesic of an image to connect a pixel with unknown\ncolor to an input pixel [22]. This type of approach works\nwell in practice but implicitly assumes the number of input\npixels is small, which is not true in our case.\n2.2. Dense correspondences\nOur approach relies on ﬁnding dense good correspon-\ndences between both images. Our approach adapted the\ntechniques of SIFT-ﬂow [14] and optical ﬂow [13] to ﬁt the\nrequirements of our problem.\nThere are many methods designed to compute dense cor-\nrespondences between images. Most stereo matching meth-\nods have been proven to be successful when the cameras\nare radiometrically similar [19]. These are not applicable in\nour case, since we are matching the red channel in one view\nwith cyan in the other. There are also approaches that han-\ndle changes in exposure [10, 7] or illumination [6]. How-\never, such approaches typically assume a bijective map-\nping between the two intensity maps, which is violated for\nanaglyphs.\nBando et al. [1] use Markov random ﬁeld to register\nacross color channels. As mentioned earlier, it is important\nto note that their assumption of local cluster shape similar-\nity based on the color lines model [16] is facilitated by small\ndisparities.\nMore recently, HaCohen et al. [5] have proposed a color\ntransfer method based on local correspondence between\nsimilar images. They look for correspondences between im-\nages with different light conditions and taken by different\ncameras. Unlike graph matching based approaches, their\napproach ﬁnds correspondences with no spatial constraints.\nIn our work, the two images to be reconstructed are highly\nspatially related; ignoring this information leads to incorrect\ncolorization.\n2.3. Color transfer\nA related subject of interest is color transfer, which is\nthe process of modifying the color of a target image to\nmatch the color characteristics of a source image. Most ap-\nproaches [18, 21, 4, 17] assume that at least the target image\nis in color (or at least greyscale). As a result, they are not\ndirectly applicable in our case. These approaches tend to\nmake use of global statistics of the image and thus may be\nless effective in recovering the true color locally.\n3. Overview of our approach\nOur approach to recover the stereo pair from an anaglyph\nis to iteratively ﬁnd dense good correspondences between\nthe two images and recolorizing them based on these cor-\nrespondences. As a post-processing stage, we detect “in-\ncompatible” colors in unmatched regions (deﬁned as colors\nthat exist in one image but not the other) and assign them to\nthe nearest known colors. This process prevents new colors\nfrom appearing in the occluded regions.\nFigure 1 summarizes our process. We use a modiﬁed ver-\nsion of SIFT-Flow to obtain an initial set of correspondences\nbetween the two images. We then recolorize the images\n290\n290\n290\nGround truth\nSIFT-Flow\nASIFT-Flow (ours)\nFigure 2. Comparison between the original SIFT-Flow and our ver-\nsion. Notice the incorrect colors transferred for the original SIFT-\nFlow in the darker lower region.",
    "id": "caed722c8ae599b14962d74a3f7b06f7",
    "file_path": "/mnt/DATA/Glucoma/LLM/cvf_papers/CVPR/2013/Joulin_Recovering_Stereo_Pairs_2013_CVPR_paper.pdf",
    "cited_papers": []
  },
  {
    "title": "Enriching Texture Analysis with Semantic Data",
    "authors": [
      "Tim Matthews",
      "Mark S. Nixon",
      "Mahesan Niranjan"
    ],
    "abstract": "We argue for the importance of explicit semantic mod-\nelling in human-centred texture analysis tasks such as re-\ntrieval, annotation, synthesis, and zero-shot learning.\nTo this end, low-level attributes are selected and used\nto deﬁne a semantic space for texture. 319 texture classes\nvarying in illumination and rotation are positioned within\nthis semantic space using a pairwise relative comparison\nprocedure. Low-level visual features used by existing tex-\nture descriptors are then assessed in terms of their corre-\nspondence to the semantic space. Textures with strong pres-\nence of attributes connoting randomness and complexity are\nshown to be poorly modelled by existing descriptors.\nIn a retrieval experiment semantic descriptors are shown\nto outperform visual descriptors. Semantic modelling of\ntexture is thus shown to provide considerable value in both\nfeature selection and in analysis tasks.\n1. Introduction\nVisual texture is an important cue in numerous processes\nof human cognition. It is known to be used in the separation\nof ‘ﬁgure’ from ‘ground’ [13], as a prompt in object recog-\nnition [22], to infer shape and pose [5], as well as in many\nother aspects of scene understanding. Over eons of human\nexistence this importance has led to the development of a\nrich lexicon suitable for concise description of texture. We\nmay speak of fractured earth, or of a rippling lake, and in\ndoing so are able to convey considerable information about\nthe surface and appearance of these objects.\nAlthough computational texture analysis has achieved\nﬁne results over recent decades, there still remains a dis-\nparity between the visual and semantic spaces of texture –\nthe so-called semantic gap. Computational approaches usu-\nally operate on the basis of a priori notions of texture not\nnecessarily tied to human experience. This means they are\noften unsuitable for applications requiring closer or more\nintuitive human interaction, such as content-based image re-\ntrieval, texture synthesis and description, or zero-shot learn-\ning, where a classiﬁcation system is taught new categories\nwithout having to observe them.\nOur work seeks to bridge this semantic gap for texture,\nand acts to unify separate research efforts into structuring\nthe semantic [1] and visual [6, 8, 23] texture spaces, and\ninto robustly identifying correspondences between semantic\nand visual data [21]. Separate semantic modelling has been\nshown to improve retrieval of natural scenes [29] and gait\nsignatures [26], and indoor-outdoor classiﬁcation of pho-\ntographs [27]. In this paper we outline a semantic mod-\nelling of texture, allowing it to be described, synthesised,\nand retrieved using ﬁne-grained high-level semantic con-\nstructs rather than solely using low-level visual properties.\nAs well as the clear beneﬁts this has for human-computer\ninteraction, the semantic data collected is a rich source of in-\nformation in its own right. Humans are capable of analysing\ntexture in a way resistant to noise, and invariant to illumi-\nnation, rotation, and scale [10, 30]. It is of tremendous ad-\nvantage to learn – either from human-provided labels, or\nfrom investigation of the underlying biological mechanisms\n– methods of image analysis that are similarly robust.\nTexture in particular provides interesting challenges of\nits own in part because it has historically proven so difﬁ-\ncult to deﬁne. We sidestep this thorny issue by adopting a\nsubjective deﬁnition of texture embedded in human experi-\nence. Because our task involves tying some visual texture\nspace to a semantic space borne from human interpretation\nof that visual space, it is ﬁtting to adopt a deﬁnition of tex-\nture derived from human perception. In this sense texture\nis anything describable by constructions from our semantic\nspace and emerges as a natural consequence of our eventual\ndeﬁnition of that space. This semantic characterisation of\ntexture allows us to address the problem of feature selection\nin a principled way, due to hundreds of thousands of years\nof embodiment within a world of diverse and abundant tex-\nture resulting in an evolved language which provides a nat-\nural balancing between expressiveness and efﬁciency.\nThe main contributions of this paper are:\n2013 IEEE Conference on Computer Vision and Pattern Recognition\n1063-6919/13 $26.00 © 2013 IEEE\nDOI 10.1109/CVPR.2013.165\n1246\n2013 IEEE Conference on Computer Vision and Pattern Recognition\n1063-6919/13 $26.00 © 2013 IEEE\nDOI 10.1109/CVPR.2013.165\n1246\n2013 IEEE Conference on Computer Vision and Pattern Recognition\n1063-6919/13 $26.00 © 2013 IEEE\nDOI 10.1109/CVPR.2013.165\n1246\n2013 IEEE Conference on Computer Vision and Pattern Recognition\n1063-6919/13 $26.00 © 2013 IEEE\nDOI 10.1109/CVPR.2013.165\n1248\n2013 IEEE Conference on Computer Vision and Pattern Recognition\n1063-6919/13 $26.00 © 2013 IEEE\nDOI 10.1109/CVPR.2013.165\n1248\n• A publicly-available1 labelling of over 300 classes of\ntexture from the Outex dataset. The design and format\nof this labelling is described in Section 2.\n• An assessment of how well a selection of visual texture\ndescriptors are able to capture this semantic data. We\nshow how textures may be ranked according to both\ntheir semantic labels and their visual features in Sec-\ntion 3, and then describe the experiment used to com-\npare these rankings in Section 4.\n• A demonstration of the beneﬁts of explicit semantic\nmodelling for texture retrieval, given in Section 5.\nWe ﬁnish with discussion of our results in Section 6.\n2. Semantic space of texture\nWe choose to construct our semantic space using at-\ntributes, low-level visual qualities – often adjectives –\nshared across objects. In this section we create an expres-\nsive but efﬁcient lexicon of attributes to make up our se-\nmantic space, select a dataset of texture from which we will\nderive our visual space, and ﬁnally obtain ‘ground truth’ la-\nbellings from human subjects so that we may discover cor-\nrespondences between these two spaces.\nAttributes have received much attention in recent liter-\nature in computer vision, particularly within object recog-\nnition. Their use permits a shift in perspective from the\ntraditional approach of object recognition in which object\nclasses themselves are atomic units of recognition. They\nallow the association of visual data with shared low-level\nqualities, facilitating efﬁcient class-level learning and gen-\neralisation [3, 14], and they provide a means for intuitive\nand ﬁne-grained description, such as when describing un-\nusual features of an object, or in stating the ways in which\none object is similar to another. Farhadi et al. [4] state that\nattributes allow us to “shift the goal of recognition from\nnaming to describing”.\nAttributes have been found to be particularly appropri-\nate for domains in which key features exist along continua,\nsuch as in biometrics [12, 24, 26] and scene classiﬁca-\ntion [20, 25]. We see texture as being ill-suited to strict\ncategorisation: key properties in which texture has been\nstated to vary include its coarseness, linearity, and regu-\nlarity [15, 28], all of which may be expected to vary contin-\nuously. Texture is particularly suitable for description with\nattributes as they may be readily sourced from the rich lex-\nicon that has evolved in order to describe it.\nNumerous elegant insights into the nature of the English-\nlanguage texture lexicon were made by Bhushan et al. [1],\nwho asked subjects to cluster 98 texture adjectives accord-\ning to similarity, without access to visual data. From the\n1www.ecs.soton.ac.uk/˜tm1e10/texture.html\nCluster interpretation\nSample words\nLinear orientation\nfurrowed, lined, pleated\nCircular orientation\ncoiled, ﬂowing, spiralled\nWoven structure\nknitted, meshed, woven\nWell-ordered\nregular, repetitive, uniform",
    "id": "a87ed631bc076ab60719dfdfffa15880",
    "file_path": "/mnt/DATA/Glucoma/LLM/cvf_papers/CVPR/2013/Matthews_Enriching_Texture_Analysis_2013_CVPR_paper.pdf",
    "cited_papers": []
  },
  {
    "title": "Detection- and Trajectory-Level Exclusion in Multiple Object Tracking",
    "authors": [
      "1Department of Computer Science",
      "TU Darmstadt"
    ],
    "abstract": "When tracking multiple targets in crowded scenarios,\nmodeling mutual exclusion between distinct targets be-\ncomes important at two levels: (1) in data association,\neach target observation should support at most one trajec-\ntory and each trajectory should be assigned at most one\nobservation per frame; (2) in trajectory estimation, two\ntrajectories should remain spatially separated at all times\nto avoid collisions.\nYet, existing trackers often sidestep\nthese important constraints. We address this using a mixed\ndiscrete-continuous conditional random ﬁeld (CRF) that ex-\nplicitly models both types of constraints: Exclusion between\nconﬂicting observations with supermodular pairwise terms,\nand exclusion between trajectories by generalizing global\nlabel costs to suppress the co-occurrence of incompatible\nlabels (trajectories). We develop an expansion move-based\nMAP estimation scheme that handles both non-submodular\nconstraints and pairwise global label costs. Furthermore,\nwe perform a statistical analysis of ground-truth trajecto-\nries to derive appropriate CRF potentials for modeling data\nﬁdelity, target dynamics, and inter-target occlusion.\n1. Introduction\nThe task of visual multi-target tracking is to recover the\nspatio-temporal trajectories of a (usually unknown) number\nof targets from a video sequence. Tracking multiple targets\n– often people or vehicles – has a wide range of applications\nranging from robotics to video surveillance. Even though\nthe ﬁeld has made tremendous progress since the early\nworks [e.g., 10], modern systems still have clear limitations,\nespecially as the observed scenes get more crowded. This\nis not entirely surprising, since the solution space grows\nrapidly as the number of visible targets and the length of\ntheir trajectories increases. Moreover, physical limits man-\ndate a growing number of constraints (such as mutual exclu-\nsion) as more targets are in close proximity to each other.\nTracking in realistic sequences is further complicated by\nbackground clutter, poor contrast, and partial or full occlu-\nsions, such as from other targets. Tracking-by-detection ap-\nproaches that rely on powerful object (class) detectors are\n(a)\n(b)\nFigure 1. Typical failure cases (top) are addressed with the pro-\nposed discrete-continuous CRF (bottom): Detections are forced to\ntake on different labels (a) and physically overlapping trajectories\nare suppressed even if they do not share detections (b).\nthus becoming increasingly popular [5, 13, 21, 23, 25, 26].\nIn this case, targets are detected independently in each\nframe with an ofﬂine-trained object detector. This not only\naddresses adverse imaging conditions, but also reduces drift\nand allows to bridge severe occlusions and other temporary\nloss of evidence. We follow this approach here.\nA large class of trackers aims to further improve robust-\nness by processing entire frame batches, rather than infer-\nring the current state solely from the track history [e.g.,\n3, 5, 16, 26]. While this may lead to potential contradictions\nin frames that occur in different batches and to a mild time\nlag, the crucial advantage is greater reliability, since longer\ntime windows afford both more data and stronger models.\nBatch-type multi-target trackers typically formulate a joint\nenergy function for all targets in all frames.\nWe can distinguish two categories of batch approaches:\nThe vast majority focuses on purely discrete optimization\nfor solving either data association [23, 26, etc.] or trajec-\ntory estimation [e.g., 5]. This allows one to encode com-\nplex constraints, including inter-object exclusion, in a natu-\nral way. The disadvantage is that the trajectories need to be\ndiscretized themselves, hence the necessarily ﬁnite spatial\nresolution can limit tracking performance and lead to visible\n2013 IEEE Conference on Computer Vision and Pattern Recognition\n1063-6919/13 $26.00 © 2013 IEEE\nDOI 10.1109/CVPR.2013.472\n3680\n2013 IEEE Conference on Computer Vision and Pattern Recognition\n1063-6919/13 $26.00 © 2013 IEEE\nDOI 10.1109/CVPR.2013.472\n3680\n2013 IEEE Conference on Computer Vision and Pattern Recognition\n1063-6919/13 $26.00 © 2013 IEEE\nDOI 10.1109/CVPR.2013.472\n3682\nartifacts. Moreover, exclusion is only handled either at the\ndata-association level [23, 26], or at the trajectory level [5].\nThe second group of methods uses alternative state spaces,\nsuch as purely continuous [3] or mixed discrete-continuous\n[4] formulations. While these allow estimating target trajec-\ntories in continuous space, they have other drawbacks. [3]\nneeded to employ an optimization scheme with ad-hoc jump\nmoves; [4] used the label cost framework of [12], which is\nnot designed for imposing exclusion constraints.\nIn this paper, we propose a mixed discrete-continuous\nconditional random ﬁeld (CRF) for multi-target tracking\nthat aims to combine the advantages of continuous-space\ntrajectory estimation with the advantages of discrete meth-\nods for enforcing exclusion constraints. We speciﬁcally ad-\ndress mutual exclusion both at the data-association and at\nthe trajectory level (cf. Fig. 1). We thus go beyond pre-\nvious discrete-continuous trackers [4] that do not perform\nexplicit exclusion reasoning, and beyond previous discrete\napproaches that model exclusion only at the trajectory [5]\nor at the data-association level [26].\nWe make the following contributions: (i) We extend the\nglobal label subset costs of [12] to pairwise label costs that\nallow penalizing the co-occurrence of competing labels in\nthe solution; (ii) we show how pairwise co-occurrence label\ncosts can be used to model trajectory exclusion for multi-\ntarget tracking; (iii) we enforce physically plausible data\nassociation with non-submodular pairwise constraints; (iv)\nwe propose an iterative MAP estimation scheme based on\nexpansion moves for the resulting non-submodular multi-\nlabel CRF energy with pairwise label costs; and (v) we an-\nalyze the statistical properties of real trajectories and obser-\nvations on ground truth data to derive CRF potentials for\nvarious model components (data ﬁdelity, dynamic model,\nocclusion duration). Together, these advances yield a more\nfaithful and more accurate model of multi-target tracking,\nwhich nevertheless remains tractable and delivers improved\ntracking results. To the best of our knowledge our approach\nis the ﬁrst to combine both unique data association of indi-\nvidual observations and physical collision-avoidance at the\ntrajectory level in a common model.\n2. Related Work\nVisual tracking has been an ongoing research topic for\ndecades, and a full literature review is beyond our scope. In\nthe presence of a single target, tracking can be performed\nby estimating the target location and motion in every frame\nand building the trajectory through interpolation [17]. In the\npresence of multiple targets an additional challenge arises,\noften referred to as data association: each detection must\nbe assigned a target identiﬁer or discarded as a false alarm.\nFiltering approaches, such as JPDA [15] or particle ﬁlters\n[8, 22], estimate the state and association online, i.e. by only\nconsidering the past states and present observations.\nAlthough online processing is desirable for time critical\napplications, batch approaches have become increasingly\npopular due to their superior robustness [3, 5, 16, 21, 26].\nTo keep the optimization tractable, the objective can be dis-\ncretized and (near-)optimal solutions can be obtained by lin-\near programming relaxations [5, 16] or computing the max-\nimal ﬂow in a network graph [21, 23, 26]. Constraints on\ndata association can also be mapped to other graph prob-\nlems, for which efﬁcient (approximate) algorithms exist\n[9, 25]. To overcome the drawback of discretization, [3] for-\nmulates the objective entirely in the continuous domain and\ncombines gradient descent and greedy jump moves for opti-\nmization. The mixed discrete-continuous model of [4] pre-\nserves the beneﬁts of a continuous trajectory space, while\nallowing for discrete data association. Trajectory level con-\nstraints are formulated as global label costs, and optimized\nwith a graph cut-based discrete-continuous scheme [12].\nMutual exclusion between targets, i.e. a term that pe-\nnalizes or entirely disallows solutions where two or more\ntargets collide, is a crucial property of multi-target track-\ning.\nApproaches that focus on data association [e.g.,\n21, 23, 26] usually represent the state space of target trajec-\ntories through the underlying detections. While this allows\na one-to-one mapping between each detection and each tra-\njectory, situations where the data is missing are not captured\nproperly, hence two trajectories may in fact intersect. Grid-\nbased methods [e.g., 5] explicitly model mutual exclusion\nbetween target locations at the trajectory level by impos-\ning linear constraints. However, such a discrete grid is a\nsomewhat crude approximation of the continuous trajectory\nspace. A continuous mutual exclusion term [3] leads to a\nnon-convex objective that is optimized with ad-hoc jumps.\nIn the present work we follow the basic idea of a mixed\ndiscrete-continuous representation [4]. Aside from repre-\nsenting trajectories in continuous space, this allows us to\nimpose mutual exclusion simultaneously at the data associ-\nation and at the trajectory level using the discrete part of the\nmodel. While these constraints go beyond the capabilities\nof the label cost optimization framework of [12], we show\nhow it can be extended appropriately.\n3. CRF Model",
    "id": "fd6f8b792fcfefdc0297375f93e43115",
    "file_path": "/mnt/DATA/Glucoma/LLM/cvf_papers/CVPR/2013/Milan_Detection-_and_Trajectory-Level_2013_CVPR_paper.pdf",
    "cited_papers": []
  },
  {
    "title": "Bottom-up Segmentation for Top-down Detection",
    "authors": [
      "1TTI Chicago"
    ],
    "abstract": "In this paper we are interested in how semantic segmen-\ntation can help object detection.\nTowards this goal, we\npropose a novel deformable part-based model which ex-\nploits region-based segmentation algorithms that compute\ncandidate object regions by bottom-up clustering followed\nby ranking of those regions. Our approach allows every\ndetection hypothesis to select a segment (including void),\nand scores each box in the image using both the traditional\nHOG ﬁlters as well as a set of novel segmentation features.\nThus our model “blends” between the detector and segmen-\ntation models. Since our features can be computed very efﬁ-\nciently given the segments, we maintain the same complex-\nity as the original DPM [14]. We demonstrate the effective-\nness of our approach in PASCAL VOC 2010, and show that\nwhen employing only a root ﬁlter our approach outperforms\nDalal & Triggs detector [12] on all classes, achieving 13%\nhigher average AP. When employing the parts, we outper-\nform the original DPM [14] in 19 out of 20 classes, achiev-\ning an improvement of 8% AP. Furthermore, we outperform\nthe previous state-of-the-art on VOC’10 test by 4%.\n1. Introduction\nOver the past few years, we have witnessed a push to-\nwards holistic approaches that try to solve multiple recog-\nnition tasks jointly [29, 6, 20, 18, 33]. This is important\nas information from multiple sources should facilitate scene\nunderstanding as a whole. For example, knowing which ob-\njects are present in the scene should simplify segmentation\nand detection tasks. Similarly, if we can detect where an ob-\nject is, segmentation should be easier as only ﬁgure-ground\nsegmentation is necessary. Existing approaches typically\ntake the output of a detector and reﬁne the regions inside\nthe boxes to produce image segmentations [22, 5, 1, 14].\nAn alternative approach is to use the candidate detections\nproduced by state-of-the-art detectors as additional features\nfor segmentation. This simple approach has proven very\nsuccessful [6, 19] in standard benchmarks.\nIn contrast, in this paper we are interested in exploit-\ning semantic segmentation in order to improve object de-\ntection. While bottom-up segmentation has been often be-\nlieved to be inferior to top-down object detectors due to its\nfrequent over- and under- segmentation, recent approaches\n[8, 1] have shown impressive results in difﬁcult datasets\nsuch as PASCAL VOC challenge. Here, we take advantage\nof region-based segmentation approaches [7], which com-\npute a set of candidate object regions by bottom-up cluster-\ning, and produce a segmentation by ranking those regions\nusing class speciﬁc rankers. Our goal is to make use of these\ncandidate object segments to bias sliding window object\ndetectors to agree with these regions. Importantly, unlike\nthe aforementioned holistic approaches, we reason about all\npossible object bounding boxes (not just candidates) to not\nlimit the expressiveness of our model.\nDeformable part-based models (DPM) [14] and its vari-\nants [2, 35, 10], are arguably the leading technique to object\ndetection 1. However, so far, there has not been many at-\ntempts to incorporate segmentation into DPMs. In this pa-\nper we propose a novel deformable part-based model, which\nexploits region-based segmentation by allowing every de-\ntection hypothesis to select a segment (including void) from\na small pool of segment candidates. Towards this goal, we\nderive simple features, which can capture the essential in-\nformation encoded in the segments. Our detector scores\neach box in the image using both the traditional HOG ﬁlters\nas well as the set of novel segmentation features. Our model\n“blends” between the detector and the segmentation models\nby boosting object hypotheses on the segments. Further-\nmore, it can recover from segmentation mistakes by exploit-\ning a powerful appearance model. Importantly, as given\nthe segments we can compute our features very efﬁciently,\nour approach has the same computational complexity as the\noriginal DPM [14].\nWe demonstrate the effectiveness of our approach in\nPASCAL VOC 2010, and show that when employing only a\nroot ﬁlter our approach outperforms Dalal & Triggs detec-\ntor [12] by 13% AP, and when employing parts, we outper-\nform the original DPM [14] by 8%. Furthermore, we out-\nperform the previous state-of-the-art on VOC2010 by 4%.\n1Poselets [4] can be shown to be very similar in spirit to DPMs\n2013 IEEE Conference on Computer Vision and Pattern Recognition\n1063-6919/13 $26.00 © 2013 IEEE\nDOI 10.1109/CVPR.2013.423\n3292\n2013 IEEE Conference on Computer Vision and Pattern Recognition\n1063-6919/13 $26.00 © 2013 IEEE\nDOI 10.1109/CVPR.2013.423\n3292\n2013 IEEE Conference on Computer Vision and Pattern Recognition\n1063-6919/13 $26.00 © 2013 IEEE\nDOI 10.1109/CVPR.2013.423\n3294\nWe believe that these results will encourage new research\non bottom-up segmentation as well as hybrid segmentation-\ndetection approaches, as our paper clearly demonstrates the\nimportance of segmentation for object detection.\nIn the remainder of the paper, we ﬁrst review related\nwork and then introduce our novel deformable part-based\nmodel, which we call segDPM. We then show our experi-\nmental evaluation and conclude with future work.\n2. Related Work\nDeformable part-based model [14] and its variants have\nbeen proven to be very successful in difﬁcult object detec-\ntion benchmarks such as PASCAL VOC challenge. Several\napproaches have tried to augment the level of supervision\nin these models. Azizpour et al. [2] use part annotations\nto help clustering different poses as well as to model oc-\nclusions. Hierarchical versions of these models have also\nbeen proposed [35], where each part is composed of a set\nof sub-parts. The relative rigidity of DPMs has been alle-\nviated in [10] by leveraging a dictionary of shape masks.\nThis allows a better treatment of variable object shape. De-\nsai et al. [13] proposed a structure prediction approach to\nperform non-maxima suppression in DPMs which exploits\npairwise relationships between multi-class candidates. The\ntree structure of DPMs allows for fast inference but can suf-\nfer from problems such as double counting observations.\nTo mitigate this, [27] consider lateral connections between\nhigh resolution parts.\nIn the past few years, a wide variety of segmentation\nalgorithms that employ object detectors as top-down cues\nhave been proposed. This is typically done in the form of\nunary features for an MRF [19], or as candidate bounding\nboxes for holistic MRFs [33, 21]. Complex features based\non shape masks were exploited in [33] to parse the scene\nholistically in terms of the objects present in the scene, their\nspatial location as well as semantic segmentation. In [26],\nheads of cats and dogs are detected with a DPM, and seg-\nmentation is performed using a GrabCut-type method. By\ncombining top-down shape information from DPM parts\nand bottom-up color and boundary cues, [32] tackle seg-\nmentation and detection task simultaneously and provide\nshape and depth ordering for the detected objects. Dai et al.\n[11] exploit a DPM to ﬁnd a rough location for the object\nof interest and reﬁne the detected bounding box according\nto occlusion boundaries and color information. [25] ﬁnd\nsilhouettes for objects by extending or reﬁning DPM boxes\ncorresponding to a reliably detectable part of an object.\nDPMs provide object-speciﬁc cues, which can be ex-\nploited to learn object segmentations [3]. In [24], masks\nfor detected objects are found by employing a group of seg-\nments corresponding to the foreground region. Other object\ndetectors have been used in the literature to help segmenting\nobject regions. For instance, while [4] ﬁnds segmentations\nfor people by aligning the masks obtained for each Poselet\n[4], [23] integrates low level segmentation cues with Pose-\nlets in a soft manner.\nThere are a few attempts to use segments/regions to im-\nprove object detection. Gu et al. [17] apply hough trans-\nform for a set of regions to cast votes on the location of\nthe object. More recently, [28] learn object shape model\nfrom a set of contours and use the learned model of con-\ntours for detection. In contrast, in this paper we proposed a\nnovel deformable-part based model, which allows each de-\ntection hypothesis to select candidate segments. Simple fea-\ntures express the fact that the detections should agree with\nthe segments. Importantly, these features can be computed\nvery efﬁciently, and thus our approach has the same com-\nputational complexity as DPM [14].\n3. Semantic Segmentation for Object Detection\nWe are interested in utilizing semantic segmentation to\nhelp object detection.\nIn particular, we take advantage\nof region-based segmentation approaches, which compute\ncandidate object regions by bottom-up clustering, and rank\nthose regions to estimate a score for each class. Towards\nthis goal we frame detection as an inference problem, where\neach detection hypothesis can select a segment from a pool\nof candidates (those returned from the segmentation as well\nas void). We derive simple features, which can be computed\nvery efﬁciently while capturing most information encoded\nin the segments. In the remainder of the section, we ﬁrst dis-\ncuss our novel DPM formulation. We then deﬁne our new\nsegment-based features and discuss learning and inference\nin our model.\n3.1. A Segmentation-Aware DPM\nFollowing [14], let p0 be a random variable encoding the\nlocation and scale of a bounding box in an image pyramid\nas well as the mixture component id. As shown in [14] a\nmixture model is necessary in order to cope with variabil-\nity in appearance as well as the different aspect ratios of\nthe training examples. Let {pi}i=1,··· ,P be a set of parts\nwhich encode bounding boxes at double the resolution of\nthe root. Denote with h the index over the set of candi-\ndate segments returned by the segmentation algorithm. We\nframe the detection problem as inference in a Markov Ran-\ndom Field (MRF), where each root ﬁlter hypothesis can se-\nlect a segment from a pool of candidates. We thus write the\nscore of a conﬁguration as\nE(p, h)\n=\nP\n�\ni=0",
    "id": "bbb99fda4a64693c9f9fef0a603a0f00",
    "file_path": "/mnt/DATA/Glucoma/LLM/cvf_papers/CVPR/2013/Fidler_Bottom-Up_Segmentation_for_2013_CVPR_paper.pdf",
    "cited_papers": []
  },
  {
    "title": "Lp-norm IDF for Large Scale Image Search",
    "authors": [
      "Liang Zheng1",
      "Shengjin Wang1",
      "Ziqiong Liu1",
      "Qi Tian2"
    ],
    "abstract": "The Inverse Document Frequency (IDF) is prevalently u-\ntilized in the Bag-of-Words based image search. The basic\nidea is to assign less weight to terms with high frequency,\nand vice versa. However, the estimation of visual word fre-\nquency is coarse and heuristic. Therefore, the effectiveness\nof the conventional IDF routine is marginal, and far from\noptimal. To tackle this problem, this paper introduces a nov-\nel IDF expression by the use of Lp-norm pooling technique.\nCarefully designed, the proposed IDF takes into account the\nterm frequency, document frequency, the complexity of im-\nages, as well as the codebook information. Optimizing the\nIDF function towards optimal balancing between TF and\npIDF weights yields the so-called Lp-norm IDF (pIDF).\nWe show that the conventional IDF is a special case of our\ngeneralized version, and two novel IDFs, i.e. the average\nIDF and the max IDF, can also be derived from our for-\nmula. Further, by counting for the term-frequency in each\nimage, the proposed Lp-norm IDF helps to alleviate the vi-\nsual word burstiness phenomenon.\nOur method is evaluated through extensive experiments\non three benchmark datasets (Oxford 5K, Paris 6K and\nFlickr 1M). We report a performance improvement of as\nlarge as 27.1% over the baseline approach. Moreover, since\nthe Lp-norm IDF is computed ofﬂine, no extra computation\nor memory cost is introduced to the system at all.\n1. Introduction\nThis paper considers the task of large scale image search\nfor particular object. Given a query image of an object, our\ngoal is to retrieve from a large image database all the images\ncontaining the same object in real time.\nRecent years have witnessed a rapid growth of research\nin image search and a myriad of models have been pro-\nposed [11, 18]. Among them, the Bag-of-Words model [15]\n�\n�\n�\n�\n�\n�\n�\n�\nFigure 1. An toy example of an image collection. Visual words\nzx and zy both occurs in all the six images, but with varying T-\nF distributions over the entire image collection. In conventional\nIDF, the IDF weights are equal to zero for both words. But when\nresorting to TF, zx and zy both have some discriminative power,\nthe problem of which will be tackled in this paper.\nis the most popular and perhaps the most successful one.\nThis model starts from the extraction of salient local re-\ngions from an image and representing each local patch as a\nhigh-dimensional feature vector (e.g. SIFT [7] or its variants\n[13]). Then the continuous high dimensional feature space\nis divided into a discrete space of visual words. This step is\nachieved by constructing a codebook through unsupervised\nclustering, e.g. k-means algorithm. To improve efﬁcien-\ncy, approximate k-means [11] and hierarchical k-means [8]\nhave been used. The Bag-of-Words model then treats each\ncluster center as a word in the codebook. In the spirit of text\nretrieval, the method quantizes each detected keypoint into\nits nearest visual word(s) and represents each image as a\nhistogram of visual words. Finally, images are ranked using\nvarious indexing methods [15, 21] in real time.\nTo measure the importance of visual words, most of\nthe existing approaches use the TF-IDF (Term Frequency-\nInverse Document Frequency) weighting scheme. However,\nthe conventional IDF method has two drawbacks. First, the\nconventional IDF functions on the image collection level. It\n2013 IEEE Conference on Computer Vision and Pattern Recognition\n1063-6919/13 $26.00 © 2013 IEEE\nDOI 10.1109/CVPR.2013.213\n1624\n2013 IEEE Conference on Computer Vision and Pattern Recognition\n1063-6919/13 $26.00 © 2013 IEEE\nDOI 10.1109/CVPR.2013.213\n1624\n2013 IEEE Conference on Computer Vision and Pattern Recognition\n1063-6919/13 $26.00 © 2013 IEEE\nDOI 10.1109/CVPR.2013.213\n1626\ndoes not take a closer look into the visual word level, where\nmultiple occurrences of a visual word are often observed.\nConsequently, it only makes a coarse estimation of visual\nword frequency. Second, as suggested in [5], IDF weight-\ning does not address the problem of burstiness. Burstiness\nbrings about a burst in false matches and compromises the\nimage search accuracy.\nIn this paper, we propose a novel IDF formula, called\n”Lp-norm IDF”, which makes a careful estimation of vi-\nsual word frequency and achieves signiﬁcant improvement\nin performance. The key idea is that the estimated visual\nword frequency is the weighted sum of the TF data across\nthe whole database. We show that the conventional IDF is\na special case of our generalized version. Meanwhile, two\nother novel IDFs, termed average IDF and max IDF, can\nbe derived from our method. Experimental studies on three\nimage search datasets conﬁrm that by integrating the ter-\nm frequency into IDF using the proposed method, image\nsearch performance is improved dramatically. Furthermore,\nsince the Lp-norm IDF is computed ofﬂine, no extra com-\nputational cost is introduced and efﬁciency is ensured.\nThe rest of the paper is organized as follows. After a\nbrief review of related work in Section 2, we introduce\nthe proposed Lp-norm IDF formula in Section 3. Visual\nword burstiness is illustrated in Section 4. In Section 5, we\ndemonstrate the experimental results of our method. Final-\nly, conclusions are drawn in Section 6.\n2. Related Work\nBuilt on the Bag-of-Words (BoW) model, a large body\nof literature has been proposed to improve performance.\nOne group of work mainly deals with the quantization\nerror. For example, soft matching [10, 16] assigns each de-\nscriptor to multiple visual words, but instead increases the\nquery time and memory overload. Hamming embedding [4]\nprovides binary signatures to ﬁlter out false matches. [2]\ndesigns quantization method by kernel density estimation,\nwhile [1, 24] utilize binary features to improve efﬁciency\nand reduce quantization error.\nAnother popular topic is to encode spatial constraints in-\nto the search framework, such as the weak geometric con-\nsistency [4], and RANSAC veriﬁcation [11]. The geometric\ncontext among local features can be also encoded into visu-\nal word assemblies [20, 22, 19]. By geometric constraints\n[23], inconsistent matches are ﬁltered out.\nThe third group of work concerns about visual word\nweighting. For example, [5] uses IDF-like weighting for-\nmulas to tackle the burstiness problem. The combination\nof these methods has obtained good accuracy. X. Wang et\nal. [17] proposes to incorporate the information of both the\nvocabulary tree and the image spatial domain into the con-\ntextual weighting. Our work, instead, re-estimate the visual\nword frequency by Lp-norm pooling in an ofﬂine manner.\nWe optimize the parameter to achieve a good balance be-\ntween TF and IDF weights.\n3. Proposed Approach\nThis section gives a formal description of our proposed\nLp-norm IDF formula. An image collection possesses N\nimages, denoted as D = {Ii}N\ni=1. Each image Ii has a set\nof keypoints {xj}di\nj=1, where di is the number of keypoints\nin Ii. Given the codebook {zi}K\nk=1 with a vocabulary size of\nK, image Ii is quantized into a vector representation vi =\n[vi,1, vi,2, ..., vi,K]T , where vi,k stands for the response of\nvisual word zk in Ii.\n3.1. Conventional TF-IDF\nThe TF part of the weighting scheme reﬂects the number\nof keypoints featured by this visual word. As a result, the\nTF distribution in an image is informative about textures,\nsuch as repetitive structures. On the other hand, the IDF\npart determines the contribution of a given visual word. The\npresence of a less common visual word in an image may be\na better discriminator than that of a more common one. The\nIDF weight of a visual word zk is denoted as:\nIDF(zk) = log N",
    "id": "5168e6b2974260e1140a1a7965a71b63",
    "file_path": "/mnt/DATA/Glucoma/LLM/cvf_papers/CVPR/2013/Zheng_Lp-Norm_IDF_for_2013_CVPR_paper.pdf",
    "cited_papers": []
  },
  {
    "title": "Multipath Sparse Coding Using Hierarchical Matching Pursuit",
    "authors": [
      "Complex real-world signals",
      "such as images",
      "contain dis-"
    ],
    "abstract": "Complex real-world signals, such as images, contain dis-\ncriminative structures that differ in many aspects includ-\ning scale, invariance, and data channel. While progress\nin deep learning shows the importance of learning features\nthrough multiple layers, it is equally important to learn fea-\ntures through multiple paths. We propose Multipath Hier-\narchical Matching Pursuit (M-HMP), a novel feature learn-\ning architecture that combines a collection of hierarchical\nsparse features for image classiﬁcation to capture multiple\naspects of discriminative structures. Our building blocks\nare MI-KSVD, a codebook learning algorithm that balances\nthe reconstruction error and the mutual incoherence of the\ncodebook, and batch orthogonal matching pursuit (OMP);\nwe apply them recursively at varying layers and scales. The\nresult is a highly discriminative image representation that\nleads to large improvements to the state-of-the-art on many\nstandard benchmarks, e.g., Caltech-101, Caltech-256, MIT-\nScenes, Oxford-IIIT Pet and Caltech-UCSD Bird-200.\n1. Introduction\nImages are high dimensional signals that change dramat-\nically under varying scales, viewpoints, lighting conditions,\nand scene layouts. How to extract features that are robust to\nthese changes is an important question in computer vision,\nand traditionally people rely on designed features such as\nSIFT. While SIFT can be understood and generalized as a\nway to go from pixels to patch descriptors [2], designing\ngood features is a challenging task that requires deep do-\nmain knowledge, and it is often difﬁcult to adapt to new\nsettings.\nFeature learning is attractive as it exploits the availability\nof data and avoids the need of feature engineering. Learning\nfeatures has become increasingly popular and effective for\nvisual recognition. A variety of learning and coding tech-\nniques have been proposed and evaluated, such as deep be-\nlief nets [12], deep autoencoders [17], deep convolutional\nneural networks [15], and hierarchical sparse coding [32, 3].\nMany are deep learning approaches that learn to push pix-\nFigure 1: Architecture of multipath sparse coding. Image patches\nof different sizes (here, 16x16 and 36x36) are encoded via multi-\nple layers of sparse coding. Each path corresponds to a speciﬁc\npatch size and number of layers (numbers inside boxes indicate\npatch size at the corresponding layer and path). Spatial pooling,\nindicated by SP, is performed between layers to generate the in-\nput features for the next layer. The ﬁnal layer of each path en-\ncodes complete image patches and generates a feature vector for\nthe whole image via another spatial pooling operation. Path fea-\ntures are then concatenated and used by a linear SVM for object\nrecognition.\nels through multiple layers of feature transforms. The re-\ncent work on Hierarchical Matching Pursuit [3] is inter-\nesting as it is efﬁcient (using Batch Orthogonal Matching\nPursuit), recursive (the same computational structure going\nfrom pixels to patches, and from patches to images), and\noutperforms many designed features and algorithms on a\nvariety of recognition benchmarks.\nOne crucial problem that is often overlooked in image\nfeature learning is the multi-facet nature of visual struc-\ntures: discriminative structures, which we want to extract,\nmay appear at varying scales with varying amounts of spa-\ntial and appearance invariance. While a generic learning\nmodel could capture such heterogeneity, it is much eas-\nier to build it into the learning architecture. In this work,\nwe propose Multipath Hierarchical Matching Pursuit (M-\nHMP), which builds on the single-path Hierarchical Match-\ning Pursuit approach to learn and combine recursive sparse\ncoding through many pathways on multiple bags of patches\nof varying size, and, most importantly, by encoding each\n2013 IEEE Conference on Computer Vision and Pattern Recognition\n1063-6919/13 $26.00 © 2013 IEEE\nDOI 10.1109/CVPR.2013.91\n660\npatch through multiple paths with a varying number of lay-\ners. See Fig. 1 for an illustration of our system. The mul-\ntipath architecture is important as it signiﬁcantly and efﬁ-\nciently expands the richness of image representation and\nleads to large improvements to the state of the art of image\nclassiﬁcation, as evaluated on a variety of object and scene\nrecognition benchmarks. Our M-HMP approach is generic\nand can adapt to new tasks, new sensor data, or new feature\nlearning and coding algorithms.\n2. Related Work\nIn the past few years, a growing amount of research on\nvisual recognition has focused on learning rich features us-\ning unsupervised and supervised hierarchical architectures.\nDeep Networks: Deep belief nets [12] learn a hierarchy of\nfeatures, layer by layer, using the unsupervised restricted\nBoltzmann machine. The learned weights are then further\nadjusted to the current task using supervised information.\nTo make deep belief nets applicable to full-size images, con-\nvolutional deep belief nets [18] use a small receptive ﬁeld\nand share the weights between the hidden and visible lay-\ners among all locations in an image. Deconvolutional net-\nworks [33] convolutionally decompose images in an unsu-\npervised way under a sparsity constraint. By building a hi-\nerarchy of such decompositions, robust representations can\nbe built from raw images for image recognition. Deep au-\ntoencoders [17] build high-level, class-speciﬁc feature de-\ntectors from a large collection of unlabeled images, for in-\nstance human and cat face detectors. Deep convolutional\nneural networks [15] won the ImageNet Large Scale Visual\nRecognition Challenge 2012 and demonstrated their poten-\ntial for training on large, labeled datasets.\nSparse Coding: For many years, sparse coding [21] has\nbeen a popular tool for modeling images. Sparse coding on\ntop of raw patches or SIFT features has achieved state-of-\nthe-art performance on face recognition, texture segmenta-\ntion [19], and generic object recognition [28, 5, 9]. Very\nrecently, multi-layer sparse coding networks including hier-\narchical sparse coding [32] and hierarchical matching pur-\nsuit [3, 4] have been proposed for building multiple level\nfeatures from raw sensor data. Such networks learn code-\nbooks at each layer in an unsupervised way such that image\npatches or pooled features can be represented by a sparse,\nlinear combination of codebook entries. With learned code-\nbooks, feature hierarchies are built from scratch, layer by\nlayer, using sparse codes and spatial pooling [3].\n3. Multipath Sparse Coding\nThis section provides an overview of our Multipath Hi-\nerarchical Matching Pursuit (M-HMP) approach. We pro-\npose a novel codebook learning algorithm, MI-KSVD, to\nmaintain mutual incoherence of the codebook, and discuss\nhow multi-layer sparse coding hierarchies for images can be\nbuilt from scratch and how multipath sparse coding helps\ncapture discriminative structures of varying characteristics.\n3.1. Codebook Learning with Mutual Incoherence",
    "id": "05fb0cf2f27eb32a728987b0f059d51f",
    "file_path": "/mnt/DATA/Glucoma/LLM/cvf_papers/CVPR/2013/Bo_Multipath_Sparse_Coding_2013_CVPR_paper.pdf",
    "cited_papers": []
  },
  {
    "title": "Sensing surface textures by tou",
    "authors": [
      "sensor",
      "when pressed against a surfac"
    ],
    "abstract": "Sensing surface textures by tou\ncapability for robots.  Until recently it w\na compliant sensor with high sen\nresolution. The GelSight sensor is co\nsensitivity and resolution exceeding \nfingertips. This opens the possibility \nrecognizing highly detailed surface tex\nsensor, when pressed against a surfac\nmap. This can be treated as an image, a\nthe tools of visual texture analysis. W\nsimple yet effective texture recognitio\nlocal binary patterns, and enhanced \nmulti-scale pyramid and a Hellinger d\nbuilt a database with 40 classes of ta\nmaterials such as fabric, wood, and san\ncan correctly categorize materials from\nhigh accuracy. This suggests that the G\nbe useful for material recognition by ro\n \n1. Introduction \nUnderstanding surface properties \nhumans and robots, and the sense of to\nsource of information. Tactile sensing\nkinds of information, including tempera\netc.. In this paper we focus on infor\nsurface geometry, which we will call su\nTo extract surface texture, it is impor\nsensor that is compliant (i.e., is soft li\nand has good spatial resolution. Many \nsuffer from incompliance [12, 13] or po\n[11] compared to human fingers. The \nGelSight sensor [1] is built from soft\nusing computer vision techniques it of\nlevels of spatial resolution.  \nA GelSight sensor delivers a detailed\nsurface being touched, in the form o\nwhere (x,y) are the point coordinates. Th\nan image, and the interpretation can b\nvision problem. Fig. 1(a) shows a ph\ndenim; Fig. 1(b) shows the height map d\nSensing and Recog\n \nRui Li \nMassachusetts Institute of \nrui@mit.edu \n \n \n \n \nch is a valuable \nwas difficult to build \nnsitivity and high \nompliant and offers \nthat of the human \nof measuring and \nxtures. The GelSight \nce, delivers a height \nand processed using \nWe have devised a \non system based on \nit by the use of a \ndistance metric. We \nactile textures using \nndpaper. Our system \nm this database with \nGelSight sensor can \nobots.  \nis important for \nouch is an important \ng can involve many \nature, slip, vibration, \nrmation about local \nurface texture here. \nrtant to have a touch \nike a human finger) \nexisting techniques \noor spatial resolution \nrecently developed \nt elastomer, and by \nffers unprecedented \nd height map of the \nf a function z(x,y), \nhis can be treated as \nbe approached as a \nhoto of a piece of \nderived by GelSight, \ndisplayed as a surface plot. Fig.\ndisplayed as a gray image. Fig. \npiece of sandpaper, the height \ndisplayed as a surface plot and t\na gray image.  \n \n    \n \n \n  (a)  \n \n \n \n \n \n(b) \n \n   \n \n \n  (d)  \n \n \n \n \n \n(e)  \n \nFigure 1: (a) A 2-dimensional (2D)\nGelSight height map of the denim r\n2D gray image of the denim heigh\ncorresponding to the height levels.\nsandpaper. (b) GelSight height map\ndifferent view. (f) 2D gray image \nbrightness corresponding to height \n \nThe problem of tactile text\nspecific properties of note. M\nsimplified, since a height map i\nshading, albedo, distance, etc.. \nsince the sensor is in direct cont\ncamera inside the GelSight dev\nand distance with respect to the \nIn most cases, the orientati\nunknown. For example, a denim\nan arbitrary rotation, so we wan\nis rotationally invariant. The\ninherently statistical. For examp\nsandpaper will look different (at\nother patch of 220-grit sandpap\nwith a classical texture recogn\nrecognize the sandpaper with \nrather, there are certain im\ncharacterize the sandpaper ev\nslightly different.  \n \ngnizing Surface Textures Using a GelSi",
    "id": "6e7d7c164d88c1c2dde6e15a6e22ef25",
    "file_path": "/mnt/DATA/Glucoma/LLM/cvf_papers/CVPR/2013/Li_Sensing_and_Recognizing_2013_CVPR_paper.pdf",
    "cited_papers": []
  },
  {
    "title": "Rolling Riemannian Manifolds to Solve the Multi-class Classiﬁcation Problem",
    "authors": [
      "Rui Caseiro1",
      "Pedro Martins1",
      "João F. Henriques1",
      "Fátima Silva Leite1",
      "Jorge Batista1"
    ],
    "abstract": "In the past few years there has been a growing inter-\nest on geometric frameworks to learn supervised classiﬁ-\ncation models on Riemannian manifolds [31, 27]. A pop-\nular framework, valid over any Riemannian manifold, was\nproposed in [31] for binary classiﬁcation. Once moving\nfrom binary to multi-class classiﬁcation this paradigm is not\nvalid anymore, due to the spread of multiple positive classes\non the manifold [27].\nIt is then natural to ask whether\nthe multi-class paradigm could be extended to operate on\na large class of Riemannian manifolds. We propose a math-\nematically well-founded classiﬁcation paradigm that allows\nto extend the work in [31] to multi-class models, taking into\naccount the structure of the space. The idea is to project\nall the data from the manifold onto an afﬁne tangent space\nat a particular point. To mitigate the distortion induced\nby local diffeomorphisms, we introduce for the ﬁrst time in\nthe computer vision community a well-founded mathemati-\ncal concept, so-called Rolling map [21, 16]. The novelty in\nthis alternate school of thought is that the manifold will be\nﬁrstly rolled (without slipping or twisting) as a rigid body,\nthen the given data is unwrapped onto the afﬁne tangent\nspace, where the classiﬁcation is performed.\n1. Introduction\nApplications in computer vision often involve the study\nof real world problems where the nonlinear constraints lead\nto data that lies on curved spaces [19, 28, 3]. When treating\ncases that cannot be solved within the standard Euclidean\ntools, it is usual to resort to some local linear approxima-\ntions or to use ad hoc solutions. Those solutions are not\nalways valid, which poses a challenge for several computer\nvision applications where data often lies in complex man-\nifolds, namely in Riemannian manifolds i.e. a nonlinear,\ncurved yet smooth, metric space (e.g. diffusion tensor pro-\ncessing [20, 1], foreground segmentation [4], object recog-\nnition/classiﬁcation [2, 27, 31, 29], activity recognition, text\ncategorization, shape analysis [28] motion/pose/epipolar\nsegmentation, multi-body factorization [25, 6]). In order\nto extract all the underlying information of the data it is re-\nquired to consider the Riemannian structure of the space.\nPrior Work :\nRecently, the development of geometric\nframeworks to learn supervised classiﬁcation models on\nRiemannian manifolds has been addressed in the computer\nvision community [31, 27]. A popular framework was de-\nrived by Tuzel et al. [31] for binary classiﬁcation on Rie-\nmannian manifolds. This classiﬁer is an additive model,\nwhere a set of weak learners are built by regression over the\nmappings of the data points on appropriate tangent planes\n(at the Karcher mean of the positive training points) and\ncombined through boosting. The consideration of the neg-\native samples in the mean computation would bias the re-\nsult, since they are assumed to be spread on the manifold\n[31, 27]. This framework was tested to detect pedestrians\nin images using as descriptor a region covariance matrix\n[30] (Sym+ - symmetric positive deﬁnite matrices), but the\nalgorithm is valid over any Riemannian manifold and can\nbe combined with several different boosting (classiﬁcation)\nmethods.\nDespite of its popularity, the Tuzel’s framework [31]\ncontains an important bottleneck [27]. Learning problems\non Riemannian manifolds are generally solved by ﬂattening\nthe manifold via local diffeomorphisms [5], i.e. the man-\nifold is locally embedded into an Euclidean space. How-\never, embedding the manifold using those local diffeomor-\nphisms leads to some problems. The exponential map is\nonto but only one-to-one in a neighborhood of a point. The\ninverse mapping (logarithmic map) is uniquely deﬁned only\naround a small neighborhood of that point. It is generally\nnot possible to deﬁne global coordinates which make the\nThis work was supported by the Portuguese Science Foundation\n(FCT) under the project Differential Geometry for Computer Vision\nPattern Recognition (PTDC/EEA-CRO/122812/2010) and through the\ngrants SFRH/BD74152/2010 (Rui Caseiro), SFRH/BPD/90200/2012 (Pe-\ndro Martins), SFRH/BD75459/2010 (João F. Henriques).\n2013 IEEE Conference on Computer Vision and Pattern Recognition\n1063-6919/13 $26.00 © 2013 IEEE\nDOI 10.1109/CVPR.2013.13\n41\n2013 IEEE Conference on Computer Vision and Pattern Recognition\n1063-6919/13 $26.00 © 2013 IEEE\nDOI 10.1109/CVPR.2013.13\n41\n2013 IEEE Conference on Computer Vision and Pattern Recognition\n1063-6919/13 $26.00 © 2013 IEEE\nDOI 10.1109/CVPR.2013.13\n41\nwhole manifold look like an Euclidean space. As argued\nby Tosato et al. [27] : once we try to change the paradigm\nfrom binary to multi-class classiﬁcation the Tuzel’s frame-\nwork [31] is not valid anymore due to the spread of multiple\npositive classes on the manifold. From this perspective, it\nis natural to see efforts for solve this bottleneck. Tuzel [31]\nendowed the Sym+ manifold with the well-known Afﬁne-\nInvariant metric, however a thorough analysis of this space\nopens a new perspective. The space of Sym+ is a special\nRiemannian manifold since there is another metric, called\nLog-Euclidean [1], which allows to overcome the above\nlimitations. As showed in [1] the simple matrix exponen-\ntial (exp) is a diffeomorphism from the Euclidean space of\nsymmetric matrices to the Sym+ space. The space of Sym+\nendowed with a Log-Euclidean metric is in fact isomorphic\n(the algebraic structure of vector space is conserved) and\nisometric (distances are conserved) with the corresponding\nEuclidean space of symmetric matrices [1], i.e. the Log-\nEuclidean framework deﬁnes a mapping where the space of\nSym+ is isomorphic, diffeomorphic and isometric to the as-\nsociated space of symmetric matrices [1]. This mapping is\nprecisely the simple matrix logarithm (log), which can be\nseen as the logarithm map at the identity [1].\nBy endowing the space of Sym+ with the Log-Euclidean\nmetric, Tosato et al. [27] proposed a mathematically well-\nfounded multi-class framework designed to operate on this\nparticular manifold (Sym+). All the data is projected onto\na unique tangent space at the identity (simple matrix loga-\nrithm), where a typical multi-class LogitBoost algorithm is\napplied [7]. More recently, by using the tensors (Sym+)\nas features aggregator, Carreira et al. [2] also embedded\nall the Sym+ manifold into an Euclidean space by endow-\ning Sym+ with the Log-Euclidean metric to perform multi-\nclass classiﬁcation using linear SVM. In this case, they pro-\nposed a novel and very efﬁcient method to perform semantic\nsegmentation, achieving results that outperforms the state-\nof-the-art being orders of magnitude faster to train and test.\nHowever, the Tosato/Carreira’s paradigm [27, 2] (embed all\nthe manifold) is not generalizable in the sense that it cannot\nbe applied to other Riemannian manifolds due to the speci-\nﬁcity of the mapping/metric used. It is then natural to ask\nwhether the multi-class concept could be extended to oper-\nate on a large class of Riemannian manifolds.\nRecently a new school of thought emerged [11, 12, 13,\n5]. This new paradigm suggests to embed the Riemannian\nmanifold into a Reproducing Kernel Hilbert Space (RKHS)\nby using Mercer kernels on Riemannian manifolds. Partic-\nulary, Hamm et al. [11, 12] proposed to use speciﬁc Grass-\nmann kernels in order to embed the Grassmann manifold\ninto a RKHS. Harandi et al. [13] used the Stein kernel to\nperform sparse coding and dictionary learning for symmet-\nric positive deﬁnite matrices. Caseiro et al. [5] proposed a\nnovel kernel-based mean shift on general Riemannian man-\nifolds, by using a general Riemannian kernel function, i.e.\nheat kernel. However, the use of kernel-based algorithms\nfor build classiﬁers on general Riemannian manifolds is\nnot a good option. Firstly, to our knowledge the heat ker-\nnel is the unique Mercer kernel suited to general Rieman-\nnian manifolds. Secondly, the calculation of the heat kernel\nconstitute a complex theoretical/technical problem and the\ncomputational burden is high. Finally, by using Mercer ker-\nnels to implicitly project the data from the manifold we are\nrestricted to use kernel-based classiﬁers.\nContributions :\nTo the best of our knowledge this is\nthe ﬁrst work that propose a mathematically well-founded\nclassiﬁcation paradigm that allows to extend/generalize the\nTuzel’s [31] and Tosato’s [27] frameworks to multi-class\nmodels on general Riemannian manifolds (considering the\nRiemannian structure of the space). The idea is to project\nall the data from the manifold onto an afﬁne tangent space\nat a particular point (e.g. identity) and then perform the\nclassiﬁcation there. To mitigate the distortion induced by\nlocal diffeomorphisms, we introduce for the ﬁrst time in\nthe computer vison community a well-founded mathemat-\nical concept, so-called Rolling map [16, 21]. The novelty\nin this alternate school of thought is that the manifold will\nbe ﬁrstly rolled (without slip and twist) as a rigid body,\nthen the given data is unwrapped onto the afﬁne tangent\nspace, where the classiﬁcation is performed. For the sake of\nbrevity the proof of concept will be done by testing with a\nmulti-class LogitBoost algorithm [27, 7] on the Grassmann\nmanifold [6, 25, 29, 28, 12, 16, 24]. We remark that our\nparadigm is also valid with others Riemannian manifolds.\n2. Rolling Maps on Riemannian Manifolds\nIn the past few years there has been a growing interest in\ndescribing mathematically rolling motions, without slip and\ntwist, of smooth manifolds (due to its analytic and geomet-\nric richness) [21, 15, 22, 16]. The study of these kinematic\nproblems proved to be relevant, in part because the knowl-\nedge on how to realize such virtual movements allows to\nsolve complicated problems on certain manifolds, by reduc-\ning them to similar problems on much simpler manifolds.\nFor example, those rolling movements have been used with\ngreat success to compute interpolating curves and solve\nother optimal control problems on manifolds [15, 22, 16].\nFor instance, to solve interpolation problems on a mani-\nfold, a combination of unwrapping techniques via, local dif-\nfeomorphisms, and rolling motions enables to project data\nfrom the manifold to its afﬁne tangent space at a point, solve\nthe interpolation problem on the latter and then obtain an in-\nterpolating curve on the manifold by wrapping back while\nunrolling. The resulting curve is deﬁned in explicit form,\nand has the advantage of being coordinate free [15, 16].\nRollings motions are rigid motions in the embedding\nspace, subject to some holonomic constraints (rolling con-\n42\n42\n42\nFigure 1. Rolling Map : M rolls upon ¯\nM = V ∼= TP0M without\nslip or twist, along a rolling curve α: [0, T] → M [22].\nditions) and nonholonomic constraints (no-slip and no-twist\nconditions). The most classical of all rolling motions is that\nof the 2-dimensional sphere rolling over the tangent plane\nat a point, along a curve, in which the nonholonomic con-\nstraints are satisﬁed by the absence of sliding and spinning\nand the holonomic constraints compels the sphere to stay\ntangent to the tangent plane during the movement.\nRecalling the general deﬁnition of rolling, as in [21], this\nrolling motion describes how two oriented connected Rie-\nmannian manifolds M and ¯\nM, having the same dimension\nand both embedded in the same Euclidean space ℜn, roll\nover each other without slip and twist. Whitney’s theorem\n[10], guarantees that a k-dimensional Riemannian manifold\ncan be isometrically embedded into some Euclidean space\nℜn for an appropriate choice of n ≥ k.\nWe assume that\n¯\nM is stationary and M rolls over\n¯\nM.\nThis is a rigid motion and so can be described by the ac-\ntion of the special Euclidean group SEn = SOn ⋉ ℜn on\nℜn. The symbol ⋉ represents the semi-direct product of the\nspecial orthogonal group (SOn, (·)) by the additive group\n(ℜn, (+)). We also assume that SOn acts transitively on\nM, that is, SOn ◦ P ⊂ M, for any P ∈ M. Elements\nh ∈ SEn are typically represented by pairs h = (R, s),\nwhere R ∈ SOn deﬁnes a rotation and s ∈ ℜn deﬁnes a\ntranslation. The action of SEn on ℜn is usually deﬁned by:\nSEn × ℜn → ℜn,\n(h, P) �→ h ◦ P = R ◦ P + s. (1)\nIn what follows, if P is a point belonging to a manifold\nM, TPM denotes de tangent space to the manifold M at\nthe point P and (TPM)⊥ denotes the normal space to M\n(with respect to the Euclidean metric) at P. A rolling mo-\ntion is described by a rolling map, which is a curve in SEn\nsatisfying several conditions. We give the formal deﬁnition\nof a rolling map, as presented in [21].\nDeﬁnition 1 A rolling map, describing how M rolls upon\n¯\nM, without slip or twist, along a smooth rolling curve\nα: [0, T] → M, is a smooth map\nh: [0, T]\n→",
    "id": "4cdefa2f573bd393f6018e97c4bbc2ca",
    "file_path": "/mnt/DATA/Glucoma/LLM/cvf_papers/CVPR/2013/Caseiro_Rolling_Riemannian_Manifolds_2013_CVPR_paper.pdf",
    "cited_papers": []
  },
  {
    "title": "Real-time No-Reference Image Quality Assessment based on Filter Learning",
    "authors": [
      "Peng Ye",
      "Jayant Kumar",
      "Le Kang",
      "David Doermann"
    ],
    "abstract": "This paper addresses the problem of general-purpose\nNo-Reference Image Quality Assessment (NR-IQA) with the\ngoal of developing a real-time, cross-domain model that can\npredict the quality of distorted images without prior knowl-\nedge of non-distorted reference images and types of distor-\ntions present in these images. The contributions of our work\nare two-fold: ﬁrst, the proposed method is highly efﬁcient.\nNR-IQA measures are often used in real-time imaging or\ncommunication systems, therefore it is important to have a\nfast NR-IQA algorithm that can be used in these real-time\napplications. Second, the proposed method has the poten-\ntial to be used in multiple image domains. Previous work\non NR-IQA focus primarily on predicting quality of natural\nscene image with respect to human perception, yet, in other\nimage domains, the ﬁnal receiver of a digital image may not\nbe a human.\nThe proposed method consists of the following compo-\nnents: (1) a local feature extractor; (2) a global feature\nextractor and (3) a regression model. While previous ap-\nproaches usually treat local feature extraction and regres-\nsion model training independently, we propose a supervised\nmethod based on back-projection, which links the two steps\nby learning a compact set of ﬁlters which can be applied\nto local image patches to obtain discriminative local fea-\ntures. Using a small set of ﬁlters, the proposed method is\nextremely fast. We have tested this method on various natu-\nral scene and document image datasets and obtained state-\nof-the-art results.\n1. Introduction\nWith the advancement of digital imaging, there has been\na tremendous growth in using digital images for represent-\ning and communicating information. In such an environ-\nment, it is critical to have good image quality assessment\nmethods to help maintain, control and enhance the quality\nof the digital images.\nThe goal of objective image quality assessment (IQA) is\nto build a computational model that can accurately predict\nthe quality of digital images with respect to human percep-\ntion or other measures of interest. Based on the availabil-\nity of reference images, objective IQA approaches can be\nclassiﬁed into: full-reference (FR), no-reference (NR) and\nreduced-reference (RR) approaches.\nThis paper addresses the most challenging category of\nobjective IQA methods – NR-IQA, which evaluates the\nquality of digital images without access to reference im-\nages [3, 11, 12, 13, 16, 19]. More speciﬁcally, we develop a\ngeneral-purpose NR-IQA algorithm which does not require\nprior knowledge of the types of distortions.\nNR-IQA has long been considered as one of the most dif-\nﬁcult problems in image analysis [20]. Without knowledge\nof the reference image and the type of distortion, this prob-\nlem may seem difﬁcult, but recently, signiﬁcant progress\nhas been made in the ﬁeld. State-of-the-art general-purpose\nNR-IQA systems [12, 13, 16, 22] have been shown to out-\nperform FR measures Peak Signal-to-Noise ratio (PSNR)\nand Structural Similarity Index Measure (SSIM) on stan-\ndard IQA dataset.\n1.1. Motivation\nSpeed is an important issue for NR-IQA systems since\nNR-IQA measures are often used in real-time imaging or\ncommunication systems. Algorithms that rely on computa-\ntionally expensive image transforms [13, 16] often can not\nbe used in these applications. By extracting image quality\nfeatures directly in spatial domain, recent algorithms COR-\nNIA [22] and BRISQUE[12] have greatly accelerated this\nprocess while maintaining high prediction accuracy. By us-\ning a compact set of ﬁlters, our method can further acceler-\nate the process.\nPrevious works on NR-IQA have focused primarily on\nnatural scene image and image quality is deﬁned with re-\nspect to human perception.\nVery limited work has been\ndone for NR-IQA for other types of images, such as camera-\ncaptured or scanned document images. Document IQA has\nbeen found to be very useful in many document image pro-\ncessing applications. For example, depending on the level\n2013 IEEE Conference on Computer Vision and Pattern Recognition\n1063-6919/13 $26.00 © 2013 IEEE\nDOI 10.1109/CVPR.2013.132\n985\n2013 IEEE Conference on Computer Vision and Pattern Recognition\n1063-6919/13 $26.00 © 2013 IEEE\nDOI 10.1109/CVPR.2013.132\n985\n2013 IEEE Conference on Computer Vision and Pattern Recognition\n1063-6919/13 $26.00 © 2013 IEEE\nDOI 10.1109/CVPR.2013.132\n985\n2013 IEEE Conference on Computer Vision and Pattern Recognition\n1063-6919/13 $26.00 © 2013 IEEE\nDOI 10.1109/CVPR.2013.132\n987\n2013 IEEE Conference on Computer Vision and Pattern Recognition\n1063-6919/13 $26.00 © 2013 IEEE\nDOI 10.1109/CVPR.2013.132\n987\nof degradation, the performance of modern OCR software\nmay suffer. Document IQA may help to automatically ﬁlter\npages with low predicted OCR accuracy or guide the selec-\ntion of document image enhancement methods.\nConventional image quality measures developed for nat-\nural scene images may not work well for document images\nsince document images have very different characteristics\nthan natural scene images. For example, most document\nimages are gray-scale or binary consisting of black text and\nwhite background. Building a NR-IQA system that can be\nadapted to images with different characteristics is a chal-\nlenging problem.\nTo sum up, the objective of this work is: ﬁrst, to develop\na fast NR-IQA method that can be used in real-time systems\nand second, to develop a general learning-based framework\nthat can be applied to various different image domains.\n1.2. Related work\nNatural Scene Statistics for NR-IQA\nNatural Scene Statistics (NSS) based approaches have been\nsuccessfully applied to IQA for natural scene images. These\nmethods are based on the following observations: ﬁrst,\nwhen images are properly normalized or transferred to some\ntransform domains (e.g. DCT or wavelet domain), local\ndescriptors (e.g. normalized intensity values, wavelet co-\nefﬁcients, etc), can be modeled by some parametric distri-\nbutions; second, the shape of these distributions are very\ndifferent for non-distorted and distorted images. These fun-\ndamental observations form basis of many recent IQA ap-\nproaches [12, 13, 16]. These methods differ from each other\nprimarily in how the local descriptors are extracted. For ex-\nample, in DIIVINE [13] local descriptors are extracted in\nwavelet domain. Cosine transform coefﬁcients based de-\nscriptors are used in BLIINDS-II [16]. BRISQUE [12] di-\nrectly models the normalized image pixel value using gen-\neralized Gaussian distributions (GGD) and models product\nof neighboring pixels by asymmetric generalized Gaussian\ndistributions (AGGD). The success of these methods rely\nlargely on how local features are computed, therefore hand-\ncraft features designed speciﬁcally for a particular domain\nare often used. This limits the application of these methods\nin other image domains.\nFeature Learning\nInstead of using hand-craft local descriptors, the proposed\napproach is based on feature learning. The goal is to learn\nlocal features whose distributions possess discriminative\nshapes for distorted and non-distorted images. Unsuper-\nvised feature learning has been explored in CORNIA [22],\nwhere the local descriptors are encoded using codeword that\nare learned in an unsupervised way. The success of this\nmethod relies on using a large set of codeword (usually in\norder of thousands), which can capture different aspects of\ndistortions. As was shown in [22], when only a small set of\ncodewords are used, the performance of this method drops\nsigniﬁcantly. Our method can be considered as a supervised\nextension of CORNIA, where instead of using a large re-\ndundant set of ﬁlters, the proposed method learns a com-\npact set of ﬁlters in a supervised way. Using a small set\na ﬁlters, our feature extraction process is much faster and\nmore memory efﬁcient.\nThe proposed supervised ﬁlter learning method is closely\nrelated to supervised dictionary learning for image classi-\nﬁcation. Earlier methods for dictionary learning focused\non reconstruction of signals and ignored label informa-\ntion. To learn a more compact and discriminative dictio-\nnary, learning approaches that jointly optimize both a re-\nconstructive and a discriminative criterion have been devel-\noped [10, 21, 7]. Unlike conventional supervised dictionary\nlearning, which requires that the linear combination of the\nlearned atoms in dictionary should be able to well represent\nimage patches, we do not have this constraint in our super-\nvised ﬁlter learning process. In fact, it will be shown later\nthat the functionality of ﬁlter for NR-IQA and codeword for\nimage classiﬁcation are very different.\nSupervised ﬁlter learning has also been explored by Jain\nand Karu in [6] for texture classiﬁcation, where feature ex-\ntraction and classiﬁcation tasks are performed by a neural\nnetwork. The learned ﬁlters are weight vectors in the ﬁrst\nlayer of the network. Our work is along the same lines of\nlearning a compact set of ﬁlters using a back-propagation\napproach but differs in ﬁnal stage where we perform support\nvector regression (SVR) using learned ﬁlters for predicting\nimage quality.\n1.3. Our approach\nA typical NR-IQA system may consist of the following\nthree components (1) a local feature extractor; (2) a global\nfeature extractor, which summarizes the distribution of local\nfeatures and (3) a regression model. Previous approaches\nusually treat local feature extraction and regression model\ntraining independently. We propose a supervised method\nbased on back-projection, which links these two steps by\njointly optimizing the prediction model and the local feature\nextractor. The learned compact set of ﬁlters when applied\nto local image patches yields more discriminative features.\nAdditionally, due to a signiﬁcant reduction in the number\nof ﬁlters, the proposed method achieves much better time\nperformance in comparison to previous approaches.",
    "id": "c8ef771e543e19b4721220bf81d79419",
    "file_path": "/mnt/DATA/Glucoma/LLM/cvf_papers/CVPR/2013/Ye_Real-Time_No-Reference_Image_2013_CVPR_paper.pdf",
    "cited_papers": []
  }
]