<html>
    <head>
        <meta charset="utf-8">
        
            <script>function neighbourhoodHighlight(params) {
  // console.log("in nieghbourhoodhighlight");
  allNodes = nodes.get({ returnType: "Object" });
  // originalNodes = JSON.parse(JSON.stringify(allNodes));
  // if something is selected:
  if (params.nodes.length > 0) {
    highlightActive = true;
    var i, j;
    var selectedNode = params.nodes[0];
    var degrees = 2;

    // mark all nodes as hard to read.
    for (let nodeId in allNodes) {
      // nodeColors[nodeId] = allNodes[nodeId].color;
      allNodes[nodeId].color = "rgba(200,200,200,0.5)";
      if (allNodes[nodeId].hiddenLabel === undefined) {
        allNodes[nodeId].hiddenLabel = allNodes[nodeId].label;
        allNodes[nodeId].label = undefined;
      }
    }
    var connectedNodes = network.getConnectedNodes(selectedNode);
    var allConnectedNodes = [];

    // get the second degree nodes
    for (i = 1; i < degrees; i++) {
      for (j = 0; j < connectedNodes.length; j++) {
        allConnectedNodes = allConnectedNodes.concat(
          network.getConnectedNodes(connectedNodes[j])
        );
      }
    }

    // all second degree nodes get a different color and their label back
    for (i = 0; i < allConnectedNodes.length; i++) {
      // allNodes[allConnectedNodes[i]].color = "pink";
      allNodes[allConnectedNodes[i]].color = "rgba(150,150,150,0.75)";
      if (allNodes[allConnectedNodes[i]].hiddenLabel !== undefined) {
        allNodes[allConnectedNodes[i]].label =
          allNodes[allConnectedNodes[i]].hiddenLabel;
        allNodes[allConnectedNodes[i]].hiddenLabel = undefined;
      }
    }

    // all first degree nodes get their own color and their label back
    for (i = 0; i < connectedNodes.length; i++) {
      // allNodes[connectedNodes[i]].color = undefined;
      allNodes[connectedNodes[i]].color = nodeColors[connectedNodes[i]];
      if (allNodes[connectedNodes[i]].hiddenLabel !== undefined) {
        allNodes[connectedNodes[i]].label =
          allNodes[connectedNodes[i]].hiddenLabel;
        allNodes[connectedNodes[i]].hiddenLabel = undefined;
      }
    }

    // the main node gets its own color and its label back.
    // allNodes[selectedNode].color = undefined;
    allNodes[selectedNode].color = nodeColors[selectedNode];
    if (allNodes[selectedNode].hiddenLabel !== undefined) {
      allNodes[selectedNode].label = allNodes[selectedNode].hiddenLabel;
      allNodes[selectedNode].hiddenLabel = undefined;
    }
  } else if (highlightActive === true) {
    // console.log("highlightActive was true");
    // reset all nodes
    for (let nodeId in allNodes) {
      // allNodes[nodeId].color = "purple";
      allNodes[nodeId].color = nodeColors[nodeId];
      // delete allNodes[nodeId].color;
      if (allNodes[nodeId].hiddenLabel !== undefined) {
        allNodes[nodeId].label = allNodes[nodeId].hiddenLabel;
        allNodes[nodeId].hiddenLabel = undefined;
      }
    }
    highlightActive = false;
  }

  // transform the object into an array
  var updateArray = [];
  if (params.nodes.length > 0) {
    for (let nodeId in allNodes) {
      if (allNodes.hasOwnProperty(nodeId)) {
        // console.log(allNodes[nodeId]);
        updateArray.push(allNodes[nodeId]);
      }
    }
    nodes.update(updateArray);
  } else {
    // console.log("Nothing was selected");
    for (let nodeId in allNodes) {
      if (allNodes.hasOwnProperty(nodeId)) {
        // console.log(allNodes[nodeId]);
        // allNodes[nodeId].color = {};
        updateArray.push(allNodes[nodeId]);
      }
    }
    nodes.update(updateArray);
  }
}

function filterHighlight(params) {
  allNodes = nodes.get({ returnType: "Object" });
  // if something is selected:
  if (params.nodes.length > 0) {
    filterActive = true;
    let selectedNodes = params.nodes;

    // hiding all nodes and saving the label
    for (let nodeId in allNodes) {
      allNodes[nodeId].hidden = true;
      if (allNodes[nodeId].savedLabel === undefined) {
        allNodes[nodeId].savedLabel = allNodes[nodeId].label;
        allNodes[nodeId].label = undefined;
      }
    }

    for (let i=0; i < selectedNodes.length; i++) {
      allNodes[selectedNodes[i]].hidden = false;
      if (allNodes[selectedNodes[i]].savedLabel !== undefined) {
        allNodes[selectedNodes[i]].label = allNodes[selectedNodes[i]].savedLabel;
        allNodes[selectedNodes[i]].savedLabel = undefined;
      }
    }

  } else if (filterActive === true) {
    // reset all nodes
    for (let nodeId in allNodes) {
      allNodes[nodeId].hidden = false;
      if (allNodes[nodeId].savedLabel !== undefined) {
        allNodes[nodeId].label = allNodes[nodeId].savedLabel;
        allNodes[nodeId].savedLabel = undefined;
      }
    }
    filterActive = false;
  }

  // transform the object into an array
  var updateArray = [];
  if (params.nodes.length > 0) {
    for (let nodeId in allNodes) {
      if (allNodes.hasOwnProperty(nodeId)) {
        updateArray.push(allNodes[nodeId]);
      }
    }
    nodes.update(updateArray);
  } else {
    for (let nodeId in allNodes) {
      if (allNodes.hasOwnProperty(nodeId)) {
        updateArray.push(allNodes[nodeId]);
      }
    }
    nodes.update(updateArray);
  }
}

function selectNode(nodes) {
  network.selectNodes(nodes);
  neighbourhoodHighlight({ nodes: nodes });
  return nodes;
}

function selectNodes(nodes) {
  network.selectNodes(nodes);
  filterHighlight({nodes: nodes});
  return nodes;
}

function highlightFilter(filter) {
  let selectedNodes = []
  let selectedProp = filter['property']
  if (filter['item'] === 'node') {
    let allNodes = nodes.get({ returnType: "Object" });
    for (let nodeId in allNodes) {
      if (allNodes[nodeId][selectedProp] && filter['value'].includes((allNodes[nodeId][selectedProp]).toString())) {
        selectedNodes.push(nodeId)
      }
    }
  }
  else if (filter['item'] === 'edge'){
    let allEdges = edges.get({returnType: 'object'});
    // check if the selected property exists for selected edge and select the nodes connected to the edge
    for (let edge in allEdges) {
      if (allEdges[edge][selectedProp] && filter['value'].includes((allEdges[edge][selectedProp]).toString())) {
        selectedNodes.push(allEdges[edge]['from'])
        selectedNodes.push(allEdges[edge]['to'])
      }
    }
  }
  selectNodes(selectedNodes)
}</script>
            <style>.vis-overlay{bottom:0;left:0;position:absolute;right:0;top:0;z-index:10}.vis-active{box-shadow:0 0 10px #86d5f8}.vis [class*=span]{min-height:0;width:auto}div.vis-color-picker{background-color:#fff;border-radius:15px;box-shadow:0 0 10px 0 rgba(0,0,0,.5);display:none;height:444px;left:30px;margin-left:30px;margin-top:-140px;padding:10px;position:absolute;top:0;width:310px;z-index:1}div.vis-color-picker div.vis-arrow{left:5px;position:absolute;top:147px}div.vis-color-picker div.vis-arrow:after,div.vis-color-picker div.vis-arrow:before{border:solid transparent;content:" ";height:0;pointer-events:none;position:absolute;right:100%;top:50%;width:0}div.vis-color-picker div.vis-arrow:after{border-color:hsla(0,0%,100%,0) #fff hsla(0,0%,100%,0) hsla(0,0%,100%,0);border-width:30px;margin-top:-30px}div.vis-color-picker div.vis-color{cursor:pointer;height:289px;position:absolute;width:289px}div.vis-color-picker div.vis-brightness{position:absolute;top:313px}div.vis-color-picker div.vis-opacity{position:absolute;top:350px}div.vis-color-picker div.vis-selector{background:#4c4c4c;background:-moz-linear-gradient(top,#4c4c4c 0,#595959 12%,#666 25%,#474747 39%,#2c2c2c 50%,#000 51%,#111 60%,#2b2b2b 76%,#1c1c1c 91%,#131313 100%);background:-webkit-gradient(linear,left top,left bottom,color-stop(0,#4c4c4c),color-stop(12%,#595959),color-stop(25%,#666),color-stop(39%,#474747),color-stop(50%,#2c2c2c),color-stop(51%,#000),color-stop(60%,#111),color-stop(76%,#2b2b2b),color-stop(91%,#1c1c1c),color-stop(100%,#131313));background:-webkit-linear-gradient(top,#4c4c4c,#595959 12%,#666 25%,#474747 39%,#2c2c2c 50%,#000 51%,#111 60%,#2b2b2b 76%,#1c1c1c 91%,#131313);background:-o-linear-gradient(top,#4c4c4c 0,#595959 12%,#666 25%,#474747 39%,#2c2c2c 50%,#000 51%,#111 60%,#2b2b2b 76%,#1c1c1c 91%,#131313 100%);background:-ms-linear-gradient(top,#4c4c4c 0,#595959 12%,#666 25%,#474747 39%,#2c2c2c 50%,#000 51%,#111 60%,#2b2b2b 76%,#1c1c1c 91%,#131313 100%);background:linear-gradient(180deg,#4c4c4c 0,#595959 12%,#666 25%,#474747 39%,#2c2c2c 50%,#000 51%,#111 60%,#2b2b2b 76%,#1c1c1c 91%,#131313);border:1px solid #fff;border-radius:15px;filter:progid:DXImageTransform.Microsoft.gradient(startColorstr="#4c4c4c",endColorstr="#131313",GradientType=0);height:15px;left:137px;position:absolute;top:137px;width:15px}div.vis-color-picker div.vis-new-color{left:159px;padding-right:2px;text-align:right}div.vis-color-picker div.vis-initial-color,div.vis-color-picker div.vis-new-color{border:1px solid rgba(0,0,0,.1);border-radius:5px;color:rgba(0,0,0,.4);font-size:10px;height:20px;line-height:20px;position:absolute;top:380px;vertical-align:middle;width:140px}div.vis-color-picker div.vis-initial-color{left:10px;padding-left:2px;text-align:left}div.vis-color-picker div.vis-label{left:10px;position:absolute;width:300px}div.vis-color-picker div.vis-label.vis-brightness{top:300px}div.vis-color-picker div.vis-label.vis-opacity{top:338px}div.vis-color-picker div.vis-button{background-color:#f7f7f7;border:2px solid #d9d9d9;border-radius:10px;cursor:pointer;height:25px;line-height:25px;position:absolute;text-align:center;top:410px;vertical-align:middle;width:68px}div.vis-color-picker div.vis-button.vis-cancel{left:5px}div.vis-color-picker div.vis-button.vis-load{left:82px}div.vis-color-picker div.vis-button.vis-apply{left:159px}div.vis-color-picker div.vis-button.vis-save{left:236px}div.vis-color-picker input.vis-range{height:20px;width:290px}div.vis-configuration{display:block;float:left;font-size:12px;position:relative}div.vis-configuration-wrapper{display:block;width:700px}div.vis-configuration-wrapper:after{clear:both;content:"";display:block}div.vis-configuration.vis-config-option-container{background-color:#fff;border:2px solid #f7f8fa;border-radius:4px;display:block;left:10px;margin-top:20px;padding-left:5px;width:495px}div.vis-configuration.vis-config-button{background-color:#f7f8fa;border:2px solid #ceced0;border-radius:4px;cursor:pointer;display:block;height:25px;left:10px;line-height:25px;margin-bottom:30px;margin-top:20px;padding-left:5px;vertical-align:middle;width:495px}div.vis-configuration.vis-config-button.hover{background-color:#4588e6;border:2px solid #214373;color:#fff}div.vis-configuration.vis-config-item{display:block;float:left;height:25px;line-height:25px;vertical-align:middle;width:495px}div.vis-configuration.vis-config-item.vis-config-s2{background-color:#f7f8fa;border-radius:3px;left:10px;padding-left:5px}div.vis-configuration.vis-config-item.vis-config-s3{background-color:#e4e9f0;border-radius:3px;left:20px;padding-left:5px}div.vis-configuration.vis-config-item.vis-config-s4{background-color:#cfd8e6;border-radius:3px;left:30px;padding-left:5px}div.vis-configuration.vis-config-header{font-size:18px;font-weight:700}div.vis-configuration.vis-config-label{height:25px;line-height:25px;width:120px}div.vis-configuration.vis-config-label.vis-config-s3{width:110px}div.vis-configuration.vis-config-label.vis-config-s4{width:100px}div.vis-configuration.vis-config-colorBlock{border:1px solid #444;border-radius:2px;cursor:pointer;height:19px;margin:0;padding:0;top:1px;width:30px}input.vis-configuration.vis-config-checkbox{left:-5px}input.vis-configuration.vis-config-rangeinput{margin:0;padding:1px;pointer-events:none;position:relative;top:-5px;width:60px}input.vis-configuration.vis-config-range{-webkit-appearance:none;background-color:transparent;border:0 solid #fff;height:20px;width:300px}input.vis-configuration.vis-config-range::-webkit-slider-runnable-track{background:#dedede;background:-moz-linear-gradient(top,#dedede 0,#c8c8c8 99%);background:-webkit-gradient(linear,left top,left bottom,color-stop(0,#dedede),color-stop(99%,#c8c8c8));background:-webkit-linear-gradient(top,#dedede,#c8c8c8 99%);background:-o-linear-gradient(top,#dedede 0,#c8c8c8 99%);background:-ms-linear-gradient(top,#dedede 0,#c8c8c8 99%);background:linear-gradient(180deg,#dedede 0,#c8c8c8 99%);border:1px solid #999;border-radius:3px;box-shadow:0 0 3px 0 #aaa;filter:progid:DXImageTransform.Microsoft.gradient(startColorstr="#dedede",endColorstr="#c8c8c8",GradientType=0);height:5px;width:300px}input.vis-configuration.vis-config-range::-webkit-slider-thumb{-webkit-appearance:none;background:#3876c2;background:-moz-linear-gradient(top,#3876c2 0,#385380 100%);background:-webkit-gradient(linear,left top,left bottom,color-stop(0,#3876c2),color-stop(100%,#385380));background:-webkit-linear-gradient(top,#3876c2,#385380);background:-o-linear-gradient(top,#3876c2 0,#385380 100%);background:-ms-linear-gradient(top,#3876c2 0,#385380 100%);background:linear-gradient(180deg,#3876c2 0,#385380);border:1px solid #14334b;border-radius:50%;box-shadow:0 0 1px 0 #111927;filter:progid:DXImageTransform.Microsoft.gradient(startColorstr="#3876c2",endColorstr="#385380",GradientType=0);height:17px;margin-top:-7px;width:17px}input.vis-configuration.vis-config-range:focus{outline:none}input.vis-configuration.vis-config-range:focus::-webkit-slider-runnable-track{background:#9d9d9d;background:-moz-linear-gradient(top,#9d9d9d 0,#c8c8c8 99%);background:-webkit-gradient(linear,left top,left bottom,color-stop(0,#9d9d9d),color-stop(99%,#c8c8c8));background:-webkit-linear-gradient(top,#9d9d9d,#c8c8c8 99%);background:-o-linear-gradient(top,#9d9d9d 0,#c8c8c8 99%);background:-ms-linear-gradient(top,#9d9d9d 0,#c8c8c8 99%);background:linear-gradient(180deg,#9d9d9d 0,#c8c8c8 99%);filter:progid:DXImageTransform.Microsoft.gradient(startColorstr="#9d9d9d",endColorstr="#c8c8c8",GradientType=0)}input.vis-configuration.vis-config-range::-moz-range-track{background:#dedede;background:-moz-linear-gradient(top,#dedede 0,#c8c8c8 99%);background:-webkit-gradient(linear,left top,left bottom,color-stop(0,#dedede),color-stop(99%,#c8c8c8));background:-webkit-linear-gradient(top,#dedede,#c8c8c8 99%);background:-o-linear-gradient(top,#dedede 0,#c8c8c8 99%);background:-ms-linear-gradient(top,#dedede 0,#c8c8c8 99%);background:linear-gradient(180deg,#dedede 0,#c8c8c8 99%);border:1px solid #999;border-radius:3px;box-shadow:0 0 3px 0 #aaa;filter:progid:DXImageTransform.Microsoft.gradient(startColorstr="#dedede",endColorstr="#c8c8c8",GradientType=0);height:10px;width:300px}input.vis-configuration.vis-config-range::-moz-range-thumb{background:#385380;border:none;border-radius:50%;height:16px;width:16px}input.vis-configuration.vis-config-range:-moz-focusring{outline:1px solid #fff;outline-offset:-1px}input.vis-configuration.vis-config-range::-ms-track{background:transparent;border-color:transparent;border-width:6px 0;color:transparent;height:5px;width:300px}input.vis-configuration.vis-config-range::-ms-fill-lower{background:#777;border-radius:10px}input.vis-configuration.vis-config-range::-ms-fill-upper{background:#ddd;border-radius:10px}input.vis-configuration.vis-config-range::-ms-thumb{background:#385380;border:none;border-radius:50%;height:16px;width:16px}input.vis-configuration.vis-config-range:focus::-ms-fill-lower{background:#888}input.vis-configuration.vis-config-range:focus::-ms-fill-upper{background:#ccc}.vis-configuration-popup{background:rgba(57,76,89,.85);border:2px solid #f2faff;border-radius:4px;color:#fff;font-size:14px;height:30px;line-height:30px;position:absolute;text-align:center;-webkit-transition:opacity .3s ease-in-out;-moz-transition:opacity .3s ease-in-out;transition:opacity .3s ease-in-out;width:150px}.vis-configuration-popup:after,.vis-configuration-popup:before{border:solid transparent;content:" ";height:0;left:100%;pointer-events:none;position:absolute;top:50%;width:0}.vis-configuration-popup:after{border-color:rgba(136,183,213,0) rgba(136,183,213,0) rgba(136,183,213,0) rgba(57,76,89,.85);border-width:8px;margin-top:-8px}.vis-configuration-popup:before{border-color:rgba(194,225,245,0) rgba(194,225,245,0) rgba(194,225,245,0) #f2faff;border-width:12px;margin-top:-12px}div.vis-tooltip{background-color:#f5f4ed;border:1px solid #808074;-moz-border-radius:3px;-webkit-border-radius:3px;border-radius:3px;box-shadow:3px 3px 10px rgba(0,0,0,.2);color:#000;font-family:verdana;font-size:14px;padding:5px;pointer-events:none;position:absolute;visibility:hidden;white-space:nowrap;z-index:5}div.vis-network div.vis-navigation div.vis-button{-webkit-touch-callout:none;background-position:2px 2px;background-repeat:no-repeat;-moz-border-radius:17px;border-radius:17px;cursor:pointer;display:inline-block;height:34px;position:absolute;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;width:34px}div.vis-network div.vis-navigation div.vis-button:hover{box-shadow:0 0 3px 3px rgba(56,207,21,.3)}div.vis-network div.vis-navigation div.vis-button:active{box-shadow:0 0 1px 3px rgba(56,207,21,.95)}div.vis-network div.vis-navigation div.vis-button.vis-up{background-image:url("data:image/png;base64,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");bottom:50px;left:55px}div.vis-network div.vis-navigation div.vis-button.vis-down{background-image:url("data:image/png;base64,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");bottom:10px;left:55px}div.vis-network div.vis-navigation div.vis-button.vis-left{background-image:url("data:image/png;base64,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");bottom:10px;left:15px}div.vis-network div.vis-navigation div.vis-button.vis-right{background-image:url("data:image/png;base64,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");bottom:10px;left:95px}div.vis-network div.vis-navigation div.vis-button.vis-zoomIn{background-image:url("data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAB4AAAAeCAYAAAA7MK6iAAAACXBIWXMAAAsTAAALEwEAmpwYAAAKT2lDQ1BQaG90b3Nob3AgSUNDIHByb2ZpbGUAAHjanVNnVFPpFj333vRCS4iAlEtvUhUIIFJCi4AUkSYqIQkQSoghodkVUcERRUUEG8igiAOOjoCMFVEsDIoK2AfkIaKOg6OIisr74Xuja9a89+bN/rXXPues852zzwfACAyWSDNRNYAMqUIeEeCDx8TG4eQuQIEKJHAAEAizZCFz/SMBAPh+PDwrIsAHvgABeNMLCADATZvAMByH/w/qQplcAYCEAcB0kThLCIAUAEB6jkKmAEBGAYCdmCZTAKAEAGDLY2LjAFAtAGAnf+bTAICd+Jl7AQBblCEVAaCRACATZYhEAGg7AKzPVopFAFgwABRmS8Q5ANgtADBJV2ZIALC3AMDOEAuyAAgMADBRiIUpAAR7AGDIIyN4AISZABRG8lc88SuuEOcqAAB4mbI8uSQ5RYFbCC1xB1dXLh4ozkkXKxQ2YQJhmkAuwnmZGTKBNA/g88wAAKCRFRHgg/P9eM4Ors7ONo62Dl8t6r8G/yJiYuP+5c+rcEAAAOF0ftH+LC+zGoA7BoBt/qIl7gRoXgugdfeLZrIPQLUAoOnaV/Nw+H48PEWhkLnZ2eXk5NhKxEJbYcpXff5nwl/AV/1s+X48/Pf14L7iJIEyXYFHBPjgwsz0TKUcz5IJhGLc5o9H/LcL//wd0yLESWK5WCoU41EScY5EmozzMqUiiUKSKcUl0v9k4t8s+wM+3zUAsGo+AXuRLahdYwP2SycQWHTA4vcAAPK7b8HUKAgDgGiD4c93/+8//UegJQCAZkmScQAAXkQkLlTKsz/HCAAARKCBKrBBG/TBGCzABhzBBdzBC/xgNoRCJMTCQhBCCmSAHHJgKayCQiiGzbAdKmAv1EAdNMBRaIaTcA4uwlW4Dj1wD/phCJ7BKLyBCQRByAgTYSHaiAFiilgjjggXmYX4IcFIBBKLJCDJiBRRIkuRNUgxUopUIFVIHfI9cgI5h1xGupE7yAAygvyGvEcxlIGyUT3UDLVDuag3GoRGogvQZHQxmo8WoJvQcrQaPYw2oefQq2gP2o8+Q8cwwOgYBzPEbDAuxsNCsTgsCZNjy7EirAyrxhqwVqwDu4n1Y8+xdwQSgUXACTYEd0IgYR5BSFhMWE7YSKggHCQ0EdoJNwkDhFHCJyKTqEu0JroR+cQYYjIxh1hILCPWEo8TLxB7iEPENyQSiUMyJ7mQAkmxpFTSEtJG0m5SI+ksqZs0SBojk8naZGuyBzmULCAryIXkneTD5DPkG+Qh8lsKnWJAcaT4U+IoUspqShnlEOU05QZlmDJBVaOaUt2ooVQRNY9aQq2htlKvUYeoEzR1mjnNgxZJS6WtopXTGmgXaPdpr+h0uhHdlR5Ol9BX0svpR+iX6AP0dwwNhhWDx4hnKBmbGAcYZxl3GK+YTKYZ04sZx1QwNzHrmOeZD5lvVVgqtip8FZHKCpVKlSaVGyovVKmqpqreqgtV81XLVI+pXlN9rkZVM1PjqQnUlqtVqp1Q61MbU2epO6iHqmeob1Q/pH5Z/YkGWcNMw09DpFGgsV/jvMYgC2MZs3gsIWsNq4Z1gTXEJrHN2Xx2KruY/R27iz2qqaE5QzNKM1ezUvOUZj8H45hx+Jx0TgnnKKeX836K3hTvKeIpG6Y0TLkxZVxrqpaXllirSKtRq0frvTau7aedpr1Fu1n7gQ5Bx0onXCdHZ4/OBZ3nU9lT3acKpxZNPTr1ri6qa6UbobtEd79up+6Ynr5egJ5Mb6feeb3n+hx9L/1U/W36p/VHDFgGswwkBtsMzhg8xTVxbzwdL8fb8VFDXcNAQ6VhlWGX4YSRudE8o9VGjUYPjGnGXOMk423GbcajJgYmISZLTepN7ppSTbmmKaY7TDtMx83MzaLN1pk1mz0x1zLnm+eb15vft2BaeFostqi2uGVJsuRaplnutrxuhVo5WaVYVVpds0atna0l1rutu6cRp7lOk06rntZnw7Dxtsm2qbcZsOXYBtuutm22fWFnYhdnt8Wuw+6TvZN9un2N/T0HDYfZDqsdWh1+c7RyFDpWOt6azpzuP33F9JbpL2dYzxDP2DPjthPLKcRpnVOb00dnF2e5c4PziIuJS4LLLpc+Lpsbxt3IveRKdPVxXeF60vWdm7Obwu2o26/uNu5p7ofcn8w0nymeWTNz0MPIQ+BR5dE/C5+VMGvfrH5PQ0+BZ7XnIy9jL5FXrdewt6V3qvdh7xc+9j5yn+M+4zw33jLeWV/MN8C3yLfLT8Nvnl+F30N/I/9k/3r/0QCngCUBZwOJgUGBWwL7+Hp8Ib+OPzrbZfay2e1BjKC5QRVBj4KtguXBrSFoyOyQrSH355jOkc5pDoVQfujW0Adh5mGLw34MJ4WHhVeGP45wiFga0TGXNXfR3ENz30T6RJZE3ptnMU85ry1KNSo+qi5qPNo3ujS6P8YuZlnM1VidWElsSxw5LiquNm5svt/87fOH4p3iC+N7F5gvyF1weaHOwvSFpxapLhIsOpZATIhOOJTwQRAqqBaMJfITdyWOCnnCHcJnIi/RNtGI2ENcKh5O8kgqTXqS7JG8NXkkxTOlLOW5hCepkLxMDUzdmzqeFpp2IG0yPTq9MYOSkZBxQqohTZO2Z+pn5mZ2y6xlhbL+xW6Lty8elQfJa7OQrAVZLQq2QqboVFoo1yoHsmdlV2a/zYnKOZarnivN7cyzytuQN5zvn//tEsIS4ZK2pYZLVy0dWOa9rGo5sjxxedsK4xUFK4ZWBqw8uIq2Km3VT6vtV5eufr0mek1rgV7ByoLBtQFr6wtVCuWFfevc1+1dT1gvWd+1YfqGnRs+FYmKrhTbF5cVf9go3HjlG4dvyr+Z3JS0qavEuWTPZtJm6ebeLZ5bDpaql+aXDm4N2dq0Dd9WtO319kXbL5fNKNu7g7ZDuaO/PLi8ZafJzs07P1SkVPRU+lQ27tLdtWHX+G7R7ht7vPY07NXbW7z3/T7JvttVAVVN1WbVZftJ+7P3P66Jqun4lvttXa1ObXHtxwPSA/0HIw6217nU1R3SPVRSj9Yr60cOxx++/p3vdy0NNg1VjZzG4iNwRHnk6fcJ3/ceDTradox7rOEH0x92HWcdL2pCmvKaRptTmvtbYlu6T8w+0dbq3nr8R9sfD5w0PFl5SvNUyWna6YLTk2fyz4ydlZ19fi753GDborZ752PO32oPb++6EHTh0kX/i+c7vDvOXPK4dPKy2+UTV7hXmq86X23qdOo8/pPTT8e7nLuarrlca7nuer21e2b36RueN87d9L158Rb/1tWeOT3dvfN6b/fF9/XfFt1+cif9zsu72Xcn7q28T7xf9EDtQdlD3YfVP1v+3Njv3H9qwHeg89HcR/cGhYPP/pH1jw9DBY+Zj8uGDYbrnjg+OTniP3L96fynQ89kzyaeF/6i/suuFxYvfvjV69fO0ZjRoZfyl5O/bXyl/erA6xmv28bCxh6+yXgzMV70VvvtwXfcdx3vo98PT+R8IH8o/2j5sfVT0Kf7kxmTk/8EA5jz/GMzLdsAAAAgY0hSTQAAeiUAAICDAAD5/wAAgOkAAHUwAADqYAAAOpgAABdvkl/FRgAABiBJREFUeNqkV2tQlOcVfp7zvgvDRe66y8htXUBR1GoFI+BtFJvRtjPJBGeaH2a8DGmbttgSTWbSJEw6TWOsrbbpTIeJZGqaTipTa6LJZDTVUTYQdNAohoso6qLucnERN0Axcb/8+HaJUHDX9Pz6vnnPe57vXJ5zzkeEIwaYcwBL/VrW0TCKqZANINEvBhSk3w9eUmC9HzjcsfarOhBGKJN84GkVJHcetvqFu4SAIYELYlpm4LpQQMqoQQKVnzeO7EYV/A8NnHMAGwHWQJmAjtg895LkFa7FU1d258UvGLBGpI4AQM9dd2TrwNn4016n9bS3LqNzsD1VKPAbfhCyqflR31thAzv+La+QxotCoNi6pn1D1s9aVli/3xtOVk72fjT1XVf17E9uHZspFBD8zdk13pdCAjsOyG6KUSEEnrT/tPHluW+cw7eQ19q2z6/t2rsYJEjZ07S6d+ukwI5/yQ7RxnYC2DZnx8dbHNs6xxs85T2R9GprZcmVwYs2BYWsmBzP83m7nIVJS73jdfdd+7PjjUu/XWUCGTtPre7ZHjxTY3Kq8DoV8Ou5u49snPGrKxN58syZ9aVXBztsigoUBd+Xt2NbfZ8llaVvah+vOz9hcX+CJenWp7eOOYS6ePpTU1w39vk+AwCzFPdDQbFGFPCUY2v9hqxfXJ0shNeHLtsUFc6UequbVvdVkwLX0GXbZPpl6Zuu/ij9x/VCBU1dU7bfdFYAIDsSFRCgeOqa9hfy/nDhwfwTKOrRd0U95n0iqch9+cKS5JVtpMCdkllhAhugCHcRwAb7z1tCEp8CCXAWAJRoCFXIYnti+sYWTQ0tll0wQMk+hGUAkBOX714xbV1IyuhxHhIMC/iR5OV9M2JmuhU1Vh7PXiakrIUQhcnLXeHQxPT4GyAtFqgwgAPF5iIFWkeu1SSLCKAweXn3/ZR5rXV7SddQpy3YDoNems9qTI5hGCitm1MOAAx0aaFCerTd84zjBed3Egq9ADA/rqD7Q3ctQC4REDmkYHb8goGgsR2tz5V0DV+xUdQoqAQ81RybU4IgFWgACgpaLLCIBUo0bv63y/aXy6+WBHWz4/IHSIGAuVooiaRgWqD3AsDVoQ6bEgtOrfJUhwrf0WUtk+r8sL6wvHvk5ijVUiJSRrQZuURtfoGMuaCoRyfP/yMy0XykgAA0DPRTxNp31x2ZFuUYBgB7bK7HNdhpKz6WXq6oQCooKghMKhkgji77vBoA1jkXlAvVfRQjFMUcmxSkRWd6gpjeu32R2kxTvyhKh1DQeud8fFBh26zfOe0xuR4JgAbzywCoRSzfeDUKatJKUQK+CjKiHZ6nZ2xzBnU7B9vixTy7qCHSQEhJU3+DtdT6mAcAFiWUeP/xyPH3Jwrfo3XzysemRcEA8F5RY8h6aPE1WwMLQ4OQ/EBANHmdGWHlzZyxk3ayB0m771yGooYy+KE0l35x0iBxZehS6ie9R1PCMaDvCzWDXA4hZ283ptwcvp6qqDBnyao6AWEQrBQQ/7y+d3YoA+NBTAaElo973p8tVFCQyipW+c3pdNu7BwBOe+tm/eniK/kPFWowpMfvuKrzzw80zSKIkWsJe0bHYu163BNwMwDsv7G36ODNtzMnM5IWZfeQgscbisvLPl1aDhLTo7I8k+n/p+dw5pGeg0WKGiS31K6vvTdmA7nx9uDZ9A3xMUIpbvSezE6MSOmbNWXewHhD6dH23o7BlqQvvrwTK6KQFpXl2WyvcE6LTB2eCPSdrurvmcUnO/cVfPD6pMteyfGs3QKpUFQoS9tU/xPH8xe+Tdd693pN/pHug0Xmqntvz1uLDo9Z9v5nnrn+dvujrI1JMUJd3OY7n97ua46douOGpkdlDoUDeG7g1NS/u/5a0Og9scCsB+ysWXSoMuyFftWJvM0E31SBjmWPznHPjy+8NjdhYfeMmJl3EiNSRgCi/25fpGu4M671zjlrm685s2fEnUoQ5lrLLW8uPLj3oX9hqgxIw8n8X1LU7yMkItCHzREZrGQV6ONmy5TggHk247sL/1jFqof/hRn/AWfqC0pI+QHBIk3tICXRrFTpF8hlJaqefh6yFxQ6HwQYlK8HAKyt3WsWxl7fAAAAAElFTkSuQmCC");bottom:10px;right:15px}div.vis-network div.vis-navigation div.vis-button.vis-zoomOut{background-image:url("data:image/png;base64,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");bottom:10px;right:55px}div.vis-network div.vis-navigation div.vis-button.vis-zoomExtends{background-image:url("data:image/png;base64,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");bottom:50px;right:15px}div.vis-network div.vis-manipulation{background:#fff;background:-moz-linear-gradient(top,#fff 0,#fcfcfc 48%,#fafafa 50%,#fcfcfc 100%);background:-webkit-gradient(linear,left top,left bottom,color-stop(0,#fff),color-stop(48%,#fcfcfc),color-stop(50%,#fafafa),color-stop(100%,#fcfcfc));background:-webkit-linear-gradient(top,#fff,#fcfcfc 48%,#fafafa 50%,#fcfcfc);background:-o-linear-gradient(top,#fff 0,#fcfcfc 48%,#fafafa 50%,#fcfcfc 100%);background:-ms-linear-gradient(top,#fff 0,#fcfcfc 48%,#fafafa 50%,#fcfcfc 100%);background:linear-gradient(180deg,#fff 0,#fcfcfc 48%,#fafafa 50%,#fcfcfc);border:0 solid #d6d9d8;border-bottom:1px;box-sizing:content-box;filter:progid:DXImageTransform.Microsoft.gradient(startColorstr="#ffffff",endColorstr="#fcfcfc",GradientType=0);height:28px;left:0;padding-top:4px;position:absolute;top:0;width:100%}div.vis-network button.vis-edit-mode,div.vis-network div.vis-edit-mode{height:30px;left:0;position:absolute;top:5px}div.vis-network button.vis-close{-webkit-touch-callout:none;background-color:transparent;background-image:url("data:image/png;base64,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");background-position:20px 3px;background-repeat:no-repeat;border:none;cursor:pointer;height:30px;position:absolute;right:0;top:0;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;width:30px}div.vis-network button.vis-close:hover{opacity:.6}div.vis-network div.vis-edit-mode button.vis-button,div.vis-network div.vis-manipulation button.vis-button{-webkit-touch-callout:none;background-color:transparent;background-position:0 0;background-repeat:no-repeat;border:none;-moz-border-radius:15px;border-radius:15px;box-sizing:content-box;cursor:pointer;float:left;font-family:verdana;font-size:12px;height:24px;margin-left:10px;padding:0 8px;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}div.vis-network div.vis-manipulation button.vis-button:hover{box-shadow:1px 1px 8px rgba(0,0,0,.2)}div.vis-network div.vis-manipulation button.vis-button:active{box-shadow:1px 1px 8px rgba(0,0,0,.5)}div.vis-network div.vis-manipulation button.vis-button.vis-back{background-image:url("data:image/png;base64,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")}div.vis-network div.vis-manipulation div.vis-none:hover{box-shadow:1px 1px 8px transparent;cursor:default}div.vis-network div.vis-manipulation div.vis-none:active{box-shadow:1px 1px 8px transparent}div.vis-network div.vis-manipulation div.vis-none{line-height:23px;padding:0}div.vis-network div.vis-manipulation div.notification{font-weight:700;margin:2px}div.vis-network div.vis-manipulation button.vis-button.vis-add{background-image:url("data:image/png;base64,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")}div.vis-network div.vis-edit-mode button.vis-button.vis-edit,div.vis-network div.vis-manipulation button.vis-button.vis-edit{background-image:url("data:image/png;base64,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")}div.vis-network div.vis-edit-mode button.vis-button.vis-edit.vis-edit-mode{background-color:#fcfcfc;border:1px solid #ccc}div.vis-network div.vis-manipulation button.vis-button.vis-connect{background-image:url("data:image/png;base64,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")}div.vis-network div.vis-manipulation button.vis-button.vis-delete{background-image:url("data:image/png;base64,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")}div.vis-network div.vis-edit-mode div.vis-label,div.vis-network div.vis-manipulation div.vis-label{line-height:25px;margin:0 0 0 23px}div.vis-network div.vis-manipulation div.vis-separator-line{background-color:#bdbdbd;display:inline-block;float:left;height:21px;margin:0 7px 0 15px;width:1px}</style>
            <script>/**
 * vis-network
 * https://visjs.github.io/vis-network/
 *
 * A dynamic, browser-based visualization library.
 *
 * @version 9.1.2
 * @date    2022-03-28T20:17:35.342Z
 *
 * @copyright (c) 2011-2017 Almende B.V, http://almende.com
 * @copyright (c) 2017-2019 visjs contributors, https://github.com/visjs
 *
 * @license
 * vis.js is dual licensed under both
 *
 *   1. The Apache 2.0 License
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 *   and
 *
 *   2. The MIT License
 *      http://opensource.org/licenses/MIT
 *
 * vis.js may be distributed under either license.
 */
!function(t,e){"object"==typeof exports&&"undefined"!=typeof module?e(exports):"function"==typeof define&&define.amd?define(["exports"],e):e((t="undefined"!=typeof globalThis?globalThis:t||self).vis=t.vis||{})}(this,(function(t){"use strict";var e="undefined"!=typeof globalThis?globalThis:"undefined"!=typeof window?window:"undefined"!=typeof global?global:"undefined"!=typeof self?self:{},i=function(t){return t&&t.Math==Math&&t},n=i("object"==typeof globalThis&&globalThis)||i("object"==typeof window&&window)||i("object"==typeof self&&self)||i("object"==typeof e&&e)||function(){return this}()||Function("return this")(),o=function(t){try{return!!t()}catch(t){return!0}},r=!o((function(){var t=function(){}.bind();return"function"!=typeof t||t.hasOwnProperty("prototype")})),s=r,a=Function.prototype,h=a.apply,l=a.call,d="object"==typeof Reflect&&Reflect.apply||(s?l.bind(h):function(){return l.apply(h,arguments)}),c=r,u=Function.prototype,f=u.bind,p=u.call,v=c&&f.bind(p,p),g=c?function(t){return t&&v(t)}:function(t){return t&&function(){return p.apply(t,arguments)}},y=function(t){return"function"==typeof t},m={},b=!o((function(){return 7!=Object.defineProperty({},1,{get:function(){return 7}})[1]})),w=r,k=Function.prototype.call,_=w?k.bind(k):function(){return k.apply(k,arguments)},x={},E={}.propertyIsEnumerable,O=Object.getOwnPropertyDescriptor,C=O&&!E.call({1:2},1);x.f=C?function(t){var e=O(this,t);return!!e&&e.enumerable}:E;var S,T,M=function(t,e){return{enumerable:!(1&t),configurable:!(2&t),writable:!(4&t),value:e}},P=g,D=P({}.toString),I=P("".slice),B=function(t){return I(D(t),8,-1)},z=g,N=o,F=B,A=n.Object,j=z("".split),R=N((function(){return!A("z").propertyIsEnumerable(0)}))?function(t){return"String"==F(t)?j(t,""):A(t)}:A,L=n.TypeError,H=function(t){if(null==t)throw L("Can't call method on "+t);return t},W=R,q=H,V=function(t){return W(q(t))},U=y,Y=function(t){return"object"==typeof t?null!==t:U(t)},X={},G=X,K=n,$=y,Z=function(t){return $(t)?t:void 0},Q=function(t,e){return arguments.length<2?Z(G[t])||Z(K[t]):G[t]&&G[t][e]||K[t]&&K[t][e]},J=g({}.isPrototypeOf),tt=Q("navigator","userAgent")||"",et=n,it=tt,nt=et.process,ot=et.Deno,rt=nt&&nt.versions||ot&&ot.version,st=rt&&rt.v8;st&&(T=(S=st.split("."))[0]>0&&S[0]<4?1:+(S[0]+S[1])),!T&&it&&(!(S=it.match(/Edge\/(\d+)/))||S[1]>=74)&&(S=it.match(/Chrome\/(\d+)/))&&(T=+S[1]);var at=T,ht=at,lt=o,dt=!!Object.getOwnPropertySymbols&&!lt((function(){var t=Symbol();return!String(t)||!(Object(t)instanceof Symbol)||!Symbol.sham&&ht&&ht<41})),ct=dt&&!Symbol.sham&&"symbol"==typeof Symbol.iterator,ut=Q,ft=y,pt=J,vt=ct,gt=n.Object,yt=vt?function(t){return"symbol"==typeof t}:function(t){var e=ut("Symbol");return ft(e)&&pt(e.prototype,gt(t))},mt=n.String,bt=function(t){try{return mt(t)}catch(t){return"Object"}},wt=y,kt=bt,_t=n.TypeError,xt=function(t){if(wt(t))return t;throw _t(kt(t)+" is not a function")},Et=xt,Ot=function(t,e){var i=t[e];return null==i?void 0:Et(i)},Ct=_,St=y,Tt=Y,Mt=n.TypeError,Pt={exports:{}},Dt=n,It=Object.defineProperty,Bt=function(t,e){try{It(Dt,t,{value:e,configurable:!0,writable:!0})}catch(i){Dt[t]=e}return e},zt="__core-js_shared__",Nt=n[zt]||Bt(zt,{}),Ft=Nt;(Pt.exports=function(t,e){return Ft[t]||(Ft[t]=void 0!==e?e:{})})("versions",[]).push({version:"3.21.1",mode:"pure",copyright:" 2014-2022 Denis Pushkarev (zloirock.ru)",license:"https://github.com/zloirock/core-js/blob/v3.21.1/LICENSE",source:"https://github.com/zloirock/core-js"});var At=H,jt=n.Object,Rt=function(t){return jt(At(t))},Lt=Rt,Ht=g({}.hasOwnProperty),Wt=Object.hasOwn||function(t,e){return Ht(Lt(t),e)},qt=g,Vt=0,Ut=Math.random(),Yt=qt(1..toString),Xt=function(t){return"Symbol("+(void 0===t?"":t)+")_"+Yt(++Vt+Ut,36)},Gt=n,Kt=Pt.exports,$t=Wt,Zt=Xt,Qt=dt,Jt=ct,te=Kt("wks"),ee=Gt.Symbol,ie=ee&&ee.for,ne=Jt?ee:ee&&ee.withoutSetter||Zt,oe=function(t){if(!$t(te,t)||!Qt&&"string"!=typeof te[t]){var e="Symbol."+t;Qt&&$t(ee,t)?te[t]=ee[t]:te[t]=Jt&&ie?ie(e):ne(e)}return te[t]},re=_,se=Y,ae=yt,he=Ot,le=function(t,e){var i,n;if("string"===e&&St(i=t.toString)&&!Tt(n=Ct(i,t)))return n;if(St(i=t.valueOf)&&!Tt(n=Ct(i,t)))return n;if("string"!==e&&St(i=t.toString)&&!Tt(n=Ct(i,t)))return n;throw Mt("Can't convert object to primitive value")},de=oe,ce=n.TypeError,ue=de("toPrimitive"),fe=function(t,e){if(!se(t)||ae(t))return t;var i,n=he(t,ue);if(n){if(void 0===e&&(e="default"),i=re(n,t,e),!se(i)||ae(i))return i;throw ce("Can't convert object to primitive value")}return void 0===e&&(e="number"),le(t,e)},pe=yt,ve=function(t){var e=fe(t,"string");return pe(e)?e:e+""},ge=Y,ye=n.document,me=ge(ye)&&ge(ye.createElement),be=function(t){return me?ye.createElement(t):{}},we=be,ke=!b&&!o((function(){return 7!=Object.defineProperty(we("div"),"a",{get:function(){return 7}}).a})),_e=b,xe=_,Ee=x,Oe=M,Ce=V,Se=ve,Te=Wt,Me=ke,Pe=Object.getOwnPropertyDescriptor;m.f=_e?Pe:function(t,e){if(t=Ce(t),e=Se(e),Me)try{return Pe(t,e)}catch(t){}if(Te(t,e))return Oe(!xe(Ee.f,t,e),t[e])};var De=o,Ie=y,Be=/#|\.prototype\./,ze=function(t,e){var i=Fe[Ne(t)];return i==je||i!=Ae&&(Ie(e)?De(e):!!e)},Ne=ze.normalize=function(t){return String(t).replace(Be,".").toLowerCase()},Fe=ze.data={},Ae=ze.NATIVE="N",je=ze.POLYFILL="P",Re=ze,Le=xt,He=r,We=g(g.bind),qe=function(t,e){return Le(t),void 0===e?t:He?We(t,e):function(){return t.apply(e,arguments)}},Ve={},Ue=b&&o((function(){return 42!=Object.defineProperty((function(){}),"prototype",{value:42,writable:!1}).prototype})),Ye=n,Xe=Y,Ge=Ye.String,Ke=Ye.TypeError,$e=function(t){if(Xe(t))return t;throw Ke(Ge(t)+" is not an object")},Ze=b,Qe=ke,Je=Ue,ti=$e,ei=ve,ii=n.TypeError,ni=Object.defineProperty,oi=Object.getOwnPropertyDescriptor,ri="enumerable",si="configurable",ai="writable";Ve.f=Ze?Je?function(t,e,i){if(ti(t),e=ei(e),ti(i),"function"==typeof t&&"prototype"===e&&"value"in i&&ai in i&&!i.writable){var n=oi(t,e);n&&n.writable&&(t[e]=i.value,i={configurable:si in i?i.configurable:n.configurable,enumerable:ri in i?i.enumerable:n.enumerable,writable:!1})}return ni(t,e,i)}:ni:function(t,e,i){if(ti(t),e=ei(e),ti(i),Qe)try{return ni(t,e,i)}catch(t){}if("get"in i||"set"in i)throw ii("Accessors not supported");return"value"in i&&(t[e]=i.value),t};var hi=Ve,li=M,di=b?function(t,e,i){return hi.f(t,e,li(1,i))}:function(t,e,i){return t[e]=i,t},ci=n,ui=d,fi=g,pi=y,vi=m.f,gi=Re,yi=X,mi=qe,bi=di,wi=Wt,ki=function(t){var e=function(i,n,o){if(this instanceof e){switch(arguments.length){case 0:return new t;case 1:return new t(i);case 2:return new t(i,n)}return new t(i,n,o)}return ui(t,this,arguments)};return e.prototype=t.prototype,e},_i=function(t,e){var i,n,o,r,s,a,h,l,d=t.target,c=t.global,u=t.stat,f=t.proto,p=c?ci:u?ci[d]:(ci[d]||{}).prototype,v=c?yi:yi[d]||bi(yi,d,{})[d],g=v.prototype;for(o in e)i=!gi(c?o:d+(u?".":"#")+o,t.forced)&&p&&wi(p,o),s=v[o],i&&(a=t.noTargetGet?(l=vi(p,o))&&l.value:p[o]),r=i&&a?a:e[o],i&&typeof s==typeof r||(h=t.bind&&i?mi(r,ci):t.wrap&&i?ki(r):f&&pi(r)?fi(r):r,(t.sham||r&&r.sham||s&&s.sham)&&bi(h,"sham",!0),bi(v,o,h),f&&(wi(yi,n=d+"Prototype")||bi(yi,n,{}),bi(yi[n],o,r),t.real&&g&&!g[o]&&bi(g,o,r)))},xi=Math.ceil,Ei=Math.floor,Oi=function(t){var e=+t;return e!=e||0===e?0:(e>0?Ei:xi)(e)},Ci=Oi,Si=Math.max,Ti=Math.min,Mi=function(t,e){var i=Ci(t);return i<0?Si(i+e,0):Ti(i,e)},Pi=Oi,Di=Math.min,Ii=function(t){return t>0?Di(Pi(t),9007199254740991):0},Bi=function(t){return Ii(t.length)},zi=V,Ni=Mi,Fi=Bi,Ai=function(t){return function(e,i,n){var o,r=zi(e),s=Fi(r),a=Ni(n,s);if(t&&i!=i){for(;s>a;)if((o=r[a++])!=o)return!0}else for(;s>a;a++)if((t||a in r)&&r[a]===i)return t||a||0;return!t&&-1}},ji={includes:Ai(!0),indexOf:Ai(!1)},Ri={},Li=Wt,Hi=V,Wi=ji.indexOf,qi=Ri,Vi=g([].push),Ui=function(t,e){var i,n=Hi(t),o=0,r=[];for(i in n)!Li(qi,i)&&Li(n,i)&&Vi(r,i);for(;e.length>o;)Li(n,i=e[o++])&&(~Wi(r,i)||Vi(r,i));return r},Yi=["constructor","hasOwnProperty","isPrototypeOf","propertyIsEnumerable","toLocaleString","toString","valueOf"],Xi=Ui,Gi=Yi,Ki=Object.keys||function(t){return Xi(t,Gi)},$i={};$i.f=Object.getOwnPropertySymbols;var Zi=b,Qi=g,Ji=_,tn=o,en=Ki,nn=$i,on=x,rn=Rt,sn=R,an=Object.assign,hn=Object.defineProperty,ln=Qi([].concat),dn=!an||tn((function(){if(Zi&&1!==an({b:1},an(hn({},"a",{enumerable:!0,get:function(){hn(this,"b",{value:3,enumerable:!1})}}),{b:2})).b)return!0;var t={},e={},i=Symbol(),n="abcdefghijklmnopqrst";return t[i]=7,n.split("").forEach((function(t){e[t]=t})),7!=an({},t)[i]||en(an({},e)).join("")!=n}))?function(t,e){for(var i=rn(t),n=arguments.length,o=1,r=nn.f,s=on.f;n>o;)for(var a,h=sn(arguments[o++]),l=r?ln(en(h),r(h)):en(h),d=l.length,c=0;d>c;)a=l[c++],Zi&&!Ji(s,h,a)||(i[a]=h[a]);return i}:an,cn=dn;_i({target:"Object",stat:!0,forced:Object.assign!==cn},{assign:cn});var un=X.Object.assign,fn=g([].slice),pn=g,vn=xt,gn=Y,yn=Wt,mn=fn,bn=r,wn=n.Function,kn=pn([].concat),_n=pn([].join),xn={},En=function(t,e,i){if(!yn(xn,e)){for(var n=[],o=0;o<e;o++)n[o]="a["+o+"]";xn[e]=wn("C,a","return new C("+_n(n,",")+")")}return xn[e](t,i)},On=bn?wn.bind:function(t){var e=vn(this),i=e.prototype,n=mn(arguments,1),o=function(){var i=kn(n,mn(arguments));return this instanceof o?En(e,i.length,i):e.apply(t,i)};return gn(i)&&(o.prototype=i),o},Cn=On;_i({target:"Function",proto:!0,forced:Function.bind!==Cn},{bind:Cn});var Sn=X,Tn=function(t){return Sn[t+"Prototype"]},Mn=Tn("Function").bind,Pn=J,Dn=Mn,In=Function.prototype,Bn=function(t){var e=t.bind;return t===In||Pn(In,t)&&e===In.bind?Dn:e},zn=Bn;function Nn(t,e,i,n){t.beginPath(),t.arc(e,i,n,0,2*Math.PI,!1),t.closePath()}function Fn(t,e,i,n,o,r){var s=Math.PI/180;n-2*r<0&&(r=n/2),o-2*r<0&&(r=o/2),t.beginPath(),t.moveTo(e+r,i),t.lineTo(e+n-r,i),t.arc(e+n-r,i+r,r,270*s,360*s,!1),t.lineTo(e+n,i+o-r),t.arc(e+n-r,i+o-r,r,0,90*s,!1),t.lineTo(e+r,i+o),t.arc(e+r,i+o-r,r,90*s,180*s,!1),t.lineTo(e,i+r),t.arc(e+r,i+r,r,180*s,270*s,!1),t.closePath()}function An(t,e,i,n,o){var r=.5522848,s=n/2*r,a=o/2*r,h=e+n,l=i+o,d=e+n/2,c=i+o/2;t.beginPath(),t.moveTo(e,c),t.bezierCurveTo(e,c-a,d-s,i,d,i),t.bezierCurveTo(d+s,i,h,c-a,h,c),t.bezierCurveTo(h,c+a,d+s,l,d,l),t.bezierCurveTo(d-s,l,e,c+a,e,c),t.closePath()}function jn(t,e,i,n,o){var r=o*(1/3),s=.5522848,a=n/2*s,h=r/2*s,l=e+n,d=i+r,c=e+n/2,u=i+r/2,f=i+(o-r/2),p=i+o;t.beginPath(),t.moveTo(l,u),t.bezierCurveTo(l,u+h,c+a,d,c,d),t.bezierCurveTo(c-a,d,e,u+h,e,u),t.bezierCurveTo(e,u-h,c-a,i,c,i),t.bezierCurveTo(c+a,i,l,u-h,l,u),t.lineTo(l,f),t.bezierCurveTo(l,f+h,c+a,p,c,p),t.bezierCurveTo(c-a,p,e,f+h,e,f),t.lineTo(e,u)}function Rn(t,e,i,n,o,r){t.beginPath(),t.moveTo(e,i);for(var s=r.length,a=n-e,h=o-i,l=h/a,d=Math.sqrt(a*a+h*h),c=0,u=!0,f=0,p=+r[0];d>=.1;)(p=+r[c++%s])>d&&(p=d),f=Math.sqrt(p*p/(1+l*l)),e+=f=a<0?-f:f,i+=l*f,!0===u?t.lineTo(e,i):t.moveTo(e,i),d-=p,u=!u}var Ln={circle:Nn,dashedLine:Rn,database:jn,diamond:function(t,e,i,n){t.beginPath(),t.lineTo(e,i+n),t.lineTo(e+n,i),t.lineTo(e,i-n),t.lineTo(e-n,i),t.closePath()},ellipse:An,ellipse_vis:An,hexagon:function(t,e,i,n){t.beginPath();var o=2*Math.PI/6;t.moveTo(e+n,i);for(var r=1;r<6;r++)t.lineTo(e+n*Math.cos(o*r),i+n*Math.sin(o*r));t.closePath()},roundRect:Fn,square:function(t,e,i,n){t.beginPath(),t.rect(e-n,i-n,2*n,2*n),t.closePath()},star:function(t,e,i,n){t.beginPath(),i+=.1*(n*=.82);for(var o=0;o<10;o++){var r=o%2==0?1.3*n:.5*n;t.lineTo(e+r*Math.sin(2*o*Math.PI/10),i-r*Math.cos(2*o*Math.PI/10))}t.closePath()},triangle:function(t,e,i,n){t.beginPath(),i+=.275*(n*=1.15);var o=2*n,r=o/2,s=Math.sqrt(3)/6*o,a=Math.sqrt(o*o-r*r);t.moveTo(e,i-(a-s)),t.lineTo(e+r,i+s),t.lineTo(e-r,i+s),t.lineTo(e,i-(a-s)),t.closePath()},triangleDown:function(t,e,i,n){t.beginPath(),i-=.275*(n*=1.15);var o=2*n,r=o/2,s=Math.sqrt(3)/6*o,a=Math.sqrt(o*o-r*r);t.moveTo(e,i+(a-s)),t.lineTo(e+r,i-s),t.lineTo(e-r,i-s),t.lineTo(e,i+(a-s)),t.closePath()}};var Hn={exports:{}};!function(t){function e(t){if(t)return function(t){for(var i in e.prototype)t[i]=e.prototype[i];return t}(t)}t.exports=e,e.prototype.on=e.prototype.addEventListener=function(t,e){return this._callbacks=this._callbacks||{},(this._callbacks["$"+t]=this._callbacks["$"+t]||[]).push(e),this},e.prototype.once=function(t,e){function i(){this.off(t,i),e.apply(this,arguments)}return i.fn=e,this.on(t,i),this},e.prototype.off=e.prototype.removeListener=e.prototype.removeAllListeners=e.prototype.removeEventListener=function(t,e){if(this._callbacks=this._callbacks||{},0==arguments.length)return this._callbacks={},this;var i,n=this._callbacks["$"+t];if(!n)return this;if(1==arguments.length)return delete this._callbacks["$"+t],this;for(var o=0;o<n.length;o++)if((i=n[o])===e||i.fn===e){n.splice(o,1);break}return 0===n.length&&delete this._callbacks["$"+t],this},e.prototype.emit=function(t){this._callbacks=this._callbacks||{};for(var e=new Array(arguments.length-1),i=this._callbacks["$"+t],n=1;n<arguments.length;n++)e[n-1]=arguments[n];if(i){n=0;for(var o=(i=i.slice(0)).length;n<o;++n)i[n].apply(this,e)}return this},e.prototype.listeners=function(t){return this._callbacks=this._callbacks||{},this._callbacks["$"+t]||[]},e.prototype.hasListeners=function(t){return!!this.listeners(t).length}}(Hn);var Wn=Hn.exports,qn={};qn[oe("toStringTag")]="z";var Vn="[object z]"===String(qn),Un=n,Yn=Vn,Xn=y,Gn=B,Kn=oe("toStringTag"),$n=Un.Object,Zn="Arguments"==Gn(function(){return arguments}()),Qn=Yn?Gn:function(t){var e,i,n;return void 0===t?"Undefined":null===t?"Null":"string"==typeof(i=function(t,e){try{return t[e]}catch(t){}}(e=$n(t),Kn))?i:Zn?Gn(e):"Object"==(n=Gn(e))&&Xn(e.callee)?"Arguments":n},Jn=Qn,to=n.String,eo=function(t){if("Symbol"===Jn(t))throw TypeError("Cannot convert a Symbol value to a string");return to(t)},io=g,no=Oi,oo=eo,ro=H,so=io("".charAt),ao=io("".charCodeAt),ho=io("".slice),lo=function(t){return function(e,i){var n,o,r=oo(ro(e)),s=no(i),a=r.length;return s<0||s>=a?t?"":void 0:(n=ao(r,s))<55296||n>56319||s+1===a||(o=ao(r,s+1))<56320||o>57343?t?so(r,s):n:t?ho(r,s,s+2):o-56320+(n-55296<<10)+65536}},co={codeAt:lo(!1),charAt:lo(!0)},uo=y,fo=Nt,po=g(Function.toString);uo(fo.inspectSource)||(fo.inspectSource=function(t){return po(t)});var vo,go,yo,mo=fo.inspectSource,bo=y,wo=mo,ko=n.WeakMap,_o=bo(ko)&&/native code/.test(wo(ko)),xo=Pt.exports,Eo=Xt,Oo=xo("keys"),Co=function(t){return Oo[t]||(Oo[t]=Eo(t))},So=_o,To=n,Mo=g,Po=Y,Do=di,Io=Wt,Bo=Nt,zo=Co,No=Ri,Fo="Object already initialized",Ao=To.TypeError,jo=To.WeakMap;if(So||Bo.state){var Ro=Bo.state||(Bo.state=new jo),Lo=Mo(Ro.get),Ho=Mo(Ro.has),Wo=Mo(Ro.set);vo=function(t,e){if(Ho(Ro,t))throw new Ao(Fo);return e.facade=t,Wo(Ro,t,e),e},go=function(t){return Lo(Ro,t)||{}},yo=function(t){return Ho(Ro,t)}}else{var qo=zo("state");No[qo]=!0,vo=function(t,e){if(Io(t,qo))throw new Ao(Fo);return e.facade=t,Do(t,qo,e),e},go=function(t){return Io(t,qo)?t[qo]:{}},yo=function(t){return Io(t,qo)}}var Vo={set:vo,get:go,has:yo,enforce:function(t){return yo(t)?go(t):vo(t,{})},getterFor:function(t){return function(e){var i;if(!Po(e)||(i=go(e)).type!==t)throw Ao("Incompatible receiver, "+t+" required");return i}}},Uo=b,Yo=Wt,Xo=Function.prototype,Go=Uo&&Object.getOwnPropertyDescriptor,Ko=Yo(Xo,"name"),$o={EXISTS:Ko,PROPER:Ko&&"something"===function(){}.name,CONFIGURABLE:Ko&&(!Uo||Uo&&Go(Xo,"name").configurable)},Zo={},Qo=b,Jo=Ue,tr=Ve,er=$e,ir=V,nr=Ki;Zo.f=Qo&&!Jo?Object.defineProperties:function(t,e){er(t);for(var i,n=ir(e),o=nr(e),r=o.length,s=0;r>s;)tr.f(t,i=o[s++],n[i]);return t};var or,rr=Q("document","documentElement"),sr=$e,ar=Zo,hr=Yi,lr=Ri,dr=rr,cr=be,ur=Co("IE_PROTO"),fr=function(){},pr=function(t){return"<script>"+t+"</"+"script>"},vr=function(t){t.write(pr("")),t.close();var e=t.parentWindow.Object;return t=null,e},gr=function(){try{or=new ActiveXObject("htmlfile")}catch(t){}var t,e;gr="undefined"!=typeof document?document.domain&&or?vr(or):((e=cr("iframe")).style.display="none",dr.appendChild(e),e.src=String("javascript:"),(t=e.contentWindow.document).open(),t.write(pr("document.F=Object")),t.close(),t.F):vr(or);for(var i=hr.length;i--;)delete gr.prototype[hr[i]];return gr()};lr[ur]=!0;var yr,mr,br,wr=Object.create||function(t,e){var i;return null!==t?(fr.prototype=sr(t),i=new fr,fr.prototype=null,i[ur]=t):i=gr(),void 0===e?i:ar.f(i,e)},kr=!o((function(){function t(){}return t.prototype.constructor=null,Object.getPrototypeOf(new t)!==t.prototype})),_r=n,xr=Wt,Er=y,Or=Rt,Cr=kr,Sr=Co("IE_PROTO"),Tr=_r.Object,Mr=Tr.prototype,Pr=Cr?Tr.getPrototypeOf:function(t){var e=Or(t);if(xr(e,Sr))return e[Sr];var i=e.constructor;return Er(i)&&e instanceof i?i.prototype:e instanceof Tr?Mr:null},Dr=di,Ir=function(t,e,i,n){n&&n.enumerable?t[e]=i:Dr(t,e,i)},Br=o,zr=y,Nr=wr,Fr=Pr,Ar=Ir,jr=oe("iterator"),Rr=!1;[].keys&&("next"in(br=[].keys())?(mr=Fr(Fr(br)))!==Object.prototype&&(yr=mr):Rr=!0);var Lr=null==yr||Br((function(){var t={};return yr[jr].call(t)!==t}));zr((yr=Lr?{}:Nr(yr))[jr])||Ar(yr,jr,(function(){return this}));var Hr={IteratorPrototype:yr,BUGGY_SAFARI_ITERATORS:Rr},Wr=Qn,qr=Vn?{}.toString:function(){return"[object "+Wr(this)+"]"},Vr=Vn,Ur=Ve.f,Yr=di,Xr=Wt,Gr=qr,Kr=oe("toStringTag"),$r=function(t,e,i,n){if(t){var o=i?t:t.prototype;Xr(o,Kr)||Ur(o,Kr,{configurable:!0,value:e}),n&&!Vr&&Yr(o,"toString",Gr)}},Zr={},Qr=Hr.IteratorPrototype,Jr=wr,ts=M,es=$r,is=Zr,ns=function(){return this},os=n,rs=y,ss=os.String,as=os.TypeError,hs=g,ls=$e,ds=function(t){if("object"==typeof t||rs(t))return t;throw as("Can't set "+ss(t)+" as a prototype")},cs=Object.setPrototypeOf||("__proto__"in{}?function(){var t,e=!1,i={};try{(t=hs(Object.getOwnPropertyDescriptor(Object.prototype,"__proto__").set))(i,[]),e=i instanceof Array}catch(t){}return function(i,n){return ls(i),ds(n),e?t(i,n):i.__proto__=n,i}}():void 0),us=_i,fs=_,ps=function(t,e,i,n){var o=e+" Iterator";return t.prototype=Jr(Qr,{next:ts(+!n,i)}),es(t,o,!1,!0),is[o]=ns,t},vs=Pr,gs=$r,ys=Ir,ms=Zr,bs=$o.PROPER,ws=Hr.BUGGY_SAFARI_ITERATORS,ks=oe("iterator"),_s="keys",xs="values",Es="entries",Os=function(){return this},Cs=function(t,e,i,n,o,r,s){ps(i,e,n);var a,h,l,d=function(t){if(t===o&&v)return v;if(!ws&&t in f)return f[t];switch(t){case _s:case xs:case Es:return function(){return new i(this,t)}}return function(){return new i(this)}},c=e+" Iterator",u=!1,f=t.prototype,p=f[ks]||f["@@iterator"]||o&&f[o],v=!ws&&p||d(o),g="Array"==e&&f.entries||p;if(g&&(a=vs(g.call(new t)))!==Object.prototype&&a.next&&(gs(a,c,!0,!0),ms[c]=Os),bs&&o==xs&&p&&p.name!==xs&&(u=!0,v=function(){return fs(p,this)}),o)if(h={values:d(xs),keys:r?v:d(_s),entries:d(Es)},s)for(l in h)(ws||u||!(l in f))&&ys(f,l,h[l]);else us({target:e,proto:!0,forced:ws||u},h);return s&&f[ks]!==v&&ys(f,ks,v,{name:o}),ms[e]=v,h},Ss=co.charAt,Ts=eo,Ms=Vo,Ps=Cs,Ds="String Iterator",Is=Ms.set,Bs=Ms.getterFor(Ds);Ps(String,"String",(function(t){Is(this,{type:Ds,string:Ts(t),index:0})}),(function(){var t,e=Bs(this),i=e.string,n=e.index;return n>=i.length?{value:void 0,done:!0}:(t=Ss(i,n),e.index+=t.length,{value:t,done:!1})}));var zs=_,Ns=$e,Fs=Ot,As=function(t,e,i){var n,o;Ns(t);try{if(!(n=Fs(t,"return"))){if("throw"===e)throw i;return i}n=zs(n,t)}catch(t){o=!0,n=t}if("throw"===e)throw i;if(o)throw n;return Ns(n),i},js=$e,Rs=As,Ls=Zr,Hs=oe("iterator"),Ws=Array.prototype,qs=function(t){return void 0!==t&&(Ls.Array===t||Ws[Hs]===t)},Vs=g,Us=o,Ys=y,Xs=Qn,Gs=mo,Ks=function(){},$s=[],Zs=Q("Reflect","construct"),Qs=/^\s*(?:class|function)\b/,Js=Vs(Qs.exec),ta=!Qs.exec(Ks),ea=function(t){if(!Ys(t))return!1;try{return Zs(Ks,$s,t),!0}catch(t){return!1}},ia=function(t){if(!Ys(t))return!1;switch(Xs(t)){case"AsyncFunction":case"GeneratorFunction":case"AsyncGeneratorFunction":return!1}try{return ta||!!Js(Qs,Gs(t))}catch(t){return!0}};ia.sham=!0;var na=!Zs||Us((function(){var t;return ea(ea.call)||!ea(Object)||!ea((function(){t=!0}))||t}))?ia:ea,oa=ve,ra=Ve,sa=M,aa=function(t,e,i){var n=oa(e);n in t?ra.f(t,n,sa(0,i)):t[n]=i},ha=Qn,la=Ot,da=Zr,ca=oe("iterator"),ua=function(t){if(null!=t)return la(t,ca)||la(t,"@@iterator")||da[ha(t)]},fa=_,pa=xt,va=$e,ga=bt,ya=ua,ma=n.TypeError,ba=function(t,e){var i=arguments.length<2?ya(t):e;if(pa(i))return va(fa(i,t));throw ma(ga(t)+" is not iterable")},wa=qe,ka=_,_a=Rt,xa=function(t,e,i,n){try{return n?e(js(i)[0],i[1]):e(i)}catch(e){Rs(t,"throw",e)}},Ea=qs,Oa=na,Ca=Bi,Sa=aa,Ta=ba,Ma=ua,Pa=n.Array,Da=oe("iterator"),Ia=!1;try{var Ba=0,za={next:function(){return{done:!!Ba++}},return:function(){Ia=!0}};za[Da]=function(){return this},Array.from(za,(function(){throw 2}))}catch(t){}var Na=function(t){var e=_a(t),i=Oa(this),n=arguments.length,o=n>1?arguments[1]:void 0,r=void 0!==o;r&&(o=wa(o,n>2?arguments[2]:void 0));var s,a,h,l,d,c,u=Ma(e),f=0;if(!u||this==Pa&&Ea(u))for(s=Ca(e),a=i?new this(s):Pa(s);s>f;f++)c=r?o(e[f],f):e[f],Sa(a,f,c);else for(d=(l=Ta(e,u)).next,a=i?new this:[];!(h=ka(d,l)).done;f++)c=r?xa(l,o,[h.value,f],!0):h.value,Sa(a,f,c);return a.length=f,a},Fa=function(t,e){if(!e&&!Ia)return!1;var i=!1;try{var n={};n[Da]=function(){return{next:function(){return{done:i=!0}}}},t(n)}catch(t){}return i};_i({target:"Array",stat:!0,forced:!Fa((function(t){Array.from(t)}))},{from:Na});var Aa=X.Array.from,ja=Aa,Ra=V,La=Zr,Ha=Vo;Ve.f;var Wa=Cs,qa="Array Iterator",Va=Ha.set,Ua=Ha.getterFor(qa);Wa(Array,"Array",(function(t,e){Va(this,{type:qa,target:Ra(t),index:0,kind:e})}),(function(){var t=Ua(this),e=t.target,i=t.kind,n=t.index++;return!e||n>=e.length?(t.target=void 0,{value:void 0,done:!0}):"keys"==i?{value:n,done:!1}:"values"==i?{value:e[n],done:!1}:{value:[n,e[n]],done:!1}}),"values"),La.Arguments=La.Array;var Ya=ua,Xa={CSSRuleList:0,CSSStyleDeclaration:0,CSSValueList:0,ClientRectList:0,DOMRectList:0,DOMStringList:0,DOMTokenList:1,DataTransferItemList:0,FileList:0,HTMLAllCollection:0,HTMLCollection:0,HTMLFormElement:0,HTMLSelectElement:0,MediaList:0,MimeTypeArray:0,NamedNodeMap:0,NodeList:1,PaintRequestList:0,Plugin:0,PluginArray:0,SVGLengthList:0,SVGNumberList:0,SVGPathSegList:0,SVGPointList:0,SVGStringList:0,SVGTransformList:0,SourceBufferList:0,StyleSheetList:0,TextTrackCueList:0,TextTrackList:0,TouchList:0},Ga=n,Ka=Qn,$a=di,Za=Zr,Qa=oe("toStringTag");for(var Ja in Xa){var th=Ga[Ja],eh=th&&th.prototype;eh&&Ka(eh)!==Qa&&$a(eh,Qa,Ja),Za[Ja]=Za.Array}var ih=Ya,nh=B,oh=Array.isArray||function(t){return"Array"==nh(t)},rh={},sh=Ui,ah=Yi.concat("length","prototype");rh.f=Object.getOwnPropertyNames||function(t){return sh(t,ah)};var hh={},lh=Mi,dh=Bi,ch=aa,uh=n.Array,fh=Math.max,ph=function(t,e,i){for(var n=dh(t),o=lh(e,n),r=lh(void 0===i?n:i,n),s=uh(fh(r-o,0)),a=0;o<r;o++,a++)ch(s,a,t[o]);return s.length=a,s},vh=B,gh=V,yh=rh.f,mh=ph,bh="object"==typeof window&&window&&Object.getOwnPropertyNames?Object.getOwnPropertyNames(window):[];hh.f=function(t){return bh&&"Window"==vh(t)?function(t){try{return yh(t)}catch(t){return mh(bh)}}(t):yh(gh(t))};var wh={},kh=oe;wh.f=kh;var _h=X,xh=Wt,Eh=wh,Oh=Ve.f,Ch=function(t){var e=_h.Symbol||(_h.Symbol={});xh(e,t)||Oh(e,t,{value:Eh.f(t)})},Sh=n,Th=oh,Mh=na,Ph=Y,Dh=oe("species"),Ih=Sh.Array,Bh=function(t){var e;return Th(t)&&(e=t.constructor,(Mh(e)&&(e===Ih||Th(e.prototype))||Ph(e)&&null===(e=e[Dh]))&&(e=void 0)),void 0===e?Ih:e},zh=function(t,e){return new(Bh(t))(0===e?0:e)},Nh=qe,Fh=R,Ah=Rt,jh=Bi,Rh=zh,Lh=g([].push),Hh=function(t){var e=1==t,i=2==t,n=3==t,o=4==t,r=6==t,s=7==t,a=5==t||r;return function(h,l,d,c){for(var u,f,p=Ah(h),v=Fh(p),g=Nh(l,d),y=jh(v),m=0,b=c||Rh,w=e?b(h,y):i||s?b(h,0):void 0;y>m;m++)if((a||m in v)&&(f=g(u=v[m],m,p),t))if(e)w[m]=f;else if(f)switch(t){case 3:return!0;case 5:return u;case 6:return m;case 2:Lh(w,u)}else switch(t){case 4:return!1;case 7:Lh(w,u)}return r?-1:n||o?o:w}},Wh={forEach:Hh(0),map:Hh(1),filter:Hh(2),some:Hh(3),every:Hh(4),find:Hh(5),findIndex:Hh(6),filterReject:Hh(7)},qh=_i,Vh=n,Uh=Q,Yh=d,Xh=_,Gh=g,Kh=b,$h=dt,Zh=o,Qh=Wt,Jh=oh,tl=y,el=Y,il=J,nl=yt,ol=$e,rl=Rt,sl=V,al=ve,hl=eo,ll=M,dl=wr,cl=Ki,ul=rh,fl=hh,pl=$i,vl=m,gl=Ve,yl=Zo,ml=x,bl=fn,wl=Ir,kl=Pt.exports,_l=Ri,xl=Xt,El=oe,Ol=wh,Cl=Ch,Sl=$r,Tl=Vo,Ml=Wh.forEach,Pl=Co("hidden"),Dl="Symbol",Il=El("toPrimitive"),Bl=Tl.set,zl=Tl.getterFor(Dl),Nl=Object.prototype,Fl=Vh.Symbol,Al=Fl&&Fl.prototype,jl=Vh.TypeError,Rl=Vh.QObject,Ll=Uh("JSON","stringify"),Hl=vl.f,Wl=gl.f,ql=fl.f,Vl=ml.f,Ul=Gh([].push),Yl=kl("symbols"),Xl=kl("op-symbols"),Gl=kl("string-to-symbol-registry"),Kl=kl("symbol-to-string-registry"),$l=kl("wks"),Zl=!Rl||!Rl.prototype||!Rl.prototype.findChild,Ql=Kh&&Zh((function(){return 7!=dl(Wl({},"a",{get:function(){return Wl(this,"a",{value:7}).a}})).a}))?function(t,e,i){var n=Hl(Nl,e);n&&delete Nl[e],Wl(t,e,i),n&&t!==Nl&&Wl(Nl,e,n)}:Wl,Jl=function(t,e){var i=Yl[t]=dl(Al);return Bl(i,{type:Dl,tag:t,description:e}),Kh||(i.description=e),i},td=function(t,e,i){t===Nl&&td(Xl,e,i),ol(t);var n=al(e);return ol(i),Qh(Yl,n)?(i.enumerable?(Qh(t,Pl)&&t[Pl][n]&&(t[Pl][n]=!1),i=dl(i,{enumerable:ll(0,!1)})):(Qh(t,Pl)||Wl(t,Pl,ll(1,{})),t[Pl][n]=!0),Ql(t,n,i)):Wl(t,n,i)},ed=function(t,e){ol(t);var i=sl(e),n=cl(i).concat(rd(i));return Ml(n,(function(e){Kh&&!Xh(id,i,e)||td(t,e,i[e])})),t},id=function(t){var e=al(t),i=Xh(Vl,this,e);return!(this===Nl&&Qh(Yl,e)&&!Qh(Xl,e))&&(!(i||!Qh(this,e)||!Qh(Yl,e)||Qh(this,Pl)&&this[Pl][e])||i)},nd=function(t,e){var i=sl(t),n=al(e);if(i!==Nl||!Qh(Yl,n)||Qh(Xl,n)){var o=Hl(i,n);return!o||!Qh(Yl,n)||Qh(i,Pl)&&i[Pl][n]||(o.enumerable=!0),o}},od=function(t){var e=ql(sl(t)),i=[];return Ml(e,(function(t){Qh(Yl,t)||Qh(_l,t)||Ul(i,t)})),i},rd=function(t){var e=t===Nl,i=ql(e?Xl:sl(t)),n=[];return Ml(i,(function(t){!Qh(Yl,t)||e&&!Qh(Nl,t)||Ul(n,Yl[t])})),n};if($h||(Fl=function(){if(il(Al,this))throw jl("Symbol is not a constructor");var t=arguments.length&&void 0!==arguments[0]?hl(arguments[0]):void 0,e=xl(t),i=function(t){this===Nl&&Xh(i,Xl,t),Qh(this,Pl)&&Qh(this[Pl],e)&&(this[Pl][e]=!1),Ql(this,e,ll(1,t))};return Kh&&Zl&&Ql(Nl,e,{configurable:!0,set:i}),Jl(e,t)},wl(Al=Fl.prototype,"toString",(function(){return zl(this).tag})),wl(Fl,"withoutSetter",(function(t){return Jl(xl(t),t)})),ml.f=id,gl.f=td,yl.f=ed,vl.f=nd,ul.f=fl.f=od,pl.f=rd,Ol.f=function(t){return Jl(El(t),t)},Kh&&Wl(Al,"description",{configurable:!0,get:function(){return zl(this).description}})),qh({global:!0,wrap:!0,forced:!$h,sham:!$h},{Symbol:Fl}),Ml(cl($l),(function(t){Cl(t)})),qh({target:Dl,stat:!0,forced:!$h},{for:function(t){var e=hl(t);if(Qh(Gl,e))return Gl[e];var i=Fl(e);return Gl[e]=i,Kl[i]=e,i},keyFor:function(t){if(!nl(t))throw jl(t+" is not a symbol");if(Qh(Kl,t))return Kl[t]},useSetter:function(){Zl=!0},useSimple:function(){Zl=!1}}),qh({target:"Object",stat:!0,forced:!$h,sham:!Kh},{create:function(t,e){return void 0===e?dl(t):ed(dl(t),e)},defineProperty:td,defineProperties:ed,getOwnPropertyDescriptor:nd}),qh({target:"Object",stat:!0,forced:!$h},{getOwnPropertyNames:od,getOwnPropertySymbols:rd}),qh({target:"Object",stat:!0,forced:Zh((function(){pl.f(1)}))},{getOwnPropertySymbols:function(t){return pl.f(rl(t))}}),Ll){var sd=!$h||Zh((function(){var t=Fl();return"[null]"!=Ll([t])||"{}"!=Ll({a:t})||"{}"!=Ll(Object(t))}));qh({target:"JSON",stat:!0,forced:sd},{stringify:function(t,e,i){var n=bl(arguments),o=e;if((el(e)||void 0!==t)&&!nl(t))return Jh(e)||(e=function(t,e){if(tl(o)&&(e=Xh(o,this,t,e)),!nl(e))return e}),n[1]=e,Yh(Ll,null,n)}})}if(!Al[Il]){var ad=Al.valueOf;wl(Al,Il,(function(t){return Xh(ad,this)}))}Sl(Fl,Dl),_l[Pl]=!0;var hd=X.Object.getOwnPropertySymbols,ld={exports:{}},dd=_i,cd=o,ud=V,fd=m.f,pd=b,vd=cd((function(){fd(1)}));dd({target:"Object",stat:!0,forced:!pd||vd,sham:!pd},{getOwnPropertyDescriptor:function(t,e){return fd(ud(t),e)}});var gd=X.Object,yd=ld.exports=function(t,e){return gd.getOwnPropertyDescriptor(t,e)};gd.getOwnPropertyDescriptor.sham&&(yd.sham=!0);var md=ld.exports,bd=md,wd=Q,kd=rh,_d=$i,xd=$e,Ed=g([].concat),Od=wd("Reflect","ownKeys")||function(t){var e=kd.f(xd(t)),i=_d.f;return i?Ed(e,i(t)):e},Cd=Od,Sd=V,Td=m,Md=aa;_i({target:"Object",stat:!0,sham:!b},{getOwnPropertyDescriptors:function(t){for(var e,i,n=Sd(t),o=Td.f,r=Cd(n),s={},a=0;r.length>a;)void 0!==(i=o(n,e=r[a++]))&&Md(s,e,i);return s}});var Pd=X.Object.getOwnPropertyDescriptors,Dd={exports:{}},Id=_i,Bd=b,zd=Zo.f;Id({target:"Object",stat:!0,forced:Object.defineProperties!==zd,sham:!Bd},{defineProperties:zd});var Nd=X.Object,Fd=Dd.exports=function(t,e){return Nd.defineProperties(t,e)};Nd.defineProperties.sham&&(Fd.sham=!0);var Ad=Dd.exports,jd={exports:{}},Rd=_i,Ld=b,Hd=Ve.f;Rd({target:"Object",stat:!0,forced:Object.defineProperty!==Hd,sham:!Ld},{defineProperty:Hd});var Wd=X.Object,qd=jd.exports=function(t,e,i){return Wd.defineProperty(t,e,i)};Wd.defineProperty.sham&&(qd.sham=!0);var Vd=jd.exports,Ud=Vd;function Yd(t,e){if(!(t instanceof e))throw new TypeError("Cannot call a class as a function")}var Xd=Vd;function Gd(t,e){for(var i=0;i<e.length;i++){var n=e[i];n.enumerable=n.enumerable||!1,n.configurable=!0,"value"in n&&(n.writable=!0),Xd(t,n.key,n)}}function Kd(t,e,i){return e&&Gd(t.prototype,e),i&&Gd(t,i),Xd(t,"prototype",{writable:!1}),t}function $d(t,e,i){return e in t?Xd(t,e,{value:i,enumerable:!0,configurable:!0,writable:!0}):t[e]=i,t}_i({target:"Array",stat:!0},{isArray:oh});var Zd=X.Array.isArray,Qd=Zd;var Jd=o,tc=at,ec=oe("species"),ic=function(t){return tc>=51||!Jd((function(){var e=[];return(e.constructor={})[ec]=function(){return{foo:1}},1!==e[t](Boolean).foo}))},nc=_i,oc=n,rc=o,sc=oh,ac=Y,hc=Rt,lc=Bi,dc=aa,cc=zh,uc=ic,fc=at,pc=oe("isConcatSpreadable"),vc=9007199254740991,gc="Maximum allowed index exceeded",yc=oc.TypeError,mc=fc>=51||!rc((function(){var t=[];return t[pc]=!1,t.concat()[0]!==t})),bc=uc("concat"),wc=function(t){if(!ac(t))return!1;var e=t[pc];return void 0!==e?!!e:sc(t)};nc({target:"Array",proto:!0,forced:!mc||!bc},{concat:function(t){var e,i,n,o,r,s=hc(this),a=cc(s,0),h=0;for(e=-1,n=arguments.length;e<n;e++)if(wc(r=-1===e?s:arguments[e])){if(h+(o=lc(r))>vc)throw yc(gc);for(i=0;i<o;i++,h++)i in r&&dc(a,h,r[i])}else{if(h>=vc)throw yc(gc);dc(a,h++,r)}return a.length=h,a}}),Ch("asyncIterator"),Ch("hasInstance"),Ch("isConcatSpreadable"),Ch("iterator"),Ch("match"),Ch("matchAll"),Ch("replace"),Ch("search"),Ch("species"),Ch("split"),Ch("toPrimitive"),Ch("toStringTag"),Ch("unscopables"),$r(n.JSON,"JSON",!0);var kc=X.Symbol,_c=kc;Ch("asyncDispose"),Ch("dispose"),Ch("matcher"),Ch("metadata"),Ch("observable"),Ch("patternMatch"),Ch("replaceAll");var xc=_c;var Ec=_i,Oc=n,Cc=oh,Sc=na,Tc=Y,Mc=Mi,Pc=Bi,Dc=V,Ic=aa,Bc=oe,zc=fn,Nc=ic("slice"),Fc=Bc("species"),Ac=Oc.Array,jc=Math.max;Ec({target:"Array",proto:!0,forced:!Nc},{slice:function(t,e){var i,n,o,r=Dc(this),s=Pc(r),a=Mc(t,s),h=Mc(void 0===e?s:e,s);if(Cc(r)&&(i=r.constructor,(Sc(i)&&(i===Ac||Cc(i.prototype))||Tc(i)&&null===(i=i[Fc]))&&(i=void 0),i===Ac||void 0===i))return zc(r,a,h);for(n=new(void 0===i?Ac:i)(jc(h-a,0)),o=0;a<h;a++,o++)a in r&&Ic(n,o,r[a]);return n.length=o,n}});var Rc=Tn("Array").slice,Lc=J,Hc=Rc,Wc=Array.prototype,qc=function(t){var e=t.slice;return t===Wc||Lc(Wc,t)&&e===Wc.slice?Hc:e},Vc=qc,Uc=Vc,Yc=Aa;function Xc(t,e){(null==e||e>t.length)&&(e=t.length);for(var i=0,n=new Array(e);i<e;i++)n[i]=t[i];return n}function Gc(t,e){var i;if(t){if("string"==typeof t)return Xc(t,e);var n=Uc(i=Object.prototype.toString.call(t)).call(i,8,-1);return"Object"===n&&t.constructor&&(n=t.constructor.name),"Map"===n||"Set"===n?Yc(t):"Arguments"===n||/^(?:Ui|I)nt(?:8|16|32)(?:Clamped)?Array$/.test(n)?Xc(t,e):void 0}}function Kc(t,e){return function(t){if(Qd(t))return t}(t)||function(t,e){var i=null==t?null:void 0!==xc&&ih(t)||t["@@iterator"];if(null!=i){var n,o,r=[],s=!0,a=!1;try{for(i=i.call(t);!(s=(n=i.next()).done)&&(r.push(n.value),!e||r.length!==e);s=!0);}catch(t){a=!0,o=t}finally{try{s||null==i.return||i.return()}finally{if(a)throw o}}return r}}(t,e)||Gc(t,e)||function(){throw new TypeError("Invalid attempt to destructure non-iterable instance.\nIn order to be iterable, non-array objects must have a [Symbol.iterator]() method.")}()}var $c=wh.f("iterator"),Zc=$c;function Qc(t){return Qc="function"==typeof xc&&"symbol"==typeof Zc?function(t){return typeof t}:function(t){return t&&"function"==typeof xc&&t.constructor===xc&&t!==xc.prototype?"symbol":typeof t},Qc(t)}function Jc(t){return function(t){if(Qd(t))return Xc(t)}(t)||function(t){if(void 0!==xc&&null!=ih(t)||null!=t["@@iterator"])return Yc(t)}(t)||Gc(t)||function(){throw new TypeError("Invalid attempt to spread non-iterable instance.\nIn order to be iterable, non-array objects must have a [Symbol.iterator]() method.")}()}var tu=kc,eu=Tn("Array").concat,iu=J,nu=eu,ou=Array.prototype,ru=function(t){var e=t.concat;return t===ou||iu(ou,t)&&e===ou.concat?nu:e},su=ru,au=Vc;_i({target:"Reflect",stat:!0},{ownKeys:Od});var hu=X.Reflect.ownKeys,lu=Zd,du=Wh.map;_i({target:"Array",proto:!0,forced:!ic("map")},{map:function(t){return du(this,t,arguments.length>1?arguments[1]:void 0)}});var cu=Tn("Array").map,uu=J,fu=cu,pu=Array.prototype,vu=function(t){var e=t.map;return t===pu||uu(pu,t)&&e===pu.map?fu:e},gu=vu,yu=Rt,mu=Ki;_i({target:"Object",stat:!0,forced:o((function(){mu(1)}))},{keys:function(t){return mu(yu(t))}});var bu=X.Object.keys,wu=_i,ku=g,_u=n.Date,xu=ku(_u.prototype.getTime);wu({target:"Date",stat:!0},{now:function(){return xu(new _u)}});var Eu=X.Date.now,Ou=o,Cu=function(t,e){var i=[][t];return!!i&&Ou((function(){i.call(null,e||function(){return 1},1)}))},Su=Wh.forEach,Tu=Cu("forEach")?[].forEach:function(t){return Su(this,t,arguments.length>1?arguments[1]:void 0)};_i({target:"Array",proto:!0,forced:[].forEach!=Tu},{forEach:Tu});var Mu=Tn("Array").forEach,Pu=Qn,Du=Wt,Iu=J,Bu=Mu,zu=Array.prototype,Nu={DOMTokenList:!0,NodeList:!0},Fu=function(t){var e=t.forEach;return t===zu||Iu(zu,t)&&e===zu.forEach||Du(Nu,Pu(t))?Bu:e},Au=_i,ju=oh,Ru=g([].reverse),Lu=[1,2];Au({target:"Array",proto:!0,forced:String(Lu)===String(Lu.reverse())},{reverse:function(){return ju(this)&&(this.length=this.length),Ru(this)}});var Hu=Tn("Array").reverse,Wu=J,qu=Hu,Vu=Array.prototype,Uu=function(t){var e=t.reverse;return t===Vu||Wu(Vu,t)&&e===Vu.reverse?qu:e},Yu=Uu,Xu=_i,Gu=n,Ku=Mi,$u=Oi,Zu=Bi,Qu=Rt,Ju=zh,tf=aa,ef=ic("splice"),nf=Gu.TypeError,of=Math.max,rf=Math.min,sf=9007199254740991,af="Maximum allowed length exceeded";Xu({target:"Array",proto:!0,forced:!ef},{splice:function(t,e){var i,n,o,r,s,a,h=Qu(this),l=Zu(h),d=Ku(t,l),c=arguments.length;if(0===c?i=n=0:1===c?(i=0,n=l-d):(i=c-2,n=rf(of($u(e),0),l-d)),l+i-n>sf)throw nf(af);for(o=Ju(h,n),r=0;r<n;r++)(s=d+r)in h&&tf(o,r,h[s]);if(o.length=n,i<n){for(r=d;r<l-n;r++)a=r+i,(s=r+n)in h?h[a]=h[s]:delete h[a];for(r=l;r>l-n+i;r--)delete h[r-1]}else if(i>n)for(r=l-n;r>d;r--)a=r+i-1,(s=r+n-1)in h?h[a]=h[s]:delete h[a];for(r=0;r<i;r++)h[r+d]=arguments[r+2];return h.length=l-n+i,o}});var hf=Tn("Array").splice,lf=J,df=hf,cf=Array.prototype,uf=function(t){var e=t.splice;return t===cf||lf(cf,t)&&e===cf.splice?df:e},ff=uf,pf=ji.includes;_i({target:"Array",proto:!0},{includes:function(t){return pf(this,t,arguments.length>1?arguments[1]:void 0)}});var vf=Tn("Array").includes,gf=Y,yf=B,mf=oe("match"),bf=function(t){var e;return gf(t)&&(void 0!==(e=t[mf])?!!e:"RegExp"==yf(t))},wf=n.TypeError,kf=oe("match"),_f=_i,xf=function(t){if(bf(t))throw wf("The method doesn't accept regular expressions");return t},Ef=H,Of=eo,Cf=function(t){var e=/./;try{"/./"[t](e)}catch(i){try{return e[kf]=!1,"/./"[t](e)}catch(t){}}return!1},Sf=g("".indexOf);_f({target:"String",proto:!0,forced:!Cf("includes")},{includes:function(t){return!!~Sf(Of(Ef(this)),Of(xf(t)),arguments.length>1?arguments[1]:void 0)}});var Tf=Tn("String").includes,Mf=J,Pf=vf,Df=Tf,If=Array.prototype,Bf=String.prototype,zf=function(t){var e=t.includes;return t===If||Mf(If,t)&&e===If.includes?Pf:"string"==typeof t||t===Bf||Mf(Bf,t)&&e===Bf.includes?Df:e},Nf=zf,Ff=Rt,Af=Pr,jf=kr;_i({target:"Object",stat:!0,forced:o((function(){Af(1)})),sham:!jf},{getPrototypeOf:function(t){return Af(Ff(t))}});var Rf=X.Object.getPrototypeOf,Lf=Rf,Hf=Wh.filter;_i({target:"Array",proto:!0,forced:!ic("filter")},{filter:function(t){return Hf(this,t,arguments.length>1?arguments[1]:void 0)}});var Wf=Tn("Array").filter,qf=J,Vf=Wf,Uf=Array.prototype,Yf=function(t){var e=t.filter;return t===Uf||qf(Uf,t)&&e===Uf.filter?Vf:e},Xf=Yf,Gf=b,Kf=g,$f=Ki,Zf=V,Qf=Kf(x.f),Jf=Kf([].push),tp=function(t){return function(e){for(var i,n=Zf(e),o=$f(n),r=o.length,s=0,a=[];r>s;)i=o[s++],Gf&&!Qf(n,i)||Jf(a,t?[i,n[i]]:n[i]);return a}},ep={entries:tp(!0),values:tp(!1)}.values;_i({target:"Object",stat:!0},{values:function(t){return ep(t)}});var ip=X.Object.values,np="\t\n\v\f\r  \u2028\u2029\ufeff",op=H,rp=eo,sp=g("".replace),ap="[\t\n\v\f\r  \u2028\u2029\ufeff]",hp=RegExp("^"+ap+ap+"*"),lp=RegExp(ap+ap+"*$"),dp=function(t){return function(e){var i=rp(op(e));return 1&t&&(i=sp(i,hp,"")),2&t&&(i=sp(i,lp,"")),i}},cp={start:dp(1),end:dp(2),trim:dp(3)},up=n,fp=o,pp=g,vp=eo,gp=cp.trim,yp=np,mp=up.parseInt,bp=up.Symbol,wp=bp&&bp.iterator,kp=/^[+-]?0x/i,_p=pp(kp.exec),xp=8!==mp(yp+"08")||22!==mp(yp+"0x16")||wp&&!fp((function(){mp(Object(wp))}))?function(t,e){var i=gp(vp(t));return mp(i,e>>>0||(_p(kp,i)?16:10))}:mp;_i({global:!0,forced:parseInt!=xp},{parseInt:xp});var Ep=X.parseInt,Op=_i,Cp=ji.indexOf,Sp=Cu,Tp=g([].indexOf),Mp=!!Tp&&1/Tp([1],1,-0)<0,Pp=Sp("indexOf");Op({target:"Array",proto:!0,forced:Mp||!Pp},{indexOf:function(t){var e=arguments.length>1?arguments[1]:void 0;return Mp?Tp(this,t,e)||0:Cp(this,t,e)}});var Dp=Tn("Array").indexOf,Ip=J,Bp=Dp,zp=Array.prototype,Np=function(t){var e=t.indexOf;return t===zp||Ip(zp,t)&&e===zp.indexOf?Bp:e},Fp=Np,Ap=$o.PROPER,jp=o,Rp=np,Lp=cp.trim;_i({target:"String",proto:!0,forced:function(t){return jp((function(){return!!Rp[t]()||""!==""[t]()||Ap&&Rp[t].name!==t}))}("trim")},{trim:function(){return Lp(this)}});var Hp=Tn("String").trim,Wp=J,qp=Hp,Vp=String.prototype,Up=function(t){var e=t.trim;return"string"==typeof t||t===Vp||Wp(Vp,t)&&e===Vp.trim?qp:e},Yp=Up;_i({target:"Object",stat:!0,sham:!b},{create:wr});var Xp=X.Object,Gp=function(t,e){return Xp.create(t,e)},Kp=Gp,$p=_i,Zp=Q,Qp=d,Jp=g,tv=o,ev=n.Array,iv=Zp("JSON","stringify"),nv=Jp(/./.exec),ov=Jp("".charAt),rv=Jp("".charCodeAt),sv=Jp("".replace),av=Jp(1..toString),hv=/[\uD800-\uDFFF]/g,lv=/^[\uD800-\uDBFF]$/,dv=/^[\uDC00-\uDFFF]$/,cv=function(t,e,i){var n=ov(i,e-1),o=ov(i,e+1);return nv(lv,t)&&!nv(dv,o)||nv(dv,t)&&!nv(lv,n)?"\\u"+av(rv(t,0),16):t},uv=tv((function(){return'"\\udf06\\ud834"'!==iv("\udf06\ud834")||'"\\udead"'!==iv("\udead")}));iv&&$p({target:"JSON",stat:!0,forced:uv},{stringify:function(t,e,i){for(var n=0,o=arguments.length,r=ev(o);n<o;n++)r[n]=arguments[n];var s=Qp(iv,null,r);return"string"==typeof s?sv(s,hv,cv):s}});var fv=X,pv=d;fv.JSON||(fv.JSON={stringify:JSON.stringify});var vv=function(t,e,i){return pv(fv.JSON.stringify,null,arguments)},gv=vv,yv=n.TypeError,mv=_i,bv=n,wv=d,kv=y,_v=fn,xv=function(t,e){if(t<e)throw yv("Not enough arguments");return t},Ev=/MSIE .\./.test(tt),Ov=bv.Function,Cv=function(t){return function(e,i){var n=xv(arguments.length,1)>2,o=kv(e)?e:Ov(e),r=n?_v(arguments,2):void 0;return t(n?function(){wv(o,this,r)}:o,i)}};mv({global:!0,bind:!0,forced:Ev},{setTimeout:Cv(bv.setTimeout),setInterval:Cv(bv.setInterval)});var Sv=X.setTimeout,Tv=Rt,Mv=Mi,Pv=Bi,Dv=function(t){for(var e=Tv(this),i=Pv(e),n=arguments.length,o=Mv(n>1?arguments[1]:void 0,i),r=n>2?arguments[2]:void 0,s=void 0===r?i:Mv(r,i);s>o;)e[o++]=t;return e};_i({target:"Array",proto:!0},{fill:Dv});var Iv,Bv=Tn("Array").fill,zv=J,Nv=Bv,Fv=Array.prototype,Av=function(t){var e=t.fill;return t===Fv||zv(Fv,t)&&e===Fv.fill?Nv:e},jv=Av;function Rv(){return Rv=Object.assign||function(t){for(var e=1;e<arguments.length;e++){var i=arguments[e];for(var n in i)Object.prototype.hasOwnProperty.call(i,n)&&(t[n]=i[n])}return t},Rv.apply(this,arguments)}function Lv(t,e){t.prototype=Object.create(e.prototype),t.prototype.constructor=t,t.__proto__=e}function Hv(t){if(void 0===t)throw new ReferenceError("this hasn't been initialised - super() hasn't been called");return t}Iv="function"!=typeof Object.assign?function(t){if(null==t)throw new TypeError("Cannot convert undefined or null to object");for(var e=Object(t),i=1;i<arguments.length;i++){var n=arguments[i];if(null!=n)for(var o in n)n.hasOwnProperty(o)&&(e[o]=n[o])}return e}:Object.assign;var Wv,qv=Iv,Vv=["","webkit","Moz","MS","ms","o"],Uv="undefined"==typeof document?{style:{}}:document.createElement("div"),Yv=Math.round,Xv=Math.abs,Gv=Date.now;function Kv(t,e){for(var i,n,o=e[0].toUpperCase()+e.slice(1),r=0;r<Vv.length;){if((n=(i=Vv[r])?i+o:e)in t)return n;r++}}Wv="undefined"==typeof window?{}:window;var $v=Kv(Uv.style,"touchAction"),Zv=void 0!==$v;var Qv="compute",Jv="auto",tg="manipulation",eg="none",ig="pan-x",ng="pan-y",og=function(){if(!Zv)return!1;var t={},e=Wv.CSS&&Wv.CSS.supports;return["auto","manipulation","pan-y","pan-x","pan-x pan-y","none"].forEach((function(i){return t[i]=!e||Wv.CSS.supports("touch-action",i)})),t}(),rg="ontouchstart"in Wv,sg=void 0!==Kv(Wv,"PointerEvent"),ag=rg&&/mobile|tablet|ip(ad|hone|od)|android/i.test(navigator.userAgent),hg="touch",lg="mouse",dg=16,cg=24,ug=["x","y"],fg=["clientX","clientY"];function pg(t,e,i){var n;if(t)if(t.forEach)t.forEach(e,i);else if(void 0!==t.length)for(n=0;n<t.length;)e.call(i,t[n],n,t),n++;else for(n in t)t.hasOwnProperty(n)&&e.call(i,t[n],n,t)}function vg(t,e){return"function"==typeof t?t.apply(e&&e[0]||void 0,e):t}function gg(t,e){return t.indexOf(e)>-1}var yg=function(){function t(t,e){this.manager=t,this.set(e)}var e=t.prototype;return e.set=function(t){t===Qv&&(t=this.compute()),Zv&&this.manager.element.style&&og[t]&&(this.manager.element.style[$v]=t),this.actions=t.toLowerCase().trim()},e.update=function(){this.set(this.manager.options.touchAction)},e.compute=function(){var t=[];return pg(this.manager.recognizers,(function(e){vg(e.options.enable,[e])&&(t=t.concat(e.getTouchAction()))})),function(t){if(gg(t,eg))return eg;var e=gg(t,ig),i=gg(t,ng);return e&&i?eg:e||i?e?ig:ng:gg(t,tg)?tg:Jv}(t.join(" "))},e.preventDefaults=function(t){var e=t.srcEvent,i=t.offsetDirection;if(this.manager.session.prevented)e.preventDefault();else{var n=this.actions,o=gg(n,eg)&&!og.none,r=gg(n,ng)&&!og["pan-y"],s=gg(n,ig)&&!og["pan-x"];if(o){var a=1===t.pointers.length,h=t.distance<2,l=t.deltaTime<250;if(a&&h&&l)return}if(!s||!r)return o||r&&6&i||s&&i&cg?this.preventSrc(e):void 0}},e.preventSrc=function(t){this.manager.session.prevented=!0,t.preventDefault()},t}();function mg(t,e){for(;t;){if(t===e)return!0;t=t.parentNode}return!1}function bg(t){var e=t.length;if(1===e)return{x:Yv(t[0].clientX),y:Yv(t[0].clientY)};for(var i=0,n=0,o=0;o<e;)i+=t[o].clientX,n+=t[o].clientY,o++;return{x:Yv(i/e),y:Yv(n/e)}}function wg(t){for(var e=[],i=0;i<t.pointers.length;)e[i]={clientX:Yv(t.pointers[i].clientX),clientY:Yv(t.pointers[i].clientY)},i++;return{timeStamp:Gv(),pointers:e,center:bg(e),deltaX:t.deltaX,deltaY:t.deltaY}}function kg(t,e,i){i||(i=ug);var n=e[i[0]]-t[i[0]],o=e[i[1]]-t[i[1]];return Math.sqrt(n*n+o*o)}function _g(t,e,i){i||(i=ug);var n=e[i[0]]-t[i[0]],o=e[i[1]]-t[i[1]];return 180*Math.atan2(o,n)/Math.PI}function xg(t,e){return t===e?1:Xv(t)>=Xv(e)?t<0?2:4:e<0?8:dg}function Eg(t,e,i){return{x:e/t||0,y:i/t||0}}function Og(t,e){var i=t.session,n=e.pointers,o=n.length;i.firstInput||(i.firstInput=wg(e)),o>1&&!i.firstMultiple?i.firstMultiple=wg(e):1===o&&(i.firstMultiple=!1);var r=i.firstInput,s=i.firstMultiple,a=s?s.center:r.center,h=e.center=bg(n);e.timeStamp=Gv(),e.deltaTime=e.timeStamp-r.timeStamp,e.angle=_g(a,h),e.distance=kg(a,h),function(t,e){var i=e.center,n=t.offsetDelta||{},o=t.prevDelta||{},r=t.prevInput||{};1!==e.eventType&&4!==r.eventType||(o=t.prevDelta={x:r.deltaX||0,y:r.deltaY||0},n=t.offsetDelta={x:i.x,y:i.y}),e.deltaX=o.x+(i.x-n.x),e.deltaY=o.y+(i.y-n.y)}(i,e),e.offsetDirection=xg(e.deltaX,e.deltaY);var l,d,c=Eg(e.deltaTime,e.deltaX,e.deltaY);e.overallVelocityX=c.x,e.overallVelocityY=c.y,e.overallVelocity=Xv(c.x)>Xv(c.y)?c.x:c.y,e.scale=s?(l=s.pointers,kg((d=n)[0],d[1],fg)/kg(l[0],l[1],fg)):1,e.rotation=s?function(t,e){return _g(e[1],e[0],fg)+_g(t[1],t[0],fg)}(s.pointers,n):0,e.maxPointers=i.prevInput?e.pointers.length>i.prevInput.maxPointers?e.pointers.length:i.prevInput.maxPointers:e.pointers.length,function(t,e){var i,n,o,r,s=t.lastInterval||e,a=e.timeStamp-s.timeStamp;if(8!==e.eventType&&(a>25||void 0===s.velocity)){var h=e.deltaX-s.deltaX,l=e.deltaY-s.deltaY,d=Eg(a,h,l);n=d.x,o=d.y,i=Xv(d.x)>Xv(d.y)?d.x:d.y,r=xg(h,l),t.lastInterval=e}else i=s.velocity,n=s.velocityX,o=s.velocityY,r=s.direction;e.velocity=i,e.velocityX=n,e.velocityY=o,e.direction=r}(i,e);var u,f=t.element,p=e.srcEvent;mg(u=p.composedPath?p.composedPath()[0]:p.path?p.path[0]:p.target,f)&&(f=u),e.target=f}function Cg(t,e,i){var n=i.pointers.length,o=i.changedPointers.length,r=1&e&&n-o==0,s=12&e&&n-o==0;i.isFirst=!!r,i.isFinal=!!s,r&&(t.session={}),i.eventType=e,Og(t,i),t.emit("hammer.input",i),t.recognize(i),t.session.prevInput=i}function Sg(t){return t.trim().split(/\s+/g)}function Tg(t,e,i){pg(Sg(e),(function(e){t.addEventListener(e,i,!1)}))}function Mg(t,e,i){pg(Sg(e),(function(e){t.removeEventListener(e,i,!1)}))}function Pg(t){var e=t.ownerDocument||t;return e.defaultView||e.parentWindow||window}var Dg=function(){function t(t,e){var i=this;this.manager=t,this.callback=e,this.element=t.element,this.target=t.options.inputTarget,this.domHandler=function(e){vg(t.options.enable,[t])&&i.handler(e)},this.init()}var e=t.prototype;return e.handler=function(){},e.init=function(){this.evEl&&Tg(this.element,this.evEl,this.domHandler),this.evTarget&&Tg(this.target,this.evTarget,this.domHandler),this.evWin&&Tg(Pg(this.element),this.evWin,this.domHandler)},e.destroy=function(){this.evEl&&Mg(this.element,this.evEl,this.domHandler),this.evTarget&&Mg(this.target,this.evTarget,this.domHandler),this.evWin&&Mg(Pg(this.element),this.evWin,this.domHandler)},t}();function Ig(t,e,i){if(t.indexOf&&!i)return t.indexOf(e);for(var n=0;n<t.length;){if(i&&t[n][i]==e||!i&&t[n]===e)return n;n++}return-1}var Bg={pointerdown:1,pointermove:2,pointerup:4,pointercancel:8,pointerout:8},zg={2:hg,3:"pen",4:lg,5:"kinect"},Ng="pointerdown",Fg="pointermove pointerup pointercancel";Wv.MSPointerEvent&&!Wv.PointerEvent&&(Ng="MSPointerDown",Fg="MSPointerMove MSPointerUp MSPointerCancel");var Ag=function(t){function e(){var i,n=e.prototype;return n.evEl=Ng,n.evWin=Fg,(i=t.apply(this,arguments)||this).store=i.manager.session.pointerEvents=[],i}return Lv(e,t),e.prototype.handler=function(t){var e=this.store,i=!1,n=t.type.toLowerCase().replace("ms",""),o=Bg[n],r=zg[t.pointerType]||t.pointerType,s=r===hg,a=Ig(e,t.pointerId,"pointerId");1&o&&(0===t.button||s)?a<0&&(e.push(t),a=e.length-1):12&o&&(i=!0),a<0||(e[a]=t,this.callback(this.manager,o,{pointers:e,changedPointers:[t],pointerType:r,srcEvent:t}),i&&e.splice(a,1))},e}(Dg);function jg(t){return Array.prototype.slice.call(t,0)}function Rg(t,e,i){for(var n=[],o=[],r=0;r<t.length;){var s=e?t[r][e]:t[r];Ig(o,s)<0&&n.push(t[r]),o[r]=s,r++}return i&&(n=e?n.sort((function(t,i){return t[e]>i[e]})):n.sort()),n}var Lg={touchstart:1,touchmove:2,touchend:4,touchcancel:8},Hg="touchstart touchmove touchend touchcancel",Wg=function(t){function e(){var i;return e.prototype.evTarget=Hg,(i=t.apply(this,arguments)||this).targetIds={},i}return Lv(e,t),e.prototype.handler=function(t){var e=Lg[t.type],i=qg.call(this,t,e);i&&this.callback(this.manager,e,{pointers:i[0],changedPointers:i[1],pointerType:hg,srcEvent:t})},e}(Dg);function qg(t,e){var i,n,o=jg(t.touches),r=this.targetIds;if(3&e&&1===o.length)return r[o[0].identifier]=!0,[o,o];var s=jg(t.changedTouches),a=[],h=this.target;if(n=o.filter((function(t){return mg(t.target,h)})),1===e)for(i=0;i<n.length;)r[n[i].identifier]=!0,i++;for(i=0;i<s.length;)r[s[i].identifier]&&a.push(s[i]),12&e&&delete r[s[i].identifier],i++;return a.length?[Rg(n.concat(a),"identifier",!0),a]:void 0}var Vg={mousedown:1,mousemove:2,mouseup:4},Ug="mousedown",Yg="mousemove mouseup",Xg=function(t){function e(){var i,n=e.prototype;return n.evEl=Ug,n.evWin=Yg,(i=t.apply(this,arguments)||this).pressed=!1,i}return Lv(e,t),e.prototype.handler=function(t){var e=Vg[t.type];1&e&&0===t.button&&(this.pressed=!0),2&e&&1!==t.which&&(e=4),this.pressed&&(4&e&&(this.pressed=!1),this.callback(this.manager,e,{pointers:[t],changedPointers:[t],pointerType:lg,srcEvent:t}))},e}(Dg);function Gg(t){var e=t.changedPointers[0];if(e.identifier===this.primaryTouch){var i={x:e.clientX,y:e.clientY},n=this.lastTouches;this.lastTouches.push(i);setTimeout((function(){var t=n.indexOf(i);t>-1&&n.splice(t,1)}),2500)}}function Kg(t,e){1&t?(this.primaryTouch=e.changedPointers[0].identifier,Gg.call(this,e)):12&t&&Gg.call(this,e)}function $g(t){for(var e=t.srcEvent.clientX,i=t.srcEvent.clientY,n=0;n<this.lastTouches.length;n++){var o=this.lastTouches[n],r=Math.abs(e-o.x),s=Math.abs(i-o.y);if(r<=25&&s<=25)return!0}return!1}var Zg=function(){return function(t){function e(e,i){var n;return(n=t.call(this,e,i)||this).handler=function(t,e,i){var o=i.pointerType===hg,r=i.pointerType===lg;if(!(r&&i.sourceCapabilities&&i.sourceCapabilities.firesTouchEvents)){if(o)Kg.call(Hv(Hv(n)),e,i);else if(r&&$g.call(Hv(Hv(n)),i))return;n.callback(t,e,i)}},n.touch=new Wg(n.manager,n.handler),n.mouse=new Xg(n.manager,n.handler),n.primaryTouch=null,n.lastTouches=[],n}return Lv(e,t),e.prototype.destroy=function(){this.touch.destroy(),this.mouse.destroy()},e}(Dg)}();function Qg(t,e,i){return!!Array.isArray(t)&&(pg(t,i[e],i),!0)}var Jg=32,ty=1;function ey(t,e){var i=e.manager;return i?i.get(t):t}function iy(t){return 16&t?"cancel":8&t?"end":4&t?"move":2&t?"start":""}var ny=function(){function t(t){void 0===t&&(t={}),this.options=Rv({enable:!0},t),this.id=ty++,this.manager=null,this.state=1,this.simultaneous={},this.requireFail=[]}var e=t.prototype;return e.set=function(t){return qv(this.options,t),this.manager&&this.manager.touchAction.update(),this},e.recognizeWith=function(t){if(Qg(t,"recognizeWith",this))return this;var e=this.simultaneous;return e[(t=ey(t,this)).id]||(e[t.id]=t,t.recognizeWith(this)),this},e.dropRecognizeWith=function(t){return Qg(t,"dropRecognizeWith",this)||(t=ey(t,this),delete this.simultaneous[t.id]),this},e.requireFailure=function(t){if(Qg(t,"requireFailure",this))return this;var e=this.requireFail;return-1===Ig(e,t=ey(t,this))&&(e.push(t),t.requireFailure(this)),this},e.dropRequireFailure=function(t){if(Qg(t,"dropRequireFailure",this))return this;t=ey(t,this);var e=Ig(this.requireFail,t);return e>-1&&this.requireFail.splice(e,1),this},e.hasRequireFailures=function(){return this.requireFail.length>0},e.canRecognizeWith=function(t){return!!this.simultaneous[t.id]},e.emit=function(t){var e=this,i=this.state;function n(i){e.manager.emit(i,t)}i<8&&n(e.options.event+iy(i)),n(e.options.event),t.additionalEvent&&n(t.additionalEvent),i>=8&&n(e.options.event+iy(i))},e.tryEmit=function(t){if(this.canEmit())return this.emit(t);this.state=Jg},e.canEmit=function(){for(var t=0;t<this.requireFail.length;){if(!(33&this.requireFail[t].state))return!1;t++}return!0},e.recognize=function(t){var e=qv({},t);if(!vg(this.options.enable,[this,e]))return this.reset(),void(this.state=Jg);56&this.state&&(this.state=1),this.state=this.process(e),30&this.state&&this.tryEmit(e)},e.process=function(t){},e.getTouchAction=function(){},e.reset=function(){},t}(),oy=function(t){function e(e){var i;return void 0===e&&(e={}),(i=t.call(this,Rv({event:"tap",pointers:1,taps:1,interval:300,time:250,threshold:9,posThreshold:10},e))||this).pTime=!1,i.pCenter=!1,i._timer=null,i._input=null,i.count=0,i}Lv(e,t);var i=e.prototype;return i.getTouchAction=function(){return[tg]},i.process=function(t){var e=this,i=this.options,n=t.pointers.length===i.pointers,o=t.distance<i.threshold,r=t.deltaTime<i.time;if(this.reset(),1&t.eventType&&0===this.count)return this.failTimeout();if(o&&r&&n){if(4!==t.eventType)return this.failTimeout();var s=!this.pTime||t.timeStamp-this.pTime<i.interval,a=!this.pCenter||kg(this.pCenter,t.center)<i.posThreshold;if(this.pTime=t.timeStamp,this.pCenter=t.center,a&&s?this.count+=1:this.count=1,this._input=t,0===this.count%i.taps)return this.hasRequireFailures()?(this._timer=setTimeout((function(){e.state=8,e.tryEmit()}),i.interval),2):8}return Jg},i.failTimeout=function(){var t=this;return this._timer=setTimeout((function(){t.state=Jg}),this.options.interval),Jg},i.reset=function(){clearTimeout(this._timer)},i.emit=function(){8===this.state&&(this._input.tapCount=this.count,this.manager.emit(this.options.event,this._input))},e}(ny),ry=function(t){function e(e){return void 0===e&&(e={}),t.call(this,Rv({pointers:1},e))||this}Lv(e,t);var i=e.prototype;return i.attrTest=function(t){var e=this.options.pointers;return 0===e||t.pointers.length===e},i.process=function(t){var e=this.state,i=t.eventType,n=6&e,o=this.attrTest(t);return n&&(8&i||!o)?16|e:n||o?4&i?8|e:2&e?4|e:2:Jg},e}(ny);function sy(t){return t===dg?"down":8===t?"up":2===t?"left":4===t?"right":""}var ay=function(t){function e(e){var i;return void 0===e&&(e={}),(i=t.call(this,Rv({event:"pan",threshold:10,pointers:1,direction:30},e))||this).pX=null,i.pY=null,i}Lv(e,t);var i=e.prototype;return i.getTouchAction=function(){var t=this.options.direction,e=[];return 6&t&&e.push(ng),t&cg&&e.push(ig),e},i.directionTest=function(t){var e=this.options,i=!0,n=t.distance,o=t.direction,r=t.deltaX,s=t.deltaY;return o&e.direction||(6&e.direction?(o=0===r?1:r<0?2:4,i=r!==this.pX,n=Math.abs(t.deltaX)):(o=0===s?1:s<0?8:dg,i=s!==this.pY,n=Math.abs(t.deltaY))),t.direction=o,i&&n>e.threshold&&o&e.direction},i.attrTest=function(t){return ry.prototype.attrTest.call(this,t)&&(2&this.state||!(2&this.state)&&this.directionTest(t))},i.emit=function(e){this.pX=e.deltaX,this.pY=e.deltaY;var i=sy(e.direction);i&&(e.additionalEvent=this.options.event+i),t.prototype.emit.call(this,e)},e}(ry),hy=function(t){function e(e){return void 0===e&&(e={}),t.call(this,Rv({event:"swipe",threshold:10,velocity:.3,direction:30,pointers:1},e))||this}Lv(e,t);var i=e.prototype;return i.getTouchAction=function(){return ay.prototype.getTouchAction.call(this)},i.attrTest=function(e){var i,n=this.options.direction;return 30&n?i=e.overallVelocity:6&n?i=e.overallVelocityX:n&cg&&(i=e.overallVelocityY),t.prototype.attrTest.call(this,e)&&n&e.offsetDirection&&e.distance>this.options.threshold&&e.maxPointers===this.options.pointers&&Xv(i)>this.options.velocity&&4&e.eventType},i.emit=function(t){var e=sy(t.offsetDirection);e&&this.manager.emit(this.options.event+e,t),this.manager.emit(this.options.event,t)},e}(ry),ly=function(t){function e(e){return void 0===e&&(e={}),t.call(this,Rv({event:"pinch",threshold:0,pointers:2},e))||this}Lv(e,t);var i=e.prototype;return i.getTouchAction=function(){return[eg]},i.attrTest=function(e){return t.prototype.attrTest.call(this,e)&&(Math.abs(e.scale-1)>this.options.threshold||2&this.state)},i.emit=function(e){if(1!==e.scale){var i=e.scale<1?"in":"out";e.additionalEvent=this.options.event+i}t.prototype.emit.call(this,e)},e}(ry),dy=function(t){function e(e){return void 0===e&&(e={}),t.call(this,Rv({event:"rotate",threshold:0,pointers:2},e))||this}Lv(e,t);var i=e.prototype;return i.getTouchAction=function(){return[eg]},i.attrTest=function(e){return t.prototype.attrTest.call(this,e)&&(Math.abs(e.rotation)>this.options.threshold||2&this.state)},e}(ry),cy=function(t){function e(e){var i;return void 0===e&&(e={}),(i=t.call(this,Rv({event:"press",pointers:1,time:251,threshold:9},e))||this)._timer=null,i._input=null,i}Lv(e,t);var i=e.prototype;return i.getTouchAction=function(){return[Jv]},i.process=function(t){var e=this,i=this.options,n=t.pointers.length===i.pointers,o=t.distance<i.threshold,r=t.deltaTime>i.time;if(this._input=t,!o||!n||12&t.eventType&&!r)this.reset();else if(1&t.eventType)this.reset(),this._timer=setTimeout((function(){e.state=8,e.tryEmit()}),i.time);else if(4&t.eventType)return 8;return Jg},i.reset=function(){clearTimeout(this._timer)},i.emit=function(t){8===this.state&&(t&&4&t.eventType?this.manager.emit(this.options.event+"up",t):(this._input.timeStamp=Gv(),this.manager.emit(this.options.event,this._input)))},e}(ny),uy={domEvents:!1,touchAction:Qv,enable:!0,inputTarget:null,inputClass:null,cssProps:{userSelect:"none",touchSelect:"none",touchCallout:"none",contentZooming:"none",userDrag:"none",tapHighlightColor:"rgba(0,0,0,0)"}},fy=[[dy,{enable:!1}],[ly,{enable:!1},["rotate"]],[hy,{direction:6}],[ay,{direction:6},["swipe"]],[oy],[oy,{event:"doubletap",taps:2},["tap"]],[cy]];function py(t,e){var i,n=t.element;n.style&&(pg(t.options.cssProps,(function(o,r){i=Kv(n.style,r),e?(t.oldCssProps[i]=n.style[i],n.style[i]=o):n.style[i]=t.oldCssProps[i]||""})),e||(t.oldCssProps={}))}var vy=function(){function t(t,e){var i,n=this;this.options=qv({},uy,e||{}),this.options.inputTarget=this.options.inputTarget||t,this.handlers={},this.session={},this.recognizers=[],this.oldCssProps={},this.element=t,this.input=new((i=this).options.inputClass||(sg?Ag:ag?Wg:rg?Zg:Xg))(i,Cg),this.touchAction=new yg(this,this.options.touchAction),py(this,!0),pg(this.options.recognizers,(function(t){var e=n.add(new t[0](t[1]));t[2]&&e.recognizeWith(t[2]),t[3]&&e.requireFailure(t[3])}),this)}var e=t.prototype;return e.set=function(t){return qv(this.options,t),t.touchAction&&this.touchAction.update(),t.inputTarget&&(this.input.destroy(),this.input.target=t.inputTarget,this.input.init()),this},e.stop=function(t){this.session.stopped=t?2:1},e.recognize=function(t){var e=this.session;if(!e.stopped){var i;this.touchAction.preventDefaults(t);var n=this.recognizers,o=e.curRecognizer;(!o||o&&8&o.state)&&(e.curRecognizer=null,o=null);for(var r=0;r<n.length;)i=n[r],2===e.stopped||o&&i!==o&&!i.canRecognizeWith(o)?i.reset():i.recognize(t),!o&&14&i.state&&(e.curRecognizer=i,o=i),r++}},e.get=function(t){if(t instanceof ny)return t;for(var e=this.recognizers,i=0;i<e.length;i++)if(e[i].options.event===t)return e[i];return null},e.add=function(t){if(Qg(t,"add",this))return this;var e=this.get(t.options.event);return e&&this.remove(e),this.recognizers.push(t),t.manager=this,this.touchAction.update(),t},e.remove=function(t){if(Qg(t,"remove",this))return this;var e=this.get(t);if(t){var i=this.recognizers,n=Ig(i,e);-1!==n&&(i.splice(n,1),this.touchAction.update())}return this},e.on=function(t,e){if(void 0===t||void 0===e)return this;var i=this.handlers;return pg(Sg(t),(function(t){i[t]=i[t]||[],i[t].push(e)})),this},e.off=function(t,e){if(void 0===t)return this;var i=this.handlers;return pg(Sg(t),(function(t){e?i[t]&&i[t].splice(Ig(i[t],e),1):delete i[t]})),this},e.emit=function(t,e){this.options.domEvents&&function(t,e){var i=document.createEvent("Event");i.initEvent(t,!0,!0),i.gesture=e,e.target.dispatchEvent(i)}(t,e);var i=this.handlers[t]&&this.handlers[t].slice();if(i&&i.length){e.type=t,e.preventDefault=function(){e.srcEvent.preventDefault()};for(var n=0;n<i.length;)i[n](e),n++}},e.destroy=function(){this.element&&py(this,!1),this.handlers={},this.session={},this.input.destroy(),this.element=null},t}(),gy={touchstart:1,touchmove:2,touchend:4,touchcancel:8},yy="touchstart",my="touchstart touchmove touchend touchcancel",by=function(t){function e(){var i,n=e.prototype;return n.evTarget=yy,n.evWin=my,(i=t.apply(this,arguments)||this).started=!1,i}return Lv(e,t),e.prototype.handler=function(t){var e=gy[t.type];if(1===e&&(this.started=!0),this.started){var i=wy.call(this,t,e);12&e&&i[0].length-i[1].length==0&&(this.started=!1),this.callback(this.manager,e,{pointers:i[0],changedPointers:i[1],pointerType:hg,srcEvent:t})}},e}(Dg);function wy(t,e){var i=jg(t.touches),n=jg(t.changedTouches);return 12&e&&(i=Rg(i.concat(n),"identifier",!0)),[i,n]}function ky(t,e,i){var n="DEPRECATED METHOD: "+e+"\n"+i+" AT \n";return function(){var e=new Error("get-stack-trace"),i=e&&e.stack?e.stack.replace(/^[^\(]+?[\n$]/gm,"").replace(/^\s+at\s+/gm,"").replace(/^Object.<anonymous>\s*\(/gm,"{anonymous}()@"):"Unknown Stack Trace",o=window.console&&(window.console.warn||window.console.log);return o&&o.call(window.console,n,i),t.apply(this,arguments)}}var _y=ky((function(t,e,i){for(var n=Object.keys(e),o=0;o<n.length;)(!i||i&&void 0===t[n[o]])&&(t[n[o]]=e[n[o]]),o++;return t}),"extend","Use `assign`."),xy=ky((function(t,e){return _y(t,e,!0)}),"merge","Use `assign`.");function Ey(t,e,i){var n,o=e.prototype;(n=t.prototype=Object.create(o)).constructor=t,n._super=o,i&&qv(n,i)}function Oy(t,e){return function(){return t.apply(e,arguments)}}var Cy=function(){var t=function(t,e){return void 0===e&&(e={}),new vy(t,Rv({recognizers:fy.concat()},e))};return t.VERSION="2.0.17-rc",t.DIRECTION_ALL=30,t.DIRECTION_DOWN=dg,t.DIRECTION_LEFT=2,t.DIRECTION_RIGHT=4,t.DIRECTION_UP=8,t.DIRECTION_HORIZONTAL=6,t.DIRECTION_VERTICAL=cg,t.DIRECTION_NONE=1,t.DIRECTION_DOWN=dg,t.INPUT_START=1,t.INPUT_MOVE=2,t.INPUT_END=4,t.INPUT_CANCEL=8,t.STATE_POSSIBLE=1,t.STATE_BEGAN=2,t.STATE_CHANGED=4,t.STATE_ENDED=8,t.STATE_RECOGNIZED=8,t.STATE_CANCELLED=16,t.STATE_FAILED=Jg,t.Manager=vy,t.Input=Dg,t.TouchAction=yg,t.TouchInput=Wg,t.MouseInput=Xg,t.PointerEventInput=Ag,t.TouchMouseInput=Zg,t.SingleTouchInput=by,t.Recognizer=ny,t.AttrRecognizer=ry,t.Tap=oy,t.Pan=ay,t.Swipe=hy,t.Pinch=ly,t.Rotate=dy,t.Press=cy,t.on=Tg,t.off=Mg,t.each=pg,t.merge=xy,t.extend=_y,t.bindFn=Oy,t.assign=qv,t.inherit=Ey,t.bindFn=Oy,t.prefixed=Kv,t.toArray=jg,t.inArray=Ig,t.uniqueArray=Rg,t.splitStr=Sg,t.boolOrFn=vg,t.hasParent=mg,t.addEventListeners=Tg,t.removeEventListeners=Mg,t.defaults=qv({},uy,{preset:fy}),t}(),Sy=Cy;function Ty(t,e){var i=bu(t);if(hd){var n=hd(t);e&&(n=Xf(n).call(n,(function(e){return bd(t,e).enumerable}))),i.push.apply(i,n)}return i}function My(t){for(var e=1;e<arguments.length;e++){var i,n,o=null!=arguments[e]?arguments[e]:{};e%2?Fu(i=Ty(Object(o),!0)).call(i,(function(e){$d(t,e,o[e])})):Pd?Ad(t,Pd(o)):Fu(n=Ty(Object(o))).call(n,(function(e){Ud(t,e,bd(o,e))}))}return t}function Py(t,e){var i=void 0!==tu&&ih(t)||t["@@iterator"];if(!i){if(lu(t)||(i=function(t,e){var i;if(!t)return;if("string"==typeof t)return Dy(t,e);var n=au(i=Object.prototype.toString.call(t)).call(i,8,-1);"Object"===n&&t.constructor&&(n=t.constructor.name);if("Map"===n||"Set"===n)return ja(t);if("Arguments"===n||/^(?:Ui|I)nt(?:8|16|32)(?:Clamped)?Array$/.test(n))return Dy(t,e)}(t))||e&&t&&"number"==typeof t.length){i&&(t=i);var n=0,o=function(){};return{s:o,n:function(){return n>=t.length?{done:!0}:{done:!1,value:t[n++]}},e:function(t){throw t},f:o}}throw new TypeError("Invalid attempt to iterate non-iterable instance.\nIn order to be iterable, non-array objects must have a [Symbol.iterator]() method.")}var r,s=!0,a=!1;return{s:function(){i=i.call(t)},n:function(){var t=i.next();return s=t.done,t},e:function(t){a=!0,r=t},f:function(){try{s||null==i.return||i.return()}finally{if(a)throw r}}}}function Dy(t,e){(null==e||e>t.length)&&(e=t.length);for(var i=0,n=new Array(e);i<e;i++)n[i]=t[i];return n}var Iy=tu("DELETE");function By(t){for(var e,i=arguments.length,n=new Array(i>1?i-1:0),o=1;o<i;o++)n[o-1]=arguments[o];return zy.apply(void 0,su(e=[{},t]).call(e,n))}function zy(){var t=Ny.apply(void 0,arguments);return Ay(t),t}function Ny(){for(var t=arguments.length,e=new Array(t),i=0;i<t;i++)e[i]=arguments[i];if(e.length<2)return e[0];var n;if(e.length>2)return Ny.apply(void 0,su(n=[zy(e[0],e[1])]).call(n,Jc(au(e).call(e,2))));var o,r=e[0],s=e[1],a=Py(hu(s));try{for(a.s();!(o=a.n()).done;){var h=o.value;Object.prototype.propertyIsEnumerable.call(s,h)&&(s[h]===Iy?delete r[h]:null===r[h]||null===s[h]||"object"!==Qc(r[h])||"object"!==Qc(s[h])||lu(r[h])||lu(s[h])?r[h]=Fy(s[h]):r[h]=Ny(r[h],s[h]))}}catch(t){a.e(t)}finally{a.f()}return r}function Fy(t){return lu(t)?gu(t).call(t,(function(t){return Fy(t)})):"object"===Qc(t)&&null!==t?Ny({},t):t}function Ay(t){for(var e=0,i=bu(t);e<i.length;e++){var n=i[e];t[n]===Iy?delete t[n]:"object"===Qc(t[n])&&null!==t[n]&&Ay(t[n])}}function jy(){for(var t=arguments.length,e=new Array(t),i=0;i<t;i++)e[i]=arguments[i];return Ry(e.length?e:[Eu()])}function Ry(t){var e=function(){for(var t=Ly(),e=t(" "),i=t(" "),n=t(" "),o=0;o<arguments.length;o++)(e-=t(o<0||arguments.length<=o?void 0:arguments[o]))<0&&(e+=1),(i-=t(o<0||arguments.length<=o?void 0:arguments[o]))<0&&(i+=1),(n-=t(o<0||arguments.length<=o?void 0:arguments[o]))<0&&(n+=1);return[e,i,n]}(t),i=Kc(e,3),n=i[0],o=i[1],r=i[2],s=1,a=function(){var t=2091639*n+2.3283064365386963e-10*s;return n=o,o=r,r=t-(s=0|t)};return a.uint32=function(){return 4294967296*a()},a.fract53=function(){return a()+11102230246251565e-32*(2097152*a()|0)},a.algorithm="Alea",a.seed=t,a.version="0.9",a}function Ly(){var t=4022871197;return function(e){for(var i=e.toString(),n=0;n<i.length;n++){var o=.02519603282416938*(t+=i.charCodeAt(n));o-=t=o>>>0,t=(o*=t)>>>0,t+=4294967296*(o-=t)}return 2.3283064365386963e-10*(t>>>0)}}var Hy="undefined"!=typeof window?window.Hammer||Sy:function(){return function(){var t=function(){};return{on:t,off:t,destroy:t,emit:t,get:function(){return{set:t}}}}()};function Wy(t){var e,i=this;this._cleanupQueue=[],this.active=!1,this._dom={container:t,overlay:document.createElement("div")},this._dom.overlay.classList.add("vis-overlay"),this._dom.container.appendChild(this._dom.overlay),this._cleanupQueue.push((function(){i._dom.overlay.parentNode.removeChild(i._dom.overlay)}));var n=Hy(this._dom.overlay);n.on("tap",zn(e=this._onTapOverlay).call(e,this)),this._cleanupQueue.push((function(){n.destroy()}));var o=["tap","doubletap","press","pinch","pan","panstart","panmove","panend"];Fu(o).call(o,(function(t){n.on(t,(function(t){t.srcEvent.stopPropagation()}))})),document&&document.body&&(this._onClick=function(e){(function(t,e){for(;t;){if(t===e)return!0;t=t.parentNode}return!1})(e.target,t)||i.deactivate()},document.body.addEventListener("click",this._onClick),this._cleanupQueue.push((function(){document.body.removeEventListener("click",i._onClick)}))),this._escListener=function(t){("key"in t?"Escape"===t.key:27===t.keyCode)&&i.deactivate()}}Wn(Wy.prototype),Wy.current=null,Wy.prototype.destroy=function(){var t,e;this.deactivate();var i,n=Py(Yu(t=ff(e=this._cleanupQueue).call(e,0)).call(t));try{for(n.s();!(i=n.n()).done;){(0,i.value)()}}catch(t){n.e(t)}finally{n.f()}},Wy.prototype.activate=function(){Wy.current&&Wy.current.deactivate(),Wy.current=this,this.active=!0,this._dom.overlay.style.display="none",this._dom.container.classList.add("vis-active"),this.emit("change"),this.emit("activate"),document.body.addEventListener("keydown",this._escListener)},Wy.prototype.deactivate=function(){this.active=!1,this._dom.overlay.style.display="block",this._dom.container.classList.remove("vis-active"),document.body.removeEventListener("keydown",this._escListener),this.emit("change"),this.emit("deactivate")},Wy.prototype._onTapOverlay=function(t){this.activate(),t.srcEvent.stopPropagation()};var qy=/^\/?Date\((-?\d+)/i,Vy=/^#?([a-f\d]{2})([a-f\d]{2})([a-f\d]{2})$/i,Uy=/^#?([a-f\d])([a-f\d])([a-f\d])$/i,Yy=/^rgb\( *(1?\d{1,2}|2[0-4]\d|25[0-5]) *, *(1?\d{1,2}|2[0-4]\d|25[0-5]) *, *(1?\d{1,2}|2[0-4]\d|25[0-5]) *\)$/i,Xy=/^rgba\( *(1?\d{1,2}|2[0-4]\d|25[0-5]) *, *(1?\d{1,2}|2[0-4]\d|25[0-5]) *, *(1?\d{1,2}|2[0-4]\d|25[0-5]) *, *([01]|0?\.\d+) *\)$/i;function Gy(t){return t instanceof Number||"number"==typeof t}function Ky(t){if(t)for(;!0===t.hasChildNodes();){var e=t.firstChild;e&&(Ky(e),t.removeChild(e))}}function $y(t){return t instanceof String||"string"==typeof t}function Zy(t){return"object"===Qc(t)&&null!==t}function Qy(t,e,i,n){var o=!1;!0===n&&(o=null===e[i]&&void 0!==t[i]),o?delete t[i]:t[i]=e[i]}function Jy(t,e){var i=arguments.length>2&&void 0!==arguments[2]&&arguments[2];for(var n in t)if(void 0!==e[n])if(null===e[n]||"object"!==Qc(e[n]))Qy(t,e,n,i);else{var o=t[n],r=e[n];Zy(o)&&Zy(r)&&Jy(o,r,i)}}var tm=un;function em(t,e,i){var n=arguments.length>3&&void 0!==arguments[3]&&arguments[3];if(lu(i))throw new TypeError("Arrays are not supported by deepExtend");for(var o=0;o<t.length;o++){var r=t[o];if(Object.prototype.hasOwnProperty.call(i,r))if(i[r]&&i[r].constructor===Object)void 0===e[r]&&(e[r]={}),e[r].constructor===Object?nm(e[r],i[r],!1,n):Qy(e,i,r,n);else{if(lu(i[r]))throw new TypeError("Arrays are not supported by deepExtend");Qy(e,i,r,n)}}return e}function im(t,e,i){var n=arguments.length>3&&void 0!==arguments[3]&&arguments[3];if(lu(i))throw new TypeError("Arrays are not supported by deepExtend");for(var o in i)if(Object.prototype.hasOwnProperty.call(i,o)&&!Nf(t).call(t,o))if(i[o]&&i[o].constructor===Object)void 0===e[o]&&(e[o]={}),e[o].constructor===Object?nm(e[o],i[o]):Qy(e,i,o,n);else if(lu(i[o])){e[o]=[];for(var r=0;r<i[o].length;r++)e[o].push(i[o][r])}else Qy(e,i,o,n);return e}function nm(t,e){var i=arguments.length>2&&void 0!==arguments[2]&&arguments[2],n=arguments.length>3&&void 0!==arguments[3]&&arguments[3];for(var o in e)if(Object.prototype.hasOwnProperty.call(e,o)||!0===i)if("object"===Qc(e[o])&&null!==e[o]&&Lf(e[o])===Object.prototype)void 0===t[o]?t[o]=nm({},e[o],i):"object"===Qc(t[o])&&null!==t[o]&&Lf(t[o])===Object.prototype?nm(t[o],e[o],i):Qy(t,e,o,n);else if(lu(e[o])){var r;t[o]=au(r=e[o]).call(r)}else Qy(t,e,o,n);return t}function om(t,e){var i;return su(i=[]).call(i,Jc(t),[e])}function rm(t){return au(t).call(t)}function sm(t){return t.getBoundingClientRect().left}function am(t){return t.getBoundingClientRect().top}function hm(t,e){if(lu(t))for(var i=t.length,n=0;n<i;n++)e(t[n],n,t);else for(var o in t)Object.prototype.hasOwnProperty.call(t,o)&&e(t[o],o,t)}var lm=ip;function dm(t,e,i,n){var o;t.addEventListener?(void 0===n&&(n=!1),"mousewheel"===e&&Nf(o=navigator.userAgent).call(o,"Firefox")&&(e="DOMMouseScroll"),t.addEventListener(e,i,n)):t.attachEvent("on"+e,i)}function cm(t,e,i,n){var o;t.removeEventListener?(void 0===n&&(n=!1),"mousewheel"===e&&Nf(o=navigator.userAgent).call(o,"Firefox")&&(e="DOMMouseScroll"),t.removeEventListener(e,i,n)):t.detachEvent("on"+e,i)}var um={asBoolean:function(t,e){return"function"==typeof t&&(t=t()),null!=t?0!=t:e||null},asNumber:function(t,e){return"function"==typeof t&&(t=t()),null!=t?Number(t)||e||null:e||null},asString:function(t,e){return"function"==typeof t&&(t=t()),null!=t?String(t):e||null},asSize:function(t,e){return"function"==typeof t&&(t=t()),$y(t)?t:Gy(t)?t+"px":e||null},asElement:function(t,e){return"function"==typeof t&&(t=t()),t||e||null}};function fm(t){var e;switch(t.length){case 3:case 4:return(e=Uy.exec(t))?{r:Ep(e[1]+e[1],16),g:Ep(e[2]+e[2],16),b:Ep(e[3]+e[3],16)}:null;case 6:case 7:return(e=Vy.exec(t))?{r:Ep(e[1],16),g:Ep(e[2],16),b:Ep(e[3],16)}:null;default:return null}}function pm(t,e){if(Nf(t).call(t,"rgba"))return t;if(Nf(t).call(t,"rgb")){var i=t.substr(Fp(t).call(t,"(")+1).replace(")","").split(",");return"rgba("+i[0]+","+i[1]+","+i[2]+","+e+")"}var n=fm(t);return null==n?t:"rgba("+n.r+","+n.g+","+n.b+","+e+")"}function vm(t,e,i){var n;return"#"+au(n=((1<<24)+(t<<16)+(e<<8)+i).toString(16)).call(n,1)}function gm(t,e){if($y(t)){var i=t;if(Em(i)){var n,o=gu(n=i.substr(4).substr(0,i.length-5).split(",")).call(n,(function(t){return Ep(t)}));i=vm(o[0],o[1],o[2])}if(!0===xm(i)){var r=_m(i),s={h:r.h,s:.8*r.s,v:Math.min(1,1.02*r.v)},a={h:r.h,s:Math.min(1,1.25*r.s),v:.8*r.v},h=km(a.h,a.s,a.v),l=km(s.h,s.s,s.v);return{background:i,border:h,highlight:{background:l,border:h},hover:{background:l,border:h}}}return{background:i,border:i,highlight:{background:i,border:i},hover:{background:i,border:i}}}return e?{background:t.background||e.background,border:t.border||e.border,highlight:$y(t.highlight)?{border:t.highlight,background:t.highlight}:{background:t.highlight&&t.highlight.background||e.highlight.background,border:t.highlight&&t.highlight.border||e.highlight.border},hover:$y(t.hover)?{border:t.hover,background:t.hover}:{border:t.hover&&t.hover.border||e.hover.border,background:t.hover&&t.hover.background||e.hover.background}}:{background:t.background||void 0,border:t.border||void 0,highlight:$y(t.highlight)?{border:t.highlight,background:t.highlight}:{background:t.highlight&&t.highlight.background||void 0,border:t.highlight&&t.highlight.border||void 0},hover:$y(t.hover)?{border:t.hover,background:t.hover}:{border:t.hover&&t.hover.border||void 0,background:t.hover&&t.hover.background||void 0}}}function ym(t,e,i){t/=255,e/=255,i/=255;var n=Math.min(t,Math.min(e,i)),o=Math.max(t,Math.max(e,i));return n===o?{h:0,s:0,v:n}:{h:60*((t===n?3:i===n?1:5)-(t===n?e-i:i===n?t-e:i-t)/(o-n))/360,s:(o-n)/o,v:o}}var mm=function(t){var e,i={};return Fu(e=t.split(";")).call(e,(function(t){if(""!=Yp(t).call(t)){var e,n,o=t.split(":"),r=Yp(e=o[0]).call(e),s=Yp(n=o[1]).call(n);i[r]=s}})),i},bm=function(t){var e;return gu(e=bu(t)).call(e,(function(e){return e+": "+t[e]})).join("; ")};function wm(t,e,i){var n,o,r,s=Math.floor(6*t),a=6*t-s,h=i*(1-e),l=i*(1-a*e),d=i*(1-(1-a)*e);switch(s%6){case 0:n=i,o=d,r=h;break;case 1:n=l,o=i,r=h;break;case 2:n=h,o=i,r=d;break;case 3:n=h,o=l,r=i;break;case 4:n=d,o=h,r=i;break;case 5:n=i,o=h,r=l}return{r:Math.floor(255*n),g:Math.floor(255*o),b:Math.floor(255*r)}}function km(t,e,i){var n=wm(t,e,i);return vm(n.r,n.g,n.b)}function _m(t){var e=fm(t);if(!e)throw new TypeError("'".concat(t,"' is not a valid color."));return ym(e.r,e.g,e.b)}function xm(t){return/(^#[0-9A-F]{6}$)|(^#[0-9A-F]{3}$)/i.test(t)}function Em(t){return Yy.test(t)}function Om(t){return Xy.test(t)}function Cm(t){if(null===t||"object"!==Qc(t))return null;if(t instanceof Element)return t;var e=Kp(t);for(var i in t)Object.prototype.hasOwnProperty.call(t,i)&&"object"==Qc(t[i])&&(e[i]=Cm(t[i]));return e}function Sm(t,e,i){var n=arguments.length>3&&void 0!==arguments[3]?arguments[3]:{},o=function(t){return null!=t},r=function(t){return null!==t&&"object"===Qc(t)},s=function(t){for(var e in t)if(Object.prototype.hasOwnProperty.call(t,e))return!1;return!0};if(!r(t))throw new Error("Parameter mergeTarget must be an object");if(!r(e))throw new Error("Parameter options must be an object");if(!o(i))throw new Error("Parameter option must have a value");if(!r(n))throw new Error("Parameter globalOptions must be an object");var a=function(t,e,i){r(t[i])||(t[i]={});var n=e[i],o=t[i];for(var s in n)Object.prototype.hasOwnProperty.call(n,s)&&(o[s]=n[s])},h=e[i],l=r(n)&&!s(n),d=l?n[i]:void 0,c=d?d.enabled:void 0;if(void 0!==h){if("boolean"==typeof h)return r(t[i])||(t[i]={}),void(t[i].enabled=h);if(null===h&&!r(t[i])){if(!o(d))return;t[i]=Kp(d)}if(r(h)){var u=!0;void 0!==h.enabled?u=h.enabled:void 0!==c&&(u=d.enabled),a(t,e,i),t[i].enabled=u}}}var Tm={linear:function(t){return t},easeInQuad:function(t){return t*t},easeOutQuad:function(t){return t*(2-t)},easeInOutQuad:function(t){return t<.5?2*t*t:(4-2*t)*t-1},easeInCubic:function(t){return t*t*t},easeOutCubic:function(t){return--t*t*t+1},easeInOutCubic:function(t){return t<.5?4*t*t*t:(t-1)*(2*t-2)*(2*t-2)+1},easeInQuart:function(t){return t*t*t*t},easeOutQuart:function(t){return 1- --t*t*t*t},easeInOutQuart:function(t){return t<.5?8*t*t*t*t:1-8*--t*t*t*t},easeInQuint:function(t){return t*t*t*t*t},easeOutQuint:function(t){return 1+--t*t*t*t*t},easeInOutQuint:function(t){return t<.5?16*t*t*t*t*t:1+16*--t*t*t*t*t}};function Mm(t,e){var i;lu(e)||(e=[e]);var n,o=Py(t);try{for(o.s();!(n=o.n()).done;){var r=n.value;if(r){i=r[e[0]];for(var s=1;s<e.length;s++)i&&(i=i[e[s]]);if(void 0!==i)break}}}catch(t){o.e(t)}finally{o.f()}return i}var Pm={black:"#000000",navy:"#000080",darkblue:"#00008B",mediumblue:"#0000CD",blue:"#0000FF",darkgreen:"#006400",green:"#008000",teal:"#008080",darkcyan:"#008B8B",deepskyblue:"#00BFFF",darkturquoise:"#00CED1",mediumspringgreen:"#00FA9A",lime:"#00FF00",springgreen:"#00FF7F",aqua:"#00FFFF",cyan:"#00FFFF",midnightblue:"#191970",dodgerblue:"#1E90FF",lightseagreen:"#20B2AA",forestgreen:"#228B22",seagreen:"#2E8B57",darkslategray:"#2F4F4F",limegreen:"#32CD32",mediumseagreen:"#3CB371",turquoise:"#40E0D0",royalblue:"#4169E1",steelblue:"#4682B4",darkslateblue:"#483D8B",mediumturquoise:"#48D1CC",indigo:"#4B0082",darkolivegreen:"#556B2F",cadetblue:"#5F9EA0",cornflowerblue:"#6495ED",mediumaquamarine:"#66CDAA",dimgray:"#696969",slateblue:"#6A5ACD",olivedrab:"#6B8E23",slategray:"#708090",lightslategray:"#778899",mediumslateblue:"#7B68EE",lawngreen:"#7CFC00",chartreuse:"#7FFF00",aquamarine:"#7FFFD4",maroon:"#800000",purple:"#800080",olive:"#808000",gray:"#808080",skyblue:"#87CEEB",lightskyblue:"#87CEFA",blueviolet:"#8A2BE2",darkred:"#8B0000",darkmagenta:"#8B008B",saddlebrown:"#8B4513",darkseagreen:"#8FBC8F",lightgreen:"#90EE90",mediumpurple:"#9370D8",darkviolet:"#9400D3",palegreen:"#98FB98",darkorchid:"#9932CC",yellowgreen:"#9ACD32",sienna:"#A0522D",brown:"#A52A2A",darkgray:"#A9A9A9",lightblue:"#ADD8E6",greenyellow:"#ADFF2F",paleturquoise:"#AFEEEE",lightsteelblue:"#B0C4DE",powderblue:"#B0E0E6",firebrick:"#B22222",darkgoldenrod:"#B8860B",mediumorchid:"#BA55D3",rosybrown:"#BC8F8F",darkkhaki:"#BDB76B",silver:"#C0C0C0",mediumvioletred:"#C71585",indianred:"#CD5C5C",peru:"#CD853F",chocolate:"#D2691E",tan:"#D2B48C",lightgrey:"#D3D3D3",palevioletred:"#D87093",thistle:"#D8BFD8",orchid:"#DA70D6",goldenrod:"#DAA520",crimson:"#DC143C",gainsboro:"#DCDCDC",plum:"#DDA0DD",burlywood:"#DEB887",lightcyan:"#E0FFFF",lavender:"#E6E6FA",darksalmon:"#E9967A",violet:"#EE82EE",palegoldenrod:"#EEE8AA",lightcoral:"#F08080",khaki:"#F0E68C",aliceblue:"#F0F8FF",honeydew:"#F0FFF0",azure:"#F0FFFF",sandybrown:"#F4A460",wheat:"#F5DEB3",beige:"#F5F5DC",whitesmoke:"#F5F5F5",mintcream:"#F5FFFA",ghostwhite:"#F8F8FF",salmon:"#FA8072",antiquewhite:"#FAEBD7",linen:"#FAF0E6",lightgoldenrodyellow:"#FAFAD2",oldlace:"#FDF5E6",red:"#FF0000",fuchsia:"#FF00FF",magenta:"#FF00FF",deeppink:"#FF1493",orangered:"#FF4500",tomato:"#FF6347",hotpink:"#FF69B4",coral:"#FF7F50",darkorange:"#FF8C00",lightsalmon:"#FFA07A",orange:"#FFA500",lightpink:"#FFB6C1",pink:"#FFC0CB",gold:"#FFD700",peachpuff:"#FFDAB9",navajowhite:"#FFDEAD",moccasin:"#FFE4B5",bisque:"#FFE4C4",mistyrose:"#FFE4E1",blanchedalmond:"#FFEBCD",papayawhip:"#FFEFD5",lavenderblush:"#FFF0F5",seashell:"#FFF5EE",cornsilk:"#FFF8DC",lemonchiffon:"#FFFACD",floralwhite:"#FFFAF0",snow:"#FFFAFA",yellow:"#FFFF00",lightyellow:"#FFFFE0",ivory:"#FFFFF0",white:"#FFFFFF"},Dm=function(){function t(){var e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:1;Yd(this,t),this.pixelRatio=e,this.generated=!1,this.centerCoordinates={x:144.5,y:144.5},this.r=289*.49,this.color={r:255,g:255,b:255,a:1},this.hueCircle=void 0,this.initialColor={r:255,g:255,b:255,a:1},this.previousColor=void 0,this.applied=!1,this.updateCallback=function(){},this.closeCallback=function(){},this._create()}return Kd(t,[{key:"insertTo",value:function(t){void 0!==this.hammer&&(this.hammer.destroy(),this.hammer=void 0),this.container=t,this.container.appendChild(this.frame),this._bindHammer(),this._setSize()}},{key:"setUpdateCallback",value:function(t){if("function"!=typeof t)throw new Error("Function attempted to set as colorPicker update callback is not a function.");this.updateCallback=t}},{key:"setCloseCallback",value:function(t){if("function"!=typeof t)throw new Error("Function attempted to set as colorPicker closing callback is not a function.");this.closeCallback=t}},{key:"_isColorString",value:function(t){if("string"==typeof t)return Pm[t]}},{key:"setColor",value:function(t){var e=!(arguments.length>1&&void 0!==arguments[1])||arguments[1];if("none"!==t){var i,n=this._isColorString(t);if(void 0!==n&&(t=n),!0===$y(t)){if(!0===Em(t)){var o=t.substr(4).substr(0,t.length-5).split(",");i={r:o[0],g:o[1],b:o[2],a:1}}else if(!0===Om(t)){var r=t.substr(5).substr(0,t.length-6).split(",");i={r:r[0],g:r[1],b:r[2],a:r[3]}}else if(!0===xm(t)){var s=fm(t);i={r:s.r,g:s.g,b:s.b,a:1}}}else if(t instanceof Object&&void 0!==t.r&&void 0!==t.g&&void 0!==t.b){var a=void 0!==t.a?t.a:"1.0";i={r:t.r,g:t.g,b:t.b,a:a}}if(void 0===i)throw new Error("Unknown color passed to the colorPicker. Supported are strings: rgb, hex, rgba. Object: rgb ({r:r,g:g,b:b,[a:a]}). Supplied: "+gv(t));this._setColor(i,e)}}},{key:"show",value:function(){void 0!==this.closeCallback&&(this.closeCallback(),this.closeCallback=void 0),this.applied=!1,this.frame.style.display="block",this._generateHueCircle()}},{key:"_hide",value:function(){var t=this,e=!(arguments.length>0&&void 0!==arguments[0])||arguments[0];!0===e&&(this.previousColor=un({},this.color)),!0===this.applied&&this.updateCallback(this.initialColor),this.frame.style.display="none",Sv((function(){void 0!==t.closeCallback&&(t.closeCallback(),t.closeCallback=void 0)}),0)}},{key:"_save",value:function(){this.updateCallback(this.color),this.applied=!1,this._hide()}},{key:"_apply",value:function(){this.applied=!0,this.updateCallback(this.color),this._updatePicker(this.color)}},{key:"_loadLast",value:function(){void 0!==this.previousColor?this.setColor(this.previousColor,!1):alert("There is no last color to load...")}},{key:"_setColor",value:function(t){var e=!(arguments.length>1&&void 0!==arguments[1])||arguments[1];!0===e&&(this.initialColor=un({},t)),this.color=t;var i=ym(t.r,t.g,t.b),n=2*Math.PI,o=this.r*i.s,r=this.centerCoordinates.x+o*Math.sin(n*i.h),s=this.centerCoordinates.y+o*Math.cos(n*i.h);this.colorPickerSelector.style.left=r-.5*this.colorPickerSelector.clientWidth+"px",this.colorPickerSelector.style.top=s-.5*this.colorPickerSelector.clientHeight+"px",this._updatePicker(t)}},{key:"_setOpacity",value:function(t){this.color.a=t/100,this._updatePicker(this.color)}},{key:"_setBrightness",value:function(t){var e=ym(this.color.r,this.color.g,this.color.b);e.v=t/100;var i=wm(e.h,e.s,e.v);i.a=this.color.a,this.color=i,this._updatePicker()}},{key:"_updatePicker",value:function(){var t=arguments.length>0&&void 0!==arguments[0]?arguments[0]:this.color,e=ym(t.r,t.g,t.b),i=this.colorPickerCanvas.getContext("2d");void 0===this.pixelRation&&(this.pixelRatio=(window.devicePixelRatio||1)/(i.webkitBackingStorePixelRatio||i.mozBackingStorePixelRatio||i.msBackingStorePixelRatio||i.oBackingStorePixelRatio||i.backingStorePixelRatio||1)),i.setTransform(this.pixelRatio,0,0,this.pixelRatio,0,0);var n=this.colorPickerCanvas.clientWidth,o=this.colorPickerCanvas.clientHeight;i.clearRect(0,0,n,o),i.putImageData(this.hueCircle,0,0),i.fillStyle="rgba(0,0,0,"+(1-e.v)+")",i.circle(this.centerCoordinates.x,this.centerCoordinates.y,this.r),jv(i).call(i),this.brightnessRange.value=100*e.v,this.opacityRange.value=100*t.a,this.initialColorDiv.style.backgroundColor="rgba("+this.initialColor.r+","+this.initialColor.g+","+this.initialColor.b+","+this.initialColor.a+")",this.newColorDiv.style.backgroundColor="rgba("+this.color.r+","+this.color.g+","+this.color.b+","+this.color.a+")"}},{key:"_setSize",value:function(){this.colorPickerCanvas.style.width="100%",this.colorPickerCanvas.style.height="100%",this.colorPickerCanvas.width=289*this.pixelRatio,this.colorPickerCanvas.height=289*this.pixelRatio}},{key:"_create",value:function(){var t,e,i,n;if(this.frame=document.createElement("div"),this.frame.className="vis-color-picker",this.colorPickerDiv=document.createElement("div"),this.colorPickerSelector=document.createElement("div"),this.colorPickerSelector.className="vis-selector",this.colorPickerDiv.appendChild(this.colorPickerSelector),this.colorPickerCanvas=document.createElement("canvas"),this.colorPickerDiv.appendChild(this.colorPickerCanvas),this.colorPickerCanvas.getContext){var o=this.colorPickerCanvas.getContext("2d");this.pixelRatio=(window.devicePixelRatio||1)/(o.webkitBackingStorePixelRatio||o.mozBackingStorePixelRatio||o.msBackingStorePixelRatio||o.oBackingStorePixelRatio||o.backingStorePixelRatio||1),this.colorPickerCanvas.getContext("2d").setTransform(this.pixelRatio,0,0,this.pixelRatio,0,0)}else{var r=document.createElement("DIV");r.style.color="red",r.style.fontWeight="bold",r.style.padding="10px",r.innerText="Error: your browser does not support HTML canvas",this.colorPickerCanvas.appendChild(r)}this.colorPickerDiv.className="vis-color",this.opacityDiv=document.createElement("div"),this.opacityDiv.className="vis-opacity",this.brightnessDiv=document.createElement("div"),this.brightnessDiv.className="vis-brightness",this.arrowDiv=document.createElement("div"),this.arrowDiv.className="vis-arrow",this.opacityRange=document.createElement("input");try{this.opacityRange.type="range",this.opacityRange.min="0",this.opacityRange.max="100"}catch(t){}this.opacityRange.value="100",this.opacityRange.className="vis-range",this.brightnessRange=document.createElement("input");try{this.brightnessRange.type="range",this.brightnessRange.min="0",this.brightnessRange.max="100"}catch(t){}this.brightnessRange.value="100",this.brightnessRange.className="vis-range",this.opacityDiv.appendChild(this.opacityRange),this.brightnessDiv.appendChild(this.brightnessRange);var s=this;this.opacityRange.onchange=function(){s._setOpacity(this.value)},this.opacityRange.oninput=function(){s._setOpacity(this.value)},this.brightnessRange.onchange=function(){s._setBrightness(this.value)},this.brightnessRange.oninput=function(){s._setBrightness(this.value)},this.brightnessLabel=document.createElement("div"),this.brightnessLabel.className="vis-label vis-brightness",this.brightnessLabel.innerText="brightness:",this.opacityLabel=document.createElement("div"),this.opacityLabel.className="vis-label vis-opacity",this.opacityLabel.innerText="opacity:",this.newColorDiv=document.createElement("div"),this.newColorDiv.className="vis-new-color",this.newColorDiv.innerText="new",this.initialColorDiv=document.createElement("div"),this.initialColorDiv.className="vis-initial-color",this.initialColorDiv.innerText="initial",this.cancelButton=document.createElement("div"),this.cancelButton.className="vis-button vis-cancel",this.cancelButton.innerText="cancel",this.cancelButton.onclick=zn(t=this._hide).call(t,this,!1),this.applyButton=document.createElement("div"),this.applyButton.className="vis-button vis-apply",this.applyButton.innerText="apply",this.applyButton.onclick=zn(e=this._apply).call(e,this),this.saveButton=document.createElement("div"),this.saveButton.className="vis-button vis-save",this.saveButton.innerText="save",this.saveButton.onclick=zn(i=this._save).call(i,this),this.loadButton=document.createElement("div"),this.loadButton.className="vis-button vis-load",this.loadButton.innerText="load last",this.loadButton.onclick=zn(n=this._loadLast).call(n,this),this.frame.appendChild(this.colorPickerDiv),this.frame.appendChild(this.arrowDiv),this.frame.appendChild(this.brightnessLabel),this.frame.appendChild(this.brightnessDiv),this.frame.appendChild(this.opacityLabel),this.frame.appendChild(this.opacityDiv),this.frame.appendChild(this.newColorDiv),this.frame.appendChild(this.initialColorDiv),this.frame.appendChild(this.cancelButton),this.frame.appendChild(this.applyButton),this.frame.appendChild(this.saveButton),this.frame.appendChild(this.loadButton)}},{key:"_bindHammer",value:function(){var t=this;this.drag={},this.pinch={},this.hammer=new Hy(this.colorPickerCanvas),this.hammer.get("pinch").set({enable:!0}),this.hammer.on("hammer.input",(function(e){e.isFirst&&t._moveSelector(e)})),this.hammer.on("tap",(function(e){t._moveSelector(e)})),this.hammer.on("panstart",(function(e){t._moveSelector(e)})),this.hammer.on("panmove",(function(e){t._moveSelector(e)})),this.hammer.on("panend",(function(e){t._moveSelector(e)}))}},{key:"_generateHueCircle",value:function(){if(!1===this.generated){var t=this.colorPickerCanvas.getContext("2d");void 0===this.pixelRation&&(this.pixelRatio=(window.devicePixelRatio||1)/(t.webkitBackingStorePixelRatio||t.mozBackingStorePixelRatio||t.msBackingStorePixelRatio||t.oBackingStorePixelRatio||t.backingStorePixelRatio||1)),t.setTransform(this.pixelRatio,0,0,this.pixelRatio,0,0);var e,i,n,o,r=this.colorPickerCanvas.clientWidth,s=this.colorPickerCanvas.clientHeight;t.clearRect(0,0,r,s),this.centerCoordinates={x:.5*r,y:.5*s},this.r=.49*r;var a,h=2*Math.PI/360,l=1/this.r;for(n=0;n<360;n++)for(o=0;o<this.r;o++)e=this.centerCoordinates.x+o*Math.sin(h*n),i=this.centerCoordinates.y+o*Math.cos(h*n),a=wm(.002777777777777778*n,o*l,1),t.fillStyle="rgb("+a.r+","+a.g+","+a.b+")",t.fillRect(e-.5,i-.5,2,2);t.strokeStyle="rgba(0,0,0,1)",t.circle(this.centerCoordinates.x,this.centerCoordinates.y,this.r),t.stroke(),this.hueCircle=t.getImageData(0,0,r,s)}this.generated=!0}},{key:"_moveSelector",value:function(t){var e=this.colorPickerDiv.getBoundingClientRect(),i=t.center.x-e.left,n=t.center.y-e.top,o=.5*this.colorPickerDiv.clientHeight,r=.5*this.colorPickerDiv.clientWidth,s=i-r,a=n-o,h=Math.atan2(s,a),l=.98*Math.min(Math.sqrt(s*s+a*a),r),d=Math.cos(h)*l+o,c=Math.sin(h)*l+r;this.colorPickerSelector.style.top=d-.5*this.colorPickerSelector.clientHeight+"px",this.colorPickerSelector.style.left=c-.5*this.colorPickerSelector.clientWidth+"px";var u=h/(2*Math.PI);u=u<0?u+1:u;var f=l/this.r,p=ym(this.color.r,this.color.g,this.color.b);p.h=u,p.s=f;var v=wm(p.h,p.s,p.v);v.a=this.color.a,this.color=v,this.initialColorDiv.style.backgroundColor="rgba("+this.initialColor.r+","+this.initialColor.g+","+this.initialColor.b+","+this.initialColor.a+")",this.newColorDiv.style.backgroundColor="rgba("+this.color.r+","+this.color.g+","+this.color.b+","+this.color.a+")"}}]),t}();function Im(){for(var t=arguments.length,e=new Array(t),i=0;i<t;i++)e[i]=arguments[i];if(e.length<1)throw new TypeError("Invalid arguments.");if(1===e.length)return document.createTextNode(e[0]);var n=document.createElement(e[0]);return n.appendChild(Im.apply(void 0,Jc(au(e).call(e,1)))),n}var Bm,zm=function(){function t(e,i,n){var o=arguments.length>3&&void 0!==arguments[3]?arguments[3]:1,r=arguments.length>4&&void 0!==arguments[4]?arguments[4]:function(){return!1};Yd(this,t),this.parent=e,this.changedOptions=[],this.container=i,this.allowCreation=!1,this.hideOption=r,this.options={},this.initialized=!1,this.popupCounter=0,this.defaultOptions={enabled:!1,filter:!0,container:void 0,showButton:!0},un(this.options,this.defaultOptions),this.configureOptions=n,this.moduleOptions={},this.domElements=[],this.popupDiv={},this.popupLimit=5,this.popupHistory={},this.colorPicker=new Dm(o),this.wrapper=void 0}return Kd(t,[{key:"setOptions",value:function(t){if(void 0!==t){this.popupHistory={},this._removePopup();var e=!0;if("string"==typeof t)this.options.filter=t;else if(lu(t))this.options.filter=t.join();else if("object"===Qc(t)){if(null==t)throw new TypeError("options cannot be null");void 0!==t.container&&(this.options.container=t.container),void 0!==Xf(t)&&(this.options.filter=Xf(t)),void 0!==t.showButton&&(this.options.showButton=t.showButton),void 0!==t.enabled&&(e=t.enabled)}else"boolean"==typeof t?(this.options.filter=!0,e=t):"function"==typeof t&&(this.options.filter=t,e=!0);!1===Xf(this.options)&&(e=!1),this.options.enabled=e}this._clean()}},{key:"setModuleOptions",value:function(t){this.moduleOptions=t,!0===this.options.enabled&&(this._clean(),void 0!==this.options.container&&(this.container=this.options.container),this._create())}},{key:"_create",value:function(){this._clean(),this.changedOptions=[];var t=Xf(this.options),e=0,i=!1;for(var n in this.configureOptions)Object.prototype.hasOwnProperty.call(this.configureOptions,n)&&(this.allowCreation=!1,i=!1,"function"==typeof t?i=(i=t(n,[]))||this._handleObject(this.configureOptions[n],[n],!0):!0!==t&&-1===Fp(t).call(t,n)||(i=!0),!1!==i&&(this.allowCreation=!0,e>0&&this._makeItem([]),this._makeHeader(n),this._handleObject(this.configureOptions[n],[n])),e++);this._makeButton(),this._push()}},{key:"_push",value:function(){this.wrapper=document.createElement("div"),this.wrapper.className="vis-configuration-wrapper",this.container.appendChild(this.wrapper);for(var t=0;t<this.domElements.length;t++)this.wrapper.appendChild(this.domElements[t]);this._showPopupIfNeeded()}},{key:"_clean",value:function(){for(var t=0;t<this.domElements.length;t++)this.wrapper.removeChild(this.domElements[t]);void 0!==this.wrapper&&(this.container.removeChild(this.wrapper),this.wrapper=void 0),this.domElements=[],this._removePopup()}},{key:"_getValue",value:function(t){for(var e=this.moduleOptions,i=0;i<t.length;i++){if(void 0===e[t[i]]){e=void 0;break}e=e[t[i]]}return e}},{key:"_makeItem",value:function(t){if(!0===this.allowCreation){var e=document.createElement("div");e.className="vis-configuration vis-config-item vis-config-s"+t.length;for(var i=arguments.length,n=new Array(i>1?i-1:0),o=1;o<i;o++)n[o-1]=arguments[o];return Fu(n).call(n,(function(t){e.appendChild(t)})),this.domElements.push(e),this.domElements.length}return 0}},{key:"_makeHeader",value:function(t){var e=document.createElement("div");e.className="vis-configuration vis-config-header",e.innerText=t,this._makeItem([],e)}},{key:"_makeLabel",value:function(t,e){var i=arguments.length>2&&void 0!==arguments[2]&&arguments[2],n=document.createElement("div");if(n.className="vis-configuration vis-config-label vis-config-s"+e.length,!0===i){for(;n.firstChild;)n.removeChild(n.firstChild);n.appendChild(Im("i","b",t))}else n.innerText=t+":";return n}},{key:"_makeDropdown",value:function(t,e,i){var n=document.createElement("select");n.className="vis-configuration vis-config-select";var o=0;void 0!==e&&-1!==Fp(t).call(t,e)&&(o=Fp(t).call(t,e));for(var r=0;r<t.length;r++){var s=document.createElement("option");s.value=t[r],r===o&&(s.selected="selected"),s.innerText=t[r],n.appendChild(s)}var a=this;n.onchange=function(){a._update(this.value,i)};var h=this._makeLabel(i[i.length-1],i);this._makeItem(i,h,n)}},{key:"_makeRange",value:function(t,e,i){var n=t[0],o=t[1],r=t[2],s=t[3],a=document.createElement("input");a.className="vis-configuration vis-config-range";try{a.type="range",a.min=o,a.max=r}catch(t){}a.step=s;var h="",l=0;if(void 0!==e){var d=1.2;e<0&&e*d<o?(a.min=Math.ceil(e*d),l=a.min,h="range increased"):e/d<o&&(a.min=Math.ceil(e/d),l=a.min,h="range increased"),e*d>r&&1!==r&&(a.max=Math.ceil(e*d),l=a.max,h="range increased"),a.value=e}else a.value=n;var c=document.createElement("input");c.className="vis-configuration vis-config-rangeinput",c.value=a.value;var u=this;a.onchange=function(){c.value=this.value,u._update(Number(this.value),i)},a.oninput=function(){c.value=this.value};var f=this._makeLabel(i[i.length-1],i),p=this._makeItem(i,f,a,c);""!==h&&this.popupHistory[p]!==l&&(this.popupHistory[p]=l,this._setupPopup(h,p))}},{key:"_makeButton",value:function(){var t=this;if(!0===this.options.showButton){var e=document.createElement("div");e.className="vis-configuration vis-config-button",e.innerText="generate options",e.onclick=function(){t._printOptions()},e.onmouseover=function(){e.className="vis-configuration vis-config-button hover"},e.onmouseout=function(){e.className="vis-configuration vis-config-button"},this.optionsContainer=document.createElement("div"),this.optionsContainer.className="vis-configuration vis-config-option-container",this.domElements.push(this.optionsContainer),this.domElements.push(e)}}},{key:"_setupPopup",value:function(t,e){var i=this;if(!0===this.initialized&&!0===this.allowCreation&&this.popupCounter<this.popupLimit){var n=document.createElement("div");n.id="vis-configuration-popup",n.className="vis-configuration-popup",n.innerText=t,n.onclick=function(){i._removePopup()},this.popupCounter+=1,this.popupDiv={html:n,index:e}}}},{key:"_removePopup",value:function(){void 0!==this.popupDiv.html&&(this.popupDiv.html.parentNode.removeChild(this.popupDiv.html),clearTimeout(this.popupDiv.hideTimeout),clearTimeout(this.popupDiv.deleteTimeout),this.popupDiv={})}},{key:"_showPopupIfNeeded",value:function(){var t=this;if(void 0!==this.popupDiv.html){var e=this.domElements[this.popupDiv.index].getBoundingClientRect();this.popupDiv.html.style.left=e.left+"px",this.popupDiv.html.style.top=e.top-30+"px",document.body.appendChild(this.popupDiv.html),this.popupDiv.hideTimeout=Sv((function(){t.popupDiv.html.style.opacity=0}),1500),this.popupDiv.deleteTimeout=Sv((function(){t._removePopup()}),1800)}}},{key:"_makeCheckbox",value:function(t,e,i){var n=document.createElement("input");n.type="checkbox",n.className="vis-configuration vis-config-checkbox",n.checked=t,void 0!==e&&(n.checked=e,e!==t&&("object"===Qc(t)?e!==t.enabled&&this.changedOptions.push({path:i,value:e}):this.changedOptions.push({path:i,value:e})));var o=this;n.onchange=function(){o._update(this.checked,i)};var r=this._makeLabel(i[i.length-1],i);this._makeItem(i,r,n)}},{key:"_makeTextInput",value:function(t,e,i){var n=document.createElement("input");n.type="text",n.className="vis-configuration vis-config-text",n.value=e,e!==t&&this.changedOptions.push({path:i,value:e});var o=this;n.onchange=function(){o._update(this.value,i)};var r=this._makeLabel(i[i.length-1],i);this._makeItem(i,r,n)}},{key:"_makeColorField",value:function(t,e,i){var n=this,o=t[1],r=document.createElement("div");"none"!==(e=void 0===e?o:e)?(r.className="vis-configuration vis-config-colorBlock",r.style.backgroundColor=e):r.className="vis-configuration vis-config-colorBlock none",e=void 0===e?o:e,r.onclick=function(){n._showColorPicker(e,r,i)};var s=this._makeLabel(i[i.length-1],i);this._makeItem(i,s,r)}},{key:"_showColorPicker",value:function(t,e,i){var n=this;e.onclick=function(){},this.colorPicker.insertTo(e),this.colorPicker.show(),this.colorPicker.setColor(t),this.colorPicker.setUpdateCallback((function(t){var o="rgba("+t.r+","+t.g+","+t.b+","+t.a+")";e.style.backgroundColor=o,n._update(o,i)})),this.colorPicker.setCloseCallback((function(){e.onclick=function(){n._showColorPicker(t,e,i)}}))}},{key:"_handleObject",value:function(t){var e=arguments.length>1&&void 0!==arguments[1]?arguments[1]:[],i=arguments.length>2&&void 0!==arguments[2]&&arguments[2],n=!1,o=Xf(this.options),r=!1;for(var s in t)if(Object.prototype.hasOwnProperty.call(t,s)){n=!0;var a=t[s],h=om(e,s);if("function"==typeof o&&!1===(n=o(s,e))&&!lu(a)&&"string"!=typeof a&&"boolean"!=typeof a&&a instanceof Object&&(this.allowCreation=!1,n=this._handleObject(a,h,!0),this.allowCreation=!1===i),!1!==n){r=!0;var l=this._getValue(h);if(lu(a))this._handleArray(a,l,h);else if("string"==typeof a)this._makeTextInput(a,l,h);else if("boolean"==typeof a)this._makeCheckbox(a,l,h);else if(a instanceof Object){if(!this.hideOption(e,s,this.moduleOptions))if(void 0!==a.enabled){var d=om(h,"enabled"),c=this._getValue(d);if(!0===c){var u=this._makeLabel(s,h,!0);this._makeItem(h,u),r=this._handleObject(a,h)||r}else this._makeCheckbox(a,c,h)}else{var f=this._makeLabel(s,h,!0);this._makeItem(h,f),r=this._handleObject(a,h)||r}}else console.error("dont know how to handle",a,s,h)}}return r}},{key:"_handleArray",value:function(t,e,i){"string"==typeof t[0]&&"color"===t[0]?(this._makeColorField(t,e,i),t[1]!==e&&this.changedOptions.push({path:i,value:e})):"string"==typeof t[0]?(this._makeDropdown(t,e,i),t[0]!==e&&this.changedOptions.push({path:i,value:e})):"number"==typeof t[0]&&(this._makeRange(t,e,i),t[0]!==e&&this.changedOptions.push({path:i,value:Number(e)}))}},{key:"_update",value:function(t,e){var i=this._constructOptions(t,e);this.parent.body&&this.parent.body.emitter&&this.parent.body.emitter.emit&&this.parent.body.emitter.emit("configChange",i),this.initialized=!0,this.parent.setOptions(i)}},{key:"_constructOptions",value:function(t,e){var i=arguments.length>2&&void 0!==arguments[2]?arguments[2]:{},n=i;t="false"!==(t="true"===t||t)&&t;for(var o=0;o<e.length;o++)"global"!==e[o]&&(void 0===n[e[o]]&&(n[e[o]]={}),o!==e.length-1?n=n[e[o]]:n[e[o]]=t);return i}},{key:"_printOptions",value:function(){for(var t=this.getOptions();this.optionsContainer.firstChild;)this.optionsContainer.removeChild(this.optionsContainer.firstChild);this.optionsContainer.appendChild(Im("pre","const options = "+gv(t,null,2)))}},{key:"getOptions",value:function(){for(var t={},e=0;e<this.changedOptions.length;e++)this._constructOptions(this.changedOptions[e].value,this.changedOptions[e].path,t);return t}}]),t}(),Nm=function(){function t(e,i){Yd(this,t),this.container=e,this.overflowMethod=i||"cap",this.x=0,this.y=0,this.padding=5,this.hidden=!1,this.frame=document.createElement("div"),this.frame.className="vis-tooltip",this.container.appendChild(this.frame)}return Kd(t,[{key:"setPosition",value:function(t,e){this.x=Ep(t),this.y=Ep(e)}},{key:"setText",value:function(t){if(t instanceof Element){for(;this.frame.firstChild;)this.frame.removeChild(this.frame.firstChild);this.frame.appendChild(t)}else this.frame.innerText=t}},{key:"show",value:function(t){if(void 0===t&&(t=!0),!0===t){var e=this.frame.clientHeight,i=this.frame.clientWidth,n=this.frame.parentNode.clientHeight,o=this.frame.parentNode.clientWidth,r=0,s=0;if("flip"==this.overflowMethod){var a=!1,h=!0;this.y-e<this.padding&&(h=!1),this.x+i>o-this.padding&&(a=!0),r=a?this.x-i:this.x,s=h?this.y-e:this.y}else(s=this.y-e)+e+this.padding>n&&(s=n-e-this.padding),s<this.padding&&(s=this.padding),(r=this.x)+i+this.padding>o&&(r=o-i-this.padding),r<this.padding&&(r=this.padding);this.frame.style.left=r+"px",this.frame.style.top=s+"px",this.frame.style.visibility="visible",this.hidden=!1}else this.hide()}},{key:"hide",value:function(){this.hidden=!0,this.frame.style.left="0",this.frame.style.top="0",this.frame.style.visibility="hidden"}},{key:"destroy",value:function(){this.frame.parentNode.removeChild(this.frame)}}]),t}(),Fm=!1,Am="background: #FFeeee; color: #dd0000",jm=function(){function t(){Yd(this,t)}return Kd(t,null,[{key:"validate",value:function(e,i,n){Fm=!1,Bm=i;var o=i;return void 0!==n&&(o=i[n]),t.parse(e,o,[]),Fm}},{key:"parse",value:function(e,i,n){for(var o in e)Object.prototype.hasOwnProperty.call(e,o)&&t.check(o,e,i,n)}},{key:"check",value:function(e,i,n,o){if(void 0!==n[e]||void 0!==n.__any__){var r=e,s=!0;void 0===n[e]&&void 0!==n.__any__&&(r="__any__",s="object"===t.getType(i[e]));var a=n[r];s&&void 0!==a.__type__&&(a=a.__type__),t.checkFields(e,i,n,r,a,o)}else t.getSuggestion(e,n,o)}},{key:"checkFields",value:function(e,i,n,o,r,s){var a=function(i){console.error("%c"+i+t.printLocation(s,e),Am)},h=t.getType(i[e]),l=r[h];void 0!==l?"array"===t.getType(l)&&-1===Fp(l).call(l,i[e])?(a('Invalid option detected in "'+e+'". Allowed values are:'+t.print(l)+' not "'+i[e]+'". '),Fm=!0):"object"===h&&"__any__"!==o&&(s=om(s,e),t.parse(i[e],n[o],s)):void 0===r.any&&(a('Invalid type received for "'+e+'". Expected: '+t.print(bu(r))+". Received ["+h+'] "'+i[e]+'"'),Fm=!0)}},{key:"getType",value:function(t){var e=Qc(t);return"object"===e?null===t?"null":t instanceof Boolean?"boolean":t instanceof Number?"number":t instanceof String?"string":lu(t)?"array":t instanceof Date?"date":void 0!==t.nodeType?"dom":!0===t._isAMomentObject?"moment":"object":"number"===e?"number":"boolean"===e?"boolean":"string"===e?"string":void 0===e?"undefined":e}},{key:"getSuggestion",value:function(e,i,n){var o,r=t.findInOptions(e,i,n,!1),s=t.findInOptions(e,Bm,[],!0);o=void 0!==r.indexMatch?" in "+t.printLocation(r.path,e,"")+'Perhaps it was incomplete? Did you mean: "'+r.indexMatch+'"?\n\n':s.distance<=4&&r.distance>s.distance?" in "+t.printLocation(r.path,e,"")+"Perhaps it was misplaced? Matching option found at: "+t.printLocation(s.path,s.closestMatch,""):r.distance<=8?'. Did you mean "'+r.closestMatch+'"?'+t.printLocation(r.path,e):". Did you mean one of these: "+t.print(bu(i))+t.printLocation(n,e),console.error('%cUnknown option detected: "'+e+'"'+o,Am),Fm=!0}},{key:"findInOptions",value:function(e,i,n){var o=arguments.length>3&&void 0!==arguments[3]&&arguments[3],r=1e9,s="",a=[],h=e.toLowerCase(),l=void 0;for(var d in i){var c=void 0;if(void 0!==i[d].__type__&&!0===o){var u=t.findInOptions(e,i[d],om(n,d));r>u.distance&&(s=u.closestMatch,a=u.path,r=u.distance,l=u.indexMatch)}else{var f;-1!==Fp(f=d.toLowerCase()).call(f,h)&&(l=d),r>(c=t.levenshteinDistance(e,d))&&(s=d,a=rm(n),r=c)}}return{closestMatch:s,path:a,distance:r,indexMatch:l}}},{key:"printLocation",value:function(t,e){for(var i=arguments.length>2&&void 0!==arguments[2]?arguments[2]:"Problem value found at: \n",n="\n\n"+i+"options = {\n",o=0;o<t.length;o++){for(var r=0;r<o+1;r++)n+="  ";n+=t[o]+": {\n"}for(var s=0;s<t.length+1;s++)n+="  ";n+=e+"\n";for(var a=0;a<t.length+1;a++){for(var h=0;h<t.length-a;h++)n+="  ";n+="}\n"}return n+"\n\n"}},{key:"print",value:function(t){return gv(t).replace(/(")|(\[)|(\])|(,"__type__")/g,"").replace(/(,)/g,", ")}},{key:"levenshteinDistance",value:function(t,e){if(0===t.length)return e.length;if(0===e.length)return t.length;var i,n,o=[];for(i=0;i<=e.length;i++)o[i]=[i];for(n=0;n<=t.length;n++)o[0][n]=n;for(i=1;i<=e.length;i++)for(n=1;n<=t.length;n++)e.charAt(i-1)==t.charAt(n-1)?o[i][n]=o[i-1][n-1]:o[i][n]=Math.min(o[i-1][n-1]+1,Math.min(o[i][n-1]+1,o[i-1][n]+1));return o[e.length][t.length]}}]),t}(),Rm=Wy,Lm=Dm,Hm=zm,Wm=Hy,qm=Nm,Vm=Am,Um=jm,Ym=Object.freeze({__proto__:null,Activator:Rm,Alea:jy,ColorPicker:Lm,Configurator:Hm,DELETE:Iy,HSVToHex:km,HSVToRGB:wm,Hammer:Wm,Popup:qm,RGBToHSV:ym,RGBToHex:vm,VALIDATOR_PRINT_STYLE:Vm,Validator:Um,addClassName:function(t,e){var i=t.className.split(" "),n=e.split(" ");i=su(i).call(i,Xf(n).call(n,(function(t){return!Nf(i).call(i,t)}))),t.className=i.join(" ")},addCssText:function(t,e){var i=mm(t.style.cssText),n=mm(e),o=My(My({},i),n);t.style.cssText=bm(o)},addEventListener:dm,binarySearchCustom:function(t,e,i,n){for(var o=0,r=0,s=t.length-1;r<=s&&o<1e4;){var a=Math.floor((r+s)/2),h=t[a],l=e(void 0===n?h[i]:h[i][n]);if(0==l)return a;-1==l?r=a+1:s=a-1,o++}return-1},binarySearchValue:function(t,e,i,n,o){var r,s,a,h,l=0,d=0,c=t.length-1;for(o=null!=o?o:function(t,e){return t==e?0:t<e?-1:1};d<=c&&l<1e4;){if(h=Math.floor(.5*(c+d)),r=t[Math.max(0,h-1)][i],s=t[h][i],a=t[Math.min(t.length-1,h+1)][i],0==o(s,e))return h;if(o(r,e)<0&&o(s,e)>0)return"before"==n?Math.max(0,h-1):h;if(o(s,e)<0&&o(a,e)>0)return"before"==n?h:Math.min(t.length-1,h+1);o(s,e)<0?d=h+1:c=h-1,l++}return-1},bridgeObject:Cm,copyAndExtendArray:om,copyArray:rm,deepExtend:nm,deepObjectAssign:zy,easingFunctions:Tm,equalArray:function(t,e){if(t.length!==e.length)return!1;for(var i=0,n=t.length;i<n;i++)if(t[i]!=e[i])return!1;return!0},extend:tm,fillIfDefined:Jy,forEach:hm,getAbsoluteLeft:sm,getAbsoluteRight:function(t){return t.getBoundingClientRect().right},getAbsoluteTop:am,getScrollBarWidth:function(){var t=document.createElement("p");t.style.width="100%",t.style.height="200px";var e=document.createElement("div");e.style.position="absolute",e.style.top="0px",e.style.left="0px",e.style.visibility="hidden",e.style.width="200px",e.style.height="150px",e.style.overflow="hidden",e.appendChild(t),document.body.appendChild(e);var i=t.offsetWidth;e.style.overflow="scroll";var n=t.offsetWidth;return i==n&&(n=e.clientWidth),document.body.removeChild(e),i-n},getTarget:function(){var t=arguments.length>0&&void 0!==arguments[0]?arguments[0]:window.event,e=null;return t&&(t.target?e=t.target:t.srcElement&&(e=t.srcElement)),e instanceof Element&&(null==e.nodeType||3!=e.nodeType||(e=e.parentNode)instanceof Element)?e:null},getType:function(t){var e=Qc(t);return"object"===e?null===t?"null":t instanceof Boolean?"Boolean":t instanceof Number?"Number":t instanceof String?"String":lu(t)?"Array":t instanceof Date?"Date":"Object":"number"===e?"Number":"boolean"===e?"Boolean":"string"===e?"String":void 0===e?"undefined":e},hasParent:function(t,e){for(var i=t;i;){if(i===e)return!0;if(!i.parentNode)return!1;i=i.parentNode}return!1},hexToHSV:_m,hexToRGB:fm,insertSort:function(t,e){for(var i=0;i<t.length;i++){var n=t[i],o=void 0;for(o=i;o>0&&e(n,t[o-1])<0;o--)t[o]=t[o-1];t[o]=n}return t},isDate:function(t){if(t instanceof Date)return!0;if($y(t)){if(qy.exec(t))return!0;if(!isNaN(Date.parse(t)))return!0}return!1},isNumber:Gy,isObject:Zy,isString:$y,isValidHex:xm,isValidRGB:Em,isValidRGBA:Om,mergeOptions:Sm,option:um,overrideOpacity:pm,parseColor:gm,preventDefault:function(t){t||(t=window.event),t&&(t.preventDefault?t.preventDefault():t.returnValue=!1)},pureDeepObjectAssign:By,recursiveDOMDelete:Ky,removeClassName:function(t,e){var i=t.className.split(" "),n=e.split(" ");i=Xf(i).call(i,(function(t){return!Nf(n).call(n,t)})),t.className=i.join(" ")},removeCssText:function(t,e){var i=mm(t.style.cssText),n=mm(e);for(var o in n)Object.prototype.hasOwnProperty.call(n,o)&&delete i[o];t.style.cssText=bm(i)},removeEventListener:cm,selectiveBridgeObject:function(t,e){if(null!==e&&"object"===Qc(e)){for(var i=Kp(e),n=0;n<t.length;n++)Object.prototype.hasOwnProperty.call(e,t[n])&&"object"==Qc(e[t[n]])&&(i[t[n]]=Cm(e[t[n]]));return i}return null},selectiveDeepExtend:em,selectiveExtend:function(t,e){if(!lu(t))throw new Error("Array with property names expected as first argument");for(var i=arguments.length,n=new Array(i>2?i-2:0),o=2;o<i;o++)n[o-2]=arguments[o];for(var r=0,s=n;r<s.length;r++)for(var a=s[r],h=0;h<t.length;h++){var l=t[h];a&&Object.prototype.hasOwnProperty.call(a,l)&&(e[l]=a[l])}return e},selectiveNotDeepExtend:im,throttle:function(t){var e=!1;return function(){e||(e=!0,requestAnimationFrame((function(){e=!1,t()})))}},toArray:lm,topMost:Mm,updateProperty:function(t,e,i){return t[e]!==i&&(t[e]=i,!0)}});function Xm(t){return eb=t,function(){var t={};ib=0,void(nb=eb.charAt(0)),pb(),"strict"===ob&&(t.strict=!0,pb());"graph"!==ob&&"digraph"!==ob||(t.type=ob,pb());rb===Qm&&(t.id=ob,pb());if("{"!=ob)throw wb("Angle bracket { expected");if(pb(),vb(t),"}"!=ob)throw wb("Angle bracket } expected");if(pb(),""!==ob)throw wb("End of file expected");return pb(),delete t.node,delete t.edge,delete t.graph,t}()}var Gm={fontsize:"font.size",fontcolor:"font.color",labelfontcolor:"font.color",fontname:"font.face",color:["color.border","color.background"],fillcolor:"color.background",tooltip:"title",labeltooltip:"title"},Km=Kp(Gm);Km.color="color.color",Km.style="dashes";var $m=0,Zm=1,Qm=2,Jm=3,tb={"{":!0,"}":!0,"[":!0,"]":!0,";":!0,"=":!0,",":!0,"->":!0,"--":!0},eb="",ib=0,nb="",ob="",rb=$m;function sb(){ib++,nb=eb.charAt(ib)}function ab(){return eb.charAt(ib+1)}function hb(t){var e=t.charCodeAt(0);return e<47?35===e||46===e:e<59?e>47:e<91?e>64:e<96?95===e:e<123&&e>96}function lb(t,e){if(t||(t={}),e)for(var i in e)e.hasOwnProperty(i)&&(t[i]=e[i]);return t}function db(t,e,i){for(var n=e.split("."),o=t;n.length;){var r=n.shift();n.length?(o[r]||(o[r]={}),o=o[r]):o[r]=i}}function cb(t,e){for(var i,n,o=null,r=[t],s=t;s.parent;)r.push(s.parent),s=s.parent;if(s.nodes)for(i=0,n=s.nodes.length;i<n;i++)if(e.id===s.nodes[i].id){o=s.nodes[i];break}for(o||(o={id:e.id},t.node&&(o.attr=lb(o.attr,t.node))),i=r.length-1;i>=0;i--){var a,h=r[i];h.nodes||(h.nodes=[]),-1===Fp(a=h.nodes).call(a,o)&&h.nodes.push(o)}e.attr&&(o.attr=lb(o.attr,e.attr))}function ub(t,e){if(t.edges||(t.edges=[]),t.edges.push(e),t.edge){var i=lb({},t.edge);e.attr=lb(i,e.attr)}}function fb(t,e,i,n,o){var r={from:e,to:i,type:n};return t.edge&&(r.attr=lb({},t.edge)),r.attr=lb(r.attr||{},o),null!=o&&o.hasOwnProperty("arrows")&&null!=o.arrows&&(r.arrows={to:{enabled:!0,type:o.arrows.type}},o.arrows=null),r}function pb(){for(rb=$m,ob="";" "===nb||"\t"===nb||"\n"===nb||"\r"===nb;)sb();do{var t=!1;if("#"===nb){for(var e=ib-1;" "===eb.charAt(e)||"\t"===eb.charAt(e);)e--;if("\n"===eb.charAt(e)||""===eb.charAt(e)){for(;""!=nb&&"\n"!=nb;)sb();t=!0}}if("/"===nb&&"/"===ab()){for(;""!=nb&&"\n"!=nb;)sb();t=!0}if("/"===nb&&"*"===ab()){for(;""!=nb;){if("*"===nb&&"/"===ab()){sb(),sb();break}sb()}t=!0}for(;" "===nb||"\t"===nb||"\n"===nb||"\r"===nb;)sb()}while(t);if(""!==nb){var i=nb+ab();if(tb[i])return rb=Zm,ob=i,sb(),void sb();if(tb[nb])return rb=Zm,ob=nb,void sb();if(hb(nb)||"-"===nb){for(ob+=nb,sb();hb(nb);)ob+=nb,sb();return"false"===ob?ob=!1:"true"===ob?ob=!0:isNaN(Number(ob))||(ob=Number(ob)),void(rb=Qm)}if('"'===nb){for(sb();""!=nb&&('"'!=nb||'"'===nb&&'"'===ab());)'"'===nb?(ob+=nb,sb()):"\\"===nb&&"n"===ab()?(ob+="\n",sb()):ob+=nb,sb();if('"'!=nb)throw wb('End of string " expected');return sb(),void(rb=Qm)}for(rb=Jm;""!=nb;)ob+=nb,sb();throw new SyntaxError('Syntax error in part "'+kb(ob,30)+'"')}rb=Zm}function vb(t){for(;""!==ob&&"}"!=ob;)gb(t),";"===ob&&pb()}function gb(t){var e=yb(t);if(e)mb(t,e);else{var i=function(t){if("node"===ob)return pb(),t.node=bb(),"node";if("edge"===ob)return pb(),t.edge=bb(),"edge";if("graph"===ob)return pb(),t.graph=bb(),"graph";return null}(t);if(!i){if(rb!=Qm)throw wb("Identifier expected");var n=ob;if(pb(),"="===ob){if(pb(),rb!=Qm)throw wb("Identifier expected");t[n]=ob,pb()}else!function(t,e){var i={id:e},n=bb();n&&(i.attr=n);cb(t,i),mb(t,e)}(t,n)}}}function yb(t){var e=null;if("subgraph"===ob&&((e={}).type="subgraph",pb(),rb===Qm&&(e.id=ob,pb())),"{"===ob){if(pb(),e||(e={}),e.parent=t,e.node=t.node,e.edge=t.edge,e.graph=t.graph,vb(e),"}"!=ob)throw wb("Angle bracket } expected");pb(),delete e.node,delete e.edge,delete e.graph,delete e.parent,t.subgraphs||(t.subgraphs=[]),t.subgraphs.push(e)}return e}function mb(t,e){for(;"->"===ob||"--"===ob;){var i,n=ob;pb();var o=yb(t);if(o)i=o;else{if(rb!=Qm)throw wb("Identifier or subgraph expected");cb(t,{id:i=ob}),pb()}ub(t,fb(t,e,i,n,bb())),e=i}}function bb(){for(var t,e,i=null,n={dashed:!0,solid:!1,dotted:[1,5]},o={dot:"circle",box:"box",crow:"crow",curve:"curve",icurve:"inv_curve",normal:"triangle",inv:"inv_triangle",diamond:"diamond",tee:"bar",vee:"vee"},r=new Array,s=new Array;"["===ob;){for(pb(),i={};""!==ob&&"]"!=ob;){if(rb!=Qm)throw wb("Attribute name expected");var a=ob;if(pb(),"="!=ob)throw wb("Equal sign = expected");if(pb(),rb!=Qm)throw wb("Attribute value expected");var h=ob;"style"===a&&(h=n[h]),"arrowhead"===a&&(a="arrows",h={to:{enabled:!0,type:o[h]}}),"arrowtail"===a&&(a="arrows",h={from:{enabled:!0,type:o[h]}}),r.push({attr:i,name:a,value:h}),s.push(a),pb(),","==ob&&pb()}if("]"!=ob)throw wb("Bracket ] expected");pb()}if(Nf(s).call(s,"dir")){var l={arrows:{}};for(t=0;t<r.length;t++)if("arrows"===r[t].name)if(null!=r[t].value.to)l.arrows.to=t;else{if(null==r[t].value.from)throw wb("Invalid value of arrows");l.arrows.from=t}else"dir"===r[t].name&&(l.dir=t);var d,c,u=r[l.dir].value;if(!Nf(s).call(s,"arrows"))if("both"===u)r.push({attr:r[l.dir].attr,name:"arrows",value:{to:{enabled:!0}}}),l.arrows.to=r.length-1,r.push({attr:r[l.dir].attr,name:"arrows",value:{from:{enabled:!0}}}),l.arrows.from=r.length-1;else if("forward"===u)r.push({attr:r[l.dir].attr,name:"arrows",value:{to:{enabled:!0}}}),l.arrows.to=r.length-1;else if("back"===u)r.push({attr:r[l.dir].attr,name:"arrows",value:{from:{enabled:!0}}}),l.arrows.from=r.length-1;else{if("none"!==u)throw wb('Invalid dir type "'+u+'"');r.push({attr:r[l.dir].attr,name:"arrows",value:""}),l.arrows.to=r.length-1}if("both"===u)l.arrows.to&&l.arrows.from?(c=r[l.arrows.to].value.to.type,d=r[l.arrows.from].value.from.type,r[l.arrows.to]={attr:r[l.arrows.to].attr,name:r[l.arrows.to].name,value:{to:{enabled:!0,type:c},from:{enabled:!0,type:d}}},ff(r).call(r,l.arrows.from,1)):l.arrows.to?(c=r[l.arrows.to].value.to.type,d="arrow",r[l.arrows.to]={attr:r[l.arrows.to].attr,name:r[l.arrows.to].name,value:{to:{enabled:!0,type:c},from:{enabled:!0,type:d}}}):l.arrows.from&&(c="arrow",d=r[l.arrows.from].value.from.type,r[l.arrows.from]={attr:r[l.arrows.from].attr,name:r[l.arrows.from].name,value:{to:{enabled:!0,type:c},from:{enabled:!0,type:d}}});else if("back"===u)l.arrows.to&&l.arrows.from?(c="",d=r[l.arrows.from].value.from.type,r[l.arrows.from]={attr:r[l.arrows.from].attr,name:r[l.arrows.from].name,value:{to:{enabled:!0,type:c},from:{enabled:!0,type:d}}}):l.arrows.to?(c="",d="arrow",l.arrows.from=l.arrows.to,r[l.arrows.from]={attr:r[l.arrows.from].attr,name:r[l.arrows.from].name,value:{to:{enabled:!0,type:c},from:{enabled:!0,type:d}}}):l.arrows.from&&(c="",d=r[l.arrows.from].value.from.type,r[l.arrows.to]={attr:r[l.arrows.from].attr,name:r[l.arrows.from].name,value:{to:{enabled:!0,type:c},from:{enabled:!0,type:d}}}),r[l.arrows.from]={attr:r[l.arrows.from].attr,name:r[l.arrows.from].name,value:{from:{enabled:!0,type:r[l.arrows.from].value.from.type}}};else if("none"===u){var f;r[f=l.arrows.to?l.arrows.to:l.arrows.from]={attr:r[f].attr,name:r[f].name,value:""}}else{if("forward"!==u)throw wb('Invalid dir type "'+u+'"');l.arrows.to&&l.arrows.from||l.arrows.to?(c=r[l.arrows.to].value.to.type,d="",r[l.arrows.to]={attr:r[l.arrows.to].attr,name:r[l.arrows.to].name,value:{to:{enabled:!0,type:c},from:{enabled:!0,type:d}}}):l.arrows.from&&(c="arrow",d="",l.arrows.to=l.arrows.from,r[l.arrows.to]={attr:r[l.arrows.to].attr,name:r[l.arrows.to].name,value:{to:{enabled:!0,type:c},from:{enabled:!0,type:d}}}),r[l.arrows.to]={attr:r[l.arrows.to].attr,name:r[l.arrows.to].name,value:{to:{enabled:!0,type:r[l.arrows.to].value.to.type}}}}ff(r).call(r,l.dir,1)}if(Nf(s).call(s,"penwidth")){var p=[];for(e=r.length,t=0;t<e;t++)"width"!==r[t].name&&("penwidth"===r[t].name&&(r[t].name="width"),p.push(r[t]));r=p}for(e=r.length,t=0;t<e;t++)db(r[t].attr,r[t].name,r[t].value);return i}function wb(t){return new SyntaxError(t+', got "'+kb(ob,30)+'" (char '+ib+")")}function kb(t,e){return t.length<=e?t:t.substr(0,27)+"..."}function _b(t,e,i){for(var n=e.split("."),o=n.pop(),r=t,s=0;s<n.length;s++){var a=n[s];a in r||(r[a]={}),r=r[a]}return r[o]=i,t}function xb(t,e){var i={};for(var n in t)if(t.hasOwnProperty(n)){var o=e[n];lu(o)?Fu(o).call(o,(function(e){_b(i,e,t[n])})):_b(i,"string"==typeof o?o:n,t[n])}return i}function Eb(t){var e,i=Xm(t),n={nodes:[],edges:[],options:{}};i.nodes&&Fu(e=i.nodes).call(e,(function(t){var e={id:t.id,label:String(t.label||t.id)};lb(e,xb(t.attr,Gm)),e.image&&(e.shape="image"),n.nodes.push(e)}));if(i.edges){var o,r=function(t){var e={from:t.from,to:t.to};return lb(e,xb(t.attr,Km)),null==e.arrows&&"->"===t.type&&(e.arrows="to"),e};Fu(o=i.edges).call(o,(function(t){var e,i,o,s,a,h,l;(e=t.from instanceof Object?t.from.nodes:{id:t.from},i=t.to instanceof Object?t.to.nodes:{id:t.to},t.from instanceof Object&&t.from.edges)&&Fu(o=t.from.edges).call(o,(function(t){var e=r(t);n.edges.push(e)}));(a=i,h=function(e,i){var o=fb(n,e.id,i.id,t.type,t.attr),s=r(o);n.edges.push(s)},lu(s=e)?Fu(s).call(s,(function(t){lu(a)?Fu(a).call(a,(function(e){h(t,e)})):h(t,a)})):lu(a)?Fu(a).call(a,(function(t){h(s,t)})):h(s,a),t.to instanceof Object&&t.to.edges)&&Fu(l=t.to.edges).call(l,(function(t){var e=r(t);n.edges.push(e)}))}))}return i.attr&&(n.options=i.attr),n}var Ob=Object.freeze({__proto__:null,parseDOT:Xm,DOTToGraph:Eb});function Cb(t,e){var i,n={edges:{inheritColor:!1},nodes:{fixed:!1,parseColor:!1}};null!=e&&(null!=e.fixed&&(n.nodes.fixed=e.fixed),null!=e.parseColor&&(n.nodes.parseColor=e.parseColor),null!=e.inheritColor&&(n.edges.inheritColor=e.inheritColor));var o=t.edges,r=gu(o).call(o,(function(t){var e={from:t.source,id:t.id,to:t.target};return null!=t.attributes&&(e.attributes=t.attributes),null!=t.label&&(e.label=t.label),null!=t.attributes&&null!=t.attributes.title&&(e.title=t.attributes.title),"Directed"===t.type&&(e.arrows="to"),t.color&&!1===n.edges.inheritColor&&(e.color=t.color),e}));return{nodes:gu(i=t.nodes).call(i,(function(t){var e={id:t.id,fixed:n.nodes.fixed&&null!=t.x&&null!=t.y};return null!=t.attributes&&(e.attributes=t.attributes),null!=t.label&&(e.label=t.label),null!=t.size&&(e.size=t.size),null!=t.attributes&&null!=t.attributes.title&&(e.title=t.attributes.title),null!=t.title&&(e.title=t.title),null!=t.x&&(e.x=t.x),null!=t.y&&(e.y=t.y),null!=t.color&&(!0===n.nodes.parseColor?e.color=t.color:e.color={background:t.color,border:t.color,highlight:{background:t.color,border:t.color},hover:{background:t.color,border:t.color}}),e})),edges:r}}var Sb=Object.freeze({__proto__:null,parseGephi:Cb}),Tb=Object.freeze({__proto__:null,en:{addDescription:"Click in an empty space to place a new node.",addEdge:"Add Edge",addNode:"Add Node",back:"Back",close:"Close",createEdgeError:"Cannot link edges to a cluster.",del:"Delete selected",deleteClusterError:"Clusters cannot be deleted.",edgeDescription:"Click on a node and drag the edge to another node to connect them.",edit:"Edit",editClusterError:"Clusters cannot be edited.",editEdge:"Edit Edge",editEdgeDescription:"Click on the control points and drag them to a node to connect to it.",editNode:"Edit Node"},de:{addDescription:"Klicke auf eine freie Stelle, um einen neuen Knoten zu plazieren.",addEdge:"Kante hinzufgen",addNode:"Knoten hinzufgen",back:"Zurck",close:"Schlieen",createEdgeError:"Es ist nicht mglich, Kanten mit Clustern zu verbinden.",del:"Lsche Auswahl",deleteClusterError:"Cluster knnen nicht gelscht werden.",edgeDescription:"Klicke auf einen Knoten und ziehe die Kante zu einem anderen Knoten, um diese zu verbinden.",edit:"Editieren",editClusterError:"Cluster knnen nicht editiert werden.",editEdge:"Kante editieren",editEdgeDescription:"Klicke auf die Verbindungspunkte und ziehe diese auf einen Knoten, um sie zu verbinden.",editNode:"Knoten editieren"},es:{addDescription:"Haga clic en un lugar vaco para colocar un nuevo nodo.",addEdge:"Aadir arista",addNode:"Aadir nodo",back:"Atrs",close:"Cerrar",createEdgeError:"No se puede conectar una arista a un grupo.",del:"Eliminar seleccin",deleteClusterError:"No es posible eliminar grupos.",edgeDescription:"Haga clic en un nodo y arrastre la arista hacia otro nodo para conectarlos.",edit:"Editar",editClusterError:"No es posible editar grupos.",editEdge:"Editar arista",editEdgeDescription:"Haga clic en un punto de control y arrastrelo a un nodo para conectarlo.",editNode:"Editar nodo"},it:{addDescription:"Clicca per aggiungere un nuovo nodo",addEdge:"Aggiungi un vertice",addNode:"Aggiungi un nodo",back:"Indietro",close:"Chiudere",createEdgeError:"Non si possono collegare vertici ad un cluster",del:"Cancella la selezione",deleteClusterError:"I cluster non possono essere cancellati",edgeDescription:"Clicca su un nodo e trascinalo ad un altro nodo per connetterli.",edit:"Modifica",editClusterError:"I clusters non possono essere modificati.",editEdge:"Modifica il vertice",editEdgeDescription:"Clicca sui Punti di controllo e trascinali ad un nodo per connetterli.",editNode:"Modifica il nodo"},nl:{addDescription:"Klik op een leeg gebied om een nieuwe node te maken.",addEdge:"Link toevoegen",addNode:"Node toevoegen",back:"Terug",close:"Sluiten",createEdgeError:"Kan geen link maken naar een cluster.",del:"Selectie verwijderen",deleteClusterError:"Clusters kunnen niet worden verwijderd.",edgeDescription:"Klik op een node en sleep de link naar een andere node om ze te verbinden.",edit:"Wijzigen",editClusterError:"Clusters kunnen niet worden aangepast.",editEdge:"Link wijzigen",editEdgeDescription:"Klik op de verbindingspunten en sleep ze naar een node om daarmee te verbinden.",editNode:"Node wijzigen"},pt:{addDescription:"Clique em um espao em branco para adicionar um novo n",addEdge:"Adicionar aresta",addNode:"Adicionar n",back:"Voltar",close:"Fechar",createEdgeError:"No foi possvel linkar arestas a um cluster.",del:"Remover selecionado",deleteClusterError:"Clusters no puderam ser removidos.",edgeDescription:"Clique em um n e arraste a aresta at outro n para conect-los",edit:"Editar",editClusterError:"Clusters no puderam ser editados.",editEdge:"Editar aresta",editEdgeDescription:"Clique nos pontos de controle e os arraste para um n para conect-los",editNode:"Editar n"},ru:{addDescription:"   ,    .",addEdge:" ",addNode:" ",back:"",close:"",createEdgeError:"    .",del:" ",deleteClusterError:"    ",edgeDescription:"        ,   .",edit:"",editClusterError:"   .",editEdge:" ",editEdgeDescription:"        ,    .",editNode:" "},cn:{addDescription:"",addEdge:"",addNode:"",back:"",close:"",createEdgeError:"",del:"",deleteClusterError:"",edgeDescription:"",edit:"",editClusterError:"",editEdge:"",editEdgeDescription:"",editNode:""},uk:{addDescription:"K   ,    .",addEdge:" ",addNode:" ",back:"",close:"",createEdgeError:"  '   .",del:" ",deleteClusterError:"    .",edgeDescription:"        ,   '.",edit:"",editClusterError:"   .",editEdge:" ",editEdgeDescription:"        ,    .",editNode:" "},fr:{addDescription:"Cliquez dans un endroit vide pour placer un nud.",addEdge:"Ajouter un lien",addNode:"Ajouter un nud",back:"Retour",close:"Fermer",createEdgeError:"Impossible de crer un lien vers un cluster.",del:"Effacer la slection",deleteClusterError:"Les clusters ne peuvent pas tre effacs.",edgeDescription:"Cliquez sur un nud et glissez le lien vers un autre nud pour les connecter.",edit:"diter",editClusterError:"Les clusters ne peuvent pas tre dits.",editEdge:"diter le lien",editEdgeDescription:"Cliquez sur les points de contrle et glissez-les pour connecter un nud.",editNode:"diter le nud"},cs:{addDescription:"Kluknutm do przdnho prostoru mete pidat nov vrchol.",addEdge:"Pidat hranu",addNode:"Pidat vrchol",back:"Zpt",close:"Zavt",createEdgeError:"Nelze pipojit hranu ke shluku.",del:"Smazat vbr",deleteClusterError:"Nelze mazat shluky.",edgeDescription:"Petaenm z jednoho vrcholu do druhho mete spojit tyto vrcholy novou hranou.",edit:"Upravit",editClusterError:"Nelze upravovat shluky.",editEdge:"Upravit hranu",editEdgeDescription:"Petaenm kontrolnho vrcholu hrany ji mete pipojit k jinmu vrcholu.",editNode:"Upravit vrchol"}});var Mb=function(){function t(){Yd(this,t),this.NUM_ITERATIONS=4,this.image=new Image,this.canvas=document.createElement("canvas")}return Kd(t,[{key:"init",value:function(){if(!this.initialized()){this.src=this.image.src;var t=this.image.width,e=this.image.height;this.width=t,this.height=e;var i=Math.floor(e/2),n=Math.floor(e/4),o=Math.floor(e/8),r=Math.floor(e/16),s=Math.floor(t/2),a=Math.floor(t/4),h=Math.floor(t/8),l=Math.floor(t/16);this.canvas.width=3*a,this.canvas.height=i,this.coordinates=[[0,0,s,i],[s,0,a,n],[s,n,h,o],[5*h,n,l,r]],this._fillMipMap()}}},{key:"initialized",value:function(){return void 0!==this.coordinates}},{key:"_fillMipMap",value:function(){var t=this.canvas.getContext("2d"),e=this.coordinates[0];t.drawImage(this.image,e[0],e[1],e[2],e[3]);for(var i=1;i<this.NUM_ITERATIONS;i++){var n=this.coordinates[i-1],o=this.coordinates[i];t.drawImage(this.canvas,n[0],n[1],n[2],n[3],o[0],o[1],o[2],o[3])}}},{key:"drawImageAtPosition",value:function(t,e,i,n,o,r){if(this.initialized())if(e>2){e*=.5;for(var s=0;e>2&&s<this.NUM_ITERATIONS;)e*=.5,s+=1;s>=this.NUM_ITERATIONS&&(s=this.NUM_ITERATIONS-1);var a=this.coordinates[s];t.drawImage(this.canvas,a[0],a[1],a[2],a[3],i,n,o,r)}else t.drawImage(this.image,i,n,o,r)}}]),t}(),Pb=function(){function t(e){Yd(this,t),this.images={},this.imageBroken={},this.callback=e}return Kd(t,[{key:"_tryloadBrokenUrl",value:function(t,e,i){void 0!==t&&void 0!==i&&(void 0!==e?(i.image.onerror=function(){console.error("Could not load brokenImage:",e)},i.image.src=e):console.warn("No broken url image defined"))}},{key:"_redrawWithImage",value:function(t){this.callback&&this.callback(t)}},{key:"load",value:function(t,e){var i=this,n=this.images[t];if(n)return n;var o=new Mb;return this.images[t]=o,o.image.onload=function(){i._fixImageCoordinates(o.image),o.init(),i._redrawWithImage(o)},o.image.onerror=function(){console.error("Could not load image:",t),i._tryloadBrokenUrl(t,e,o)},o.image.src=t,o}},{key:"_fixImageCoordinates",value:function(t){0===t.width&&(document.body.appendChild(t),t.width=t.offsetWidth,t.height=t.offsetHeight,document.body.removeChild(t))}}]),t}(),Db={exports:{}},Ib=o((function(){if("function"==typeof ArrayBuffer){var t=new ArrayBuffer(8);Object.isExtensible(t)&&Object.defineProperty(t,"a",{value:8})}})),Bb=o,zb=Y,Nb=B,Fb=Ib,Ab=Object.isExtensible,jb=Bb((function(){Ab(1)}))||Fb?function(t){return!!zb(t)&&((!Fb||"ArrayBuffer"!=Nb(t))&&(!Ab||Ab(t)))}:Ab,Rb=!o((function(){return Object.isExtensible(Object.preventExtensions({}))})),Lb=_i,Hb=g,Wb=Ri,qb=Y,Vb=Wt,Ub=Ve.f,Yb=rh,Xb=hh,Gb=jb,Kb=Rb,$b=!1,Zb=Xt("meta"),Qb=0,Jb=function(t){Ub(t,Zb,{value:{objectID:"O"+Qb++,weakData:{}}})},tw=Db.exports={enable:function(){tw.enable=function(){},$b=!0;var t=Yb.f,e=Hb([].splice),i={};i[Zb]=1,t(i).length&&(Yb.f=function(i){for(var n=t(i),o=0,r=n.length;o<r;o++)if(n[o]===Zb){e(n,o,1);break}return n},Lb({target:"Object",stat:!0,forced:!0},{getOwnPropertyNames:Xb.f}))},fastKey:function(t,e){if(!qb(t))return"symbol"==typeof t?t:("string"==typeof t?"S":"P")+t;if(!Vb(t,Zb)){if(!Gb(t))return"F";if(!e)return"E";Jb(t)}return t[Zb].objectID},getWeakData:function(t,e){if(!Vb(t,Zb)){if(!Gb(t))return!0;if(!e)return!1;Jb(t)}return t[Zb].weakData},onFreeze:function(t){return Kb&&$b&&Gb(t)&&!Vb(t,Zb)&&Jb(t),t}};Wb[Zb]=!0;var ew=qe,iw=_,nw=$e,ow=bt,rw=qs,sw=Bi,aw=J,hw=ba,lw=ua,dw=As,cw=n.TypeError,uw=function(t,e){this.stopped=t,this.result=e},fw=uw.prototype,pw=function(t,e,i){var n,o,r,s,a,h,l,d=i&&i.that,c=!(!i||!i.AS_ENTRIES),u=!(!i||!i.IS_ITERATOR),f=!(!i||!i.INTERRUPTED),p=ew(e,d),v=function(t){return n&&dw(n,"normal",t),new uw(!0,t)},g=function(t){return c?(nw(t),f?p(t[0],t[1],v):p(t[0],t[1])):f?p(t,v):p(t)};if(u)n=t;else{if(!(o=lw(t)))throw cw(ow(t)+" is not iterable");if(rw(o)){for(r=0,s=sw(t);s>r;r++)if((a=g(t[r]))&&aw(fw,a))return a;return new uw(!1)}n=hw(t,o)}for(h=n.next;!(l=iw(h,n)).done;){try{a=g(l.value)}catch(t){dw(n,"throw",t)}if("object"==typeof a&&a&&aw(fw,a))return a}return new uw(!1)},vw=J,gw=n.TypeError,yw=function(t,e){if(vw(e,t))return t;throw gw("Incorrect invocation")},mw=_i,bw=n,ww=Db.exports,kw=o,_w=di,xw=pw,Ew=yw,Ow=y,Cw=Y,Sw=$r,Tw=Ve.f,Mw=Wh.forEach,Pw=b,Dw=Vo.set,Iw=Vo.getterFor,Bw=function(t,e,i){var n,o=-1!==t.indexOf("Map"),r=-1!==t.indexOf("Weak"),s=o?"set":"add",a=bw[t],h=a&&a.prototype,l={};if(Pw&&Ow(a)&&(r||h.forEach&&!kw((function(){(new a).entries().next()})))){var d=(n=e((function(e,i){Dw(Ew(e,d),{type:t,collection:new a}),null!=i&&xw(i,e[s],{that:e,AS_ENTRIES:o})}))).prototype,c=Iw(t);Mw(["add","clear","delete","forEach","get","has","set","keys","values","entries"],(function(t){var e="add"==t||"set"==t;!(t in h)||r&&"clear"==t||_w(d,t,(function(i,n){var o=c(this).collection;if(!e&&r&&!Cw(i))return"get"==t&&void 0;var s=o[t](0===i?0:i,n);return e?this:s}))})),r||Tw(d,"size",{configurable:!0,get:function(){return c(this).collection.size}})}else n=i.getConstructor(e,t,o,s),ww.enable();return Sw(n,t,!1,!0),l[t]=n,mw({global:!0,forced:!0},l),r||i.setStrong(n,t,o),n},zw=Ir,Nw=function(t,e,i){for(var n in e)i&&i.unsafe&&t[n]?t[n]=e[n]:zw(t,n,e[n],i);return t},Fw=Q,Aw=Ve,jw=b,Rw=oe("species"),Lw=Ve.f,Hw=wr,Ww=Nw,qw=qe,Vw=yw,Uw=pw,Yw=Cs,Xw=function(t){var e=Fw(t),i=Aw.f;jw&&e&&!e[Rw]&&i(e,Rw,{configurable:!0,get:function(){return this}})},Gw=b,Kw=Db.exports.fastKey,$w=Vo.set,Zw=Vo.getterFor,Qw={getConstructor:function(t,e,i,n){var o=t((function(t,o){Vw(t,r),$w(t,{type:e,index:Hw(null),first:void 0,last:void 0,size:0}),Gw||(t.size=0),null!=o&&Uw(o,t[n],{that:t,AS_ENTRIES:i})})),r=o.prototype,s=Zw(e),a=function(t,e,i){var n,o,r=s(t),a=h(t,e);return a?a.value=i:(r.last=a={index:o=Kw(e,!0),key:e,value:i,previous:n=r.last,next:void 0,removed:!1},r.first||(r.first=a),n&&(n.next=a),Gw?r.size++:t.size++,"F"!==o&&(r.index[o]=a)),t},h=function(t,e){var i,n=s(t),o=Kw(e);if("F"!==o)return n.index[o];for(i=n.first;i;i=i.next)if(i.key==e)return i};return Ww(r,{clear:function(){for(var t=s(this),e=t.index,i=t.first;i;)i.removed=!0,i.previous&&(i.previous=i.previous.next=void 0),delete e[i.index],i=i.next;t.first=t.last=void 0,Gw?t.size=0:this.size=0},delete:function(t){var e=this,i=s(e),n=h(e,t);if(n){var o=n.next,r=n.previous;delete i.index[n.index],n.removed=!0,r&&(r.next=o),o&&(o.previous=r),i.first==n&&(i.first=o),i.last==n&&(i.last=r),Gw?i.size--:e.size--}return!!n},forEach:function(t){for(var e,i=s(this),n=qw(t,arguments.length>1?arguments[1]:void 0);e=e?e.next:i.first;)for(n(e.value,e.key,this);e&&e.removed;)e=e.previous},has:function(t){return!!h(this,t)}}),Ww(r,i?{get:function(t){var e=h(this,t);return e&&e.value},set:function(t,e){return a(this,0===t?0:t,e)}}:{add:function(t){return a(this,t=0===t?0:t,t)}}),Gw&&Lw(r,"size",{get:function(){return s(this).size}}),o},setStrong:function(t,e,i){var n=e+" Iterator",o=Zw(e),r=Zw(n);Yw(t,e,(function(t,e){$w(this,{type:n,target:t,state:o(t),kind:e,last:void 0})}),(function(){for(var t=r(this),e=t.kind,i=t.last;i&&i.removed;)i=i.previous;return t.target&&(t.last=i=i?i.next:t.state.first)?"keys"==e?{value:i.key,done:!1}:"values"==e?{value:i.value,done:!1}:{value:[i.key,i.value],done:!1}:(t.target=void 0,{value:void 0,done:!0})}),i?"entries":"values",!i,!0),Xw(e)}};Bw("Map",(function(t){return function(){return t(this,arguments.length?arguments[0]:void 0)}}),Qw);var Jw=X.Map,tk=function(){function t(){Yd(this,t),this.clear(),this._defaultIndex=0,this._groupIndex=0,this._defaultGroups=[{border:"#2B7CE9",background:"#97C2FC",highlight:{border:"#2B7CE9",background:"#D2E5FF"},hover:{border:"#2B7CE9",background:"#D2E5FF"}},{border:"#FFA500",background:"#FFFF00",highlight:{border:"#FFA500",background:"#FFFFA3"},hover:{border:"#FFA500",background:"#FFFFA3"}},{border:"#FA0A10",background:"#FB7E81",highlight:{border:"#FA0A10",background:"#FFAFB1"},hover:{border:"#FA0A10",background:"#FFAFB1"}},{border:"#41A906",background:"#7BE141",highlight:{border:"#41A906",background:"#A1EC76"},hover:{border:"#41A906",background:"#A1EC76"}},{border:"#E129F0",background:"#EB7DF4",highlight:{border:"#E129F0",background:"#F0B3F5"},hover:{border:"#E129F0",background:"#F0B3F5"}},{border:"#7C29F0",background:"#AD85E4",highlight:{border:"#7C29F0",background:"#D3BDF0"},hover:{border:"#7C29F0",background:"#D3BDF0"}},{border:"#C37F00",background:"#FFA807",highlight:{border:"#C37F00",background:"#FFCA66"},hover:{border:"#C37F00",background:"#FFCA66"}},{border:"#4220FB",background:"#6E6EFD",highlight:{border:"#4220FB",background:"#9B9BFD"},hover:{border:"#4220FB",background:"#9B9BFD"}},{border:"#FD5A77",background:"#FFC0CB",highlight:{border:"#FD5A77",background:"#FFD1D9"},hover:{border:"#FD5A77",background:"#FFD1D9"}},{border:"#4AD63A",background:"#C2FABC",highlight:{border:"#4AD63A",background:"#E6FFE3"},hover:{border:"#4AD63A",background:"#E6FFE3"}},{border:"#990000",background:"#EE0000",highlight:{border:"#BB0000",background:"#FF3333"},hover:{border:"#BB0000",background:"#FF3333"}},{border:"#FF6000",background:"#FF6000",highlight:{border:"#FF6000",background:"#FF6000"},hover:{border:"#FF6000",background:"#FF6000"}},{border:"#97C2FC",background:"#2B7CE9",highlight:{border:"#D2E5FF",background:"#2B7CE9"},hover:{border:"#D2E5FF",background:"#2B7CE9"}},{border:"#399605",background:"#255C03",highlight:{border:"#399605",background:"#255C03"},hover:{border:"#399605",background:"#255C03"}},{border:"#B70054",background:"#FF007E",highlight:{border:"#B70054",background:"#FF007E"},hover:{border:"#B70054",background:"#FF007E"}},{border:"#AD85E4",background:"#7C29F0",highlight:{border:"#D3BDF0",background:"#7C29F0"},hover:{border:"#D3BDF0",background:"#7C29F0"}},{border:"#4557FA",background:"#000EA1",highlight:{border:"#6E6EFD",background:"#000EA1"},hover:{border:"#6E6EFD",background:"#000EA1"}},{border:"#FFC0CB",background:"#FD5A77",highlight:{border:"#FFD1D9",background:"#FD5A77"},hover:{border:"#FFD1D9",background:"#FD5A77"}},{border:"#C2FABC",background:"#74D66A",highlight:{border:"#E6FFE3",background:"#74D66A"},hover:{border:"#E6FFE3",background:"#74D66A"}},{border:"#EE0000",background:"#990000",highlight:{border:"#FF3333",background:"#BB0000"},hover:{border:"#FF3333",background:"#BB0000"}}],this.options={},this.defaultOptions={useDefaultGroups:!0},un(this.options,this.defaultOptions)}return Kd(t,[{key:"setOptions",value:function(t){var e=["useDefaultGroups"];if(void 0!==t)for(var i in t)if(Object.prototype.hasOwnProperty.call(t,i)&&-1===Fp(e).call(e,i)){var n=t[i];this.add(i,n)}}},{key:"clear",value:function(){this._groups=new Jw,this._groupNames=[]}},{key:"get",value:function(t){var e=!(arguments.length>1&&void 0!==arguments[1])||arguments[1],i=this._groups.get(t);if(void 0===i&&e)if(!1===this.options.useDefaultGroups&&this._groupNames.length>0){var n=this._groupIndex%this._groupNames.length;++this._groupIndex,(i={}).color=this._groups.get(this._groupNames[n]),this._groups.set(t,i)}else{var o=this._defaultIndex%this._defaultGroups.length;this._defaultIndex++,(i={}).color=this._defaultGroups[o],this._groups.set(t,i)}return i}},{key:"add",value:function(t,e){return this._groups.has(t)||this._groupNames.push(t),this._groups.set(t,e),e}}]),t}();_i({target:"Number",stat:!0},{isNaN:function(t){return t!=t}});var ek=X.Number.isNaN,ik=n.isFinite,nk=Number.isFinite||function(t){return"number"==typeof t&&ik(t)};_i({target:"Number",stat:!0},{isFinite:nk});var ok=X.Number.isFinite,rk=Wh.some;_i({target:"Array",proto:!0,forced:!Cu("some")},{some:function(t){return rk(this,t,arguments.length>1?arguments[1]:void 0)}});var sk=Tn("Array").some,ak=J,hk=sk,lk=Array.prototype,dk=function(t){var e=t.some;return t===lk||ak(lk,t)&&e===lk.some?hk:e},ck=dk,uk=na,fk=bt,pk=n.TypeError,vk=_i,gk=d,yk=On,mk=function(t){if(uk(t))return t;throw pk(fk(t)+" is not a constructor")},bk=$e,wk=Y,kk=wr,_k=o,xk=Q("Reflect","construct"),Ek=Object.prototype,Ok=[].push,Ck=_k((function(){function t(){}return!(xk((function(){}),[],t)instanceof t)})),Sk=!_k((function(){xk((function(){}))})),Tk=Ck||Sk;vk({target:"Reflect",stat:!0,forced:Tk,sham:Tk},{construct:function(t,e){mk(t),bk(e);var i=arguments.length<3?t:mk(arguments[2]);if(Sk&&!Ck)return xk(t,e,i);if(t==i){switch(e.length){case 0:return new t;case 1:return new t(e[0]);case 2:return new t(e[0],e[1]);case 3:return new t(e[0],e[1],e[2]);case 4:return new t(e[0],e[1],e[2],e[3])}var n=[null];return gk(Ok,n,e),new(gk(yk,t,n))}var o=i.prototype,r=kk(wk(o)?o:Ek),s=gk(t,r,e);return wk(s)?s:r}});var Mk=X.Reflect.construct;function Pk(t){if(void 0===t)throw new ReferenceError("this hasn't been initialised - super() hasn't been called");return t}var Dk=Gp;_i({target:"Object",stat:!0},{setPrototypeOf:cs});var Ik=X.Object.setPrototypeOf;function Bk(t,e){return Bk=Ik||function(t,e){return t.__proto__=e,t},Bk(t,e)}function zk(t,e){if("function"!=typeof e&&null!==e)throw new TypeError("Super expression must either be null or a function");t.prototype=Dk(e&&e.prototype,{constructor:{value:t,writable:!0,configurable:!0}}),Xd(t,"prototype",{writable:!1}),e&&Bk(t,e)}function Nk(t,e){if(e&&("object"===Qc(e)||"function"==typeof e))return e;if(void 0!==e)throw new TypeError("Derived constructors may only return object or undefined");return Pk(t)}var Fk=Rf;function Ak(t){return Ak=Ik?Fk:function(t){return t.__proto__||Fk(t)},Ak(t)}var jk={exports:{}};!function(t){var e=function(t){var e,i=Object.prototype,n=i.hasOwnProperty,o="function"==typeof Symbol?Symbol:{},r=o.iterator||"@@iterator",s=o.asyncIterator||"@@asyncIterator",a=o.toStringTag||"@@toStringTag";function h(t,e,i){return Object.defineProperty(t,e,{value:i,enumerable:!0,configurable:!0,writable:!0}),t[e]}try{h({},"")}catch(t){h=function(t,e,i){return t[e]=i}}function l(t,e,i,n){var o=e&&e.prototype instanceof g?e:g,r=Object.create(o.prototype),s=new T(n||[]);return r._invoke=function(t,e,i){var n=c;return function(o,r){if(n===f)throw new Error("Generator is already running");if(n===p){if("throw"===o)throw r;return P()}for(i.method=o,i.arg=r;;){var s=i.delegate;if(s){var a=O(s,i);if(a){if(a===v)continue;return a}}if("next"===i.method)i.sent=i._sent=i.arg;else if("throw"===i.method){if(n===c)throw n=p,i.arg;i.dispatchException(i.arg)}else"return"===i.method&&i.abrupt("return",i.arg);n=f;var h=d(t,e,i);if("normal"===h.type){if(n=i.done?p:u,h.arg===v)continue;return{value:h.arg,done:i.done}}"throw"===h.type&&(n=p,i.method="throw",i.arg=h.arg)}}}(t,i,s),r}function d(t,e,i){try{return{type:"normal",arg:t.call(e,i)}}catch(t){return{type:"throw",arg:t}}}t.wrap=l;var c="suspendedStart",u="suspendedYield",f="executing",p="completed",v={};function g(){}function y(){}function m(){}var b={};h(b,r,(function(){return this}));var w=Object.getPrototypeOf,k=w&&w(w(M([])));k&&k!==i&&n.call(k,r)&&(b=k);var _=m.prototype=g.prototype=Object.create(b);function x(t){["next","throw","return"].forEach((function(e){h(t,e,(function(t){return this._invoke(e,t)}))}))}function E(t,e){function i(o,r,s,a){var h=d(t[o],t,r);if("throw"!==h.type){var l=h.arg,c=l.value;return c&&"object"==typeof c&&n.call(c,"__await")?e.resolve(c.__await).then((function(t){i("next",t,s,a)}),(function(t){i("throw",t,s,a)})):e.resolve(c).then((function(t){l.value=t,s(l)}),(function(t){return i("throw",t,s,a)}))}a(h.arg)}var o;this._invoke=function(t,n){function r(){return new e((function(e,o){i(t,n,e,o)}))}return o=o?o.then(r,r):r()}}function O(t,i){var n=t.iterator[i.method];if(n===e){if(i.delegate=null,"throw"===i.method){if(t.iterator.return&&(i.method="return",i.arg=e,O(t,i),"throw"===i.method))return v;i.method="throw",i.arg=new TypeError("The iterator does not provide a 'throw' method")}return v}var o=d(n,t.iterator,i.arg);if("throw"===o.type)return i.method="throw",i.arg=o.arg,i.delegate=null,v;var r=o.arg;return r?r.done?(i[t.resultName]=r.value,i.next=t.nextLoc,"return"!==i.method&&(i.method="next",i.arg=e),i.delegate=null,v):r:(i.method="throw",i.arg=new TypeError("iterator result is not an object"),i.delegate=null,v)}function C(t){var e={tryLoc:t[0]};1 in t&&(e.catchLoc=t[1]),2 in t&&(e.finallyLoc=t[2],e.afterLoc=t[3]),this.tryEntries.push(e)}function S(t){var e=t.completion||{};e.type="normal",delete e.arg,t.completion=e}function T(t){this.tryEntries=[{tryLoc:"root"}],t.forEach(C,this),this.reset(!0)}function M(t){if(t){var i=t[r];if(i)return i.call(t);if("function"==typeof t.next)return t;if(!isNaN(t.length)){var o=-1,s=function i(){for(;++o<t.length;)if(n.call(t,o))return i.value=t[o],i.done=!1,i;return i.value=e,i.done=!0,i};return s.next=s}}return{next:P}}function P(){return{value:e,done:!0}}return y.prototype=m,h(_,"constructor",m),h(m,"constructor",y),y.displayName=h(m,a,"GeneratorFunction"),t.isGeneratorFunction=function(t){var e="function"==typeof t&&t.constructor;return!!e&&(e===y||"GeneratorFunction"===(e.displayName||e.name))},t.mark=function(t){return Object.setPrototypeOf?Object.setPrototypeOf(t,m):(t.__proto__=m,h(t,a,"GeneratorFunction")),t.prototype=Object.create(_),t},t.awrap=function(t){return{__await:t}},x(E.prototype),h(E.prototype,s,(function(){return this})),t.AsyncIterator=E,t.async=function(e,i,n,o,r){void 0===r&&(r=Promise);var s=new E(l(e,i,n,o),r);return t.isGeneratorFunction(i)?s:s.next().then((function(t){return t.done?t.value:s.next()}))},x(_),h(_,a,"Generator"),h(_,r,(function(){return this})),h(_,"toString",(function(){return"[object Generator]"})),t.keys=function(t){var e=[];for(var i in t)e.push(i);return e.reverse(),function i(){for(;e.length;){var n=e.pop();if(n in t)return i.value=n,i.done=!1,i}return i.done=!0,i}},t.values=M,T.prototype={constructor:T,reset:function(t){if(this.prev=0,this.next=0,this.sent=this._sent=e,this.done=!1,this.delegate=null,this.method="next",this.arg=e,this.tryEntries.forEach(S),!t)for(var i in this)"t"===i.charAt(0)&&n.call(this,i)&&!isNaN(+i.slice(1))&&(this[i]=e)},stop:function(){this.done=!0;var t=this.tryEntries[0].completion;if("throw"===t.type)throw t.arg;return this.rval},dispatchException:function(t){if(this.done)throw t;var i=this;function o(n,o){return a.type="throw",a.arg=t,i.next=n,o&&(i.method="next",i.arg=e),!!o}for(var r=this.tryEntries.length-1;r>=0;--r){var s=this.tryEntries[r],a=s.completion;if("root"===s.tryLoc)return o("end");if(s.tryLoc<=this.prev){var h=n.call(s,"catchLoc"),l=n.call(s,"finallyLoc");if(h&&l){if(this.prev<s.catchLoc)return o(s.catchLoc,!0);if(this.prev<s.finallyLoc)return o(s.finallyLoc)}else if(h){if(this.prev<s.catchLoc)return o(s.catchLoc,!0)}else{if(!l)throw new Error("try statement without catch or finally");if(this.prev<s.finallyLoc)return o(s.finallyLoc)}}}},abrupt:function(t,e){for(var i=this.tryEntries.length-1;i>=0;--i){var o=this.tryEntries[i];if(o.tryLoc<=this.prev&&n.call(o,"finallyLoc")&&this.prev<o.finallyLoc){var r=o;break}}r&&("break"===t||"continue"===t)&&r.tryLoc<=e&&e<=r.finallyLoc&&(r=null);var s=r?r.completion:{};return s.type=t,s.arg=e,r?(this.method="next",this.next=r.finallyLoc,v):this.complete(s)},complete:function(t,e){if("throw"===t.type)throw t.arg;return"break"===t.type||"continue"===t.type?this.next=t.arg:"return"===t.type?(this.rval=this.arg=t.arg,this.method="return",this.next="end"):"normal"===t.type&&e&&(this.next=e),v},finish:function(t){for(var e=this.tryEntries.length-1;e>=0;--e){var i=this.tryEntries[e];if(i.finallyLoc===t)return this.complete(i.completion,i.afterLoc),S(i),v}},catch:function(t){for(var e=this.tryEntries.length-1;e>=0;--e){var i=this.tryEntries[e];if(i.tryLoc===t){var n=i.completion;if("throw"===n.type){var o=n.arg;S(i)}return o}}throw new Error("illegal catch attempt")},delegateYield:function(t,i,n){return this.delegate={iterator:M(t),resultName:i,nextLoc:n},"next"===this.method&&(this.arg=e),v}},t}(t.exports);try{regeneratorRuntime=e}catch(t){"object"==typeof globalThis?globalThis.regeneratorRuntime=e:Function("r","regeneratorRuntime = r")(e)}}(jk);var Rk=jk.exports,Lk=xt,Hk=Rt,Wk=R,qk=Bi,Vk=n.TypeError,Uk=function(t){return function(e,i,n,o){Lk(i);var r=Hk(e),s=Wk(r),a=qk(r),h=t?a-1:0,l=t?-1:1;if(n<2)for(;;){if(h in s){o=s[h],h+=l;break}if(h+=l,t?h<0:a<=h)throw Vk("Reduce of empty array with no initial value")}for(;t?h>=0:a>h;h+=l)h in s&&(o=i(o,s[h],h,r));return o}},Yk={left:Uk(!1),right:Uk(!0)},Xk="process"==B(n.process),Gk=Yk.left,Kk=at,$k=Xk;_i({target:"Array",proto:!0,forced:!Cu("reduce")||!$k&&Kk>79&&Kk<83},{reduce:function(t){var e=arguments.length;return Gk(this,t,e,e>1?arguments[1]:void 0)}});var Zk=Tn("Array").reduce,Qk=J,Jk=Zk,t_=Array.prototype,e_=function(t){var e=t.reduce;return t===t_||Qk(t_,t)&&e===t_.reduce?Jk:e},i_=e_,n_=oh,o_=Bi,r_=qe,s_=n.TypeError,a_=function(t,e,i,n,o,r,s,a){for(var h,l,d=o,c=0,u=!!s&&r_(s,a);c<n;){if(c in i){if(h=u?u(i[c],c,e):i[c],r>0&&n_(h))l=o_(h),d=a_(t,e,h,l,d,r-1)-1;else{if(d>=9007199254740991)throw s_("Exceed the acceptable array length");t[d]=h}d++}c++}return d},h_=a_,l_=xt,d_=Rt,c_=Bi,u_=zh;_i({target:"Array",proto:!0},{flatMap:function(t){var e,i=d_(this),n=c_(i);return l_(t),(e=u_(i,0)).length=h_(e,i,i,n,0,1,t,arguments.length>1?arguments[1]:void 0),e}});var f_=Tn("Array").flatMap,p_=J,v_=f_,g_=Array.prototype,y_=function(t){var e=t.flatMap;return t===g_||p_(g_,t)&&e===g_.flatMap?v_:e},m_=y_;Bw("Set",(function(t){return function(){return t(this,arguments.length?arguments[0]:void 0)}}),Qw);var b_=X.Set,w_=$c,k_=ba,__=ph,x_=Math.floor,E_=function(t,e){var i=t.length,n=x_(i/2);return i<8?O_(t,e):C_(t,E_(__(t,0,n),e),E_(__(t,n),e),e)},O_=function(t,e){for(var i,n,o=t.length,r=1;r<o;){for(n=r,i=t[r];n&&e(t[n-1],i)>0;)t[n]=t[--n];n!==r++&&(t[n]=i)}return t},C_=function(t,e,i,n){for(var o=e.length,r=i.length,s=0,a=0;s<o||a<r;)t[s+a]=s<o&&a<r?n(e[s],i[a])<=0?e[s++]:i[a++]:s<o?e[s++]:i[a++];return t},S_=E_,T_=tt.match(/firefox\/(\d+)/i),M_=!!T_&&+T_[1],P_=/MSIE|Trident/.test(tt),D_=tt.match(/AppleWebKit\/(\d+)\./),I_=!!D_&&+D_[1],B_=_i,z_=g,N_=xt,F_=Rt,A_=Bi,j_=eo,R_=o,L_=S_,H_=Cu,W_=M_,q_=P_,V_=at,U_=I_,Y_=[],X_=z_(Y_.sort),G_=z_(Y_.push),K_=R_((function(){Y_.sort(void 0)})),$_=R_((function(){Y_.sort(null)})),Z_=H_("sort"),Q_=!R_((function(){if(V_)return V_<70;if(!(W_&&W_>3)){if(q_)return!0;if(U_)return U_<603;var t,e,i,n,o="";for(t=65;t<76;t++){switch(e=String.fromCharCode(t),t){case 66:case 69:case 70:case 72:i=3;break;case 68:case 71:i=4;break;default:i=2}for(n=0;n<47;n++)Y_.push({k:e+n,v:i})}for(Y_.sort((function(t,e){return e.v-t.v})),n=0;n<Y_.length;n++)e=Y_[n].k.charAt(0),o.charAt(o.length-1)!==e&&(o+=e);return"DGBEFHACIJK"!==o}}));B_({target:"Array",proto:!0,forced:K_||!$_||!Z_||!Q_},{sort:function(t){void 0!==t&&N_(t);var e=F_(this);if(Q_)return void 0===t?X_(e):X_(e,t);var i,n,o=[],r=A_(e);for(n=0;n<r;n++)n in e&&G_(o,e[n]);for(L_(o,function(t){return function(e,i){return void 0===i?-1:void 0===e?1:void 0!==t?+t(e,i)||0:j_(e)>j_(i)?1:-1}}(t)),i=o.length,n=0;n<i;)e[n]=o[n++];for(;n<r;)delete e[n++];return e}});var J_,tx=Tn("Array").sort,ex=J,ix=tx,nx=Array.prototype,ox=function(t){var e=t.sort;return t===nx||ex(nx,t)&&e===nx.sort?ix:e},rx=ox,sx=Tn("Array").keys,ax=Qn,hx=Wt,lx=J,dx=sx,cx=Array.prototype,ux={DOMTokenList:!0,NodeList:!0},fx=function(t){var e=t.keys;return t===cx||lx(cx,t)&&e===cx.keys||hx(ux,ax(t))?dx:e},px=Tn("Array").values,vx=Qn,gx=Wt,yx=J,mx=px,bx=Array.prototype,wx={DOMTokenList:!0,NodeList:!0},kx=function(t){var e=t.values;return t===bx||yx(bx,t)&&e===bx.values||gx(wx,vx(t))?mx:e},_x=Tn("Array").entries,xx=Qn,Ex=Wt,Ox=J,Cx=_x,Sx=Array.prototype,Tx={DOMTokenList:!0,NodeList:!0},Mx=function(t){var e=t.entries;return t===Sx||Ox(Sx,t)&&e===Sx.entries||Ex(Tx,xx(t))?Cx:e},Px=new Uint8Array(16);function Dx(){if(!J_&&!(J_="undefined"!=typeof crypto&&crypto.getRandomValues&&crypto.getRandomValues.bind(crypto)||"undefined"!=typeof msCrypto&&"function"==typeof msCrypto.getRandomValues&&msCrypto.getRandomValues.bind(msCrypto)))throw new Error("crypto.getRandomValues() not supported. See https://github.com/uuidjs/uuid#getrandomvalues-not-supported");return J_(Px)}var Ix=/^(?:[0-9a-f]{8}-[0-9a-f]{4}-[1-5][0-9a-f]{3}-[89ab][0-9a-f]{3}-[0-9a-f]{12}|00000000-0000-0000-0000-000000000000)$/i;function Bx(t){return"string"==typeof t&&Ix.test(t)}for(var zx,Nx=[],Fx=0;Fx<256;++Fx)Nx.push((Fx+256).toString(16).substr(1));function Ax(t,e,i){var n=(t=t||{}).random||(t.rng||Dx)();if(n[6]=15&n[6]|64,n[8]=63&n[8]|128,e){i=i||0;for(var o=0;o<16;++o)e[i+o]=n[o];return e}return function(t){var e=arguments.length>1&&void 0!==arguments[1]?arguments[1]:0,i=(Nx[t[e+0]]+Nx[t[e+1]]+Nx[t[e+2]]+Nx[t[e+3]]+"-"+Nx[t[e+4]]+Nx[t[e+5]]+"-"+Nx[t[e+6]]+Nx[t[e+7]]+"-"+Nx[t[e+8]]+Nx[t[e+9]]+"-"+Nx[t[e+10]]+Nx[t[e+11]]+Nx[t[e+12]]+Nx[t[e+13]]+Nx[t[e+14]]+Nx[t[e+15]]).toLowerCase();if(!Bx(i))throw TypeError("Stringified UUID is invalid");return i}(n)}function jx(t,e){var i=bu(t);if(hd){var n=hd(t);e&&(n=Xf(n).call(n,(function(e){return bd(t,e).enumerable}))),i.push.apply(i,n)}return i}function Rx(t){for(var e=1;e<arguments.length;e++){var i,n,o=null!=arguments[e]?arguments[e]:{};e%2?Fu(i=jx(Object(o),!0)).call(i,(function(e){$d(t,e,o[e])})):Pd?Ad(t,Pd(o)):Fu(n=jx(Object(o))).call(n,(function(e){Ud(t,e,bd(o,e))}))}return t}function Lx(t){var e=function(){if("undefined"==typeof Reflect||!Mk)return!1;if(Mk.sham)return!1;if("function"==typeof Proxy)return!0;try{return Boolean.prototype.valueOf.call(Mk(Boolean,[],(function(){}))),!0}catch(t){return!1}}();return function(){var i,n=Ak(t);if(e){var o=Ak(this).constructor;i=Mk(n,arguments,o)}else i=n.apply(this,arguments);return Nk(this,i)}}function Hx(t,e){var i=void 0!==tu&&ih(t)||t["@@iterator"];if(!i){if(lu(t)||(i=function(t,e){var i;if(!t)return;if("string"==typeof t)return Wx(t,e);var n=au(i=Object.prototype.toString.call(t)).call(i,8,-1);"Object"===n&&t.constructor&&(n=t.constructor.name);if("Map"===n||"Set"===n)return ja(t);if("Arguments"===n||/^(?:Ui|I)nt(?:8|16|32)(?:Clamped)?Array$/.test(n))return Wx(t,e)}(t))||e&&t&&"number"==typeof t.length){i&&(t=i);var n=0,o=function(){};return{s:o,n:function(){return n>=t.length?{done:!0}:{done:!1,value:t[n++]}},e:function(t){throw t},f:o}}throw new TypeError("Invalid attempt to iterate non-iterable instance.\nIn order to be iterable, non-array objects must have a [Symbol.iterator]() method.")}var r,s=!0,a=!1;return{s:function(){i=i.call(t)},n:function(){var t=i.next();return s=t.done,t},e:function(t){a=!0,r=t},f:function(){try{s||null==i.return||i.return()}finally{if(a)throw r}}}}function Wx(t,e){(null==e||e>t.length)&&(e=t.length);for(var i=0,n=new Array(e);i<e;i++)n[i]=t[i];return n}var qx=function(){function t(e,i,n){var o,r,s;Yd(this,t),$d(this,"_source",void 0),$d(this,"_transformers",void 0),$d(this,"_target",void 0),$d(this,"_listeners",{add:zn(o=this._add).call(o,this),remove:zn(r=this._remove).call(r,this),update:zn(s=this._update).call(s,this)}),this._source=e,this._transformers=i,this._target=n}return Kd(t,[{key:"all",value:function(){return this._target.update(this._transformItems(this._source.get())),this}},{key:"start",value:function(){return this._source.on("add",this._listeners.add),this._source.on("remove",this._listeners.remove),this._source.on("update",this._listeners.update),this}},{key:"stop",value:function(){return this._source.off("add",this._listeners.add),this._source.off("remove",this._listeners.remove),this._source.off("update",this._listeners.update),this}},{key:"_transformItems",value:function(t){var e;return i_(e=this._transformers).call(e,(function(t,e){return e(t)}),t)}},{key:"_add",value:function(t,e){null!=e&&this._target.add(this._transformItems(this._source.get(e.items)))}},{key:"_update",value:function(t,e){null!=e&&this._target.update(this._transformItems(this._source.get(e.items)))}},{key:"_remove",value:function(t,e){null!=e&&this._target.remove(this._transformItems(e.oldData))}}]),t}(),Vx=function(){function t(e){Yd(this,t),$d(this,"_source",void 0),$d(this,"_transformers",[]),this._source=e}return Kd(t,[{key:"filter",value:function(t){return this._transformers.push((function(e){return Xf(e).call(e,t)})),this}},{key:"map",value:function(t){return this._transformers.push((function(e){return gu(e).call(e,t)})),this}},{key:"flatMap",value:function(t){return this._transformers.push((function(e){return m_(e).call(e,t)})),this}},{key:"to",value:function(t){return new qx(this._source,this._transformers,t)}}]),t}();function Ux(t){return"string"==typeof t||"number"==typeof t}var Yx=function(){function t(e){Yd(this,t),$d(this,"delay",void 0),$d(this,"max",void 0),$d(this,"_queue",[]),$d(this,"_timeout",null),$d(this,"_extended",null),this.delay=null,this.max=1/0,this.setOptions(e)}return Kd(t,[{key:"setOptions",value:function(t){t&&void 0!==t.delay&&(this.delay=t.delay),t&&void 0!==t.max&&(this.max=t.max),this._flushIfNeeded()}},{key:"destroy",value:function(){if(this.flush(),this._extended){for(var t=this._extended.object,e=this._extended.methods,i=0;i<e.length;i++){var n=e[i];n.original?t[n.name]=n.original:delete t[n.name]}this._extended=null}}},{key:"replace",value:function(t,e){var i=this,n=t[e];if(!n)throw new Error("Method "+e+" undefined");t[e]=function(){for(var t=arguments.length,e=new Array(t),o=0;o<t;o++)e[o]=arguments[o];i.queue({args:e,fn:n,context:this})}}},{key:"queue",value:function(t){"function"==typeof t?this._queue.push({fn:t}):this._queue.push(t),this._flushIfNeeded()}},{key:"_flushIfNeeded",value:function(){var t=this;this._queue.length>this.max&&this.flush(),null!=this._timeout&&(clearTimeout(this._timeout),this._timeout=null),this.queue.length>0&&"number"==typeof this.delay&&(this._timeout=Sv((function(){t.flush()}),this.delay))}},{key:"flush",value:function(){var t,e;Fu(t=ff(e=this._queue).call(e,0)).call(t,(function(t){t.fn.apply(t.context||t.fn,t.args||[])}))}}],[{key:"extend",value:function(e,i){var n=new t(i);if(void 0!==e.flush)throw new Error("Target object already has a property flush");e.flush=function(){n.flush()};var o=[{name:"flush",original:void 0}];if(i&&i.replace)for(var r=0;r<i.replace.length;r++){var s=i.replace[r];o.push({name:s,original:e[s]}),n.replace(e,s)}return n._extended={object:e,methods:o},n}}]),t}(),Xx=function(){function t(){Yd(this,t),$d(this,"_subscribers",{"*":[],add:[],remove:[],update:[]}),$d(this,"subscribe",t.prototype.on),$d(this,"unsubscribe",t.prototype.off)}return Kd(t,[{key:"_trigger",value:function(t,e,i){var n,o;if("*"===t)throw new Error("Cannot trigger event *");Fu(n=su(o=[]).call(o,Jc(this._subscribers[t]),Jc(this._subscribers["*"]))).call(n,(function(n){n(t,e,null!=i?i:null)}))}},{key:"on",value:function(t,e){"function"==typeof e&&this._subscribers[t].push(e)}},{key:"off",value:function(t,e){var i;this._subscribers[t]=Xf(i=this._subscribers[t]).call(i,(function(t){return t!==e}))}}]),t}();zx=w_;var Gx=function(){function t(e){Yd(this,t),$d(this,"_pairs",void 0),this._pairs=e}return Kd(t,[{key:zx,value:Rk.mark((function t(){var e,i,n,o,r;return Rk.wrap((function(t){for(;;)switch(t.prev=t.next){case 0:e=Hx(this._pairs),t.prev=1,e.s();case 3:if((i=e.n()).done){t.next=9;break}return n=Kc(i.value,2),o=n[0],r=n[1],t.next=7,[o,r];case 7:t.next=3;break;case 9:t.next=14;break;case 11:t.prev=11,t.t0=t.catch(1),e.e(t.t0);case 14:return t.prev=14,e.f(),t.finish(14);case 17:case"end":return t.stop()}}),t,this,[[1,11,14,17]])}))},{key:"entries",value:Rk.mark((function t(){var e,i,n,o,r;return Rk.wrap((function(t){for(;;)switch(t.prev=t.next){case 0:e=Hx(this._pairs),t.prev=1,e.s();case 3:if((i=e.n()).done){t.next=9;break}return n=Kc(i.value,2),o=n[0],r=n[1],t.next=7,[o,r];case 7:t.next=3;break;case 9:t.next=14;break;case 11:t.prev=11,t.t0=t.catch(1),e.e(t.t0);case 14:return t.prev=14,e.f(),t.finish(14);case 17:case"end":return t.stop()}}),t,this,[[1,11,14,17]])}))},{key:"keys",value:Rk.mark((function t(){var e,i,n,o;return Rk.wrap((function(t){for(;;)switch(t.prev=t.next){case 0:e=Hx(this._pairs),t.prev=1,e.s();case 3:if((i=e.n()).done){t.next=9;break}return n=Kc(i.value,1),o=n[0],t.next=7,o;case 7:t.next=3;break;case 9:t.next=14;break;case 11:t.prev=11,t.t0=t.catch(1),e.e(t.t0);case 14:return t.prev=14,e.f(),t.finish(14);case 17:case"end":return t.stop()}}),t,this,[[1,11,14,17]])}))},{key:"values",value:Rk.mark((function t(){var e,i,n,o;return Rk.wrap((function(t){for(;;)switch(t.prev=t.next){case 0:e=Hx(this._pairs),t.prev=1,e.s();case 3:if((i=e.n()).done){t.next=9;break}return n=Kc(i.value,2),o=n[1],t.next=7,o;case 7:t.next=3;break;case 9:t.next=14;break;case 11:t.prev=11,t.t0=t.catch(1),e.e(t.t0);case 14:return t.prev=14,e.f(),t.finish(14);case 17:case"end":return t.stop()}}),t,this,[[1,11,14,17]])}))},{key:"toIdArray",value:function(){var t;return gu(t=Jc(this._pairs)).call(t,(function(t){return t[0]}))}},{key:"toItemArray",value:function(){var t;return gu(t=Jc(this._pairs)).call(t,(function(t){return t[1]}))}},{key:"toEntryArray",value:function(){return Jc(this._pairs)}},{key:"toObjectMap",value:function(){var t,e=Kp(null),i=Hx(this._pairs);try{for(i.s();!(t=i.n()).done;){var n=Kc(t.value,2),o=n[0],r=n[1];e[o]=r}}catch(t){i.e(t)}finally{i.f()}return e}},{key:"toMap",value:function(){return new Jw(this._pairs)}},{key:"toIdSet",value:function(){return new b_(this.toIdArray())}},{key:"toItemSet",value:function(){return new b_(this.toItemArray())}},{key:"cache",value:function(){return new t(Jc(this._pairs))}},{key:"distinct",value:function(t){var e,i=new b_,n=Hx(this._pairs);try{for(n.s();!(e=n.n()).done;){var o=Kc(e.value,2),r=o[0],s=o[1];i.add(t(s,r))}}catch(t){n.e(t)}finally{n.f()}return i}},{key:"filter",value:function(e){var i=this._pairs;return new t($d({},w_,Rk.mark((function t(){var n,o,r,s,a;return Rk.wrap((function(t){for(;;)switch(t.prev=t.next){case 0:n=Hx(i),t.prev=1,n.s();case 3:if((o=n.n()).done){t.next=10;break}if(r=Kc(o.value,2),s=r[0],a=r[1],!e(a,s)){t.next=8;break}return t.next=8,[s,a];case 8:t.next=3;break;case 10:t.next=15;break;case 12:t.prev=12,t.t0=t.catch(1),n.e(t.t0);case 15:return t.prev=15,n.f(),t.finish(15);case 18:case"end":return t.stop()}}),t,null,[[1,12,15,18]])}))))}},{key:"forEach",value:function(t){var e,i=Hx(this._pairs);try{for(i.s();!(e=i.n()).done;){var n=Kc(e.value,2),o=n[0];t(n[1],o)}}catch(t){i.e(t)}finally{i.f()}}},{key:"map",value:function(e){var i=this._pairs;return new t($d({},w_,Rk.mark((function t(){var n,o,r,s,a;return Rk.wrap((function(t){for(;;)switch(t.prev=t.next){case 0:n=Hx(i),t.prev=1,n.s();case 3:if((o=n.n()).done){t.next=9;break}return r=Kc(o.value,2),s=r[0],a=r[1],t.next=7,[s,e(a,s)];case 7:t.next=3;break;case 9:t.next=14;break;case 11:t.prev=11,t.t0=t.catch(1),n.e(t.t0);case 14:return t.prev=14,n.f(),t.finish(14);case 17:case"end":return t.stop()}}),t,null,[[1,11,14,17]])}))))}},{key:"max",value:function(t){var e=k_(this._pairs),i=e.next();if(i.done)return null;for(var n=i.value[1],o=t(i.value[1],i.value[0]);!(i=e.next()).done;){var r=Kc(i.value,2),s=r[0],a=r[1],h=t(a,s);h>o&&(o=h,n=a)}return n}},{key:"min",value:function(t){var e=k_(this._pairs),i=e.next();if(i.done)return null;for(var n=i.value[1],o=t(i.value[1],i.value[0]);!(i=e.next()).done;){var r=Kc(i.value,2),s=r[0],a=r[1],h=t(a,s);h<o&&(o=h,n=a)}return n}},{key:"reduce",value:function(t,e){var i,n=Hx(this._pairs);try{for(n.s();!(i=n.n()).done;){var o=Kc(i.value,2),r=o[0];e=t(e,o[1],r)}}catch(t){n.e(t)}finally{n.f()}return e}},{key:"sort",value:function(e){var i=this;return new t($d({},w_,(function(){var t;return k_(rx(t=Jc(i._pairs)).call(t,(function(t,i){var n=Kc(t,2),o=n[0],r=n[1],s=Kc(i,2),a=s[0],h=s[1];return e(r,h,o,a)})))})))}}]),t}();var Kx=function(t){zk(i,t);var e=Lx(i);function i(t,n){var o;return Yd(this,i),$d(Pk(o=e.call(this)),"flush",void 0),$d(Pk(o),"length",void 0),$d(Pk(o),"_options",void 0),$d(Pk(o),"_data",void 0),$d(Pk(o),"_idProp",void 0),$d(Pk(o),"_queue",null),t&&!lu(t)&&(n=t,t=[]),o._options=n||{},o._data=new Jw,o.length=0,o._idProp=o._options.fieldId||"id",t&&t.length&&o.add(t),o.setOptions(n),o}return Kd(i,[{key:"idProp",get:function(){return this._idProp}},{key:"setOptions",value:function(t){t&&void 0!==t.queue&&(!1===t.queue?this._queue&&(this._queue.destroy(),this._queue=null):(this._queue||(this._queue=Yx.extend(this,{replace:["add","update","remove"]})),t.queue&&"object"===Qc(t.queue)&&this._queue.setOptions(t.queue)))}},{key:"add",value:function(t,e){var i,n=this,o=[];if(lu(t)){var r=gu(t).call(t,(function(t){return t[n._idProp]}));if(ck(r).call(r,(function(t){return n._data.has(t)})))throw new Error("A duplicate id was found in the parameter array.");for(var s=0,a=t.length;s<a;s++)i=this._addItem(t[s]),o.push(i)}else{if(!t||"object"!==Qc(t))throw new Error("Unknown dataType");i=this._addItem(t),o.push(i)}return o.length&&this._trigger("add",{items:o},e),o}},{key:"update",value:function(t,e){var i=this,n=[],o=[],r=[],s=[],a=this._idProp,h=function(t){var e=t[a];if(null!=e&&i._data.has(e)){var h=t,l=un({},i._data.get(e)),d=i._updateItem(h);o.push(d),s.push(h),r.push(l)}else{var c=i._addItem(t);n.push(c)}};if(lu(t))for(var l=0,d=t.length;l<d;l++)t[l]&&"object"===Qc(t[l])?h(t[l]):console.warn("Ignoring input item, which is not an object at index "+l);else{if(!t||"object"!==Qc(t))throw new Error("Unknown dataType");h(t)}if(n.length&&this._trigger("add",{items:n},e),o.length){var c={items:o,oldData:r,data:s};this._trigger("update",c,e)}return su(n).call(n,o)}},{key:"updateOnly",value:function(t,e){var i,n=this;lu(t)||(t=[t]);var o=gu(i=gu(t).call(t,(function(t){var e=n._data.get(t[n._idProp]);if(null==e)throw new Error("Updating non-existent items is not allowed.");return{oldData:e,update:t}}))).call(i,(function(t){var e=t.oldData,i=t.update,o=e[n._idProp],r=By(e,i);return n._data.set(o,r),{id:o,oldData:e,updatedData:r}}));if(o.length){var r={items:gu(o).call(o,(function(t){return t.id})),oldData:gu(o).call(o,(function(t){return t.oldData})),data:gu(o).call(o,(function(t){return t.updatedData}))};return this._trigger("update",r,e),r.items}return[]}},{key:"get",value:function(t,e){var i=void 0,n=void 0,o=void 0;Ux(t)?(i=t,o=e):lu(t)?(n=t,o=e):o=t;var r,s=o&&"Object"===o.returnType?"Object":"Array",a=o&&Xf(o),h=[],l=void 0,d=void 0,c=void 0;if(null!=i)(l=this._data.get(i))&&a&&!a(l)&&(l=void 0);else if(null!=n)for(var u=0,f=n.length;u<f;u++)null==(l=this._data.get(n[u]))||a&&!a(l)||h.push(l);else for(var p,v=0,g=(d=Jc(fx(p=this._data).call(p))).length;v<g;v++)c=d[v],null==(l=this._data.get(c))||a&&!a(l)||h.push(l);if(o&&o.order&&null==i&&this._sort(h,o.order),o&&o.fields){var y=o.fields;if(null!=i&&null!=l)l=this._filterFields(l,y);else for(var m=0,b=h.length;m<b;m++)h[m]=this._filterFields(h[m],y)}if("Object"==s){for(var w={},k=0,_=h.length;k<_;k++){var x=h[k];w[x[this._idProp]]=x}return w}return null!=i?null!==(r=l)&&void 0!==r?r:null:h}},{key:"getIds",value:function(t){var e=this._data,i=t&&Xf(t),n=t&&t.order,o=Jc(fx(e).call(e)),r=[];if(i)if(n){for(var s=[],a=0,h=o.length;a<h;a++){var l=o[a],d=this._data.get(l);null!=d&&i(d)&&s.push(d)}this._sort(s,n);for(var c=0,u=s.length;c<u;c++)r.push(s[c][this._idProp])}else for(var f=0,p=o.length;f<p;f++){var v=o[f],g=this._data.get(v);null!=g&&i(g)&&r.push(g[this._idProp])}else if(n){for(var y=[],m=0,b=o.length;m<b;m++){var w=o[m];y.push(e.get(w))}this._sort(y,n);for(var k=0,_=y.length;k<_;k++)r.push(y[k][this._idProp])}else for(var x=0,E=o.length;x<E;x++){var O=o[x],C=e.get(O);null!=C&&r.push(C[this._idProp])}return r}},{key:"getDataSet",value:function(){return this}},{key:"forEach",value:function(t,e){var i=e&&Xf(e),n=this._data,o=Jc(fx(n).call(n));if(e&&e.order)for(var r=this.get(e),s=0,a=r.length;s<a;s++){var h=r[s];t(h,h[this._idProp])}else for(var l=0,d=o.length;l<d;l++){var c=o[l],u=this._data.get(c);null==u||i&&!i(u)||t(u,c)}}},{key:"map",value:function(t,e){for(var i=e&&Xf(e),n=[],o=this._data,r=Jc(fx(o).call(o)),s=0,a=r.length;s<a;s++){var h=r[s],l=this._data.get(h);null==l||i&&!i(l)||n.push(t(l,h))}return e&&e.order&&this._sort(n,e.order),n}},{key:"_filterFields",value:function(t,e){var i;return t?i_(i=lu(e)?e:bu(e)).call(i,(function(e,i){return e[i]=t[i],e}),{}):t}},{key:"_sort",value:function(t,e){if("string"==typeof e){var i=e;rx(t).call(t,(function(t,e){var n=t[i],o=e[i];return n>o?1:n<o?-1:0}))}else{if("function"!=typeof e)throw new TypeError("Order must be a function or a string");rx(t).call(t,e)}}},{key:"remove",value:function(t,e){for(var i=[],n=[],o=lu(t)?t:[t],r=0,s=o.length;r<s;r++){var a=this._remove(o[r]);if(a){var h=a[this._idProp];null!=h&&(i.push(h),n.push(a))}}return i.length&&this._trigger("remove",{items:i,oldData:n},e),i}},{key:"_remove",value:function(t){var e;if(Ux(t)?e=t:t&&"object"===Qc(t)&&(e=t[this._idProp]),null!=e&&this._data.has(e)){var i=this._data.get(e)||null;return this._data.delete(e),--this.length,i}return null}},{key:"clear",value:function(t){for(var e,i=Jc(fx(e=this._data).call(e)),n=[],o=0,r=i.length;o<r;o++)n.push(this._data.get(i[o]));return this._data.clear(),this.length=0,this._trigger("remove",{items:i,oldData:n},t),i}},{key:"max",value:function(t){var e,i,n=null,o=null,r=Hx(kx(e=this._data).call(e));try{for(r.s();!(i=r.n()).done;){var s=i.value,a=s[t];"number"==typeof a&&(null==o||a>o)&&(n=s,o=a)}}catch(t){r.e(t)}finally{r.f()}return n||null}},{key:"min",value:function(t){var e,i,n=null,o=null,r=Hx(kx(e=this._data).call(e));try{for(r.s();!(i=r.n()).done;){var s=i.value,a=s[t];"number"==typeof a&&(null==o||a<o)&&(n=s,o=a)}}catch(t){r.e(t)}finally{r.f()}return n||null}},{key:"distinct",value:function(t){for(var e=this._data,i=Jc(fx(e).call(e)),n=[],o=0,r=0,s=i.length;r<s;r++){for(var a=i[r],h=e.get(a)[t],l=!1,d=0;d<o;d++)if(n[d]==h){l=!0;break}l||void 0===h||(n[o]=h,o++)}return n}},{key:"_addItem",value:function(t){var e=function(t,e){return null==t[e]&&(t[e]=Ax()),t}(t,this._idProp),i=e[this._idProp];if(this._data.has(i))throw new Error("Cannot add item: item with id "+i+" already exists");return this._data.set(i,e),++this.length,i}},{key:"_updateItem",value:function(t){var e=t[this._idProp];if(null==e)throw new Error("Cannot update item: item has no id (item: "+gv(t)+")");var i=this._data.get(e);if(!i)throw new Error("Cannot update item: no item with id "+e+" found");return this._data.set(e,Rx(Rx({},i),t)),e}},{key:"stream",value:function(t){if(t){var e=this._data;return new Gx($d({},w_,Rk.mark((function i(){var n,o,r,s;return Rk.wrap((function(i){for(;;)switch(i.prev=i.next){case 0:n=Hx(t),i.prev=1,n.s();case 3:if((o=n.n()).done){i.next=11;break}if(r=o.value,null==(s=e.get(r))){i.next=9;break}return i.next=9,[r,s];case 9:i.next=3;break;case 11:i.next=16;break;case 13:i.prev=13,i.t0=i.catch(1),n.e(i.t0);case 16:return i.prev=16,n.f(),i.finish(16);case 19:case"end":return i.stop()}}),i,null,[[1,13,16,19]])}))))}var i;return new Gx($d({},w_,zn(i=Mx(this._data)).call(i,this._data)))}}]),i}(Xx),$x=function(t){zk(i,t);var e=Lx(i);function i(t,n){var o,r;return Yd(this,i),$d(Pk(r=e.call(this)),"length",0),$d(Pk(r),"_listener",void 0),$d(Pk(r),"_data",void 0),$d(Pk(r),"_ids",new b_),$d(Pk(r),"_options",void 0),r._options=n||{},r._listener=zn(o=r._onEvent).call(o,Pk(r)),r.setData(t),r}return Kd(i,[{key:"idProp",get:function(){return this.getDataSet().idProp}},{key:"setData",value:function(t){if(this._data){this._data.off&&this._data.off("*",this._listener);var e=this._data.getIds({filter:Xf(this._options)}),i=this._data.get(e);this._ids.clear(),this.length=0,this._trigger("remove",{items:e,oldData:i})}if(null!=t){this._data=t;for(var n=this._data.getIds({filter:Xf(this._options)}),o=0,r=n.length;o<r;o++){var s=n[o];this._ids.add(s)}this.length=n.length,this._trigger("add",{items:n})}else this._data=new Kx;this._data.on&&this._data.on("*",this._listener)}},{key:"refresh",value:function(){for(var t=this._data.getIds({filter:Xf(this._options)}),e=Jc(this._ids),i={},n=[],o=[],r=[],s=0,a=t.length;s<a;s++){var h=t[s];i[h]=!0,this._ids.has(h)||(n.push(h),this._ids.add(h))}for(var l=0,d=e.length;l<d;l++){var c=e[l],u=this._data.get(c);null==u?console.error("If you see this, report it please."):i[c]||(o.push(c),r.push(u),this._ids.delete(c))}this.length+=n.length-o.length,n.length&&this._trigger("add",{items:n}),o.length&&this._trigger("remove",{items:o,oldData:r})}},{key:"get",value:function(t,e){if(null==this._data)return null;var i,n=null;Ux(t)||lu(t)?(n=t,i=e):i=t;var o=un({},this._options,i),r=Xf(this._options),s=i&&Xf(i);return r&&s&&(o.filter=function(t){return r(t)&&s(t)}),null==n?this._data.get(o):this._data.get(n,o)}},{key:"getIds",value:function(t){if(this._data.length){var e,i=Xf(this._options),n=null!=t?Xf(t):null;return e=n?i?function(t){return i(t)&&n(t)}:n:i,this._data.getIds({filter:e,order:t&&t.order})}return[]}},{key:"forEach",value:function(t,e){if(this._data){var i,n,o=Xf(this._options),r=e&&Xf(e);n=r?o?function(t){return o(t)&&r(t)}:r:o,Fu(i=this._data).call(i,t,{filter:n,order:e&&e.order})}}},{key:"map",value:function(t,e){if(this._data){var i,n,o=Xf(this._options),r=e&&Xf(e);return n=r?o?function(t){return o(t)&&r(t)}:r:o,gu(i=this._data).call(i,t,{filter:n,order:e&&e.order})}return[]}},{key:"getDataSet",value:function(){return this._data.getDataSet()}},{key:"stream",value:function(t){var e;return this._data.stream(t||$d({},w_,zn(e=fx(this._ids)).call(e,this._ids)))}},{key:"dispose",value:function(){var t;null!==(t=this._data)&&void 0!==t&&t.off&&this._data.off("*",this._listener);var e,n="This data view has already been disposed of.",o={get:function(){throw new Error(n)},set:function(){throw new Error(n)},configurable:!1},r=Hx(hu(i.prototype));try{for(r.s();!(e=r.n()).done;){var s=e.value;Ud(this,s,o)}}catch(t){r.e(t)}finally{r.f()}}},{key:"_onEvent",value:function(t,e,i){if(e&&e.items&&this._data){var n=e.items,o=[],r=[],s=[],a=[],h=[],l=[];switch(t){case"add":for(var d=0,c=n.length;d<c;d++){var u=n[d];this.get(u)&&(this._ids.add(u),o.push(u))}break;case"update":for(var f=0,p=n.length;f<p;f++){var v=n[f];this.get(v)?this._ids.has(v)?(r.push(v),h.push(e.data[f]),a.push(e.oldData[f])):(this._ids.add(v),o.push(v)):this._ids.has(v)&&(this._ids.delete(v),s.push(v),l.push(e.oldData[f]))}break;case"remove":for(var g=0,y=n.length;g<y;g++){var m=n[g];this._ids.has(m)&&(this._ids.delete(m),s.push(m),l.push(e.oldData[g]))}}this.length+=o.length-s.length,o.length&&this._trigger("add",{items:o},i),r.length&&this._trigger("update",{items:r,oldData:a,data:h},i),s.length&&this._trigger("remove",{items:s,oldData:l},i)}}}]),i}(Xx);function Zx(t,e){return"object"===Qc(e)&&null!==e&&t===e.idProp&&"function"==typeof e.add&&"function"==typeof e.clear&&"function"==typeof e.distinct&&"function"==typeof Fu(e)&&"function"==typeof e.get&&"function"==typeof e.getDataSet&&"function"==typeof e.getIds&&"number"==typeof e.length&&"function"==typeof gu(e)&&"function"==typeof e.max&&"function"==typeof e.min&&"function"==typeof e.off&&"function"==typeof e.on&&"function"==typeof e.remove&&"function"==typeof e.setOptions&&"function"==typeof e.stream&&"function"==typeof e.update&&"function"==typeof e.updateOnly}function Qx(t,e){return"object"===Qc(e)&&null!==e&&t===e.idProp&&"function"==typeof Fu(e)&&"function"==typeof e.get&&"function"==typeof e.getDataSet&&"function"==typeof e.getIds&&"number"==typeof e.length&&"function"==typeof gu(e)&&"function"==typeof e.off&&"function"==typeof e.on&&"function"==typeof e.stream&&Zx(t,e.getDataSet())}var Jx=Object.freeze({__proto__:null,DELETE:Iy,DataSet:Kx,DataStream:Gx,DataView:$x,Queue:Yx,createNewDataPipeFrom:function(t){return new Vx(t)},isDataSetLike:Zx,isDataViewLike:Qx}),tE=n,eE=o,iE=eo,nE=cp.trim,oE=g("".charAt),rE=tE.parseFloat,sE=tE.Symbol,aE=sE&&sE.iterator,hE=1/rE("\t\n\v\f\r  \u2028\u2029\ufeff-0")!=-1/0||aE&&!eE((function(){rE(Object(aE))}))?function(t){var e=nE(iE(t)),i=rE(e);return 0===i&&"-"==oE(e,0)?-0:i}:rE;_i({global:!0,forced:parseFloat!=hE},{parseFloat:hE});var lE=X.parseFloat,dE=_i,cE=o,uE=hh.f;dE({target:"Object",stat:!0,forced:cE((function(){return!Object.getOwnPropertyNames(1)}))},{getOwnPropertyNames:uE});var fE=X.Object,pE=function(t){return fE.getOwnPropertyNames(t)},vE=pE;function gE(t,e){var i=["node","edge","label"],n=!0,o=Mm(e,"chosen");if("boolean"==typeof o)n=o;else if("object"===Qc(o)){if(-1===Fp(i).call(i,t))throw new Error("choosify: subOption '"+t+"' should be one of '"+i.join("', '")+"'");var r=Mm(e,["chosen",t]);"boolean"!=typeof r&&"function"!=typeof r||(n=r)}return n}function yE(t,e,i){if(t.width<=0||t.height<=0)return!1;if(void 0!==i){var n={x:e.x-i.x,y:e.y-i.y};if(0!==i.angle){var o=-i.angle;e={x:Math.cos(o)*n.x-Math.sin(o)*n.y,y:Math.sin(o)*n.x+Math.cos(o)*n.y}}else e=n}var r=t.x+t.width,s=t.y+t.width;return t.left<e.x&&r>e.x&&t.top<e.y&&s>e.y}function mE(t){return"string"==typeof t&&""!==t}function bE(t,e,i,n){var o=n.x,r=n.y;if("function"==typeof n.distanceToBorder){var s=n.distanceToBorder(t,e),a=Math.sin(e)*s,h=Math.cos(e)*s;h===s?(o+=s,r=n.y):a===s?(o=n.x,r-=s):(o+=h,r-=a)}else n.shape.width>n.shape.height?(o=n.x+.5*n.shape.width,r=n.y-i):(o=n.x+i,r=n.y-.5*n.shape.height);return{x:o,y:r}}var wE=function(){function t(e){Yd(this,t),this.measureText=e,this.current=0,this.width=0,this.height=0,this.lines=[]}return Kd(t,[{key:"_add",value:function(t,e){var i=arguments.length>2&&void 0!==arguments[2]?arguments[2]:"normal";void 0===this.lines[t]&&(this.lines[t]={width:0,height:0,blocks:[]});var n=e;void 0!==e&&""!==e||(n=" ");var o=this.measureText(n,i),r=un({},kx(o));r.text=e,r.width=o.width,r.mod=i,void 0!==e&&""!==e||(r.width=0),this.lines[t].blocks.push(r),this.lines[t].width+=r.width}},{key:"curWidth",value:function(){var t=this.lines[this.current];return void 0===t?0:t.width}},{key:"append",value:function(t){var e=arguments.length>1&&void 0!==arguments[1]?arguments[1]:"normal";this._add(this.current,t,e)}},{key:"newLine",value:function(t){var e=arguments.length>1&&void 0!==arguments[1]?arguments[1]:"normal";this._add(this.current,t,e),this.current++}},{key:"determineLineHeights",value:function(){for(var t=0;t<this.lines.length;t++){var e=this.lines[t],i=0;if(void 0!==e.blocks)for(var n=0;n<e.blocks.length;n++){var o=e.blocks[n];i<o.height&&(i=o.height)}e.height=i}}},{key:"determineLabelSize",value:function(){for(var t=0,e=0,i=0;i<this.lines.length;i++){var n=this.lines[i];n.width>t&&(t=n.width),e+=n.height}this.width=t,this.height=e}},{key:"removeEmptyBlocks",value:function(){for(var t=[],e=0;e<this.lines.length;e++){var i=this.lines[e];if(0!==i.blocks.length&&(e!==this.lines.length-1||0!==i.width)){var n={};un(n,i),n.blocks=[];for(var o=void 0,r=[],s=0;s<i.blocks.length;s++){var a=i.blocks[s];0!==a.width?r.push(a):void 0===o&&(o=a)}0===r.length&&void 0!==o&&r.push(o),n.blocks=r,t.push(n)}}return t}},{key:"finalize",value:function(){this.determineLineHeights(),this.determineLabelSize();var t=this.removeEmptyBlocks();return{width:this.width,height:this.height,lines:t}}}]),t}(),kE={"<b>":/<b>/,"<i>":/<i>/,"<code>":/<code>/,"</b>":/<\/b>/,"</i>":/<\/i>/,"</code>":/<\/code>/,"*":/\*/,_:/_/,"`":/`/,afterBold:/[^*]/,afterItal:/[^_]/,afterMono:/[^`]/},_E=function(){function t(e){Yd(this,t),this.text=e,this.bold=!1,this.ital=!1,this.mono=!1,this.spacing=!1,this.position=0,this.buffer="",this.modStack=[],this.blocks=[]}return Kd(t,[{key:"mod",value:function(){return 0===this.modStack.length?"normal":this.modStack[0]}},{key:"modName",value:function(){return 0===this.modStack.length?"normal":"mono"===this.modStack[0]?"mono":this.bold&&this.ital?"boldital":this.bold?"bold":this.ital?"ital":void 0}},{key:"emitBlock",value:function(){this.spacing&&(this.add(" "),this.spacing=!1),this.buffer.length>0&&(this.blocks.push({text:this.buffer,mod:this.modName()}),this.buffer="")}},{key:"add",value:function(t){" "===t&&(this.spacing=!0),this.spacing&&(this.buffer+=" ",this.spacing=!1)," "!=t&&(this.buffer+=t)}},{key:"parseWS",value:function(t){return!!/[ \t]/.test(t)&&(this.mono?this.add(t):this.spacing=!0,!0)}},{key:"setTag",value:function(t){this.emitBlock(),this[t]=!0,this.modStack.unshift(t)}},{key:"unsetTag",value:function(t){this.emitBlock(),this[t]=!1,this.modStack.shift()}},{key:"parseStartTag",value:function(t,e){return!(this.mono||this[t]||!this.match(e))&&(this.setTag(t),!0)}},{key:"match",value:function(t){var e=!(arguments.length>1&&void 0!==arguments[1])||arguments[1],i=this.prepareRegExp(t),n=Kc(i,2),o=n[0],r=n[1],s=o.test(this.text.substr(this.position,r));return s&&e&&(this.position+=r-1),s}},{key:"parseEndTag",value:function(t,e,i){var n=this.mod()===t;return!(!(n="mono"===t?n&&this.mono:n&&!this.mono)||!this.match(e))&&(void 0!==i?(this.position===this.text.length-1||this.match(i,!1))&&this.unsetTag(t):this.unsetTag(t),!0)}},{key:"replace",value:function(t,e){return!!this.match(t)&&(this.add(e),this.position+=length-1,!0)}},{key:"prepareRegExp",value:function(t){var e,i;if(t instanceof RegExp)i=t,e=1;else{var n=kE[t];i=void 0!==n?n:new RegExp(t),e=t.length}return[i,e]}}]),t}(),xE=function(){function t(e,i,n,o){var r=this;Yd(this,t),this.ctx=e,this.parent=i,this.selected=n,this.hover=o;this.lines=new wE((function(t,i){if(void 0===t)return 0;var s=r.parent.getFormattingValues(e,n,o,i),a=0;""!==t&&(a=r.ctx.measureText(t).width);return{width:a,values:s}}))}return Kd(t,[{key:"process",value:function(t){if(!mE(t))return this.lines.finalize();var e=this.parent.fontOptions;t=(t=t.replace(/\r\n/g,"\n")).replace(/\r/g,"\n");var i=String(t).split("\n"),n=i.length;if(e.multi)for(var o=0;o<n;o++){var r=this.splitBlocks(i[o],e.multi);if(void 0!==r)if(0!==r.length){if(e.maxWdt>0)for(var s=0;s<r.length;s++){var a=r[s].mod,h=r[s].text;this.splitStringIntoLines(h,a,!0)}else for(var l=0;l<r.length;l++){var d=r[l].mod,c=r[l].text;this.lines.append(c,d)}this.lines.newLine()}else this.lines.newLine("")}else if(e.maxWdt>0)for(var u=0;u<n;u++)this.splitStringIntoLines(i[u]);else for(var f=0;f<n;f++)this.lines.newLine(i[f]);return this.lines.finalize()}},{key:"decodeMarkupSystem",value:function(t){var e="none";return"markdown"===t||"md"===t?e="markdown":!0!==t&&"html"!==t||(e="html"),e}},{key:"splitHtmlBlocks",value:function(t){for(var e=new _E(t),i=function(t){return!!/&/.test(t)&&(e.replace(e.text,"&lt;","<")||e.replace(e.text,"&amp;","&")||e.add("&"),!0)};e.position<e.text.length;){var n=e.text.charAt(e.position);e.parseWS(n)||/</.test(n)&&(e.parseStartTag("bold","<b>")||e.parseStartTag("ital","<i>")||e.parseStartTag("mono","<code>")||e.parseEndTag("bold","</b>")||e.parseEndTag("ital","</i>")||e.parseEndTag("mono","</code>"))||i(n)||e.add(n),e.position++}return e.emitBlock(),e.blocks}},{key:"splitMarkdownBlocks",value:function(t){for(var e=this,i=new _E(t),n=!0,o=function(t){return!!/\\/.test(t)&&(i.position<e.text.length+1&&(i.position++,t=e.text.charAt(i.position),/ \t/.test(t)?i.spacing=!0:(i.add(t),n=!1)),!0)};i.position<i.text.length;){var r=i.text.charAt(i.position);i.parseWS(r)||o(r)||(n||i.spacing)&&(i.parseStartTag("bold","*")||i.parseStartTag("ital","_")||i.parseStartTag("mono","`"))||i.parseEndTag("bold","*","afterBold")||i.parseEndTag("ital","_","afterItal")||i.parseEndTag("mono","`","afterMono")||(i.add(r),n=!1),i.position++}return i.emitBlock(),i.blocks}},{key:"splitBlocks",value:function(t,e){var i=this.decodeMarkupSystem(e);return"none"===i?[{text:t,mod:"normal"}]:"markdown"===i?this.splitMarkdownBlocks(t):"html"===i?this.splitHtmlBlocks(t):void 0}},{key:"overMaxWidth",value:function(t){var e=this.ctx.measureText(t).width;return this.lines.curWidth()+e>this.parent.fontOptions.maxWdt}},{key:"getLongestFit",value:function(t){for(var e="",i=0;i<t.length;){var n=e+(""===e?"":" ")+t[i];if(this.overMaxWidth(n))break;e=n,i++}return i}},{key:"getLongestFitWord",value:function(t){for(var e=0;e<t.length&&!this.overMaxWidth(au(t).call(t,0,e));)e++;return e}},{key:"splitStringIntoLines",value:function(t){var e=arguments.length>1&&void 0!==arguments[1]?arguments[1]:"normal",i=arguments.length>2&&void 0!==arguments[2]&&arguments[2];this.parent.getFormattingValues(this.ctx,this.selected,this.hover,e);for(var n=(t=(t=t.replace(/^( +)/g,"$1\r")).replace(/([^\r][^ ]*)( +)/g,"$1\r$2\r")).split("\r");n.length>0;){var o=this.getLongestFit(n);if(0===o){var r=n[0],s=this.getLongestFitWord(r);this.lines.newLine(au(r).call(r,0,s),e),n[0]=au(r).call(r,s)}else{var a=o;" "===n[o-1]?o--:" "===n[a]&&a++;var h=au(n).call(n,0,o).join("");o==n.length&&i?this.lines.append(h,e):this.lines.newLine(h,e),n=au(n).call(n,a)}}}}]),t}(),EE=["bold","ital","boldital","mono"],OE=function(){function t(e,i){var n=arguments.length>2&&void 0!==arguments[2]&&arguments[2];Yd(this,t),this.body=e,this.pointToSelf=!1,this.baseSize=void 0,this.fontOptions={},this.setOptions(i),this.size={top:0,left:0,width:0,height:0,yLine:0},this.isEdgeLabel=n}return Kd(t,[{key:"setOptions",value:function(t){if(this.elementOptions=t,this.initFontOptions(t.font),mE(t.label)?this.labelDirty=!0:t.label=void 0,void 0!==t.font&&null!==t.font)if("string"==typeof t.font)this.baseSize=this.fontOptions.size;else if("object"===Qc(t.font)){var e=t.font.size;void 0!==e&&(this.baseSize=e)}}},{key:"initFontOptions",value:function(e){var i=this;hm(EE,(function(t){i.fontOptions[t]={}})),t.parseFontString(this.fontOptions,e)?this.fontOptions.vadjust=0:hm(e,(function(t,e){null!=t&&"object"!==Qc(t)&&(i.fontOptions[e]=t)}))}},{key:"constrain",value:function(t){var e={constrainWidth:!1,maxWdt:-1,minWdt:-1,constrainHeight:!1,minHgt:-1,valign:"middle"},i=Mm(t,"widthConstraint");if("number"==typeof i)e.maxWdt=Number(i),e.minWdt=Number(i);else if("object"===Qc(i)){var n=Mm(t,["widthConstraint","maximum"]);"number"==typeof n&&(e.maxWdt=Number(n));var o=Mm(t,["widthConstraint","minimum"]);"number"==typeof o&&(e.minWdt=Number(o))}var r=Mm(t,"heightConstraint");if("number"==typeof r)e.minHgt=Number(r);else if("object"===Qc(r)){var s=Mm(t,["heightConstraint","minimum"]);"number"==typeof s&&(e.minHgt=Number(s));var a=Mm(t,["heightConstraint","valign"]);"string"==typeof a&&("top"!==a&&"bottom"!==a||(e.valign=a))}return e}},{key:"update",value:function(t,e){this.setOptions(t,!0),this.propagateFonts(e),nm(this.fontOptions,this.constrain(e)),this.fontOptions.chooser=gE("label",e)}},{key:"adjustSizes",value:function(t){var e=t?t.right+t.left:0;this.fontOptions.constrainWidth&&(this.fontOptions.maxWdt-=e,this.fontOptions.minWdt-=e);var i=t?t.top+t.bottom:0;this.fontOptions.constrainHeight&&(this.fontOptions.minHgt-=i)}},{key:"addFontOptionsToPile",value:function(t,e){for(var i=0;i<e.length;++i)this.addFontToPile(t,e[i])}},{key:"addFontToPile",value:function(t,e){if(void 0!==e&&void 0!==e.font&&null!==e.font){var i=e.font;t.push(i)}}},{key:"getBasicOptions",value:function(e){for(var i={},n=0;n<e.length;++n){var o=e[n],r={};t.parseFontString(r,o)&&(o=r),hm(o,(function(t,e){void 0!==t&&(Object.prototype.hasOwnProperty.call(i,e)||(-1!==Fp(EE).call(EE,e)?i[e]={}:i[e]=t))}))}return i}},{key:"getFontOption",value:function(e,i,n){for(var o,r=0;r<e.length;++r){var s=e[r];if(Object.prototype.hasOwnProperty.call(s,i)){if(null==(o=s[i]))continue;var a={};if(t.parseFontString(a,o)&&(o=a),Object.prototype.hasOwnProperty.call(o,n))return o[n]}}if(Object.prototype.hasOwnProperty.call(this.fontOptions,n))return this.fontOptions[n];throw new Error("Did not find value for multi-font for property: '"+n+"'")}},{key:"getFontOptions",value:function(t,e){for(var i={},n=["color","size","face","mod","vadjust"],o=0;o<n.length;++o){var r=n[o];i[r]=this.getFontOption(t,e,r)}return i}},{key:"propagateFonts",value:function(t){var e=this,i=[];this.addFontOptionsToPile(i,t),this.fontOptions=this.getBasicOptions(i);for(var n=function(t){var n=EE[t],o=e.fontOptions[n];hm(e.getFontOptions(i,n),(function(t,e){o[e]=t})),o.size=Number(o.size),o.vadjust=Number(o.vadjust)},o=0;o<EE.length;++o)n(o)}},{key:"draw",value:function(t,e,i,n,o){var r=arguments.length>5&&void 0!==arguments[5]?arguments[5]:"middle";if(void 0!==this.elementOptions.label){var s=this.fontOptions.size*this.body.view.scale;this.elementOptions.label&&s<this.elementOptions.scaling.label.drawThreshold-1||(s>=this.elementOptions.scaling.label.maxVisible&&(s=Number(this.elementOptions.scaling.label.maxVisible)/this.body.view.scale),this.calculateLabelSize(t,n,o,e,i,r),this._drawBackground(t),this._drawText(t,e,this.size.yLine,r,s))}}},{key:"_drawBackground",value:function(t){if(void 0!==this.fontOptions.background&&"none"!==this.fontOptions.background){t.fillStyle=this.fontOptions.background;var e=this.getSize();t.fillRect(e.left,e.top,e.width,e.height)}}},{key:"_drawText",value:function(t,e,i){var n=arguments.length>3&&void 0!==arguments[3]?arguments[3]:"middle",o=arguments.length>4?arguments[4]:void 0,r=this._setAlignment(t,e,i,n),s=Kc(r,2);e=s[0],i=s[1],t.textAlign="left",e-=this.size.width/2,this.fontOptions.valign&&this.size.height>this.size.labelHeight&&("top"===this.fontOptions.valign&&(i-=(this.size.height-this.size.labelHeight)/2),"bottom"===this.fontOptions.valign&&(i+=(this.size.height-this.size.labelHeight)/2));for(var a=0;a<this.lineCount;a++){var h=this.lines[a];if(h&&h.blocks){var l=0;this.isEdgeLabel||"center"===this.fontOptions.align?l+=(this.size.width-h.width)/2:"right"===this.fontOptions.align&&(l+=this.size.width-h.width);for(var d=0;d<h.blocks.length;d++){var c=h.blocks[d];t.font=c.font;var u=this._getColor(c.color,o,c.strokeColor),f=Kc(u,2),p=f[0],v=f[1];c.strokeWidth>0&&(t.lineWidth=c.strokeWidth,t.strokeStyle=v,t.lineJoin="round"),t.fillStyle=p,c.strokeWidth>0&&t.strokeText(c.text,e+l,i+c.vadjust),t.fillText(c.text,e+l,i+c.vadjust),l+=c.width}i+=h.height}}}},{key:"_setAlignment",value:function(t,e,i,n){if(this.isEdgeLabel&&"horizontal"!==this.fontOptions.align&&!1===this.pointToSelf){e=0,i=0;"top"===this.fontOptions.align?(t.textBaseline="alphabetic",i-=4):"bottom"===this.fontOptions.align?(t.textBaseline="hanging",i+=4):t.textBaseline="middle"}else t.textBaseline=n;return[e,i]}},{key:"_getColor",value:function(t,e,i){var n=t||"#000000",o=i||"#ffffff";if(e<=this.elementOptions.scaling.label.drawThreshold){var r=Math.max(0,Math.min(1,1-(this.elementOptions.scaling.label.drawThreshold-e)));n=pm(n,r),o=pm(o,r)}return[n,o]}},{key:"getTextSize",value:function(t){var e=arguments.length>1&&void 0!==arguments[1]&&arguments[1],i=arguments.length>2&&void 0!==arguments[2]&&arguments[2];return this._processLabel(t,e,i),{width:this.size.width,height:this.size.height,lineCount:this.lineCount}}},{key:"getSize",value:function(){var t=this.size.left,e=this.size.top-1;if(this.isEdgeLabel){var i=.5*-this.size.width;switch(this.fontOptions.align){case"middle":t=i,e=.5*-this.size.height;break;case"top":t=i,e=-(this.size.height+2);break;case"bottom":t=i,e=2}}return{left:t,top:e,width:this.size.width,height:this.size.height}}},{key:"calculateLabelSize",value:function(t,e,i){var n=arguments.length>3&&void 0!==arguments[3]?arguments[3]:0,o=arguments.length>4&&void 0!==arguments[4]?arguments[4]:0,r=arguments.length>5&&void 0!==arguments[5]?arguments[5]:"middle";this._processLabel(t,e,i),this.size.left=n-.5*this.size.width,this.size.top=o-.5*this.size.height,this.size.yLine=o+.5*(1-this.lineCount)*this.fontOptions.size,"hanging"===r&&(this.size.top+=.5*this.fontOptions.size,this.size.top+=4,this.size.yLine+=4)}},{key:"getFormattingValues",value:function(t,e,i,n){var o=function(t,e,i){return"normal"===e?"mod"===i?"":t[i]:void 0!==t[e][i]?t[e][i]:t[i]},r={color:o(this.fontOptions,n,"color"),size:o(this.fontOptions,n,"size"),face:o(this.fontOptions,n,"face"),mod:o(this.fontOptions,n,"mod"),vadjust:o(this.fontOptions,n,"vadjust"),strokeWidth:this.fontOptions.strokeWidth,strokeColor:this.fontOptions.strokeColor};(e||i)&&("normal"===n&&!0===this.fontOptions.chooser&&this.elementOptions.labelHighlightBold?r.mod="bold":"function"==typeof this.fontOptions.chooser&&this.fontOptions.chooser(r,this.elementOptions.id,e,i));var s="";return void 0!==r.mod&&""!==r.mod&&(s+=r.mod+" "),s+=r.size+"px "+r.face,t.font=s.replace(/"/g,""),r.font=t.font,r.height=r.size,r}},{key:"differentState",value:function(t,e){return t!==this.selectedState||e!==this.hoverState}},{key:"_processLabelText",value:function(t,e,i,n){return new xE(t,this,e,i).process(n)}},{key:"_processLabel",value:function(t,e,i){if(!1!==this.labelDirty||this.differentState(e,i)){var n=this._processLabelText(t,e,i,this.elementOptions.label);this.fontOptions.minWdt>0&&n.width<this.fontOptions.minWdt&&(n.width=this.fontOptions.minWdt),this.size.labelHeight=n.height,this.fontOptions.minHgt>0&&n.height<this.fontOptions.minHgt&&(n.height=this.fontOptions.minHgt),this.lines=n.lines,this.lineCount=n.lines.length,this.size.width=n.width,this.size.height=n.height,this.selectedState=e,this.hoverState=i,this.labelDirty=!1}}},{key:"visible",value:function(){return 0!==this.size.width&&0!==this.size.height&&void 0!==this.elementOptions.label&&!(this.fontOptions.size*this.body.view.scale<this.elementOptions.scaling.label.drawThreshold-1)}}],[{key:"parseFontString",value:function(t,e){if(!e||"string"!=typeof e)return!1;var i=e.split(" ");return t.size=+i[0].replace("px",""),t.face=i[1],t.color=i[2],!0}}]),t}(),CE=function(){function t(e,i,n){Yd(this,t),this.body=i,this.labelModule=n,this.setOptions(e),this.top=void 0,this.left=void 0,this.height=void 0,this.width=void 0,this.radius=void 0,this.margin=void 0,this.refreshNeeded=!0,this.boundingBox={top:0,left:0,right:0,bottom:0}}return Kd(t,[{key:"setOptions",value:function(t){this.options=t}},{key:"_setMargins",value:function(t){this.margin={},this.options.margin&&("object"==Qc(this.options.margin)?(this.margin.top=this.options.margin.top,this.margin.right=this.options.margin.right,this.margin.bottom=this.options.margin.bottom,this.margin.left=this.options.margin.left):(this.margin.top=this.options.margin,this.margin.right=this.options.margin,this.margin.bottom=this.options.margin,this.margin.left=this.options.margin)),t.adjustSizes(this.margin)}},{key:"_distanceToBorder",value:function(t,e){var i=this.options.borderWidth;return t&&this.resize(t),Math.min(Math.abs(this.width/2/Math.cos(e)),Math.abs(this.height/2/Math.sin(e)))+i}},{key:"enableShadow",value:function(t,e){e.shadow&&(t.shadowColor=e.shadowColor,t.shadowBlur=e.shadowSize,t.shadowOffsetX=e.shadowX,t.shadowOffsetY=e.shadowY)}},{key:"disableShadow",value:function(t,e){e.shadow&&(t.shadowColor="rgba(0,0,0,0)",t.shadowBlur=0,t.shadowOffsetX=0,t.shadowOffsetY=0)}},{key:"enableBorderDashes",value:function(t,e){if(!1!==e.borderDashes)if(void 0!==t.setLineDash){var i=e.borderDashes;!0===i&&(i=[5,15]),t.setLineDash(i)}else console.warn("setLineDash is not supported in this browser. The dashed borders cannot be used."),this.options.shapeProperties.borderDashes=!1,e.borderDashes=!1}},{key:"disableBorderDashes",value:function(t,e){!1!==e.borderDashes&&(void 0!==t.setLineDash?t.setLineDash([0]):(console.warn("setLineDash is not supported in this browser. The dashed borders cannot be used."),this.options.shapeProperties.borderDashes=!1,e.borderDashes=!1))}},{key:"needsRefresh",value:function(t,e){return!0===this.refreshNeeded?(this.refreshNeeded=!1,!0):void 0===this.width||this.labelModule.differentState(t,e)}},{key:"initContextForDraw",value:function(t,e){var i=e.borderWidth/this.body.view.scale;t.lineWidth=Math.min(this.width,i),t.strokeStyle=e.borderColor,t.fillStyle=e.color}},{key:"performStroke",value:function(t,e){var i=e.borderWidth/this.body.view.scale;t.save(),i>0&&(this.enableBorderDashes(t,e),t.stroke(),this.disableBorderDashes(t,e)),t.restore()}},{key:"performFill",value:function(t,e){t.save(),t.fillStyle=e.color,this.enableShadow(t,e),jv(t).call(t),this.disableShadow(t,e),t.restore(),this.performStroke(t,e)}},{key:"_addBoundingBoxMargin",value:function(t){this.boundingBox.left-=t,this.boundingBox.top-=t,this.boundingBox.bottom+=t,this.boundingBox.right+=t}},{key:"_updateBoundingBox",value:function(t,e,i,n,o){void 0!==i&&this.resize(i,n,o),this.left=t-this.width/2,this.top=e-this.height/2,this.boundingBox.left=this.left,this.boundingBox.top=this.top,this.boundingBox.bottom=this.top+this.height,this.boundingBox.right=this.left+this.width}},{key:"updateBoundingBox",value:function(t,e,i,n,o){this._updateBoundingBox(t,e,i,n,o)}},{key:"getDimensionsFromLabel",value:function(t,e,i){this.textSize=this.labelModule.getTextSize(t,e,i);var n=this.textSize.width,o=this.textSize.height;return 0===n&&(n=14,o=14),{width:n,height:o}}}]),t}();function SE(t){var e=function(){if("undefined"==typeof Reflect||!Mk)return!1;if(Mk.sham)return!1;if("function"==typeof Proxy)return!0;try{return Boolean.prototype.valueOf.call(Mk(Boolean,[],(function(){}))),!0}catch(t){return!1}}();return function(){var i,n=Ak(t);if(e){var o=Ak(this).constructor;i=Mk(n,arguments,o)}else i=n.apply(this,arguments);return Nk(this,i)}}var TE=function(t){zk(i,t);var e=SE(i);function i(t,n,o){var r;return Yd(this,i),(r=e.call(this,t,n,o))._setMargins(o),r}return Kd(i,[{key:"resize",value:function(t){var e=arguments.length>1&&void 0!==arguments[1]?arguments[1]:this.selected,i=arguments.length>2&&void 0!==arguments[2]?arguments[2]:this.hover;if(this.needsRefresh(e,i)){var n=this.getDimensionsFromLabel(t,e,i);this.width=n.width+this.margin.right+this.margin.left,this.height=n.height+this.margin.top+this.margin.bottom,this.radius=this.width/2}}},{key:"draw",value:function(t,e,i,n,o,r){this.resize(t,n,o),this.left=e-this.width/2,this.top=i-this.height/2,this.initContextForDraw(t,r),Fn(t,this.left,this.top,this.width,this.height,r.borderRadius),this.performFill(t,r),this.updateBoundingBox(e,i,t,n,o),this.labelModule.draw(t,this.left+this.textSize.width/2+this.margin.left,this.top+this.textSize.height/2+this.margin.top,n,o)}},{key:"updateBoundingBox",value:function(t,e,i,n,o){this._updateBoundingBox(t,e,i,n,o);var r=this.options.shapeProperties.borderRadius;this._addBoundingBoxMargin(r)}},{key:"distanceToBorder",value:function(t,e){t&&this.resize(t);var i=this.options.borderWidth;return Math.min(Math.abs(this.width/2/Math.cos(e)),Math.abs(this.height/2/Math.sin(e)))+i}}]),i}(CE);function ME(t){var e=function(){if("undefined"==typeof Reflect||!Mk)return!1;if(Mk.sham)return!1;if("function"==typeof Proxy)return!0;try{return Boolean.prototype.valueOf.call(Mk(Boolean,[],(function(){}))),!0}catch(t){return!1}}();return function(){var i,n=Ak(t);if(e){var o=Ak(this).constructor;i=Mk(n,arguments,o)}else i=n.apply(this,arguments);return Nk(this,i)}}var PE=function(t){zk(i,t);var e=ME(i);function i(t,n,o){var r;return Yd(this,i),(r=e.call(this,t,n,o)).labelOffset=0,r.selected=!1,r}return Kd(i,[{key:"setOptions",value:function(t,e,i){this.options=t,void 0===e&&void 0===i||this.setImages(e,i)}},{key:"setImages",value:function(t,e){e&&this.selected?(this.imageObj=e,this.imageObjAlt=t):(this.imageObj=t,this.imageObjAlt=e)}},{key:"switchImages",value:function(t){var e=t&&!this.selected||!t&&this.selected;if(this.selected=t,void 0!==this.imageObjAlt&&e){var i=this.imageObj;this.imageObj=this.imageObjAlt,this.imageObjAlt=i}}},{key:"_getImagePadding",value:function(){var t={top:0,right:0,bottom:0,left:0};if(this.options.imagePadding){var e=this.options.imagePadding;"object"==Qc(e)?(t.top=e.top,t.right=e.right,t.bottom=e.bottom,t.left=e.left):(t.top=e,t.right=e,t.bottom=e,t.left=e)}return t}},{key:"_resizeImage",value:function(){var t,e;if(!1===this.options.shapeProperties.useImageSize){var i=1,n=1;this.imageObj.width&&this.imageObj.height&&(this.imageObj.width>this.imageObj.height?i=this.imageObj.width/this.imageObj.height:n=this.imageObj.height/this.imageObj.width),t=2*this.options.size*i,e=2*this.options.size*n}else{var o=this._getImagePadding();t=this.imageObj.width+o.left+o.right,e=this.imageObj.height+o.top+o.bottom}this.width=t,this.height=e,this.radius=.5*this.width}},{key:"_drawRawCircle",value:function(t,e,i,n){this.initContextForDraw(t,n),Nn(t,e,i,n.size),this.performFill(t,n)}},{key:"_drawImageAtPosition",value:function(t,e){if(0!=this.imageObj.width){t.globalAlpha=void 0!==e.opacity?e.opacity:1,this.enableShadow(t,e);var i=1;!0===this.options.shapeProperties.interpolation&&(i=this.imageObj.width/this.width/this.body.view.scale);var n=this._getImagePadding(),o=this.left+n.left,r=this.top+n.top,s=this.width-n.left-n.right,a=this.height-n.top-n.bottom;this.imageObj.drawImageAtPosition(t,i,o,r,s,a),this.disableShadow(t,e)}}},{key:"_drawImageLabel",value:function(t,e,i,n,o){var r=0;if(void 0!==this.height){r=.5*this.height;var s=this.labelModule.getTextSize(t,n,o);s.lineCount>=1&&(r+=s.height/2)}var a=i+r;this.options.label&&(this.labelOffset=r),this.labelModule.draw(t,e,a,n,o,"hanging")}}]),i}(CE);function DE(t){var e=function(){if("undefined"==typeof Reflect||!Mk)return!1;if(Mk.sham)return!1;if("function"==typeof Proxy)return!0;try{return Boolean.prototype.valueOf.call(Mk(Boolean,[],(function(){}))),!0}catch(t){return!1}}();return function(){var i,n=Ak(t);if(e){var o=Ak(this).constructor;i=Mk(n,arguments,o)}else i=n.apply(this,arguments);return Nk(this,i)}}var IE=function(t){zk(i,t);var e=DE(i);function i(t,n,o){var r;return Yd(this,i),(r=e.call(this,t,n,o))._setMargins(o),r}return Kd(i,[{key:"resize",value:function(t){var e=arguments.length>1&&void 0!==arguments[1]?arguments[1]:this.selected,i=arguments.length>2&&void 0!==arguments[2]?arguments[2]:this.hover;if(this.needsRefresh(e,i)){var n=this.getDimensionsFromLabel(t,e,i),o=Math.max(n.width+this.margin.right+this.margin.left,n.height+this.margin.top+this.margin.bottom);this.options.size=o/2,this.width=o,this.height=o,this.radius=this.width/2}}},{key:"draw",value:function(t,e,i,n,o,r){this.resize(t,n,o),this.left=e-this.width/2,this.top=i-this.height/2,this._drawRawCircle(t,e,i,r),this.updateBoundingBox(e,i),this.labelModule.draw(t,this.left+this.textSize.width/2+this.margin.left,i,n,o)}},{key:"updateBoundingBox",value:function(t,e){this.boundingBox.top=e-this.options.size,this.boundingBox.left=t-this.options.size,this.boundingBox.right=t+this.options.size,this.boundingBox.bottom=e+this.options.size}},{key:"distanceToBorder",value:function(t){return t&&this.resize(t),.5*this.width}}]),i}(PE);function BE(t){var e=function(){if("undefined"==typeof Reflect||!Mk)return!1;if(Mk.sham)return!1;if("function"==typeof Proxy)return!0;try{return Boolean.prototype.valueOf.call(Mk(Boolean,[],(function(){}))),!0}catch(t){return!1}}();return function(){var i,n=Ak(t);if(e){var o=Ak(this).constructor;i=Mk(n,arguments,o)}else i=n.apply(this,arguments);return Nk(this,i)}}var zE=function(t){zk(i,t);var e=BE(i);function i(t,n,o,r,s){var a;return Yd(this,i),(a=e.call(this,t,n,o)).setImages(r,s),a}return Kd(i,[{key:"resize",value:function(t){var e=arguments.length>1&&void 0!==arguments[1]?arguments[1]:this.selected,i=arguments.length>2&&void 0!==arguments[2]?arguments[2]:this.hover,n=void 0===this.imageObj.src||void 0===this.imageObj.width||void 0===this.imageObj.height;if(n){var o=2*this.options.size;return this.width=o,this.height=o,void(this.radius=.5*this.width)}this.needsRefresh(e,i)&&this._resizeImage()}},{key:"draw",value:function(t,e,i,n,o,r){this.switchImages(n),this.resize();var s=e,a=i;"top-left"===this.options.shapeProperties.coordinateOrigin?(this.left=e,this.top=i,s+=this.width/2,a+=this.height/2):(this.left=e-this.width/2,this.top=i-this.height/2),this._drawRawCircle(t,s,a,r),t.save(),t.clip(),this._drawImageAtPosition(t,r),t.restore(),this._drawImageLabel(t,s,a,n,o),this.updateBoundingBox(e,i)}},{key:"updateBoundingBox",value:function(t,e){"top-left"===this.options.shapeProperties.coordinateOrigin?(this.boundingBox.top=e,this.boundingBox.left=t,this.boundingBox.right=t+2*this.options.size,this.boundingBox.bottom=e+2*this.options.size):(this.boundingBox.top=e-this.options.size,this.boundingBox.left=t-this.options.size,this.boundingBox.right=t+this.options.size,this.boundingBox.bottom=e+this.options.size),this.boundingBox.left=Math.min(this.boundingBox.left,this.labelModule.size.left),this.boundingBox.right=Math.max(this.boundingBox.right,this.labelModule.size.left+this.labelModule.size.width),this.boundingBox.bottom=Math.max(this.boundingBox.bottom,this.boundingBox.bottom+this.labelOffset)}},{key:"distanceToBorder",value:function(t){return t&&this.resize(t),.5*this.width}}]),i}(PE);function NE(t){var e=function(){if("undefined"==typeof Reflect||!Mk)return!1;if(Mk.sham)return!1;if("function"==typeof Proxy)return!0;try{return Boolean.prototype.valueOf.call(Mk(Boolean,[],(function(){}))),!0}catch(t){return!1}}();return function(){var i,n=Ak(t);if(e){var o=Ak(this).constructor;i=Mk(n,arguments,o)}else i=n.apply(this,arguments);return Nk(this,i)}}var FE=function(t){zk(i,t);var e=NE(i);function i(t,n,o){return Yd(this,i),e.call(this,t,n,o)}return Kd(i,[{key:"resize",value:function(t){var e=arguments.length>1&&void 0!==arguments[1]?arguments[1]:this.selected,i=arguments.length>2&&void 0!==arguments[2]?arguments[2]:this.hover,n=arguments.length>3&&void 0!==arguments[3]?arguments[3]:{size:this.options.size};if(this.needsRefresh(e,i)){var o,r;this.labelModule.getTextSize(t,e,i);var s=2*n.size;this.width=null!==(o=this.customSizeWidth)&&void 0!==o?o:s,this.height=null!==(r=this.customSizeHeight)&&void 0!==r?r:s,this.radius=.5*this.width}}},{key:"_drawShape",value:function(t,e,i,n,o,r,s,a){var h,l=this;return this.resize(t,r,s,a),this.left=n-this.width/2,this.top=o-this.height/2,this.initContextForDraw(t,a),(h=e,Object.prototype.hasOwnProperty.call(Ln,h)?Ln[h]:function(t){for(var e=arguments.length,i=new Array(e>1?e-1:0),n=1;n<e;n++)i[n-1]=arguments[n];CanvasRenderingContext2D.prototype[h].call(t,i)})(t,n,o,a.size),this.performFill(t,a),void 0!==this.options.icon&&void 0!==this.options.icon.code&&(t.font=(r?"bold ":"")+this.height/2+"px "+(this.options.icon.face||"FontAwesome"),t.fillStyle=this.options.icon.color||"black",t.textAlign="center",t.textBaseline="middle",t.fillText(this.options.icon.code,n,o)),{drawExternalLabel:function(){if(void 0!==l.options.label){l.labelModule.calculateLabelSize(t,r,s,n,o,"hanging");var e=o+.5*l.height+.5*l.labelModule.size.height;l.labelModule.draw(t,n,e,r,s,"hanging")}l.updateBoundingBox(n,o)}}}},{key:"updateBoundingBox",value:function(t,e){this.boundingBox.top=e-this.options.size,this.boundingBox.left=t-this.options.size,this.boundingBox.right=t+this.options.size,this.boundingBox.bottom=e+this.options.size,void 0!==this.options.label&&this.labelModule.size.width>0&&(this.boundingBox.left=Math.min(this.boundingBox.left,this.labelModule.size.left),this.boundingBox.right=Math.max(this.boundingBox.right,this.labelModule.size.left+this.labelModule.size.width),this.boundingBox.bottom=Math.max(this.boundingBox.bottom,this.boundingBox.bottom+this.labelModule.size.height))}}]),i}(CE);function AE(t,e){var i=bu(t);if(hd){var n=hd(t);e&&(n=Xf(n).call(n,(function(e){return bd(t,e).enumerable}))),i.push.apply(i,n)}return i}function jE(t){for(var e=1;e<arguments.length;e++){var i,n,o=null!=arguments[e]?arguments[e]:{};e%2?Fu(i=AE(Object(o),!0)).call(i,(function(e){$d(t,e,o[e])})):Pd?Ad(t,Pd(o)):Fu(n=AE(Object(o))).call(n,(function(e){Ud(t,e,bd(o,e))}))}return t}function RE(t){var e=function(){if("undefined"==typeof Reflect||!Mk)return!1;if(Mk.sham)return!1;if("function"==typeof Proxy)return!0;try{return Boolean.prototype.valueOf.call(Mk(Boolean,[],(function(){}))),!0}catch(t){return!1}}();return function(){var i,n=Ak(t);if(e){var o=Ak(this).constructor;i=Mk(n,arguments,o)}else i=n.apply(this,arguments);return Nk(this,i)}}var LE=function(t){zk(i,t);var e=RE(i);function i(t,n,o,r){var s;return Yd(this,i),(s=e.call(this,t,n,o,r)).ctxRenderer=r,s}return Kd(i,[{key:"draw",value:function(t,e,i,n,o,r){this.resize(t,n,o,r),this.left=e-this.width/2,this.top=i-this.height/2,t.save();var s=this.ctxRenderer({ctx:t,id:this.options.id,x:e,y:i,state:{selected:n,hover:o},style:jE({},r),label:this.options.label});if(null!=s.drawNode&&s.drawNode(),t.restore(),s.drawExternalLabel){var a=s.drawExternalLabel;s.drawExternalLabel=function(){t.save(),a(),t.restore()}}return s.nodeDimensions&&(this.customSizeWidth=s.nodeDimensions.width,this.customSizeHeight=s.nodeDimensions.height),s}},{key:"distanceToBorder",value:function(t,e){return this._distanceToBorder(t,e)}}]),i}(FE);function HE(t){var e=function(){if("undefined"==typeof Reflect||!Mk)return!1;if(Mk.sham)return!1;if("function"==typeof Proxy)return!0;try{return Boolean.prototype.valueOf.call(Mk(Boolean,[],(function(){}))),!0}catch(t){return!1}}();return function(){var i,n=Ak(t);if(e){var o=Ak(this).constructor;i=Mk(n,arguments,o)}else i=n.apply(this,arguments);return Nk(this,i)}}var WE=function(t){zk(i,t);var e=HE(i);function i(t,n,o){var r;return Yd(this,i),(r=e.call(this,t,n,o))._setMargins(o),r}return Kd(i,[{key:"resize",value:function(t,e,i){if(this.needsRefresh(e,i)){var n=this.getDimensionsFromLabel(t,e,i).width+this.margin.right+this.margin.left;this.width=n,this.height=n,this.radius=this.width/2}}},{key:"draw",value:function(t,e,i,n,o,r){this.resize(t,n,o),this.left=e-this.width/2,this.top=i-this.height/2,this.initContextForDraw(t,r),jn(t,e-this.width/2,i-this.height/2,this.width,this.height),this.performFill(t,r),this.updateBoundingBox(e,i,t,n,o),this.labelModule.draw(t,this.left+this.textSize.width/2+this.margin.left,this.top+this.textSize.height/2+this.margin.top,n,o)}},{key:"distanceToBorder",value:function(t,e){return this._distanceToBorder(t,e)}}]),i}(CE);function qE(t){var e=function(){if("undefined"==typeof Reflect||!Mk)return!1;if(Mk.sham)return!1;if("function"==typeof Proxy)return!0;try{return Boolean.prototype.valueOf.call(Mk(Boolean,[],(function(){}))),!0}catch(t){return!1}}();return function(){var i,n=Ak(t);if(e){var o=Ak(this).constructor;i=Mk(n,arguments,o)}else i=n.apply(this,arguments);return Nk(this,i)}}var VE=function(t){zk(i,t);var e=qE(i);function i(t,n,o){return Yd(this,i),e.call(this,t,n,o)}return Kd(i,[{key:"draw",value:function(t,e,i,n,o,r){return this._drawShape(t,"diamond",4,e,i,n,o,r)}},{key:"distanceToBorder",value:function(t,e){return this._distanceToBorder(t,e)}}]),i}(FE);function UE(t){var e=function(){if("undefined"==typeof Reflect||!Mk)return!1;if(Mk.sham)return!1;if("function"==typeof Proxy)return!0;try{return Boolean.prototype.valueOf.call(Mk(Boolean,[],(function(){}))),!0}catch(t){return!1}}();return function(){var i,n=Ak(t);if(e){var o=Ak(this).constructor;i=Mk(n,arguments,o)}else i=n.apply(this,arguments);return Nk(this,i)}}var YE=function(t){zk(i,t);var e=UE(i);function i(t,n,o){return Yd(this,i),e.call(this,t,n,o)}return Kd(i,[{key:"draw",value:function(t,e,i,n,o,r){return this._drawShape(t,"circle",2,e,i,n,o,r)}},{key:"distanceToBorder",value:function(t){return t&&this.resize(t),this.options.size}}]),i}(FE);function XE(t){var e=function(){if("undefined"==typeof Reflect||!Mk)return!1;if(Mk.sham)return!1;if("function"==typeof Proxy)return!0;try{return Boolean.prototype.valueOf.call(Mk(Boolean,[],(function(){}))),!0}catch(t){return!1}}();return function(){var i,n=Ak(t);if(e){var o=Ak(this).constructor;i=Mk(n,arguments,o)}else i=n.apply(this,arguments);return Nk(this,i)}}var GE=function(t){zk(i,t);var e=XE(i);function i(t,n,o){return Yd(this,i),e.call(this,t,n,o)}return Kd(i,[{key:"resize",value:function(t){var e=arguments.length>1&&void 0!==arguments[1]?arguments[1]:this.selected,i=arguments.length>2&&void 0!==arguments[2]?arguments[2]:this.hover;if(this.needsRefresh(e,i)){var n=this.getDimensionsFromLabel(t,e,i);this.height=2*n.height,this.width=n.width+n.height,this.radius=.5*this.width}}},{key:"draw",value:function(t,e,i,n,o,r){this.resize(t,n,o),this.left=e-.5*this.width,this.top=i-.5*this.height,this.initContextForDraw(t,r),An(t,this.left,this.top,this.width,this.height),this.performFill(t,r),this.updateBoundingBox(e,i,t,n,o),this.labelModule.draw(t,e,i,n,o)}},{key:"distanceToBorder",value:function(t,e){t&&this.resize(t);var i=.5*this.width,n=.5*this.height,o=Math.sin(e)*i,r=Math.cos(e)*n;return i*n/Math.sqrt(o*o+r*r)}}]),i}(CE);function KE(t){var e=function(){if("undefined"==typeof Reflect||!Mk)return!1;if(Mk.sham)return!1;if("function"==typeof Proxy)return!0;try{return Boolean.prototype.valueOf.call(Mk(Boolean,[],(function(){}))),!0}catch(t){return!1}}();return function(){var i,n=Ak(t);if(e){var o=Ak(this).constructor;i=Mk(n,arguments,o)}else i=n.apply(this,arguments);return Nk(this,i)}}var $E=function(t){zk(i,t);var e=KE(i);function i(t,n,o){var r;return Yd(this,i),(r=e.call(this,t,n,o))._setMargins(o),r}return Kd(i,[{key:"resize",value:function(t,e,i){this.needsRefresh(e,i)&&(this.iconSize={width:Number(this.options.icon.size),height:Number(this.options.icon.size)},this.width=this.iconSize.width+this.margin.right+this.margin.left,this.height=this.iconSize.height+this.margin.top+this.margin.bottom,this.radius=.5*this.width)}},{key:"draw",value:function(t,e,i,n,o,r){var s=this;return this.resize(t,n,o),this.options.icon.size=this.options.icon.size||50,this.left=e-this.width/2,this.top=i-this.height/2,this._icon(t,e,i,n,o,r),{drawExternalLabel:function(){if(void 0!==s.options.label){s.labelModule.draw(t,s.left+s.iconSize.width/2+s.margin.left,i+s.height/2+5,n)}s.updateBoundingBox(e,i)}}}},{key:"updateBoundingBox",value:function(t,e){if(this.boundingBox.top=e-.5*this.options.icon.size,this.boundingBox.left=t-.5*this.options.icon.size,this.boundingBox.right=t+.5*this.options.icon.size,this.boundingBox.bottom=e+.5*this.options.icon.size,void 0!==this.options.label&&this.labelModule.size.width>0){this.boundingBox.left=Math.min(this.boundingBox.left,this.labelModule.size.left),this.boundingBox.right=Math.max(this.boundingBox.right,this.labelModule.size.left+this.labelModule.size.width),this.boundingBox.bottom=Math.max(this.boundingBox.bottom,this.boundingBox.bottom+this.labelModule.size.height+5)}}},{key:"_icon",value:function(t,e,i,n,o,r){var s=Number(this.options.icon.size);void 0!==this.options.icon.code?(t.font=[null!=this.options.icon.weight?this.options.icon.weight:n?"bold":"",(null!=this.options.icon.weight&&n?5:0)+s+"px",this.options.icon.face].join(" "),t.fillStyle=this.options.icon.color||"black",t.textAlign="center",t.textBaseline="middle",this.enableShadow(t,r),t.fillText(this.options.icon.code,e,i),this.disableShadow(t,r)):console.error("When using the icon shape, you need to define the code in the icon options object. This can be done per node or globally.")}},{key:"distanceToBorder",value:function(t,e){return this._distanceToBorder(t,e)}}]),i}(CE);function ZE(t){var e=function(){if("undefined"==typeof Reflect||!Mk)return!1;if(Mk.sham)return!1;if("function"==typeof Proxy)return!0;try{return Boolean.prototype.valueOf.call(Mk(Boolean,[],(function(){}))),!0}catch(t){return!1}}();return function(){var i,n=Ak(t);if(e){var o=Ak(this).constructor;i=Mk(n,arguments,o)}else i=n.apply(this,arguments);return Nk(this,i)}}var QE=function(t){zk(i,t);var e=ZE(i);function i(t,n,o,r,s){var a;return Yd(this,i),(a=e.call(this,t,n,o)).setImages(r,s),a}return Kd(i,[{key:"resize",value:function(t){var e=arguments.length>1&&void 0!==arguments[1]?arguments[1]:this.selected,i=arguments.length>2&&void 0!==arguments[2]?arguments[2]:this.hover,n=void 0===this.imageObj.src||void 0===this.imageObj.width||void 0===this.imageObj.height;if(n){var o=2*this.options.size;return this.width=o,void(this.height=o)}this.needsRefresh(e,i)&&this._resizeImage()}},{key:"draw",value:function(t,e,i,n,o,r){t.save(),this.switchImages(n),this.resize();var s=e,a=i;if("top-left"===this.options.shapeProperties.coordinateOrigin?(this.left=e,this.top=i,s+=this.width/2,a+=this.height/2):(this.left=e-this.width/2,this.top=i-this.height/2),!0===this.options.shapeProperties.useBorderWithImage){var h=this.options.borderWidth,l=this.options.borderWidthSelected||2*this.options.borderWidth,d=(n?l:h)/this.body.view.scale;t.lineWidth=Math.min(this.width,d),t.beginPath();var c=n?this.options.color.highlight.border:o?this.options.color.hover.border:this.options.color.border,u=n?this.options.color.highlight.background:o?this.options.color.hover.background:this.options.color.background;void 0!==r.opacity&&(c=pm(c,r.opacity),u=pm(u,r.opacity)),t.strokeStyle=c,t.fillStyle=u,t.rect(this.left-.5*t.lineWidth,this.top-.5*t.lineWidth,this.width+t.lineWidth,this.height+t.lineWidth),jv(t).call(t),this.performStroke(t,r),t.closePath()}this._drawImageAtPosition(t,r),this._drawImageLabel(t,s,a,n,o),this.updateBoundingBox(e,i),t.restore()}},{key:"updateBoundingBox",value:function(t,e){this.resize(),"top-left"===this.options.shapeProperties.coordinateOrigin?(this.left=t,this.top=e):(this.left=t-this.width/2,this.top=e-this.height/2),this.boundingBox.left=this.left,this.boundingBox.top=this.top,this.boundingBox.bottom=this.top+this.height,this.boundingBox.right=this.left+this.width,void 0!==this.options.label&&this.labelModule.size.width>0&&(this.boundingBox.left=Math.min(this.boundingBox.left,this.labelModule.size.left),this.boundingBox.right=Math.max(this.boundingBox.right,this.labelModule.size.left+this.labelModule.size.width),this.boundingBox.bottom=Math.max(this.boundingBox.bottom,this.boundingBox.bottom+this.labelOffset))}},{key:"distanceToBorder",value:function(t,e){return this._distanceToBorder(t,e)}}]),i}(PE);function JE(t){var e=function(){if("undefined"==typeof Reflect||!Mk)return!1;if(Mk.sham)return!1;if("function"==typeof Proxy)return!0;try{return Boolean.prototype.valueOf.call(Mk(Boolean,[],(function(){}))),!0}catch(t){return!1}}();return function(){var i,n=Ak(t);if(e){var o=Ak(this).constructor;i=Mk(n,arguments,o)}else i=n.apply(this,arguments);return Nk(this,i)}}var tO=function(t){zk(i,t);var e=JE(i);function i(t,n,o){return Yd(this,i),e.call(this,t,n,o)}return Kd(i,[{key:"draw",value:function(t,e,i,n,o,r){return this._drawShape(t,"square",2,e,i,n,o,r)}},{key:"distanceToBorder",value:function(t,e){return this._distanceToBorder(t,e)}}]),i}(FE);function eO(t){var e=function(){if("undefined"==typeof Reflect||!Mk)return!1;if(Mk.sham)return!1;if("function"==typeof Proxy)return!0;try{return Boolean.prototype.valueOf.call(Mk(Boolean,[],(function(){}))),!0}catch(t){return!1}}();return function(){var i,n=Ak(t);if(e){var o=Ak(this).constructor;i=Mk(n,arguments,o)}else i=n.apply(this,arguments);return Nk(this,i)}}var iO=function(t){zk(i,t);var e=eO(i);function i(t,n,o){return Yd(this,i),e.call(this,t,n,o)}return Kd(i,[{key:"draw",value:function(t,e,i,n,o,r){return this._drawShape(t,"hexagon",4,e,i,n,o,r)}},{key:"distanceToBorder",value:function(t,e){return this._distanceToBorder(t,e)}}]),i}(FE);function nO(t){var e=function(){if("undefined"==typeof Reflect||!Mk)return!1;if(Mk.sham)return!1;if("function"==typeof Proxy)return!0;try{return Boolean.prototype.valueOf.call(Mk(Boolean,[],(function(){}))),!0}catch(t){return!1}}();return function(){var i,n=Ak(t);if(e){var o=Ak(this).constructor;i=Mk(n,arguments,o)}else i=n.apply(this,arguments);return Nk(this,i)}}var oO=function(t){zk(i,t);var e=nO(i);function i(t,n,o){return Yd(this,i),e.call(this,t,n,o)}return Kd(i,[{key:"draw",value:function(t,e,i,n,o,r){return this._drawShape(t,"star",4,e,i,n,o,r)}},{key:"distanceToBorder",value:function(t,e){return this._distanceToBorder(t,e)}}]),i}(FE);function rO(t){var e=function(){if("undefined"==typeof Reflect||!Mk)return!1;if(Mk.sham)return!1;if("function"==typeof Proxy)return!0;try{return Boolean.prototype.valueOf.call(Mk(Boolean,[],(function(){}))),!0}catch(t){return!1}}();return function(){var i,n=Ak(t);if(e){var o=Ak(this).constructor;i=Mk(n,arguments,o)}else i=n.apply(this,arguments);return Nk(this,i)}}var sO=function(t){zk(i,t);var e=rO(i);function i(t,n,o){var r;return Yd(this,i),(r=e.call(this,t,n,o))._setMargins(o),r}return Kd(i,[{key:"resize",value:function(t,e,i){this.needsRefresh(e,i)&&(this.textSize=this.labelModule.getTextSize(t,e,i),this.width=this.textSize.width+this.margin.right+this.margin.left,this.height=this.textSize.height+this.margin.top+this.margin.bottom,this.radius=.5*this.width)}},{key:"draw",value:function(t,e,i,n,o,r){this.resize(t,n,o),this.left=e-this.width/2,this.top=i-this.height/2,this.enableShadow(t,r),this.labelModule.draw(t,this.left+this.textSize.width/2+this.margin.left,this.top+this.textSize.height/2+this.margin.top,n,o),this.disableShadow(t,r),this.updateBoundingBox(e,i,t,n,o)}},{key:"distanceToBorder",value:function(t,e){return this._distanceToBorder(t,e)}}]),i}(CE);function aO(t){var e=function(){if("undefined"==typeof Reflect||!Mk)return!1;if(Mk.sham)return!1;if("function"==typeof Proxy)return!0;try{return Boolean.prototype.valueOf.call(Mk(Boolean,[],(function(){}))),!0}catch(t){return!1}}();return function(){var i,n=Ak(t);if(e){var o=Ak(this).constructor;i=Mk(n,arguments,o)}else i=n.apply(this,arguments);return Nk(this,i)}}var hO=function(t){zk(i,t);var e=aO(i);function i(t,n,o){return Yd(this,i),e.call(this,t,n,o)}return Kd(i,[{key:"draw",value:function(t,e,i,n,o,r){return this._drawShape(t,"triangle",3,e,i,n,o,r)}},{key:"distanceToBorder",value:function(t,e){return this._distanceToBorder(t,e)}}]),i}(FE);function lO(t){var e=function(){if("undefined"==typeof Reflect||!Mk)return!1;if(Mk.sham)return!1;if("function"==typeof Proxy)return!0;try{return Boolean.prototype.valueOf.call(Mk(Boolean,[],(function(){}))),!0}catch(t){return!1}}();return function(){var i,n=Ak(t);if(e){var o=Ak(this).constructor;i=Mk(n,arguments,o)}else i=n.apply(this,arguments);return Nk(this,i)}}var dO=function(t){zk(i,t);var e=lO(i);function i(t,n,o){return Yd(this,i),e.call(this,t,n,o)}return Kd(i,[{key:"draw",value:function(t,e,i,n,o,r){return this._drawShape(t,"triangleDown",3,e,i,n,o,r)}},{key:"distanceToBorder",value:function(t,e){return this._distanceToBorder(t,e)}}]),i}(FE);function cO(t,e){var i=bu(t);if(hd){var n=hd(t);e&&(n=Xf(n).call(n,(function(e){return bd(t,e).enumerable}))),i.push.apply(i,n)}return i}function uO(t){for(var e=1;e<arguments.length;e++){var i,n,o=null!=arguments[e]?arguments[e]:{};e%2?Fu(i=cO(Object(o),!0)).call(i,(function(e){$d(t,e,o[e])})):Pd?Ad(t,Pd(o)):Fu(n=cO(Object(o))).call(n,(function(e){Ud(t,e,bd(o,e))}))}return t}var fO=function(){function t(e,i,n,o,r,s){Yd(this,t),this.options=Cm(r),this.globalOptions=r,this.defaultOptions=s,this.body=i,this.edges=[],this.id=void 0,this.imagelist=n,this.grouplist=o,this.x=void 0,this.y=void 0,this.baseSize=this.options.size,this.baseFontSize=this.options.font.size,this.predefinedPosition=!1,this.selected=!1,this.hover=!1,this.labelModule=new OE(this.body,this.options,!1),this.setOptions(e)}return Kd(t,[{key:"attachEdge",value:function(t){var e;-1===Fp(e=this.edges).call(e,t)&&this.edges.push(t)}},{key:"detachEdge",value:function(t){var e,i,n=Fp(e=this.edges).call(e,t);-1!=n&&ff(i=this.edges).call(i,n,1)}},{key:"setOptions",value:function(e){var i=this.options.shape;if(e){if(void 0!==e.color&&(this._localColor=e.color),void 0!==e.id&&(this.id=e.id),void 0===this.id)throw new Error("Node must have an id");t.checkMass(e,this.id),void 0!==e.x&&(null===e.x?(this.x=void 0,this.predefinedPosition=!1):(this.x=Ep(e.x),this.predefinedPosition=!0)),void 0!==e.y&&(null===e.y?(this.y=void 0,this.predefinedPosition=!1):(this.y=Ep(e.y),this.predefinedPosition=!0)),void 0!==e.size&&(this.baseSize=e.size),void 0!==e.value&&(e.value=lE(e.value)),t.parseOptions(this.options,e,!0,this.globalOptions,this.grouplist);var n=[e,this.options,this.defaultOptions];return this.chooser=gE("node",n),this._load_images(),this.updateLabelModule(e),void 0!==e.opacity&&t.checkOpacity(e.opacity)&&(this.options.opacity=e.opacity),this.updateShape(i),void 0!==e.hidden||void 0!==e.physics}}},{key:"_load_images",value:function(){if(("circularImage"===this.options.shape||"image"===this.options.shape)&&void 0===this.options.image)throw new Error("Option image must be defined for node type '"+this.options.shape+"'");if(void 0!==this.options.image){if(void 0===this.imagelist)throw new Error("Internal Error: No images provided");if("string"==typeof this.options.image)this.imageObj=this.imagelist.load(this.options.image,this.options.brokenImage,this.id);else{if(void 0===this.options.image.unselected)throw new Error("No unselected image provided");this.imageObj=this.imagelist.load(this.options.image.unselected,this.options.brokenImage,this.id),void 0!==this.options.image.selected?this.imageObjAlt=this.imagelist.load(this.options.image.selected,this.options.brokenImage,this.id):this.imageObjAlt=void 0}}}},{key:"getFormattingValues",value:function(){var t={color:this.options.color.background,opacity:this.options.opacity,borderWidth:this.options.borderWidth,borderColor:this.options.color.border,size:this.options.size,borderDashes:this.options.shapeProperties.borderDashes,borderRadius:this.options.shapeProperties.borderRadius,shadow:this.options.shadow.enabled,shadowColor:this.options.shadow.color,shadowSize:this.options.shadow.size,shadowX:this.options.shadow.x,shadowY:this.options.shadow.y};if(this.selected||this.hover?!0===this.chooser?this.selected?(null!=this.options.borderWidthSelected?t.borderWidth=this.options.borderWidthSelected:t.borderWidth*=2,t.color=this.options.color.highlight.background,t.borderColor=this.options.color.highlight.border,t.shadow=this.options.shadow.enabled):this.hover&&(t.color=this.options.color.hover.background,t.borderColor=this.options.color.hover.border,t.shadow=this.options.shadow.enabled):"function"==typeof this.chooser&&(this.chooser(t,this.options.id,this.selected,this.hover),!1===t.shadow&&(t.shadowColor===this.options.shadow.color&&t.shadowSize===this.options.shadow.size&&t.shadowX===this.options.shadow.x&&t.shadowY===this.options.shadow.y||(t.shadow=!0))):t.shadow=this.options.shadow.enabled,void 0!==this.options.opacity){var e=this.options.opacity;t.borderColor=pm(t.borderColor,e),t.color=pm(t.color,e),t.shadowColor=pm(t.shadowColor,e)}return t}},{key:"updateLabelModule",value:function(e){void 0!==this.options.label&&null!==this.options.label||(this.options.label=""),t.updateGroupOptions(this.options,uO(uO({},e),{},{color:e&&e.color||this._localColor||void 0}),this.grouplist);var i=this.grouplist.get(this.options.group,!1),n=[e,this.options,i,this.globalOptions,this.defaultOptions];this.labelModule.update(this.options,n),void 0!==this.labelModule.baseSize&&(this.baseFontSize=this.labelModule.baseSize)}},{key:"updateShape",value:function(t){if(t===this.options.shape&&this.shape)this.shape.setOptions(this.options,this.imageObj,this.imageObjAlt);else switch(this.options.shape){case"box":this.shape=new TE(this.options,this.body,this.labelModule);break;case"circle":this.shape=new IE(this.options,this.body,this.labelModule);break;case"circularImage":this.shape=new zE(this.options,this.body,this.labelModule,this.imageObj,this.imageObjAlt);break;case"custom":this.shape=new LE(this.options,this.body,this.labelModule,this.options.ctxRenderer);break;case"database":this.shape=new WE(this.options,this.body,this.labelModule);break;case"diamond":this.shape=new VE(this.options,this.body,this.labelModule);break;case"dot":this.shape=new YE(this.options,this.body,this.labelModule);break;case"ellipse":default:this.shape=new GE(this.options,this.body,this.labelModule);break;case"icon":this.shape=new $E(this.options,this.body,this.labelModule);break;case"image":this.shape=new QE(this.options,this.body,this.labelModule,this.imageObj,this.imageObjAlt);break;case"square":this.shape=new tO(this.options,this.body,this.labelModule);break;case"hexagon":this.shape=new iO(this.options,this.body,this.labelModule);break;case"star":this.shape=new oO(this.options,this.body,this.labelModule);break;case"text":this.shape=new sO(this.options,this.body,this.labelModule);break;case"triangle":this.shape=new hO(this.options,this.body,this.labelModule);break;case"triangleDown":this.shape=new dO(this.options,this.body,this.labelModule)}this.needsRefresh()}},{key:"select",value:function(){this.selected=!0,this.needsRefresh()}},{key:"unselect",value:function(){this.selected=!1,this.needsRefresh()}},{key:"needsRefresh",value:function(){this.shape.refreshNeeded=!0}},{key:"getTitle",value:function(){return this.options.title}},{key:"distanceToBorder",value:function(t,e){return this.shape.distanceToBorder(t,e)}},{key:"isFixed",value:function(){return this.options.fixed.x&&this.options.fixed.y}},{key:"isSelected",value:function(){return this.selected}},{key:"getValue",value:function(){return this.options.value}},{key:"getLabelSize",value:function(){return this.labelModule.size()}},{key:"setValueRange",value:function(t,e,i){if(void 0!==this.options.value){var n=this.options.scaling.customScalingFunction(t,e,i,this.options.value),o=this.options.scaling.max-this.options.scaling.min;if(!0===this.options.scaling.label.enabled){var r=this.options.scaling.label.max-this.options.scaling.label.min;this.options.font.size=this.options.scaling.label.min+n*r}this.options.size=this.options.scaling.min+n*o}else this.options.size=this.baseSize,this.options.font.size=this.baseFontSize;this.updateLabelModule()}},{key:"draw",value:function(t){var e=this.getFormattingValues();return this.shape.draw(t,this.x,this.y,this.selected,this.hover,e)||{}}},{key:"updateBoundingBox",value:function(t){this.shape.updateBoundingBox(this.x,this.y,t)}},{key:"resize",value:function(t){var e=this.getFormattingValues();this.shape.resize(t,this.selected,this.hover,e)}},{key:"getItemsOnPoint",value:function(t){var e=[];return this.labelModule.visible()&&yE(this.labelModule.getSize(),t)&&e.push({nodeId:this.id,labelId:0}),yE(this.shape.boundingBox,t)&&e.push({nodeId:this.id}),e}},{key:"isOverlappingWith",value:function(t){return this.shape.left<t.right&&this.shape.left+this.shape.width>t.left&&this.shape.top<t.bottom&&this.shape.top+this.shape.height>t.top}},{key:"isBoundingBoxOverlappingWith",value:function(t){return this.shape.boundingBox.left<t.right&&this.shape.boundingBox.right>t.left&&this.shape.boundingBox.top<t.bottom&&this.shape.boundingBox.bottom>t.top}}],[{key:"checkOpacity",value:function(t){return 0<=t&&t<=1}},{key:"checkCoordinateOrigin",value:function(t){return void 0===t||"center"===t||"top-left"===t}},{key:"updateGroupOptions",value:function(e,i,n){var o;if(void 0!==n){var r=e.group;if(void 0!==i&&void 0!==i.group&&r!==i.group)throw new Error("updateGroupOptions: group values in options don't match.");if("number"==typeof r||"string"==typeof r&&""!=r){var s=n.get(r);void 0!==s.opacity&&void 0===i.opacity&&(t.checkOpacity(s.opacity)||(console.error("Invalid option for node opacity. Value must be between 0 and 1, found: "+s.opacity),s.opacity=void 0));var a=Xf(o=vE(i)).call(o,(function(t){return null!=i[t]}));a.push("font"),im(a,e,s),e.color=gm(e.color)}}}},{key:"parseOptions",value:function(e,i){var n=arguments.length>2&&void 0!==arguments[2]&&arguments[2],o=arguments.length>3&&void 0!==arguments[3]?arguments[3]:{},r=arguments.length>4?arguments[4]:void 0,s=["color","fixed","shadow"];if(im(s,e,i,n),t.checkMass(i),void 0!==e.opacity&&(t.checkOpacity(e.opacity)||(console.error("Invalid option for node opacity. Value must be between 0 and 1, found: "+e.opacity),e.opacity=void 0)),void 0!==i.opacity&&(t.checkOpacity(i.opacity)||(console.error("Invalid option for node opacity. Value must be between 0 and 1, found: "+i.opacity),i.opacity=void 0)),i.shapeProperties&&!t.checkCoordinateOrigin(i.shapeProperties.coordinateOrigin)&&console.error("Invalid option for node coordinateOrigin, found: "+i.shapeProperties.coordinateOrigin),Sm(e,i,"shadow",o),void 0!==i.color&&null!==i.color){var a=gm(i.color);Jy(e.color,a)}else!0===n&&null===i.color&&(e.color=Cm(o.color));void 0!==i.fixed&&null!==i.fixed&&("boolean"==typeof i.fixed?(e.fixed.x=i.fixed,e.fixed.y=i.fixed):(void 0!==i.fixed.x&&"boolean"==typeof i.fixed.x&&(e.fixed.x=i.fixed.x),void 0!==i.fixed.y&&"boolean"==typeof i.fixed.y&&(e.fixed.y=i.fixed.y))),!0===n&&null===i.font&&(e.font=Cm(o.font)),t.updateGroupOptions(e,i,r),void 0!==i.scaling&&Sm(e.scaling,i.scaling,"label",o.scaling)}},{key:"checkMass",value:function(t,e){if(void 0!==t.mass&&t.mass<=0){var i="";void 0!==e&&(i=" in node id: "+e),console.error("%cNegative or zero mass disallowed"+i+", setting mass to 1.",Vm),t.mass=1}}}]),t}();function pO(t,e){var i=void 0!==tu&&ih(t)||t["@@iterator"];if(!i){if(lu(t)||(i=function(t,e){var i;if(!t)return;if("string"==typeof t)return vO(t,e);var n=au(i=Object.prototype.toString.call(t)).call(i,8,-1);"Object"===n&&t.constructor&&(n=t.constructor.name);if("Map"===n||"Set"===n)return ja(t);if("Arguments"===n||/^(?:Ui|I)nt(?:8|16|32)(?:Clamped)?Array$/.test(n))return vO(t,e)}(t))||e&&t&&"number"==typeof t.length){i&&(t=i);var n=0,o=function(){};return{s:o,n:function(){return n>=t.length?{done:!0}:{done:!1,value:t[n++]}},e:function(t){throw t},f:o}}throw new TypeError("Invalid attempt to iterate non-iterable instance.\nIn order to be iterable, non-array objects must have a [Symbol.iterator]() method.")}var r,s=!0,a=!1;return{s:function(){i=i.call(t)},n:function(){var t=i.next();return s=t.done,t},e:function(t){a=!0,r=t},f:function(){try{s||null==i.return||i.return()}finally{if(a)throw r}}}}function vO(t,e){(null==e||e>t.length)&&(e=t.length);for(var i=0,n=new Array(e);i<e;i++)n[i]=t[i];return n}var gO=function(){function t(e,i,n,o){var r,s=this;if(Yd(this,t),this.body=e,this.images=i,this.groups=n,this.layoutEngine=o,this.body.functions.createNode=zn(r=this.create).call(r,this),this.nodesListeners={add:function(t,e){s.add(e.items)},update:function(t,e){s.update(e.items,e.data,e.oldData)},remove:function(t,e){s.remove(e.items)}},this.defaultOptions={borderWidth:1,borderWidthSelected:void 0,brokenImage:void 0,color:{border:"#2B7CE9",background:"#97C2FC",highlight:{border:"#2B7CE9",background:"#D2E5FF"},hover:{border:"#2B7CE9",background:"#D2E5FF"}},opacity:void 0,fixed:{x:!1,y:!1},font:{color:"#343434",size:14,face:"arial",background:"none",strokeWidth:0,strokeColor:"#ffffff",align:"center",vadjust:0,multi:!1,bold:{mod:"bold"},boldital:{mod:"bold italic"},ital:{mod:"italic"},mono:{mod:"",size:15,face:"monospace",vadjust:2}},group:void 0,hidden:!1,icon:{face:"FontAwesome",code:void 0,size:50,color:"#2B7CE9"},image:void 0,imagePadding:{top:0,right:0,bottom:0,left:0},label:void 0,labelHighlightBold:!0,level:void 0,margin:{top:5,right:5,bottom:5,left:5},mass:1,physics:!0,scaling:{min:10,max:30,label:{enabled:!1,min:14,max:30,maxVisible:30,drawThreshold:5},customScalingFunction:function(t,e,i,n){if(e===t)return.5;var o=1/(e-t);return Math.max(0,(n-t)*o)}},shadow:{enabled:!1,color:"rgba(0,0,0,0.5)",size:10,x:5,y:5},shape:"ellipse",shapeProperties:{borderDashes:!1,borderRadius:6,interpolation:!0,useImageSize:!1,useBorderWithImage:!1,coordinateOrigin:"center"},size:25,title:void 0,value:void 0,x:void 0,y:void 0},this.defaultOptions.mass<=0)throw"Internal error: mass in defaultOptions of NodesHandler may not be zero or negative";this.options=Cm(this.defaultOptions),this.bindEventListeners()}return Kd(t,[{key:"bindEventListeners",value:function(){var t,e,i=this;this.body.emitter.on("refreshNodes",zn(t=this.refresh).call(t,this)),this.body.emitter.on("refresh",zn(e=this.refresh).call(e,this)),this.body.emitter.on("destroy",(function(){hm(i.nodesListeners,(function(t,e){i.body.data.nodes&&i.body.data.nodes.off(e,t)})),delete i.body.functions.createNode,delete i.nodesListeners.add,delete i.nodesListeners.update,delete i.nodesListeners.remove,delete i.nodesListeners}))}},{key:"setOptions",value:function(t){if(void 0!==t){if(fO.parseOptions(this.options,t),void 0!==t.opacity&&(ek(t.opacity)||!ok(t.opacity)||t.opacity<0||t.opacity>1?console.error("Invalid option for node opacity. Value must be between 0 and 1, found: "+t.opacity):this.options.opacity=t.opacity),void 0!==t.shape)for(var e in this.body.nodes)Object.prototype.hasOwnProperty.call(this.body.nodes,e)&&this.body.nodes[e].updateShape();if(void 0!==t.font||void 0!==t.widthConstraint||void 0!==t.heightConstraint)for(var i=0,n=bu(this.body.nodes);i<n.length;i++){var o=n[i];this.body.nodes[o].updateLabelModule(),this.body.nodes[o].needsRefresh()}if(void 0!==t.size)for(var r in this.body.nodes)Object.prototype.hasOwnProperty.call(this.body.nodes,r)&&this.body.nodes[r].needsRefresh();void 0===t.hidden&&void 0===t.physics||this.body.emitter.emit("_dataChanged")}}},{key:"setData",value:function(t){var e=arguments.length>1&&void 0!==arguments[1]&&arguments[1],i=this.body.data.nodes;if(Qx("id",t))this.body.data.nodes=t;else if(lu(t))this.body.data.nodes=new Kx,this.body.data.nodes.add(t);else{if(t)throw new TypeError("Array or DataSet expected");this.body.data.nodes=new Kx}if(i&&hm(this.nodesListeners,(function(t,e){i.off(e,t)})),this.body.nodes={},this.body.data.nodes){var n=this;hm(this.nodesListeners,(function(t,e){n.body.data.nodes.on(e,t)}));var o=this.body.data.nodes.getIds();this.add(o,!0)}!1===e&&this.body.emitter.emit("_dataChanged")}},{key:"add",value:function(t){for(var e,i=arguments.length>1&&void 0!==arguments[1]&&arguments[1],n=[],o=0;o<t.length;o++){e=t[o];var r=this.body.data.nodes.get(e),s=this.create(r);n.push(s),this.body.nodes[e]=s}this.layoutEngine.positionInitially(n),!1===i&&this.body.emitter.emit("_dataChanged")}},{key:"update",value:function(t,e,i){for(var n=this.body.nodes,o=!1,r=0;r<t.length;r++){var s=t[r],a=n[s],h=e[r];void 0!==a?a.setOptions(h)&&(o=!0):(o=!0,a=this.create(h),n[s]=a)}o||void 0===i||(o=ck(e).call(e,(function(t,e){var n=i[e];return n&&n.level!==t.level}))),!0===o?this.body.emitter.emit("_dataChanged"):this.body.emitter.emit("_dataUpdated")}},{key:"remove",value:function(t){for(var e=this.body.nodes,i=0;i<t.length;i++){delete e[t[i]]}this.body.emitter.emit("_dataChanged")}},{key:"create",value:function(t){var e=arguments.length>1&&void 0!==arguments[1]?arguments[1]:fO;return new e(t,this.body,this.images,this.groups,this.options,this.defaultOptions)}},{key:"refresh",value:function(){var t=this,e=arguments.length>0&&void 0!==arguments[0]&&arguments[0];hm(this.body.nodes,(function(i,n){var o=t.body.data.nodes.get(n);void 0!==o&&(!0===e&&i.setOptions({x:null,y:null}),i.setOptions({fixed:!1}),i.setOptions(o))}))}},{key:"getPositions",value:function(t){var e={};if(void 0!==t){if(!0===lu(t)){for(var i=0;i<t.length;i++)if(void 0!==this.body.nodes[t[i]]){var n=this.body.nodes[t[i]];e[t[i]]={x:Math.round(n.x),y:Math.round(n.y)}}}else if(void 0!==this.body.nodes[t]){var o=this.body.nodes[t];e[t]={x:Math.round(o.x),y:Math.round(o.y)}}}else for(var r=0;r<this.body.nodeIndices.length;r++){var s=this.body.nodes[this.body.nodeIndices[r]];e[this.body.nodeIndices[r]]={x:Math.round(s.x),y:Math.round(s.y)}}return e}},{key:"getPosition",value:function(t){if(null==t)throw new TypeError("No id was specified for getPosition method.");if(null==this.body.nodes[t])throw new ReferenceError("NodeId provided for getPosition does not exist. Provided: ".concat(t));return{x:Math.round(this.body.nodes[t].x),y:Math.round(this.body.nodes[t].y)}}},{key:"storePositions",value:function(){var t,e=[],i=this.body.data.nodes.getDataSet(),n=pO(i.get());try{for(n.s();!(t=n.n()).done;){var o=t.value,r=o.id,s=this.body.nodes[r],a=Math.round(s.x),h=Math.round(s.y);o.x===a&&o.y===h||e.push({id:r,x:a,y:h})}}catch(t){n.e(t)}finally{n.f()}i.update(e)}},{key:"getBoundingBox",value:function(t){if(void 0!==this.body.nodes[t])return this.body.nodes[t].shape.boundingBox}},{key:"getConnectedNodes",value:function(t,e){var i=[];if(void 0!==this.body.nodes[t])for(var n=this.body.nodes[t],o={},r=0;r<n.edges.length;r++){var s=n.edges[r];"to"!==e&&s.toId==n.id?void 0===o[s.fromId]&&(i.push(s.fromId),o[s.fromId]=!0):"from"!==e&&s.fromId==n.id&&void 0===o[s.toId]&&(i.push(s.toId),o[s.toId]=!0)}return i}},{key:"getConnectedEdges",value:function(t){var e=[];if(void 0!==this.body.nodes[t])for(var i=this.body.nodes[t],n=0;n<i.edges.length;n++)e.push(i.edges[n].id);else console.error("NodeId provided for getConnectedEdges does not exist. Provided: ",t);return e}},{key:"moveNode",value:function(t,e,i){var n=this;void 0!==this.body.nodes[t]?(this.body.nodes[t].x=Number(e),this.body.nodes[t].y=Number(i),Sv((function(){n.body.emitter.emit("startSimulation")}),0)):console.error("Node id supplied to moveNode does not exist. Provided: ",t)}}]),t}(),yO=Wt,mO=_,bO=Y,wO=$e,kO=function(t){return void 0!==t&&(yO(t,"value")||yO(t,"writable"))},_O=m,xO=Pr;_i({target:"Reflect",stat:!0},{get:function t(e,i){var n,o,r=arguments.length<3?e:arguments[2];return wO(e)===r?e[i]:(n=_O.f(e,i))?kO(n)?n.value:void 0===n.get?void 0:mO(n.get,r):bO(o=xO(e))?t(o,i,r):void 0}});var EO=X.Reflect.get,OO=md;function CO(t,e){for(;!Object.prototype.hasOwnProperty.call(t,e)&&null!==(t=Ak(t)););return t}function SO(){return SO="undefined"!=typeof Reflect&&EO?EO:function(t,e,i){var n=CO(t,e);if(n){var o=OO(n,e);return o.get?o.get.call(arguments.length<3?t:i):o.value}},SO.apply(this,arguments)}var TO=_i,MO=Math.hypot,PO=Math.abs,DO=Math.sqrt;TO({target:"Math",stat:!0,forced:!!MO&&MO(1/0,NaN)!==1/0},{hypot:function(t,e){for(var i,n,o=0,r=0,s=arguments.length,a=0;r<s;)a<(i=PO(arguments[r++]))?(o=o*(n=a/i)*n+1,a=i):o+=i>0?(n=i/a)*n:i;return a===1/0?1/0:a*DO(o)}});var IO=X.Math.hypot;function BO(t){var e=function(){if("undefined"==typeof Reflect||!Mk)return!1;if(Mk.sham)return!1;if("function"==typeof Proxy)return!0;try{return Boolean.prototype.valueOf.call(Mk(Boolean,[],(function(){}))),!0}catch(t){return!1}}();return function(){var i,n=Ak(t);if(e){var o=Ak(this).constructor;i=Mk(n,arguments,o)}else i=n.apply(this,arguments);return Nk(this,i)}}var zO=function(){function t(){Yd(this,t)}return Kd(t,null,[{key:"transform",value:function(t,e){lu(t)||(t=[t]);for(var i=e.point.x,n=e.point.y,o=e.angle,r=e.length,s=0;s<t.length;++s){var a=t[s],h=a.x*Math.cos(o)-a.y*Math.sin(o),l=a.x*Math.sin(o)+a.y*Math.cos(o);a.x=i+r*h,a.y=n+r*l}}},{key:"drawPath",value:function(t,e){t.beginPath(),t.moveTo(e[0].x,e[0].y);for(var i=1;i<e.length;++i)t.lineTo(e[i].x,e[i].y);t.closePath()}}]),t}(),NO=function(t){zk(i,t);var e=BO(i);function i(){return Yd(this,i),e.apply(this,arguments)}return Kd(i,null,[{key:"draw",value:function(t,e){if(e.image){t.save(),t.translate(e.point.x,e.point.y),t.rotate(Math.PI/2+e.angle);var i=null!=e.imageWidth?e.imageWidth:e.image.width,n=null!=e.imageHeight?e.imageHeight:e.image.height;e.image.drawImageAtPosition(t,1,-i/2,0,i,n),t.restore()}return!1}}]),i}(zO),FO=function(t){zk(i,t);var e=BO(i);function i(){return Yd(this,i),e.apply(this,arguments)}return Kd(i,null,[{key:"draw",value:function(t,e){var i=[{x:0,y:0},{x:-1,y:.3},{x:-.9,y:0},{x:-1,y:-.3}];return zO.transform(i,e),zO.drawPath(t,i),!0}}]),i}(zO),AO=function(){function t(){Yd(this,t)}return Kd(t,null,[{key:"draw",value:function(t,e){var i=[{x:-1,y:0},{x:0,y:.3},{x:-.4,y:0},{x:0,y:-.3}];return zO.transform(i,e),zO.drawPath(t,i),!0}}]),t}(),jO=function(){function t(){Yd(this,t)}return Kd(t,null,[{key:"draw",value:function(t,e){var i={x:-.4,y:0};zO.transform(i,e),t.strokeStyle=t.fillStyle,t.fillStyle="rgba(0, 0, 0, 0)";var n=Math.PI,o=e.angle-n/2,r=e.angle+n/2;return t.beginPath(),t.arc(i.x,i.y,.4*e.length,o,r,!1),t.stroke(),!0}}]),t}(),RO=function(){function t(){Yd(this,t)}return Kd(t,null,[{key:"draw",value:function(t,e){var i={x:-.3,y:0};zO.transform(i,e),t.strokeStyle=t.fillStyle,t.fillStyle="rgba(0, 0, 0, 0)";var n=Math.PI,o=e.angle+n/2,r=e.angle+3*n/2;return t.beginPath(),t.arc(i.x,i.y,.4*e.length,o,r,!1),t.stroke(),!0}}]),t}(),LO=function(){function t(){Yd(this,t)}return Kd(t,null,[{key:"draw",value:function(t,e){var i=[{x:.02,y:0},{x:-1,y:.3},{x:-1,y:-.3}];return zO.transform(i,e),zO.drawPath(t,i),!0}}]),t}(),HO=function(){function t(){Yd(this,t)}return Kd(t,null,[{key:"draw",value:function(t,e){var i=[{x:0,y:.3},{x:0,y:-.3},{x:-1,y:0}];return zO.transform(i,e),zO.drawPath(t,i),!0}}]),t}(),WO=function(){function t(){Yd(this,t)}return Kd(t,null,[{key:"draw",value:function(t,e){var i={x:-.4,y:0};return zO.transform(i,e),Nn(t,i.x,i.y,.4*e.length),!0}}]),t}(),qO=function(){function t(){Yd(this,t)}return Kd(t,null,[{key:"draw",value:function(t,e){var i=[{x:0,y:.5},{x:0,y:-.5},{x:-.15,y:-.5},{x:-.15,y:.5}];return zO.transform(i,e),zO.drawPath(t,i),!0}}]),t}(),VO=function(){function t(){Yd(this,t)}return Kd(t,null,[{key:"draw",value:function(t,e){var i=[{x:0,y:.3},{x:0,y:-.3},{x:-.6,y:-.3},{x:-.6,y:.3}];return zO.transform(i,e),zO.drawPath(t,i),!0}}]),t}(),UO=function(){function t(){Yd(this,t)}return Kd(t,null,[{key:"draw",value:function(t,e){var i=[{x:0,y:0},{x:-.5,y:-.3},{x:-1,y:0},{x:-.5,y:.3}];return zO.transform(i,e),zO.drawPath(t,i),!0}}]),t}(),YO=function(){function t(){Yd(this,t)}return Kd(t,null,[{key:"draw",value:function(t,e){var i=[{x:-1,y:.3},{x:-.5,y:0},{x:-1,y:-.3},{x:0,y:0}];return zO.transform(i,e),zO.drawPath(t,i),!0}}]),t}(),XO=function(){function t(){Yd(this,t)}return Kd(t,null,[{key:"draw",value:function(t,e){var i;switch(e.type&&(i=e.type.toLowerCase()),i){case"image":return NO.draw(t,e);case"circle":return WO.draw(t,e);case"box":return VO.draw(t,e);case"crow":return AO.draw(t,e);case"curve":return jO.draw(t,e);case"diamond":return UO.draw(t,e);case"inv_curve":return RO.draw(t,e);case"triangle":return LO.draw(t,e);case"inv_triangle":return HO.draw(t,e);case"bar":return qO.draw(t,e);case"vee":return YO.draw(t,e);default:return FO.draw(t,e)}}}]),t}();function GO(t,e){var i=bu(t);if(hd){var n=hd(t);e&&(n=Xf(n).call(n,(function(e){return bd(t,e).enumerable}))),i.push.apply(i,n)}return i}function KO(t){for(var e=1;e<arguments.length;e++){var i,n,o=null!=arguments[e]?arguments[e]:{};e%2?Fu(i=GO(Object(o),!0)).call(i,(function(e){$d(t,e,o[e])})):Pd?Ad(t,Pd(o)):Fu(n=GO(Object(o))).call(n,(function(e){Ud(t,e,bd(o,e))}))}return t}var $O=function(){function t(e,i,n){Yd(this,t),this._body=i,this._labelModule=n,this.color={},this.colorDirty=!0,this.hoverWidth=1.5,this.selectionWidth=2,this.setOptions(e),this.fromPoint=this.from,this.toPoint=this.to}return Kd(t,[{key:"connect",value:function(){this.from=this._body.nodes[this.options.from],this.to=this._body.nodes[this.options.to]}},{key:"cleanup",value:function(){return!1}},{key:"setOptions",value:function(t){this.options=t,this.from=this._body.nodes[this.options.from],this.to=this._body.nodes[this.options.to],this.id=this.options.id}},{key:"drawLine",value:function(t,e,i,n){var o=arguments.length>4&&void 0!==arguments[4]?arguments[4]:this.getViaNode();t.strokeStyle=this.getColor(t,e),t.lineWidth=e.width,!1!==e.dashes?this._drawDashedLine(t,e,o):this._drawLine(t,e,o)}},{key:"_drawLine",value:function(t,e,i,n,o){if(this.from!=this.to)this._line(t,e,i,n,o);else{var r=Kc(this._getCircleData(t),3),s=r[0],a=r[1],h=r[2];this._circle(t,e,s,a,h)}}},{key:"_drawDashedLine",value:function(t,e,i,n,o){t.lineCap="round";var r=lu(e.dashes)?e.dashes:[5,5];if(void 0!==t.setLineDash){if(t.save(),t.setLineDash(r),t.lineDashOffset=0,this.from!=this.to)this._line(t,e,i);else{var s=Kc(this._getCircleData(t),3),a=s[0],h=s[1],l=s[2];this._circle(t,e,a,h,l)}t.setLineDash([0]),t.lineDashOffset=0,t.restore()}else{if(this.from!=this.to)Rn(t,this.from.x,this.from.y,this.to.x,this.to.y,r);else{var d=Kc(this._getCircleData(t),3),c=d[0],u=d[1],f=d[2];this._circle(t,e,c,u,f)}this.enableShadow(t,e),t.stroke(),this.disableShadow(t,e)}}},{key:"findBorderPosition",value:function(t,e,i){return this.from!=this.to?this._findBorderPosition(t,e,i):this._findBorderPositionCircle(t,e,i)}},{key:"findBorderPositions",value:function(t){if(this.from!=this.to)return{from:this._findBorderPosition(this.from,t),to:this._findBorderPosition(this.to,t)};var e,i=Kc(au(e=this._getCircleData(t)).call(e,0,2),2),n=i[0],o=i[1];return{from:this._findBorderPositionCircle(this.from,t,{x:n,y:o,low:.25,high:.6,direction:-1}),to:this._findBorderPositionCircle(this.from,t,{x:n,y:o,low:.6,high:.8,direction:1})}}},{key:"_getCircleData",value:function(t){var e=this.options.selfReference.size;void 0!==t&&void 0===this.from.shape.width&&this.from.shape.resize(t);var i=bE(t,this.options.selfReference.angle,e,this.from);return[i.x,i.y,e]}},{key:"_pointOnCircle",value:function(t,e,i,n){var o=2*n*Math.PI;return{x:t+i*Math.cos(o),y:e-i*Math.sin(o)}}},{key:"_findBorderPositionCircle",value:function(t,e,i){var n,o=i.x,r=i.y,s=i.low,a=i.high,h=i.direction,l=this.options.selfReference.size,d=.5*(s+a),c=0;!0===this.options.arrowStrikethrough&&(-1===h?c=this.options.endPointOffset.from:1===h&&(c=this.options.endPointOffset.to));var u=0;do{d=.5*(s+a),n=this._pointOnCircle(o,r,l,d);var f=Math.atan2(t.y-n.y,t.x-n.x),p=t.distanceToBorder(e,f)+c-Math.sqrt(Math.pow(n.x-t.x,2)+Math.pow(n.y-t.y,2));if(Math.abs(p)<.05)break;p>0?h>0?s=d:a=d:h>0?a=d:s=d,++u}while(s<=a&&u<10);return KO(KO({},n),{},{t:d})}},{key:"getLineWidth",value:function(t,e){return!0===t?Math.max(this.selectionWidth,.3/this._body.view.scale):!0===e?Math.max(this.hoverWidth,.3/this._body.view.scale):Math.max(this.options.width,.3/this._body.view.scale)}},{key:"getColor",value:function(t,e){if(!1!==e.inheritsColor){if("both"===e.inheritsColor&&this.from.id!==this.to.id){var i=t.createLinearGradient(this.from.x,this.from.y,this.to.x,this.to.y),n=this.from.options.color.highlight.border,o=this.to.options.color.highlight.border;return!1===this.from.selected&&!1===this.to.selected?(n=pm(this.from.options.color.border,e.opacity),o=pm(this.to.options.color.border,e.opacity)):!0===this.from.selected&&!1===this.to.selected?o=this.to.options.color.border:!1===this.from.selected&&!0===this.to.selected&&(n=this.from.options.color.border),i.addColorStop(0,n),i.addColorStop(1,o),i}return"to"===e.inheritsColor?pm(this.to.options.color.border,e.opacity):pm(this.from.options.color.border,e.opacity)}return pm(e.color,e.opacity)}},{key:"_circle",value:function(t,e,i,n,o){this.enableShadow(t,e);var r=0,s=2*Math.PI;if(!this.options.selfReference.renderBehindTheNode){var a=this.options.selfReference.angle,h=this.options.selfReference.angle+Math.PI,l=this._findBorderPositionCircle(this.from,t,{x:i,y:n,low:a,high:h,direction:-1}),d=this._findBorderPositionCircle(this.from,t,{x:i,y:n,low:a,high:h,direction:1});r=Math.atan2(l.y-n,l.x-i),s=Math.atan2(d.y-n,d.x-i)}t.beginPath(),t.arc(i,n,o,r,s,!1),t.stroke(),this.disableShadow(t,e)}},{key:"getDistanceToEdge",value:function(t,e,i,n,o,r){if(this.from!=this.to)return this._getDistanceToEdge(t,e,i,n,o,r);var s=Kc(this._getCircleData(void 0),3),a=s[0],h=s[1],l=s[2],d=a-o,c=h-r;return Math.abs(Math.sqrt(d*d+c*c)-l)}},{key:"_getDistanceToLine",value:function(t,e,i,n,o,r){var s=i-t,a=n-e,h=((o-t)*s+(r-e)*a)/(s*s+a*a);h>1?h=1:h<0&&(h=0);var l=t+h*s-o,d=e+h*a-r;return Math.sqrt(l*l+d*d)}},{key:"getArrowData",value:function(t,e,i,n,o,r){var s,a,h,l,d,c,u,f=r.width;"from"===e?(h=this.from,l=this.to,d=r.fromArrowScale<0,c=Math.abs(r.fromArrowScale),u=r.fromArrowType):"to"===e?(h=this.to,l=this.from,d=r.toArrowScale<0,c=Math.abs(r.toArrowScale),u=r.toArrowType):(h=this.to,l=this.from,d=r.middleArrowScale<0,c=Math.abs(r.middleArrowScale),u=r.middleArrowType);var p=15*c+3*f;if(h!=l){var v=p/IO(h.x-l.x,h.y-l.y);if("middle"!==e)if(!0===this.options.smooth.enabled){var g=this._findBorderPosition(h,t,{via:i}),y=this.getPoint(g.t+v*("from"===e?1:-1),i);s=Math.atan2(g.y-y.y,g.x-y.x),a=g}else s=Math.atan2(h.y-l.y,h.x-l.x),a=this._findBorderPosition(h,t);else{var m=(d?-v:v)/2,b=this.getPoint(.5+m,i),w=this.getPoint(.5-m,i);s=Math.atan2(b.y-w.y,b.x-w.x),a=this.getPoint(.5,i)}}else{var k=Kc(this._getCircleData(t),3),_=k[0],x=k[1],E=k[2];if("from"===e){var O=this.options.selfReference.angle,C=this.options.selfReference.angle+Math.PI,S=this._findBorderPositionCircle(this.from,t,{x:_,y:x,low:O,high:C,direction:-1});s=-2*S.t*Math.PI+1.5*Math.PI+.1*Math.PI,a=S}else if("to"===e){var T=this.options.selfReference.angle,M=this.options.selfReference.angle+Math.PI,P=this._findBorderPositionCircle(this.from,t,{x:_,y:x,low:T,high:M,direction:1});s=-2*P.t*Math.PI+1.5*Math.PI-1.1*Math.PI,a=P}else{var D=this.options.selfReference.angle/(2*Math.PI);a=this._pointOnCircle(_,x,E,D),s=-2*D*Math.PI+1.5*Math.PI+.1*Math.PI}}return{point:a,core:{x:a.x-.9*p*Math.cos(s),y:a.y-.9*p*Math.sin(s)},angle:s,length:p,type:u}}},{key:"drawArrowHead",value:function(t,e,i,n,o){t.strokeStyle=this.getColor(t,e),t.fillStyle=t.strokeStyle,t.lineWidth=e.width,XO.draw(t,o)&&(this.enableShadow(t,e),jv(t).call(t),this.disableShadow(t,e))}},{key:"enableShadow",value:function(t,e){!0===e.shadow&&(t.shadowColor=e.shadowColor,t.shadowBlur=e.shadowSize,t.shadowOffsetX=e.shadowX,t.shadowOffsetY=e.shadowY)}},{key:"disableShadow",value:function(t,e){!0===e.shadow&&(t.shadowColor="rgba(0,0,0,0)",t.shadowBlur=0,t.shadowOffsetX=0,t.shadowOffsetY=0)}},{key:"drawBackground",value:function(t,e){if(!1!==e.background){var i={strokeStyle:t.strokeStyle,lineWidth:t.lineWidth,dashes:t.dashes};t.strokeStyle=e.backgroundColor,t.lineWidth=e.backgroundSize,this.setStrokeDashed(t,e.backgroundDashes),t.stroke(),t.strokeStyle=i.strokeStyle,t.lineWidth=i.lineWidth,t.dashes=i.dashes,this.setStrokeDashed(t,e.dashes)}}},{key:"setStrokeDashed",value:function(t,e){if(!1!==e)if(void 0!==t.setLineDash){var i=lu(e)?e:[5,5];t.setLineDash(i)}else console.warn("setLineDash is not supported in this browser. The dashed stroke cannot be used.");else void 0!==t.setLineDash?t.setLineDash([]):console.warn("setLineDash is not supported in this browser. The dashed stroke cannot be used.")}}]),t}();function ZO(t,e){var i=bu(t);if(hd){var n=hd(t);e&&(n=Xf(n).call(n,(function(e){return bd(t,e).enumerable}))),i.push.apply(i,n)}return i}function QO(t){for(var e=1;e<arguments.length;e++){var i,n,o=null!=arguments[e]?arguments[e]:{};e%2?Fu(i=ZO(Object(o),!0)).call(i,(function(e){$d(t,e,o[e])})):Pd?Ad(t,Pd(o)):Fu(n=ZO(Object(o))).call(n,(function(e){Ud(t,e,bd(o,e))}))}return t}function JO(t){var e=function(){if("undefined"==typeof Reflect||!Mk)return!1;if(Mk.sham)return!1;if("function"==typeof Proxy)return!0;try{return Boolean.prototype.valueOf.call(Mk(Boolean,[],(function(){}))),!0}catch(t){return!1}}();return function(){var i,n=Ak(t);if(e){var o=Ak(this).constructor;i=Mk(n,arguments,o)}else i=n.apply(this,arguments);return Nk(this,i)}}var tC=function(t){zk(i,t);var e=JO(i);function i(t,n,o){return Yd(this,i),e.call(this,t,n,o)}return Kd(i,[{key:"_findBorderPositionBezier",value:function(t,e){var i,n,o=arguments.length>2&&void 0!==arguments[2]?arguments[2]:this._getViaCoordinates(),r=10,s=.2,a=!1,h=1,l=0,d=this.to,c=this.options.endPointOffset?this.options.endPointOffset.to:0;t.id===this.from.id&&(d=this.from,a=!0,c=this.options.endPointOffset?this.options.endPointOffset.from:0),!1===this.options.arrowStrikethrough&&(c=0);var u=0;do{n=.5*(l+h),i=this.getPoint(n,o);var f=Math.atan2(d.y-i.y,d.x-i.x),p=d.distanceToBorder(e,f)+c,v=Math.sqrt(Math.pow(i.x-d.x,2)+Math.pow(i.y-d.y,2)),g=p-v;if(Math.abs(g)<s)break;g<0?!1===a?l=n:h=n:!1===a?h=n:l=n,++u}while(l<=h&&u<r);return QO(QO({},i),{},{t:n})}},{key:"_getDistanceToBezierEdge",value:function(t,e,i,n,o,r,s){var a,h,l,d,c,u=1e9,f=t,p=e;for(h=1;h<10;h++)l=.1*h,d=Math.pow(1-l,2)*t+2*l*(1-l)*s.x+Math.pow(l,2)*i,c=Math.pow(1-l,2)*e+2*l*(1-l)*s.y+Math.pow(l,2)*n,h>0&&(u=(a=this._getDistanceToLine(f,p,d,c,o,r))<u?a:u),f=d,p=c;return u}},{key:"_bezierCurve",value:function(t,e,i,n){t.beginPath(),t.moveTo(this.fromPoint.x,this.fromPoint.y),null!=i&&null!=i.x?null!=n&&null!=n.x?t.bezierCurveTo(i.x,i.y,n.x,n.y,this.toPoint.x,this.toPoint.y):t.quadraticCurveTo(i.x,i.y,this.toPoint.x,this.toPoint.y):t.lineTo(this.toPoint.x,this.toPoint.y),this.drawBackground(t,e),this.enableShadow(t,e),t.stroke(),this.disableShadow(t,e)}},{key:"getViaNode",value:function(){return this._getViaCoordinates()}}]),i}($O);function eC(t){var e=function(){if("undefined"==typeof Reflect||!Mk)return!1;if(Mk.sham)return!1;if("function"==typeof Proxy)return!0;try{return Boolean.prototype.valueOf.call(Mk(Boolean,[],(function(){}))),!0}catch(t){return!1}}();return function(){var i,n=Ak(t);if(e){var o=Ak(this).constructor;i=Mk(n,arguments,o)}else i=n.apply(this,arguments);return Nk(this,i)}}var iC=function(t){zk(i,t);var e=eC(i);function i(t,n,o){var r;return Yd(this,i),(r=e.call(this,t,n,o)).via=r.via,r._boundFunction=function(){r.positionBezierNode()},r._body.emitter.on("_repositionBezierNodes",r._boundFunction),r}return Kd(i,[{key:"setOptions",value:function(t){SO(Ak(i.prototype),"setOptions",this).call(this,t);var e=!1;this.options.physics!==t.physics&&(e=!0),this.options=t,this.id=this.options.id,this.from=this._body.nodes[this.options.from],this.to=this._body.nodes[this.options.to],this.setupSupportNode(),this.connect(),!0===e&&(this.via.setOptions({physics:this.options.physics}),this.positionBezierNode())}},{key:"connect",value:function(){this.from=this._body.nodes[this.options.from],this.to=this._body.nodes[this.options.to],void 0===this.from||void 0===this.to||!1===this.options.physics||this.from.id===this.to.id?this.via.setOptions({physics:!1}):this.via.setOptions({physics:!0})}},{key:"cleanup",value:function(){return this._body.emitter.off("_repositionBezierNodes",this._boundFunction),void 0!==this.via&&(delete this._body.nodes[this.via.id],this.via=void 0,!0)}},{key:"setupSupportNode",value:function(){if(void 0===this.via){var t="edgeId:"+this.id,e=this._body.functions.createNode({id:t,shape:"circle",physics:!0,hidden:!0});this._body.nodes[t]=e,this.via=e,this.via.parentEdgeId=this.id,this.positionBezierNode()}}},{key:"positionBezierNode",value:function(){void 0!==this.via&&void 0!==this.from&&void 0!==this.to?(this.via.x=.5*(this.from.x+this.to.x),this.via.y=.5*(this.from.y+this.to.y)):void 0!==this.via&&(this.via.x=0,this.via.y=0)}},{key:"_line",value:function(t,e,i){this._bezierCurve(t,e,i)}},{key:"_getViaCoordinates",value:function(){return this.via}},{key:"getViaNode",value:function(){return this.via}},{key:"getPoint",value:function(t){var e=arguments.length>1&&void 0!==arguments[1]?arguments[1]:this.via;if(this.from===this.to){var i=this._getCircleData(),n=Kc(i,3),o=n[0],r=n[1],s=n[2],a=2*Math.PI*(1-t);return{x:o+s*Math.sin(a),y:r+s-s*(1-Math.cos(a))}}return{x:Math.pow(1-t,2)*this.fromPoint.x+2*t*(1-t)*e.x+Math.pow(t,2)*this.toPoint.x,y:Math.pow(1-t,2)*this.fromPoint.y+2*t*(1-t)*e.y+Math.pow(t,2)*this.toPoint.y}}},{key:"_findBorderPosition",value:function(t,e){return this._findBorderPositionBezier(t,e,this.via)}},{key:"_getDistanceToEdge",value:function(t,e,i,n,o,r){return this._getDistanceToBezierEdge(t,e,i,n,o,r,this.via)}}]),i}(tC);function nC(t){var e=function(){if("undefined"==typeof Reflect||!Mk)return!1;if(Mk.sham)return!1;if("function"==typeof Proxy)return!0;try{return Boolean.prototype.valueOf.call(Mk(Boolean,[],(function(){}))),!0}catch(t){return!1}}();return function(){var i,n=Ak(t);if(e){var o=Ak(this).constructor;i=Mk(n,arguments,o)}else i=n.apply(this,arguments);return Nk(this,i)}}var oC=function(t){zk(i,t);var e=nC(i);function i(t,n,o){return Yd(this,i),e.call(this,t,n,o)}return Kd(i,[{key:"_line",value:function(t,e,i){this._bezierCurve(t,e,i)}},{key:"getViaNode",value:function(){return this._getViaCoordinates()}},{key:"_getViaCoordinates",value:function(){var t,e,i=this.options.smooth.roundness,n=this.options.smooth.type,o=Math.abs(this.from.x-this.to.x),r=Math.abs(this.from.y-this.to.y);if("discrete"===n||"diagonalCross"===n){var s,a;s=a=o<=r?i*r:i*o,this.from.x>this.to.x&&(s=-s),this.from.y>=this.to.y&&(a=-a);var h=this.from.x+s,l=this.from.y+a;return"discrete"===n&&(o<=r?h=o<i*r?this.from.x:h:l=r<i*o?this.from.y:l),{x:h,y:l}}if("straightCross"===n){var d=(1-i)*o,c=(1-i)*r;return o<=r?(d=0,this.from.y<this.to.y&&(c=-c)):(this.from.x<this.to.x&&(d=-d),c=0),{x:this.to.x+d,y:this.to.y+c}}if("horizontal"===n){var u=(1-i)*o;return this.from.x<this.to.x&&(u=-u),{x:this.to.x+u,y:this.from.y}}if("vertical"===n){var f=(1-i)*r;return this.from.y<this.to.y&&(f=-f),{x:this.from.x,y:this.to.y+f}}if("curvedCW"===n){o=this.to.x-this.from.x,r=this.from.y-this.to.y;var p=Math.sqrt(o*o+r*r),v=Math.PI,g=(Math.atan2(r,o)+(.5*i+.5)*v)%(2*v);return{x:this.from.x+(.5*i+.5)*p*Math.sin(g),y:this.from.y+(.5*i+.5)*p*Math.cos(g)}}if("curvedCCW"===n){o=this.to.x-this.from.x,r=this.from.y-this.to.y;var y=Math.sqrt(o*o+r*r),m=Math.PI,b=(Math.atan2(r,o)+(.5*-i+.5)*m)%(2*m);return{x:this.from.x+(.5*i+.5)*y*Math.sin(b),y:this.from.y+(.5*i+.5)*y*Math.cos(b)}}t=e=o<=r?i*r:i*o,this.from.x>this.to.x&&(t=-t),this.from.y>=this.to.y&&(e=-e);var w=this.from.x+t,k=this.from.y+e;return o<=r?w=this.from.x<=this.to.x?this.to.x<w?this.to.x:w:this.to.x>w?this.to.x:w:k=this.from.y>=this.to.y?this.to.y>k?this.to.y:k:this.to.y<k?this.to.y:k,{x:w,y:k}}},{key:"_findBorderPosition",value:function(t,e){var i=arguments.length>2&&void 0!==arguments[2]?arguments[2]:{};return this._findBorderPositionBezier(t,e,i.via)}},{key:"_getDistanceToEdge",value:function(t,e,i,n,o,r){var s=arguments.length>6&&void 0!==arguments[6]?arguments[6]:this._getViaCoordinates();return this._getDistanceToBezierEdge(t,e,i,n,o,r,s)}},{key:"getPoint",value:function(t){var e=arguments.length>1&&void 0!==arguments[1]?arguments[1]:this._getViaCoordinates(),i=t,n=Math.pow(1-i,2)*this.fromPoint.x+2*i*(1-i)*e.x+Math.pow(i,2)*this.toPoint.x,o=Math.pow(1-i,2)*this.fromPoint.y+2*i*(1-i)*e.y+Math.pow(i,2)*this.toPoint.y;return{x:n,y:o}}}]),i}(tC);function rC(t){var e=function(){if("undefined"==typeof Reflect||!Mk)return!1;if(Mk.sham)return!1;if("function"==typeof Proxy)return!0;try{return Boolean.prototype.valueOf.call(Mk(Boolean,[],(function(){}))),!0}catch(t){return!1}}();return function(){var i,n=Ak(t);if(e){var o=Ak(this).constructor;i=Mk(n,arguments,o)}else i=n.apply(this,arguments);return Nk(this,i)}}var sC=function(t){zk(i,t);var e=rC(i);function i(t,n,o){return Yd(this,i),e.call(this,t,n,o)}return Kd(i,[{key:"_getDistanceToBezierEdge2",value:function(t,e,i,n,o,r,s,a){for(var h=1e9,l=t,d=e,c=[0,0,0,0],u=1;u<10;u++){var f=.1*u;c[0]=Math.pow(1-f,3),c[1]=3*f*Math.pow(1-f,2),c[2]=3*Math.pow(f,2)*(1-f),c[3]=Math.pow(f,3);var p=c[0]*t+c[1]*s.x+c[2]*a.x+c[3]*i,v=c[0]*e+c[1]*s.y+c[2]*a.y+c[3]*n;if(u>0){var g=this._getDistanceToLine(l,d,p,v,o,r);h=g<h?g:h}l=p,d=v}return h}}]),i}(tC);function aC(t){var e=function(){if("undefined"==typeof Reflect||!Mk)return!1;if(Mk.sham)return!1;if("function"==typeof Proxy)return!0;try{return Boolean.prototype.valueOf.call(Mk(Boolean,[],(function(){}))),!0}catch(t){return!1}}();return function(){var i,n=Ak(t);if(e){var o=Ak(this).constructor;i=Mk(n,arguments,o)}else i=n.apply(this,arguments);return Nk(this,i)}}var hC=function(t){zk(i,t);var e=aC(i);function i(t,n,o){return Yd(this,i),e.call(this,t,n,o)}return Kd(i,[{key:"_line",value:function(t,e,i){var n=i[0],o=i[1];this._bezierCurve(t,e,n,o)}},{key:"_getViaCoordinates",value:function(){var t,e,i,n,o=this.from.x-this.to.x,r=this.from.y-this.to.y,s=this.options.smooth.roundness;return(Math.abs(o)>Math.abs(r)||!0===this.options.smooth.forceDirection||"horizontal"===this.options.smooth.forceDirection)&&"vertical"!==this.options.smooth.forceDirection?(e=this.from.y,n=this.to.y,t=this.from.x-s*o,i=this.to.x+s*o):(e=this.from.y-s*r,n=this.to.y+s*r,t=this.from.x,i=this.to.x),[{x:t,y:e},{x:i,y:n}]}},{key:"getViaNode",value:function(){return this._getViaCoordinates()}},{key:"_findBorderPosition",value:function(t,e){return this._findBorderPositionBezier(t,e)}},{key:"_getDistanceToEdge",value:function(t,e,i,n,o,r){var s=arguments.length>6&&void 0!==arguments[6]?arguments[6]:this._getViaCoordinates(),a=Kc(s,2),h=a[0],l=a[1];return this._getDistanceToBezierEdge2(t,e,i,n,o,r,h,l)}},{key:"getPoint",value:function(t){var e=arguments.length>1&&void 0!==arguments[1]?arguments[1]:this._getViaCoordinates(),i=Kc(e,2),n=i[0],o=i[1],r=t,s=[Math.pow(1-r,3),3*r*Math.pow(1-r,2),3*Math.pow(r,2)*(1-r),Math.pow(r,3)],a=s[0]*this.fromPoint.x+s[1]*n.x+s[2]*o.x+s[3]*this.toPoint.x,h=s[0]*this.fromPoint.y+s[1]*n.y+s[2]*o.y+s[3]*this.toPoint.y;return{x:a,y:h}}}]),i}(sC);function lC(t){var e=function(){if("undefined"==typeof Reflect||!Mk)return!1;if(Mk.sham)return!1;if("function"==typeof Proxy)return!0;try{return Boolean.prototype.valueOf.call(Mk(Boolean,[],(function(){}))),!0}catch(t){return!1}}();return function(){var i,n=Ak(t);if(e){var o=Ak(this).constructor;i=Mk(n,arguments,o)}else i=n.apply(this,arguments);return Nk(this,i)}}var dC=function(t){zk(i,t);var e=lC(i);function i(t,n,o){return Yd(this,i),e.call(this,t,n,o)}return Kd(i,[{key:"_line",value:function(t,e){t.beginPath(),t.moveTo(this.fromPoint.x,this.fromPoint.y),t.lineTo(this.toPoint.x,this.toPoint.y),this.enableShadow(t,e),t.stroke(),this.disableShadow(t,e)}},{key:"getViaNode",value:function(){}},{key:"getPoint",value:function(t){return{x:(1-t)*this.fromPoint.x+t*this.toPoint.x,y:(1-t)*this.fromPoint.y+t*this.toPoint.y}}},{key:"_findBorderPosition",value:function(t,e){var i=this.to,n=this.from;t.id===this.from.id&&(i=this.from,n=this.to);var o=Math.atan2(i.y-n.y,i.x-n.x),r=i.x-n.x,s=i.y-n.y,a=Math.sqrt(r*r+s*s),h=(a-t.distanceToBorder(e,o))/a;return{x:(1-h)*n.x+h*i.x,y:(1-h)*n.y+h*i.y,t:0}}},{key:"_getDistanceToEdge",value:function(t,e,i,n,o,r){return this._getDistanceToLine(t,e,i,n,o,r)}}]),i}($O),cC=function(){function t(e,i,n,o,r){if(Yd(this,t),void 0===i)throw new Error("No body provided");this.options=Cm(o),this.globalOptions=o,this.defaultOptions=r,this.body=i,this.imagelist=n,this.id=void 0,this.fromId=void 0,this.toId=void 0,this.selected=!1,this.hover=!1,this.labelDirty=!0,this.baseWidth=this.options.width,this.baseFontSize=this.options.font.size,this.from=void 0,this.to=void 0,this.edgeType=void 0,this.connected=!1,this.labelModule=new OE(this.body,this.options,!0),this.setOptions(e)}return Kd(t,[{key:"setOptions",value:function(e){if(e){var i=void 0!==e.physics&&this.options.physics!==e.physics||void 0!==e.hidden&&(this.options.hidden||!1)!==(e.hidden||!1)||void 0!==e.from&&this.options.from!==e.from||void 0!==e.to&&this.options.to!==e.to;t.parseOptions(this.options,e,!0,this.globalOptions),void 0!==e.id&&(this.id=e.id),void 0!==e.from&&(this.fromId=e.from),void 0!==e.to&&(this.toId=e.to),void 0!==e.title&&(this.title=e.title),void 0!==e.value&&(e.value=lE(e.value));var n=[e,this.options,this.defaultOptions];return this.chooser=gE("edge",n),this.updateLabelModule(e),i=this.updateEdgeType()||i,this._setInteractionWidths(),this.connect(),i}}},{key:"getFormattingValues",value:function(){var t=!0===this.options.arrows.to||!0===this.options.arrows.to.enabled,e=!0===this.options.arrows.from||!0===this.options.arrows.from.enabled,i=!0===this.options.arrows.middle||!0===this.options.arrows.middle.enabled,n=this.options.color.inherit,o={toArrow:t,toArrowScale:this.options.arrows.to.scaleFactor,toArrowType:this.options.arrows.to.type,toArrowSrc:this.options.arrows.to.src,toArrowImageWidth:this.options.arrows.to.imageWidth,toArrowImageHeight:this.options.arrows.to.imageHeight,middleArrow:i,middleArrowScale:this.options.arrows.middle.scaleFactor,middleArrowType:this.options.arrows.middle.type,middleArrowSrc:this.options.arrows.middle.src,middleArrowImageWidth:this.options.arrows.middle.imageWidth,middleArrowImageHeight:this.options.arrows.middle.imageHeight,fromArrow:e,fromArrowScale:this.options.arrows.from.scaleFactor,fromArrowType:this.options.arrows.from.type,fromArrowSrc:this.options.arrows.from.src,fromArrowImageWidth:this.options.arrows.from.imageWidth,fromArrowImageHeight:this.options.arrows.from.imageHeight,arrowStrikethrough:this.options.arrowStrikethrough,color:n?void 0:this.options.color.color,inheritsColor:n,opacity:this.options.color.opacity,hidden:this.options.hidden,length:this.options.length,shadow:this.options.shadow.enabled,shadowColor:this.options.shadow.color,shadowSize:this.options.shadow.size,shadowX:this.options.shadow.x,shadowY:this.options.shadow.y,dashes:this.options.dashes,width:this.options.width,background:this.options.background.enabled,backgroundColor:this.options.background.color,backgroundSize:this.options.background.size,backgroundDashes:this.options.background.dashes};if(this.selected||this.hover)if(!0===this.chooser){if(this.selected){var r=this.options.selectionWidth;"function"==typeof r?o.width=r(o.width):"number"==typeof r&&(o.width+=r),o.width=Math.max(o.width,.3/this.body.view.scale),o.color=this.options.color.highlight,o.shadow=this.options.shadow.enabled}else if(this.hover){var s=this.options.hoverWidth;"function"==typeof s?o.width=s(o.width):"number"==typeof s&&(o.width+=s),o.width=Math.max(o.width,.3/this.body.view.scale),o.color=this.options.color.hover,o.shadow=this.options.shadow.enabled}}else"function"==typeof this.chooser&&(this.chooser(o,this.options.id,this.selected,this.hover),void 0!==o.color&&(o.inheritsColor=!1),!1===o.shadow&&(o.shadowColor===this.options.shadow.color&&o.shadowSize===this.options.shadow.size&&o.shadowX===this.options.shadow.x&&o.shadowY===this.options.shadow.y||(o.shadow=!0)));else o.shadow=this.options.shadow.enabled,o.width=Math.max(o.width,.3/this.body.view.scale);return o}},{key:"updateLabelModule",value:function(t){var e=[t,this.options,this.globalOptions,this.defaultOptions];this.labelModule.update(this.options,e),void 0!==this.labelModule.baseSize&&(this.baseFontSize=this.labelModule.baseSize)}},{key:"updateEdgeType",value:function(){var t=this.options.smooth,e=!1,i=!0;return void 0!==this.edgeType&&((this.edgeType instanceof iC&&!0===t.enabled&&"dynamic"===t.type||this.edgeType instanceof hC&&!0===t.enabled&&"cubicBezier"===t.type||this.edgeType instanceof oC&&!0===t.enabled&&"dynamic"!==t.type&&"cubicBezier"!==t.type||this.edgeType instanceof dC&&!1===t.type.enabled)&&(i=!1),!0===i&&(e=this.cleanup())),!0===i?!0===t.enabled?"dynamic"===t.type?(e=!0,this.edgeType=new iC(this.options,this.body,this.labelModule)):"cubicBezier"===t.type?this.edgeType=new hC(this.options,this.body,this.labelModule):this.edgeType=new oC(this.options,this.body,this.labelModule):this.edgeType=new dC(this.options,this.body,this.labelModule):this.edgeType.setOptions(this.options),e}},{key:"connect",value:function(){this.disconnect(),this.from=this.body.nodes[this.fromId]||void 0,this.to=this.body.nodes[this.toId]||void 0,this.connected=void 0!==this.from&&void 0!==this.to,!0===this.connected?(this.from.attachEdge(this),this.to.attachEdge(this)):(this.from&&this.from.detachEdge(this),this.to&&this.to.detachEdge(this)),this.edgeType.connect()}},{key:"disconnect",value:function(){this.from&&(this.from.detachEdge(this),this.from=void 0),this.to&&(this.to.detachEdge(this),this.to=void 0),this.connected=!1}},{key:"getTitle",value:function(){return this.title}},{key:"isSelected",value:function(){return this.selected}},{key:"getValue",value:function(){return this.options.value}},{key:"setValueRange",value:function(t,e,i){if(void 0!==this.options.value){var n=this.options.scaling.customScalingFunction(t,e,i,this.options.value),o=this.options.scaling.max-this.options.scaling.min;if(!0===this.options.scaling.label.enabled){var r=this.options.scaling.label.max-this.options.scaling.label.min;this.options.font.size=this.options.scaling.label.min+n*r}this.options.width=this.options.scaling.min+n*o}else this.options.width=this.baseWidth,this.options.font.size=this.baseFontSize;this._setInteractionWidths(),this.updateLabelModule()}},{key:"_setInteractionWidths",value:function(){"function"==typeof this.options.hoverWidth?this.edgeType.hoverWidth=this.options.hoverWidth(this.options.width):this.edgeType.hoverWidth=this.options.hoverWidth+this.options.width,"function"==typeof this.options.selectionWidth?this.edgeType.selectionWidth=this.options.selectionWidth(this.options.width):this.edgeType.selectionWidth=this.options.selectionWidth+this.options.width}},{key:"draw",value:function(t){var e=this.getFormattingValues();if(!e.hidden){var i=this.edgeType.getViaNode();this.edgeType.drawLine(t,e,this.selected,this.hover,i),this.drawLabel(t,i)}}},{key:"drawArrows",value:function(t){var e=this.getFormattingValues();if(!e.hidden){var i=this.edgeType.getViaNode(),n={};this.edgeType.fromPoint=this.edgeType.from,this.edgeType.toPoint=this.edgeType.to,e.fromArrow&&(n.from=this.edgeType.getArrowData(t,"from",i,this.selected,this.hover,e),!1===e.arrowStrikethrough&&(this.edgeType.fromPoint=n.from.core),e.fromArrowSrc&&(n.from.image=this.imagelist.load(e.fromArrowSrc)),e.fromArrowImageWidth&&(n.from.imageWidth=e.fromArrowImageWidth),e.fromArrowImageHeight&&(n.from.imageHeight=e.fromArrowImageHeight)),e.toArrow&&(n.to=this.edgeType.getArrowData(t,"to",i,this.selected,this.hover,e),!1===e.arrowStrikethrough&&(this.edgeType.toPoint=n.to.core),e.toArrowSrc&&(n.to.image=this.imagelist.load(e.toArrowSrc)),e.toArrowImageWidth&&(n.to.imageWidth=e.toArrowImageWidth),e.toArrowImageHeight&&(n.to.imageHeight=e.toArrowImageHeight)),e.middleArrow&&(n.middle=this.edgeType.getArrowData(t,"middle",i,this.selected,this.hover,e),e.middleArrowSrc&&(n.middle.image=this.imagelist.load(e.middleArrowSrc)),e.middleArrowImageWidth&&(n.middle.imageWidth=e.middleArrowImageWidth),e.middleArrowImageHeight&&(n.middle.imageHeight=e.middleArrowImageHeight)),e.fromArrow&&this.edgeType.drawArrowHead(t,e,this.selected,this.hover,n.from),e.middleArrow&&this.edgeType.drawArrowHead(t,e,this.selected,this.hover,n.middle),e.toArrow&&this.edgeType.drawArrowHead(t,e,this.selected,this.hover,n.to)}}},{key:"drawLabel",value:function(t,e){if(void 0!==this.options.label){var i,n=this.from,o=this.to;if(this.labelModule.differentState(this.selected,this.hover)&&this.labelModule.getTextSize(t,this.selected,this.hover),n.id!=o.id){this.labelModule.pointToSelf=!1,i=this.edgeType.getPoint(.5,e),t.save();var r=this._getRotation(t);0!=r.angle&&(t.translate(r.x,r.y),t.rotate(r.angle)),this.labelModule.draw(t,i.x,i.y,this.selected,this.hover),t.restore()}else{this.labelModule.pointToSelf=!0;var s=bE(t,this.options.selfReference.angle,this.options.selfReference.size,n);i=this._pointOnCircle(s.x,s.y,this.options.selfReference.size,this.options.selfReference.angle),this.labelModule.draw(t,i.x,i.y,this.selected,this.hover)}}}},{key:"getItemsOnPoint",value:function(t){var e=[];if(this.labelModule.visible()){var i=this._getRotation();yE(this.labelModule.getSize(),t,i)&&e.push({edgeId:this.id,labelId:0})}var n={left:t.x,top:t.y};return this.isOverlappingWith(n)&&e.push({edgeId:this.id}),e}},{key:"isOverlappingWith",value:function(t){if(this.connected){var e=this.from.x,i=this.from.y,n=this.to.x,o=this.to.y,r=t.left,s=t.top;return this.edgeType.getDistanceToEdge(e,i,n,o,r,s)<10}return!1}},{key:"_getRotation",value:function(t){var e=this.edgeType.getViaNode(),i=this.edgeType.getPoint(.5,e);void 0!==t&&this.labelModule.calculateLabelSize(t,this.selected,this.hover,i.x,i.y);var n={x:i.x,y:this.labelModule.size.yLine,angle:0};if(!this.labelModule.visible())return n;if("horizontal"===this.options.font.align)return n;var o=this.from.y-this.to.y,r=this.from.x-this.to.x,s=Math.atan2(o,r);return(s<-1&&r<0||s>0&&r<0)&&(s+=Math.PI),n.angle=s,n}},{key:"_pointOnCircle",value:function(t,e,i,n){return{x:t+i*Math.cos(n),y:e-i*Math.sin(n)}}},{key:"select",value:function(){this.selected=!0}},{key:"unselect",value:function(){this.selected=!1}},{key:"cleanup",value:function(){return this.edgeType.cleanup()}},{key:"remove",value:function(){this.cleanup(),this.disconnect(),delete this.body.edges[this.id]}},{key:"endPointsValid",value:function(){return void 0!==this.body.nodes[this.fromId]&&void 0!==this.body.nodes[this.toId]}}],[{key:"parseOptions",value:function(t,e){var i=arguments.length>2&&void 0!==arguments[2]&&arguments[2],n=arguments.length>3&&void 0!==arguments[3]?arguments[3]:{},o=arguments.length>4&&void 0!==arguments[4]&&arguments[4],r=["endPointOffset","arrowStrikethrough","id","from","hidden","hoverWidth","labelHighlightBold","length","line","opacity","physics","scaling","selectionWidth","selfReferenceSize","selfReference","to","title","value","width","font","chosen","widthConstraint"];if(em(r,t,e,i),void 0!==e.endPointOffset&&void 0!==e.endPointOffset.from&&(ok(e.endPointOffset.from)?t.endPointOffset.from=e.endPointOffset.from:(t.endPointOffset.from=void 0!==n.endPointOffset.from?n.endPointOffset.from:0,console.error("endPointOffset.from is not a valid number"))),void 0!==e.endPointOffset&&void 0!==e.endPointOffset.to&&(ok(e.endPointOffset.to)?t.endPointOffset.to=e.endPointOffset.to:(t.endPointOffset.to=void 0!==n.endPointOffset.to?n.endPointOffset.to:0,console.error("endPointOffset.to is not a valid number"))),mE(e.label)?t.label=e.label:mE(t.label)||(t.label=void 0),Sm(t,e,"smooth",n),Sm(t,e,"shadow",n),Sm(t,e,"background",n),void 0!==e.dashes&&null!==e.dashes?t.dashes=e.dashes:!0===i&&null===e.dashes&&(t.dashes=Kp(n.dashes)),void 0!==e.scaling&&null!==e.scaling?(void 0!==e.scaling.min&&(t.scaling.min=e.scaling.min),void 0!==e.scaling.max&&(t.scaling.max=e.scaling.max),Sm(t.scaling,e.scaling,"label",n.scaling)):!0===i&&null===e.scaling&&(t.scaling=Kp(n.scaling)),void 0!==e.arrows&&null!==e.arrows)if("string"==typeof e.arrows){var s=e.arrows.toLowerCase();t.arrows.to.enabled=-1!=Fp(s).call(s,"to"),t.arrows.middle.enabled=-1!=Fp(s).call(s,"middle"),t.arrows.from.enabled=-1!=Fp(s).call(s,"from")}else{if("object"!==Qc(e.arrows))throw new Error("The arrow newOptions can only be an object or a string. Refer to the documentation. You used:"+gv(e.arrows));Sm(t.arrows,e.arrows,"to",n.arrows),Sm(t.arrows,e.arrows,"middle",n.arrows),Sm(t.arrows,e.arrows,"from",n.arrows)}else!0===i&&null===e.arrows&&(t.arrows=Kp(n.arrows));if(void 0!==e.color&&null!==e.color){var a=$y(e.color)?{color:e.color,highlight:e.color,hover:e.color,inherit:!1,opacity:1}:e.color,h=t.color;if(o)nm(h,n.color,!1,i);else for(var l in h)Object.prototype.hasOwnProperty.call(h,l)&&delete h[l];if($y(h))h.color=h,h.highlight=h,h.hover=h,h.inherit=!1,void 0===a.opacity&&(h.opacity=1);else{var d=!1;void 0!==a.color&&(h.color=a.color,d=!0),void 0!==a.highlight&&(h.highlight=a.highlight,d=!0),void 0!==a.hover&&(h.hover=a.hover,d=!0),void 0!==a.inherit&&(h.inherit=a.inherit),void 0!==a.opacity&&(h.opacity=Math.min(1,Math.max(0,a.opacity))),!0===d?h.inherit=!1:void 0===h.inherit&&(h.inherit="from")}}else!0===i&&null===e.color&&(t.color=Cm(n.color));!0===i&&null===e.font&&(t.font=Cm(n.font)),Object.prototype.hasOwnProperty.call(e,"selfReferenceSize")&&(console.warn("The selfReferenceSize property has been deprecated. Please use selfReference property instead. The selfReference can be set like thise selfReference:{size:30, angle:Math.PI / 4}"),t.selfReference.size=e.selfReferenceSize)}}]),t}(),uC=function(){function t(e,i,n){var o,r=this;Yd(this,t),this.body=e,this.images=i,this.groups=n,this.body.functions.createEdge=zn(o=this.create).call(o,this),this.edgesListeners={add:function(t,e){r.add(e.items)},update:function(t,e){r.update(e.items)},remove:function(t,e){r.remove(e.items)}},this.options={},this.defaultOptions={arrows:{to:{enabled:!1,scaleFactor:1,type:"arrow"},middle:{enabled:!1,scaleFactor:1,type:"arrow"},from:{enabled:!1,scaleFactor:1,type:"arrow"}},endPointOffset:{from:0,to:0},arrowStrikethrough:!0,color:{color:"#848484",highlight:"#848484",hover:"#848484",inherit:"from",opacity:1},dashes:!1,font:{color:"#343434",size:14,face:"arial",background:"none",strokeWidth:2,strokeColor:"#ffffff",align:"horizontal",multi:!1,vadjust:0,bold:{mod:"bold"},boldital:{mod:"bold italic"},ital:{mod:"italic"},mono:{mod:"",size:15,face:"courier new",vadjust:2}},hidden:!1,hoverWidth:1.5,label:void 0,labelHighlightBold:!0,length:void 0,physics:!0,scaling:{min:1,max:15,label:{enabled:!0,min:14,max:30,maxVisible:30,drawThreshold:5},customScalingFunction:function(t,e,i,n){if(e===t)return.5;var o=1/(e-t);return Math.max(0,(n-t)*o)}},selectionWidth:1.5,selfReference:{size:20,angle:Math.PI/4,renderBehindTheNode:!0},shadow:{enabled:!1,color:"rgba(0,0,0,0.5)",size:10,x:5,y:5},background:{enabled:!1,color:"rgba(111,111,111,1)",size:10,dashes:!1},smooth:{enabled:!0,type:"dynamic",forceDirection:"none",roundness:.5},title:void 0,width:1,value:void 0},nm(this.options,this.defaultOptions),this.bindEventListeners()}return Kd(t,[{key:"bindEventListeners",value:function(){var t,e,i=this;this.body.emitter.on("_forceDisableDynamicCurves",(function(t){var e=!(arguments.length>1&&void 0!==arguments[1])||arguments[1];"dynamic"===t&&(t="continuous");var n=!1;for(var o in i.body.edges)if(Object.prototype.hasOwnProperty.call(i.body.edges,o)){var r=i.body.edges[o],s=i.body.data.edges.get(o);if(null!=s){var a=s.smooth;void 0!==a&&!0===a.enabled&&"dynamic"===a.type&&(void 0===t?r.setOptions({smooth:!1}):r.setOptions({smooth:{type:t}}),n=!0)}}!0===e&&!0===n&&i.body.emitter.emit("_dataChanged")})),this.body.emitter.on("_dataUpdated",(function(){i.reconnectEdges()})),this.body.emitter.on("refreshEdges",zn(t=this.refresh).call(t,this)),this.body.emitter.on("refresh",zn(e=this.refresh).call(e,this)),this.body.emitter.on("destroy",(function(){hm(i.edgesListeners,(function(t,e){i.body.data.edges&&i.body.data.edges.off(e,t)})),delete i.body.functions.createEdge,delete i.edgesListeners.add,delete i.edgesListeners.update,delete i.edgesListeners.remove,delete i.edgesListeners}))}},{key:"setOptions",value:function(t){if(void 0!==t){cC.parseOptions(this.options,t,!0,this.defaultOptions,!0);var e=!1;if(void 0!==t.smooth)for(var i in this.body.edges)Object.prototype.hasOwnProperty.call(this.body.edges,i)&&(e=this.body.edges[i].updateEdgeType()||e);if(void 0!==t.font)for(var n in this.body.edges)Object.prototype.hasOwnProperty.call(this.body.edges,n)&&this.body.edges[n].updateLabelModule();void 0===t.hidden&&void 0===t.physics&&!0!==e||this.body.emitter.emit("_dataChanged")}}},{key:"setData",value:function(t){var e=this,i=arguments.length>1&&void 0!==arguments[1]&&arguments[1],n=this.body.data.edges;if(Qx("id",t))this.body.data.edges=t;else if(lu(t))this.body.data.edges=new Kx,this.body.data.edges.add(t);else{if(t)throw new TypeError("Array or DataSet expected");this.body.data.edges=new Kx}if(n&&hm(this.edgesListeners,(function(t,e){n.off(e,t)})),this.body.edges={},this.body.data.edges){hm(this.edgesListeners,(function(t,i){e.body.data.edges.on(i,t)}));var o=this.body.data.edges.getIds();this.add(o,!0)}this.body.emitter.emit("_adjustEdgesForHierarchicalLayout"),!1===i&&this.body.emitter.emit("_dataChanged")}},{key:"add",value:function(t){for(var e=arguments.length>1&&void 0!==arguments[1]&&arguments[1],i=this.body.edges,n=this.body.data.edges,o=0;o<t.length;o++){var r=t[o],s=i[r];s&&s.disconnect();var a=n.get(r,{showInternalIds:!0});i[r]=this.create(a)}this.body.emitter.emit("_adjustEdgesForHierarchicalLayout"),!1===e&&this.body.emitter.emit("_dataChanged")}},{key:"update",value:function(t){for(var e=this.body.edges,i=this.body.data.edges,n=!1,o=0;o<t.length;o++){var r=t[o],s=i.get(r),a=e[r];void 0!==a?(a.disconnect(),n=a.setOptions(s)||n,a.connect()):(this.body.edges[r]=this.create(s),n=!0)}!0===n?(this.body.emitter.emit("_adjustEdgesForHierarchicalLayout"),this.body.emitter.emit("_dataChanged")):this.body.emitter.emit("_dataUpdated")}},{key:"remove",value:function(t){var e=!(arguments.length>1&&void 0!==arguments[1])||arguments[1];if(0!==t.length){var i=this.body.edges;hm(t,(function(t){var e=i[t];void 0!==e&&e.remove()})),e&&this.body.emitter.emit("_dataChanged")}}},{key:"refresh",value:function(){var t=this;hm(this.body.edges,(function(e,i){var n=t.body.data.edges.get(i);void 0!==n&&e.setOptions(n)}))}},{key:"create",value:function(t){return new cC(t,this.body,this.images,this.options,this.defaultOptions)}},{key:"reconnectEdges",value:function(){var t,e=this.body.nodes,i=this.body.edges;for(t in e)Object.prototype.hasOwnProperty.call(e,t)&&(e[t].edges=[]);for(t in i)if(Object.prototype.hasOwnProperty.call(i,t)){var n=i[t];n.from=null,n.to=null,n.connect()}}},{key:"getConnectedNodes",value:function(t){var e=[];if(void 0!==this.body.edges[t]){var i=this.body.edges[t];void 0!==i.fromId&&e.push(i.fromId),void 0!==i.toId&&e.push(i.toId)}return e}},{key:"_updateState",value:function(){this._addMissingEdges(),this._removeInvalidEdges()}},{key:"_removeInvalidEdges",value:function(){var t=this,e=[];hm(this.body.edges,(function(i,n){var o=t.body.nodes[i.toId],r=t.body.nodes[i.fromId];void 0!==o&&!0===o.isCluster||void 0!==r&&!0===r.isCluster||void 0!==o&&void 0!==r||e.push(n)})),this.remove(e,!1)}},{key:"_addMissingEdges",value:function(){var t=this.body.data.edges;if(null!=t){var e=this.body.edges,i=[];Fu(t).call(t,(function(t,n){void 0===e[n]&&i.push(n)})),this.add(i,!0)}}}]),t}(),fC=function(){function t(e,i,n){Yd(this,t),this.body=e,this.physicsBody=i,this.barnesHutTree,this.setOptions(n),this._rng=jy("BARNES HUT SOLVER")}return Kd(t,[{key:"setOptions",value:function(t){this.options=t,this.thetaInversed=1/this.options.theta,this.overlapAvoidanceFactor=1-Math.max(0,Math.min(1,this.options.avoidOverlap))}},{key:"solve",value:function(){if(0!==this.options.gravitationalConstant&&this.physicsBody.physicsNodeIndices.length>0){var t,e=this.body.nodes,i=this.physicsBody.physicsNodeIndices,n=i.length,o=this._formBarnesHutTree(e,i);this.barnesHutTree=o;for(var r=0;r<n;r++)(t=e[i[r]]).options.mass>0&&this._getForceContributions(o.root,t)}}},{key:"_getForceContributions",value:function(t,e){this._getForceContribution(t.children.NW,e),this._getForceContribution(t.children.NE,e),this._getForceContribution(t.children.SW,e),this._getForceContribution(t.children.SE,e)}},{key:"_getForceContribution",value:function(t,e){if(t.childrenCount>0){var i=t.centerOfMass.x-e.x,n=t.centerOfMass.y-e.y,o=Math.sqrt(i*i+n*n);o*t.calcSize>this.thetaInversed?this._calculateForces(o,i,n,e,t):4===t.childrenCount?this._getForceContributions(t,e):t.children.data.id!=e.id&&this._calculateForces(o,i,n,e,t)}}},{key:"_calculateForces",value:function(t,e,i,n,o){0===t&&(e=t=.1),this.overlapAvoidanceFactor<1&&n.shape.radius&&(t=Math.max(.1+this.overlapAvoidanceFactor*n.shape.radius,t-n.shape.radius));var r=this.options.gravitationalConstant*o.mass*n.options.mass/Math.pow(t,3),s=e*r,a=i*r;this.physicsBody.forces[n.id].x+=s,this.physicsBody.forces[n.id].y+=a}},{key:"_formBarnesHutTree",value:function(t,e){for(var i,n=e.length,o=t[e[0]].x,r=t[e[0]].y,s=t[e[0]].x,a=t[e[0]].y,h=1;h<n;h++){var l=t[e[h]],d=l.x,c=l.y;l.options.mass>0&&(d<o&&(o=d),d>s&&(s=d),c<r&&(r=c),c>a&&(a=c))}var u=Math.abs(s-o)-Math.abs(a-r);u>0?(r-=.5*u,a+=.5*u):(o+=.5*u,s-=.5*u);var f=Math.max(1e-5,Math.abs(s-o)),p=.5*f,v=.5*(o+s),g=.5*(r+a),y={root:{centerOfMass:{x:0,y:0},mass:0,range:{minX:v-p,maxX:v+p,minY:g-p,maxY:g+p},size:f,calcSize:1/f,children:{data:null},maxWidth:0,level:0,childrenCount:4}};this._splitBranch(y.root);for(var m=0;m<n;m++)(i=t[e[m]]).options.mass>0&&this._placeInTree(y.root,i);return y}},{key:"_updateBranchMass",value:function(t,e){var i=t.centerOfMass,n=t.mass+e.options.mass,o=1/n;i.x=i.x*t.mass+e.x*e.options.mass,i.x*=o,i.y=i.y*t.mass+e.y*e.options.mass,i.y*=o,t.mass=n;var r=Math.max(Math.max(e.height,e.radius),e.width);t.maxWidth=t.maxWidth<r?r:t.maxWidth}},{key:"_placeInTree",value:function(t,e,i){1==i&&void 0!==i||this._updateBranchMass(t,e);var n,o=t.children.NW.range;n=o.maxX>e.x?o.maxY>e.y?"NW":"SW":o.maxY>e.y?"NE":"SE",this._placeInRegion(t,e,n)}},{key:"_placeInRegion",value:function(t,e,i){var n=t.children[i];switch(n.childrenCount){case 0:n.children.data=e,n.childrenCount=1,this._updateBranchMass(n,e);break;case 1:n.children.data.x===e.x&&n.children.data.y===e.y?(e.x+=this._rng(),e.y+=this._rng()):(this._splitBranch(n),this._placeInTree(n,e));break;case 4:this._placeInTree(n,e)}}},{key:"_splitBranch",value:function(t){var e=null;1===t.childrenCount&&(e=t.children.data,t.mass=0,t.centerOfMass.x=0,t.centerOfMass.y=0),t.childrenCount=4,t.children.data=null,this._insertRegion(t,"NW"),this._insertRegion(t,"NE"),this._insertRegion(t,"SW"),this._insertRegion(t,"SE"),null!=e&&this._placeInTree(t,e)}},{key:"_insertRegion",value:function(t,e){var i,n,o,r,s=.5*t.size;switch(e){case"NW":i=t.range.minX,n=t.range.minX+s,o=t.range.minY,r=t.range.minY+s;break;case"NE":i=t.range.minX+s,n=t.range.maxX,o=t.range.minY,r=t.range.minY+s;break;case"SW":i=t.range.minX,n=t.range.minX+s,o=t.range.minY+s,r=t.range.maxY;break;case"SE":i=t.range.minX+s,n=t.range.maxX,o=t.range.minY+s,r=t.range.maxY}t.children[e]={centerOfMass:{x:0,y:0},mass:0,range:{minX:i,maxX:n,minY:o,maxY:r},size:.5*t.size,calcSize:2*t.calcSize,children:{data:null},maxWidth:0,level:t.level+1,childrenCount:0}}},{key:"_debug",value:function(t,e){void 0!==this.barnesHutTree&&(t.lineWidth=1,this._drawBranch(this.barnesHutTree.root,t,e))}},{key:"_drawBranch",value:function(t,e,i){void 0===i&&(i="#FF0000"),4===t.childrenCount&&(this._drawBranch(t.children.NW,e),this._drawBranch(t.children.NE,e),this._drawBranch(t.children.SE,e),this._drawBranch(t.children.SW,e)),e.strokeStyle=i,e.beginPath(),e.moveTo(t.range.minX,t.range.minY),e.lineTo(t.range.maxX,t.range.minY),e.stroke(),e.beginPath(),e.moveTo(t.range.maxX,t.range.minY),e.lineTo(t.range.maxX,t.range.maxY),e.stroke(),e.beginPath(),e.moveTo(t.range.maxX,t.range.maxY),e.lineTo(t.range.minX,t.range.maxY),e.stroke(),e.beginPath(),e.moveTo(t.range.minX,t.range.maxY),e.lineTo(t.range.minX,t.range.minY),e.stroke()}}]),t}(),pC=function(){function t(e,i,n){Yd(this,t),this._rng=jy("REPULSION SOLVER"),this.body=e,this.physicsBody=i,this.setOptions(n)}return Kd(t,[{key:"setOptions",value:function(t){this.options=t}},{key:"solve",value:function(){for(var t,e,i,n,o,r,s,a,h=this.body.nodes,l=this.physicsBody.physicsNodeIndices,d=this.physicsBody.forces,c=this.options.nodeDistance,u=-2/3/c,f=0;f<l.length-1;f++){s=h[l[f]];for(var p=f+1;p<l.length;p++)t=(a=h[l[p]]).x-s.x,e=a.y-s.y,0===(i=Math.sqrt(t*t+e*e))&&(t=i=.1*this._rng()),i<2*c&&(r=i<.5*c?1:u*i+1.3333333333333333,n=t*(r/=i),o=e*r,d[s.id].x-=n,d[s.id].y-=o,d[a.id].x+=n,d[a.id].y+=o)}}}]),t}(),vC=function(){function t(e,i,n){Yd(this,t),this.body=e,this.physicsBody=i,this.setOptions(n)}return Kd(t,[{key:"setOptions",value:function(t){this.options=t,this.overlapAvoidanceFactor=Math.max(0,Math.min(1,this.options.avoidOverlap||0))}},{key:"solve",value:function(){for(var t=this.body.nodes,e=this.physicsBody.physicsNodeIndices,i=this.physicsBody.forces,n=this.options.nodeDistance,o=0;o<e.length-1;o++)for(var r=t[e[o]],s=o+1;s<e.length;s++){var a=t[e[s]];if(r.level===a.level){var h=n+this.overlapAvoidanceFactor*((r.shape.radius||0)/2+(a.shape.radius||0)/2),l=a.x-r.x,d=a.y-r.y,c=Math.sqrt(l*l+d*d),u=void 0;u=c<h?-Math.pow(.05*c,2)+Math.pow(.05*h,2):0,0!==c&&(u/=c);var f=l*u,p=d*u;i[r.id].x-=f,i[r.id].y-=p,i[a.id].x+=f,i[a.id].y+=p}}}}]),t}(),gC=function(){function t(e,i,n){Yd(this,t),this.body=e,this.physicsBody=i,this.setOptions(n)}return Kd(t,[{key:"setOptions",value:function(t){this.options=t}},{key:"solve",value:function(){for(var t,e,i,n,o,r=this.physicsBody.physicsEdgeIndices,s=this.body.edges,a=0;a<r.length;a++)!0===(e=s[r[a]]).connected&&e.toId!==e.fromId&&void 0!==this.body.nodes[e.toId]&&void 0!==this.body.nodes[e.fromId]&&(void 0!==e.edgeType.via?(t=void 0===e.options.length?this.options.springLength:e.options.length,i=e.to,n=e.edgeType.via,o=e.from,this._calculateSpringForce(i,n,.5*t),this._calculateSpringForce(n,o,.5*t)):(t=void 0===e.options.length?1.5*this.options.springLength:e.options.length,this._calculateSpringForce(e.from,e.to,t)))}},{key:"_calculateSpringForce",value:function(t,e,i){var n=t.x-e.x,o=t.y-e.y,r=Math.max(Math.sqrt(n*n+o*o),.01),s=this.options.springConstant*(i-r)/r,a=n*s,h=o*s;void 0!==this.physicsBody.forces[t.id]&&(this.physicsBody.forces[t.id].x+=a,this.physicsBody.forces[t.id].y+=h),void 0!==this.physicsBody.forces[e.id]&&(this.physicsBody.forces[e.id].x-=a,this.physicsBody.forces[e.id].y-=h)}}]),t}(),yC=function(){function t(e,i,n){Yd(this,t),this.body=e,this.physicsBody=i,this.setOptions(n)}return Kd(t,[{key:"setOptions",value:function(t){this.options=t}},{key:"solve",value:function(){for(var t,e,i,n,o,r,s,a,h,l,d=this.body.edges,c=.5,u=this.physicsBody.physicsEdgeIndices,f=this.physicsBody.physicsNodeIndices,p=this.physicsBody.forces,v=0;v<f.length;v++){var g=f[v];p[g].springFx=0,p[g].springFy=0}for(var y=0;y<u.length;y++)!0===(e=d[u[y]]).connected&&(t=void 0===e.options.length?this.options.springLength:e.options.length,i=e.from.x-e.to.x,n=e.from.y-e.to.y,a=0===(a=Math.sqrt(i*i+n*n))?.01:a,o=i*(s=this.options.springConstant*(t-a)/a),r=n*s,e.to.level!=e.from.level?(void 0!==p[e.toId]&&(p[e.toId].springFx-=o,p[e.toId].springFy-=r),void 0!==p[e.fromId]&&(p[e.fromId].springFx+=o,p[e.fromId].springFy+=r)):(void 0!==p[e.toId]&&(p[e.toId].x-=c*o,p[e.toId].y-=c*r),void 0!==p[e.fromId]&&(p[e.fromId].x+=c*o,p[e.fromId].y+=c*r)));s=1;for(var m=0;m<f.length;m++){var b=f[m];h=Math.min(s,Math.max(-s,p[b].springFx)),l=Math.min(s,Math.max(-s,p[b].springFy)),p[b].x+=h,p[b].y+=l}for(var w=0,k=0,_=0;_<f.length;_++){var x=f[_];w+=p[x].x,k+=p[x].y}for(var E=w/f.length,O=k/f.length,C=0;C<f.length;C++){var S=f[C];p[S].x-=E,p[S].y-=O}}}]),t}(),mC=function(){function t(e,i,n){Yd(this,t),this.body=e,this.physicsBody=i,this.setOptions(n)}return Kd(t,[{key:"setOptions",value:function(t){this.options=t}},{key:"solve",value:function(){for(var t,e,i,n,o=this.body.nodes,r=this.physicsBody.physicsNodeIndices,s=this.physicsBody.forces,a=0;a<r.length;a++){t=-(n=o[r[a]]).x,e=-n.y,i=Math.sqrt(t*t+e*e),this._calculateForces(i,t,e,s,n)}}},{key:"_calculateForces",value:function(t,e,i,n,o){var r=0===t?0:this.options.centralGravity/t;n[o.id].x=e*r,n[o.id].y=i*r}}]),t}();function bC(t){var e=function(){if("undefined"==typeof Reflect||!Mk)return!1;if(Mk.sham)return!1;if("function"==typeof Proxy)return!0;try{return Boolean.prototype.valueOf.call(Mk(Boolean,[],(function(){}))),!0}catch(t){return!1}}();return function(){var i,n=Ak(t);if(e){var o=Ak(this).constructor;i=Mk(n,arguments,o)}else i=n.apply(this,arguments);return Nk(this,i)}}var wC=function(t){zk(i,t);var e=bC(i);function i(t,n,o){var r;return Yd(this,i),(r=e.call(this,t,n,o))._rng=jy("FORCE ATLAS 2 BASED REPULSION SOLVER"),r}return Kd(i,[{key:"_calculateForces",value:function(t,e,i,n,o){0===t&&(e=t=.1*this._rng()),this.overlapAvoidanceFactor<1&&n.shape.radius&&(t=Math.max(.1+this.overlapAvoidanceFactor*n.shape.radius,t-n.shape.radius));var r=n.edges.length+1,s=this.options.gravitationalConstant*o.mass*n.options.mass*r/Math.pow(t,2),a=e*s,h=i*s;this.physicsBody.forces[n.id].x+=a,this.physicsBody.forces[n.id].y+=h}}]),i}(fC);function kC(t){var e=function(){if("undefined"==typeof Reflect||!Mk)return!1;if(Mk.sham)return!1;if("function"==typeof Proxy)return!0;try{return Boolean.prototype.valueOf.call(Mk(Boolean,[],(function(){}))),!0}catch(t){return!1}}();return function(){var i,n=Ak(t);if(e){var o=Ak(this).constructor;i=Mk(n,arguments,o)}else i=n.apply(this,arguments);return Nk(this,i)}}var _C=function(t){zk(i,t);var e=kC(i);function i(t,n,o){return Yd(this,i),e.call(this,t,n,o)}return Kd(i,[{key:"_calculateForces",value:function(t,e,i,n,o){if(t>0){var r=o.edges.length+1,s=this.options.centralGravity*r*o.options.mass;n[o.id].x=e*s,n[o.id].y=i*s}}}]),i}(mC),xC=function(){function t(e){Yd(this,t),this.body=e,this.physicsBody={physicsNodeIndices:[],physicsEdgeIndices:[],forces:{},velocities:{}},this.physicsEnabled=!0,this.simulationInterval=1e3/60,this.requiresTimeout=!0,this.previousStates={},this.referenceState={},this.freezeCache={},this.renderTimer=void 0,this.adaptiveTimestep=!1,this.adaptiveTimestepEnabled=!1,this.adaptiveCounter=0,this.adaptiveInterval=3,this.stabilized=!1,this.startedStabilization=!1,this.stabilizationIterations=0,this.ready=!1,this.options={},this.defaultOptions={enabled:!0,barnesHut:{theta:.5,gravitationalConstant:-2e3,centralGravity:.3,springLength:95,springConstant:.04,damping:.09,avoidOverlap:0},forceAtlas2Based:{theta:.5,gravitationalConstant:-50,centralGravity:.01,springConstant:.08,springLength:100,damping:.4,avoidOverlap:0},repulsion:{centralGravity:.2,springLength:200,springConstant:.05,nodeDistance:100,damping:.09,avoidOverlap:0},hierarchicalRepulsion:{centralGravity:0,springLength:100,springConstant:.01,nodeDistance:120,damping:.09},maxVelocity:50,minVelocity:.75,solver:"barnesHut",stabilization:{enabled:!0,iterations:1e3,updateInterval:50,onlyDynamicEdges:!1,fit:!0},timestep:.5,adaptiveTimestep:!0,wind:{x:0,y:0}},un(this.options,this.defaultOptions),this.timestep=.5,this.layoutFailed=!1,this.bindEventListeners()}return Kd(t,[{key:"bindEventListeners",value:function(){var t=this;this.body.emitter.on("initPhysics",(function(){t.initPhysics()})),this.body.emitter.on("_layoutFailed",(function(){t.layoutFailed=!0})),this.body.emitter.on("resetPhysics",(function(){t.stopSimulation(),t.ready=!1})),this.body.emitter.on("disablePhysics",(function(){t.physicsEnabled=!1,t.stopSimulation()})),this.body.emitter.on("restorePhysics",(function(){t.setOptions(t.options),!0===t.ready&&t.startSimulation()})),this.body.emitter.on("startSimulation",(function(){!0===t.ready&&t.startSimulation()})),this.body.emitter.on("stopSimulation",(function(){t.stopSimulation()})),this.body.emitter.on("destroy",(function(){t.stopSimulation(!1),t.body.emitter.off()})),this.body.emitter.on("_dataChanged",(function(){t.updatePhysicsData()}))}},{key:"setOptions",value:function(t){if(void 0!==t)if(!1===t)this.options.enabled=!1,this.physicsEnabled=!1,this.stopSimulation();else if(!0===t)this.options.enabled=!0,this.physicsEnabled=!0,this.startSimulation();else{this.physicsEnabled=!0,im(["stabilization"],this.options,t),Sm(this.options,t,"stabilization"),void 0===t.enabled&&(this.options.enabled=!0),!1===this.options.enabled&&(this.physicsEnabled=!1,this.stopSimulation());var e=this.options.wind;e&&(("number"!=typeof e.x||ek(e.x))&&(e.x=0),("number"!=typeof e.y||ek(e.y))&&(e.y=0)),this.timestep=this.options.timestep}this.init()}},{key:"init",value:function(){var t;"forceAtlas2Based"===this.options.solver?(t=this.options.forceAtlas2Based,this.nodesSolver=new wC(this.body,this.physicsBody,t),this.edgesSolver=new gC(this.body,this.physicsBody,t),this.gravitySolver=new _C(this.body,this.physicsBody,t)):"repulsion"===this.options.solver?(t=this.options.repulsion,this.nodesSolver=new pC(this.body,this.physicsBody,t),this.edgesSolver=new gC(this.body,this.physicsBody,t),this.gravitySolver=new mC(this.body,this.physicsBody,t)):"hierarchicalRepulsion"===this.options.solver?(t=this.options.hierarchicalRepulsion,this.nodesSolver=new vC(this.body,this.physicsBody,t),this.edgesSolver=new yC(this.body,this.physicsBody,t),this.gravitySolver=new mC(this.body,this.physicsBody,t)):(t=this.options.barnesHut,this.nodesSolver=new fC(this.body,this.physicsBody,t),this.edgesSolver=new gC(this.body,this.physicsBody,t),this.gravitySolver=new mC(this.body,this.physicsBody,t)),this.modelOptions=t}},{key:"initPhysics",value:function(){!0===this.physicsEnabled&&!0===this.options.enabled?!0===this.options.stabilization.enabled?this.stabilize():(this.stabilized=!1,this.ready=!0,this.body.emitter.emit("fit",{},this.layoutFailed),this.startSimulation()):(this.ready=!0,this.body.emitter.emit("fit"))}},{key:"startSimulation",value:function(){var t;!0===this.physicsEnabled&&!0===this.options.enabled?(this.stabilized=!1,this.adaptiveTimestep=!1,this.body.emitter.emit("_resizeNodes"),void 0===this.viewFunction&&(this.viewFunction=zn(t=this.simulationStep).call(t,this),this.body.emitter.on("initRedraw",this.viewFunction),this.body.emitter.emit("_startRendering"))):this.body.emitter.emit("_redraw")}},{key:"stopSimulation",value:function(){var t=!(arguments.length>0&&void 0!==arguments[0])||arguments[0];this.stabilized=!0,!0===t&&this._emitStabilized(),void 0!==this.viewFunction&&(this.body.emitter.off("initRedraw",this.viewFunction),this.viewFunction=void 0,!0===t&&this.body.emitter.emit("_stopRendering"))}},{key:"simulationStep",value:function(){var t=Eu();this.physicsTick(),(Eu()-t<.4*this.simulationInterval||!0===this.runDoubleSpeed)&&!1===this.stabilized&&(this.physicsTick(),this.runDoubleSpeed=!0),!0===this.stabilized&&this.stopSimulation()}},{key:"_emitStabilized",value:function(){var t=this,e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:this.stabilizationIterations;(this.stabilizationIterations>1||!0===this.startedStabilization)&&Sv((function(){t.body.emitter.emit("stabilized",{iterations:e}),t.startedStabilization=!1,t.stabilizationIterations=0}),0)}},{key:"physicsStep",value:function(){this.gravitySolver.solve(),this.nodesSolver.solve(),this.edgesSolver.solve(),this.moveNodes()}},{key:"adjustTimeStep",value:function(){!0===this._evaluateStepQuality()?this.timestep=1.2*this.timestep:this.timestep/1.2<this.options.timestep?this.timestep=this.options.timestep:(this.adaptiveCounter=-1,this.timestep=Math.max(this.options.timestep,this.timestep/1.2))}},{key:"physicsTick",value:function(){if(this._startStabilizing(),!0!==this.stabilized){if(!0===this.adaptiveTimestep&&!0===this.adaptiveTimestepEnabled)this.adaptiveCounter%this.adaptiveInterval==0?(this.timestep=2*this.timestep,this.physicsStep(),this.revert(),this.timestep=.5*this.timestep,this.physicsStep(),this.physicsStep(),this.adjustTimeStep()):this.physicsStep(),this.adaptiveCounter+=1;else this.timestep=this.options.timestep,this.physicsStep();!0===this.stabilized&&this.revert(),this.stabilizationIterations++}}},{key:"updatePhysicsData",value:function(){this.physicsBody.forces={},this.physicsBody.physicsNodeIndices=[],this.physicsBody.physicsEdgeIndices=[];var t=this.body.nodes,e=this.body.edges;for(var i in t)Object.prototype.hasOwnProperty.call(t,i)&&!0===t[i].options.physics&&this.physicsBody.physicsNodeIndices.push(t[i].id);for(var n in e)Object.prototype.hasOwnProperty.call(e,n)&&!0===e[n].options.physics&&this.physicsBody.physicsEdgeIndices.push(e[n].id);for(var o=0;o<this.physicsBody.physicsNodeIndices.length;o++){var r=this.physicsBody.physicsNodeIndices[o];this.physicsBody.forces[r]={x:0,y:0},void 0===this.physicsBody.velocities[r]&&(this.physicsBody.velocities[r]={x:0,y:0})}for(var s in this.physicsBody.velocities)void 0===t[s]&&delete this.physicsBody.velocities[s]}},{key:"revert",value:function(){var t=bu(this.previousStates),e=this.body.nodes,i=this.physicsBody.velocities;this.referenceState={};for(var n=0;n<t.length;n++){var o=t[n];void 0!==e[o]?!0===e[o].options.physics&&(this.referenceState[o]={positions:{x:e[o].x,y:e[o].y}},i[o].x=this.previousStates[o].vx,i[o].y=this.previousStates[o].vy,e[o].x=this.previousStates[o].x,e[o].y=this.previousStates[o].y):delete this.previousStates[o]}}},{key:"_evaluateStepQuality",value:function(){var t,e,i=this.body.nodes,n=this.referenceState;for(var o in this.referenceState)if(Object.prototype.hasOwnProperty.call(this.referenceState,o)&&void 0!==i[o]&&(t=i[o].x-n[o].positions.x,e=i[o].y-n[o].positions.y,Math.sqrt(Math.pow(t,2)+Math.pow(e,2))>.3))return!1;return!0}},{key:"moveNodes",value:function(){for(var t=this.physicsBody.physicsNodeIndices,e=0,i=0,n=0;n<t.length;n++){var o=t[n],r=this._performStep(o);e=Math.max(e,r),i+=r}this.adaptiveTimestepEnabled=i/t.length<5,this.stabilized=e<this.options.minVelocity}},{key:"calculateComponentVelocity",value:function(t,e,i){t+=(e-this.modelOptions.damping*t)/i*this.timestep;var n=this.options.maxVelocity||1e9;return Math.abs(t)>n&&(t=t>0?n:-n),t}},{key:"_performStep",value:function(t){var e=this.body.nodes[t],i=this.physicsBody.forces[t];this.options.wind&&(i.x+=this.options.wind.x,i.y+=this.options.wind.y);var n=this.physicsBody.velocities[t];return this.previousStates[t]={x:e.x,y:e.y,vx:n.x,vy:n.y},!1===e.options.fixed.x?(n.x=this.calculateComponentVelocity(n.x,i.x,e.options.mass),e.x+=n.x*this.timestep):(i.x=0,n.x=0),!1===e.options.fixed.y?(n.y=this.calculateComponentVelocity(n.y,i.y,e.options.mass),e.y+=n.y*this.timestep):(i.y=0,n.y=0),Math.sqrt(Math.pow(n.x,2)+Math.pow(n.y,2))}},{key:"_freezeNodes",value:function(){var t=this.body.nodes;for(var e in t)if(Object.prototype.hasOwnProperty.call(t,e)&&t[e].x&&t[e].y){var i=t[e].options.fixed;this.freezeCache[e]={x:i.x,y:i.y},i.x=!0,i.y=!0}}},{key:"_restoreFrozenNodes",value:function(){var t=this.body.nodes;for(var e in t)Object.prototype.hasOwnProperty.call(t,e)&&void 0!==this.freezeCache[e]&&(t[e].options.fixed.x=this.freezeCache[e].x,t[e].options.fixed.y=this.freezeCache[e].y);this.freezeCache={}}},{key:"stabilize",value:function(){var t=this,e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:this.options.stabilization.iterations;"number"!=typeof e&&(e=this.options.stabilization.iterations,console.error("The stabilize method needs a numeric amount of iterations. Switching to default: ",e)),0!==this.physicsBody.physicsNodeIndices.length?(this.adaptiveTimestep=this.options.adaptiveTimestep,this.body.emitter.emit("_resizeNodes"),this.stopSimulation(),this.stabilized=!1,this.body.emitter.emit("_blockRedraw"),this.targetIterations=e,!0===this.options.stabilization.onlyDynamicEdges&&this._freezeNodes(),this.stabilizationIterations=0,Sv((function(){return t._stabilizationBatch()}),0)):this.ready=!0}},{key:"_startStabilizing",value:function(){return!0!==this.startedStabilization&&(this.body.emitter.emit("startStabilizing"),this.startedStabilization=!0,!0)}},{key:"_stabilizationBatch",value:function(){var t=this,e=function(){return!1===t.stabilized&&t.stabilizationIterations<t.targetIterations},i=function(){t.body.emitter.emit("stabilizationProgress",{iterations:t.stabilizationIterations,total:t.targetIterations})};this._startStabilizing()&&i();for(var n,o=0;e()&&o<this.options.stabilization.updateInterval;)this.physicsTick(),o++;(i(),e())?Sv(zn(n=this._stabilizationBatch).call(n,this),0):this._finalizeStabilization()}},{key:"_finalizeStabilization",value:function(){this.body.emitter.emit("_allowRedraw"),!0===this.options.stabilization.fit&&this.body.emitter.emit("fit"),!0===this.options.stabilization.onlyDynamicEdges&&this._restoreFrozenNodes(),this.body.emitter.emit("stabilizationIterationsDone"),this.body.emitter.emit("_requestRedraw"),!0===this.stabilized?this._emitStabilized():this.startSimulation(),this.ready=!0}},{key:"_drawForces",value:function(t){for(var e=0;e<this.physicsBody.physicsNodeIndices.length;e++){var i=this.physicsBody.physicsNodeIndices[e],n=this.body.nodes[i],o=this.physicsBody.forces[i],r=Math.sqrt(Math.pow(o.x,2)+Math.pow(o.x,2)),s=Math.min(Math.max(5,r),15),a=3*s,h=km((180-180*Math.min(1,Math.max(0,.03*r)))/360,1,1),l={x:n.x+20*o.x,y:n.y+20*o.y};t.lineWidth=s,t.strokeStyle=h,t.beginPath(),t.moveTo(n.x,n.y),t.lineTo(l.x,l.y),t.stroke();var d=Math.atan2(o.y,o.x);t.fillStyle=h,XO.draw(t,{type:"arrow",point:l,angle:d,length:a}),jv(t).call(t)}}}]),t}(),EC=function(){function t(){Yd(this,t)}return Kd(t,null,[{key:"getRange",value:function(t){var e,i=arguments.length>1&&void 0!==arguments[1]?arguments[1]:[],n=1e9,o=-1e9,r=1e9,s=-1e9;if(i.length>0)for(var a=0;a<i.length;a++)r>(e=t[i[a]]).shape.boundingBox.left&&(r=e.shape.boundingBox.left),s<e.shape.boundingBox.right&&(s=e.shape.boundingBox.right),n>e.shape.boundingBox.top&&(n=e.shape.boundingBox.top),o<e.shape.boundingBox.bottom&&(o=e.shape.boundingBox.bottom);return 1e9===r&&-1e9===s&&1e9===n&&-1e9===o&&(n=0,o=0,r=0,s=0),{minX:r,maxX:s,minY:n,maxY:o}}},{key:"getRangeCore",value:function(t){var e,i=arguments.length>1&&void 0!==arguments[1]?arguments[1]:[],n=1e9,o=-1e9,r=1e9,s=-1e9;if(i.length>0)for(var a=0;a<i.length;a++)r>(e=t[i[a]]).x&&(r=e.x),s<e.x&&(s=e.x),n>e.y&&(n=e.y),o<e.y&&(o=e.y);return 1e9===r&&-1e9===s&&1e9===n&&-1e9===o&&(n=0,o=0,r=0,s=0),{minX:r,maxX:s,minY:n,maxY:o}}},{key:"findCenter",value:function(t){return{x:.5*(t.maxX+t.minX),y:.5*(t.maxY+t.minY)}}},{key:"cloneOptions",value:function(t,e){var i={};return void 0===e||"node"===e?(nm(i,t.options,!0),i.x=t.x,i.y=t.y,i.amountOfConnections=t.edges.length):nm(i,t.options,!0),i}}]),t}();function OC(t){var e=function(){if("undefined"==typeof Reflect||!Mk)return!1;if(Mk.sham)return!1;if("function"==typeof Proxy)return!0;try{return Boolean.prototype.valueOf.call(Mk(Boolean,[],(function(){}))),!0}catch(t){return!1}}();return function(){var i,n=Ak(t);if(e){var o=Ak(this).constructor;i=Mk(n,arguments,o)}else i=n.apply(this,arguments);return Nk(this,i)}}var CC=function(t){zk(i,t);var e=OC(i);function i(t,n,o,r,s,a){var h;return Yd(this,i),(h=e.call(this,t,n,o,r,s,a)).isCluster=!0,h.containedNodes={},h.containedEdges={},h}return Kd(i,[{key:"_openChildCluster",value:function(t){var e=this,i=this.body.nodes[t];if(void 0===this.containedNodes[t])throw new Error("node with id: "+t+" not in current cluster");if(!i.isCluster)throw new Error("node with id: "+t+" is not a cluster");delete this.containedNodes[t],hm(i.edges,(function(t){delete e.containedEdges[t.id]})),hm(i.containedNodes,(function(t,i){e.containedNodes[i]=t})),i.containedNodes={},hm(i.containedEdges,(function(t,i){e.containedEdges[i]=t})),i.containedEdges={},hm(i.edges,(function(t){hm(e.edges,(function(i){var n,o,r=Fp(n=i.clusteringEdgeReplacingIds).call(n,t.id);-1!==r&&(hm(t.clusteringEdgeReplacingIds,(function(t){i.clusteringEdgeReplacingIds.push(t),e.body.edges[t].edgeReplacedById=i.id})),ff(o=i.clusteringEdgeReplacingIds).call(o,r,1))}))})),i.edges=[]}}]),i}(fO),SC=function(){function t(e){var i=this;Yd(this,t),this.body=e,this.clusteredNodes={},this.clusteredEdges={},this.options={},this.defaultOptions={},un(this.options,this.defaultOptions),this.body.emitter.on("_resetData",(function(){i.clusteredNodes={},i.clusteredEdges={}}))}return Kd(t,[{key:"clusterByHubsize",value:function(t,e){void 0===t?t=this._getHubSize():"object"===Qc(t)&&(e=this._checkOptions(t),t=this._getHubSize());for(var i=[],n=0;n<this.body.nodeIndices.length;n++){var o=this.body.nodes[this.body.nodeIndices[n]];o.edges.length>=t&&i.push(o.id)}for(var r=0;r<i.length;r++)this.clusterByConnection(i[r],e,!0);this.body.emitter.emit("_dataChanged")}},{key:"cluster",value:function(){var t=this,e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:{},i=!(arguments.length>1&&void 0!==arguments[1])||arguments[1];if(void 0===e.joinCondition)throw new Error("Cannot call clusterByNodeData without a joinCondition function in the options.");e=this._checkOptions(e);var n={},o={};hm(this.body.nodes,(function(i,r){i.options&&!0===e.joinCondition(i.options)&&(n[r]=i,hm(i.edges,(function(e){void 0===t.clusteredEdges[e.id]&&(o[e.id]=e)})))})),this._cluster(n,o,e,i)}},{key:"clusterByEdgeCount",value:function(t,e){var i=this,n=!(arguments.length>2&&void 0!==arguments[2])||arguments[2];e=this._checkOptions(e);for(var o,r,s,a=[],h={},l=function(n){var l={},d={},c=i.body.nodeIndices[n],u=i.body.nodes[c];if(void 0===h[c]){s=0,r=[];for(var f=0;f<u.edges.length;f++)o=u.edges[f],void 0===i.clusteredEdges[o.id]&&(o.toId!==o.fromId&&s++,r.push(o));if(s===t){for(var p=function(t){if(void 0===e.joinCondition||null===e.joinCondition)return!0;var i=EC.cloneOptions(t);return e.joinCondition(i)},v=!0,g=0;g<r.length;g++){o=r[g];var y=i._getConnectedId(o,c);if(!p(u)){v=!1;break}d[o.id]=o,l[c]=u,l[y]=i.body.nodes[y],h[c]=!0}if(bu(l).length>0&&bu(d).length>0&&!0===v){var m=function(){for(var t=0;t<a.length;++t)for(var e in l)if(void 0!==a[t].nodes[e])return a[t]}();if(void 0!==m){for(var b in l)void 0===m.nodes[b]&&(m.nodes[b]=l[b]);for(var w in d)void 0===m.edges[w]&&(m.edges[w]=d[w])}else a.push({nodes:l,edges:d})}}}},d=0;d<this.body.nodeIndices.length;d++)l(d);for(var c=0;c<a.length;c++)this._cluster(a[c].nodes,a[c].edges,e,!1);!0===n&&this.body.emitter.emit("_dataChanged")}},{key:"clusterOutliers",value:function(t){var e=!(arguments.length>1&&void 0!==arguments[1])||arguments[1];this.clusterByEdgeCount(1,t,e)}},{key:"clusterBridges",value:function(t){var e=!(arguments.length>1&&void 0!==arguments[1])||arguments[1];this.clusterByEdgeCount(2,t,e)}},{key:"clusterByConnection",value:function(t,e){var i,n=!(arguments.length>2&&void 0!==arguments[2])||arguments[2];if(void 0===t)throw new Error("No nodeId supplied to clusterByConnection!");if(void 0===this.body.nodes[t])throw new Error("The nodeId given to clusterByConnection does not exist!");var o=this.body.nodes[t];void 0===(e=this._checkOptions(e,o)).clusterNodeProperties.x&&(e.clusterNodeProperties.x=o.x),void 0===e.clusterNodeProperties.y&&(e.clusterNodeProperties.y=o.y),void 0===e.clusterNodeProperties.fixed&&(e.clusterNodeProperties.fixed={},e.clusterNodeProperties.fixed.x=o.options.fixed.x,e.clusterNodeProperties.fixed.y=o.options.fixed.y);var r={},s={},a=o.id,h=EC.cloneOptions(o);r[a]=o;for(var l=0;l<o.edges.length;l++){var d=o.edges[l];if(void 0===this.clusteredEdges[d.id]){var c=this._getConnectedId(d,a);if(void 0===this.clusteredNodes[c])if(c!==a)if(void 0===e.joinCondition)s[d.id]=d,r[c]=this.body.nodes[c];else{var u=EC.cloneOptions(this.body.nodes[c]);!0===e.joinCondition(h,u)&&(s[d.id]=d,r[c]=this.body.nodes[c])}else s[d.id]=d}}var f=gu(i=bu(r)).call(i,(function(t){return r[t].id}));for(var p in r)if(Object.prototype.hasOwnProperty.call(r,p))for(var v=r[p],g=0;g<v.edges.length;g++){var y=v.edges[g];Fp(f).call(f,this._getConnectedId(y,v.id))>-1&&(s[y.id]=y)}this._cluster(r,s,e,n)}},{key:"_createClusterEdges",value:function(t,e,i,n){for(var o,r,s,a,h,l,d=bu(t),c=[],u=0;u<d.length;u++){s=t[r=d[u]];for(var f=0;f<s.edges.length;f++)o=s.edges[f],void 0===this.clusteredEdges[o.id]&&(o.toId==o.fromId?e[o.id]=o:o.toId==r?(a=i.id,l=h=o.fromId):(a=o.toId,h=i.id,l=a),void 0===t[l]&&c.push({edge:o,fromId:h,toId:a}))}for(var p=[],v=function(t){for(var e=0;e<p.length;e++){var i=p[e],n=t.fromId===i.fromId&&t.toId===i.toId,o=t.fromId===i.toId&&t.toId===i.fromId;if(n||o)return i}return null},g=0;g<c.length;g++){var y=c[g],m=y.edge,b=v(y);null===b?(b=this._createClusteredEdge(y.fromId,y.toId,m,n),p.push(b)):b.clusteringEdgeReplacingIds.push(m.id),this.body.edges[m.id].edgeReplacedById=b.id,this._backupEdgeOptions(m),m.setOptions({physics:!1})}}},{key:"_checkOptions",value:function(){var t=arguments.length>0&&void 0!==arguments[0]?arguments[0]:{};return void 0===t.clusterEdgeProperties&&(t.clusterEdgeProperties={}),void 0===t.clusterNodeProperties&&(t.clusterNodeProperties={}),t}},{key:"_cluster",value:function(t,e,i){var n=!(arguments.length>3&&void 0!==arguments[3])||arguments[3],o=[];for(var r in t)Object.prototype.hasOwnProperty.call(t,r)&&void 0!==this.clusteredNodes[r]&&o.push(r);for(var s=0;s<o.length;++s)delete t[o[s]];if(0!=bu(t).length&&(1!=bu(t).length||1==i.clusterNodeProperties.allowSingleNodeCluster)){var a=nm({},i.clusterNodeProperties);if(void 0!==i.processProperties){var h=[];for(var l in t)if(Object.prototype.hasOwnProperty.call(t,l)){var d=EC.cloneOptions(t[l]);h.push(d)}var c=[];for(var u in e)if(Object.prototype.hasOwnProperty.call(e,u)&&"clusterEdge:"!==u.substr(0,12)){var f=EC.cloneOptions(e[u],"edge");c.push(f)}if(!(a=i.processProperties(a,h,c)))throw new Error("The processProperties function does not return properties!")}void 0===a.id&&(a.id="cluster:"+Ax());var p=a.id;void 0===a.label&&(a.label="cluster");var v=void 0;void 0===a.x&&(v=this._getClusterPosition(t),a.x=v.x),void 0===a.y&&(void 0===v&&(v=this._getClusterPosition(t)),a.y=v.y),a.id=p;var g=this.body.functions.createNode(a,CC);g.containedNodes=t,g.containedEdges=e,g.clusterEdgeProperties=i.clusterEdgeProperties,this.body.nodes[a.id]=g,this._clusterEdges(t,e,a,i.clusterEdgeProperties),a.id=void 0,!0===n&&this.body.emitter.emit("_dataChanged")}}},{key:"_backupEdgeOptions",value:function(t){void 0===this.clusteredEdges[t.id]&&(this.clusteredEdges[t.id]={physics:t.options.physics})}},{key:"_restoreEdge",value:function(t){var e=this.clusteredEdges[t.id];void 0!==e&&(t.setOptions({physics:e.physics}),delete this.clusteredEdges[t.id])}},{key:"isCluster",value:function(t){return void 0!==this.body.nodes[t]?!0===this.body.nodes[t].isCluster:(console.error("Node does not exist."),!1)}},{key:"_getClusterPosition",value:function(t){for(var e,i=bu(t),n=t[i[0]].x,o=t[i[0]].x,r=t[i[0]].y,s=t[i[0]].y,a=1;a<i.length;a++)n=(e=t[i[a]]).x<n?e.x:n,o=e.x>o?e.x:o,r=e.y<r?e.y:r,s=e.y>s?e.y:s;return{x:.5*(n+o),y:.5*(r+s)}}},{key:"openCluster",value:function(t,e){var i=!(arguments.length>2&&void 0!==arguments[2])||arguments[2];if(void 0===t)throw new Error("No clusterNodeId supplied to openCluster.");var n=this.body.nodes[t];if(void 0===n)throw new Error("The clusterNodeId supplied to openCluster does not exist.");if(!0!==n.isCluster||void 0===n.containedNodes||void 0===n.containedEdges)throw new Error("The node:"+t+" is not a valid cluster.");var o=this.findNode(t),r=Fp(o).call(o,t)-1;if(r>=0){var s=o[r],a=this.body.nodes[s];return a._openChildCluster(t),delete this.body.nodes[t],void(!0===i&&this.body.emitter.emit("_dataChanged"))}var h=n.containedNodes,l=n.containedEdges;if(void 0!==e&&void 0!==e.releaseFunction&&"function"==typeof e.releaseFunction){var d={},c={x:n.x,y:n.y};for(var u in h)if(Object.prototype.hasOwnProperty.call(h,u)){var f=this.body.nodes[u];d[u]={x:f.x,y:f.y}}var p=e.releaseFunction(c,d);for(var v in h)if(Object.prototype.hasOwnProperty.call(h,v)){var g=this.body.nodes[v];void 0!==p[v]&&(g.x=void 0===p[v].x?n.x:p[v].x,g.y=void 0===p[v].y?n.y:p[v].y)}}else hm(h,(function(t){!1===t.options.fixed.x&&(t.x=n.x),!1===t.options.fixed.y&&(t.y=n.y)}));for(var y in h)if(Object.prototype.hasOwnProperty.call(h,y)){var m=this.body.nodes[y];m.vx=n.vx,m.vy=n.vy,m.setOptions({physics:!0}),delete this.clusteredNodes[y]}for(var b=[],w=0;w<n.edges.length;w++)b.push(n.edges[w]);for(var k=0;k<b.length;k++){for(var _=b[k],x=this._getConnectedId(_,t),E=this.clusteredNodes[x],O=0;O<_.clusteringEdgeReplacingIds.length;O++){var C=_.clusteringEdgeReplacingIds[O],S=this.body.edges[C];if(void 0!==S)if(void 0!==E){var T=this.body.nodes[E.clusterId];T.containedEdges[S.id]=S,delete l[S.id];var M=S.fromId,P=S.toId;S.toId==x?P=E.clusterId:M=E.clusterId,this._createClusteredEdge(M,P,S,T.clusterEdgeProperties,{hidden:!1,physics:!0})}else this._restoreEdge(S)}_.remove()}for(var D in l)Object.prototype.hasOwnProperty.call(l,D)&&this._restoreEdge(l[D]);delete this.body.nodes[t],!0===i&&this.body.emitter.emit("_dataChanged")}},{key:"getNodesInCluster",value:function(t){var e=[];if(!0===this.isCluster(t)){var i=this.body.nodes[t].containedNodes;for(var n in i)Object.prototype.hasOwnProperty.call(i,n)&&e.push(this.body.nodes[n].id)}return e}},{key:"findNode",value:function(t){for(var e,i=[],n=0;void 0!==this.clusteredNodes[t]&&n<100;){if(void 0===(e=this.body.nodes[t]))return[];i.push(e.id),t=this.clusteredNodes[t].clusterId,n++}return void 0===(e=this.body.nodes[t])?[]:(i.push(e.id),Yu(i).call(i),i)}},{key:"updateClusteredNode",value:function(t,e){if(void 0===t)throw new Error("No clusteredNodeId supplied to updateClusteredNode.");if(void 0===e)throw new Error("No newOptions supplied to updateClusteredNode.");if(void 0===this.body.nodes[t])throw new Error("The clusteredNodeId supplied to updateClusteredNode does not exist.");this.body.nodes[t].setOptions(e),this.body.emitter.emit("_dataChanged")}},{key:"updateEdge",value:function(t,e){if(void 0===t)throw new Error("No startEdgeId supplied to updateEdge.");if(void 0===e)throw new Error("No newOptions supplied to updateEdge.");if(void 0===this.body.edges[t])throw new Error("The startEdgeId supplied to updateEdge does not exist.");for(var i=this.getClusteredEdges(t),n=0;n<i.length;n++){this.body.edges[i[n]].setOptions(e)}this.body.emitter.emit("_dataChanged")}},{key:"getClusteredEdges",value:function(t){for(var e=[],i=0;void 0!==t&&void 0!==this.body.edges[t]&&i<100;)e.push(this.body.edges[t].id),t=this.body.edges[t].edgeReplacedById,i++;return Yu(e).call(e),e}},{key:"getBaseEdge",value:function(t){return this.getBaseEdges(t)[0]}},{key:"getBaseEdges",value:function(t){for(var e=[t],i=[],n=[],o=0;e.length>0&&o<100;){var r=e.pop();if(void 0!==r){var s=this.body.edges[r];if(void 0!==s){o++;var a=s.clusteringEdgeReplacingIds;if(void 0===a)n.push(r);else for(var h=0;h<a.length;++h){var l=a[h];-1===Fp(e).call(e,a)&&-1===Fp(i).call(i,a)&&e.push(l)}i.push(r)}}}return n}},{key:"_getConnectedId",value:function(t,e){return t.toId!=e?t.toId:(t.fromId,t.fromId)}},{key:"_getHubSize",value:function(){for(var t=0,e=0,i=0,n=0,o=0;o<this.body.nodeIndices.length;o++){var r=this.body.nodes[this.body.nodeIndices[o]];r.edges.length>n&&(n=r.edges.length),t+=r.edges.length,e+=Math.pow(r.edges.length,2),i+=1}t/=i;var s=(e/=i)-Math.pow(t,2),a=Math.sqrt(s),h=Math.floor(t+2*a);return h>n&&(h=n),h}},{key:"_createClusteredEdge",value:function(t,e,i,n,o){var r=EC.cloneOptions(i,"edge");nm(r,n),r.from=t,r.to=e,r.id="clusterEdge:"+Ax(),void 0!==o&&nm(r,o);var s=this.body.functions.createEdge(r);return s.clusteringEdgeReplacingIds=[i.id],s.connect(),this.body.edges[s.id]=s,s}},{key:"_clusterEdges",value:function(t,e,i,n){if(e instanceof cC){var o=e,r={};r[o.id]=o,e=r}if(t instanceof fO){var s=t,a={};a[s.id]=s,t=a}if(null==i)throw new Error("_clusterEdges: parameter clusterNode required");for(var h in void 0===n&&(n=i.clusterEdgeProperties),this._createClusterEdges(t,e,i,n),e)if(Object.prototype.hasOwnProperty.call(e,h)&&void 0!==this.body.edges[h]){var l=this.body.edges[h];this._backupEdgeOptions(l),l.setOptions({physics:!1})}for(var d in t)Object.prototype.hasOwnProperty.call(t,d)&&(this.clusteredNodes[d]={clusterId:i.id,node:this.body.nodes[d]},this.body.nodes[d].setOptions({physics:!1}))}},{key:"_getClusterNodeForNode",value:function(t){if(void 0!==t){var e=this.clusteredNodes[t];if(void 0!==e){var i=e.clusterId;if(void 0!==i)return this.body.nodes[i]}}}},{key:"_filter",value:function(t,e){var i=[];return hm(t,(function(t){e(t)&&i.push(t)})),i}},{key:"_updateState",value:function(){var t,e=this,i=[],n={},o=function(t){hm(e.body.nodes,(function(e){!0===e.isCluster&&t(e)}))};for(t in this.clusteredNodes){if(Object.prototype.hasOwnProperty.call(this.clusteredNodes,t))void 0===this.body.nodes[t]&&i.push(t)}o((function(t){for(var e=0;e<i.length;e++)delete t.containedNodes[i[e]]}));for(var r=0;r<i.length;r++)delete this.clusteredNodes[i[r]];hm(this.clusteredEdges,(function(t){var i=e.body.edges[t];void 0!==i&&i.endPointsValid()||(n[t]=t)})),o((function(t){hm(t.containedEdges,(function(t,e){t.endPointsValid()||n[e]||(n[e]=e)}))})),hm(this.body.edges,(function(t,i){var o=!0,r=t.clusteringEdgeReplacingIds;if(void 0!==r){var s=0;hm(r,(function(t){var i=e.body.edges[t];void 0!==i&&i.endPointsValid()&&(s+=1)})),o=s>0}t.endPointsValid()&&o||(n[i]=i)})),o((function(t){hm(n,(function(i){delete t.containedEdges[i],hm(t.edges,(function(o,r){o.id!==i?o.clusteringEdgeReplacingIds=e._filter(o.clusteringEdgeReplacingIds,(function(t){return!n[t]})):t.edges[r]=null})),t.edges=e._filter(t.edges,(function(t){return null!==t}))}))})),hm(n,(function(t){delete e.clusteredEdges[t]})),hm(n,(function(t){delete e.body.edges[t]})),hm(bu(this.body.edges),(function(t){var i=e.body.edges[t],n=e._isClusteredNode(i.fromId)||e._isClusteredNode(i.toId);if(n!==e._isClusteredEdge(i.id))if(n){var o=e._getClusterNodeForNode(i.fromId);void 0!==o&&e._clusterEdges(e.body.nodes[i.fromId],i,o);var r=e._getClusterNodeForNode(i.toId);void 0!==r&&e._clusterEdges(e.body.nodes[i.toId],i,r)}else delete e._clusterEdges[t],e._restoreEdge(i)}));for(var s=!1,a=!0,h=function(){var t=[];o((function(e){var i=bu(e.containedNodes).length,n=!0===e.options.allowSingleNodeCluster;(n&&i<1||!n&&i<2)&&t.push(e.id)}));for(var i=0;i<t.length;++i)e.openCluster(t[i],{},!1);a=t.length>0,s=s||a};a;)h();s&&this._updateState()}},{key:"_isClusteredNode",value:function(t){return void 0!==this.clusteredNodes[t]}},{key:"_isClusteredEdge",value:function(t){return void 0!==this.clusteredEdges[t]}}]),t}();function TC(t,e){var i=void 0!==tu&&ih(t)||t["@@iterator"];if(!i){if(lu(t)||(i=function(t,e){var i;if(!t)return;if("string"==typeof t)return MC(t,e);var n=au(i=Object.prototype.toString.call(t)).call(i,8,-1);"Object"===n&&t.constructor&&(n=t.constructor.name);if("Map"===n||"Set"===n)return ja(t);if("Arguments"===n||/^(?:Ui|I)nt(?:8|16|32)(?:Clamped)?Array$/.test(n))return MC(t,e)}(t))||e&&t&&"number"==typeof t.length){i&&(t=i);var n=0,o=function(){};return{s:o,n:function(){return n>=t.length?{done:!0}:{done:!1,value:t[n++]}},e:function(t){throw t},f:o}}throw new TypeError("Invalid attempt to iterate non-iterable instance.\nIn order to be iterable, non-array objects must have a [Symbol.iterator]() method.")}var r,s=!0,a=!1;return{s:function(){i=i.call(t)},n:function(){var t=i.next();return s=t.done,t},e:function(t){a=!0,r=t},f:function(){try{s||null==i.return||i.return()}finally{if(a)throw r}}}}function MC(t,e){(null==e||e>t.length)&&(e=t.length);for(var i=0,n=new Array(e);i<e;i++)n[i]=t[i];return n}var PC=function(){function t(e,i){var n;Yd(this,t),void 0!==window&&(n=window.requestAnimationFrame||window.mozRequestAnimationFrame||window.webkitRequestAnimationFrame||window.msRequestAnimationFrame),window.requestAnimationFrame=void 0===n?function(t){t()}:n,this.body=e,this.canvas=i,this.redrawRequested=!1,this.renderTimer=void 0,this.requiresTimeout=!0,this.renderingActive=!1,this.renderRequests=0,this.allowRedraw=!0,this.dragging=!1,this.zooming=!1,this.options={},this.defaultOptions={hideEdgesOnDrag:!1,hideEdgesOnZoom:!1,hideNodesOnDrag:!1},un(this.options,this.defaultOptions),this._determineBrowserMethod(),this.bindEventListeners()}return Kd(t,[{key:"bindEventListeners",value:function(){var t,e=this;this.body.emitter.on("dragStart",(function(){e.dragging=!0})),this.body.emitter.on("dragEnd",(function(){e.dragging=!1})),this.body.emitter.on("zoom",(function(){e.zooming=!0,window.clearTimeout(e.zoomTimeoutId),e.zoomTimeoutId=Sv((function(){var t;e.zooming=!1,zn(t=e._requestRedraw).call(t,e)()}),250)})),this.body.emitter.on("_resizeNodes",(function(){e._resizeNodes()})),this.body.emitter.on("_redraw",(function(){!1===e.renderingActive&&e._redraw()})),this.body.emitter.on("_blockRedraw",(function(){e.allowRedraw=!1})),this.body.emitter.on("_allowRedraw",(function(){e.allowRedraw=!0,e.redrawRequested=!1})),this.body.emitter.on("_requestRedraw",zn(t=this._requestRedraw).call(t,this)),this.body.emitter.on("_startRendering",(function(){e.renderRequests+=1,e.renderingActive=!0,e._startRendering()})),this.body.emitter.on("_stopRendering",(function(){e.renderRequests-=1,e.renderingActive=e.renderRequests>0,e.renderTimer=void 0})),this.body.emitter.on("destroy",(function(){e.renderRequests=0,e.allowRedraw=!1,e.renderingActive=!1,!0===e.requiresTimeout?clearTimeout(e.renderTimer):window.cancelAnimationFrame(e.renderTimer),e.body.emitter.off()}))}},{key:"setOptions",value:function(t){if(void 0!==t){em(["hideEdgesOnDrag","hideEdgesOnZoom","hideNodesOnDrag"],this.options,t)}}},{key:"_requestNextFrame",value:function(t,e){if("undefined"!=typeof window){var i,n=window;return!0===this.requiresTimeout?i=Sv(t,e):n.requestAnimationFrame&&(i=n.requestAnimationFrame(t)),i}}},{key:"_startRendering",value:function(){var t;!0===this.renderingActive&&(void 0===this.renderTimer&&(this.renderTimer=this._requestNextFrame(zn(t=this._renderStep).call(t,this),this.simulationInterval)))}},{key:"_renderStep",value:function(){!0===this.renderingActive&&(this.renderTimer=void 0,!0===this.requiresTimeout&&this._startRendering(),this._redraw(),!1===this.requiresTimeout&&this._startRendering())}},{key:"redraw",value:function(){this.body.emitter.emit("setSize"),this._redraw()}},{key:"_requestRedraw",value:function(){var t=this;!0!==this.redrawRequested&&!1===this.renderingActive&&!0===this.allowRedraw&&(this.redrawRequested=!0,this._requestNextFrame((function(){t._redraw(!1)}),0))}},{key:"_redraw",value:function(){var t=arguments.length>0&&void 0!==arguments[0]&&arguments[0];if(!0===this.allowRedraw){this.body.emitter.emit("initRedraw"),this.redrawRequested=!1;var e={drawExternalLabels:null};0!==this.canvas.frame.canvas.width&&0!==this.canvas.frame.canvas.height||this.canvas.setSize(),this.canvas.setTransform();var i=this.canvas.getContext(),n=this.canvas.frame.canvas.clientWidth,o=this.canvas.frame.canvas.clientHeight;if(i.clearRect(0,0,n,o),0===this.canvas.frame.clientWidth)return;if(i.save(),i.translate(this.body.view.translation.x,this.body.view.translation.y),i.scale(this.body.view.scale,this.body.view.scale),i.beginPath(),this.body.emitter.emit("beforeDrawing",i),i.closePath(),!1===t&&(!1===this.dragging||!0===this.dragging&&!1===this.options.hideEdgesOnDrag)&&(!1===this.zooming||!0===this.zooming&&!1===this.options.hideEdgesOnZoom)&&this._drawEdges(i),!1===this.dragging||!0===this.dragging&&!1===this.options.hideNodesOnDrag){var r=this._drawNodes(i,t),s=r.drawExternalLabels;e.drawExternalLabels=s}!1===t&&(!1===this.dragging||!0===this.dragging&&!1===this.options.hideEdgesOnDrag)&&(!1===this.zooming||!0===this.zooming&&!1===this.options.hideEdgesOnZoom)&&this._drawArrows(i),null!=e.drawExternalLabels&&e.drawExternalLabels(),!1===t&&this._drawSelectionBox(i),i.beginPath(),this.body.emitter.emit("afterDrawing",i),i.closePath(),i.restore(),!0===t&&i.clearRect(0,0,n,o)}}},{key:"_resizeNodes",value:function(){this.canvas.setTransform();var t=this.canvas.getContext();t.save(),t.translate(this.body.view.translation.x,this.body.view.translation.y),t.scale(this.body.view.scale,this.body.view.scale);var e,i=this.body.nodes;for(var n in i)Object.prototype.hasOwnProperty.call(i,n)&&((e=i[n]).resize(t),e.updateBoundingBox(t,e.selected));t.restore()}},{key:"_drawNodes",value:function(t){for(var e,i,n=arguments.length>1&&void 0!==arguments[1]&&arguments[1],o=this.body.nodes,r=this.body.nodeIndices,s=[],a=[],h=20,l=this.canvas.DOMtoCanvas({x:-h,y:-h}),d=this.canvas.DOMtoCanvas({x:this.canvas.frame.canvas.clientWidth+h,y:this.canvas.frame.canvas.clientHeight+h}),c={top:l.y,left:l.x,bottom:d.y,right:d.x},u=[],f=0;f<r.length;f++)if((e=o[r[f]]).hover)a.push(r[f]);else if(e.isSelected())s.push(r[f]);else if(!0===n){var p=e.draw(t);null!=p.drawExternalLabel&&u.push(p.drawExternalLabel)}else if(!0===e.isBoundingBoxOverlappingWith(c)){var v=e.draw(t);null!=v.drawExternalLabel&&u.push(v.drawExternalLabel)}else e.updateBoundingBox(t,e.selected);var g=s.length,y=a.length;for(i=0;i<g;i++){var m=(e=o[s[i]]).draw(t);null!=m.drawExternalLabel&&u.push(m.drawExternalLabel)}for(i=0;i<y;i++){var b=(e=o[a[i]]).draw(t);null!=b.drawExternalLabel&&u.push(b.drawExternalLabel)}return{drawExternalLabels:function(){var t,e=TC(u);try{for(e.s();!(t=e.n()).done;){(0,t.value)()}}catch(t){e.e(t)}finally{e.f()}}}}},{key:"_drawEdges",value:function(t){for(var e=this.body.edges,i=this.body.edgeIndices,n=0;n<i.length;n++){var o=e[i[n]];!0===o.connected&&o.draw(t)}}},{key:"_drawArrows",value:function(t){for(var e=this.body.edges,i=this.body.edgeIndices,n=0;n<i.length;n++){var o=e[i[n]];!0===o.connected&&o.drawArrows(t)}}},{key:"_determineBrowserMethod",value:function(){if("undefined"!=typeof window){var t=navigator.userAgent.toLowerCase();this.requiresTimeout=!1,(-1!=Fp(t).call(t,"msie 9.0")||-1!=Fp(t).call(t,"safari")&&Fp(t).call(t,"chrome")<=-1)&&(this.requiresTimeout=!0)}else this.requiresTimeout=!0}},{key:"_drawSelectionBox",value:function(t){if(this.body.selectionBox.show){t.beginPath();var e=this.body.selectionBox.position.end.x-this.body.selectionBox.position.start.x,i=this.body.selectionBox.position.end.y-this.body.selectionBox.position.start.y;t.rect(this.body.selectionBox.position.start.x,this.body.selectionBox.position.start.y,e,i),t.fillStyle="rgba(151, 194, 252, 0.2)",t.fillRect(this.body.selectionBox.position.start.x,this.body.selectionBox.position.start.y,e,i),t.strokeStyle="rgba(151, 194, 252, 1)",t.stroke()}else t.closePath()}}]),t}(),DC=X.setInterval;function IC(t,e){e.inputHandler=function(t){t.isFirst&&e(t)},t.on("hammer.input",e.inputHandler)}function BC(t,e){return e.inputHandler=function(t){t.isFinal&&e(t)},t.on("hammer.input",e.inputHandler)}var zC=function(){function t(e){Yd(this,t),this.body=e,this.pixelRatio=1,this.cameraState={},this.initialized=!1,this.canvasViewCenter={},this._cleanupCallbacks=[],this.options={},this.defaultOptions={autoResize:!0,height:"100%",width:"100%"},un(this.options,this.defaultOptions),this.bindEventListeners()}return Kd(t,[{key:"bindEventListeners",value:function(){var t,e=this;this.body.emitter.once("resize",(function(t){0!==t.width&&(e.body.view.translation.x=.5*t.width),0!==t.height&&(e.body.view.translation.y=.5*t.height)})),this.body.emitter.on("setSize",zn(t=this.setSize).call(t,this)),this.body.emitter.on("destroy",(function(){e.hammerFrame.destroy(),e.hammer.destroy(),e._cleanUp()}))}},{key:"setOptions",value:function(t){var e=this;if(void 0!==t){em(["width","height","autoResize"],this.options,t)}if(this._cleanUp(),!0===this.options.autoResize){var i;if(window.ResizeObserver){var n=new ResizeObserver((function(){!0===e.setSize()&&e.body.emitter.emit("_requestRedraw")})),o=this.frame;n.observe(o),this._cleanupCallbacks.push((function(){n.unobserve(o)}))}else{var r=DC((function(){!0===e.setSize()&&e.body.emitter.emit("_requestRedraw")}),1e3);this._cleanupCallbacks.push((function(){clearInterval(r)}))}var s=zn(i=this._onResize).call(i,this);dm(window,"resize",s),this._cleanupCallbacks.push((function(){cm(window,"resize",s)}))}}},{key:"_cleanUp",value:function(){var t,e,i;Fu(t=Yu(e=ff(i=this._cleanupCallbacks).call(i,0)).call(e)).call(t,(function(t){try{t()}catch(t){console.error(t)}}))}},{key:"_onResize",value:function(){this.setSize(),this.body.emitter.emit("_redraw")}},{key:"_getCameraState",value:function(){var t=arguments.length>0&&void 0!==arguments[0]?arguments[0]:this.pixelRatio;!0===this.initialized&&(this.cameraState.previousWidth=this.frame.canvas.width/t,this.cameraState.previousHeight=this.frame.canvas.height/t,this.cameraState.scale=this.body.view.scale,this.cameraState.position=this.DOMtoCanvas({x:.5*this.frame.canvas.width/t,y:.5*this.frame.canvas.height/t}))}},{key:"_setCameraState",value:function(){if(void 0!==this.cameraState.scale&&0!==this.frame.canvas.clientWidth&&0!==this.frame.canvas.clientHeight&&0!==this.pixelRatio&&this.cameraState.previousWidth>0&&this.cameraState.previousHeight>0){var t=this.frame.canvas.width/this.pixelRatio/this.cameraState.previousWidth,e=this.frame.canvas.height/this.pixelRatio/this.cameraState.previousHeight,i=this.cameraState.scale;1!=t&&1!=e?i=.5*this.cameraState.scale*(t+e):1!=t?i=this.cameraState.scale*t:1!=e&&(i=this.cameraState.scale*e),this.body.view.scale=i;var n=this.DOMtoCanvas({x:.5*this.frame.canvas.clientWidth,y:.5*this.frame.canvas.clientHeight}),o={x:n.x-this.cameraState.position.x,y:n.y-this.cameraState.position.y};this.body.view.translation.x+=o.x*this.body.view.scale,this.body.view.translation.y+=o.y*this.body.view.scale}}},{key:"_prepareValue",value:function(t){if("number"==typeof t)return t+"px";if("string"==typeof t){if(-1!==Fp(t).call(t,"%")||-1!==Fp(t).call(t,"px"))return t;if(-1===Fp(t).call(t,"%"))return t+"px"}throw new Error("Could not use the value supplied for width or height:"+t)}},{key:"_create",value:function(){for(;this.body.container.hasChildNodes();)this.body.container.removeChild(this.body.container.firstChild);if(this.frame=document.createElement("div"),this.frame.className="vis-network",this.frame.style.position="relative",this.frame.style.overflow="hidden",this.frame.tabIndex=0,this.frame.canvas=document.createElement("canvas"),this.frame.canvas.style.position="relative",this.frame.appendChild(this.frame.canvas),this.frame.canvas.getContext)this._setPixelRatio(),this.setTransform();else{var t=document.createElement("DIV");t.style.color="red",t.style.fontWeight="bold",t.style.padding="10px",t.innerText="Error: your browser does not support HTML canvas",this.frame.canvas.appendChild(t)}this.body.container.appendChild(this.frame),this.body.view.scale=1,this.body.view.translation={x:.5*this.frame.canvas.clientWidth,y:.5*this.frame.canvas.clientHeight},this._bindHammer()}},{key:"_bindHammer",value:function(){var t=this;void 0!==this.hammer&&this.hammer.destroy(),this.drag={},this.pinch={},this.hammer=new Wm(this.frame.canvas),this.hammer.get("pinch").set({enable:!0}),this.hammer.get("pan").set({threshold:5,direction:Wm.DIRECTION_ALL}),IC(this.hammer,(function(e){t.body.eventListeners.onTouch(e)})),this.hammer.on("tap",(function(e){t.body.eventListeners.onTap(e)})),this.hammer.on("doubletap",(function(e){t.body.eventListeners.onDoubleTap(e)})),this.hammer.on("press",(function(e){t.body.eventListeners.onHold(e)})),this.hammer.on("panstart",(function(e){t.body.eventListeners.onDragStart(e)})),this.hammer.on("panmove",(function(e){t.body.eventListeners.onDrag(e)})),this.hammer.on("panend",(function(e){t.body.eventListeners.onDragEnd(e)})),this.hammer.on("pinch",(function(e){t.body.eventListeners.onPinch(e)})),this.frame.canvas.addEventListener("wheel",(function(e){t.body.eventListeners.onMouseWheel(e)})),this.frame.canvas.addEventListener("mousemove",(function(e){t.body.eventListeners.onMouseMove(e)})),this.frame.canvas.addEventListener("contextmenu",(function(e){t.body.eventListeners.onContext(e)})),this.hammerFrame=new Wm(this.frame),BC(this.hammerFrame,(function(e){t.body.eventListeners.onRelease(e)}))}},{key:"setSize",value:function(){var t=arguments.length>0&&void 0!==arguments[0]?arguments[0]:this.options.width,e=arguments.length>1&&void 0!==arguments[1]?arguments[1]:this.options.height;t=this._prepareValue(t),e=this._prepareValue(e);var i=!1,n=this.frame.canvas.width,o=this.frame.canvas.height,r=this.pixelRatio;if(this._setPixelRatio(),t!=this.options.width||e!=this.options.height||this.frame.style.width!=t||this.frame.style.height!=e)this._getCameraState(r),this.frame.style.width=t,this.frame.style.height=e,this.frame.canvas.style.width="100%",this.frame.canvas.style.height="100%",this.frame.canvas.width=Math.round(this.frame.canvas.clientWidth*this.pixelRatio),this.frame.canvas.height=Math.round(this.frame.canvas.clientHeight*this.pixelRatio),this.options.width=t,this.options.height=e,this.canvasViewCenter={x:.5*this.frame.clientWidth,y:.5*this.frame.clientHeight},i=!0;else{var s=Math.round(this.frame.canvas.clientWidth*this.pixelRatio),a=Math.round(this.frame.canvas.clientHeight*this.pixelRatio);this.frame.canvas.width===s&&this.frame.canvas.height===a||this._getCameraState(r),this.frame.canvas.width!==s&&(this.frame.canvas.width=s,i=!0),this.frame.canvas.height!==a&&(this.frame.canvas.height=a,i=!0)}return!0===i&&(this.body.emitter.emit("resize",{width:Math.round(this.frame.canvas.width/this.pixelRatio),height:Math.round(this.frame.canvas.height/this.pixelRatio),oldWidth:Math.round(n/this.pixelRatio),oldHeight:Math.round(o/this.pixelRatio)}),this._setCameraState()),this.initialized=!0,i}},{key:"getContext",value:function(){return this.frame.canvas.getContext("2d")}},{key:"_determinePixelRatio",value:function(){var t=this.getContext();if(void 0===t)throw new Error("Could not get canvax context");var e=1;return"undefined"!=typeof window&&(e=window.devicePixelRatio||1),e/(t.webkitBackingStorePixelRatio||t.mozBackingStorePixelRatio||t.msBackingStorePixelRatio||t.oBackingStorePixelRatio||t.backingStorePixelRatio||1)}},{key:"_setPixelRatio",value:function(){this.pixelRatio=this._determinePixelRatio()}},{key:"setTransform",value:function(){var t=this.getContext();if(void 0===t)throw new Error("Could not get canvax context");t.setTransform(this.pixelRatio,0,0,this.pixelRatio,0,0)}},{key:"_XconvertDOMtoCanvas",value:function(t){return(t-this.body.view.translation.x)/this.body.view.scale}},{key:"_XconvertCanvasToDOM",value:function(t){return t*this.body.view.scale+this.body.view.translation.x}},{key:"_YconvertDOMtoCanvas",value:function(t){return(t-this.body.view.translation.y)/this.body.view.scale}},{key:"_YconvertCanvasToDOM",value:function(t){return t*this.body.view.scale+this.body.view.translation.y}},{key:"canvasToDOM",value:function(t){return{x:this._XconvertCanvasToDOM(t.x),y:this._YconvertCanvasToDOM(t.y)}}},{key:"DOMtoCanvas",value:function(t){return{x:this._XconvertDOMtoCanvas(t.x),y:this._YconvertDOMtoCanvas(t.y)}}}]),t}();function NC(t,e){var i=un({nodes:e,minZoomLevel:Number.MIN_VALUE,maxZoomLevel:1},null!=t?t:{});if(!lu(i.nodes))throw new TypeError("Nodes has to be an array of ids.");if(0===i.nodes.length&&(i.nodes=e),!("number"==typeof i.minZoomLevel&&i.minZoomLevel>0))throw new TypeError("Min zoom level has to be a number higher than zero.");if(!("number"==typeof i.maxZoomLevel&&i.minZoomLevel<=i.maxZoomLevel))throw new TypeError("Max zoom level has to be a number higher than min zoom level.");return i}var FC=function(){function t(e,i){var n,o,r=this;Yd(this,t),this.body=e,this.canvas=i,this.animationSpeed=1/this.renderRefreshRate,this.animationEasingFunction="easeInOutQuint",this.easingTime=0,this.sourceScale=0,this.targetScale=0,this.sourceTranslation=0,this.targetTranslation=0,this.lockedOnNodeId=void 0,this.lockedOnNodeOffset=void 0,this.touchTime=0,this.viewFunction=void 0,this.body.emitter.on("fit",zn(n=this.fit).call(n,this)),this.body.emitter.on("animationFinished",(function(){r.body.emitter.emit("_stopRendering")})),this.body.emitter.on("unlockNode",zn(o=this.releaseNode).call(o,this))}return Kd(t,[{key:"setOptions",value:function(){var t=arguments.length>0&&void 0!==arguments[0]?arguments[0]:{};this.options=t}},{key:"fit",value:function(t){var e=arguments.length>1&&void 0!==arguments[1]&&arguments[1];t=NC(t,this.body.nodeIndices);var i,n,o=this.canvas.frame.canvas.clientWidth,r=this.canvas.frame.canvas.clientHeight;if(0===o||0===r)n=1,i=EC.getRange(this.body.nodes,t.nodes);else if(!0===e){var s=0;for(var a in this.body.nodes)if(Object.prototype.hasOwnProperty.call(this.body.nodes,a)){var h=this.body.nodes[a];!0===h.predefinedPosition&&(s+=1)}if(s>.5*this.body.nodeIndices.length)return void this.fit(t,!1);i=EC.getRange(this.body.nodes,t.nodes);var l=this.body.nodeIndices.length;n=12.662/(l+7.4147)+.0964822;var d=Math.min(o/600,r/600);n*=d}else{this.body.emitter.emit("_resizeNodes"),i=EC.getRange(this.body.nodes,t.nodes);var c=1.1*Math.abs(i.maxX-i.minX),u=1.1*Math.abs(i.maxY-i.minY),f=o/c,p=r/u;n=f<=p?f:p}n>t.maxZoomLevel?n=t.maxZoomLevel:n<t.minZoomLevel&&(n=t.minZoomLevel);var v=EC.findCenter(i),g={position:v,scale:n,animation:t.animation};this.moveTo(g)}},{key:"focus",value:function(t){var e=arguments.length>1&&void 0!==arguments[1]?arguments[1]:{};if(void 0!==this.body.nodes[t]){var i={x:this.body.nodes[t].x,y:this.body.nodes[t].y};e.position=i,e.lockedOnNode=t,this.moveTo(e)}else console.error("Node: "+t+" cannot be found.")}},{key:"moveTo",value:function(t){if(void 0!==t){if(null!=t.offset){if(null!=t.offset.x){if(t.offset.x=+t.offset.x,!ok(t.offset.x))throw new TypeError('The option "offset.x" has to be a finite number.')}else t.offset.x=0;if(null!=t.offset.y){if(t.offset.y=+t.offset.y,!ok(t.offset.y))throw new TypeError('The option "offset.y" has to be a finite number.')}else t.offset.x=0}else t.offset={x:0,y:0};if(null!=t.position){if(null!=t.position.x){if(t.position.x=+t.position.x,!ok(t.position.x))throw new TypeError('The option "position.x" has to be a finite number.')}else t.position.x=0;if(null!=t.position.y){if(t.position.y=+t.position.y,!ok(t.position.y))throw new TypeError('The option "position.y" has to be a finite number.')}else t.position.x=0}else t.position=this.getViewPosition();if(null!=t.scale){if(t.scale=+t.scale,!(t.scale>0))throw new TypeError('The option "scale" has to be a number greater than zero.')}else t.scale=this.body.view.scale;void 0===t.animation&&(t.animation={duration:0}),!1===t.animation&&(t.animation={duration:0}),!0===t.animation&&(t.animation={}),void 0===t.animation.duration&&(t.animation.duration=1e3),void 0===t.animation.easingFunction&&(t.animation.easingFunction="easeInOutQuad"),this.animateView(t)}else t={}}},{key:"animateView",value:function(t){if(void 0!==t){this.animationEasingFunction=t.animation.easingFunction,this.releaseNode(),!0===t.locked&&(this.lockedOnNodeId=t.lockedOnNode,this.lockedOnNodeOffset=t.offset),0!=this.easingTime&&this._transitionRedraw(!0),this.sourceScale=this.body.view.scale,this.sourceTranslation=this.body.view.translation,this.targetScale=t.scale,this.body.view.scale=this.targetScale;var e,i,n=this.canvas.DOMtoCanvas({x:.5*this.canvas.frame.canvas.clientWidth,y:.5*this.canvas.frame.canvas.clientHeight}),o=n.x-t.position.x,r=n.y-t.position.y;if(this.targetTranslation={x:this.sourceTranslation.x+o*this.targetScale+t.offset.x,y:this.sourceTranslation.y+r*this.targetScale+t.offset.y},0===t.animation.duration)if(null!=this.lockedOnNodeId)this.viewFunction=zn(e=this._lockedRedraw).call(e,this),this.body.emitter.on("initRedraw",this.viewFunction);else this.body.view.scale=this.targetScale,this.body.view.translation=this.targetTranslation,this.body.emitter.emit("_requestRedraw");else this.animationSpeed=1/(60*t.animation.duration*.001)||1/60,this.animationEasingFunction=t.animation.easingFunction,this.viewFunction=zn(i=this._transitionRedraw).call(i,this),this.body.emitter.on("initRedraw",this.viewFunction),this.body.emitter.emit("_startRendering")}}},{key:"_lockedRedraw",value:function(){var t=this.body.nodes[this.lockedOnNodeId].x,e=this.body.nodes[this.lockedOnNodeId].y,i=this.canvas.DOMtoCanvas({x:.5*this.canvas.frame.canvas.clientWidth,y:.5*this.canvas.frame.canvas.clientHeight}),n=i.x-t,o=i.y-e,r=this.body.view.translation,s={x:r.x+n*this.body.view.scale+this.lockedOnNodeOffset.x,y:r.y+o*this.body.view.scale+this.lockedOnNodeOffset.y};this.body.view.translation=s}},{key:"releaseNode",value:function(){void 0!==this.lockedOnNodeId&&void 0!==this.viewFunction&&(this.body.emitter.off("initRedraw",this.viewFunction),this.lockedOnNodeId=void 0,this.lockedOnNodeOffset=void 0)}},{key:"_transitionRedraw",value:function(){var t=arguments.length>0&&void 0!==arguments[0]&&arguments[0];this.easingTime+=this.animationSpeed,this.easingTime=!0===t?1:this.easingTime;var e=Tm[this.animationEasingFunction](this.easingTime);if(this.body.view.scale=this.sourceScale+(this.targetScale-this.sourceScale)*e,this.body.view.translation={x:this.sourceTranslation.x+(this.targetTranslation.x-this.sourceTranslation.x)*e,y:this.sourceTranslation.y+(this.targetTranslation.y-this.sourceTranslation.y)*e},this.easingTime>=1){var i;if(this.body.emitter.off("initRedraw",this.viewFunction),this.easingTime=0,null!=this.lockedOnNodeId)this.viewFunction=zn(i=this._lockedRedraw).call(i,this),this.body.emitter.on("initRedraw",this.viewFunction);this.body.emitter.emit("animationFinished")}}},{key:"getScale",value:function(){return this.body.view.scale}},{key:"getViewPosition",value:function(){return this.canvas.DOMtoCanvas({x:.5*this.canvas.frame.canvas.clientWidth,y:.5*this.canvas.frame.canvas.clientHeight})}}]),t}();function AC(t){var e,i=t&&t.preventDefault||!1,n=t&&t.container||window,o={},r={keydown:{},keyup:{}},s={};for(e=97;e<=122;e++)s[String.fromCharCode(e)]={code:e-97+65,shift:!1};for(e=65;e<=90;e++)s[String.fromCharCode(e)]={code:e,shift:!0};for(e=0;e<=9;e++)s[""+e]={code:48+e,shift:!1};for(e=1;e<=12;e++)s["F"+e]={code:111+e,shift:!1};for(e=0;e<=9;e++)s["num"+e]={code:96+e,shift:!1};s["num*"]={code:106,shift:!1},s["num+"]={code:107,shift:!1},s["num-"]={code:109,shift:!1},s["num/"]={code:111,shift:!1},s["num."]={code:110,shift:!1},s.left={code:37,shift:!1},s.up={code:38,shift:!1},s.right={code:39,shift:!1},s.down={code:40,shift:!1},s.space={code:32,shift:!1},s.enter={code:13,shift:!1},s.shift={code:16,shift:void 0},s.esc={code:27,shift:!1},s.backspace={code:8,shift:!1},s.tab={code:9,shift:!1},s.ctrl={code:17,shift:!1},s.alt={code:18,shift:!1},s.delete={code:46,shift:!1},s.pageup={code:33,shift:!1},s.pagedown={code:34,shift:!1},s["="]={code:187,shift:!1},s["-"]={code:189,shift:!1},s["]"]={code:221,shift:!1},s["["]={code:219,shift:!1};var a=function(t){l(t,"keydown")},h=function(t){l(t,"keyup")},l=function(t,e){if(void 0!==r[e][t.keyCode]){for(var n=r[e][t.keyCode],o=0;o<n.length;o++)(void 0===n[o].shift||1==n[o].shift&&1==t.shiftKey||0==n[o].shift&&0==t.shiftKey)&&n[o].fn(t);1==i&&t.preventDefault()}};return o.bind=function(t,e,i){if(void 0===i&&(i="keydown"),void 0===s[t])throw new Error("unsupported key: "+t);void 0===r[i][s[t].code]&&(r[i][s[t].code]=[]),r[i][s[t].code].push({fn:e,shift:s[t].shift})},o.bindAll=function(t,e){for(var i in void 0===e&&(e="keydown"),s)s.hasOwnProperty(i)&&o.bind(i,t,e)},o.getKey=function(t){for(var e in s)if(s.hasOwnProperty(e)){if(1==t.shiftKey&&1==s[e].shift&&t.keyCode==s[e].code)return e;if(0==t.shiftKey&&0==s[e].shift&&t.keyCode==s[e].code)return e;if(t.keyCode==s[e].code&&"shift"==e)return e}return"unknown key, currently not supported"},o.unbind=function(t,e,i){if(void 0===i&&(i="keydown"),void 0===s[t])throw new Error("unsupported key: "+t);if(void 0!==e){var n=[],o=r[i][s[t].code];if(void 0!==o)for(var a=0;a<o.length;a++)o[a].fn==e&&o[a].shift==s[t].shift||n.push(r[i][s[t].code][a]);r[i][s[t].code]=n}else r[i][s[t].code]=[]},o.reset=function(){r={keydown:{},keyup:{}}},o.destroy=function(){r={keydown:{},keyup:{}},n.removeEventListener("keydown",a,!0),n.removeEventListener("keyup",h,!0)},n.addEventListener("keydown",a,!0),n.addEventListener("keyup",h,!0),o}var jC=Object.freeze({__proto__:null,default:AC}),RC=function(){function t(e,i){var n=this;Yd(this,t),this.body=e,this.canvas=i,this.iconsCreated=!1,this.navigationHammers=[],this.boundFunctions={},this.touchTime=0,this.activated=!1,this.body.emitter.on("activate",(function(){n.activated=!0,n.configureKeyboardBindings()})),this.body.emitter.on("deactivate",(function(){n.activated=!1,n.configureKeyboardBindings()})),this.body.emitter.on("destroy",(function(){void 0!==n.keycharm&&n.keycharm.destroy()})),this.options={}}return Kd(t,[{key:"setOptions",value:function(t){void 0!==t&&(this.options=t,this.create())}},{key:"create",value:function(){!0===this.options.navigationButtons?!1===this.iconsCreated&&this.loadNavigationElements():!0===this.iconsCreated&&this.cleanNavigation(),this.configureKeyboardBindings()}},{key:"cleanNavigation",value:function(){if(0!=this.navigationHammers.length){for(var t=0;t<this.navigationHammers.length;t++)this.navigationHammers[t].destroy();this.navigationHammers=[]}this.navigationDOM&&this.navigationDOM.wrapper&&this.navigationDOM.wrapper.parentNode&&this.navigationDOM.wrapper.parentNode.removeChild(this.navigationDOM.wrapper),this.iconsCreated=!1}},{key:"loadNavigationElements",value:function(){var t=this;this.cleanNavigation(),this.navigationDOM={};var e=["up","down","left","right","zoomIn","zoomOut","zoomExtends"],i=["_moveUp","_moveDown","_moveLeft","_moveRight","_zoomIn","_zoomOut","_fit"];this.navigationDOM.wrapper=document.createElement("div"),this.navigationDOM.wrapper.className="vis-navigation",this.canvas.frame.appendChild(this.navigationDOM.wrapper);for(var n=0;n<e.length;n++){this.navigationDOM[e[n]]=document.createElement("div"),this.navigationDOM[e[n]].className="vis-button vis-"+e[n],this.navigationDOM.wrapper.appendChild(this.navigationDOM[e[n]]);var o,r,s=new Wm(this.navigationDOM[e[n]]);if("_fit"===i[n])IC(s,zn(o=this._fit).call(o,this));else IC(s,zn(r=this.bindToRedraw).call(r,this,i[n]));this.navigationHammers.push(s)}var a=new Wm(this.canvas.frame);BC(a,(function(){t._stopMovement()})),this.navigationHammers.push(a),this.iconsCreated=!0}},{key:"bindToRedraw",value:function(t){var e;void 0===this.boundFunctions[t]&&(this.boundFunctions[t]=zn(e=this[t]).call(e,this),this.body.emitter.on("initRedraw",this.boundFunctions[t]),this.body.emitter.emit("_startRendering"))}},{key:"unbindFromRedraw",value:function(t){void 0!==this.boundFunctions[t]&&(this.body.emitter.off("initRedraw",this.boundFunctions[t]),this.body.emitter.emit("_stopRendering"),delete this.boundFunctions[t])}},{key:"_fit",value:function(){(new Date).valueOf()-this.touchTime>700&&(this.body.emitter.emit("fit",{duration:700}),this.touchTime=(new Date).valueOf())}},{key:"_stopMovement",value:function(){for(var t in this.boundFunctions)Object.prototype.hasOwnProperty.call(this.boundFunctions,t)&&(this.body.emitter.off("initRedraw",this.boundFunctions[t]),this.body.emitter.emit("_stopRendering"));this.boundFunctions={}}},{key:"_moveUp",value:function(){this.body.view.translation.y+=this.options.keyboard.speed.y}},{key:"_moveDown",value:function(){this.body.view.translation.y-=this.options.keyboard.speed.y}},{key:"_moveLeft",value:function(){this.body.view.translation.x+=this.options.keyboard.speed.x}},{key:"_moveRight",value:function(){this.body.view.translation.x-=this.options.keyboard.speed.x}},{key:"_zoomIn",value:function(){var t=this.body.view.scale,e=this.body.view.scale*(1+this.options.keyboard.speed.zoom),i=this.body.view.translation,n=e/t,o=(1-n)*this.canvas.canvasViewCenter.x+i.x*n,r=(1-n)*this.canvas.canvasViewCenter.y+i.y*n;this.body.view.scale=e,this.body.view.translation={x:o,y:r},this.body.emitter.emit("zoom",{direction:"+",scale:this.body.view.scale,pointer:null})}},{key:"_zoomOut",value:function(){var t=this.body.view.scale,e=this.body.view.scale/(1+this.options.keyboard.speed.zoom),i=this.body.view.translation,n=e/t,o=(1-n)*this.canvas.canvasViewCenter.x+i.x*n,r=(1-n)*this.canvas.canvasViewCenter.y+i.y*n;this.body.view.scale=e,this.body.view.translation={x:o,y:r},this.body.emitter.emit("zoom",{direction:"-",scale:this.body.view.scale,pointer:null})}},{key:"configureKeyboardBindings",value:function(){var t,e,i,n,o,r,s,a,h,l,d,c,u,f,p,v,g,y,m,b,w,k,_,x,E=this;(void 0!==this.keycharm&&this.keycharm.destroy(),!0===this.options.keyboard.enabled)&&(!0===this.options.keyboard.bindToWindow?this.keycharm=AC({container:window,preventDefault:!0}):this.keycharm=AC({container:this.canvas.frame,preventDefault:!0}),this.keycharm.reset(),!0===this.activated&&(zn(t=this.keycharm).call(t,"up",(function(){E.bindToRedraw("_moveUp")}),"keydown"),zn(e=this.keycharm).call(e,"down",(function(){E.bindToRedraw("_moveDown")}),"keydown"),zn(i=this.keycharm).call(i,"left",(function(){E.bindToRedraw("_moveLeft")}),"keydown"),zn(n=this.keycharm).call(n,"right",(function(){E.bindToRedraw("_moveRight")}),"keydown"),zn(o=this.keycharm).call(o,"=",(function(){E.bindToRedraw("_zoomIn")}),"keydown"),zn(r=this.keycharm).call(r,"num+",(function(){E.bindToRedraw("_zoomIn")}),"keydown"),zn(s=this.keycharm).call(s,"num-",(function(){E.bindToRedraw("_zoomOut")}),"keydown"),zn(a=this.keycharm).call(a,"-",(function(){E.bindToRedraw("_zoomOut")}),"keydown"),zn(h=this.keycharm).call(h,"[",(function(){E.bindToRedraw("_zoomOut")}),"keydown"),zn(l=this.keycharm).call(l,"]",(function(){E.bindToRedraw("_zoomIn")}),"keydown"),zn(d=this.keycharm).call(d,"pageup",(function(){E.bindToRedraw("_zoomIn")}),"keydown"),zn(c=this.keycharm).call(c,"pagedown",(function(){E.bindToRedraw("_zoomOut")}),"keydown"),zn(u=this.keycharm).call(u,"up",(function(){E.unbindFromRedraw("_moveUp")}),"keyup"),zn(f=this.keycharm).call(f,"down",(function(){E.unbindFromRedraw("_moveDown")}),"keyup"),zn(p=this.keycharm).call(p,"left",(function(){E.unbindFromRedraw("_moveLeft")}),"keyup"),zn(v=this.keycharm).call(v,"right",(function(){E.unbindFromRedraw("_moveRight")}),"keyup"),zn(g=this.keycharm).call(g,"=",(function(){E.unbindFromRedraw("_zoomIn")}),"keyup"),zn(y=this.keycharm).call(y,"num+",(function(){E.unbindFromRedraw("_zoomIn")}),"keyup"),zn(m=this.keycharm).call(m,"num-",(function(){E.unbindFromRedraw("_zoomOut")}),"keyup"),zn(b=this.keycharm).call(b,"-",(function(){E.unbindFromRedraw("_zoomOut")}),"keyup"),zn(w=this.keycharm).call(w,"[",(function(){E.unbindFromRedraw("_zoomOut")}),"keyup"),zn(k=this.keycharm).call(k,"]",(function(){E.unbindFromRedraw("_zoomIn")}),"keyup"),zn(_=this.keycharm).call(_,"pageup",(function(){E.unbindFromRedraw("_zoomIn")}),"keyup"),zn(x=this.keycharm).call(x,"pagedown",(function(){E.unbindFromRedraw("_zoomOut")}),"keyup")))}}]),t}();function LC(t,e){var i=void 0!==tu&&ih(t)||t["@@iterator"];if(!i){if(lu(t)||(i=function(t,e){var i;if(!t)return;if("string"==typeof t)return HC(t,e);var n=au(i=Object.prototype.toString.call(t)).call(i,8,-1);"Object"===n&&t.constructor&&(n=t.constructor.name);if("Map"===n||"Set"===n)return ja(t);if("Arguments"===n||/^(?:Ui|I)nt(?:8|16|32)(?:Clamped)?Array$/.test(n))return HC(t,e)}(t))||e&&t&&"number"==typeof t.length){i&&(t=i);var n=0,o=function(){};return{s:o,n:function(){return n>=t.length?{done:!0}:{done:!1,value:t[n++]}},e:function(t){throw t},f:o}}throw new TypeError("Invalid attempt to iterate non-iterable instance.\nIn order to be iterable, non-array objects must have a [Symbol.iterator]() method.")}var r,s=!0,a=!1;return{s:function(){i=i.call(t)},n:function(){var t=i.next();return s=t.done,t},e:function(t){a=!0,r=t},f:function(){try{s||null==i.return||i.return()}finally{if(a)throw r}}}}function HC(t,e){(null==e||e>t.length)&&(e=t.length);for(var i=0,n=new Array(e);i<e;i++)n[i]=t[i];return n}var WC=function(){function t(e,i,n){var o,r,s,a,h,l,d,c,u,f,p,v,g;Yd(this,t),this.body=e,this.canvas=i,this.selectionHandler=n,this.navigationHandler=new RC(e,i),this.body.eventListeners.onTap=zn(o=this.onTap).call(o,this),this.body.eventListeners.onTouch=zn(r=this.onTouch).call(r,this),this.body.eventListeners.onDoubleTap=zn(s=this.onDoubleTap).call(s,this),this.body.eventListeners.onHold=zn(a=this.onHold).call(a,this),this.body.eventListeners.onDragStart=zn(h=this.onDragStart).call(h,this),this.body.eventListeners.onDrag=zn(l=this.onDrag).call(l,this),this.body.eventListeners.onDragEnd=zn(d=this.onDragEnd).call(d,this),this.body.eventListeners.onMouseWheel=zn(c=this.onMouseWheel).call(c,this),this.body.eventListeners.onPinch=zn(u=this.onPinch).call(u,this),this.body.eventListeners.onMouseMove=zn(f=this.onMouseMove).call(f,this),this.body.eventListeners.onRelease=zn(p=this.onRelease).call(p,this),this.body.eventListeners.onContext=zn(v=this.onContext).call(v,this),this.touchTime=0,this.drag={},this.pinch={},this.popup=void 0,this.popupObj=void 0,this.popupTimer=void 0,this.body.functions.getPointer=zn(g=this.getPointer).call(g,this),this.options={},this.defaultOptions={dragNodes:!0,dragView:!0,hover:!1,keyboard:{enabled:!1,speed:{x:10,y:10,zoom:.02},bindToWindow:!0,autoFocus:!0},navigationButtons:!1,tooltipDelay:300,zoomView:!0,zoomSpeed:1},un(this.options,this.defaultOptions),this.bindEventListeners()}return Kd(t,[{key:"bindEventListeners",value:function(){var t=this;this.body.emitter.on("destroy",(function(){clearTimeout(t.popupTimer),delete t.body.functions.getPointer}))}},{key:"setOptions",value:function(t){if(void 0!==t){im(["hideEdgesOnDrag","hideEdgesOnZoom","hideNodesOnDrag","keyboard","multiselect","selectable","selectConnectedEdges"],this.options,t),Sm(this.options,t,"keyboard"),t.tooltip&&(un(this.options.tooltip,t.tooltip),t.tooltip.color&&(this.options.tooltip.color=gm(t.tooltip.color)))}this.navigationHandler.setOptions(this.options)}},{key:"getPointer",value:function(t){return{x:t.x-sm(this.canvas.frame.canvas),y:t.y-am(this.canvas.frame.canvas)}}},{key:"onTouch",value:function(t){(new Date).valueOf()-this.touchTime>50&&(this.drag.pointer=this.getPointer(t.center),this.drag.pinched=!1,this.pinch.scale=this.body.view.scale,this.touchTime=(new Date).valueOf())}},{key:"onTap",value:function(t){var e=this.getPointer(t.center),i=this.selectionHandler.options.multiselect&&(t.changedPointers[0].ctrlKey||t.changedPointers[0].metaKey);this.checkSelectionChanges(e,i),this.selectionHandler.commitAndEmit(e,t),this.selectionHandler.generateClickEvent("click",t,e)}},{key:"onDoubleTap",value:function(t){var e=this.getPointer(t.center);this.selectionHandler.generateClickEvent("doubleClick",t,e)}},{key:"onHold",value:function(t){var e=this.getPointer(t.center),i=this.selectionHandler.options.multiselect;this.checkSelectionChanges(e,i),this.selectionHandler.commitAndEmit(e,t),this.selectionHandler.generateClickEvent("click",t,e),this.selectionHandler.generateClickEvent("hold",t,e)}},{key:"onRelease",value:function(t){if((new Date).valueOf()-this.touchTime>10){var e=this.getPointer(t.center);this.selectionHandler.generateClickEvent("release",t,e),this.touchTime=(new Date).valueOf()}}},{key:"onContext",value:function(t){var e=this.getPointer({x:t.clientX,y:t.clientY});this.selectionHandler.generateClickEvent("oncontext",t,e)}},{key:"checkSelectionChanges",value:function(t){var e=arguments.length>1&&void 0!==arguments[1]&&arguments[1];!0===e?this.selectionHandler.selectAdditionalOnPoint(t):this.selectionHandler.selectOnPoint(t)}},{key:"_determineDifference",value:function(t,e){var i=function(t,e){for(var i=[],n=0;n<t.length;n++){var o=t[n];-1===Fp(e).call(e,o)&&i.push(o)}return i};return{nodes:i(t.nodes,e.nodes),edges:i(t.edges,e.edges)}}},{key:"onDragStart",value:function(t){if(!this.drag.dragging){void 0===this.drag.pointer&&this.onTouch(t);var e=this.selectionHandler.getNodeAt(this.drag.pointer);if(this.drag.dragging=!0,this.drag.selection=[],this.drag.translation=un({},this.body.view.translation),this.drag.nodeId=void 0,t.srcEvent.shiftKey){this.body.selectionBox.show=!0;var i=this.getPointer(t.center);this.body.selectionBox.position.start={x:this.canvas._XconvertDOMtoCanvas(i.x),y:this.canvas._YconvertDOMtoCanvas(i.y)},this.body.selectionBox.position.end={x:this.canvas._XconvertDOMtoCanvas(i.x),y:this.canvas._YconvertDOMtoCanvas(i.y)}}if(void 0!==e&&!0===this.options.dragNodes){this.drag.nodeId=e.id,!1===e.isSelected()&&this.selectionHandler.setSelection({nodes:[e.id]}),this.selectionHandler.generateClickEvent("dragStart",t,this.drag.pointer);var n,o=LC(this.selectionHandler.getSelectedNodes());try{for(o.s();!(n=o.n()).done;){var r=n.value,s={id:r.id,node:r,x:r.x,y:r.y,xFixed:r.options.fixed.x,yFixed:r.options.fixed.y};r.options.fixed.x=!0,r.options.fixed.y=!0,this.drag.selection.push(s)}}catch(t){o.e(t)}finally{o.f()}}else this.selectionHandler.generateClickEvent("dragStart",t,this.drag.pointer,void 0,!0)}}},{key:"onDrag",value:function(t){var e=this;if(!0!==this.drag.pinched){this.body.emitter.emit("unlockNode");var i=this.getPointer(t.center),n=this.drag.selection;if(n&&n.length&&!0===this.options.dragNodes){this.selectionHandler.generateClickEvent("dragging",t,i);var o=i.x-this.drag.pointer.x,r=i.y-this.drag.pointer.y;Fu(n).call(n,(function(t){var i=t.node;!1===t.xFixed&&(i.x=e.canvas._XconvertDOMtoCanvas(e.canvas._XconvertCanvasToDOM(t.x)+o)),!1===t.yFixed&&(i.y=e.canvas._YconvertDOMtoCanvas(e.canvas._YconvertCanvasToDOM(t.y)+r))})),this.body.emitter.emit("startSimulation")}else{if(t.srcEvent.shiftKey){if(this.selectionHandler.generateClickEvent("dragging",t,i,void 0,!0),void 0===this.drag.pointer)return void this.onDragStart(t);this.body.selectionBox.position.end={x:this.canvas._XconvertDOMtoCanvas(i.x),y:this.canvas._YconvertDOMtoCanvas(i.y)},this.body.emitter.emit("_requestRedraw")}if(!0===this.options.dragView&&!t.srcEvent.shiftKey){if(this.selectionHandler.generateClickEvent("dragging",t,i,void 0,!0),void 0===this.drag.pointer)return void this.onDragStart(t);var s=i.x-this.drag.pointer.x,a=i.y-this.drag.pointer.y;this.body.view.translation={x:this.drag.translation.x+s,y:this.drag.translation.y+a},this.body.emitter.emit("_requestRedraw")}}}}},{key:"onDragEnd",value:function(t){var e=this;if(this.drag.dragging=!1,this.body.selectionBox.show){var i;this.body.selectionBox.show=!1;var n=this.body.selectionBox.position,o={minX:Math.min(n.start.x,n.end.x),minY:Math.min(n.start.y,n.end.y),maxX:Math.max(n.start.x,n.end.x),maxY:Math.max(n.start.y,n.end.y)},r=Xf(i=this.body.nodeIndices).call(i,(function(t){var i=e.body.nodes[t];return i.x>=o.minX&&i.x<=o.maxX&&i.y>=o.minY&&i.y<=o.maxY}));Fu(r).call(r,(function(t){return e.selectionHandler.selectObject(e.body.nodes[t])}));var s=this.getPointer(t.center);this.selectionHandler.commitAndEmit(s,t),this.selectionHandler.generateClickEvent("dragEnd",t,this.getPointer(t.center),void 0,!0),this.body.emitter.emit("_requestRedraw")}else{var a=this.drag.selection;a&&a.length?(Fu(a).call(a,(function(t){t.node.options.fixed.x=t.xFixed,t.node.options.fixed.y=t.yFixed})),this.selectionHandler.generateClickEvent("dragEnd",t,this.getPointer(t.center)),this.body.emitter.emit("startSimulation")):(this.selectionHandler.generateClickEvent("dragEnd",t,this.getPointer(t.center),void 0,!0),this.body.emitter.emit("_requestRedraw"))}}},{key:"onPinch",value:function(t){var e=this.getPointer(t.center);this.drag.pinched=!0,void 0===this.pinch.scale&&(this.pinch.scale=1);var i=this.pinch.scale*t.scale;this.zoom(i,e)}},{key:"zoom",value:function(t,e){if(!0===this.options.zoomView){var i=this.body.view.scale;t<1e-5&&(t=1e-5),t>10&&(t=10);var n=void 0;void 0!==this.drag&&!0===this.drag.dragging&&(n=this.canvas.DOMtoCanvas(this.drag.pointer));var o=this.body.view.translation,r=t/i,s=(1-r)*e.x+o.x*r,a=(1-r)*e.y+o.y*r;if(this.body.view.scale=t,this.body.view.translation={x:s,y:a},null!=n){var h=this.canvas.canvasToDOM(n);this.drag.pointer.x=h.x,this.drag.pointer.y=h.y}this.body.emitter.emit("_requestRedraw"),i<t?this.body.emitter.emit("zoom",{direction:"+",scale:this.body.view.scale,pointer:e}):this.body.emitter.emit("zoom",{direction:"-",scale:this.body.view.scale,pointer:e})}}},{key:"onMouseWheel",value:function(t){if(!0===this.options.zoomView){if(0!==t.deltaY){var e=this.body.view.scale;e*=1+(t.deltaY<0?1:-1)*(.1*this.options.zoomSpeed);var i=this.getPointer({x:t.clientX,y:t.clientY});this.zoom(e,i)}t.preventDefault()}}},{key:"onMouseMove",value:function(t){var e=this,i=this.getPointer({x:t.clientX,y:t.clientY}),n=!1;void 0!==this.popup&&(!1===this.popup.hidden&&this._checkHidePopup(i),!1===this.popup.hidden&&(n=!0,this.popup.setPosition(i.x+3,i.y-5),this.popup.show())),this.options.keyboard.autoFocus&&!1===this.options.keyboard.bindToWindow&&!0===this.options.keyboard.enabled&&this.canvas.frame.focus(),!1===n&&(void 0!==this.popupTimer&&(clearInterval(this.popupTimer),this.popupTimer=void 0),this.drag.dragging||(this.popupTimer=Sv((function(){return e._checkShowPopup(i)}),this.options.tooltipDelay))),!0===this.options.hover&&this.selectionHandler.hoverObject(t,i)}},{key:"_checkShowPopup",value:function(t){var e=this.canvas._XconvertDOMtoCanvas(t.x),i=this.canvas._YconvertDOMtoCanvas(t.y),n={left:e,top:i,right:e,bottom:i},o=void 0===this.popupObj?void 0:this.popupObj.id,r=!1,s="node";if(void 0===this.popupObj){for(var a,h=this.body.nodeIndices,l=this.body.nodes,d=[],c=0;c<h.length;c++)!0===(a=l[h[c]]).isOverlappingWith(n)&&(r=!0,void 0!==a.getTitle()&&d.push(h[c]));d.length>0&&(this.popupObj=l[d[d.length-1]],r=!0)}if(void 0===this.popupObj&&!1===r){for(var u,f=this.body.edgeIndices,p=this.body.edges,v=[],g=0;g<f.length;g++)!0===(u=p[f[g]]).isOverlappingWith(n)&&!0===u.connected&&void 0!==u.getTitle()&&v.push(f[g]);v.length>0&&(this.popupObj=p[v[v.length-1]],s="edge")}void 0!==this.popupObj?this.popupObj.id!==o&&(void 0===this.popup&&(this.popup=new qm(this.canvas.frame)),this.popup.popupTargetType=s,this.popup.popupTargetId=this.popupObj.id,this.popup.setPosition(t.x+3,t.y-5),this.popup.setText(this.popupObj.getTitle()),this.popup.show(),this.body.emitter.emit("showPopup",this.popupObj.id)):void 0!==this.popup&&(this.popup.hide(),this.body.emitter.emit("hidePopup"))}},{key:"_checkHidePopup",value:function(t){var e=this.selectionHandler._pointerToPositionObject(t),i=!1;if("node"===this.popup.popupTargetType){if(void 0!==this.body.nodes[this.popup.popupTargetId]&&!0===(i=this.body.nodes[this.popup.popupTargetId].isOverlappingWith(e))){var n=this.selectionHandler.getNodeAt(t);i=void 0!==n&&n.id===this.popup.popupTargetId}}else void 0===this.selectionHandler.getNodeAt(t)&&void 0!==this.body.edges[this.popup.popupTargetId]&&(i=this.body.edges[this.popup.popupTargetId].isOverlappingWith(e));!1===i&&(this.popupObj=void 0,this.popup.hide(),this.body.emitter.emit("hidePopup"))}}]),t}(),qC=g,VC=Nw,UC=Db.exports.getWeakData,YC=$e,XC=Y,GC=yw,KC=pw,$C=Wt,ZC=Vo.set,QC=Vo.getterFor,JC=Wh.find,tS=Wh.findIndex,eS=qC([].splice),iS=0,nS=function(t){return t.frozen||(t.frozen=new oS)},oS=function(){this.entries=[]},rS=function(t,e){return JC(t.entries,(function(t){return t[0]===e}))};oS.prototype={get:function(t){var e=rS(this,t);if(e)return e[1]},has:function(t){return!!rS(this,t)},set:function(t,e){var i=rS(this,t);i?i[1]=e:this.entries.push([t,e])},delete:function(t){var e=tS(this.entries,(function(e){return e[0]===t}));return~e&&eS(this.entries,e,1),!!~e}};var sS,aS={getConstructor:function(t,e,i,n){var o=t((function(t,o){GC(t,r),ZC(t,{type:e,id:iS++,frozen:void 0}),null!=o&&KC(o,t[n],{that:t,AS_ENTRIES:i})})),r=o.prototype,s=QC(e),a=function(t,e,i){var n=s(t),o=UC(YC(e),!0);return!0===o?nS(n).set(e,i):o[n.id]=i,t};return VC(r,{delete:function(t){var e=s(this);if(!XC(t))return!1;var i=UC(t);return!0===i?nS(e).delete(t):i&&$C(i,e.id)&&delete i[e.id]},has:function(t){var e=s(this);if(!XC(t))return!1;var i=UC(t);return!0===i?nS(e).has(t):i&&$C(i,e.id)}}),VC(r,i?{get:function(t){var e=s(this);if(XC(t)){var i=UC(t);return!0===i?nS(e).get(t):i?i[e.id]:void 0}},set:function(t,e){return a(this,t,e)}}:{add:function(t){return a(this,t,!0)}}),o}},hS=n,lS=g,dS=Nw,cS=Db.exports,uS=Bw,fS=aS,pS=Y,vS=jb,gS=Vo.enforce,yS=_o,mS=!hS.ActiveXObject&&"ActiveXObject"in hS,bS=function(t){return function(){return t(this,arguments.length?arguments[0]:void 0)}},wS=uS("WeakMap",bS,fS);if(yS&&mS){sS=fS.getConstructor(bS,"WeakMap",!0),cS.enable();var kS=wS.prototype,_S=lS(kS.delete),xS=lS(kS.has),ES=lS(kS.get),OS=lS(kS.set);dS(kS,{delete:function(t){if(pS(t)&&!vS(t)){var e=gS(this);return e.frozen||(e.frozen=new sS),_S(this,t)||e.frozen.delete(t)}return _S(this,t)},has:function(t){if(pS(t)&&!vS(t)){var e=gS(this);return e.frozen||(e.frozen=new sS),xS(this,t)||e.frozen.has(t)}return xS(this,t)},get:function(t){if(pS(t)&&!vS(t)){var e=gS(this);return e.frozen||(e.frozen=new sS),xS(this,t)?ES(this,t):e.frozen.get(t)}return ES(this,t)},set:function(t,e){if(pS(t)&&!vS(t)){var i=gS(this);i.frozen||(i.frozen=new sS),xS(this,t)?OS(this,t,e):i.frozen.set(t,e)}else OS(this,t,e);return this}})}var CS,SS,TS,MS,PS,DS=X.WeakMap;function IS(t,e,i,n){if("a"===i&&!n)throw new TypeError("Private accessor was defined without a getter");if("function"==typeof e?t!==e||!n:!e.has(t))throw new TypeError("Cannot read private member from an object whose class did not declare it");return"m"===i?n:"a"===i?n.call(t):n?n.value:e.get(t)}function BS(t,e,i,n,o){if("m"===n)throw new TypeError("Private method is not writable");if("a"===n&&!o)throw new TypeError("Private accessor was defined without a setter");if("function"==typeof e?t!==e||!o:!e.has(t))throw new TypeError("Cannot write private member to an object whose class did not declare it");return"a"===n?o.call(t,i):o?o.value=i:e.set(t,i),i}function zS(t,e){var i=void 0!==tu&&ih(t)||t["@@iterator"];if(!i){if(lu(t)||(i=function(t,e){var i;if(!t)return;if("string"==typeof t)return NS(t,e);var n=au(i=Object.prototype.toString.call(t)).call(i,8,-1);"Object"===n&&t.constructor&&(n=t.constructor.name);if("Map"===n||"Set"===n)return ja(t);if("Arguments"===n||/^(?:Ui|I)nt(?:8|16|32)(?:Clamped)?Array$/.test(n))return NS(t,e)}(t))||e&&t&&"number"==typeof t.length){i&&(t=i);var n=0,o=function(){};return{s:o,n:function(){return n>=t.length?{done:!0}:{done:!1,value:t[n++]}},e:function(t){throw t},f:o}}throw new TypeError("Invalid attempt to iterate non-iterable instance.\nIn order to be iterable, non-array objects must have a [Symbol.iterator]() method.")}var r,s=!0,a=!1;return{s:function(){i=i.call(t)},n:function(){var t=i.next();return s=t.done,t},e:function(t){a=!0,r=t},f:function(){try{s||null==i.return||i.return()}finally{if(a)throw r}}}}function NS(t,e){(null==e||e>t.length)&&(e=t.length);for(var i=0,n=new Array(e);i<e;i++)n[i]=t[i];return n}function FS(t,e){var i,n=new b_,o=zS(e);try{for(o.s();!(i=o.n()).done;){var r=i.value;t.has(r)||n.add(r)}}catch(t){o.e(t)}finally{o.f()}return n}var AS=function(){function t(){Yd(this,t),CS.set(this,new b_),SS.set(this,new b_)}return Kd(t,[{key:"size",get:function(){return IS(this,SS,"f").size}},{key:"add",value:function(){for(var t=arguments.length,e=new Array(t),i=0;i<t;i++)e[i]=arguments[i];for(var n=0,o=e;n<o.length;n++){var r=o[n];IS(this,SS,"f").add(r)}}},{key:"delete",value:function(){for(var t=arguments.length,e=new Array(t),i=0;i<t;i++)e[i]=arguments[i];for(var n=0,o=e;n<o.length;n++){var r=o[n];IS(this,SS,"f").delete(r)}}},{key:"clear",value:function(){IS(this,SS,"f").clear()}},{key:"getSelection",value:function(){return Jc(IS(this,SS,"f"))}},{key:"getChanges",value:function(){return{added:Jc(FS(IS(this,CS,"f"),IS(this,SS,"f"))),deleted:Jc(FS(IS(this,SS,"f"),IS(this,CS,"f"))),previous:Jc(new b_(IS(this,CS,"f"))),current:Jc(new b_(IS(this,SS,"f")))}}},{key:"commit",value:function(){var t=this.getChanges();BS(this,CS,IS(this,SS,"f"),"f"),BS(this,SS,new b_(IS(this,CS,"f")),"f");var e,i=zS(t.added);try{for(i.s();!(e=i.n()).done;){e.value.select()}}catch(t){i.e(t)}finally{i.f()}var n,o=zS(t.deleted);try{for(o.s();!(n=o.n()).done;){n.value.unselect()}}catch(t){o.e(t)}finally{o.f()}return t}}]),t}();CS=new DS,SS=new DS;var jS=function(){function t(){var e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:function(){};Yd(this,t),TS.set(this,new AS),MS.set(this,new AS),PS.set(this,void 0),BS(this,PS,e,"f")}return Kd(t,[{key:"sizeNodes",get:function(){return IS(this,TS,"f").size}},{key:"sizeEdges",get:function(){return IS(this,MS,"f").size}},{key:"getNodes",value:function(){return IS(this,TS,"f").getSelection()}},{key:"getEdges",value:function(){return IS(this,MS,"f").getSelection()}},{key:"addNodes",value:function(){var t;(t=IS(this,TS,"f")).add.apply(t,arguments)}},{key:"addEdges",value:function(){var t;(t=IS(this,MS,"f")).add.apply(t,arguments)}},{key:"deleteNodes",value:function(t){IS(this,TS,"f").delete(t)}},{key:"deleteEdges",value:function(t){IS(this,MS,"f").delete(t)}},{key:"clear",value:function(){IS(this,TS,"f").clear(),IS(this,MS,"f").clear()}},{key:"commit",value:function(){for(var t,e,i={nodes:IS(this,TS,"f").commit(),edges:IS(this,MS,"f").commit()},n=arguments.length,o=new Array(n),r=0;r<n;r++)o[r]=arguments[r];return(t=IS(this,PS,"f")).call.apply(t,su(e=[this,i]).call(e,o)),i}}]),t}();function RS(t,e){var i=void 0!==tu&&ih(t)||t["@@iterator"];if(!i){if(lu(t)||(i=function(t,e){var i;if(!t)return;if("string"==typeof t)return LS(t,e);var n=au(i=Object.prototype.toString.call(t)).call(i,8,-1);"Object"===n&&t.constructor&&(n=t.constructor.name);if("Map"===n||"Set"===n)return ja(t);if("Arguments"===n||/^(?:Ui|I)nt(?:8|16|32)(?:Clamped)?Array$/.test(n))return LS(t,e)}(t))||e&&t&&"number"==typeof t.length){i&&(t=i);var n=0,o=function(){};return{s:o,n:function(){return n>=t.length?{done:!0}:{done:!1,value:t[n++]}},e:function(t){throw t},f:o}}throw new TypeError("Invalid attempt to iterate non-iterable instance.\nIn order to be iterable, non-array objects must have a [Symbol.iterator]() method.")}var r,s=!0,a=!1;return{s:function(){i=i.call(t)},n:function(){var t=i.next();return s=t.done,t},e:function(t){a=!0,r=t},f:function(){try{s||null==i.return||i.return()}finally{if(a)throw r}}}}function LS(t,e){(null==e||e>t.length)&&(e=t.length);for(var i=0,n=new Array(e);i<e;i++)n[i]=t[i];return n}TS=new DS,MS=new DS,PS=new DS;var HS=function(){function t(e,i){var n=this;Yd(this,t),this.body=e,this.canvas=i,this._selectionAccumulator=new jS,this.hoverObj={nodes:{},edges:{}},this.options={},this.defaultOptions={multiselect:!1,selectable:!0,selectConnectedEdges:!0,hoverConnectedEdges:!0},un(this.options,this.defaultOptions),this.body.emitter.on("_dataChanged",(function(){n.updateSelection()}))}return Kd(t,[{key:"setOptions",value:function(t){if(void 0!==t){em(["multiselect","hoverConnectedEdges","selectable","selectConnectedEdges"],this.options,t)}}},{key:"selectOnPoint",value:function(t){var e=!1;if(!0===this.options.selectable){var i=this.getNodeAt(t)||this.getEdgeAt(t);this.unselectAll(),void 0!==i&&(e=this.selectObject(i)),this.body.emitter.emit("_requestRedraw")}return e}},{key:"selectAdditionalOnPoint",value:function(t){var e=!1;if(!0===this.options.selectable){var i=this.getNodeAt(t)||this.getEdgeAt(t);void 0!==i&&(e=!0,!0===i.isSelected()?this.deselectObject(i):this.selectObject(i),this.body.emitter.emit("_requestRedraw"))}return e}},{key:"_initBaseEvent",value:function(t,e){var i={};return i.pointer={DOM:{x:e.x,y:e.y},canvas:this.canvas.DOMtoCanvas(e)},i.event=t,i}},{key:"generateClickEvent",value:function(t,e,i,n){var o=arguments.length>4&&void 0!==arguments[4]&&arguments[4],r=this._initBaseEvent(e,i);if(!0===o)r.nodes=[],r.edges=[];else{var s=this.getSelection();r.nodes=s.nodes,r.edges=s.edges}void 0!==n&&(r.previousSelection=n),"click"==t&&(r.items=this.getClickedItems(i)),void 0!==e.controlEdge&&(r.controlEdge=e.controlEdge),this.body.emitter.emit(t,r)}},{key:"selectObject",value:function(t){var e=arguments.length>1&&void 0!==arguments[1]?arguments[1]:this.options.selectConnectedEdges;if(void 0!==t){if(t instanceof fO){var i;if(!0===e)(i=this._selectionAccumulator).addEdges.apply(i,Jc(t.edges));this._selectionAccumulator.addNodes(t)}else this._selectionAccumulator.addEdges(t);return!0}return!1}},{key:"deselectObject",value:function(t){!0===t.isSelected()&&(t.selected=!1,this._removeFromSelection(t))}},{key:"_getAllNodesOverlappingWith",value:function(t){for(var e=[],i=this.body.nodes,n=0;n<this.body.nodeIndices.length;n++){var o=this.body.nodeIndices[n];i[o].isOverlappingWith(t)&&e.push(o)}return e}},{key:"_pointerToPositionObject",value:function(t){var e=this.canvas.DOMtoCanvas(t);return{left:e.x-1,top:e.y+1,right:e.x+1,bottom:e.y-1}}},{key:"getNodeAt",value:function(t){var e=!(arguments.length>1&&void 0!==arguments[1])||arguments[1],i=this._pointerToPositionObject(t),n=this._getAllNodesOverlappingWith(i);return n.length>0?!0===e?this.body.nodes[n[n.length-1]]:n[n.length-1]:void 0}},{key:"_getEdgesOverlappingWith",value:function(t,e){for(var i=this.body.edges,n=0;n<this.body.edgeIndices.length;n++){var o=this.body.edgeIndices[n];i[o].isOverlappingWith(t)&&e.push(o)}}},{key:"_getAllEdgesOverlappingWith",value:function(t){var e=[];return this._getEdgesOverlappingWith(t,e),e}},{key:"getEdgeAt",value:function(t){for(var e=!(arguments.length>1&&void 0!==arguments[1])||arguments[1],i=this.canvas.DOMtoCanvas(t),n=10,o=null,r=this.body.edges,s=0;s<this.body.edgeIndices.length;s++){var a=this.body.edgeIndices[s],h=r[a];if(h.connected){var l=h.from.x,d=h.from.y,c=h.to.x,u=h.to.y,f=h.edgeType.getDistanceToEdge(l,d,c,u,i.x,i.y);f<n&&(o=a,n=f)}}return null!==o?!0===e?this.body.edges[o]:o:void 0}},{key:"_addToHover",value:function(t){t instanceof fO?this.hoverObj.nodes[t.id]=t:this.hoverObj.edges[t.id]=t}},{key:"_removeFromSelection",value:function(t){var e;t instanceof fO?(this._selectionAccumulator.deleteNodes(t),(e=this._selectionAccumulator).deleteEdges.apply(e,Jc(t.edges))):this._selectionAccumulator.deleteEdges(t)}},{key:"unselectAll",value:function(){this._selectionAccumulator.clear()}},{key:"getSelectedNodeCount",value:function(){return this._selectionAccumulator.sizeNodes}},{key:"getSelectedEdgeCount",value:function(){return this._selectionAccumulator.sizeEdges}},{key:"_hoverConnectedEdges",value:function(t){for(var e=0;e<t.edges.length;e++){var i=t.edges[e];i.hover=!0,this._addToHover(i)}}},{key:"emitBlurEvent",value:function(t,e,i){var n=this._initBaseEvent(t,e);!0===i.hover&&(i.hover=!1,i instanceof fO?(n.node=i.id,this.body.emitter.emit("blurNode",n)):(n.edge=i.id,this.body.emitter.emit("blurEdge",n)))}},{key:"emitHoverEvent",value:function(t,e,i){var n=this._initBaseEvent(t,e),o=!1;return!1===i.hover&&(i.hover=!0,this._addToHover(i),o=!0,i instanceof fO?(n.node=i.id,this.body.emitter.emit("hoverNode",n)):(n.edge=i.id,this.body.emitter.emit("hoverEdge",n))),o}},{key:"hoverObject",value:function(t,e){var i=this.getNodeAt(e);void 0===i&&(i=this.getEdgeAt(e));var n=!1;for(var o in this.hoverObj.nodes)Object.prototype.hasOwnProperty.call(this.hoverObj.nodes,o)&&(void 0===i||i instanceof fO&&i.id!=o||i instanceof cC)&&(this.emitBlurEvent(t,e,this.hoverObj.nodes[o]),delete this.hoverObj.nodes[o],n=!0);for(var r in this.hoverObj.edges)Object.prototype.hasOwnProperty.call(this.hoverObj.edges,r)&&(!0===n?(this.hoverObj.edges[r].hover=!1,delete this.hoverObj.edges[r]):(void 0===i||i instanceof cC&&i.id!=r||i instanceof fO&&!i.hover)&&(this.emitBlurEvent(t,e,this.hoverObj.edges[r]),delete this.hoverObj.edges[r],n=!0));if(void 0!==i){var s=bu(this.hoverObj.edges).length,a=bu(this.hoverObj.nodes).length;(n||i instanceof cC&&0===s&&0===a||i instanceof fO&&0===s&&0===a)&&(n=this.emitHoverEvent(t,e,i)),i instanceof fO&&!0===this.options.hoverConnectedEdges&&this._hoverConnectedEdges(i)}!0===n&&this.body.emitter.emit("_requestRedraw")}},{key:"commitWithoutEmitting",value:function(){this._selectionAccumulator.commit()}},{key:"commitAndEmit",value:function(t,e){var i=!1,n=this._selectionAccumulator.commit(),o={nodes:n.nodes.previous,edges:n.edges.previous};n.edges.deleted.length>0&&(this.generateClickEvent("deselectEdge",e,t,o),i=!0),n.nodes.deleted.length>0&&(this.generateClickEvent("deselectNode",e,t,o),i=!0),n.nodes.added.length>0&&(this.generateClickEvent("selectNode",e,t),i=!0),n.edges.added.length>0&&(this.generateClickEvent("selectEdge",e,t),i=!0),!0===i&&this.generateClickEvent("select",e,t)}},{key:"getSelection",value:function(){return{nodes:this.getSelectedNodeIds(),edges:this.getSelectedEdgeIds()}}},{key:"getSelectedNodes",value:function(){return this._selectionAccumulator.getNodes()}},{key:"getSelectedEdges",value:function(){return this._selectionAccumulator.getEdges()}},{key:"getSelectedNodeIds",value:function(){var t;return gu(t=this._selectionAccumulator.getNodes()).call(t,(function(t){return t.id}))}},{key:"getSelectedEdgeIds",value:function(){var t;return gu(t=this._selectionAccumulator.getEdges()).call(t,(function(t){return t.id}))}},{key:"setSelection",value:function(t){var e=arguments.length>1&&void 0!==arguments[1]?arguments[1]:{};if(!t||!t.nodes&&!t.edges)throw new TypeError("Selection must be an object with nodes and/or edges properties");if((e.unselectAll||void 0===e.unselectAll)&&this.unselectAll(),t.nodes){var i,n=RS(t.nodes);try{for(n.s();!(i=n.n()).done;){var o=i.value,r=this.body.nodes[o];if(!r)throw new RangeError('Node with id "'+o+'" not found');this.selectObject(r,e.highlightEdges)}}catch(t){n.e(t)}finally{n.f()}}if(t.edges){var s,a=RS(t.edges);try{for(a.s();!(s=a.n()).done;){var h=s.value,l=this.body.edges[h];if(!l)throw new RangeError('Edge with id "'+h+'" not found');this.selectObject(l)}}catch(t){a.e(t)}finally{a.f()}}this.body.emitter.emit("_requestRedraw"),this._selectionAccumulator.commit()}},{key:"selectNodes",value:function(t){var e=!(arguments.length>1&&void 0!==arguments[1])||arguments[1];if(!t||void 0===t.length)throw"Selection must be an array with ids";this.setSelection({nodes:t},{highlightEdges:e})}},{key:"selectEdges",value:function(t){if(!t||void 0===t.length)throw"Selection must be an array with ids";this.setSelection({edges:t})}},{key:"updateSelection",value:function(){for(var t in this._selectionAccumulator.getNodes())Object.prototype.hasOwnProperty.call(this.body.nodes,t.id)||this._selectionAccumulator.deleteNodes(t);for(var e in this._selectionAccumulator.getEdges())Object.prototype.hasOwnProperty.call(this.body.edges,e.id)||this._selectionAccumulator.deleteEdges(e)}},{key:"getClickedItems",value:function(t){for(var e=this.canvas.DOMtoCanvas(t),i=[],n=this.body.nodeIndices,o=this.body.nodes,r=n.length-1;r>=0;r--){var s=o[n[r]].getItemsOnPoint(e);i.push.apply(i,s)}for(var a=this.body.edgeIndices,h=this.body.edges,l=a.length-1;l>=0;l--){var d=h[a[l]].getItemsOnPoint(e);i.push.apply(i,d)}return i}}]),t}(),WS={};!function(t){!function(t){function e(t,e){if(!(t instanceof e))throw new TypeError("Cannot call a class as a function")}t.__esModule=!0,t.sort=v;var i=32,n=7,o=256,r=[1,10,100,1e3,1e4,1e5,1e6,1e7,1e8,1e9];function s(t){return t<1e5?t<100?t<10?0:1:t<1e4?t<1e3?2:3:4:t<1e7?t<1e6?5:6:t<1e9?t<1e8?7:8:9}function a(t,e){if(t===e)return 0;if(~~t===t&&~~e===e){if(0===t||0===e)return t<e?-1:1;if(t<0||e<0){if(e>=0)return-1;if(t>=0)return 1;t=-t,e=-e}var i=s(t),n=s(e),o=0;return i<n?(t*=r[n-i-1],e/=10,o=-1):i>n&&(e*=r[i-n-1],t/=10,o=1),t===e?o:t<e?-1:1}var a=String(t),h=String(e);return a===h?0:a<h?-1:1}function h(t){for(var e=0;t>=i;)e|=1&t,t>>=1;return t+e}function l(t,e,i,n){var o=e+1;if(o===i)return 1;if(n(t[o++],t[e])<0){for(;o<i&&n(t[o],t[o-1])<0;)o++;d(t,e,o)}else for(;o<i&&n(t[o],t[o-1])>=0;)o++;return o-e}function d(t,e,i){for(i--;e<i;){var n=t[e];t[e++]=t[i],t[i--]=n}}function c(t,e,i,n,o){for(n===e&&n++;n<i;n++){for(var r=t[n],s=e,a=n;s<a;){var h=s+a>>>1;o(r,t[h])<0?a=h:s=h+1}var l=n-s;switch(l){case 3:t[s+3]=t[s+2];case 2:t[s+2]=t[s+1];case 1:t[s+1]=t[s];break;default:for(;l>0;)t[s+l]=t[s+l-1],l--}t[s]=r}}function u(t,e,i,n,o,r){var s=0,a=0,h=1;if(r(t,e[i+o])>0){for(a=n-o;h<a&&r(t,e[i+o+h])>0;)s=h,(h=1+(h<<1))<=0&&(h=a);h>a&&(h=a),s+=o,h+=o}else{for(a=o+1;h<a&&r(t,e[i+o-h])<=0;)s=h,(h=1+(h<<1))<=0&&(h=a);h>a&&(h=a);var l=s;s=o-h,h=o-l}for(s++;s<h;){var d=s+(h-s>>>1);r(t,e[i+d])>0?s=d+1:h=d}return h}function f(t,e,i,n,o,r){var s=0,a=0,h=1;if(r(t,e[i+o])<0){for(a=o+1;h<a&&r(t,e[i+o-h])<0;)s=h,(h=1+(h<<1))<=0&&(h=a);h>a&&(h=a);var l=s;s=o-h,h=o-l}else{for(a=n-o;h<a&&r(t,e[i+o+h])>=0;)s=h,(h=1+(h<<1))<=0&&(h=a);h>a&&(h=a),s+=o,h+=o}for(s++;s<h;){var d=s+(h-s>>>1);r(t,e[i+d])<0?h=d:s=d+1}return h}var p=function(){function t(i,r){e(this,t),this.array=null,this.compare=null,this.minGallop=n,this.length=0,this.tmpStorageLength=o,this.stackLength=0,this.runStart=null,this.runLength=null,this.stackSize=0,this.array=i,this.compare=r,this.length=i.length,this.length<2*o&&(this.tmpStorageLength=this.length>>>1),this.tmp=new Array(this.tmpStorageLength),this.stackLength=this.length<120?5:this.length<1542?10:this.length<119151?19:40,this.runStart=new Array(this.stackLength),this.runLength=new Array(this.stackLength)}return t.prototype.pushRun=function(t,e){this.runStart[this.stackSize]=t,this.runLength[this.stackSize]=e,this.stackSize+=1},t.prototype.mergeRuns=function(){for(;this.stackSize>1;){var t=this.stackSize-2;if(t>=1&&this.runLength[t-1]<=this.runLength[t]+this.runLength[t+1]||t>=2&&this.runLength[t-2]<=this.runLength[t]+this.runLength[t-1])this.runLength[t-1]<this.runLength[t+1]&&t--;else if(this.runLength[t]>this.runLength[t+1])break;this.mergeAt(t)}},t.prototype.forceMergeRuns=function(){for(;this.stackSize>1;){var t=this.stackSize-2;t>0&&this.runLength[t-1]<this.runLength[t+1]&&t--,this.mergeAt(t)}},t.prototype.mergeAt=function(t){var e=this.compare,i=this.array,n=this.runStart[t],o=this.runLength[t],r=this.runStart[t+1],s=this.runLength[t+1];this.runLength[t]=o+s,t===this.stackSize-3&&(this.runStart[t+1]=this.runStart[t+2],this.runLength[t+1]=this.runLength[t+2]),this.stackSize--;var a=f(i[r],i,n,o,0,e);n+=a,0!=(o-=a)&&0!==(s=u(i[n+o-1],i,r,s,s-1,e))&&(o<=s?this.mergeLow(n,o,r,s):this.mergeHigh(n,o,r,s))},t.prototype.mergeLow=function(t,e,i,o){var r=this.compare,s=this.array,a=this.tmp,h=0;for(h=0;h<e;h++)a[h]=s[t+h];var l=0,d=i,c=t;if(s[c++]=s[d++],0!=--o)if(1!==e){for(var p=this.minGallop;;){var v=0,g=0,y=!1;do{if(r(s[d],a[l])<0){if(s[c++]=s[d++],g++,v=0,0==--o){y=!0;break}}else if(s[c++]=a[l++],v++,g=0,1==--e){y=!0;break}}while((v|g)<p);if(y)break;do{if(0!==(v=f(s[d],a,l,e,0,r))){for(h=0;h<v;h++)s[c+h]=a[l+h];if(c+=v,l+=v,(e-=v)<=1){y=!0;break}}if(s[c++]=s[d++],0==--o){y=!0;break}if(0!==(g=u(a[l],s,d,o,0,r))){for(h=0;h<g;h++)s[c+h]=s[d+h];if(c+=g,d+=g,0==(o-=g)){y=!0;break}}if(s[c++]=a[l++],1==--e){y=!0;break}p--}while(v>=n||g>=n);if(y)break;p<0&&(p=0),p+=2}if(this.minGallop=p,p<1&&(this.minGallop=1),1===e){for(h=0;h<o;h++)s[c+h]=s[d+h];s[c+o]=a[l]}else{if(0===e)throw new Error("mergeLow preconditions were not respected");for(h=0;h<e;h++)s[c+h]=a[l+h]}}else{for(h=0;h<o;h++)s[c+h]=s[d+h];s[c+o]=a[l]}else for(h=0;h<e;h++)s[c+h]=a[l+h]},t.prototype.mergeHigh=function(t,e,i,o){var r=this.compare,s=this.array,a=this.tmp,h=0;for(h=0;h<o;h++)a[h]=s[i+h];var l=t+e-1,d=o-1,c=i+o-1,p=0,v=0;if(s[c--]=s[l--],0!=--e)if(1!==o){for(var g=this.minGallop;;){var y=0,m=0,b=!1;do{if(r(a[d],s[l])<0){if(s[c--]=s[l--],y++,m=0,0==--e){b=!0;break}}else if(s[c--]=a[d--],m++,y=0,1==--o){b=!0;break}}while((y|m)<g);if(b)break;do{if(0!=(y=e-f(a[d],s,t,e,e-1,r))){for(e-=y,v=1+(c-=y),p=1+(l-=y),h=y-1;h>=0;h--)s[v+h]=s[p+h];if(0===e){b=!0;break}}if(s[c--]=a[d--],1==--o){b=!0;break}if(0!=(m=o-u(s[l],a,0,o,o-1,r))){for(o-=m,v=1+(c-=m),p=1+(d-=m),h=0;h<m;h++)s[v+h]=a[p+h];if(o<=1){b=!0;break}}if(s[c--]=s[l--],0==--e){b=!0;break}g--}while(y>=n||m>=n);if(b)break;g<0&&(g=0),g+=2}if(this.minGallop=g,g<1&&(this.minGallop=1),1===o){for(v=1+(c-=e),p=1+(l-=e),h=e-1;h>=0;h--)s[v+h]=s[p+h];s[c]=a[d]}else{if(0===o)throw new Error("mergeHigh preconditions were not respected");for(p=c-(o-1),h=0;h<o;h++)s[p+h]=a[h]}}else{for(v=1+(c-=e),p=1+(l-=e),h=e-1;h>=0;h--)s[v+h]=s[p+h];s[c]=a[d]}else for(p=c-(o-1),h=0;h<o;h++)s[p+h]=a[h]},t}();function v(t,e,n,o){if(!Array.isArray(t))throw new TypeError("Can only sort arrays");e?"function"!=typeof e&&(o=n,n=e,e=a):e=a,n||(n=0),o||(o=t.length);var r=o-n;if(!(r<2)){var s=0;if(r<i)c(t,n,o,n+(s=l(t,n,o,e)),e);else{var d=new p(t,e),u=h(r);do{if((s=l(t,n,o,e))<u){var f=r;f>u&&(f=u),c(t,n,n+f,n+s,e),s=f}d.pushRun(n,s),d.mergeRuns(),r-=s,n+=s}while(0!==r);d.forceMergeRuns()}}}}(t)}(WS);var qS=WS;function VS(t){var e=function(){if("undefined"==typeof Reflect||!Mk)return!1;if(Mk.sham)return!1;if("function"==typeof Proxy)return!0;try{return Boolean.prototype.valueOf.call(Mk(Boolean,[],(function(){}))),!0}catch(t){return!1}}();return function(){var i,n=Ak(t);if(e){var o=Ak(this).constructor;i=Mk(n,arguments,o)}else i=n.apply(this,arguments);return Nk(this,i)}}var US=function(){function t(){Yd(this,t)}return Kd(t,[{key:"abstract",value:function(){throw new Error("Can't instantiate abstract class!")}},{key:"fake_use",value:function(){}},{key:"curveType",value:function(){return this.abstract()}},{key:"getPosition",value:function(t){return this.fake_use(t),this.abstract()}},{key:"setPosition",value:function(t,e){var i=arguments.length>2&&void 0!==arguments[2]?arguments[2]:void 0;this.fake_use(t,e,i),this.abstract()}},{key:"getTreeSize",value:function(t){return this.fake_use(t),this.abstract()}},{key:"sort",value:function(t){this.fake_use(t),this.abstract()}},{key:"fix",value:function(t,e){this.fake_use(t,e),this.abstract()}},{key:"shift",value:function(t,e){this.fake_use(t,e),this.abstract()}}]),t}(),YS=function(t){zk(i,t);var e=VS(i);function i(t){var n;return Yd(this,i),(n=e.call(this)).layout=t,n}return Kd(i,[{key:"curveType",value:function(){return"horizontal"}},{key:"getPosition",value:function(t){return t.x}},{key:"setPosition",value:function(t,e){var i=arguments.length>2&&void 0!==arguments[2]?arguments[2]:void 0;void 0!==i&&this.layout.hierarchical.addToOrdering(t,i),t.x=e}},{key:"getTreeSize",value:function(t){var e=this.layout.hierarchical.getTreeSize(this.layout.body.nodes,t);return{min:e.min_x,max:e.max_x}}},{key:"sort",value:function(t){qS.sort(t,(function(t,e){return t.x-e.x}))}},{key:"fix",value:function(t,e){t.y=this.layout.options.hierarchical.levelSeparation*e,t.options.fixed.y=!0}},{key:"shift",value:function(t,e){this.layout.body.nodes[t].x+=e}}]),i}(US),XS=function(t){zk(i,t);var e=VS(i);function i(t){var n;return Yd(this,i),(n=e.call(this)).layout=t,n}return Kd(i,[{key:"curveType",value:function(){return"vertical"}},{key:"getPosition",value:function(t){return t.y}},{key:"setPosition",value:function(t,e){var i=arguments.length>2&&void 0!==arguments[2]?arguments[2]:void 0;void 0!==i&&this.layout.hierarchical.addToOrdering(t,i),t.y=e}},{key:"getTreeSize",value:function(t){var e=this.layout.hierarchical.getTreeSize(this.layout.body.nodes,t);return{min:e.min_y,max:e.max_y}}},{key:"sort",value:function(t){qS.sort(t,(function(t,e){return t.y-e.y}))}},{key:"fix",value:function(t,e){t.x=this.layout.options.hierarchical.levelSeparation*e,t.options.fixed.x=!0}},{key:"shift",value:function(t,e){this.layout.body.nodes[t].y+=e}}]),i}(US),GS=Wh.every;_i({target:"Array",proto:!0,forced:!Cu("every")},{every:function(t){return GS(this,t,arguments.length>1?arguments[1]:void 0)}});var KS=Tn("Array").every,$S=J,ZS=KS,QS=Array.prototype,JS=function(t){var e=t.every;return t===QS||$S(QS,t)&&e===QS.every?ZS:e},tT=JS;function eT(t,e){var i=void 0!==tu&&ih(t)||t["@@iterator"];if(!i){if(lu(t)||(i=function(t,e){var i;if(!t)return;if("string"==typeof t)return iT(t,e);var n=au(i=Object.prototype.toString.call(t)).call(i,8,-1);"Object"===n&&t.constructor&&(n=t.constructor.name);if("Map"===n||"Set"===n)return ja(t);if("Arguments"===n||/^(?:Ui|I)nt(?:8|16|32)(?:Clamped)?Array$/.test(n))return iT(t,e)}(t))||e&&t&&"number"==typeof t.length){i&&(t=i);var n=0,o=function(){};return{s:o,n:function(){return n>=t.length?{done:!0}:{done:!1,value:t[n++]}},e:function(t){throw t},f:o}}throw new TypeError("Invalid attempt to iterate non-iterable instance.\nIn order to be iterable, non-array objects must have a [Symbol.iterator]() method.")}var r,s=!0,a=!1;return{s:function(){i=i.call(t)},n:function(){var t=i.next();return s=t.done,t},e:function(t){a=!0,r=t},f:function(){try{s||null==i.return||i.return()}finally{if(a)throw r}}}}function iT(t,e){(null==e||e>t.length)&&(e=t.length);for(var i=0,n=new Array(e);i<e;i++)n[i]=t[i];return n}function nT(t,e){var i=new b_;return Fu(t).call(t,(function(t){var e;Fu(e=t.edges).call(e,(function(t){t.connected&&i.add(t)}))})),Fu(i).call(i,(function(t){var i=t.from.id,n=t.to.id;null==e[i]&&(e[i]=0),(null==e[n]||e[i]>=e[n])&&(e[n]=e[i]+1)})),e}function oT(t,e,i,n){var o,r,s=Kp(null),a=i_(o=Jc(kx(n).call(n))).call(o,(function(t,e){return t+1+e.edges.length}),0),h=i+"Id",l="to"===i?1:-1,d=eT(n);try{var c=function(){var o=Kc(r.value,2),d=o[0],c=o[1];if(!n.has(d)||!t(c))return"continue";s[d]=0;for(var u=[c],f=0,p=void 0,v=function(){var t,o;if(!n.has(d))return"continue";var r=s[p.id]+l;if(Fu(t=Xf(o=p.edges).call(o,(function(t){return t.connected&&t.to!==t.from&&t[i]!==p&&n.has(t.toId)&&n.has(t.fromId)}))).call(t,(function(t){var n=t[h],o=s[n];(null==o||e(r,o))&&(s[n]=r,u.push(t[i]))})),f>a)return{v:{v:nT(n,s)}};++f};p=u.pop();){var g=v();if("continue"!==g&&"object"===Qc(g))return g.v}};for(d.s();!(r=d.n()).done;){var u=c();if("continue"!==u&&"object"===Qc(u))return u.v}}catch(t){d.e(t)}finally{d.f()}return s}var rT=function(){function t(){Yd(this,t),this.childrenReference={},this.parentReference={},this.trees={},this.distributionOrdering={},this.levels={},this.distributionIndex={},this.isTree=!1,this.treeIndex=-1}return Kd(t,[{key:"addRelation",value:function(t,e){void 0===this.childrenReference[t]&&(this.childrenReference[t]=[]),this.childrenReference[t].push(e),void 0===this.parentReference[e]&&(this.parentReference[e]=[]),this.parentReference[e].push(t)}},{key:"checkIfTree",value:function(){for(var t in this.parentReference)if(this.parentReference[t].length>1)return void(this.isTree=!1);this.isTree=!0}},{key:"numTrees",value:function(){return this.treeIndex+1}},{key:"setTreeIndex",value:function(t,e){void 0!==e&&void 0===this.trees[t.id]&&(this.trees[t.id]=e,this.treeIndex=Math.max(e,this.treeIndex))}},{key:"ensureLevel",value:function(t){void 0===this.levels[t]&&(this.levels[t]=0)}},{key:"getMaxLevel",value:function(t){var e=this,i={};return function t(n){if(void 0!==i[n])return i[n];var o=e.levels[n];if(e.childrenReference[n]){var r=e.childrenReference[n];if(r.length>0)for(var s=0;s<r.length;s++)o=Math.max(o,t(r[s]))}return i[n]=o,o}(t)}},{key:"levelDownstream",value:function(t,e){void 0===this.levels[e.id]&&(void 0===this.levels[t.id]&&(this.levels[t.id]=0),this.levels[e.id]=this.levels[t.id]+1)}},{key:"setMinLevelToZero",value:function(t){var e=1e9;for(var i in t)Object.prototype.hasOwnProperty.call(t,i)&&void 0!==this.levels[i]&&(e=Math.min(this.levels[i],e));for(var n in t)Object.prototype.hasOwnProperty.call(t,n)&&void 0!==this.levels[n]&&(this.levels[n]-=e)}},{key:"getTreeSize",value:function(t,e){var i=1e9,n=-1e9,o=1e9,r=-1e9;for(var s in this.trees)if(Object.prototype.hasOwnProperty.call(this.trees,s)&&this.trees[s]===e){var a=t[s];i=Math.min(a.x,i),n=Math.max(a.x,n),o=Math.min(a.y,o),r=Math.max(a.y,r)}return{min_x:i,max_x:n,min_y:o,max_y:r}}},{key:"hasSameParent",value:function(t,e){var i=this.parentReference[t.id],n=this.parentReference[e.id];if(void 0===i||void 0===n)return!1;for(var o=0;o<i.length;o++)for(var r=0;r<n.length;r++)if(i[o]==n[r])return!0;return!1}},{key:"inSameSubNetwork",value:function(t,e){return this.trees[t.id]===this.trees[e.id]}},{key:"getLevels",value:function(){return bu(this.distributionOrdering)}},{key:"addToOrdering",value:function(t,e){void 0===this.distributionOrdering[e]&&(this.distributionOrdering[e]=[]);var i=!1,n=this.distributionOrdering[e];for(var o in n)if(n[o]===t){i=!0;break}i||(this.distributionOrdering[e].push(t),this.distributionIndex[t.id]=this.distributionOrdering[e].length-1)}}]),t}(),sT=function(){function t(e){Yd(this,t),this.body=e,this._resetRNG(Math.random()+":"+Eu()),this.setPhysics=!1,this.options={},this.optionsBackup={physics:{}},this.defaultOptions={randomSeed:void 0,improvedLayout:!0,clusterThreshold:150,hierarchical:{enabled:!1,levelSeparation:150,nodeSpacing:100,treeSpacing:200,blockShifting:!0,edgeMinimization:!0,parentCentralization:!0,direction:"UD",sortMethod:"hubsize"}},un(this.options,this.defaultOptions),this.bindEventListeners()}return Kd(t,[{key:"bindEventListeners",value:function(){var t=this;this.body.emitter.on("_dataChanged",(function(){t.setupHierarchicalLayout()})),this.body.emitter.on("_dataLoaded",(function(){t.layoutNetwork()})),this.body.emitter.on("_resetHierarchicalLayout",(function(){t.setupHierarchicalLayout()})),this.body.emitter.on("_adjustEdgesForHierarchicalLayout",(function(){if(!0===t.options.hierarchical.enabled){var e=t.direction.curveType();t.body.emitter.emit("_forceDisableDynamicCurves",e,!1)}}))}},{key:"setOptions",value:function(t,e){if(void 0!==t){var i=this.options.hierarchical,n=i.enabled;if(em(["randomSeed","improvedLayout","clusterThreshold"],this.options,t),Sm(this.options,t,"hierarchical"),void 0!==t.randomSeed&&this._resetRNG(t.randomSeed),!0===i.enabled)return!0===n&&this.body.emitter.emit("refresh",!0),"RL"===i.direction||"DU"===i.direction?i.levelSeparation>0&&(i.levelSeparation*=-1):i.levelSeparation<0&&(i.levelSeparation*=-1),this.setDirectionStrategy(),this.body.emitter.emit("_resetHierarchicalLayout"),this.adaptAllOptionsForHierarchicalLayout(e);if(!0===n)return this.body.emitter.emit("refresh"),nm(e,this.optionsBackup)}return e}},{key:"_resetRNG",value:function(t){this.initialRandomSeed=t,this._rng=jy(this.initialRandomSeed)}},{key:"adaptAllOptionsForHierarchicalLayout",value:function(t){if(!0===this.options.hierarchical.enabled){var e=this.optionsBackup.physics;void 0===t.physics||!0===t.physics?(t.physics={enabled:void 0===e.enabled||e.enabled,solver:"hierarchicalRepulsion"},e.enabled=void 0===e.enabled||e.enabled,e.solver=e.solver||"barnesHut"):"object"===Qc(t.physics)?(e.enabled=void 0===t.physics.enabled||t.physics.enabled,e.solver=t.physics.solver||"barnesHut",t.physics.solver="hierarchicalRepulsion"):!1!==t.physics&&(e.solver="barnesHut",t.physics={solver:"hierarchicalRepulsion"});var i=this.direction.curveType();if(void 0===t.edges)this.optionsBackup.edges={smooth:{enabled:!0,type:"dynamic"}},t.edges={smooth:!1};else if(void 0===t.edges.smooth)this.optionsBackup.edges={smooth:{enabled:!0,type:"dynamic"}},t.edges.smooth=!1;else if("boolean"==typeof t.edges.smooth)this.optionsBackup.edges={smooth:t.edges.smooth},t.edges.smooth={enabled:t.edges.smooth,type:i};else{var n=t.edges.smooth;void 0!==n.type&&"dynamic"!==n.type&&(i=n.type),this.optionsBackup.edges={smooth:{enabled:void 0===n.enabled||n.enabled,type:void 0===n.type?"dynamic":n.type,roundness:void 0===n.roundness?.5:n.roundness,forceDirection:void 0!==n.forceDirection&&n.forceDirection}},t.edges.smooth={enabled:void 0===n.enabled||n.enabled,type:i,roundness:void 0===n.roundness?.5:n.roundness,forceDirection:void 0!==n.forceDirection&&n.forceDirection}}this.body.emitter.emit("_forceDisableDynamicCurves",i)}return t}},{key:"positionInitially",value:function(t){if(!0!==this.options.hierarchical.enabled){this._resetRNG(this.initialRandomSeed);for(var e=t.length+50,i=0;i<t.length;i++){var n=t[i],o=2*Math.PI*this._rng();void 0===n.x&&(n.x=e*Math.cos(o)),void 0===n.y&&(n.y=e*Math.sin(o))}}}},{key:"layoutNetwork",value:function(){if(!0!==this.options.hierarchical.enabled&&!0===this.options.improvedLayout){for(var t=this.body.nodeIndices,e=0,i=0;i<t.length;i++){!0===this.body.nodes[t[i]].predefinedPosition&&(e+=1)}if(e<.5*t.length){var n=0,o=this.options.clusterThreshold,r={clusterNodeProperties:{shape:"ellipse",label:"",group:"",font:{multi:!1}},clusterEdgeProperties:{label:"",font:{multi:!1},smooth:{enabled:!1}}};if(t.length>o){for(var s=t.length;t.length>o&&n<=10;){n+=1;var a=t.length;if(n%3==0?this.body.modules.clustering.clusterBridges(r):this.body.modules.clustering.clusterOutliers(r),a==t.length&&n%3!=0)return this._declusterAll(),this.body.emitter.emit("_layoutFailed"),void console.info("This network could not be positioned by this version of the improved layout algorithm. Please disable improvedLayout for better performance.")}this.body.modules.kamadaKawai.setOptions({springLength:Math.max(150,2*s)})}n>10&&console.info("The clustering didn't succeed within the amount of interations allowed, progressing with partial result."),this.body.modules.kamadaKawai.solve(t,this.body.edgeIndices,!0),this._shiftToCenter();for(var h=0;h<t.length;h++){var l=this.body.nodes[t[h]];!1===l.predefinedPosition&&(l.x+=70*(.5-this._rng()),l.y+=70*(.5-this._rng()))}this._declusterAll(),this.body.emitter.emit("_repositionBezierNodes")}}}},{key:"_shiftToCenter",value:function(){for(var t=EC.getRangeCore(this.body.nodes,this.body.nodeIndices),e=EC.findCenter(t),i=0;i<this.body.nodeIndices.length;i++){var n=this.body.nodes[this.body.nodeIndices[i]];n.x-=e.x,n.y-=e.y}}},{key:"_declusterAll",value:function(){for(var t=!0;!0===t;){t=!1;for(var e=0;e<this.body.nodeIndices.length;e++)!0===this.body.nodes[this.body.nodeIndices[e]].isCluster&&(t=!0,this.body.modules.clustering.openCluster(this.body.nodeIndices[e],{},!1));!0===t&&this.body.emitter.emit("_dataChanged")}}},{key:"getSeed",value:function(){return this.initialRandomSeed}},{key:"setupHierarchicalLayout",value:function(){if(!0===this.options.hierarchical.enabled&&this.body.nodeIndices.length>0){var t,e,i=!1,n=!1;for(e in this.lastNodeOnLevel={},this.hierarchical=new rT,this.body.nodes)Object.prototype.hasOwnProperty.call(this.body.nodes,e)&&(void 0!==(t=this.body.nodes[e]).options.level?(i=!0,this.hierarchical.levels[e]=t.options.level):n=!0);if(!0===n&&!0===i)throw new Error("To use the hierarchical layout, nodes require either no predefined levels or levels have to be defined for all nodes.");if(!0===n){var o=this.options.hierarchical.sortMethod;"hubsize"===o?this._determineLevelsByHubsize():"directed"===o?this._determineLevelsDirected():"custom"===o&&this._determineLevelsCustomCallback()}for(var r in this.body.nodes)Object.prototype.hasOwnProperty.call(this.body.nodes,r)&&this.hierarchical.ensureLevel(r);var s=this._getDistribution();this._generateMap(),this._placeNodesByHierarchy(s),this._condenseHierarchy(),this._shiftToCenter()}}},{key:"_condenseHierarchy",value:function(){var t=this,e=!1,i={},n=function(e,i){var n=t.hierarchical.trees;for(var o in n)Object.prototype.hasOwnProperty.call(n,o)&&n[o]===e&&t.direction.shift(o,i)},o=function(){for(var e=[],i=0;i<t.hierarchical.numTrees();i++)e.push(t.direction.getTreeSize(i));return e},r=function e(i,n){if(!n[i.id]&&(n[i.id]=!0,t.hierarchical.childrenReference[i.id])){var o=t.hierarchical.childrenReference[i.id];if(o.length>0)for(var r=0;r<o.length;r++)e(t.body.nodes[o[r]],n)}},s=function(e){var i=arguments.length>1&&void 0!==arguments[1]?arguments[1]:1e9,n=1e9,o=1e9,r=1e9,s=-1e9;for(var a in e)if(Object.prototype.hasOwnProperty.call(e,a)){var h=t.body.nodes[a],l=t.hierarchical.levels[h.id],d=t.direction.getPosition(h),c=t._getSpaceAroundNode(h,e),u=Kc(c,2),f=u[0],p=u[1];n=Math.min(f,n),o=Math.min(p,o),l<=i&&(r=Math.min(d,r),s=Math.max(d,s))}return[r,s,n,o]},a=function(e,i){var n=t.hierarchical.getMaxLevel(e.id),o=t.hierarchical.getMaxLevel(i.id);return Math.min(n,o)},h=function(e,i,n){for(var o=t.hierarchical,r=0;r<i.length;r++){var s=i[r],a=o.distributionOrdering[s];if(a.length>1)for(var h=0;h<a.length-1;h++){var l=a[h],d=a[h+1];o.hasSameParent(l,d)&&o.inSameSubNetwork(l,d)&&e(l,d,n)}}},l=function(i,n){var o=arguments.length>2&&void 0!==arguments[2]&&arguments[2],h=t.direction.getPosition(i),l=t.direction.getPosition(n),d=Math.abs(l-h),c=t.options.hierarchical.nodeSpacing;if(d>c){var u={},f={};r(i,u),r(n,f);var p=a(i,n),v=s(u,p),g=s(f,p),y=v[1],m=g[0],b=g[2],w=Math.abs(y-m);if(w>c){var k=y-m+c;k<-b+c&&(k=-b+c),k<0&&(t._shiftBlock(n.id,k),e=!0,!0===o&&t._centerParent(n))}}},d=function(n,o){for(var a=o.id,h=o.edges,l=t.hierarchical.levels[o.id],d=t.options.hierarchical.levelSeparation*t.options.hierarchical.levelSeparation,c={},u=[],f=0;f<h.length;f++){var p=h[f];if(p.toId!=p.fromId){var v=p.toId==a?p.from:p.to;c[h[f].id]=v,t.hierarchical.levels[v.id]<l&&u.push(p)}}var g=function(e,i){for(var n=0,o=0;o<i.length;o++)if(void 0!==c[i[o].id]){var r=t.direction.getPosition(c[i[o].id])-e;n+=r/Math.sqrt(r*r+d)}return n},y=function(e,i){for(var n=0,o=0;o<i.length;o++)if(void 0!==c[i[o].id]){var r=t.direction.getPosition(c[i[o].id])-e;n-=d*Math.pow(r*r+d,-1.5)}return n},m=function(e,i){for(var n=t.direction.getPosition(o),r={},s=0;s<e;s++){var a=g(n,i),h=y(n,i);if(void 0!==r[n-=Math.max(-40,Math.min(40,Math.round(a/h)))])break;r[n]=s}return n},b=m(n,u);!function(n){var a=t.direction.getPosition(o);if(void 0===i[o.id]){var h={};r(o,h),i[o.id]=h}var l=s(i[o.id]),d=l[2],c=l[3],u=n-a,f=0;u>0?f=Math.min(u,c-t.options.hierarchical.nodeSpacing):u<0&&(f=-Math.min(-u,d-t.options.hierarchical.nodeSpacing)),0!=f&&(t._shiftBlock(o.id,f),e=!0)}(b),function(i){var n=t.direction.getPosition(o),r=Kc(t._getSpaceAroundNode(o),2),s=r[0],a=r[1],h=i-n,l=n;h>0?l=Math.min(n+(a-t.options.hierarchical.nodeSpacing),i):h<0&&(l=Math.max(n-(s-t.options.hierarchical.nodeSpacing),i)),l!==n&&(t.direction.setPosition(o,l),e=!0)}(b=m(n,h))};!0===this.options.hierarchical.blockShifting&&(function(i){var n=t.hierarchical.getLevels();n=Yu(n).call(n);for(var o=0;o<i&&(e=!1,h(l,n,!0),!0===e);o++);}(5),function(){for(var e in t.body.nodes)Object.prototype.hasOwnProperty.call(t.body.nodes,e)&&t._centerParent(t.body.nodes[e])}()),!0===this.options.hierarchical.edgeMinimization&&function(i){var n=t.hierarchical.getLevels();n=Yu(n).call(n);for(var o=0;o<i;o++){e=!1;for(var r=0;r<n.length;r++)for(var s=n[r],a=t.hierarchical.distributionOrdering[s],h=0;h<a.length;h++)d(1e3,a[h]);if(!0!==e)break}}(20),!0===this.options.hierarchical.parentCentralization&&function(){var e=t.hierarchical.getLevels();e=Yu(e).call(e);for(var i=0;i<e.length;i++)for(var n=e[i],o=t.hierarchical.distributionOrdering[n],r=0;r<o.length;r++)t._centerParent(o[r])}(),function(){for(var e=o(),i=0,r=0;r<e.length-1;r++){i+=e[r].max-e[r+1].min+t.options.hierarchical.treeSpacing,n(r+1,i)}}()}},{key:"_getSpaceAroundNode",value:function(t,e){var i=!0;void 0===e&&(i=!1);var n=this.hierarchical.levels[t.id];if(void 0!==n){var o=this.hierarchical.distributionIndex[t.id],r=this.direction.getPosition(t),s=this.hierarchical.distributionOrdering[n],a=1e9,h=1e9;if(0!==o){var l=s[o-1];if(!0===i&&void 0===e[l.id]||!1===i)a=r-this.direction.getPosition(l)}if(o!=s.length-1){var d=s[o+1];if(!0===i&&void 0===e[d.id]||!1===i){var c=this.direction.getPosition(d);h=Math.min(h,c-r)}}return[a,h]}return[0,0]}},{key:"_centerParent",value:function(t){if(this.hierarchical.parentReference[t.id])for(var e=this.hierarchical.parentReference[t.id],i=0;i<e.length;i++){var n=e[i],o=this.body.nodes[n],r=this.hierarchical.childrenReference[n];if(void 0!==r){var s=this._getCenterPosition(r),a=this.direction.getPosition(o),h=Kc(this._getSpaceAroundNode(o),2),l=h[0],d=h[1],c=a-s;(c<0&&Math.abs(c)<d-this.options.hierarchical.nodeSpacing||c>0&&Math.abs(c)<l-this.options.hierarchical.nodeSpacing)&&this.direction.setPosition(o,s)}}}},{key:"_placeNodesByHierarchy",value:function(t){for(var e in this.positionedNodes={},t)if(Object.prototype.hasOwnProperty.call(t,e)){var i,n=bu(t[e]);n=this._indexArrayToNodes(n),rx(i=this.direction).call(i,n);for(var o=0,r=0;r<n.length;r++){var s=n[r];if(void 0===this.positionedNodes[s.id]){var a=this.options.hierarchical.nodeSpacing,h=a*o;o>0&&(h=this.direction.getPosition(n[r-1])+a),this.direction.setPosition(s,h,e),this._validatePositionAndContinue(s,e,h),o++}}}}},{key:"_placeBranchNodes",value:function(t,e){var i,n=this.hierarchical.childrenReference[t];if(void 0!==n){for(var o=[],r=0;r<n.length;r++)o.push(this.body.nodes[n[r]]);rx(i=this.direction).call(i,o);for(var s=0;s<o.length;s++){var a=o[s],h=this.hierarchical.levels[a.id];if(!(h>e&&void 0===this.positionedNodes[a.id]))return;var l=this.options.hierarchical.nodeSpacing,d=void 0;d=0===s?this.direction.getPosition(this.body.nodes[t]):this.direction.getPosition(o[s-1])+l,this.direction.setPosition(a,d,h),this._validatePositionAndContinue(a,h,d)}var c=this._getCenterPosition(o);this.direction.setPosition(this.body.nodes[t],c,e)}}},{key:"_validatePositionAndContinue",value:function(t,e,i){if(this.hierarchical.isTree){if(void 0!==this.lastNodeOnLevel[e]){var n=this.direction.getPosition(this.body.nodes[this.lastNodeOnLevel[e]]);if(i-n<this.options.hierarchical.nodeSpacing){var o=n+this.options.hierarchical.nodeSpacing-i,r=this._findCommonParent(this.lastNodeOnLevel[e],t.id);this._shiftBlock(r.withChild,o)}}this.lastNodeOnLevel[e]=t.id,this.positionedNodes[t.id]=!0,this._placeBranchNodes(t.id,e)}}},{key:"_indexArrayToNodes",value:function(t){for(var e=[],i=0;i<t.length;i++)e.push(this.body.nodes[t[i]]);return e}},{key:"_getDistribution",value:function(){var t,e,i={};for(t in this.body.nodes)if(Object.prototype.hasOwnProperty.call(this.body.nodes,t)){e=this.body.nodes[t];var n=void 0===this.hierarchical.levels[t]?0:this.hierarchical.levels[t];this.direction.fix(e,n),void 0===i[n]&&(i[n]={}),i[n][t]=e}return i}},{key:"_getActiveEdges",value:function(t){var e=this,i=[];return hm(t.edges,(function(t){var n;-1!==Fp(n=e.body.edgeIndices).call(n,t.id)&&i.push(t)})),i}},{key:"_getHubSizes",value:function(){var t=this,e={};hm(this.body.nodeIndices,(function(i){var n=t.body.nodes[i],o=t._getActiveEdges(n).length;e[o]=!0}));var i=[];return hm(e,(function(t){i.push(Number(t))})),rx(qS).call(qS,i,(function(t,e){return e-t})),i}},{key:"_determineLevelsByHubsize",value:function(){for(var t=this,e=function(e,i){t.hierarchical.levelDownstream(e,i)},i=this._getHubSizes(),n=function(n){var o=i[n];if(0===o)return"break";hm(t.body.nodeIndices,(function(i){var n=t.body.nodes[i];o===t._getActiveEdges(n).length&&t._crawlNetwork(e,i)}))},o=0;o<i.length;++o){if("break"===n(o))break}}},{key:"_determineLevelsCustomCallback",value:function(){var t=this;this._crawlNetwork((function(e,i,n){var o=t.hierarchical.levels[e.id];void 0===o&&(o=t.hierarchical.levels[e.id]=1e5);var r=(EC.cloneOptions(e,"node"),EC.cloneOptions(i,"node"),void EC.cloneOptions(n,"edge"));t.hierarchical.levels[i.id]=o+r})),this.hierarchical.setMinLevelToZero(this.body.nodes)}},{key:"_determineLevelsDirected",value:function(){var t,e=this,i=i_(t=this.body.nodeIndices).call(t,(function(t,i){return t.set(i,e.body.nodes[i]),t}),new Jw);"roots"===this.options.hierarchical.shakeTowards?this.hierarchical.levels=function(t){return oT((function(e){var i,n;return tT(i=Xf(n=e.edges).call(n,(function(e){return t.has(e.toId)}))).call(i,(function(t){return t.from===e}))}),(function(t,e){return e<t}),"to",t)}(i):this.hierarchical.levels=function(t){return oT((function(e){var i,n;return tT(i=Xf(n=e.edges).call(n,(function(e){return t.has(e.toId)}))).call(i,(function(t){return t.to===e}))}),(function(t,e){return e>t}),"from",t)}(i),this.hierarchical.setMinLevelToZero(this.body.nodes)}},{key:"_generateMap",value:function(){var t=this;this._crawlNetwork((function(e,i){t.hierarchical.levels[i.id]>t.hierarchical.levels[e.id]&&t.hierarchical.addRelation(e.id,i.id)})),this.hierarchical.checkIfTree()}},{key:"_crawlNetwork",value:function(){var t=this,e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:function(){},i=arguments.length>1?arguments[1]:void 0,n={},o=function i(o,r){if(void 0===n[o.id]){var s;t.hierarchical.setTreeIndex(o,r),n[o.id]=!0;for(var a=t._getActiveEdges(o),h=0;h<a.length;h++){var l=a[h];!0===l.connected&&(s=l.toId==o.id?l.from:l.to,o.id!=s.id&&(e(o,s,l),i(s,r)))}}};if(void 0===i)for(var r=0,s=0;s<this.body.nodeIndices.length;s++){var a=this.body.nodeIndices[s];if(void 0===n[a]){var h=this.body.nodes[a];o(h,r),r+=1}}else{var l=this.body.nodes[i];if(void 0===l)return void console.error("Node not found:",i);o(l)}}},{key:"_shiftBlock",value:function(t,e){var i=this,n={};!function t(o){if(!n[o]){n[o]=!0,i.direction.shift(o,e);var r=i.hierarchical.childrenReference[o];if(void 0!==r)for(var s=0;s<r.length;s++)t(r[s])}}(t)}},{key:"_findCommonParent",value:function(t,e){var i=this,n={};return function t(e,n){var o=i.hierarchical.parentReference[n];if(void 0!==o)for(var r=0;r<o.length;r++){var s=o[r];e[s]=!0,t(e,s)}}(n,t),function t(e,n){var o=i.hierarchical.parentReference[n];if(void 0!==o)for(var r=0;r<o.length;r++){var s=o[r];if(void 0!==e[s])return{foundParent:s,withChild:n};var a=t(e,s);if(null!==a.foundParent)return a}return{foundParent:null,withChild:n}}(n,e)}},{key:"setDirectionStrategy",value:function(){var t="UD"===this.options.hierarchical.direction||"DU"===this.options.hierarchical.direction;this.direction=t?new YS(this):new XS(this)}},{key:"_getCenterPosition",value:function(t){for(var e=1e9,i=-1e9,n=0;n<t.length;n++){var o=void 0;if(void 0!==t[n].id)o=t[n];else{var r=t[n];o=this.body.nodes[r]}var s=this.direction.getPosition(o);e=Math.min(e,s),i=Math.max(i,s)}return.5*(e+i)}}]),t}();function aT(t,e){var i=void 0!==tu&&ih(t)||t["@@iterator"];if(!i){if(lu(t)||(i=function(t,e){var i;if(!t)return;if("string"==typeof t)return hT(t,e);var n=au(i=Object.prototype.toString.call(t)).call(i,8,-1);"Object"===n&&t.constructor&&(n=t.constructor.name);if("Map"===n||"Set"===n)return ja(t);if("Arguments"===n||/^(?:Ui|I)nt(?:8|16|32)(?:Clamped)?Array$/.test(n))return hT(t,e)}(t))||e&&t&&"number"==typeof t.length){i&&(t=i);var n=0,o=function(){};return{s:o,n:function(){return n>=t.length?{done:!0}:{done:!1,value:t[n++]}},e:function(t){throw t},f:o}}throw new TypeError("Invalid attempt to iterate non-iterable instance.\nIn order to be iterable, non-array objects must have a [Symbol.iterator]() method.")}var r,s=!0,a=!1;return{s:function(){i=i.call(t)},n:function(){var t=i.next();return s=t.done,t},e:function(t){a=!0,r=t},f:function(){try{s||null==i.return||i.return()}finally{if(a)throw r}}}}function hT(t,e){(null==e||e>t.length)&&(e=t.length);for(var i=0,n=new Array(e);i<e;i++)n[i]=t[i];return n}var lT=function(){function t(e,i,n,o){var r,s,a=this;Yd(this,t),this.body=e,this.canvas=i,this.selectionHandler=n,this.interactionHandler=o,this.editMode=!1,this.manipulationDiv=void 0,this.editModeDiv=void 0,this.closeDiv=void 0,this._domEventListenerCleanupQueue=[],this.temporaryUIFunctions={},this.temporaryEventFunctions=[],this.touchTime=0,this.temporaryIds={nodes:[],edges:[]},this.guiEnabled=!1,this.inMode=!1,this.selectedControlNode=void 0,this.options={},this.defaultOptions={enabled:!1,initiallyActive:!1,addNode:!0,addEdge:!0,editNode:void 0,editEdge:!0,deleteNode:!0,deleteEdge:!0,controlNodeStyle:{shape:"dot",size:6,color:{background:"#ff0000",border:"#3c3c3c",highlight:{background:"#07f968",border:"#3c3c3c"}},borderWidth:2,borderWidthSelected:2}},un(this.options,this.defaultOptions),this.body.emitter.on("destroy",(function(){a._clean()})),this.body.emitter.on("_dataChanged",zn(r=this._restore).call(r,this)),this.body.emitter.on("_resetData",zn(s=this._restore).call(s,this))}return Kd(t,[{key:"_restore",value:function(){!1!==this.inMode&&(!0===this.options.initiallyActive?this.enableEditMode():this.disableEditMode())}},{key:"setOptions",value:function(t,e,i){void 0!==e&&(void 0!==e.locale?this.options.locale=e.locale:this.options.locale=i.locale,void 0!==e.locales?this.options.locales=e.locales:this.options.locales=i.locales),void 0!==t&&("boolean"==typeof t?this.options.enabled=t:(this.options.enabled=!0,nm(this.options,t)),!0===this.options.initiallyActive&&(this.editMode=!0),this._setup())}},{key:"toggleEditMode",value:function(){!0===this.editMode?this.disableEditMode():this.enableEditMode()}},{key:"enableEditMode",value:function(){this.editMode=!0,this._clean(),!0===this.guiEnabled&&(this.manipulationDiv.style.display="block",this.closeDiv.style.display="block",this.editModeDiv.style.display="none",this.showManipulatorToolbar())}},{key:"disableEditMode",value:function(){this.editMode=!1,this._clean(),!0===this.guiEnabled&&(this.manipulationDiv.style.display="none",this.closeDiv.style.display="none",this.editModeDiv.style.display="block",this._createEditButton())}},{key:"showManipulatorToolbar",value:function(){if(this._clean(),this.manipulationDOM={},!0===this.guiEnabled){var t,e;this.editMode=!0,this.manipulationDiv.style.display="block",this.closeDiv.style.display="block";var i=this.selectionHandler.getSelectedNodeCount(),n=this.selectionHandler.getSelectedEdgeCount(),o=i+n,r=this.options.locales[this.options.locale],s=!1;!1!==this.options.addNode&&(this._createAddNodeButton(r),s=!0),!1!==this.options.addEdge&&(!0===s?this._createSeperator(1):s=!0,this._createAddEdgeButton(r)),1===i&&"function"==typeof this.options.editNode?(!0===s?this._createSeperator(2):s=!0,this._createEditNodeButton(r)):1===n&&0===i&&!1!==this.options.editEdge&&(!0===s?this._createSeperator(3):s=!0,this._createEditEdgeButton(r)),0!==o&&(i>0&&!1!==this.options.deleteNode||0===i&&!1!==this.options.deleteEdge)&&(!0===s&&this._createSeperator(4),this._createDeleteButton(r)),this._bindElementEvents(this.closeDiv,zn(t=this.toggleEditMode).call(t,this)),this._temporaryBindEvent("select",zn(e=this.showManipulatorToolbar).call(e,this))}this.body.emitter.emit("_redraw")}},{key:"addNodeMode",value:function(){var t;if(!0!==this.editMode&&this.enableEditMode(),this._clean(),this.inMode="addNode",!0===this.guiEnabled){var e,i=this.options.locales[this.options.locale];this.manipulationDOM={},this._createBackButton(i),this._createSeperator(),this._createDescription(i.addDescription||this.options.locales.en.addDescription),this._bindElementEvents(this.closeDiv,zn(e=this.toggleEditMode).call(e,this))}this._temporaryBindEvent("click",zn(t=this._performAddNode).call(t,this))}},{key:"editNode",value:function(){var t=this;!0!==this.editMode&&this.enableEditMode(),this._clean();var e=this.selectionHandler.getSelectedNodes()[0];if(void 0!==e){if(this.inMode="editNode","function"!=typeof this.options.editNode)throw new Error("No function has been configured to handle the editing of nodes.");if(!0!==e.isCluster){var i=nm({},e.options,!1);if(i.x=e.x,i.y=e.y,2!==this.options.editNode.length)throw new Error("The function for edit does not support two arguments (data, callback)");this.options.editNode(i,(function(e){null!=e&&"editNode"===t.inMode&&t.body.data.nodes.getDataSet().update(e),t.showManipulatorToolbar()}))}else alert(this.options.locales[this.options.locale].editClusterError||this.options.locales.en.editClusterError)}else this.showManipulatorToolbar()}},{key:"addEdgeMode",value:function(){var t,e,i,n,o;if(!0!==this.editMode&&this.enableEditMode(),this._clean(),this.inMode="addEdge",!0===this.guiEnabled){var r,s=this.options.locales[this.options.locale];this.manipulationDOM={},this._createBackButton(s),this._createSeperator(),this._createDescription(s.edgeDescription||this.options.locales.en.edgeDescription),this._bindElementEvents(this.closeDiv,zn(r=this.toggleEditMode).call(r,this))}this._temporaryBindUI("onTouch",zn(t=this._handleConnect).call(t,this)),this._temporaryBindUI("onDragEnd",zn(e=this._finishConnect).call(e,this)),this._temporaryBindUI("onDrag",zn(i=this._dragControlNode).call(i,this)),this._temporaryBindUI("onRelease",zn(n=this._finishConnect).call(n,this)),this._temporaryBindUI("onDragStart",zn(o=this._dragStartEdge).call(o,this)),this._temporaryBindUI("onHold",(function(){}))}},{key:"editEdgeMode",value:function(){if(!0!==this.editMode&&this.enableEditMode(),this._clean(),this.inMode="editEdge","object"!==Qc(this.options.editEdge)||"function"!=typeof this.options.editEdge.editWithoutDrag||(this.edgeBeingEditedId=this.selectionHandler.getSelectedEdgeIds()[0],void 0===this.edgeBeingEditedId)){if(!0===this.guiEnabled){var t,e=this.options.locales[this.options.locale];this.manipulationDOM={},this._createBackButton(e),this._createSeperator(),this._createDescription(e.editEdgeDescription||this.options.locales.en.editEdgeDescription),this._bindElementEvents(this.closeDiv,zn(t=this.toggleEditMode).call(t,this))}if(this.edgeBeingEditedId=this.selectionHandler.getSelectedEdgeIds()[0],void 0!==this.edgeBeingEditedId){var i,n,o,r,s=this.body.edges[this.edgeBeingEditedId],a=this._getNewTargetNode(s.from.x,s.from.y),h=this._getNewTargetNode(s.to.x,s.to.y);this.temporaryIds.nodes.push(a.id),this.temporaryIds.nodes.push(h.id),this.body.nodes[a.id]=a,this.body.nodeIndices.push(a.id),this.body.nodes[h.id]=h,this.body.nodeIndices.push(h.id),this._temporaryBindUI("onTouch",zn(i=this._controlNodeTouch).call(i,this)),this._temporaryBindUI("onTap",(function(){})),this._temporaryBindUI("onHold",(function(){})),this._temporaryBindUI("onDragStart",zn(n=this._controlNodeDragStart).call(n,this)),this._temporaryBindUI("onDrag",zn(o=this._controlNodeDrag).call(o,this)),this._temporaryBindUI("onDragEnd",zn(r=this._controlNodeDragEnd).call(r,this)),this._temporaryBindUI("onMouseMove",(function(){})),this._temporaryBindEvent("beforeDrawing",(function(t){var e=s.edgeType.findBorderPositions(t);!1===a.selected&&(a.x=e.from.x,a.y=e.from.y),!1===h.selected&&(h.x=e.to.x,h.y=e.to.y)})),this.body.emitter.emit("_redraw")}else this.showManipulatorToolbar()}else{var l=this.body.edges[this.edgeBeingEditedId];this._performEditEdge(l.from.id,l.to.id)}}},{key:"deleteSelected",value:function(){var t=this;!0!==this.editMode&&this.enableEditMode(),this._clean(),this.inMode="delete";var e=this.selectionHandler.getSelectedNodeIds(),i=this.selectionHandler.getSelectedEdgeIds(),n=void 0;if(e.length>0){for(var o=0;o<e.length;o++)if(!0===this.body.nodes[e[o]].isCluster)return void alert(this.options.locales[this.options.locale].deleteClusterError||this.options.locales.en.deleteClusterError);"function"==typeof this.options.deleteNode&&(n=this.options.deleteNode)}else i.length>0&&"function"==typeof this.options.deleteEdge&&(n=this.options.deleteEdge);if("function"==typeof n){var r={nodes:e,edges:i};if(2!==n.length)throw new Error("The function for delete does not support two arguments (data, callback)");n(r,(function(e){null!=e&&"delete"===t.inMode?(t.body.data.edges.getDataSet().remove(e.edges),t.body.data.nodes.getDataSet().remove(e.nodes),t.body.emitter.emit("startSimulation"),t.showManipulatorToolbar()):(t.body.emitter.emit("startSimulation"),t.showManipulatorToolbar())}))}else this.body.data.edges.getDataSet().remove(i),this.body.data.nodes.getDataSet().remove(e),this.body.emitter.emit("startSimulation"),this.showManipulatorToolbar()}},{key:"_setup",value:function(){!0===this.options.enabled?(this.guiEnabled=!0,this._createWrappers(),!1===this.editMode?this._createEditButton():this.showManipulatorToolbar()):(this._removeManipulationDOM(),this.guiEnabled=!1)}},{key:"_createWrappers",value:function(){var t,e;(void 0===this.manipulationDiv&&(this.manipulationDiv=document.createElement("div"),this.manipulationDiv.className="vis-manipulation",!0===this.editMode?this.manipulationDiv.style.display="block":this.manipulationDiv.style.display="none",this.canvas.frame.appendChild(this.manipulationDiv)),void 0===this.editModeDiv&&(this.editModeDiv=document.createElement("div"),this.editModeDiv.className="vis-edit-mode",!0===this.editMode?this.editModeDiv.style.display="none":this.editModeDiv.style.display="block",this.canvas.frame.appendChild(this.editModeDiv)),void 0===this.closeDiv)&&(this.closeDiv=document.createElement("button"),this.closeDiv.className="vis-close",this.closeDiv.setAttribute("aria-label",null!==(t=null===(e=this.options.locales[this.options.locale])||void 0===e?void 0:e.close)&&void 0!==t?t:this.options.locales.en.close),this.closeDiv.style.display=this.manipulationDiv.style.display,this.canvas.frame.appendChild(this.closeDiv))}},{key:"_getNewTargetNode",value:function(t,e){var i=nm({},this.options.controlNodeStyle);i.id="targetNode"+Ax(),i.hidden=!1,i.physics=!1,i.x=t,i.y=e;var n=this.body.functions.createNode(i);return n.shape.boundingBox={left:t,right:t,top:e,bottom:e},n}},{key:"_createEditButton",value:function(){var t;this._clean(),this.manipulationDOM={},Ky(this.editModeDiv);var e=this.options.locales[this.options.locale],i=this._createButton("editMode","vis-edit vis-edit-mode",e.edit||this.options.locales.en.edit);this.editModeDiv.appendChild(i),this._bindElementEvents(i,zn(t=this.toggleEditMode).call(t,this))}},{key:"_clean",value:function(){this.inMode=!1,!0===this.guiEnabled&&(Ky(this.editModeDiv),Ky(this.manipulationDiv),this._cleanupDOMEventListeners()),this._cleanupTemporaryNodesAndEdges(),this._unbindTemporaryUIs(),this._unbindTemporaryEvents(),this.body.emitter.emit("restorePhysics")}},{key:"_cleanupDOMEventListeners",value:function(){var t,e,i=aT(ff(t=this._domEventListenerCleanupQueue).call(t,0));try{for(i.s();!(e=i.n()).done;){(0,e.value)()}}catch(t){i.e(t)}finally{i.f()}}},{key:"_removeManipulationDOM",value:function(){this._clean(),Ky(this.manipulationDiv),Ky(this.editModeDiv),Ky(this.closeDiv),this.manipulationDiv&&this.canvas.frame.removeChild(this.manipulationDiv),this.editModeDiv&&this.canvas.frame.removeChild(this.editModeDiv),this.closeDiv&&this.canvas.frame.removeChild(this.closeDiv),this.manipulationDiv=void 0,this.editModeDiv=void 0,this.closeDiv=void 0}},{key:"_createSeperator",value:function(){var t=arguments.length>0&&void 0!==arguments[0]?arguments[0]:1;this.manipulationDOM["seperatorLineDiv"+t]=document.createElement("div"),this.manipulationDOM["seperatorLineDiv"+t].className="vis-separator-line",this.manipulationDiv.appendChild(this.manipulationDOM["seperatorLineDiv"+t])}},{key:"_createAddNodeButton",value:function(t){var e,i=this._createButton("addNode","vis-add",t.addNode||this.options.locales.en.addNode);this.manipulationDiv.appendChild(i),this._bindElementEvents(i,zn(e=this.addNodeMode).call(e,this))}},{key:"_createAddEdgeButton",value:function(t){var e,i=this._createButton("addEdge","vis-connect",t.addEdge||this.options.locales.en.addEdge);this.manipulationDiv.appendChild(i),this._bindElementEvents(i,zn(e=this.addEdgeMode).call(e,this))}},{key:"_createEditNodeButton",value:function(t){var e,i=this._createButton("editNode","vis-edit",t.editNode||this.options.locales.en.editNode);this.manipulationDiv.appendChild(i),this._bindElementEvents(i,zn(e=this.editNode).call(e,this))}},{key:"_createEditEdgeButton",value:function(t){var e,i=this._createButton("editEdge","vis-edit",t.editEdge||this.options.locales.en.editEdge);this.manipulationDiv.appendChild(i),this._bindElementEvents(i,zn(e=this.editEdgeMode).call(e,this))}},{key:"_createDeleteButton",value:function(t){var e,i;i=this.options.rtl?"vis-delete-rtl":"vis-delete";var n=this._createButton("delete",i,t.del||this.options.locales.en.del);this.manipulationDiv.appendChild(n),this._bindElementEvents(n,zn(e=this.deleteSelected).call(e,this))}},{key:"_createBackButton",value:function(t){var e,i=this._createButton("back","vis-back",t.back||this.options.locales.en.back);this.manipulationDiv.appendChild(i),this._bindElementEvents(i,zn(e=this.showManipulatorToolbar).call(e,this))}},{key:"_createButton",value:function(t,e,i){var n=arguments.length>3&&void 0!==arguments[3]?arguments[3]:"vis-label";return this.manipulationDOM[t+"Div"]=document.createElement("button"),this.manipulationDOM[t+"Div"].className="vis-button "+e,this.manipulationDOM[t+"Label"]=document.createElement("div"),this.manipulationDOM[t+"Label"].className=n,this.manipulationDOM[t+"Label"].innerText=i,this.manipulationDOM[t+"Div"].appendChild(this.manipulationDOM[t+"Label"]),this.manipulationDOM[t+"Div"]}},{key:"_createDescription",value:function(t){this.manipulationDOM.descriptionLabel=document.createElement("div"),this.manipulationDOM.descriptionLabel.className="vis-none",this.manipulationDOM.descriptionLabel.innerText=t,this.manipulationDiv.appendChild(this.manipulationDOM.descriptionLabel)}},{key:"_temporaryBindEvent",value:function(t,e){this.temporaryEventFunctions.push({event:t,boundFunction:e}),this.body.emitter.on(t,e)}},{key:"_temporaryBindUI",value:function(t,e){if(void 0===this.body.eventListeners[t])throw new Error("This UI function does not exist. Typo? You tried: "+t+" possible are: "+gv(bu(this.body.eventListeners)));this.temporaryUIFunctions[t]=this.body.eventListeners[t],this.body.eventListeners[t]=e}},{key:"_unbindTemporaryUIs",value:function(){for(var t in this.temporaryUIFunctions)Object.prototype.hasOwnProperty.call(this.temporaryUIFunctions,t)&&(this.body.eventListeners[t]=this.temporaryUIFunctions[t],delete this.temporaryUIFunctions[t]);this.temporaryUIFunctions={}}},{key:"_unbindTemporaryEvents",value:function(){for(var t=0;t<this.temporaryEventFunctions.length;t++){var e=this.temporaryEventFunctions[t].event,i=this.temporaryEventFunctions[t].boundFunction;this.body.emitter.off(e,i)}this.temporaryEventFunctions=[]}},{key:"_bindElementEvents",value:function(t,e){var i=new Wm(t,{});IC(i,e),this._domEventListenerCleanupQueue.push((function(){i.destroy()}));var n=function(t){var i=t.keyCode,n=t.key;"Enter"!==n&&" "!==n&&13!==i&&32!==i||e()};t.addEventListener("keyup",n,!1),this._domEventListenerCleanupQueue.push((function(){t.removeEventListener("keyup",n,!1)}))}},{key:"_cleanupTemporaryNodesAndEdges",value:function(){for(var t=0;t<this.temporaryIds.edges.length;t++){var e;this.body.edges[this.temporaryIds.edges[t]].disconnect(),delete this.body.edges[this.temporaryIds.edges[t]];var i,n=Fp(e=this.body.edgeIndices).call(e,this.temporaryIds.edges[t]);if(-1!==n)ff(i=this.body.edgeIndices).call(i,n,1)}for(var o=0;o<this.temporaryIds.nodes.length;o++){var r;delete this.body.nodes[this.temporaryIds.nodes[o]];var s,a=Fp(r=this.body.nodeIndices).call(r,this.temporaryIds.nodes[o]);if(-1!==a)ff(s=this.body.nodeIndices).call(s,a,1)}this.temporaryIds={nodes:[],edges:[]}}},{key:"_controlNodeTouch",value:function(t){this.selectionHandler.unselectAll(),this.lastTouch=this.body.functions.getPointer(t.center),this.lastTouch.translation=un({},this.body.view.translation)}},{key:"_controlNodeDragStart",value:function(){var t=this.lastTouch,e=this.selectionHandler._pointerToPositionObject(t),i=this.body.nodes[this.temporaryIds.nodes[0]],n=this.body.nodes[this.temporaryIds.nodes[1]],o=this.body.edges[this.edgeBeingEditedId];this.selectedControlNode=void 0;var r=i.isOverlappingWith(e),s=n.isOverlappingWith(e);!0===r?(this.selectedControlNode=i,o.edgeType.from=i):!0===s&&(this.selectedControlNode=n,o.edgeType.to=n),void 0!==this.selectedControlNode&&this.selectionHandler.selectObject(this.selectedControlNode),this.body.emitter.emit("_redraw")}},{key:"_controlNodeDrag",value:function(t){this.body.emitter.emit("disablePhysics");var e=this.body.functions.getPointer(t.center),i=this.canvas.DOMtoCanvas(e);void 0!==this.selectedControlNode?(this.selectedControlNode.x=i.x,this.selectedControlNode.y=i.y):this.interactionHandler.onDrag(t),this.body.emitter.emit("_redraw")}},{key:"_controlNodeDragEnd",value:function(t){var e=this.body.functions.getPointer(t.center),i=this.selectionHandler._pointerToPositionObject(e),n=this.body.edges[this.edgeBeingEditedId];if(void 0!==this.selectedControlNode){this.selectionHandler.unselectAll();for(var o=this.selectionHandler._getAllNodesOverlappingWith(i),r=void 0,s=o.length-1;s>=0;s--)if(o[s]!==this.selectedControlNode.id){r=this.body.nodes[o[s]];break}if(void 0!==r&&void 0!==this.selectedControlNode)if(!0===r.isCluster)alert(this.options.locales[this.options.locale].createEdgeError||this.options.locales.en.createEdgeError);else{var a=this.body.nodes[this.temporaryIds.nodes[0]];this.selectedControlNode.id===a.id?this._performEditEdge(r.id,n.to.id):this._performEditEdge(n.from.id,r.id)}else n.updateEdgeType(),this.body.emitter.emit("restorePhysics");this.body.emitter.emit("_redraw")}}},{key:"_handleConnect",value:function(t){if((new Date).valueOf()-this.touchTime>100){this.lastTouch=this.body.functions.getPointer(t.center),this.lastTouch.translation=un({},this.body.view.translation),this.interactionHandler.drag.pointer=this.lastTouch,this.interactionHandler.drag.translation=this.lastTouch.translation;var e=this.lastTouch,i=this.selectionHandler.getNodeAt(e);if(void 0!==i)if(!0===i.isCluster)alert(this.options.locales[this.options.locale].createEdgeError||this.options.locales.en.createEdgeError);else{var n=this._getNewTargetNode(i.x,i.y);this.body.nodes[n.id]=n,this.body.nodeIndices.push(n.id);var o=this.body.functions.createEdge({id:"connectionEdge"+Ax(),from:i.id,to:n.id,physics:!1,smooth:{enabled:!0,type:"continuous",roundness:.5}});this.body.edges[o.id]=o,this.body.edgeIndices.push(o.id),this.temporaryIds.nodes.push(n.id),this.temporaryIds.edges.push(o.id)}this.touchTime=(new Date).valueOf()}}},{key:"_dragControlNode",value:function(t){var e=this.body.functions.getPointer(t.center),i=this.selectionHandler._pointerToPositionObject(e),n=void 0;void 0!==this.temporaryIds.edges[0]&&(n=this.body.edges[this.temporaryIds.edges[0]].fromId);for(var o=this.selectionHandler._getAllNodesOverlappingWith(i),r=void 0,s=o.length-1;s>=0;s--){var a;if(-1===Fp(a=this.temporaryIds.nodes).call(a,o[s])){r=this.body.nodes[o[s]];break}}if(t.controlEdge={from:n,to:r?r.id:void 0},this.selectionHandler.generateClickEvent("controlNodeDragging",t,e),void 0!==this.temporaryIds.nodes[0]){var h=this.body.nodes[this.temporaryIds.nodes[0]];h.x=this.canvas._XconvertDOMtoCanvas(e.x),h.y=this.canvas._YconvertDOMtoCanvas(e.y),this.body.emitter.emit("_redraw")}else this.interactionHandler.onDrag(t)}},{key:"_finishConnect",value:function(t){var e=this.body.functions.getPointer(t.center),i=this.selectionHandler._pointerToPositionObject(e),n=void 0;void 0!==this.temporaryIds.edges[0]&&(n=this.body.edges[this.temporaryIds.edges[0]].fromId);for(var o=this.selectionHandler._getAllNodesOverlappingWith(i),r=void 0,s=o.length-1;s>=0;s--){var a;if(-1===Fp(a=this.temporaryIds.nodes).call(a,o[s])){r=this.body.nodes[o[s]];break}}this._cleanupTemporaryNodesAndEdges(),void 0!==r&&(!0===r.isCluster?alert(this.options.locales[this.options.locale].createEdgeError||this.options.locales.en.createEdgeError):void 0!==this.body.nodes[n]&&void 0!==this.body.nodes[r.id]&&this._performAddEdge(n,r.id)),t.controlEdge={from:n,to:r?r.id:void 0},this.selectionHandler.generateClickEvent("controlNodeDragEnd",t,e),this.body.emitter.emit("_redraw")}},{key:"_dragStartEdge",value:function(t){var e=this.lastTouch;this.selectionHandler.generateClickEvent("dragStart",t,e,void 0,!0)}},{key:"_performAddNode",value:function(t){var e=this,i={id:Ax(),x:t.pointer.canvas.x,y:t.pointer.canvas.y,label:"new"};if("function"==typeof this.options.addNode){if(2!==this.options.addNode.length)throw this.showManipulatorToolbar(),new Error("The function for add does not support two arguments (data,callback)");this.options.addNode(i,(function(t){null!=t&&"addNode"===e.inMode&&e.body.data.nodes.getDataSet().add(t),e.showManipulatorToolbar()}))}else this.body.data.nodes.getDataSet().add(i),this.showManipulatorToolbar()}},{key:"_performAddEdge",value:function(t,e){var i=this,n={from:t,to:e};if("function"==typeof this.options.addEdge){if(2!==this.options.addEdge.length)throw new Error("The function for connect does not support two arguments (data,callback)");this.options.addEdge(n,(function(t){null!=t&&"addEdge"===i.inMode&&(i.body.data.edges.getDataSet().add(t),i.selectionHandler.unselectAll(),i.showManipulatorToolbar())}))}else this.body.data.edges.getDataSet().add(n),this.selectionHandler.unselectAll(),this.showManipulatorToolbar()}},{key:"_performEditEdge",value:function(t,e){var i=this,n={id:this.edgeBeingEditedId,from:t,to:e,label:this.body.data.edges.get(this.edgeBeingEditedId).label},o=this.options.editEdge;if("object"===Qc(o)&&(o=o.editWithoutDrag),"function"==typeof o){if(2!==o.length)throw new Error("The function for edit does not support two arguments (data, callback)");o(n,(function(t){null==t||"editEdge"!==i.inMode?(i.body.edges[n.id].updateEdgeType(),i.body.emitter.emit("_redraw"),i.showManipulatorToolbar()):(i.body.data.edges.getDataSet().update(t),i.selectionHandler.unselectAll(),i.showManipulatorToolbar())}))}else this.body.data.edges.getDataSet().update(n),this.selectionHandler.unselectAll(),this.showManipulatorToolbar()}}]),t}(),dT="string",cT="boolean",uT="number",fT="array",pT="object",vT=["arrow","bar","box","circle","crow","curve","diamond","image","inv_curve","inv_triangle","triangle","vee"],gT={borderWidth:{number:uT},borderWidthSelected:{number:uT,undefined:"undefined"},brokenImage:{string:dT,undefined:"undefined"},chosen:{label:{boolean:cT,function:"function"},node:{boolean:cT,function:"function"},__type__:{object:pT,boolean:cT}},color:{border:{string:dT},background:{string:dT},highlight:{border:{string:dT},background:{string:dT},__type__:{object:pT,string:dT}},hover:{border:{string:dT},background:{string:dT},__type__:{object:pT,string:dT}},__type__:{object:pT,string:dT}},opacity:{number:uT,undefined:"undefined"},fixed:{x:{boolean:cT},y:{boolean:cT},__type__:{object:pT,boolean:cT}},font:{align:{string:dT},color:{string:dT},size:{number:uT},face:{string:dT},background:{string:dT},strokeWidth:{number:uT},strokeColor:{string:dT},vadjust:{number:uT},multi:{boolean:cT,string:dT},bold:{color:{string:dT},size:{number:uT},face:{string:dT},mod:{string:dT},vadjust:{number:uT},__type__:{object:pT,string:dT}},boldital:{color:{string:dT},size:{number:uT},face:{string:dT},mod:{string:dT},vadjust:{number:uT},__type__:{object:pT,string:dT}},ital:{color:{string:dT},size:{number:uT},face:{string:dT},mod:{string:dT},vadjust:{number:uT},__type__:{object:pT,string:dT}},mono:{color:{string:dT},size:{number:uT},face:{string:dT},mod:{string:dT},vadjust:{number:uT},__type__:{object:pT,string:dT}},__type__:{object:pT,string:dT}},group:{string:dT,number:uT,undefined:"undefined"},heightConstraint:{minimum:{number:uT},valign:{string:dT},__type__:{object:pT,boolean:cT,number:uT}},hidden:{boolean:cT},icon:{face:{string:dT},code:{string:dT},size:{number:uT},color:{string:dT},weight:{string:dT,number:uT},__type__:{object:pT}},id:{string:dT,number:uT},image:{selected:{string:dT,undefined:"undefined"},unselected:{string:dT,undefined:"undefined"},__type__:{object:pT,string:dT}},imagePadding:{top:{number:uT},right:{number:uT},bottom:{number:uT},left:{number:uT},__type__:{object:pT,number:uT}},label:{string:dT,undefined:"undefined"},labelHighlightBold:{boolean:cT},level:{number:uT,undefined:"undefined"},margin:{top:{number:uT},right:{number:uT},bottom:{number:uT},left:{number:uT},__type__:{object:pT,number:uT}},mass:{number:uT},physics:{boolean:cT},scaling:{min:{number:uT},max:{number:uT},label:{enabled:{boolean:cT},min:{number:uT},max:{number:uT},maxVisible:{number:uT},drawThreshold:{number:uT},__type__:{object:pT,boolean:cT}},customScalingFunction:{function:"function"},__type__:{object:pT}},shadow:{enabled:{boolean:cT},color:{string:dT},size:{number:uT},x:{number:uT},y:{number:uT},__type__:{object:pT,boolean:cT}},shape:{string:["custom","ellipse","circle","database","box","text","image","circularImage","diamond","dot","star","triangle","triangleDown","square","icon","hexagon"]},ctxRenderer:{function:"function"},shapeProperties:{borderDashes:{boolean:cT,array:fT},borderRadius:{number:uT},interpolation:{boolean:cT},useImageSize:{boolean:cT},useBorderWithImage:{boolean:cT},coordinateOrigin:{string:["center","top-left"]},__type__:{object:pT}},size:{number:uT},title:{string:dT,dom:"dom",undefined:"undefined"},value:{number:uT,undefined:"undefined"},widthConstraint:{minimum:{number:uT},maximum:{number:uT},__type__:{object:pT,boolean:cT,number:uT}},x:{number:uT},y:{number:uT},__type__:{object:pT}},yT={configure:{enabled:{boolean:cT},filter:{boolean:cT,string:dT,array:fT,function:"function"},container:{dom:"dom"},showButton:{boolean:cT},__type__:{object:pT,boolean:cT,string:dT,array:fT,function:"function"}},edges:{arrows:{to:{enabled:{boolean:cT},scaleFactor:{number:uT},type:{string:vT},imageHeight:{number:uT},imageWidth:{number:uT},src:{string:dT},__type__:{object:pT,boolean:cT}},middle:{enabled:{boolean:cT},scaleFactor:{number:uT},type:{string:vT},imageWidth:{number:uT},imageHeight:{number:uT},src:{string:dT},__type__:{object:pT,boolean:cT}},from:{enabled:{boolean:cT},scaleFactor:{number:uT},type:{string:vT},imageWidth:{number:uT},imageHeight:{number:uT},src:{string:dT},__type__:{object:pT,boolean:cT}},__type__:{string:["from","to","middle"],object:pT}},endPointOffset:{from:{number:uT},to:{number:uT},__type__:{object:pT,number:uT}},arrowStrikethrough:{boolean:cT},background:{enabled:{boolean:cT},color:{string:dT},size:{number:uT},dashes:{boolean:cT,array:fT},__type__:{object:pT,boolean:cT}},chosen:{label:{boolean:cT,function:"function"},edge:{boolean:cT,function:"function"},__type__:{object:pT,boolean:cT}},color:{color:{string:dT},highlight:{string:dT},hover:{string:dT},inherit:{string:["from","to","both"],boolean:cT},opacity:{number:uT},__type__:{object:pT,string:dT}},dashes:{boolean:cT,array:fT},font:{color:{string:dT},size:{number:uT},face:{string:dT},background:{string:dT},strokeWidth:{number:uT},strokeColor:{string:dT},align:{string:["horizontal","top","middle","bottom"]},vadjust:{number:uT},multi:{boolean:cT,string:dT},bold:{color:{string:dT},size:{number:uT},face:{string:dT},mod:{string:dT},vadjust:{number:uT},__type__:{object:pT,string:dT}},boldital:{color:{string:dT},size:{number:uT},face:{string:dT},mod:{string:dT},vadjust:{number:uT},__type__:{object:pT,string:dT}},ital:{color:{string:dT},size:{number:uT},face:{string:dT},mod:{string:dT},vadjust:{number:uT},__type__:{object:pT,string:dT}},mono:{color:{string:dT},size:{number:uT},face:{string:dT},mod:{string:dT},vadjust:{number:uT},__type__:{object:pT,string:dT}},__type__:{object:pT,string:dT}},hidden:{boolean:cT},hoverWidth:{function:"function",number:uT},label:{string:dT,undefined:"undefined"},labelHighlightBold:{boolean:cT},length:{number:uT,undefined:"undefined"},physics:{boolean:cT},scaling:{min:{number:uT},max:{number:uT},label:{enabled:{boolean:cT},min:{number:uT},max:{number:uT},maxVisible:{number:uT},drawThreshold:{number:uT},__type__:{object:pT,boolean:cT}},customScalingFunction:{function:"function"},__type__:{object:pT}},selectionWidth:{function:"function",number:uT},selfReferenceSize:{number:uT},selfReference:{size:{number:uT},angle:{number:uT},renderBehindTheNode:{boolean:cT},__type__:{object:pT}},shadow:{enabled:{boolean:cT},color:{string:dT},size:{number:uT},x:{number:uT},y:{number:uT},__type__:{object:pT,boolean:cT}},smooth:{enabled:{boolean:cT},type:{string:["dynamic","continuous","discrete","diagonalCross","straightCross","horizontal","vertical","curvedCW","curvedCCW","cubicBezier"]},roundness:{number:uT},forceDirection:{string:["horizontal","vertical","none"],boolean:cT},__type__:{object:pT,boolean:cT}},title:{string:dT,undefined:"undefined"},width:{number:uT},widthConstraint:{maximum:{number:uT},__type__:{object:pT,boolean:cT,number:uT}},value:{number:uT,undefined:"undefined"},__type__:{object:pT}},groups:{useDefaultGroups:{boolean:cT},__any__:gT,__type__:{object:pT}},interaction:{dragNodes:{boolean:cT},dragView:{boolean:cT},hideEdgesOnDrag:{boolean:cT},hideEdgesOnZoom:{boolean:cT},hideNodesOnDrag:{boolean:cT},hover:{boolean:cT},keyboard:{enabled:{boolean:cT},speed:{x:{number:uT},y:{number:uT},zoom:{number:uT},__type__:{object:pT}},bindToWindow:{boolean:cT},autoFocus:{boolean:cT},__type__:{object:pT,boolean:cT}},multiselect:{boolean:cT},navigationButtons:{boolean:cT},selectable:{boolean:cT},selectConnectedEdges:{boolean:cT},hoverConnectedEdges:{boolean:cT},tooltipDelay:{number:uT},zoomView:{boolean:cT},zoomSpeed:{number:uT},__type__:{object:pT}},layout:{randomSeed:{undefined:"undefined",number:uT,string:dT},improvedLayout:{boolean:cT},clusterThreshold:{number:uT},hierarchical:{enabled:{boolean:cT},levelSeparation:{number:uT},nodeSpacing:{number:uT},treeSpacing:{number:uT},blockShifting:{boolean:cT},edgeMinimization:{boolean:cT},parentCentralization:{boolean:cT},direction:{string:["UD","DU","LR","RL"]},sortMethod:{string:["hubsize","directed"]},shakeTowards:{string:["leaves","roots"]},__type__:{object:pT,boolean:cT}},__type__:{object:pT}},manipulation:{enabled:{boolean:cT},initiallyActive:{boolean:cT},addNode:{boolean:cT,function:"function"},addEdge:{boolean:cT,function:"function"},editNode:{function:"function"},editEdge:{editWithoutDrag:{function:"function"},__type__:{object:pT,boolean:cT,function:"function"}},deleteNode:{boolean:cT,function:"function"},deleteEdge:{boolean:cT,function:"function"},controlNodeStyle:gT,__type__:{object:pT,boolean:cT}},nodes:gT,physics:{enabled:{boolean:cT},barnesHut:{theta:{number:uT},gravitationalConstant:{number:uT},centralGravity:{number:uT},springLength:{number:uT},springConstant:{number:uT},damping:{number:uT},avoidOverlap:{number:uT},__type__:{object:pT}},forceAtlas2Based:{theta:{number:uT},gravitationalConstant:{number:uT},centralGravity:{number:uT},springLength:{number:uT},springConstant:{number:uT},damping:{number:uT},avoidOverlap:{number:uT},__type__:{object:pT}},repulsion:{centralGravity:{number:uT},springLength:{number:uT},springConstant:{number:uT},nodeDistance:{number:uT},damping:{number:uT},__type__:{object:pT}},hierarchicalRepulsion:{centralGravity:{number:uT},springLength:{number:uT},springConstant:{number:uT},nodeDistance:{number:uT},damping:{number:uT},avoidOverlap:{number:uT},__type__:{object:pT}},maxVelocity:{number:uT},minVelocity:{number:uT},solver:{string:["barnesHut","repulsion","hierarchicalRepulsion","forceAtlas2Based"]},stabilization:{enabled:{boolean:cT},iterations:{number:uT},updateInterval:{number:uT},onlyDynamicEdges:{boolean:cT},fit:{boolean:cT},__type__:{object:pT,boolean:cT}},timestep:{number:uT},adaptiveTimestep:{boolean:cT},wind:{x:{number:uT},y:{number:uT},__type__:{object:pT}},__type__:{object:pT,boolean:cT}},autoResize:{boolean:cT},clickToUse:{boolean:cT},locale:{string:dT},locales:{__any__:{any:"any"},__type__:{object:pT}},height:{string:dT},width:{string:dT},__type__:{object:pT}},mT={nodes:{borderWidth:[1,0,10,1],borderWidthSelected:[2,0,10,1],color:{border:["color","#2B7CE9"],background:["color","#97C2FC"],highlight:{border:["color","#2B7CE9"],background:["color","#D2E5FF"]},hover:{border:["color","#2B7CE9"],background:["color","#D2E5FF"]}},opacity:[0,0,1,.1],fixed:{x:!1,y:!1},font:{color:["color","#343434"],size:[14,0,100,1],face:["arial","verdana","tahoma"],background:["color","none"],strokeWidth:[0,0,50,1],strokeColor:["color","#ffffff"]},hidden:!1,labelHighlightBold:!0,physics:!0,scaling:{min:[10,0,200,1],max:[30,0,200,1],label:{enabled:!1,min:[14,0,200,1],max:[30,0,200,1],maxVisible:[30,0,200,1],drawThreshold:[5,0,20,1]}},shadow:{enabled:!1,color:"rgba(0,0,0,0.5)",size:[10,0,20,1],x:[5,-30,30,1],y:[5,-30,30,1]},shape:["ellipse","box","circle","database","diamond","dot","square","star","text","triangle","triangleDown","hexagon"],shapeProperties:{borderDashes:!1,borderRadius:[6,0,20,1],interpolation:!0,useImageSize:!1},size:[25,0,200,1]},edges:{arrows:{to:{enabled:!1,scaleFactor:[1,0,3,.05],type:"arrow"},middle:{enabled:!1,scaleFactor:[1,0,3,.05],type:"arrow"},from:{enabled:!1,scaleFactor:[1,0,3,.05],type:"arrow"}},endPointOffset:{from:[0,-10,10,1],to:[0,-10,10,1]},arrowStrikethrough:!0,color:{color:["color","#848484"],highlight:["color","#848484"],hover:["color","#848484"],inherit:["from","to","both",!0,!1],opacity:[1,0,1,.05]},dashes:!1,font:{color:["color","#343434"],size:[14,0,100,1],face:["arial","verdana","tahoma"],background:["color","none"],strokeWidth:[2,0,50,1],strokeColor:["color","#ffffff"],align:["horizontal","top","middle","bottom"]},hidden:!1,hoverWidth:[1.5,0,5,.1],labelHighlightBold:!0,physics:!0,scaling:{min:[1,0,100,1],max:[15,0,100,1],label:{enabled:!0,min:[14,0,200,1],max:[30,0,200,1],maxVisible:[30,0,200,1],drawThreshold:[5,0,20,1]}},selectionWidth:[1.5,0,5,.1],selfReferenceSize:[20,0,200,1],selfReference:{size:[20,0,200,1],angle:[Math.PI/2,-6*Math.PI,6*Math.PI,Math.PI/8],renderBehindTheNode:!0},shadow:{enabled:!1,color:"rgba(0,0,0,0.5)",size:[10,0,20,1],x:[5,-30,30,1],y:[5,-30,30,1]},smooth:{enabled:!0,type:["dynamic","continuous","discrete","diagonalCross","straightCross","horizontal","vertical","curvedCW","curvedCCW","cubicBezier"],forceDirection:["horizontal","vertical","none"],roundness:[.5,0,1,.05]},width:[1,0,30,1]},layout:{hierarchical:{enabled:!1,levelSeparation:[150,20,500,5],nodeSpacing:[100,20,500,5],treeSpacing:[200,20,500,5],blockShifting:!0,edgeMinimization:!0,parentCentralization:!0,direction:["UD","DU","LR","RL"],sortMethod:["hubsize","directed"],shakeTowards:["leaves","roots"]}},interaction:{dragNodes:!0,dragView:!0,hideEdgesOnDrag:!1,hideEdgesOnZoom:!1,hideNodesOnDrag:!1,hover:!1,keyboard:{enabled:!1,speed:{x:[10,0,40,1],y:[10,0,40,1],zoom:[.02,0,.1,.005]},bindToWindow:!0,autoFocus:!0},multiselect:!1,navigationButtons:!1,selectable:!0,selectConnectedEdges:!0,hoverConnectedEdges:!0,tooltipDelay:[300,0,1e3,25],zoomView:!0,zoomSpeed:[1,.1,2,.1]},manipulation:{enabled:!1,initiallyActive:!1},physics:{enabled:!0,barnesHut:{theta:[.5,.1,1,.05],gravitationalConstant:[-2e3,-3e4,0,50],centralGravity:[.3,0,10,.05],springLength:[95,0,500,5],springConstant:[.04,0,1.2,.005],damping:[.09,0,1,.01],avoidOverlap:[0,0,1,.01]},forceAtlas2Based:{theta:[.5,.1,1,.05],gravitationalConstant:[-50,-500,0,1],centralGravity:[.01,0,1,.005],springLength:[95,0,500,5],springConstant:[.08,0,1.2,.005],damping:[.4,0,1,.01],avoidOverlap:[0,0,1,.01]},repulsion:{centralGravity:[.2,0,10,.05],springLength:[200,0,500,5],springConstant:[.05,0,1.2,.005],nodeDistance:[100,0,500,5],damping:[.09,0,1,.01]},hierarchicalRepulsion:{centralGravity:[.2,0,10,.05],springLength:[100,0,500,5],springConstant:[.01,0,1.2,.005],nodeDistance:[120,0,500,5],damping:[.09,0,1,.01],avoidOverlap:[0,0,1,.01]},maxVelocity:[50,0,150,1],minVelocity:[.1,.01,.5,.01],solver:["barnesHut","forceAtlas2Based","repulsion","hierarchicalRepulsion"],timestep:[.5,.01,1,.01],wind:{x:[0,-10,10,.1],y:[0,-10,10,.1]}}},bT=function(t,e,i){var n;return!(!Nf(t).call(t,"physics")||!Nf(n=mT.physics.solver).call(n,e)||i.physics.solver===e||"wind"===e)},wT=Object.freeze({__proto__:null,configuratorHideOption:bT,allOptions:yT,configureOptions:mT}),kT=function(){function t(){Yd(this,t)}return Kd(t,[{key:"getDistances",value:function(t,e,i){for(var n={},o=t.edges,r=0;r<e.length;r++){var s={};n[e[r]]=s;for(var a=0;a<e.length;a++)s[e[a]]=r==a?0:1e9}for(var h=0;h<i.length;h++){var l=o[i[h]];!0===l.connected&&void 0!==n[l.fromId]&&void 0!==n[l.toId]&&(n[l.fromId][l.toId]=1,n[l.toId][l.fromId]=1)}for(var d=e.length,c=0;c<d;c++)for(var u=e[c],f=n[u],p=0;p<d-1;p++)for(var v=e[p],g=n[v],y=p+1;y<d;y++){var m=e[y],b=n[m],w=Math.min(g[m],g[u]+f[m]);g[m]=w,b[v]=w}return n}}]),t}(),_T=function(){function t(e,i,n){Yd(this,t),this.body=e,this.springLength=i,this.springConstant=n,this.distanceSolver=new kT}return Kd(t,[{key:"setOptions",value:function(t){t&&(t.springLength&&(this.springLength=t.springLength),t.springConstant&&(this.springConstant=t.springConstant))}},{key:"solve",value:function(t,e){var i=arguments.length>2&&void 0!==arguments[2]&&arguments[2],n=this.distanceSolver.getDistances(this.body,t,e);this._createL_matrix(n),this._createK_matrix(n),this._createE_matrix();for(var o=.01,r=1,s=0,a=Math.max(1e3,Math.min(10*this.body.nodeIndices.length,6e3)),h=5,l=1e9,d=0,c=0,u=0,f=0,p=0;l>o&&s<a;){s+=1;var v=this._getHighestEnergyNode(i),g=Kc(v,4);for(d=g[0],l=g[1],c=g[2],u=g[3],f=l,p=0;f>r&&p<h;){p+=1,this._moveNode(d,c,u);var y=this._getEnergy(d),m=Kc(y,3);f=m[0],c=m[1],u=m[2]}}}},{key:"_getHighestEnergyNode",value:function(t){for(var e=this.body.nodeIndices,i=this.body.nodes,n=0,o=e[0],r=0,s=0,a=0;a<e.length;a++){var h=e[a];if(!0!==i[h].predefinedPosition||!0===i[h].isCluster&&!0===t||!0!==i[h].options.fixed.x||!0!==i[h].options.fixed.y){var l=Kc(this._getEnergy(h),3),d=l[0],c=l[1],u=l[2];n<d&&(n=d,o=h,r=c,s=u)}}return[o,n,r,s]}},{key:"_getEnergy",value:function(t){var e=Kc(this.E_sums[t],2),i=e[0],n=e[1];return[Math.sqrt(Math.pow(i,2)+Math.pow(n,2)),i,n]}},{key:"_moveNode",value:function(t,e,i){for(var n=this.body.nodeIndices,o=this.body.nodes,r=0,s=0,a=0,h=o[t].x,l=o[t].y,d=this.K_matrix[t],c=this.L_matrix[t],u=0;u<n.length;u++){var f=n[u];if(f!==t){var p=o[f].x,v=o[f].y,g=d[f],y=c[f],m=1/Math.pow(Math.pow(h-p,2)+Math.pow(l-v,2),1.5);r+=g*(1-y*Math.pow(l-v,2)*m),s+=g*(y*(h-p)*(l-v)*m),a+=g*(1-y*Math.pow(h-p,2)*m)}}var b=(e/r+i/s)/(s/r-a/s),w=-(s*b+e)/r;o[t].x+=w,o[t].y+=b,this._updateE_matrix(t)}},{key:"_createL_matrix",value:function(t){var e=this.body.nodeIndices,i=this.springLength;this.L_matrix=[];for(var n=0;n<e.length;n++){this.L_matrix[e[n]]={};for(var o=0;o<e.length;o++)this.L_matrix[e[n]][e[o]]=i*t[e[n]][e[o]]}}},{key:"_createK_matrix",value:function(t){var e=this.body.nodeIndices,i=this.springConstant;this.K_matrix=[];for(var n=0;n<e.length;n++){this.K_matrix[e[n]]={};for(var o=0;o<e.length;o++)this.K_matrix[e[n]][e[o]]=i*Math.pow(t[e[n]][e[o]],-2)}}},{key:"_createE_matrix",value:function(){var t=this.body.nodeIndices,e=this.body.nodes;this.E_matrix={},this.E_sums={};for(var i=0;i<t.length;i++)this.E_matrix[t[i]]=[];for(var n=0;n<t.length;n++){for(var o=t[n],r=e[o].x,s=e[o].y,a=0,h=0,l=n;l<t.length;l++){var d=t[l];if(d!==o){var c=e[d].x,u=e[d].y,f=1/Math.sqrt(Math.pow(r-c,2)+Math.pow(s-u,2));this.E_matrix[o][l]=[this.K_matrix[o][d]*(r-c-this.L_matrix[o][d]*(r-c)*f),this.K_matrix[o][d]*(s-u-this.L_matrix[o][d]*(s-u)*f)],this.E_matrix[d][n]=this.E_matrix[o][l],a+=this.E_matrix[o][l][0],h+=this.E_matrix[o][l][1]}}this.E_sums[o]=[a,h]}}},{key:"_updateE_matrix",value:function(t){for(var e=this.body.nodeIndices,i=this.body.nodes,n=this.E_matrix[t],o=this.K_matrix[t],r=this.L_matrix[t],s=i[t].x,a=i[t].y,h=0,l=0,d=0;d<e.length;d++){var c=e[d];if(c!==t){var u=n[d],f=u[0],p=u[1],v=i[c].x,g=i[c].y,y=1/Math.sqrt(Math.pow(s-v,2)+Math.pow(a-g,2)),m=o[c]*(s-v-r[c]*(s-v)*y),b=o[c]*(a-g-r[c]*(a-g)*y);n[d]=[m,b],h+=m,l+=b;var w=this.E_sums[c];w[0]+=m-f,w[1]+=b-p}}this.E_sums[t]=[h,l]}}]),t}();function xT(t,e,i){var n,o,r,s,a=this;if(!(this instanceof xT))throw new SyntaxError("Constructor must be called with the new operator");this.options={},this.defaultOptions={locale:"en",locales:Tb,clickToUse:!1},un(this.options,this.defaultOptions),this.body={container:t,nodes:{},nodeIndices:[],edges:{},edgeIndices:[],emitter:{on:zn(n=this.on).call(n,this),off:zn(o=this.off).call(o,this),emit:zn(r=this.emit).call(r,this),once:zn(s=this.once).call(s,this)},eventListeners:{onTap:function(){},onTouch:function(){},onDoubleTap:function(){},onHold:function(){},onDragStart:function(){},onDrag:function(){},onDragEnd:function(){},onMouseWheel:function(){},onPinch:function(){},onMouseMove:function(){},onRelease:function(){},onContext:function(){}},data:{nodes:null,edges:null},functions:{createNode:function(){},createEdge:function(){},getPointer:function(){}},modules:{},view:{scale:1,translation:{x:0,y:0}},selectionBox:{show:!1,position:{start:{x:0,y:0},end:{x:0,y:0}}}},this.bindEventListeners(),this.images=new Pb((function(){return a.body.emitter.emit("_requestRedraw")})),this.groups=new tk,this.canvas=new zC(this.body),this.selectionHandler=new HS(this.body,this.canvas),this.interactionHandler=new WC(this.body,this.canvas,this.selectionHandler),this.view=new FC(this.body,this.canvas),this.renderer=new PC(this.body,this.canvas),this.physics=new xC(this.body),this.layoutEngine=new sT(this.body),this.clustering=new SC(this.body),this.manipulation=new lT(this.body,this.canvas,this.selectionHandler,this.interactionHandler),this.nodesHandler=new gO(this.body,this.images,this.groups,this.layoutEngine),this.edgesHandler=new uC(this.body,this.images,this.groups),this.body.modules.kamadaKawai=new _T(this.body,150,.05),this.body.modules.clustering=this.clustering,this.canvas._create(),this.setOptions(i),this.setData(e)}function ET(t){for(var e in t)Object.prototype.hasOwnProperty.call(t,e)&&(t[e].redundant=t[e].used,t[e].used=[])}function OT(t){for(var e in t)if(Object.prototype.hasOwnProperty.call(t,e)&&t[e].redundant){for(var i=0;i<t[e].redundant.length;i++)t[e].redundant[i].parentNode.removeChild(t[e].redundant[i]);t[e].redundant=[]}}function CT(t,e,i){var n;return Object.prototype.hasOwnProperty.call(e,t)?e[t].redundant.length>0?(n=e[t].redundant[0],e[t].redundant.shift()):(n=document.createElementNS("http://www.w3.org/2000/svg",t),i.appendChild(n)):(n=document.createElementNS("http://www.w3.org/2000/svg",t),e[t]={used:[],redundant:[]},i.appendChild(n)),e[t].used.push(n),n}Wn(xT.prototype),xT.prototype.setOptions=function(t){var e=this;if(null===t&&(t=void 0),void 0!==t){!0===Um.validate(t,yT)&&console.error("%cErrors have been found in the supplied options object.",Vm);if(em(["locale","locales","clickToUse"],this.options,t),void 0!==t.locale&&(t.locale=function(t,e){try{var i=Kc(e.split(/[-_ /]/,2),2),n=i[0],o=i[1],r=null!=n?n.toLowerCase():null,s=null!=o?o.toUpperCase():null;if(r&&s){var a,h=r+"-"+s;if(Object.prototype.hasOwnProperty.call(t,h))return h;console.warn(su(a="Unknown variant ".concat(s," of language ")).call(a,r,"."))}if(r){var l=r;if(Object.prototype.hasOwnProperty.call(t,l))return l;console.warn("Unknown language ".concat(r))}return console.warn("Unknown locale ".concat(e,", falling back to English.")),"en"}catch(t){return console.error(t),console.warn("Unexpected error while normalizing locale ".concat(e,", falling back to English.")),"en"}}(t.locales||this.options.locales,t.locale)),t=this.layoutEngine.setOptions(t.layout,t),this.canvas.setOptions(t),this.groups.setOptions(t.groups),this.nodesHandler.setOptions(t.nodes),this.edgesHandler.setOptions(t.edges),this.physics.setOptions(t.physics),this.manipulation.setOptions(t.manipulation,t,this.options),this.interactionHandler.setOptions(t.interaction),this.renderer.setOptions(t.interaction),this.selectionHandler.setOptions(t.interaction),void 0!==t.groups&&this.body.emitter.emit("refreshNodes"),"configure"in t&&(this.configurator||(this.configurator=new Hm(this,this.body.container,mT,this.canvas.pixelRatio,bT)),this.configurator.setOptions(t.configure)),this.configurator&&!0===this.configurator.options.enabled){var i={nodes:{},edges:{},layout:{},interaction:{},manipulation:{},physics:{},global:{}};nm(i.nodes,this.nodesHandler.options),nm(i.edges,this.edgesHandler.options),nm(i.layout,this.layoutEngine.options),nm(i.interaction,this.selectionHandler.options),nm(i.interaction,this.renderer.options),nm(i.interaction,this.interactionHandler.options),nm(i.manipulation,this.manipulation.options),nm(i.physics,this.physics.options),nm(i.global,this.canvas.options),nm(i.global,this.options),this.configurator.setModuleOptions(i)}void 0!==t.clickToUse?!0===t.clickToUse?void 0===this.activator&&(this.activator=new Rm(this.canvas.frame),this.activator.on("change",(function(){e.body.emitter.emit("activate")}))):(void 0!==this.activator&&(this.activator.destroy(),delete this.activator),this.body.emitter.emit("activate")):this.body.emitter.emit("activate"),this.canvas.setSize(),this.body.emitter.emit("startSimulation")}},xT.prototype._updateVisibleIndices=function(){var t=this.body.nodes,e=this.body.edges;for(var i in this.body.nodeIndices=[],this.body.edgeIndices=[],t)Object.prototype.hasOwnProperty.call(t,i)&&(this.clustering._isClusteredNode(i)||!1!==t[i].options.hidden||this.body.nodeIndices.push(t[i].id));for(var n in e)if(Object.prototype.hasOwnProperty.call(e,n)){var o=e[n],r=t[o.fromId],s=t[o.toId],a=void 0!==r&&void 0!==s;!this.clustering._isClusteredEdge(n)&&!1===o.options.hidden&&a&&!1===r.options.hidden&&!1===s.options.hidden&&this.body.edgeIndices.push(o.id)}},xT.prototype.bindEventListeners=function(){var t=this;this.body.emitter.on("_dataChanged",(function(){t.edgesHandler._updateState(),t.body.emitter.emit("_dataUpdated")})),this.body.emitter.on("_dataUpdated",(function(){t.clustering._updateState(),t._updateVisibleIndices(),t._updateValueRange(t.body.nodes),t._updateValueRange(t.body.edges),t.body.emitter.emit("startSimulation"),t.body.emitter.emit("_requestRedraw")}))},xT.prototype.setData=function(t){if(this.body.emitter.emit("resetPhysics"),this.body.emitter.emit("_resetData"),this.selectionHandler.unselectAll(),t&&t.dot&&(t.nodes||t.edges))throw new SyntaxError('Data must contain either parameter "dot" or  parameter pair "nodes" and "edges", but not both.');if(this.setOptions(t&&t.options),t&&t.dot){console.warn("The dot property has been deprecated. Please use the static convertDot method to convert DOT into vis.network format and use the normal data format with nodes and edges. This converter is used like this: var data = vis.network.convertDot(dotString);");var e=Eb(t.dot);this.setData(e)}else if(t&&t.gephi){console.warn("The gephi property has been deprecated. Please use the static convertGephi method to convert gephi into vis.network format and use the normal data format with nodes and edges. This converter is used like this: var data = vis.network.convertGephi(gephiJson);");var i=Cb(t.gephi);this.setData(i)}else this.nodesHandler.setData(t&&t.nodes,!0),this.edgesHandler.setData(t&&t.edges,!0),this.body.emitter.emit("_dataChanged"),this.body.emitter.emit("_dataLoaded"),this.body.emitter.emit("initPhysics")},xT.prototype.destroy=function(){for(var t in this.body.emitter.emit("destroy"),this.body.emitter.off(),this.off(),delete this.groups,delete this.canvas,delete this.selectionHandler,delete this.interactionHandler,delete this.view,delete this.renderer,delete this.physics,delete this.layoutEngine,delete this.clustering,delete this.manipulation,delete this.nodesHandler,delete this.edgesHandler,delete this.configurator,delete this.images,this.body.nodes)Object.prototype.hasOwnProperty.call(this.body.nodes,t)&&delete this.body.nodes[t];for(var e in this.body.edges)Object.prototype.hasOwnProperty.call(this.body.edges,e)&&delete this.body.edges[e];Ky(this.body.container)},xT.prototype._updateValueRange=function(t){var e,i=void 0,n=void 0,o=0;for(e in t)if(Object.prototype.hasOwnProperty.call(t,e)){var r=t[e].getValue();void 0!==r&&(i=void 0===i?r:Math.min(r,i),n=void 0===n?r:Math.max(r,n),o+=r)}if(void 0!==i&&void 0!==n)for(e in t)Object.prototype.hasOwnProperty.call(t,e)&&t[e].setValueRange(i,n,o)},xT.prototype.isActive=function(){return!this.activator||this.activator.active},xT.prototype.setSize=function(){return this.canvas.setSize.apply(this.canvas,arguments)},xT.prototype.canvasToDOM=function(){return this.canvas.canvasToDOM.apply(this.canvas,arguments)},xT.prototype.DOMtoCanvas=function(){return this.canvas.DOMtoCanvas.apply(this.canvas,arguments)},xT.prototype.findNode=function(){return this.clustering.findNode.apply(this.clustering,arguments)},xT.prototype.isCluster=function(){return this.clustering.isCluster.apply(this.clustering,arguments)},xT.prototype.openCluster=function(){return this.clustering.openCluster.apply(this.clustering,arguments)},xT.prototype.cluster=function(){return this.clustering.cluster.apply(this.clustering,arguments)},xT.prototype.getNodesInCluster=function(){return this.clustering.getNodesInCluster.apply(this.clustering,arguments)},xT.prototype.clusterByConnection=function(){return this.clustering.clusterByConnection.apply(this.clustering,arguments)},xT.prototype.clusterByHubsize=function(){return this.clustering.clusterByHubsize.apply(this.clustering,arguments)},xT.prototype.updateClusteredNode=function(){return this.clustering.updateClusteredNode.apply(this.clustering,arguments)},xT.prototype.getClusteredEdges=function(){return this.clustering.getClusteredEdges.apply(this.clustering,arguments)},xT.prototype.getBaseEdge=function(){return this.clustering.getBaseEdge.apply(this.clustering,arguments)},xT.prototype.getBaseEdges=function(){return this.clustering.getBaseEdges.apply(this.clustering,arguments)},xT.prototype.updateEdge=function(){return this.clustering.updateEdge.apply(this.clustering,arguments)},xT.prototype.clusterOutliers=function(){return this.clustering.clusterOutliers.apply(this.clustering,arguments)},xT.prototype.getSeed=function(){return this.layoutEngine.getSeed.apply(this.layoutEngine,arguments)},xT.prototype.enableEditMode=function(){return this.manipulation.enableEditMode.apply(this.manipulation,arguments)},xT.prototype.disableEditMode=function(){return this.manipulation.disableEditMode.apply(this.manipulation,arguments)},xT.prototype.addNodeMode=function(){return this.manipulation.addNodeMode.apply(this.manipulation,arguments)},xT.prototype.editNode=function(){return this.manipulation.editNode.apply(this.manipulation,arguments)},xT.prototype.editNodeMode=function(){return console.warn("Deprecated: Please use editNode instead of editNodeMode."),this.manipulation.editNode.apply(this.manipulation,arguments)},xT.prototype.addEdgeMode=function(){return this.manipulation.addEdgeMode.apply(this.manipulation,arguments)},xT.prototype.editEdgeMode=function(){return this.manipulation.editEdgeMode.apply(this.manipulation,arguments)},xT.prototype.deleteSelected=function(){return this.manipulation.deleteSelected.apply(this.manipulation,arguments)},xT.prototype.getPositions=function(){return this.nodesHandler.getPositions.apply(this.nodesHandler,arguments)},xT.prototype.getPosition=function(){return this.nodesHandler.getPosition.apply(this.nodesHandler,arguments)},xT.prototype.storePositions=function(){return this.nodesHandler.storePositions.apply(this.nodesHandler,arguments)},xT.prototype.moveNode=function(){return this.nodesHandler.moveNode.apply(this.nodesHandler,arguments)},xT.prototype.getBoundingBox=function(){return this.nodesHandler.getBoundingBox.apply(this.nodesHandler,arguments)},xT.prototype.getConnectedNodes=function(t){return void 0!==this.body.nodes[t]?this.nodesHandler.getConnectedNodes.apply(this.nodesHandler,arguments):this.edgesHandler.getConnectedNodes.apply(this.edgesHandler,arguments)},xT.prototype.getConnectedEdges=function(){return this.nodesHandler.getConnectedEdges.apply(this.nodesHandler,arguments)},xT.prototype.startSimulation=function(){return this.physics.startSimulation.apply(this.physics,arguments)},xT.prototype.stopSimulation=function(){return this.physics.stopSimulation.apply(this.physics,arguments)},xT.prototype.stabilize=function(){return this.physics.stabilize.apply(this.physics,arguments)},xT.prototype.getSelection=function(){return this.selectionHandler.getSelection.apply(this.selectionHandler,arguments)},xT.prototype.setSelection=function(){return this.selectionHandler.setSelection.apply(this.selectionHandler,arguments)},xT.prototype.getSelectedNodes=function(){return this.selectionHandler.getSelectedNodeIds.apply(this.selectionHandler,arguments)},xT.prototype.getSelectedEdges=function(){return this.selectionHandler.getSelectedEdgeIds.apply(this.selectionHandler,arguments)},xT.prototype.getNodeAt=function(){var t=this.selectionHandler.getNodeAt.apply(this.selectionHandler,arguments);return void 0!==t&&void 0!==t.id?t.id:t},xT.prototype.getEdgeAt=function(){var t=this.selectionHandler.getEdgeAt.apply(this.selectionHandler,arguments);return void 0!==t&&void 0!==t.id?t.id:t},xT.prototype.selectNodes=function(){return this.selectionHandler.selectNodes.apply(this.selectionHandler,arguments)},xT.prototype.selectEdges=function(){return this.selectionHandler.selectEdges.apply(this.selectionHandler,arguments)},xT.prototype.unselectAll=function(){this.selectionHandler.unselectAll.apply(this.selectionHandler,arguments),this.selectionHandler.commitWithoutEmitting.apply(this.selectionHandler),this.redraw()},xT.prototype.redraw=function(){return this.renderer.redraw.apply(this.renderer,arguments)},xT.prototype.getScale=function(){return this.view.getScale.apply(this.view,arguments)},xT.prototype.getViewPosition=function(){return this.view.getViewPosition.apply(this.view,arguments)},xT.prototype.fit=function(){return this.view.fit.apply(this.view,arguments)},xT.prototype.moveTo=function(){return this.view.moveTo.apply(this.view,arguments)},xT.prototype.focus=function(){return this.view.focus.apply(this.view,arguments)},xT.prototype.releaseNode=function(){return this.view.releaseNode.apply(this.view,arguments)},xT.prototype.getOptionsFromConfigurator=function(){var t={};return this.configurator&&(t=this.configurator.getOptions.apply(this.configurator)),t};var ST=Object.freeze({__proto__:null,prepareElements:ET,cleanupElements:OT,resetElements:function(t){ET(t),OT(t),ET(t)},getSVGElement:CT,getDOMElement:function(t,e,i,n){var o;return Object.prototype.hasOwnProperty.call(e,t)?e[t].redundant.length>0?(o=e[t].redundant[0],e[t].redundant.shift()):(o=document.createElement(t),void 0!==n?i.insertBefore(o,n):i.appendChild(o)):(o=document.createElement(t),e[t]={used:[],redundant:[]},void 0!==n?i.insertBefore(o,n):i.appendChild(o)),e[t].used.push(o),o},drawPoint:function(t,e,i,n,o,r){var s;if("circle"==i.style?((s=CT("circle",n,o)).setAttributeNS(null,"cx",t),s.setAttributeNS(null,"cy",e),s.setAttributeNS(null,"r",.5*i.size)):((s=CT("rect",n,o)).setAttributeNS(null,"x",t-.5*i.size),s.setAttributeNS(null,"y",e-.5*i.size),s.setAttributeNS(null,"width",i.size),s.setAttributeNS(null,"height",i.size)),void 0!==i.styles&&s.setAttributeNS(null,"style",i.styles),s.setAttributeNS(null,"class",i.className+" vis-point"),r){var a=CT("text",n,o);r.xOffset&&(t+=r.xOffset),r.yOffset&&(e+=r.yOffset),r.content&&(a.textContent=r.content),r.className&&a.setAttributeNS(null,"class",r.className+" vis-label"),a.setAttributeNS(null,"x",t),a.setAttributeNS(null,"y",e)}return s},drawBar:function(t,e,i,n,o,r,s,a){if(0!=n){n<0&&(e-=n*=-1);var h=CT("rect",r,s);h.setAttributeNS(null,"x",t-.5*i),h.setAttributeNS(null,"y",e),h.setAttributeNS(null,"width",i),h.setAttributeNS(null,"height",n),h.setAttributeNS(null,"class",o),a&&h.setAttributeNS(null,"style",a)}}}),TT={Images:Pb,dotparser:Ob,gephiParser:Sb,allOptions:wT,convertDot:Eb,convertGephi:Cb},MT=Object.freeze({__proto__:null,network:TT,DOMutil:ST,util:Ym,data:Jx,Hammer:Wm,keycharm:jC,DataSet:Kx,DataView:$x,Queue:Yx,Network:xT});t.DOMutil=ST,t.DataSet=Kx,t.DataView=$x,t.Hammer=Wm,t.Network=xT,t.Queue=Yx,t.data=Jx,t.default=MT,t.keycharm=jC,t.network=TT,t.util=Ym,Object.defineProperty(t,"__esModule",{value:!0})}));
//# sourceMappingURL=vis-network.min.js.map</script>
            
        
<center>
<h1></h1>
</center>

<!-- <link rel="stylesheet" href="../node_modules/vis/dist/vis.min.css" type="text/css" />
<script type="text/javascript" src="../node_modules/vis/dist/vis.js"> </script>-->
        <link
          href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/css/bootstrap.min.css"
          rel="stylesheet"
          integrity="sha384-eOJMYsd53ii+scO/bJGFsiCZc+5NDVN2yr8+0RDqr0Ql0h+rP48ckxlpbzKgwra6"
          crossorigin="anonymous"
        />
        <script
          src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/js/bootstrap.bundle.min.js"
          integrity="sha384-JEW9xMcG8R+pH31jmWH6WWP0WintQrMb4s7ZOdauHnUtxwoG2vI5DkLtS3qm9Ekf"
          crossorigin="anonymous"
        ></script>


        <center>
          <h1></h1>
        </center>
        <style type="text/css">

             #mynetwork {
                 width: 100%;
                 height: 750px;
                 background-color: #ffffff;
                 border: 1px solid lightgray;
                 position: relative;
                 float: left;
             }

             
             #loadingBar {
                 position:absolute;
                 top:0px;
                 left:0px;
                 width: 100%;
                 height: 750px;
                 background-color:rgba(200,200,200,0.8);
                 -webkit-transition: all 0.5s ease;
                 -moz-transition: all 0.5s ease;
                 -ms-transition: all 0.5s ease;
                 -o-transition: all 0.5s ease;
                 transition: all 0.5s ease;
                 opacity:1;
             }

             #bar {
                 position:absolute;
                 top:0px;
                 left:0px;
                 width:20px;
                 height:20px;
                 margin:auto auto auto auto;
                 border-radius:11px;
                 border:2px solid rgba(30,30,30,0.05);
                 background: rgb(0, 173, 246); /* Old browsers */
                 box-shadow: 2px 0px 4px rgba(0,0,0,0.4);
             }

             #border {
                 position:absolute;
                 top:10px;
                 left:10px;
                 width:500px;
                 height:23px;
                 margin:auto auto auto auto;
                 box-shadow: 0px 0px 4px rgba(0,0,0,0.2);
                 border-radius:10px;
             }

             #text {
                 position:absolute;
                 top:8px;
                 left:530px;
                 width:30px;
                 height:50px;
                 margin:auto auto auto auto;
                 font-size:22px;
                 color: #000000;
             }

             div.outerBorder {
                 position:relative;
                 top:400px;
                 width:600px;
                 height:44px;
                 margin:auto auto auto auto;
                 border:8px solid rgba(0,0,0,0.1);
                 background: rgb(252,252,252); /* Old browsers */
                 background: -moz-linear-gradient(top,  rgba(252,252,252,1) 0%, rgba(237,237,237,1) 100%); /* FF3.6+ */
                 background: -webkit-gradient(linear, left top, left bottom, color-stop(0%,rgba(252,252,252,1)), color-stop(100%,rgba(237,237,237,1))); /* Chrome,Safari4+ */
                 background: -webkit-linear-gradient(top,  rgba(252,252,252,1) 0%,rgba(237,237,237,1) 100%); /* Chrome10+,Safari5.1+ */
                 background: -o-linear-gradient(top,  rgba(252,252,252,1) 0%,rgba(237,237,237,1) 100%); /* Opera 11.10+ */
                 background: -ms-linear-gradient(top,  rgba(252,252,252,1) 0%,rgba(237,237,237,1) 100%); /* IE10+ */
                 background: linear-gradient(to bottom,  rgba(252,252,252,1) 0%,rgba(237,237,237,1) 100%); /* W3C */
                 filter: progid:DXImageTransform.Microsoft.gradient( startColorstr='#fcfcfc', endColorstr='#ededed',GradientType=0 ); /* IE6-9 */
                 border-radius:72px;
                 box-shadow: 0px 0px 10px rgba(0,0,0,0.2);
             }
             

             

             
        </style>
    </head>


    <body>
        <div class="card" style="width: 100%">
            
            
            <div id="mynetwork" class="card-body"></div>
        </div>

        
            <div id="loadingBar">
              <div class="outerBorder">
                <div id="text">0%</div>
                <div id="border">
                  <div id="bar"></div>
                </div>
              </div>
            </div>
        
        

        <script type="text/javascript">

              // initialize global variables.
              var edges;
              var nodes;
              var allNodes;
              var allEdges;
              var nodeColors;
              var originalNodes;
              var network;
              var container;
              var options, data;
              var filter = {
                  item : '',
                  property : '',
                  value : []
              };

              

              

              // This method is responsible for drawing the graph, returns the drawn network
              function drawGraph() {
                  var container = document.getElementById('mynetwork');

                  

                  // parsing and collecting nodes and edges from the python
                  nodes = new vis.DataSet([{"color": "#97c2fc", "id": "Saurabh Singh", "label": "Saurabh Singh", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Learning a Sequential Search for Landmarks", "label": "Learning a Sequential Search for Landmarks", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "University of Illinois, Urbana-Champaign", "label": "University of Illinois, Urbana-Champaign", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "University of Indiana", "label": "University of Indiana", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "CVPR", "label": "CVPR", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "2015", "label": "2015", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Derek Hoiem", "label": "Derek Hoiem", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Rock_Completing_3D_Object_2015_CVPR_paper.pdf", "label": "Rock_Completing_3D_Object_2015_CVPR_paper.pdf", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "University of Illinois at Urbana-Champaign", "label": "University of Illinois at Urbana-Champaign", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "David Forsyth", "label": "David Forsyth", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Benenson, R., Mathias, M., Tuytelaars, T., \u0026 Van Gool, L. (2013)", "label": "Benenson, R., Mathias, M., Tuytelaars, T., \u0026 Van Gool, L. (2013)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Bayesian color constancy revisited", "label": "Bayesian color constancy revisited", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Efficient belief propagation", "label": "Efficient belief propagation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Computer Vision", "label": "Computer Vision", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "conference", "label": "conference", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "reference [3]", "label": "reference [3]", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "multi-column deep neural networks", "label": "multi-column deep neural networks", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "computer vision", "label": "computer vision", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Image Co-segmentation", "label": "Image Co-segmentation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Object Detection", "label": "Object Detection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Visual semantic search", "label": "Visual semantic search", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "person re-identi\ufb01cation", "label": "person re-identi\ufb01cation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Entropy rate superpixel segmentation", "label": "Entropy rate superpixel segmentation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "tric min-cuts paper", "label": "tric min-cuts paper", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Beyond lambert", "label": "Beyond lambert", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "research paper", "label": "research paper", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Furukawa et al.", "label": "Furukawa et al.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "constrained parametric min-cuts", "label": "constrained parametric min-cuts", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Variational layered dynamic textures", "label": "Variational layered dynamic textures", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Glaucoma", "label": "Glaucoma", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Deep Multiple Instance Learning", "label": "Deep Multiple Instance Learning", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Sequential Search", "label": "Sequential Search", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Landmarks", "label": "Landmarks", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Ollama", "label": "Ollama", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "method", "label": "method", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "landmarks", "label": "landmarks", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "appearance", "label": "appearance", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "parsing human body layouts", "label": "parsing human body layouts", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "finding landmarks in images of birds", "label": "finding landmarks in images of birds", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "sequential search", "label": "sequential search", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "spatial model", "label": "spatial model", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "strong performance", "label": "strong performance", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "model problems", "label": "model problems", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "state-of-the-art results", "label": "state-of-the-art results", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "contour detection", "label": "contour detection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "feature engineering", "label": "feature engineering", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "subpixel-level accuracy", "label": "subpixel-level accuracy", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "robustness", "label": "robustness", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "images", "label": "images", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "sparse error tensors", "label": "sparse error tensors", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "limitations", "label": "limitations", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "traditional methods", "label": "traditional methods", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "state-of-the-art methods", "label": "state-of-the-art methods", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "ground-level query images", "label": "ground-level query images", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "aerial imagery", "label": "aerial imagery", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "feature representation", "label": "feature representation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "border ownership assignment", "label": "border ownership assignment", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Structured Random Forests (SRF)", "label": "Structured Random Forests (SRF)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "border ownership structure", "label": "border ownership structure", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "shape descriptors", "label": "shape descriptors", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "spectral properties", "label": "spectral properties", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "semi-global grouping cues", "label": "semi-global grouping cues", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Berkeley Segmentation Dataset (BSDS)", "label": "Berkeley Segmentation Dataset (BSDS)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "NYU Depth V2 dataset", "label": "NYU Depth V2 dataset", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "multi-stage approaches", "label": "multi-stage approaches", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "scale invariance", "label": "scale invariance", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "four simple color features", "label": "four simple color features", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "regression trees", "label": "regression trees", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "effectiveness", "label": "effectiveness", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "high percentage of outliers", "label": "high percentage of outliers", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "16x speedup", "label": "16x speedup", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Deformable Part Model", "label": "Deformable Part Model", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "parallel computing", "label": "parallel computing", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "frame", "label": "frame", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "enhancement of regions", "label": "enhancement of regions", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "high fidelity", "label": "high fidelity", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "temporal consistency", "label": "temporal consistency", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "texture classification", "label": "texture classification", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "KTH-TIPS2 dataset", "label": "KTH-TIPS2 dataset", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "FMD dataset", "label": "FMD dataset", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "DTD dataset", "label": "DTD dataset", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "convolutional temporal feature pooling architectures", "label": "convolutional temporal feature pooling architectures", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "video", "label": "video", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "datasets", "label": "datasets", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "unseen target crowd scene", "label": "unseen target crowd scene", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "trained CNN model", "label": "trained CNN model", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "dense depth optimization", "label": "dense depth optimization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "view pairing", "label": "view pairing", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "stereo depth estimation", "label": "stereo depth estimation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "per-image paralleization", "label": "per-image paralleization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "SfM points", "label": "SfM points", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "work on depth", "label": "work on depth", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "synthesized right views", "label": "synthesized right views", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "local tangent planes", "label": "local tangent planes", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "two steps", "label": "two steps", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "high completion rate", "label": "high completion rate", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "lowest errors", "label": "lowest errors", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "experimental results", "label": "experimental results", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "geodesic-preserving term", "label": "geodesic-preserving term", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "optical flow field", "label": "optical flow field", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "piecewise parametric flow model", "label": "piecewise parametric flow model", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "homogeneous motions", "label": "homogeneous motions", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "complex motions", "label": "complex motions", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "equity constraint", "label": "equity constraint", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "top-tier performances", "label": "top-tier performances", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Optical flow benchmarks", "label": "Optical flow benchmarks", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "unsupervised video salieny detection method", "label": "unsupervised video salieny detection method", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "human activity recognition", "label": "human activity recognition", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "training of activity detection algorithms", "label": "training of activity detection algorithms", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "challenging datasets", "label": "challenging datasets", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "favorable performance", "label": "favorable performance", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "3D pose from 2D joint locations", "label": "3D pose from 2D joint locations", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "over-complete dictionary of poses", "label": "over-complete dictionary of poses", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "good generalization", "label": "good generalization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "body pose", "label": "body pose", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "3D pose", "label": "3D pose", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "over-completes dictionary of poses", "label": "over-completes dictionary of poses", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "impossible poses", "label": "impossible poses", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "recent work", "label": "recent work", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Mcgill dataset", "label": "Mcgill dataset", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "state-of-the-art performance", "label": "state-of-the-art performance", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "optimization algorithm", "label": "optimization algorithm", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "visual attention", "label": "visual attention", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "deep neural networks", "label": "deep neural networks", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "additional annotations", "label": "additional annotations", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "state-of-the-art image restoration methods", "label": "state-of-the-art image restoration methods", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "learning features", "label": "learning features", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "learning similarity metric", "label": "learning similarity metric", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "state of the art", "label": "state of the art", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "over-fitting", "label": "over-fitting", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "linear discriminant embedding", "label": "linear discriminant embedding", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "binary tests", "label": "binary tests", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "localize object", "label": "localize object", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "collection of images", "label": "collection of images", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "bounding box", "label": "bounding box", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "videos", "label": "videos", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "object", "label": "object", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "appearance models", "label": "appearance models", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "unseen objects", "label": "unseen objects", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "LFW", "label": "LFW", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "YouTube Faces", "label": "YouTube Faces", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "RGB-D action datasets", "label": "RGB-D action datasets", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "promising results", "label": "promising results", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "tracking inaccuracies", "label": "tracking inaccuracies", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "image", "label": "image", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "GIS data", "label": "GIS data", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "insight", "label": "insight", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "min-cuts", "label": "min-cuts", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "ability to separate specular reflection", "label": "ability to separate specular reflection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "saturation of underlying surface colors", "label": "saturation of underlying surface colors", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "accurate surface shape estimation", "label": "accurate surface shape estimation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "robustness to light source position errors", "label": "robustness to light source position errors", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "salient object detection", "label": "salient object detection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "object proposal applications", "label": "object proposal applications", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "better accuracy", "label": "better accuracy", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "dense projections", "label": "dense projections", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "other methods", "label": "other methods", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "accuracy", "label": "accuracy", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "computational cost", "label": "computational cost", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "depth data", "label": "depth data", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "noisy images", "label": "noisy images", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "depth channel", "label": "depth channel", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "detection of object-like regions", "label": "detection of object-like regions", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "depth-based local features", "label": "depth-based local features", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "comparable performance", "label": "comparable performance", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "single input image", "label": "single input image", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Gaussian Mixture Model", "label": "Gaussian Mixture Model", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "reflection removal", "label": "reflection removal", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "depth maps", "label": "depth maps", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "visual fidelity", "label": "visual fidelity", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "four orders of magnitude faster", "label": "four orders of magnitude faster", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "small loss in performance", "label": "small loss in performance", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "runtime", "label": "runtime", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "stereo approaches", "label": "stereo approaches", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "optical flow approaches", "label": "optical flow approaches", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "scene flow approaches", "label": "scene flow approaches", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "qualitative results", "label": "qualitative results", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "state-of-the-art stereo", "label": "state-of-the-art stereo", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "optical flow", "label": "optical flow", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "scene flow", "label": "scene flow", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "scene flow dataset", "label": "scene flow dataset", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "localization", "label": "localization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "factor of 100", "label": "factor of 100", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "global appearance models", "label": "global appearance models", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "dynamic location models", "label": "dynamic location models", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "elements within energy minimization framework", "label": "elements within energy minimization framework", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "superiority over existing algorithms", "label": "superiority over existing algorithms", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "novel method", "label": "novel method", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "superior denois-ing accuracy", "label": "superior denois-ing accuracy", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "label corruption levels", "label": "label corruption levels", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "transparent object reconstruction", "label": "transparent object reconstruction", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "images of objects", "label": "images of objects", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "landmark addition", "label": "landmark addition", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "object proposal windows", "label": "object proposal windows", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "groups", "label": "groups", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "learned function", "label": "learned function", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "expand groups", "label": "expand groups", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "landmark group", "label": "landmark group", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "scoring function", "label": "scoring function", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "data labelled with landmarks", "label": "data labelled with landmarks", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "data", "label": "data", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "kinematics of landmark groups", "label": "kinematics of landmark groups", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "landmark", "label": "landmark", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "initial landmark", "label": "initial landmark", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "compressed", "label": "compressed", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "projected", "label": "projected", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "2D, 3D and 4D", "label": "2D, 3D and 4D", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Method", "label": "Method", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "outliers", "label": "outliers", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Texture Classification", "label": "Texture Classification", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "KTH-TIPS2", "label": "KTH-TIPS2", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "FMD", "label": "FMD", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "DTD", "label": "DTD", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "State-of-the-art approaches", "label": "State-of-the-art approaches", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "unknown reflectance maps", "label": "unknown reflectance maps", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "fine detail", "label": "fine detail", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Reflectance Maps", "label": "Reflectance Maps", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Fine Detail", "label": "Fine Detail", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "noisy cases", "label": "noisy cases", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Deep Features", "label": "Deep Features", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "State-of-the-Art Performance", "label": "State-of-the-Art Performance", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "generalization power", "label": "generalization power", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "missing RGB data", "label": "missing RGB data", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "missing depth data", "label": "missing depth data", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Other methods", "label": "Other methods", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "High-Dimensional Binary Encoding", "label": "High-Dimensional Binary Encoding", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Andriluka et al. (2009)", "label": "Andriluka et al. (2009)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "People detection", "label": "People detection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "articulated pose estimation", "label": "articulated pose estimation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "graphical model", "label": "graphical model", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Barto (1998)", "label": "Barto (1998)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Reinforcement learning", "label": "Reinforcement learning", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Felzenszwalb and Huttenlocher (2005)", "label": "Felzenszwalb and Huttenlocher (2005)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Pictorial structures", "label": "Pictorial structures", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Fergus et al. (2003)", "label": "Fergus et al. (2003)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Unsupervised scale-invariant learning", "label": "Unsupervised scale-invariant learning", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Doll\u00b4ar et al. (2009)", "label": "Doll\u00b4ar et al. (2009)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Integral channel features", "label": "Integral channel features", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Eichner and Ferrari (2012)", "label": "Eichner and Ferrari (2012)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "collective human pose estimation", "label": "collective human pose estimation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Doll\u00e1r, P.", "label": "Doll\u00e1r, P.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "BMVC", "label": "BMVC", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Eichner, M.", "label": "Eichner, M.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Appearance sharing", "label": "Appearance sharing", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Fei-Fei, L.", "label": "Fei-Fei, L.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "One-shot learning", "label": "One-shot learning", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Felzenszwalb, P. F.", "label": "Felzenszwalb, P. F.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Cascade object detection", "label": "Cascade object detection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Object detection grammar", "label": "Object detection grammar", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Object detection with grammar models", "label": "Object detection with grammar models", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Object detection with discriminatively trained part-based models", "label": "Object detection with discriminatively trained part-based models", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Huttenlocher, D. P.", "label": "Huttenlocher, D. P.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Fergus, R.", "label": "Fergus, R.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Sparse object category model", "label": "Sparse object category model", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Wang, Y.", "label": "Wang, Y.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Multiple tree models", "label": "Multiple tree models", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "ECCV", "label": "ECCV", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "reference [1]", "label": "reference [1]", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "variant co-occurrence local binary pattern", "label": "variant co-occurrence local binary pattern", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "geospatial image segmentation approach", "label": "geospatial image segmentation approach", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Gedas Bertasius", "label": "Gedas Bertasius", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "DeepEdge", "label": "DeepEdge", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "University of Pennsylvania", "label": "University of Pennsylvania", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Deep Network", "label": "Deep Network", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Contour Detection", "label": "Contour Detection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Top-Down Contour Detection", "label": "Top-Down Contour Detection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Bifurcated Deep Network", "label": "Bifurcated Deep Network", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Lorenzo Torresani", "label": "Lorenzo Torresani", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Dartmouth College", "label": "Dartmouth College", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Contour detection", "label": "Contour detection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "low-level features", "label": "low-level features", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "speed of image retrieval", "label": "speed of image retrieval", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "computationally bounded sparse projections", "label": "computationally bounded sparse projections", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "orthogonality constraint", "label": "orthogonality constraint", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "template matching", "label": "template matching", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "recognition", "label": "recognition", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "detection", "label": "detection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "retrieval", "label": "retrieval", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "object recognition", "label": "object recognition", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "conv-net", "label": "conv-net", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Action Recognition", "label": "Action Recognition", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "pictorial structures", "label": "pictorial structures", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Arbel\u00e1ez et al. (2011)", "label": "Arbel\u00e1ez et al. (2011)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Contour detection and hierarchical image segmentation", "label": "Contour detection and hierarchical image segmentation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Hierarchical Image Segmentation", "label": "Hierarchical Image Segmentation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Lim et al. (2013)", "label": "Lim et al. (2013)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Sketch tokens", "label": "Sketch tokens", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Long et al. (2014)", "label": "Long et al. (2014)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Fully convolutional networks", "label": "Fully convolutional networks", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "semantic segmentation", "label": "semantic segmentation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Malik et al. (2001)", "label": "Malik et al. (2001)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Contour and texture analysis", "label": "Contour and texture analysis", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Girshick", "label": "Girshick", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Rich feature hierarchies", "label": "Rich feature hierarchies", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Microsoft Research", "label": "Microsoft Research", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Rich Feature Hierarchies", "label": "Rich Feature Hierarchies", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "feature hierarchies", "label": "feature hierarchies", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Object detection", "label": "Object detection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Semantic segmentation", "label": "Semantic segmentation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)", "label": "IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "arXiv preprint", "label": "arXiv preprint", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "object detection", "label": "object detection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Donahue", "label": "Donahue", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Darrell", "label": "Darrell", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Hariharan", "label": "Hariharan", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Hypercolumns", "label": "Hypercolumns", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "University of California, Berkeley", "label": "University of California, Berkeley", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Object Segmentation", "label": "Object Segmentation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Arbel\u00e1ez", "label": "Arbel\u00e1ez", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Universidad de los Andes", "label": "Universidad de los Andes", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Semantic Segmentation", "label": "Semantic Segmentation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Iandola", "label": "Iandola", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Densenet", "label": "Densenet", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Jia", "label": "Jia", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Caffe", "label": "Caffe", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Large-scale object classification", "label": "Large-scale object classification", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Convolutional architecture", "label": "Convolutional architecture", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "fast feature embedding", "label": "fast feature embedding", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "deep learning framework", "label": "deep learning framework", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "computer vision tasks", "label": "computer vision tasks", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "intrinsic image decomposition", "label": "intrinsic image decomposition", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Girshik", "label": "Girshik", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "part-based models", "label": "part-based models", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Shelhamer et al.", "label": "Shelhamer et al.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Ren et al.", "label": "Ren et al.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Scale-invariant contour completion", "label": "Scale-invariant contour completion", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Condition random fields", "label": "Condition random fields", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Jianbo Shi", "label": "Jianbo Shi", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Wen Wang", "label": "Wen Wang", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Discrimi nant Analysis", "label": "Discrimi nant Analysis", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Key Laboratory of Intelligent Information Processing", "label": "Key Laboratory of Intelligent Information Processing", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Institute of Computing Technology", "label": "Institute of Computing Technology", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "wen.wang@vipl.ict.ac.cn", "label": "wen.wang@vipl.ict.ac.cn", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Gaussian Distributions", "label": "Gaussian Distributions", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Face Recognition", "label": "Face Recognition", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Ruiping Wang", "label": "Ruiping Wang", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Wang_Discriminant_Analysis_on_2015_CVPR_paper", "label": "Wang_Discriminant_Analysis_on_2015_CVPR_paper", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "wangruiping@ict.ac.cn", "label": "wangruiping@ict.ac.cn", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Gaussian Mixture Models", "label": "Gaussian Mixture Models", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Riemannian Manifold", "label": "Riemannian Manifold", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Discriminant Analysis", "label": "Discriminant Analysis", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Kernel Methods", "label": "Kernel Methods", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Image Sets", "label": "Image Sets", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Pose variation", "label": "Pose variation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "PEP (Probabilistic Elastic Part) Model", "label": "PEP (Probabilistic Elastic Part) Model", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "PaSC", "label": "PaSC", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Poisson Editing", "label": "Poisson Editing", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "3D Morphable Models", "label": "3D Morphable Models", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Zhiwu Huang", "label": "Zhiwu Huang", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Wang_Discribminant_Analysis_on_2015_CVPR_paper", "label": "Wang_Discribminant_Analysis_on_2015_CVPR_paper", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "zhiwu.huang@vipl.ict.ac.cn", "label": "zhiwu.huang@vipl.ict.ac.cn", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Shiguan Shan", "label": "Shiguan Shan", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "sgshan@ict.ac.cn", "label": "sgshan@ict.ac.cn", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Xilin Chen", "label": "Xilin Chen", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "xlchen@ict.ac.cn", "label": "xlchen@ict.ac.cn", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "DARG", "label": "DARG", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "face recognition problem", "label": "face recognition problem", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "image sets", "label": "image sets", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Gaussian components", "label": "Gaussian components", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Gaussian distributions", "label": "Gaussian distributions", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Riemannian manifold", "label": "Riemannian manifold", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Kernel Discrimiant Analysis", "label": "Kernel Discrimiant Analysis", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "probabilistic kernels", "label": "probabilistic kernels", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "geometry", "label": "geometry", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "proposed method", "label": "proposed method", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "face recognition databases", "label": "face recognition databases", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "superior performance", "label": "superior performance", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "state-of-the-art algorithms", "label": "state-of-the-art algorithms", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "MultiPIE dataset", "label": "MultiPIE dataset", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "similar 3D models", "label": "similar 3D models", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "symmetries", "label": "symmetries", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "challenging", "label": "challenging", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "state-of-the-art approaches", "label": "state-of-the-art approaches", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "existing methods", "label": "existing methods", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Russian components", "label": "Russian components", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "different subjects", "label": "different subjects", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "prior probabilities", "label": "prior probabilities", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "cvpr_papers", "label": "cvpr_papers", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Aranndi et al. (2005)", "label": "Aranndi et al. (2005)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Face recognition with image sets using manifold density divergence", "label": "Face recognition with image sets using manifold density divergence", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Amar \u0026 Nagaoka (2000)", "label": "Amar \u0026 Nagaoka (2000)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Methods of Information Geometry", "label": "Methods of Information Geometry", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Information Geometry", "label": "Information Geometry", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Chan et al. (2004)", "label": "Chan et al. (2004)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Probabilistic Kernels", "label": "Probabilistic Kernels", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Information Divergence", "label": "Information Divergence", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Probabilistic KernELS", "label": "Probabilistic KernELS", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Chan, A. B.", "label": "Chan, A. B.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Modeling, clustering, and segmenting video", "label": "Modeling, clustering, and segmenting video", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Moreno, P. J.", "label": "Moreno, P. J.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Cevikalp, H.", "label": "Cevikalp, H.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Triggs, B.", "label": "Triggs, B.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Image Sets Alignment", "label": "Image Sets Alignment", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Video-based Face Recognition", "label": "Video-based Face Recognition", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Cui, Z.", "label": "Cui, Z.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Grassmann Discriminant Analysis", "label": "Grassmann Discriminant Analysis", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Subspace-based Learning", "label": "Subspace-based Learning", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Hamm, J.", "label": "Hamm, J.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Lee, D. D.", "label": "Lee, D. D.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Sparse Approximated Nearest Points", "label": "Sparse Approximated Nearest Points", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Image Set Classification", "label": "Image Set Classification", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Hu, Y.", "label": "Hu, Y.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Sparse approximated nearest points", "label": "Sparse approximated nearest points", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "IEEE International Conference on Computer Vision and Pattern Recognized (CVPR)", "label": "IEEE International Conference on Computer Vision and Pattern Recognized (CVPR)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Harandi, M. T.", "label": "Harandi, M. T.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Grasmannian kernels", "label": "Grasmannian kernels", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Jayasumana, S.", "label": "Jayasumana, S.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "IEEE Computer Society on Computer Vision and Pattern Recognition (CVPR)", "label": "IEEE Computer Society on Computer Vision and Pattern Recognition (CVPR)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Kim, M.", "label": "Kim, M.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Face tracking and recognition", "label": "Face tracking and recognition", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)", "label": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Chinese Academy of Sciences", "label": "Chinese Academy of Sciences", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "hanics", "label": "hanics", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Beijing", "label": "Beijing", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Super-resolution Person Re-identi\ufb01cation", "label": "Super-resolution Person Re-identi\ufb01cation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Xiao-Yuan Jing", "label": "Xiao-Yuan Jing", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Xiaoke Zhu", "label": "Xiaoke Zhu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Fei Wu", "label": "Fei Wu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Xinge You", "label": "Xinge You", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Qinglong Liu", "label": "Qinglong Liu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Dong Yue", "label": "Dong Yue", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Ruimin Hu", "label": "Ruimin Hu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Baowen Xu", "label": "Baowen Xu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "State Key Laboratory of Software Engineering", "label": "State Key Laboratory of Software Engineering", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Huazhong University of Science and Technology", "label": "Huazhong University of Science and Technology", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Nanjing University of Posts and Telecommunications", "label": "Nanjing University of Posts and Telecommunications", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "National Engineering Research Center for Multimedia Software", "label": "National Engineering Research Center for Multimedia Software", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "School of Computer", "label": "School of Computer", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Jing_Super-Resolution_Person_Re-Identification_2015_CVPR_paper.pdf", "label": "Jing_Super-Resolution_Person_Re-Identification_2015_CVPR_paper.pdf", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Person re-identification", "label": "Person re-identification", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "surveillance applications", "label": "surveillance applications", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "forensics applications", "label": "forensics applications", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "semi-coupled dictionaries", "label": "semi-coupled dictionaries", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "SLD2L", "label": "SLD2L", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "converting LR probe image features", "label": "converting LR probe image features", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "3D shape matching", "label": "3D shape matching", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "3D shape retrieval", "label": "3D shape retrieval", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "proposed approach", "label": "proposed approach", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "discriminant term", "label": "discriminant term", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "converted features are far from different-person HR gallery features", "label": "converted features are far from different-person HR gallery features", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "low-rank regularization", "label": "low-rank regularization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "intrinsic feature space", "label": "intrinsic feature space", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "HR images", "label": "HR images", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "LR images", "label": "LR images", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "HR gallery images", "label": "HR gallery images", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "features", "label": "features", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "discriminative for categorization", "label": "discriminative for categorization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "semantic obviousness", "label": "semantic obviousness", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "local characteristics", "label": "local characteristics", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "LR probe images", "label": "LR probe images", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "public datasets", "label": "public datasets", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "HR gallery", "label": "HR gallery", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Super-resolution person re-identification", "label": "Super-resolution person re-identification", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "research", "label": "research", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "predicting human gaze", "label": "predicting human gaze", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "early stage", "label": "early stage", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Low-rank discriminant dictionary learning", "label": "Low-rank discriminant dictionary learning", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "machine learning", "label": "machine learning", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Semi-coupled dictionaries", "label": "Semi-coupled dictionaries", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "person re-identification", "label": "person re-identification", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "match pedestrian images", "label": "match pedestrian images", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Mahalanobis distance", "label": "Mahalanobis distance", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "cross-dataset task", "label": "cross-dataset task", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Feature representation learning", "label": "Feature representation learning", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Bak et al. (2010)", "label": "Bak et al. (2010)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Bedagkar-Gala \u0026 Shah (2014)", "label": "Bedagkar-Gala \u0026 Shah (2014)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "person re-identi\ufb01cation approaches", "label": "person re-identi\ufb01cation approaches", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Liu et al. (2014)", "label": "Liu et al. (2014)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "semi-supervised coupled dictionary learning", "label": "semi-supervised coupled dictionary learning", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Liu, X.", "label": "Liu, X.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Semi-supervised coupled dictionary learning for person re-identi\ufb01cation", "label": "Semi-supervised coupled dictionary learning for person re-identi\ufb01cation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "CVPR, IEEE Conference on", "label": "CVPR, IEEE Conference on", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Ma, L.", "label": "Ma, L.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Sparse representation for face recognition based on discriminative low-rank dictionary learning", "label": "Sparse representation for face recognition based on discriminative low-rank dictionary learning", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Person re-identi\ufb01cation over camera networks using multi-task distance metric learning", "label": "Person re-identi\ufb01cation over camera networks using multi-task distance metric learning", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Person re-identi\ufb01cation over camera networks", "label": "Person re-identi\ufb01cation over camera networks", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Gray, D.", "label": "Gray, D.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Evaluating appearance models for recognition, reacquisition, and tracking", "label": "Evaluating appearance models for recognition, reacquisition, and tracking", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Viewpoint invariant pedestrian recognition with an ensemble of localized features", "label": "Viewpoint invariant pedestrian recognition with an ensemble of localized features", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Viewpoint invariant pedestrian recognition", "label": "Viewpoint invariant pedestrian recognition", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Performance Evaluation of Tracking and Surveillance, IEEE workshop on", "label": "Performance Evaluation of Tracking and Surveillance, IEEE workshop on", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Image Processing, IEEE Transactions on", "label": "Image Processing, IEEE Transactions on", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Image super-resolution via sparse representation", "label": "Image super-resolution via sparse representation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Hirzer, M.", "label": "Hirzer, M.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Person re-identi\ufb01cation by descriptive and discriminative classification", "label": "Person re-identi\ufb01cation by descriptive and discriminative classification", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Zheng, W.-S.", "label": "Zheng, W.-S.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Reidenti\ufb01cation by relative distance comparison", "label": "Reidenti\ufb01cation by relative distance comparison", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Image Analysis", "label": "Image Analysis", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Pattern Analysis and Machine Intelligence, IEEE Transactions on", "label": "Pattern Analysis and Machine Intelligence, IEEE Transactions on", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "re Engineering", "label": "re Engineering", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Wuhan University", "label": "Wuhan University", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "China", "label": "China", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "School of Electronic Information and Communications", "label": "School of Electronic Information and Communications", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Ronan Collobert", "label": "Ronan Collobert", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "From Image-level to Pixel-level Labeling", "label": "From Image-level to Pixel-level Labeling", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Facebook AI Research", "label": "Facebook AI Research", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "ronan@coltobert.com", "label": "ronan@coltobert.com", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Menlo Park", "label": "Menlo Park", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Multimedia Software", "label": "Multimedia Software", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "We", "label": "We", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "object segmentation", "label": "object segmentation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "object class information", "label": "object class information", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "object and motion segmentation", "label": "object and motion segmentation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "weakly supervised segmentation task", "label": "weakly supervised segmentation task", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Multiple Instance Learning (MIL) framework", "label": "Multiple Instance Learning (MIL) framework", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "training image", "label": "training image", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "pixel corresponding to image class label", "label": "pixel corresponding to image class label", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "segmentation task", "label": "segmentation task", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "inferring pixels belonging to class of object", "label": "inferring pixels belonging to class of object", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "model", "label": "model", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Convolutional Neural Network", "label": "Convolutional Neural Network", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "training", "label": "training", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "pixels", "label": "pixels", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "right pixels", "label": "right pixels", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "state of the art results", "label": "state of the art results", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "weakly supervised object segmentation task", "label": "weakly supervised object segmentation task", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "fully-supervised segmentation approaches", "label": "fully-supervised segmentation approaches", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "existing ZSL algorithms", "label": "existing ZSL algorithms", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "performance gains", "label": "performance gains", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "patch representations", "label": "patch representations", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Intrinsic Images in the Wild dataset", "label": "Intrinsic Images in the Wild dataset", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "low-rank", "label": "low-rank", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "global salience effects", "label": "global salience effects", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "state-of-the-art accuracy", "label": "state-of-the-art accuracy", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "loss minimization", "label": "loss minimization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "computer vision models", "label": "computer vision models", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "human input", "label": "human input", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Markov Decision Process", "label": "Markov Decision Process", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "bottom-up segmentation", "label": "bottom-up segmentation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "We Are Family", "label": "We Are Family", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "computations", "label": "computations", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Stickmen dataset", "label": "Stickmen dataset", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "performance improvements", "label": "performance improvements", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "light transport complexities", "label": "light transport complexities", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "pose estimation", "label": "pose estimation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "pixels important for image classification", "label": "pixels important for image classification", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "structured prediction subroutine", "label": "structured prediction subroutine", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "network-based model", "label": "network-based model", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "important pixels", "label": "important pixels", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "system", "label": "system", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Imaginet dataset", "label": "Imaginet dataset", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "interest regions", "label": "interest regions", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "user\u0027s passive participation", "label": "user\u0027s passive participation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "without user participation", "label": "without user participation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "calibration", "label": "calibration", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "efficient", "label": "efficient", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "noisy data", "label": "noisy data", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "missing data", "label": "missing data", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "sudden camera motions", "label": "sudden camera motions", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "no training data", "label": "no training data", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "comparably to batch methods", "label": "comparably to batch methods", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "sequential methods", "label": "sequential methods", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "elegant structure", "label": "elegant structure", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "speed", "label": "speed", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "pixel-level salience computation", "label": "pixel-level salience computation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "object proposal generation framework", "label": "object proposal generation framework", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "supervised machine learning", "label": "supervised machine learning", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "synthesize images", "label": "synthesize images", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "redirection of gaze", "label": "redirection of gaze", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "computationally efficient", "label": "computationally efficient", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "laptop", "label": "laptop", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "uncanny valley effect", "label": "uncanny valley effect", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "image to annotate", "label": "image to annotate", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "annotation constraints", "label": "annotation constraints", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "object annotations", "label": "object annotations", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "relationships", "label": "relationships", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "dependencies", "label": "dependencies", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "segmentation experiments", "label": "segmentation experiments", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Pascal VOC dataset", "label": "Pascal VOC dataset", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Model", "label": "Model", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Quasi-parametric Model", "label": "Quasi-parametric Model", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "KNN Images", "label": "KNN Images", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Semantic Regions", "label": "Semantic Regions", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Matched Regions", "label": "Matched Regions", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Result", "label": "Result", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "task", "label": "task", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Salient Object Detection", "label": "Salient Object Detection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Weakly Supervised Segmentation", "label": "Weakly Supervised Segmentation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "approach", "label": "approach", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "textured 3D non-rigid models", "label": "textured 3D non-rigid models", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "photometric information", "label": "photometric information", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "shape manifold", "label": "shape manifold", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "new discretization method", "label": "new discretization method", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "high performance", "label": "high performance", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "existing learning-based methods", "label": "existing learning-based methods", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "best results", "label": "best results", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "complex non-uniform motion blur", "label": "complex non-uniform motion blur", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "non-uniform motion blur", "label": "non-uniform motion blur", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "deblurring model", "label": "deblurring model", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "original enhancement algorithms are unknown", "label": "original enhancement algorithms are unknown", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "original enhancement algorithms are inaccessible", "label": "original enhancement algorithms are inaccessible", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "first-person point-of-view videos", "label": "first-person point-of-view videos", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Cutkosky grasp taxonomy", "label": "Cutkosky grasp taxonomy", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "consistent performance gain", "label": "consistent performance gain", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "depth estimation", "label": "depth estimation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "synthesizing intermediate views", "label": "synthesizing intermediate views", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "depth perception", "label": "depth perception", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "improvements", "label": "improvements", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "lower computational complexity", "label": "lower computational complexity", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Stanford Background dataset", "label": "Stanford Background dataset", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "SIFT Flow datasets", "label": "SIFT Flow datasets", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "semiglobal matching", "label": "semiglobal matching", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "non-flat manifolds", "label": "non-flat manifolds", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Euclidean spaces", "label": "Euclidean spaces", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "global representations", "label": "global representations", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "spatial consistency", "label": "spatial consistency", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "training data", "label": "training data", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "encoding method", "label": "encoding method", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "part-based region matching", "label": "part-based region matching", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "evaluations", "label": "evaluations", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "generating motion part candidates", "label": "generating motion part candidates", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "objective function", "label": "objective function", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "expensive annotations", "label": "expensive annotations", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "significant improvements", "label": "significant improvements", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "competitive performance", "label": "competitive performance", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "lightness differences between pixels", "label": "lightness differences between pixels", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "depth features", "label": "depth features", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "RGB visual features", "label": "RGB visual features", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "high efficiency", "label": "high efficiency", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "robust performance", "label": "robust performance", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "varying camera-to-scene arrangements", "label": "varying camera-to-scene arrangements", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "mutual failures", "label": "mutual failures", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "50 fps", "label": "50 fps", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "generalizability", "label": "generalizability", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "data fusion approach", "label": "data fusion approach", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "projections reliability", "label": "projections reliability", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "super-pixel segmentations", "label": "super-pixel segmentations", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "alignment of projections", "label": "alignment of projections", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Robotics (Sarcos) dataset", "label": "Robotics (Sarcos) dataset", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "spatial features", "label": "spatial features", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "generalization", "label": "generalization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "new structures", "label": "new structures", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "furniture design", "label": "furniture design", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "archaeology", "label": "archaeology", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "ILSVRC2014 dataset", "label": "ILSVRC2014 dataset", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "human-in-the-loop labeling approach", "label": "human-in-the-loop labeling approach", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "highly accurate bottom-up object segmentation", "label": "highly accurate bottom-up object segmentation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "set of regions", "label": "set of regions", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "ensemble of figure-ground segmentation models", "label": "ensemble of figure-ground segmentation models", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "statistical structure", "label": "statistical structure", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "parsing humans", "label": "parsing humans", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "gaze-enabled egocentric video dataset", "label": "gaze-enabled egocentric video dataset", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "visual cues", "label": "visual cues", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "textual cues", "label": "textual cues", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "quantitative", "label": "quantitative", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "theoretical properties", "label": "theoretical properties", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "synthetic data sets", "label": "synthetic data sets", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "set of cliques", "label": "set of cliques", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "real-world data sets", "label": "real-world data sets", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "salience as prior", "label": "salience as prior", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "spatial edges", "label": "spatial edges", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "temporal motion boundaries", "label": "temporal motion boundaries", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "uni\ufb01ed saliency detection framework", "label": "uni\ufb01ed saliency detection framework", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "state-of-the-art solution", "label": "state-of-the-art solution", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "2D, 3D and 4D data", "label": "2D, 3D and 4D data", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "image labels", "label": "image labels", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "light path triangulation", "label": "light path triangulation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "unknown refractive indices", "label": "unknown refractive indices", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "complex transparent objects", "label": "complex transparent objects", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "feasibility", "label": "feasibility", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "improving object detection", "label": "improving object detection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "representative parts", "label": "representative parts", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "existing NR-IQA algorithms", "label": "existing NR-IQA algorithms", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "comparable results", "label": "comparable results", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "generalization ability", "label": "generalization ability", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "applicability to object segmentation", "label": "applicability to object segmentation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "general object and motion segmentation", "label": "general object and motion segmentation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "unsupervised segmentation", "label": "unsupervised segmentation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "best task-specific approaches", "label": "best task-specific approaches", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "task-specific approaches", "label": "task-specific approaches", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Convolutional Neural Networks", "label": "Convolutional Neural Networks", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "network type", "label": "network type", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "salience", "label": "salience", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Real-time Performance", "label": "Real-time Performance", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Bounding Box Calibration", "label": "Bounding Box Calibration", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Architecture", "label": "Architecture", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Multiple Instance Learning", "label": "Multiple Instance Learning", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Image-level Training", "label": "Image-level Training", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "training method", "label": "training method", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "HMMs", "label": "HMMs", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Hidden Markov Models", "label": "Hidden Markov Models", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Arbel\u00e1ez et al. (2009)", "label": "Arbel\u00e1ez et al. (2009)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Multiscale combinatorial grouping", "label": "Multiscale combinatorial grouping", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Boyd \u0026 Vandenberghe (2004)", "label": "Boyd \u0026 Vandenberghe (2004)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Convex optimization", "label": "Convex optimization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Cambridge University Press", "label": "Cambridge University Press", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Bridle (1990)", "label": "Bridle (1990)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Probabilistic interpretation of feedforward classification network outputs", "label": "Probabilistic interpretation of feedforward classification network outputs", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Probabilistic interpretation", "label": "Probabilistic interpretation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Statistical pattern recognition", "label": "Statistical pattern recognition", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Feedforward classification network outputs", "label": "Feedforward classification network outputs", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Efficient graph-based image segmentation", "label": "Efficient graph-based image segmentation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "International Journal of Computer Vision (IJCV)", "label": "International Journal of Computer Vision (IJCV)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Simultaneous detection and segmentation", "label": "Simultaneous detection and segmentation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "European Conference on Computer Vision (ECCV)", "label": "European Conference on Computer Vision (ECCV)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Fine-grained Localization", "label": "Fine-grained Localization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Graph-based image segmentation", "label": "Graph-based image segmentation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Image segmentation", "label": "Image segmentation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "probabilistic bottom-up aggregation", "label": "probabilistic bottom-up aggregation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Hariharan et al.", "label": "Hariharan et al.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Discriminative decorrelation for clustering and classification", "label": "Discriminative decorrelation for clustering and classification", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Hypercolumns for object segmentation and fine-grained localization", "label": "Hypercolumns for object segmentation and fine-grained localization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Krizhevsky et al.", "label": "Krizhevsky et al.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "NIPS", "label": "NIPS", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "deep convolutional neural networks", "label": "deep convolutional neural networks", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Optimal teaching for limited-capacity human learners", "label": "Optimal teaching for limited-capacity human learners", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Angular quantization", "label": "Angular quantization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "machine learning research", "label": "machine learning research", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "LeCun et al.", "label": "LeCun et al.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Proceedings of the IEEE", "label": "Proceedings of the IEEE", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Maron \u0026 Lozano-P\u00e9rez", "label": "Maron \u0026 Lozano-P\u00e9rez", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Pedro O. Pinheiro", "label": "Pedro O. Pinheiro", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Idiap Research Institute", "label": "Idiap Research Institute", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Yeqing Li", "label": "Yeqing Li", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Deep Sparse Representation", "label": "Deep Sparse Representation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "University of Texas at Arlington", "label": "University of Texas at Arlington", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "image registration technique", "label": "image registration technique", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Chen Chen", "label": "Chen Chen", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Fei Yang", "label": "Fei Yang", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Junzhou Huang", "label": "Junzhou Huang", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Deep Sparse representation", "label": "Deep Sparse representation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "USA", "label": "USA", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Li_Deep_Sparse_Representation_2015_CVPR_paper.pdf", "label": "Li_Deep_Sparse_Representation_2015_CVPR_paper.pdf", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "paper", "label": "paper", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "similarity measure", "label": "similarity measure", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "limitation", "label": "limitation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "new deep architecture", "label": "new deep architecture", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Jianping Shi", "label": "Jianping Shi", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Li Xu", "label": "Li Xu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "parametrization of the trifocal tensor", "label": "parametrization of the trifocal tensor", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Weighted Heat Kernel Signature (W-HKS)", "label": "Weighted Heat Kernel Signature (W-HKS)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Human Detection", "label": "Human Detection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "combination models", "label": "combination models", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "HOG-III features", "label": "HOG-III features", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "weighted-NMS fusion algorithm", "label": "weighted-NMS fusion algorithm", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "fundamental matrix estimation problem", "label": "fundamental matrix estimation problem", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "problem of estimating and removing non-uniform motion blur", "label": "problem of estimating and removing non-uniform motion blur", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "problem", "label": "problem", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "framework", "label": "framework", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "sub-categorization model", "label": "sub-categorization model", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Edge Radiance Profiles", "label": "Edge Radiance Profiles", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Interactive Machine Teaching algorithm", "label": "Interactive Machine Teaching algorithm", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "unified co-saliency detection framework", "label": "unified co-saliency detection framework", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "disparity refinement", "label": "disparity refinement", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "impact of parameters", "label": "impact of parameters", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "depth image enhancement method", "label": "depth image enhancement method", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "pixel-level segmentation", "label": "pixel-level segmentation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Long Short Term Memory (LSTM) recurrent neural networks", "label": "Long Short Term Memory (LSTM) recurrent neural networks", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "algorithm", "label": "algorithm", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "pre-training scheme", "label": "pre-training scheme", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "power factorization", "label": "power factorization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "GPCA", "label": "GPCA", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Sparse representation", "label": "Sparse representation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "CNN architectures", "label": "CNN architectures", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "unknown sparsity", "label": "unknown sparsity", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "visual attributes", "label": "visual attributes", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "semantic class label graph", "label": "semantic class label graph", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "absorbing Markov chain process", "label": "absorbing Markov chain process", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "two principled approaches", "label": "two principled approaches", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "supplementary material", "label": "supplementary material", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "KITTI dataset", "label": "KITTI dataset", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "coding and dictionary learning", "label": "coding and dictionary learning", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "geodesic-preserving method", "label": "geodesic-preserving method", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "solution", "label": "solution", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "salience model", "label": "salience model", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "KITTI benchmark", "label": "KITTI benchmark", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "MPI Sintel benchmark", "label": "MPI Sintel benchmark", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Middlebury benchmark", "label": "Middlebury benchmark", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Suha Kwak", "label": "Suha Kwak", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Cordelia Schmid", "label": "Cordelia Schmid", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Unsupervised Object Discovery", "label": "Unsupervised Object Discovery", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "generic instance search problem", "label": "generic instance search problem", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "shape feature learning scheme", "label": "shape feature learning scheme", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "challenge", "label": "challenge", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "system\u0027s performance", "label": "system\u0027s performance", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "image restoration", "label": "image restoration", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "\u21130TV-PADMM", "label": "\u21130TV-PADMM", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "BOLD (Binary Online Learned Descriptor)", "label": "BOLD (Binary Online Learned Descriptor)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "multiple instance learning framework", "label": "multiple instance learning framework", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "absolute pose of a perspective camera", "label": "absolute pose of a perspective camera", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "structure and motion estimation", "label": "structure and motion estimation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "age invariant face recognition", "label": "age invariant face recognition", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "measure of salience", "label": "measure of salience", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "geo-semantic segmentation method", "label": "geo-semantic segmentation method", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Markov Chain approach", "label": "Markov Chain approach", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Hierarchical approaches", "label": "Hierarchical approaches", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "efficient computation", "label": "efficient computation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "salient region detection", "label": "salient region detection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "visual salience detection", "label": "visual salience detection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Sparse Kernel Multi-task Learning (SKMTL) models", "label": "Sparse Kernel Multi-task Learning (SKMTL) models", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "multiple homographies estimation", "label": "multiple homographies estimation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "photometric stereo", "label": "photometric stereo", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "mesh deformation approach", "label": "mesh deformation approach", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "deep hashing (DH) approach", "label": "deep hashing (DH) approach", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "deep neural network", "label": "deep neural network", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "object detection advancements", "label": "object detection advancements", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "crowd engineering", "label": "crowd engineering", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "problem of learning long binary codes", "label": "problem of learning long binary codes", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "lack of effective regularizer", "label": "lack of effective regularizer", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "high computational cost", "label": "high computational cost", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "sparsity encouraging regularizer", "label": "sparsity encouraging regularizer", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Chen Xianjie", "label": "Chen Xianjie", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Alan Yuilie", "label": "Alan Yuilie", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "unified approach", "label": "unified approach", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "egocentric video summarization", "label": "egocentric video summarization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "image alignment", "label": "image alignment", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "complex system", "label": "complex system", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "problem of recovering a complete 3D model", "label": "problem of recovering a complete 3D model", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "viewpoint-based shape matching", "label": "viewpoint-based shape matching", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "3D deformation", "label": "3D deformation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "3D mesh analysis", "label": "3D mesh analysis", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "3D model synthesis", "label": "3D model synthesis", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "method for selecting features", "label": "method for selecting features", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "analysis of impact", "label": "analysis of impact", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "unsupervised method", "label": "unsupervised method", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "robust regression method", "label": "robust regression method", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Hui Wu", "label": "Hui Wu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Richard Souvenir", "label": "Richard Souvenir", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "surface shape reconstruction problem", "label": "surface shape reconstruction problem", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "volumetric deformation model", "label": "volumetric deformation model", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Adaptive Region Pooling", "label": "Adaptive Region Pooling", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "effectiveness of ARP", "label": "effectiveness of ARP", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "new no-referece (NR) image quality assessment (IQA) framework", "label": "new no-referece (NR) image quality assessment (IQA) framework", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "video segmentation", "label": "video segmentation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "deep sparse representation", "label": "deep sparse representation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "gradient domain", "label": "gradient domain", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "frequency domain", "label": "frequency domain", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "4,665 KM2", "label": "4,665 KM2", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "altered gaze direction", "label": "altered gaze direction", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "artifacts", "label": "artifacts", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "spatially-varying intensity distortions", "label": "spatially-varying intensity distortions", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "ground-truth eye-gaze", "label": "ground-truth eye-gaze", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "RASL", "label": "RASL", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "efficiency", "label": "efficiency", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "sparse decomposition", "label": "sparse decomposition", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "low-rank decomposition", "label": "low-rank decomposition", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "state-of-the-art", "label": "state-of-the-art", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "comparable", "label": "comparable", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Deformable medical image registration", "label": "Deformable medical image registration", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "IEEE Transactions on Medical Imaging", "label": "IEEE Transactions on Medical Imaging", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "journal", "label": "journal", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "32", "label": "32", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "7", "label": "7", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Robust principal component analysis", "label": "Robust principal component analysis", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Journal of the ACM", "label": "Journal of the ACM", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "58", "label": "58", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "3", "label": "3", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Deep sparse representation", "label": "Deep sparse representation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Deep Learning", "label": "Deep Learning", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Traditional Deep Features", "label": "Traditional Deep Features", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "CNNs", "label": "CNNs", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Convolutional deep belief networks", "label": "Convolutional deep belief networks", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Intensity Distortions", "label": "Intensity Distortions", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "existing approaches", "label": "existing approaches", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "predefined weights", "label": "predefined weights", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "initialization", "label": "initialization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "error accumulation", "label": "error accumulation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "incomplete constraint satisfaction", "label": "incomplete constraint satisfaction", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "user interaction", "label": "user interaction", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "surface-based strategies", "label": "surface-based strategies", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Pattern Recognition", "label": "Pattern Recognition", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "35", "label": "35", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "2", "label": "2", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Automatic color constancy algorithm selection and combination", "label": "Automatic color constancy algorithm selection and combination", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "IEEE Transactions on Geoscience and Remote Sensing", "label": "IEEE Transactions on Geoscience and Remote Sensing", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "46", "label": "46", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Automatic analysis of the difference image", "label": "Automatic analysis of the difference image", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "A latent analysis of earth surface dynamic evolution", "label": "A latent analysis of earth surface dynamic evolution", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "IEEE Transactions on Geoscientific and Remote Sensing", "label": "IEEE Transactions on Geoscientific and Remote Sensing", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "5", "label": "5", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Medical image analysis", "label": "Medical image analysis", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "18", "label": "18", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "6", "label": "6", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "H", "label": "H", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Landmark matching based retinal image alignment", "label": "Landmark matching based retinal image alignment", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Maguire", "label": "Maguire", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Brainard", "label": "Brainard", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Tzimiropouulos", "label": "Tzimiropouulos", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Robust FFT-based scale-invariant image registration", "label": "Robust FFT-based scale-invariant image registration", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Argyriou", "label": "Argyriou", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Zafeiriou", "label": "Zafeiriou", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Unifying Holistic and Parts-Based Deformable Model Fitting", "label": "Unifying Holistic and Parts-Based Deformable Model Fitting", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Stathaki", "label": "Stathaki", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Viola", "label": "Viola", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Alignment by maximization of mutual information", "label": "Alignment by maximization of mutual information", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Rapid object detection", "label": "Rapid object detection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Wells III", "label": "Wells III", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Gross", "label": "Gross", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Multi-pie", "label": "Multi-pie", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Matthews", "label": "Matthews", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Cohn", "label": "Cohn", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Kanade", "label": "Kanade", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Baker", "label": "Baker", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Zitova", "label": "Zitova", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Image registration methods", "label": "Image registration methods", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Flusser", "label": "Flusser", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Tsung-Yi Lin", "label": "Tsung-Yi Lin", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Lin_Learning_Deep_Representations_2015_CVPR_paper", "label": "Lin_Learning_Deep_Representations_2015_CVPR_paper", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Cornell Tech", "label": "Cornell Tech", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Image and vision computing", "label": "Image and vision computing", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "21", "label": "21", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "977\u20131000", "label": "977\u20131000", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Yin Cui", "label": "Yin Cui", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Serge Belongie", "label": "Serge Belongie", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "James Hays", "label": "James Hays", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Brown University", "label": "Brown University", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "geo-tagged images", "label": "geo-tagged images", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "image-based geolocalization algorithms", "label": "image-based geolocalization algorithms", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "ground-level images", "label": "ground-level images", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Where-CNN", "label": "Where-CNN", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "deep learning approach", "label": "deep learning approach", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "face verification", "label": "face verification", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "dataset", "label": "dataset", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "78K aligned cross-view image pairs", "label": "78K aligned cross-view image pairs", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "108 crowd scenes", "label": "108 crowd scenes", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "nearly 200,000 head", "label": "nearly 200,000 head", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "200,000 head annotations", "label": "200,000 head annotations", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "cross-scene crowd counting methods", "label": "cross-scene crowd counting methods", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "new", "label": "new", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "pose-dependent model of joint limits", "label": "pose-dependent model of joint limits", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "research purposes", "label": "research purposes", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "annotated through crowdsourcing", "label": "annotated through crowdsourcing", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "ground truth", "label": "ground truth", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "existing annotations", "label": "existing annotations", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "new possibilities", "label": "new possibilities", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "matching views", "label": "matching views", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "close", "label": "close", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "mismatched views", "label": "mismatched views", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "far apart", "label": "far apart", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Geolocalization", "label": "Geolocalization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Aerial Imagery", "label": "Aerial Imagery", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Cross-View Matching", "label": "Cross-View Matching", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Feature Representation", "label": "Feature Representation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Novel Locations", "label": "Novel Locations", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Deep Convolutional Neural Networks", "label": "Deep Convolutional Neural Networks", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Image Classification", "label": "Image Classification", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Google Street View", "label": "Google Street View", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "World", "label": "World", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Distinctive Image Features", "label": "Distinctive Image Features", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Scale-Invariant Keypoints", "label": "Scale-Invariant Keypoints", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Deepface", "label": "Deepface", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Human-Level Performance", "label": "Human-Level Performance", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Learning a Similarity Metric", "label": "Learning a Similarity Metric", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Face Verification", "label": "Face Verification", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Chopra et al. (2005)", "label": "Chopra et al. (2005)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Lin et al. (2013)", "label": "Lin et al. (2013)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Bansal \u0026 Daniilidis (2014)", "label": "Bansal \u0026 Daniilidis (2014)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "van der Maaten \u0026 Hinton (2008)", "label": "van der Maaten \u0026 Hinton (2008)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "JMLR", "label": "JMLR", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Matching words and pictures", "label": "Matching words and pictures", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Xiao et al. (2010)", "label": "Xiao et al. (2010)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Felzenszwalb et al. (2010)", "label": "Felzenszwalb et al. (2010)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "PAMI", "label": "PAMI", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "minimization in vision", "label": "minimization in vision", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Junho Yim", "label": "Junho Yim", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Rotating Your Face Using Multi-task Deep Neural Network", "label": "Rotating Your Face Using Multi-task Deep Neural Network", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Heechul Jung", "label": "Heechul Jung", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Rotating Your face Using Multi-task Deep Neural Network", "label": "Rotating Your face Using Multi-task Deep Neural Network", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "School of Electrical Engineerin, KAIST", "label": "School of Electrical Engineerin, KAIST", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "ByungIn Yoo", "label": "ByungIn Yoo", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Samsung Advanced Institute of Technology", "label": "Samsung Advanced Institute of Technology", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Changkyu Choi", "label": "Changkyu Choi", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Dusik Park", "label": "Dusik Park", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Junmo Kim", "label": "Junmo Kim", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "School of Electrical Engineering, KAIST", "label": "School of Electrical Engineering, KAIST", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "School of Electrical Engineering", "label": "School of Electrical Engineering", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "junmo.kim@kaisten.ac.kr", "label": "junmo.kim@kaisten.ac.kr", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "KAIST", "label": "KAIST", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Unsupervised Simultaneous Orthogonal Basis Clustering Feature Selection", "label": "Unsupervised Simultaneous Orthogonal Basis Clustering Feature Selection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Department of Automation", "label": "Department of Automation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Yim_Rotating_Your_Face_2015_CVPR_paper.pdf", "label": "Yim_Rotating_Your_Face_2015_CVPR_paper.pdf", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "CVPR paper", "label": "CVPR paper", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "publication", "label": "publication", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Face recognition", "label": "Face recognition", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "viewpoint and illumination changes", "label": "viewpoint and illumination changes", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Feret methodology", "label": "Feret methodology", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "novel type of multitask learning", "label": "novel type of multitask learning", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "target-pose face image", "label": "target-pose face image", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "arbitrary pose and illumination image", "label": "arbitrary pose and illumination image", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "target pose", "label": "target pose", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "user\u2019s intention", "label": "user\u2019s intention", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "multi-task model", "label": "multi-task model", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "identity preservation", "label": "identity preservation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Controlled Pose Image (CPI)", "label": "Controlled Pose Image (CPI)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "pose-illumination- invariant feature", "label": "pose-illumination- invariant feature", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Proposed method", "label": "Proposed method", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "state-of-the-art detection performance", "label": "state-of-the-art detection performance", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "14 FPS", "label": "14 FPS", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "100 FPS", "label": "100 FPS", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Ramakrishna Vedantam", "label": "Ramakrishna Vedantam", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "CIDEr", "label": "CIDEr", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Image Description Evaluation", "label": "Image Description Evaluation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "CIDEr-D", "label": "CIDEr-D", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Automated Metrics", "label": "Automated Metrics", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "C. Lawrence Zitnick", "label": "C. Lawrence Zitnick", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Devi Parikh", "label": "Devi Parikh", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Virginia Tech", "label": "Virginia Tech", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Image Description", "label": "Image Description", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Natural Language Processing", "label": "Natural Language Processing", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Sequence Learning", "label": "Sequence Learning", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "perceptual representations", "label": "perceptual representations", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Hand-Object Interaction", "label": "Hand-Object Interaction", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Material Recognition", "label": "Material Recognition", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "IEEE", "label": "IEEE", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Convolutional Neural Networks (CNNs)", "label": "Convolutional Neural Networks (CNNs)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "PASCAL VOC 2007", "label": "PASCAL VOC 2007", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "PASCUAL VOC 2012", "label": "PASCUAL VOC 2012", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Attribute Classification", "label": "Attribute Classification", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Paradigm", "label": "Paradigm", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Human Consensus", "label": "Human Consensus", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Triplet-based Method", "label": "Triplet-based Method", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Automated Metric", "label": "Automated Metric", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Datasets", "label": "Datasets", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "PASCAL-50S", "label": "PASCAL-50S", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "ABSTRACT-50S", "label": "ABSTRACT-50S", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Metric", "label": "Metric", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Human Judgment", "label": "Human Judgment", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "nsensus", "label": "nsensus", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "metric", "label": "metric", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "human judgment", "label": "human judgment", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "existing metrics", "label": "existing metrics", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "sentences", "label": "sentences", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "various sources", "label": "various sources", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "MS COCO evaluation server", "label": "MS COCO evaluation server", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "systematic evaluation", "label": "systematic evaluation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "image description approaches", "label": "image description approaches", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "protocol", "label": "protocol", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Cem Keskin", "label": "Cem Keskin", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Pushmeet Kohli", "label": "Pushmeet Kohli", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Shahram Izadi", "label": "Shahram Izadi", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Zhenzhong Lan", "label": "Zhenzhong Lan", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Beyond Gaussian Pyramid", "label": "Beyond Gaussian Pyramid", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "School of Computer Science, Carnegie Mellon University", "label": "School of Computer Science, Carnegie Mellon University", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Alexander G. Hauptmann", "label": "Alexander G. Hauptmann", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Bhiksha Raj", "label": "Bhiksha Raj", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "bhiksha@cs.cmu.edu", "label": "bhiksha@cs.cmu.edu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Lan_Beyond_Gausian_Pyramid_2015_CVPR_supplemental", "label": "Lan_Beyond_Gausian_Pyramid_2015_CVPR_supplemental", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Beyond Gaussian Pyramid: Multi-skip Feature Stacking for Action Recognition", "label": "Beyond Gaussian Pyramid: Multi-skip Feature Stacking for Action Recognition", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "proof of theorem 1", "label": "proof of theorem 1", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "proof of theorem 2", "label": "proof of theorem 2", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Matrix Bernstein\u0027s Inequality", "label": "Matrix Bernstein\u0027s Inequality", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Feature Stacking", "label": "Feature Stacking", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Trajectory Group Selection", "label": "Trajectory Group Selection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Fisher Vector Representation", "label": "Fisher Vector Representation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Trajectories", "label": "Trajectories", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Ming Lin", "label": "Ming Lin", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Condition Number", "label": "Condition Number", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Alexander G. Hauptman", "label": "Alexander G. Hauptman", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "cli@cs.cmu.edu", "label": "cli@cs.cmu.edu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Kwang In Kim", "label": "Kwang In Kim", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Kim_Local_High-Order_Regularization_2015_CVPR_paper.pdf", "label": "Kim_Local_High-Order_Regularization_2015_CVPR_paper.pdf", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "James Tompkin", "label": "James Tompkin", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Hanspeter Pfister", "label": "Hanspeter Pfister", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Kim_Local_High-Order_RegularIZATION_2015_CVPR_paper.pdf", "label": "Kim_Local_High-Order_RegularIZATION_2015_CVPR_paper.pdf", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Local High-order Regularization on Data Manifolds", "label": "Local High-order Regularization on Data Manifolds", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Christian Theobalt", "label": "Christian Theobalt", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "MPI for Informatics", "label": "MPI for Informatics", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Max Planck Institute for Informatics", "label": "Max Planck Institute for Informatics", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Carnegie Mellon University", "label": "Carnegie Mellon University", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "School of Computer Science", "label": "School of Computer Science", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Pittsburgh", "label": "Pittsburgh", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Beijing Institute of Technology", "label": "Beijing Institute of Technology", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Graph Laplacian Regularizer", "label": "Graph Laplacian Regularizer", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "degeneracy", "label": "degeneracy", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Iterated Graph Laplacian", "label": "Iterated Graph Laplacian", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "computational complexity", "label": "computational complexity", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Proposed Regularizer", "label": "Proposed Regularizer", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "sparsity", "label": "sparsity", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "local derivative evaluations", "label": "local derivative evaluations", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Approach", "label": "Approach", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "manifold approximation", "label": "manifold approximation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Textured 3D Shape Retrieval", "label": "Textured 3D Shape Retrieval", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "SED dataset", "label": "SED dataset", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "HKU-IS dataset", "label": "HKU-IS dataset", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Superior Performance", "label": "Superior Performance", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Precision", "label": "Precision", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Recall", "label": "Recall", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "F-measure", "label": "F-measure", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Mean Absolute Error", "label": "Mean Absolute Error", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "ILSVRC2014 object detection dataset", "label": "ILSVRC2014 object detection dataset", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "image representation", "label": "image representation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Input features", "label": "Input features", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Labels", "label": "Labels", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Strong theoretical properties", "label": "Strong theoretical properties", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Performance", "label": "Performance", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "performance improvement", "label": "performance improvement", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Experiments", "label": "Experiments", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "performance", "label": "performance", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "convincing performance", "label": "convincing performance", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "effectiveness of approach", "label": "effectiveness of approach", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "RMRC dataset", "label": "RMRC dataset", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "four large image datasets", "label": "four large image datasets", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "faster convergence", "label": "faster convergence", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "large-scale datasets", "label": "large-scale datasets", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "improvement", "label": "improvement", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "speed-up of up to a factor of 100", "label": "speed-up of up to a factor of 100", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "ImageNET", "label": "ImageNET", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "GIST1M", "label": "GIST1M", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "SUN-attribute", "label": "SUN-attribute", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Manifold Approximation", "label": "Manifold Approximation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "surrogate geometry", "label": "surrogate geometry", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Laplacian eigenmaps", "label": "Laplacian eigenmaps", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Graph Laplacian Regularization", "label": "Graph Laplacian Regularization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Semi-Supervised Learning", "label": "Semi-Supervised Learning", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "dimensionality reduction", "label": "dimensionality reduction", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "High-Order Derivatives", "label": "High-Order Derivatives", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Hessian eigenmaps", "label": "Hessian eigenmaps", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "locally linear embedding technique", "label": "locally linear embedding technique", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "high-dimensional data", "label": "high-dimensional data", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Hessian eigenMaps", "label": "Hessian eigenMaps", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "locally linear embedding", "label": "locally linear embedding", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Normalized cuts", "label": "Normalized cuts", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "image segmentation", "label": "image segmentation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "lossy data compression", "label": "lossy data compression", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Reproducing Kernel Hilbert Space (RKHS)", "label": "Reproducing Kernel Hilbert Space (RKHS)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Spectral Clustering", "label": "Spectral Clustering", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Statistics and Computing", "label": "Statistics and Computing", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "clustering", "label": "clustering", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Plug-in classifiers", "label": "Plug-in classifiers", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "fast learning rates", "label": "fast learning rates", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Supervised Learning", "label": "Supervised Learning", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "MIT Press", "label": "MIT Press", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "2006", "label": "2006", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "learning binary codes", "label": "learning binary codes", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Normalized Cuts", "label": "Normalized Cuts", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Image Segmentation", "label": "Image Segmentation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "graph-based image segmentation", "label": "graph-based image segmentation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "73.1% mean class accuracy", "label": "73.1% mean class accuracy", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Real Analysis and Probability", "label": "Real Analysis and Probability", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Graphs", "label": "Graphs", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Manifolds", "label": "Manifolds", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Graph Laplacians", "label": "Graph Laplacians", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Pointwise Consistency", "label": "Pointwise Consistency", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Just Noticeable Defocus Blur Detection and Estimation", "label": "Just Noticeable Defocus Blur Detection and Estimation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "The Chinese University of Hong Kong", "label": "The Chinese University of Hong Kong", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Lenovo R\u0026T", "label": "Lenovo R\u0026T", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Jiaya Jia", "label": "Jiaya Jia", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Just Noticeable Defocus Blur Detectio and Estimation", "label": "Just Noticeable Defocus Blur Detectio and Estimation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Deep LAC", "label": "Deep LAC", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Shi_Just_Noticeable_Defocus_2015_CVPR_paper.pdf", "label": "Shi_Just_Noticeable_Defocus_2015_CVPR_paper.pdf", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "just noticeable blur", "label": "just noticeable blur", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "defocus", "label": "defocus", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "small number of pixels", "label": "small number of pixels", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "slight edge blurriness", "label": "slight edge blurriness", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "informative clues", "label": "informative clues", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "depth", "label": "depth", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "blur descriptors", "label": "blur descriptors", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "local information", "label": "local information", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "blur feature", "label": "blur feature", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "sparse representation", "label": "sparse representation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "image decomposition", "label": "image decomposition", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "sparse edge representation", "label": "sparse edge representation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "blur strength estimation", "label": "blur strength estimation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "age decomposition", "label": "age decomposition", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "feature", "label": "feature", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "generality", "label": "generality", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Bernt Schiele", "label": "Bernt Schiele", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Filtered Channel Features for Pedestrian Detection", "label": "Filtered Channel Features for Pedestrian Detection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Taking a Deeper Look at Pedestrians", "label": "Taking a Deeper Look at Pedestrians", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Zhang_Filtered_Feature_Channels_2015_CVPR_supplemental.pdf", "label": "Zhang_Filtered_Feature_Channels_2015_CVPR_supplemental.pdf", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Shanshan Zhang", "label": "Shanshan Zhang", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Zhang_Filtered_Feature_Channels_2015_CVPR_supplemental", "label": "Zhang_Filtered_Feature_Channels_2015_CVPR_supplemental", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Rodrigo Benenson", "label": "Rodrigo Benenson", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Checkerboards4x3 model", "label": "Checkerboards4x3 model", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "pedestrian detection model", "label": "pedestrian detection model", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Roerei model", "label": "Roerei model", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "weaker model", "label": "weaker model", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "filtered channels", "label": "filtered channels", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "areas of pedestrian deemed informative", "label": "areas of pedestrian deemed informative", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "extraction of discriminative information", "label": "extraction of discriminative information", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "channel U", "label": "channel U", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "face", "label": "face", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "channel L", "label": "channel L", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "body", "label": "body", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "gradient magnitude channel", "label": "gradient magnitude channel", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "filter usage distribution", "label": "filter usage distribution", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "filter bank families", "label": "filter bank families", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "filter", "label": "filter", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "feature for decision tree split nodes", "label": "feature for decision tree split nodes", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "decision tree split node feature", "label": "decision tree split node feature", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "text", "label": "text", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Benenson, R., Mathias, M., Tuytelaars, T., \u0026 Van Gools, L. (2013)", "label": "Benenson, R., Mathias, M., Tuytelaars, T., \u0026 Van Gools, L. (2013)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Roerei, et al.", "label": "Roerei, et al.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "filter usage", "label": "filter usage", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "ACF", "label": "ACF", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "decision tree split nodes", "label": "decision tree split nodes", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "filter features", "label": "filter features", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "decision tree", "label": "decision tree", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "spatial feature distribution", "label": "spatial feature distribution", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "pedestrian detection", "label": "pedestrian detection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "re-identification", "label": "re-identification", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "image analysis task", "label": "image analysis task", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "quotient Riemannian manifold", "label": "quotient Riemannian manifold", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "almost symmetric", "label": "almost symmetric", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "preferred camera", "label": "preferred camera", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "optimization techniques on manifolds", "label": "optimization techniques on manifolds", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Riemannian structure", "label": "Riemannian structure", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "notion of distance", "label": "notion of distance", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "distance between trifocal tensors", "label": "distance between trifocal tensors", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "meaningful results", "label": "meaningful results", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Structure from Motion problem", "label": "Structure from Motion problem", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "work", "label": "work", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "new formulation of the trifocal tensor", "label": "new formulation of the trifocal tensor", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "angular support", "label": "angular support", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "foundational approaches", "label": "foundational approaches", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "importance", "label": "importance", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "object appearance model", "label": "object appearance model", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "familiar objects", "label": "familiar objects", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "trifocal tensor", "label": "trifocal tensor", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "tensor", "label": "tensor", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Structure from Motion", "label": "Structure from Motion", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Trifocal Tensor", "label": "Trifocal Tensor", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Geometric Computer Vision", "label": "Geometric Computer Vision", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "distances", "label": "distances", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Camera Calibration", "label": "Camera Calibration", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Absolute Pose Estimation", "label": "Absolute Pose Estimation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Optimization Algorithms", "label": "Optimization Algorithms", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Absil, Mahony, and Sepulchre", "label": "Absil, Mahony, and Sepulchre", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Nonlinear Programming", "label": "Nonlinear Programming", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Bertsekas", "label": "Bertsekas", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Manopt", "label": "Manopt", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Boumal, Mishra, Absil, and Sepulchre", "label": "Boumal, Mishra, Absil, and Sepulchre", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Lines and points", "label": "Lines and points", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Hartley\u0027s work", "label": "Hartley\u0027s work", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Multiple View Geometry", "label": "Multiple View Geometry", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Hartley and Zisserman", "label": "Hartley and Zisserman", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Hartley, R. I.", "label": "Hartley, R. I.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "views and the trifocal tensor", "label": "views and the trifocal tensor", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Projective reconstruction from line correspondences", "label": "Projective reconstruction from line correspondences", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Multiple View Geometry in Computer Vision", "label": "Multiple View Geometry in Computer Vision", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Int. J. Comput. Vision", "label": "Int. J. Comput. Vision", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "IEEE Conf. on Computer Vision and Pattern Recognition", "label": "IEEE Conf. on Computer Vision and Pattern Recognition", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Papapdoulo, T.", "label": "Papapdoulo, T.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "A new characterization of the trifocal tensor", "label": "A new characterization of the trifocal tensor", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "European Conference on Computer Vision", "label": "European Conference on Computer Vision", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Image and Vision Computing", "label": "Image and Vision Computing", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Numerically stable optimization", "label": "Numerically stable optimization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Kendall, D. G.", "label": "Kendall, D. G.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Shape Manifolds, Procustean Metrics, and Complex Projective Spaces", "label": "Shape Manifolds, Procustean Metrics, and Complex Projective Spaces", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Bulletin of the London Mathematical Society", "label": "Bulletin of the London Mathematical Society", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Torr, P.", "label": "Torr, P.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Robust parameterization and computation of the trifocal tensor", "label": "Robust parameterization and computation of the trifocal tensor", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Robust parameterization", "label": "Robust parameterization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Weng, J.", "label": "Weng, J.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Motion and structure", "label": "Motion and structure", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Grasp Laboratory", "label": "Grasp Laboratory", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "tron@seas.upenn.edu", "label": "tron@seas.upenn.edu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Roberto Tron", "label": "Roberto Tron", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "kostas@cis.upenn.edu", "label": "kostas@cis.upenn.edu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Kostas Daniilidis", "label": "Kostas Daniilidis", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Fast 2D Border Ownership Assignment", "label": "Fast 2D Border Ownership Assignment", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Cornelia Ferm\u00fcller", "label": "Cornelia Ferm\u00fcller", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Ching L. Teo", "label": "Ching L. Teo", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Yiannis Aloimonos", "label": "Yiannis Aloimonos", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Computer Vision Lab", "label": "Computer Vision Lab", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Computer Vision\u003c0xC2\u003e\u003c0xA0\u003eLab", "label": "Computer Vision\u003c0xC2\u003e\u003c0xA0\u003eLab", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Teo_Fast_2D_Border_2015_CVPR_paper.pdf", "label": "Teo_Fast_2D_Border_2015_CVPR_paper.pdf", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "PDF document", "label": "PDF document", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "boundary detection", "label": "boundary detection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "HoG-like descriptors", "label": "HoG-like descriptors", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "photometric features", "label": "photometric features", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "PCA", "label": "PCA", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "perceived depth", "label": "perceived depth", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Experimental results", "label": "Experimental results", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Berkeley Segmentation Dataset", "label": "Berkeley Segmentation Dataset", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Method\u0027s effectiveness", "label": "Method\u0027s effectiveness", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "MoG Regression outperforms subspace clustering methods", "label": "MoG Regression outperforms subspace clustering methods", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "effectiveness of proposed methods", "label": "effectiveness of proposed methods", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Feature Extraction", "label": "Feature Extraction", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "HoG", "label": "HoG", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Cheng et al. (2014)", "label": "Cheng et al. (2014)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Bing", "label": "Bing", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Dalal \u0026 Triggs (2005)", "label": "Dalal \u0026 Triggs (2005)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Histograms of oriented gradients", "label": "Histograms of oriented gradients", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "HOG features", "label": "HOG features", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "human detection", "label": "human detection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Binarized normed gradients", "label": "Binarized normed gradients", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "objectness estimation", "label": "objectness estimation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "300fps", "label": "300fps", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Fast edge detection", "label": "Fast edge detection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "structured forests", "label": "structured forests", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Fast feature pyramids", "label": "Fast feature pyramids", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "3D scenes", "label": "3D scenes", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "grammar models", "label": "grammar models", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Category-independent object proposals", "label": "Category-independent object proposals", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "diverse ranking", "label": "diverse ranking", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Extremely randomized trees", "label": "Extremely randomized trees", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "machine learning algorithm", "label": "machine learning algorithm", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Perceptual organization", "label": "Perceptual organization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "recognition of indoor scenes", "label": "recognition of indoor scenes", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Random decision forests", "label": "Random decision forests", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "University of Maryland", "label": "University of Maryland", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "College Park, MD", "label": "College Park, MD", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Mostafa Abdelrahman", "label": "Mostafa Abdelrahman", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Heat Diffusion Over Weighted Manifolds", "label": "Heat Diffusion Over Weighted Manifolds", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Electrical Engineering Department", "label": "Electrical Engineering Department", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Assiut University", "label": "Assiut University", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "descriptor", "label": "descriptor", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Aly Farag", "label": "Aly Farag", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "CVIP Lab", "label": "CVIP Lab", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "University of Louisville", "label": "University of Louisville", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "David Swanson", "label": "David Swanson", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Department of Mathematics", "label": "Department of Mathematics", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Moumen T. El-Melegy", "label": "Moumen T. El-Melegy", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Mumford-Shah energy", "label": "Mumford-Shah energy", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "binary strings", "label": "binary strings", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "masked Hamming distance calculation", "label": "masked Hamming distance calculation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Abdelrahman_Heat_Diffusion_Over_2015_CVPR_paper.pdf", "label": "Abdelrahman_Heat_Diffusion_Over_2015_CVPR_paper.pdf", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "existing descriptors", "label": "existing descriptors", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "geometric properties", "label": "geometric properties", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "topological properties", "label": "topological properties", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "discretization method", "label": "discretization method", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "finite element approximation", "label": "finite element approximation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "weighted heat kernel signature", "label": "weighted heat kernel signature", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "geometric information", "label": "geometric information", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "method for scale invariance", "label": "method for scale invariance", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "heat kernel signature", "label": "heat kernel signature", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "challenges", "label": "challenges", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "pure geometric methods", "label": "pure geometric methods", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "pure photometric methods", "label": "pure photometric methods", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "approach\u0027s performance", "label": "approach\u0027s performance", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "image datasets", "label": "image datasets", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "video datasets", "label": "video datasets", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "textured shape retrieval", "label": "textured shape retrieval", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Challenges", "label": "Challenges", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Pure Geometric Methods", "label": "Pure Geometric Methods", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Photometric Shape Descriptors", "label": "Photometric Shape Descriptors", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Weighted Heat Kernel Signature", "label": "Weighted Heat Kernel Signature", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Heat Diffusion on Manifold", "label": "Heat Diffusion on Manifold", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Assiut", "label": "Assiut", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Si Liu", "label": "Si Liu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Matching-CNN Meets KNN", "label": "Matching-CNN Meets KNN", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "SKLOIs", "label": "SKLOIs", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Xiaodan Liang", "label": "Xiaodan Liang", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "National University of Singapore", "label": "National University of Singapore", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Luoqi Liu", "label": "Luoqi Liu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Xiaohui Shen", "label": "Xiaohui Shen", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Adobe Research", "label": "Adobe Research", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "A Convolutional Neural Network Cascade", "label": "A Convolutional Neural Network Cascade", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Salient Object Subitizing", "label": "Salient Object Subitizing", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Jianchao Yang", "label": "Jianchao Yang", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Changshen Xu", "label": "Changshen Xu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "IA", "label": "IA", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Liang Lin", "label": "Liang Lin", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Sun Yat-sen University", "label": "Sun Yat-sen University", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Xiaochun Cao", "label": "Xiaochun Cao", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Shuicheng Yan", "label": "Shuicheng Yan", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Motion Part Regularization", "label": "Motion Part Regularization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Work", "label": "Work", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Solution", "label": "Solution", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Human Parsing", "label": "Human Parsing", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "KNN Framework", "label": "KNN Framework", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "M-CNN", "label": "M-CNN", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Matching Confidence", "label": "Matching Confidence", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Displacements", "label": "Displacements", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "superpixel smoothing", "label": "superpixel smoothing", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "matched regions", "label": "matched regions", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Evaluations", "label": "Evaluations", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Performance Gains", "label": "Performance Gains", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "object characteristics", "label": "object characteristics", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "convincing", "label": "convincing", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "IIE", "label": "IIE", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "ineering", "label": "ineering", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "OIS", "label": "OIS", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Yunsheng Jiang", "label": "Yunsheng Jiang", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Combination Features and Models for Human Detection", "label": "Combination Features and Models for Human Detection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Jinwen Ma", "label": "Jinwen Ma", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Combination Features and Models for Human Description", "label": "Combination Features and Models for Human Description", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Peking University", "label": "Peking University", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Department of Information Science", "label": "Department of Information Science", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Jiang_Combination_Features_and_2015_CVPR_paper.pdf", "label": "Jiang_Combination_Features_and_2015_CVPR_paper.pdf", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "complementary features", "label": "complementary features", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "existing features", "label": "existing features", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "biases", "label": "biases", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "color features", "label": "color features", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "approaches", "label": "approaches", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "detection performance", "label": "detection performance", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "computational efficiency", "label": "computational efficiency", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "flexible", "label": "flexible", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "experiments", "label": "experiments", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "PASCAL VOC datasets", "label": "PASCAL VOC datasets", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "more accurate segmentation", "label": "more accurate segmentation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "reliability", "label": "reliability", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "superiority in speed", "label": "superiority in speed", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "competitive surface quality", "label": "competitive surface quality", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "effect of blur", "label": "effect of blur", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "geometric derivations", "label": "geometric derivations", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "PASPAL VOC 2", "label": "PASPAL VOC 2", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "PASPAL VOC 2007", "label": "PASPAL VOC 2007", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "classification", "label": "classification", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "image annotation", "label": "image annotation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Belongie et al. (2001)", "label": "Belongie et al. (2001)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "IEEE Int\u0027l Conf. on Computer Vision (IC CV)", "label": "IEEE Int\u0027l Conf. on Computer Vision (IC CV)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Hubel (1995)", "label": "Hubel (1995)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Eye, brain, and vision", "label": "Eye, brain, and vision", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Ioffe \u0026 Forsyth (2001)", "label": "Ioffe \u0026 Forsyth (2001)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Dalal (2006)", "label": "Dalal (2006)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Finding people in images and videos", "label": "Finding people in images and videos", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Felzenszwalb et al. (2008)", "label": "Felzenszwalb et al. (2008)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "deformable part models", "label": "deformable part models", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "HOG-III Features", "label": "HOG-III Features", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Model Fusion", "label": "Model Fusion", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Weighted-NMS", "label": "Weighted-NMS", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Matching Shapes", "label": "Matching Shapes", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "elzenszwalb, P. F.", "label": "elzenszwalb, P. F.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "A discriminatively trained, multiscale, deformable part model", "label": "A discriminatively trained, multiscale, deformable part model", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Girshick, R. B.", "label": "Girshick, R. B.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Viola, P.", "label": "Viola, P.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Robust real-time face detection", "label": "Robust real-time face detection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "International Journal of Computer Vision", "label": "International Journal of Computer Vision", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "IEEE Int\u2019l Conf. on Computer Vision (ICCV) Workshops", "label": "IEEE Int\u2019l Conf. on Computer Vision (ICCV) Workshops", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Advances in Neural Information Processing Systems", "label": "Advances in Neural Information Processing Systems", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "A discriminatively trained, mult scale, deformable part model", "label": "A discriminatively trained, mult scale, deformable part model", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "polynomial equation solving", "label": "polynomial equation solving", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Learning to count objects in images", "label": "Learning to count objects in images", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Gkioxari, G.", "label": "Gkioxari, G.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Using k-poselets for detecting people and localizing their keypoints", "label": "Using k-poselets for detecting people and localizing their keypoints", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Hariharan, B.", "label": "Hariharan, B.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Malik, J.", "label": "Malik, J.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "GkioxARI, G.", "label": "GkioxARI, G.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Girshick, R.", "label": "Girshick, R.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Rich feature hierarchies for accurate object detection and semantic segmentation", "label": "Rich feature hierarchies for accurate object detection and semantic segmentation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "YunshEng Jiang", "label": "YunshEng Jiang", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Cheng", "label": "Cheng", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Effective Learning-Based Illuminant Estimation Using Simple Features", "label": "Effective Learning-Based Illuminant Estimation Using Simple Features", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Price", "label": "Price", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Cohen", "label": "Cohen", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Michael S. Brown", "label": "Michael S. Brown", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Bing: Binarized normed gradients", "label": "Bing: Binarized normed gradients", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Illumination estimation", "label": "Illumination estimation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "determining chromaticity", "label": "determining chromaticity", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "white-balancing", "label": "white-balancing", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "computational color constancy", "label": "computational color constancy", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "ill-posed", "label": "ill-posed", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "nonconvex", "label": "nonconvex", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "sequence of convex semi-de\ufb01nite programs", "label": "sequence of convex semi-de\ufb01nite programs", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "temporally consistent video post-processing", "label": "temporally consistent video post-processing", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "discovery problem", "label": "discovery problem", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "inherently ill-posed", "label": "inherently ill-posed", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "3D shape motion recovery", "label": "3D shape motion recovery", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "modern color constancy data sets", "label": "modern color constancy data sets", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "results", "label": "results", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "color constancy data sets", "label": "color constancy data sets", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "CMU mocap dataset", "label": "CMU mocap dataset", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "manual annotations", "label": "manual annotations", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "our approach", "label": "our approach", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Forsyth, D. A.", "label": "Forsyth, D. A.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "novel algorithm", "label": "novel algorithm", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Variable-source shading analysis", "label": "Variable-source shading analysis", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Bani\u0107, N.", "label": "Bani\u0107, N.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Color dog", "label": "Color dog", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "global illumination estimation", "label": "global illumination estimation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Funt, B.", "label": "Funt, B.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "support vector regression", "label": "support vector regression", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "illumination chromaticity", "label": "illumination chromaticity", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Gao, S.", "label": "Gao, S.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "color constancy", "label": "color constancy", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Ef\ufb01cient color constancy with local surface re\ufb02ectance statistics", "label": "Ef\ufb01cient color constancy with local surface re\ufb02ectance statistics", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "local surface reflectance statistics", "label": "local surface reflectance statistics", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "specular reflection", "label": "specular reflection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Learning-Based Methods", "label": "Learning-Based Methods", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "slower than", "label": "slower than", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Color and Imaging Conference", "label": "Color and Imaging Conference", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "via support vector regression", "label": "via support vector regression", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Barnard, K.", "label": "Barnard, K.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "A comparison of computational color constancy algorithms", "label": "A comparison of computational color constancy algorithms", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "A data set for color research", "label": "A data set for color research", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "TIP", "label": "TIP", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Improving color constancy using indoor - outdoor image classification", "label": "Improving color constancy using indoor - outdoor image classification", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "IEEE Transactions on Image Processing", "label": "IEEE Transactions on Image Processing", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Color Research \u0026 Application", "label": "Color Research \u0026 Application", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Gehler, P. V.", "label": "Gehler, P. V.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Bianco, S.", "label": "Bianco, S.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "16", "label": "16", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "The regularized iteratively reweighted mad method", "label": "The regularized iteratively reweighted mad method", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Locally linear regression for pose-invariant face recognition", "label": "Locally linear regression for pose-invariant face recognition", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Botev, Z.", "label": "Botev, Z.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Kernel density estimation via diffusion", "label": "Kernel density estimation via diffusion", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "The Annals of Statistics", "label": "The Annals of Statistics", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Dongliang Cheng", "label": "Dongliang Cheng", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Brian Price", "label": "Brian Price", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Scott Cohen", "label": "Scott Cohen", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Robust Regression on Image Manifolds for Ordered Label Denoising", "label": "Robust Regression on Image Manifolds for Ordered Label Denoising", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "University of North Carolina at Charlotte", "label": "University of North Carolina at Charlotte", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "University of North Carolin", "label": "University of North Carolin", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Robust Regression on Image Manifold for Ordered Label Denoising", "label": "Robust Regression on Image Manifold for Ordered Label Denoising", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Wu_Robust_Regression_on_2015_CVPR_supplemental.pdf", "label": "Wu_Robust_Regression_on_2015_CVPR_supplemental.pdf", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "supplemental material", "label": "supplemental material", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Wu_Robust_Regression_on_2015_CVPR_supplemental", "label": "Wu_Robust_Regression_on_2015_CVPR_supplemental", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Robust Regression", "label": "Robust Regression", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Wu_Robust_Reservation_on_2015_CVPR_supplemental", "label": "Wu_Robust_Reservation_on_2015_CVPR_supplemental", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Ordered Label Denoising", "label": "Ordered Label Denoising", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Figures 1-4", "label": "Figures 1-4", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "RANSC", "label": "RANSC", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "K-NN", "label": "K-NN", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "RBFN", "label": "RBFN", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "SVR", "label": "SVR", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "KSPCA", "label": "KSPCA", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "H3R", "label": "H3R", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Statue Data Set", "label": "Statue Data Set", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Image Manifolds", "label": "Image Manifolds", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Face Pose Estimation", "label": "Face Pose Estimation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Y. Cheng", "label": "Y. Cheng", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "A Convex Optimization Approach", "label": "A Convex Optimization Approach", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Northeastern University", "label": "Northeastern University", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "J. A. Lopez", "label": "J. A. Lopez", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "O. Camps", "label": "O. Camps", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "M. Sznaier", "label": "M. Sznaier", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Cheng_A_Convex_Optimization_2015_CVPR_paper", "label": "Cheng_A_Convex_Optimization_2015_CVPR_paper", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "hwu13@uncc.edu", "label": "hwu13@uncc.edu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "souvenir@uncc.edu", "label": "souvenir@uncc.edu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "general nonconvex", "label": "general nonconvex", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "rank-2 constraint", "label": "rank-2 constraint", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "noise", "label": "noise", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "damage detection", "label": "damage detection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "semi-supervised learning", "label": "semi-supervised learning", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "hierarchical shape features", "label": "hierarchical shape features", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "novel insights", "label": "novel insights", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "convolutional neural network", "label": "convolutional neural network", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "visually similar neighbors", "label": "visually similar neighbors", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "deep information", "label": "deep information", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "wide information", "label": "wide information", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "multiple visual features", "label": "multiple visual features", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "relationship", "label": "relationship", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "region-keyword pairs", "label": "region-keyword pairs", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "labeled 2D samples", "label": "labeled 2D samples", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "3D CAD models", "label": "3D CAD models", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "lack of 3D training data", "label": "lack of 3D training data", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "model-based generative tracking", "label": "model-based generative tracking", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "discriminative hand pose detection", "label": "discriminative hand pose detection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "experimentation", "label": "experimentation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "extensible to other range-weighted filters", "label": "extensible to other range-weighted filters", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "range-weighted algorithms", "label": "range-weighted algorithms", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "clustered structures", "label": "clustered structures", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "solutions", "label": "solutions", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "partially-occluded small instances", "label": "partially-occluded small instances", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "heterogenous types of input data", "label": "heterogenous types of input data", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "LIVE dataset", "label": "LIVE dataset", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "extensible", "label": "extensible", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "partially labeled correspondences", "label": "partially labeled correspondences", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "co-occurrence information", "label": "co-occurrence information", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "computer", "label": "computer", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "labeled images", "label": "labeled images", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Adaptive Algorithms", "label": "Adaptive Algorithms", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "global maximization of consensus", "label": "global maximization of consensus", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "tree search problem", "label": "tree search problem", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "A* search", "label": "A* search", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "orders of magnitude faster performance", "label": "orders of magnitude faster performance", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "previous exact methods", "label": "previous exact methods", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "energy", "label": "energy", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "multi-label graph cut algorithm", "label": "multi-label graph cut algorithm", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Iterively Reweighted Least Squares (IRLS) algorithm", "label": "Iterively Reweighted Least Squares (IRLS) algorithm", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "stereo correspondence estimation", "label": "stereo correspondence estimation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "image inpainting problems", "label": "image inpainting problems", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "graph-cut-based algorithms", "label": "graph-cut-based algorithms", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "lower energy values", "label": "lower energy values", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "detection-guided optimization strategy", "label": "detection-guided optimization strategy", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "region-of-interest", "label": "region-of-interest", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "objects", "label": "objects", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "eye tracking data", "label": "eye tracking data", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "spatio-temporal mixed graph", "label": "spatio-temporal mixed graph", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "binary linear integer programming", "label": "binary linear integer programming", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "local estimation", "label": "local estimation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "global search", "label": "global search", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "strengths", "label": "strengths", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "dominant orientations", "label": "dominant orientations", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "interpretation", "label": "interpretation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "polygon", "label": "polygon", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "creation", "label": "creation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "geometrically motivated criterion", "label": "geometrically motivated criterion", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "total runtime", "label": "total runtime", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "block-coordinate Frank-Wolfe (BCFW) algorithm", "label": "block-coordinate Frank-Wolfe (BCFW) algorithm", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "submodular maximization", "label": "submodular maximization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "mis-labeled examples", "label": "mis-labeled examples", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "optimization", "label": "optimization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "robust", "label": "robust", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Multiple view geometry", "label": "Multiple view geometry", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "topic", "label": "topic", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "geometric relationships", "label": "geometric relationships", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Hartley, R. \u0026 Zisserman, A.", "label": "Hartley, R. \u0026 Zisserman, A.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Mohan, K. \u0026 Fazel, M.", "label": "Mohan, K. \u0026 Fazel, M.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Iterative reweighted algorithms", "label": "Iterative reweighted algorithms", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Lasserre, J. B.", "label": "Lasserre, J. B.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Global optimization with polynomials", "label": "Global optimization with polynomials", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "polynomial optimization methods", "label": "polynomial optimization methods", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "background clutter", "label": "background clutter", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Fundamental Matrix Estimation", "label": "Fundamental Matrix Estimation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Robust Optimization", "label": "Robust Optimization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Rank-Constrained Optimization", "label": "Rank-Constrained Optimization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Global optimization", "label": "Global optimization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "polynomial methods", "label": "polynomial methods", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "optimization methods", "label": "optimization methods", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Lasserre", "label": "Lasserre", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Global optimization with polynomials and the problem of moments", "label": "Global optimization with polynomials and the problem of moments", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Bugarin et al.", "label": "Bugarin et al.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "polynomial global optimization", "label": "polynomial global optimization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "eight-point algorithm", "label": "eight-point algorithm", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "fundamental matrix estimation", "label": "fundamental matrix estimation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "computer vision technique", "label": "computer vision technique", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "International journal of computer vision", "label": "International journal of computer vision", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Computer Vision and Pattern Recognition (CVPR)", "label": "Computer Vision and Pattern Recognition (CVPR)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Mlesac", "label": "Mlesac", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Computer Vision and Image Understanding", "label": "Computer Vision and Image Understanding", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "fundamental matrix estimation method", "label": "fundamental matrix estimation method", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Torr \u0026 Murray", "label": "Torr \u0026 Murray", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "fundamental matrix estimation methods", "label": "fundamental matrix estimation methods", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "taxonomies", "label": "taxonomies", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "comparative study", "label": "comparative study", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "robust estimator", "label": "robust estimator", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "SDP relaxations", "label": "SDP relaxations", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "SIAM Journal on Optimization", "label": "SIAM Journal on Optimization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "polynomial optimization", "label": "polynomial optimization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Zheng, Y., Sugimoto, S., \u0026 Okutomi, M.", "label": "Zheng, Y., Sugimoto, S., \u0026 Okutomi, M.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Lasserre (2006)", "label": "Lasserre (2006)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Sugaya \u0026 Kanatani (2007)", "label": "Sugaya \u0026 Kanatani (2007)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "high-accuracy computation", "label": "high-accuracy computation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Boston", "label": "Boston", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "RANSA", "label": "RANSA", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "image analysis", "label": "image analysis", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "model fitting", "label": "model fitting", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "computer vision problems", "label": "computer vision problems", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "number of inliers", "label": "number of inliers", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "University", "label": "University", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Massachusetts", "label": "Massachusetts", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Jian Sun", "label": "Jian Sun", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Learning a Convolutional Neural Network", "label": "Learning a Convolutional Neural Network", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Convolutional Neural Networks at Constrained Time Cost", "label": "Convolutional Neural Networks at Constrained Time Cost", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "A Geodesic-Preserving Method for Image Warping", "label": "A Geodesic-Preserving Method for Image Warping", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Sparse Projections for High-Dimensional Binary Codes", "label": "Sparse Projections for High-Dimensional Binary Codes", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Non-uniform Motion Blur Removal", "label": "Non-uniform Motion Blur Removal", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Wenfei Cao", "label": "Wenfei Cao", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Jean Ponce", "label": "Jean Ponce", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Unsupervised Object Detection and Localization", "label": "Unsupervised Object Detection and Localization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Unsupervised Object Discovery and Localization in the Wild", "label": "Unsupervised Object Discovery and Localization in the Wild", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "\u00c9cole Normale Sup\u00e9rieure", "label": "\u00c9cole Normale Sup\u00e9rieure", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "PSL Research University", "label": "PSL Research University", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "lopez.jo@husky.neu.edu", "label": "lopez.jo@husky.neu.edu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "camps@coe.neu.edu", "label": "camps@coe.neu.edu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "msznaier@coe.neu.edu", "label": "msznaier@coe.neu.edu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "authors", "label": "authors", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "kernels", "label": "kernels", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "general Riemannian coding framework", "label": "general Riemannian coding framework", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "energy function", "label": "energy function", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Gauss-Newton method", "label": "Gauss-Newton method", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "novel approach", "label": "novel approach", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "CNN", "label": "CNN", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "structured loss", "label": "structured loss", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "motion kernels", "label": "motion kernels", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "image rotations", "label": "image rotations", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Markov random field model", "label": "Markov random field model", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "inferring dense non-uniform motion blur field", "label": "inferring dense non-uniform motion blur field", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "motion smoothness", "label": "motion smoothness", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "motion blur", "label": "motion blur", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "patch-level image prior", "label": "patch-level image prior", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "non-uniform model", "label": "non-uniform model", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "image quality", "label": "image quality", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "image artifact", "label": "image artifact", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "patch-based image processing", "label": "patch-based image processing", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "ion blur", "label": "ion blur", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Object detection systems", "label": "Object detection systems", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "large number of classes", "label": "large number of classes", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "deep convolutional neural network (CNN)", "label": "deep convolutional neural network (CNN)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "extensive convolution operations", "label": "extensive convolution operations", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "long detection times", "label": "long detection times", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "sparse coding methods", "label": "sparse coding methods", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Regularized Sparse Coding", "label": "Regularized Sparse Coding", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "filter functionality reconstruction", "label": "filter functionality reconstruction", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "score map error", "label": "score map error", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "0.04 mAP drop", "label": "0.04 mAP drop", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "ILSVIRC 2013", "label": "ILSVIRC 2013", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "GPUs", "label": "GPUs", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Ting-Hsuan Chao", "label": "Ting-Hsuan Chao", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "National Taiwan University", "label": "National Taiwan University", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Yen-Liang Lin", "label": "Yen-Liang Lin", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Yin-Hsi Kuo", "label": "Yin-Hsi Kuo", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Winston H. Hsu", "label": "Winston H. Hsu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Xiao-Ming Wu", "label": "Xiao-Ming Wu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "author", "label": "author", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "New Insights into Laplacian Similarity Search", "label": "New Insights into Laplacian Similarity Search", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Department of Electrical Engineering, Columbia University", "label": "Department of Electrical Engineering, Columbia University", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "xmwu@ee.columbia.edu", "label": "xmwu@ee.columbia.edu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Zhenguo Li", "label": "Zhenguo Li", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Huawei Noah\u2019s Ark Lab, Hong Kong", "label": "Huawei Noah\u2019s Ark Lab, Hong Kong", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "li.zhenguo@huawei.com", "label": "li.zhenguo@huawei.com", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "New Insights into Laplacian Similarity Research", "label": "New Insights into Laplacian Similarity Research", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Shih-Fu Chang", "label": "Shih-Fu Chang", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "sfchang@ee.columbia.edu", "label": "sfchang@ee.columbia.edu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Attributes and Categories for Generic Instance Search from One Example", "label": "Attributes and Categories for Generic Instance Search from One Example", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "X.-M. Wu", "label": "X.-M. Wu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Analyzing the harmonic structure in graph-based learning", "label": "Analyzing the harmonic structure in graph-based learning", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "New insights into laplacian similarity search", "label": "New insights into laplacian similarity search", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Columbia University", "label": "Columbia University", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "harmonic structure", "label": "harmonic structure", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Xuan Dong", "label": "Xuan Dong", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Region-based Temporally Consistent Video Post-processing", "label": "Region-based Temporally Consistent Video Post-processing", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Tsinghua University", "label": "Tsinghua University", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Boyan Bonev", "label": "Boyan Bonev", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Alan L. Yuille", "label": "Alan L. Yuille", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Dong_Region-Based_Temporally_Consistent_2015_CVPR_paper.pdf", "label": "Dong_Region-Based_Temporally_Consistent_2015_CVPR_paper.pdf", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Region-based Temporically Consistent Video Post-processing", "label": "Region-based Temporically Consistent Video Post-processing", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "UC Los Angeles", "label": "UC Los Angeles", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Region-based Temporately Consistent Video Post-processing", "label": "Region-based Temporately Consistent Video Post-processing", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Yu Zhu", "label": "Yu Zhu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Northwestern Polytechnical University", "label": "Northwestern Polytechnical University", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Department of Electrical Engineering", "label": "Department of Electrical Engineering", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "goal", "label": "goal", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "fidelity", "label": "fidelity", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "semantically segmented images", "label": "semantically segmented images", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "localizing every object in an image", "label": "localizing every object in an image", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "reconstruct a 3D model automatically", "label": "reconstruct a 3D model automatically", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "enhancement algorithms", "label": "enhancement algorithms", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "spatially consistent prior", "label": "spatially consistent prior", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "pixels with same RGB values", "label": "pixels with same RGB values", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "frames", "label": "frames", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Slic superpixels", "label": "Slic superpixels", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "superpixel methods", "label": "superpixel methods", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "label": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "superpixels", "label": "superpixels", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "state-of-the-art superpixel methods", "label": "state-of-the-art superpixel methods", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "IEEE TPAM, 2012", "label": "IEEE TPAM, 2012", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "reference [4]", "label": "reference [4]", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Nonparametric Discriminant Analysis for Face Recognition", "label": "Nonparametric Discriminant Analysis for Face Recognition", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Multisculse local phase quantization for robust component-based face recognition", "label": "Multisculse local phase quantization for robust component-based face recognition", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "local phase quantization", "label": "local phase quantization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "textures", "label": "textures", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Spacetime texture representation and recognition", "label": "Spacetime texture representation and recognition", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "tone management", "label": "tone management", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "ACM Trans. on Graph.", "label": "ACM Trans. on Graph.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Tonal stabilization of video", "label": "Tonal stabilization of video", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Patch-based high dynamic range video", "label": "Patch-based high dynamic range video", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "video color grading", "label": "video color grading", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "color transformation", "label": "color transformation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Image Enhancement Algorithms", "label": "Image Enhancement Algorithms", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Y. Chang", "label": "Y. Chang", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Example-based color transformation of image and video using basic color categories", "label": "Example-based color transformation of image and video using basic color categories", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Example-based color transformation", "label": "Example-based color transformation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "S. Saito", "label": "S. Saito", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "M. Nakajima", "label": "M. Nakajima", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Example-based color transform", "label": "Example-based color transform", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "article", "label": "article", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "1\u201311", "label": "1\u201311", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "2013", "label": "2013", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Z. Farbman", "label": "Z. Farbman", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "D. Lischinski", "label": "D. Lischinski", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "P. F. Felzenszwalb", "label": "P. F. Felzenszwalb", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "D. P. Huttenlocher", "label": "D. P. Huttenlocher", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "M. Grundmann", "label": "M. Grundmann", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Post-processing approach", "label": "Post-processing approach", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Y. Hacohen", "label": "Y. Hacohen", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Non-rigid dense correspondence", "label": "Non-rigid dense correspondence", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "N. K. Kalantari", "label": "N. K. Kalantari", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "age enhancement", "label": "age enhancement", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "ACM Trans. Graph.", "label": "ACM Trans. Graph.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "N. K. Kalantria", "label": "N. K. Kalantria", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "S. B. Kang", "label": "S. B. Kang", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "High dynamic range video", "label": "High dynamic range video", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Lionel Gueguen", "label": "Lionel Gueguen", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Large-Scale Damage Detection Using Satellite Imagery", "label": "Large-Scale Damage Detection Using Satellite Imagery", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Raffay Hamid", "label": "Raffay Hamid", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "DigitalGlobe Inc.", "label": "DigitalGlobe Inc.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "mhamid@digitalGlobe.com", "label": "mhamid@digitalGlobe.com", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Satellite imagery", "label": "Satellite imagery", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "assessing damages", "label": "assessing damages", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Manual inspection", "label": "Manual inspection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "vast amount of data", "label": "vast amount of data", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "crowd counting", "label": "crowd counting", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "study", "label": "study", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "88 million images", "label": "88 million images", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "sun angle", "label": "sun angle", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "sensor resolution", "label": "sensor resolution", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "registration differences", "label": "registration differences", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "time constraints during offline training", "label": "time constraints during offline training", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "12 locations", "label": "12 locations", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "user study", "label": "user study", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "ten-fold reduction", "label": "ten-fold reduction", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "representation", "label": "representation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "human annotation time", "label": "human annotation time", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "bag-of-visual words setting", "label": "bag-of-visual words setting", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "five alternatives", "label": "five alternatives", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "time efficiency", "label": "time efficiency", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "annotation process", "label": "annotation process", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "core challenge", "label": "core challenge", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "rich representation", "label": "rich representation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "detection accuracy", "label": "detection accuracy", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "manual inspection", "label": "manual inspection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "minimal loss", "label": "minimal loss", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "User study", "label": "User study", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "ten-fold reduction in annotation time", "label": "ten-fold reduction in annotation time", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "minimal loss in detection accuracy", "label": "minimal loss in detection accuracy", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Damage detection", "label": "Damage detection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Hierarchical shape features", "label": "Hierarchical shape features", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Semi-supervised learning", "label": "Semi-supervised learning", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Novelty detection", "label": "Novelty detection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "gigantic image collections", "label": "gigantic image collections", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Xia et al. (2010)", "label": "Xia et al. (2010)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Shape-based invariant texture indexing", "label": "Shape-based invariant texture indexing", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Markou \u0026 Singh (2003)", "label": "Markou \u0026 Singh (2003)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Blanchard et al. (2010)", "label": "Blanchard et al. (2010)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Semi-supervised novelty detection", "label": "Semi-supervised novelty detection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Bruzone \u0026 Prieto (2000)", "label": "Bruzone \u0026 Prieto (2000)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Difference image", "label": "Difference image", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Unsupervised change detection", "label": "Unsupervised change detection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Satellite imagery analysis", "label": "Satellite imagery analysis", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Bruzzone", "label": "Bruzzone", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Prieto", "label": "Prieto", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Gerard", "label": "Gerard", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "A quasi-linear algorithm", "label": "A quasi-linear algorithm", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "International Symposium on Mathematical Morphology", "label": "International Symposium on Mathematical Morphology", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Monasse", "label": "Monasse", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Fast computation of a contrast-invariant image representation", "label": "Fast computation of a contrast-invariant image representation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Guichard", "label": "Guichard", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Nielsen", "label": "Nielsen", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Vaduva", "label": "Vaduva", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "at", "label": "at", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "A latent analysis of earth surface dynamic evolution using change map time series", "label": "A latent analysis of earth surface dynamic evolution using change map time series", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Lazarescu", "label": "Lazarescu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Datcu", "label": "Datcu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "A latent anisotropy of earth surface dynamic evolution using change map time series", "label": "A latent anisotropy of earth surface dynamic evolution using change map time series", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Gomez-Chova", "label": "Gomez-Chova", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Kernel-based framework for multitemporal and multisource remote sensing data classification and change detection", "label": "Kernel-based framework for multitemporal and multisource remote sensing data classification and change detection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Wang", "label": "Wang", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Locality-constrained linear coding for image classification", "label": "Locality-constrained linear coding for image classification", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Semi-supervised hashing", "label": "Semi-supervised hashing", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Weakly Supervised Localization", "label": "Weakly Supervised Localization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "LIBLINEAR", "label": "LIBLINEAR", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Yang", "label": "Yang", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "A survey on transfer learning", "label": "A survey on transfer learning", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Song", "label": "Song", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Fusing Subcategory Probabilities for Texture Classi\ufb01cation", "label": "Fusing Subcategory Probabilities for Texture Classi\ufb01cation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Cai", "label": "Cai", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Gueguen", "label": "Gueguen", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Hamid", "label": "Hamid", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Yang Song", "label": "Yang Song", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Fusing Subcategory Probabilities for Texture Classification", "label": "Fusing Subcategory Probabilities for Texture Classification", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "BMIT Research Group", "label": "BMIT Research Group", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers", "label": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Song_Fusing_Subcategory_Probabilities_2015_CVPR_paper.pdf", "label": "Song_Fusing_Subcategory_Probabilities_2015_CVPR_paper.pdf", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Qing Li", "label": "Qing Li", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Fusing SubCategory Probabilities for Texture Classification", "label": "Fusing SubCategory Probabilities for Texture Classification", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Fan Zhang", "label": "Fan Zhang", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "David Dagan Feng", "label": "David Dagan Feng", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Heng Huang", "label": "Heng Huang", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Department of Computer Science and Engineering", "label": "Department of Computer Science and Engineering", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Texture classification", "label": "Texture classification", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "high intra-class variation", "label": "high intra-class variation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "class", "label": "class", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "subcategories", "label": "subcategories", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "distinctiveness", "label": "distinctiveness", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "representativeness", "label": "representativeness", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "subcategory probabilities", "label": "subcategory probabilities", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "contribution levels", "label": "contribution levels", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "cluster qualities", "label": "cluster qualities", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "fused probability", "label": "fused probability", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "multiclass classification probability", "label": "multiclass classification probability", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "University of Sydney", "label": "University of Sydney", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "School of IT", "label": "School of IT", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Australia", "label": "Australia", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "University of Texas, Arlington", "label": "University of Texas, Arlington", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Manohar Paluri", "label": "Manohar Paluri", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Beyond Frontal Faces", "label": "Beyond Frontal Faces", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Zhang_Beyond_Frontal_Faces_2015_CVPR_paper", "label": "Zhang_Beyond_Frontal_Faces_2015_CVPR_paper", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Yaniv Taigman", "label": "Yaniv Taigman", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Rob Fergus", "label": "Rob Fergus", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Lubomir Bourdev", "label": "Lubomir Bourdev", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "recognizing peoples\u2019 identities", "label": "recognizing peoples\u2019 identities", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "PIPA dataset", "label": "PIPA dataset", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "60000 instances", "label": "60000 instances", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "2000 individuals", "label": "2000 individuals", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "PIPER method", "label": "PIPER method", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "face recognizer", "label": "face recognizer", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "global recognizer", "label": "global recognizer", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "person images", "label": "person images", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "frontal face", "label": "frontal face", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "person recognizers", "label": "person recognizers", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "deep convolutional networks", "label": "deep convolutional networks", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "discount pose variations", "label": "discount pose variations", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "PIPER", "label": "PIPER", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "DeepFace", "label": "DeepFace", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "unconstrained setup", "label": "unconstrained setup", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "best face recognizers", "label": "best face recognizers", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "LFW dataset", "label": "LFW dataset", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Pose Invariant Recognition", "label": "Pose Invariant Recognition", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Paul Wohlhart", "label": "Paul Wohlhart", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Institute for Computer Vision and Graphics, Graz University of Technology, Austria", "label": "Institute for Computer Vision and Graphics, Graz University of Technology, Austria", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Learning Descriptors for Object Recognition and 3D Pose Estimation", "label": "Learning Descriptors for Object Recognition and 3D Pose Estimation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Institute for Computer Vision and Graphics", "label": "Institute for Computer Vision and Graphics", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "{wohlhart}@icg.tugraz.at", "label": "{wohlhart}@icg.tugraz.at", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "2015 CVPR paper", "label": "2015 CVPR paper", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Computer Graphics", "label": "Computer Graphics", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Austria", "label": "Austria", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle", "label": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/", "label": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Graz University of Technology", "label": "Graz University of Technology", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Institute for Computer Graphics and Vision", "label": "Institute for Computer Graphics and Vision", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Learning Descriptors", "label": "Learning Descriptors", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "A shape-preserving approach to image resizing", "label": "A shape-preserving approach to image resizing", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Vincent LePetit", "label": "Vincent LePetit", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Joe Yue-Hei Ng", "label": "Joe Yue-Hei Ng", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Beyond Short Snippets", "label": "Beyond Short Snippets", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "University of Maryland, College Park", "label": "University of Maryland, College Park", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Matthew Hausknecht", "label": "Matthew Hausknecht", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "University of Texas at Austin", "label": "University of Texas at Austin", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Sudheendra Vijayanarasimhan", "label": "Sudheendra Vijayanarasimhan", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Google, Inc.", "label": "Google, Inc.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Oriol Vinyals", "label": "Oriol Vinyals", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Rajat Monga", "label": "Rajat Monga", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Beyond Short Snppets", "label": "Beyond Short Snppets", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "George Toderici", "label": "George Toderici", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Convolutional neural networks (CNNs)", "label": "Convolutional neural networks (CNNs)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "image recognition problems", "label": "image recognition problems", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "deep neural network architectures", "label": "deep neural network architectures", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "image information", "label": "image information", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "longer time periods", "label": "longer time periods", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "ordered sequence of frames", "label": "ordered sequence of frames", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "novel object", "label": "novel object", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "recurrent neural network", "label": "recurrent neural network", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Long Short-Term Memory (LSTM) cells", "label": "Long Short-Term Memory (LSTM) cells", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "LSTM cells", "label": "LSTM cells", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "output of CNN", "label": "output of CNN", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "networks", "label": "networks", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "UCF-101 datasets", "label": "UCF-101 datasets", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "local contextual information", "label": "local contextual information", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "global contextual information", "label": "global contextual information", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "RGB values", "label": "RGB values", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "alternative algorithms", "label": "alternative algorithms", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "88.6%", "label": "88.6%", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "88.0%", "label": "88.0%", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "optical flow information", "label": "optical flow information", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "73.0%", "label": "73.0%", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "82.6%", "label": "82.6%", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Sports 1 million dataset", "label": "Sports 1 million dataset", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "73.1%", "label": "73.1%", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "60.9%", "label": "60.9%", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Ioannis Gkiouslekas", "label": "Ioannis Gkiouslekas", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "On the Appearance of Translucnt Edges", "label": "On the Appearance of Translucnt Edges", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Harvard SEAS", "label": "Harvard SEAS", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Gkiouslekas", "label": "Gkiouslekas", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Higher Education", "label": "Higher Education", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Bruce Walter", "label": "Bruce Walter", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Cornell University", "label": "Cornell University", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "bruce.walter@cornell.edu", "label": "bruce.walter@cornell.edu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "On the Appearance of Translueceny Edges", "label": "On the Appearance of Translueceny Edges", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Department of Computer Science", "label": "Department of Computer Science", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Edward H. Adelson", "label": "Edward H. Adelson", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Massachusetts Institute of Technology", "label": "Massachusetts Institute of Technology", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "adelson@cail.mit.edu", "label": "adelson@cail.mit.edu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "CSAIL", "label": "CSAIL", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "LIDS", "label": "LIDS", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Walter", "label": "Walter", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Microfacet models", "label": "Microfacet models", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "refraction through rough surfaces", "label": "refraction through rough surfaces", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "EGSR", "label": "EGSR", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Ioannis Gkioleakas", "label": "Ioannis Gkioleakas", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "igkio@seas.harvard.edu", "label": "igkio@seas.harvard.edu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Academic Degrees", "label": "Academic Degrees", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Ioannis Gkiousleas", "label": "Ioannis Gkiousleas", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Todd Zickler", "label": "Todd Zickler", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "On the Appearence of Translueceny Edges", "label": "On the Appearence of Translueceny Edges", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Kavita Bala", "label": "Kavita Bala", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Material recognition in 2015 CVPR paper", "label": "Material recognition in 2015 CVPR paper", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Bell_Material_Recognition_in_2015_CVPR_paper", "label": "Bell_Material_Recognition_in_2015_CVPR_paper", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Department of Computer Science, Cornell University", "label": "Department of Computer Science, Cornell University", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "kb@cs.cornell.edu", "label": "kb@cs.cornell.edu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Naeemullah Khan", "label": "Naeemullah Khan", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Shape-Tailored Local Descriptors", "label": "Shape-Tailored Local Descriptors", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Segmentation", "label": "Segmentation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Tracking", "label": "Tracking", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Marei Algarni", "label": "Marei Algarni", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "King Abdullah University of Science \u0026 Technology (KAUST)", "label": "King Abdullah University of Science \u0026 Technology (KAUST)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Anthony Yezzi", "label": "Anthony Yezzi", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Shape-Tailed Local Descriptors", "label": "Shape-Tailed Local Descriptors", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Georgia Institute of Technology", "label": "Georgia Institute of Technology", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Ganesh Sundaramoorthi", "label": "Ganesh Sundaramoorthi", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "descriptors", "label": "descriptors", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "dense descriptors", "label": "dense descriptors", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "texture segmentation", "label": "texture segmentation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "shape-dependent scale spaces", "label": "shape-dependent scale spaces", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "aggregate image data across boundary", "label": "aggregate image data across boundary", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "accurate segmentation", "label": "accurate segmentation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "shape-dependent", "label": "shape-dependent", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "non-shape dependent", "label": "non-shape dependent", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "3D models", "label": "3D models", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "SIFTs", "label": "SIFTs", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "oriented gradients", "label": "oriented gradients", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "textured object tracking", "label": "textured object tracking", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Shape-Tailored Descriptors (STLD)", "label": "Shape-Tailored Descriptors (STLD)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Shape-Tailori Descriptors (STLD)", "label": "Shape-Tailori Descriptors (STLD)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "non-shape dependent descriptors", "label": "non-shape dependent descriptors", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Local Descriptors", "label": "Local Descriptors", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "De-An Huang", "label": "De-An Huang", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "How Do We Use Our Hands?", "label": "How Do We Use Our Hands?", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Common Grasps", "label": "Common Grasps", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Cnegie Mellon University", "label": "Cnegie Mellon University", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "common grasps", "label": "common grasps", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Partial Differential Equations (PDE)", "label": "Partial Differential Equations (PDE)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Minghuang Ma", "label": "Minghuang Ma", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "How DoWe Use Our Hands?", "label": "How DoWe Use Our Hands?", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Canezie Mellon University", "label": "Canezie Mellon University", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Wei-Chiu Ma", "label": "Wei-Chiu Ma", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Kris M. Kitani", "label": "Kris M. Kitani", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Common Graspt", "label": "Common Graspt", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "KAUST", "label": "KAUST", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Saudi Arabia", "label": "Saudi Arabia", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "ganesh.sundaramoorthi@kust.edu.sa", "label": "ganesh.sundaramoorthi@kust.edu.sa", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Huang_How_Do_We2015_CVPR_paper.pdf", "label": "Huang_How_Do_We2015_CVPR_paper.pdf", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "computer vision techniques", "label": "computer vision techniques", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "advance prehensile analysis", "label": "advance prehensile analysis", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "prehensile analysis", "label": "prehensile analysis", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "multi-disciplinary field", "label": "multi-disciplinary field", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "researchers", "label": "researchers", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "hand-object interaction videos", "label": "hand-object interaction videos", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "wearable cameras", "label": "wearable cameras", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "automatically discover common modes of human hand use", "label": "automatically discover common modes of human hand use", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "increase in egocentric videos", "label": "increase in egocentric videos", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "unsupervised clustering techniques", "label": "unsupervised clustering techniques", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "first-person point-of-view camera", "label": "first-person point-of-view camera", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "observing human hand use", "label": "observing human hand use", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "common modes of human hand use", "label": "common modes of human hand use", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "choreographed scenarios", "label": "choreographed scenarios", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Grasp Taxonomy", "label": "Grasp Taxonomy", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "hand-object interaction", "label": "hand-object interaction", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Determinantal Point Process (DPP)", "label": "Determinantal Point Process (DPP)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "N. Ailon", "label": "N. Ailon", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Streaming k-means approximation", "label": "Streaming k-means approximation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "W. Barbakh", "label": "W. Barbakh", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Online clustering algorithms", "label": "Online clustering algorithms", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "A. Fathi", "label": "A. Fathi", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Social interactions: A first-person perspective", "label": "Social interactions: A first-person perspective", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "R. Filipovych", "label": "R. Filipovych", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Recognizing primitive interactions", "label": "Recognizing primitive interactions", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "J. Case-Smith", "label": "J. Case-Smith", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Development of hand skills in children", "label": "Development of hand skills in children", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "hand skills", "label": "hand skills", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "L. Cheng", "label": "L. Cheng", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Pixel-level hand detection in ego-centric videos", "label": "Pixel-level hand detection in ego-centric videos", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "American Occupational Therapy Association", "label": "American Occupational Therapy Association", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "M. Cutkosky", "label": "M. Cutkosky", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "On grasp choice, grasp models", "label": "On grasp choice, grasp models", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "H. N. Djidjev", "label": "H. N. Djidjev", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Computing shortest paths", "label": "Computing shortest paths", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "C. Desai", "label": "C. Desai", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Discriminaitive models for static human-object interactions", "label": "Discriminaitive models for static human-object interactions", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Kris M. Kitan", "label": "Kris M. Kitan", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "deanh@andrew.cmu.edu", "label": "deanh@andrew.cmu.edu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "minghuam@andrew.cmu.edu", "label": "minghuam@andrew.cmu.edu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "weichium@andrew.cmu.edu", "label": "weichium@andrew.cmu.edu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Edward Johns", "label": "Edward Johns", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Becoming the Expert", "label": "Becoming the Expert", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Machine Teaching", "label": "Machine Teaching", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Oisin Mac Aodha", "label": "Oisin Mac Aodha", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Becoming the Efficient", "label": "Becoming the Efficient", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Gabriel J. Brostow", "label": "Gabriel J. Brostow", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Johns_Becoming_the_Expert_2015_CVPR_paper.pdf", "label": "Johns_Becoming_the_Expert_2015_CVPR_paper.pdf", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "challenging visual concepts", "label": "challenging visual concepts", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "teaching strategy", "label": "teaching strategy", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "experts", "label": "experts", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "annotators", "label": "annotators", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "stereo matching", "label": "stereo matching", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "ground control points", "label": "ground control points", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "interpreting line drawings", "label": "interpreting line drawings", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "expertise", "label": "expertise", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Interactive Machine Teaching", "label": "Interactive Machine Teaching", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Human Learning", "label": "Human Learning", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Visual Classification", "label": "Visual Classification", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Bruner", "label": "Bruner", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "The Process of Education", "label": "The Process of Education", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "foundational concepts", "label": "foundational concepts", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "teaching and learning", "label": "teaching and learning", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Curriculum learning", "label": "Curriculum learning", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "teaching strategies", "label": "teaching strategies", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Love", "label": "Love", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Categorization", "label": "Categorization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "cognitive neuroscience", "label": "cognitive neuroscience", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "core tasks", "label": "core tasks", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "active learning", "label": "active learning", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "overview", "label": "overview", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "technique", "label": "technique", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Machine teaching", "label": "Machine teaching", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "education", "label": "education", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "high accuracy", "label": "high accuracy", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "salient objects", "label": "salient objects", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "number of objects", "label": "number of objects", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "sampling estimation", "label": "sampling estimation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "error", "label": "error", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "machine teaching", "label": "machine teaching", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "approach toward optimal education", "label": "approach toward optimal education", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "paper\u0027s design", "label": "paper\u0027s design", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "learners with limited cognitive capacity", "label": "learners with limited cognitive capacity", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "algorithmic teaching", "label": "algorithmic teaching", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "background", "label": "background", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Love \u0026 Patil", "label": "Love \u0026 Patil", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "teaching learners", "label": "teaching learners", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Balbach \u0026 Zeugmann", "label": "Balbach \u0026 Zeugmann", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "algorithmic teaching methods", "label": "algorithmic teaching methods", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Basu \u0026 Christensen", "label": "Basu \u0026 Christensen", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "teaching classification boundaries", "label": "teaching classification boundaries", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "teaching classification tasks", "label": "teaching classification tasks", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Language and Automata Theory and Applications", "label": "Language and Automata Theory and Applications", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Recent developments in algorithmic teaching", "label": "Recent developments in algorithmic teaching", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Gigu`ere \u0026 Love", "label": "Gigu`ere \u0026 Love", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "cognitive limitations", "label": "cognitive limitations", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Chin", "label": "Chin", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "University College London", "label": "University College London", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Eriksson", "label": "Eriksson", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Suter", "label": "Suter", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Chin_Efficient_Globally_Optimal_2015_CVPR_paper.pdf", "label": "Chin_Efficient_Globally_Optimal_2015_CVPR_paper.pdf", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Efficient Globally Optimal Consensus Maximisation", "label": "Efficient Globally Optimal Consensus Maximisation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Maximum Consensus", "label": "Maximum Consensus", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Robust Estimation", "label": "Robust Estimation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Optimization Problems", "label": "Optimization Problems", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "randomized sample-and-test techniques", "label": "randomized sample-and-test techniques", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "optimality", "label": "optimality", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "globally optimal algorithms", "label": "globally optimal algorithms", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "randomized methods", "label": "randomized methods", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "LP-type methods", "label": "LP-type methods", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "A* Search", "label": "A* Search", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Search Algorithm", "label": "Search Algorithm", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "globally optimal results", "label": "globally optimal results", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Tree Search", "label": "Tree Search", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "LP-type Methods", "label": "LP-type Methods", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "N. Amenta", "label": "N. Amenta", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Optimal Point Placement for Mesh Smoothing", "label": "Optimal Point Placement for Mesh Smoothing", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "M. Bern", "label": "M. Bern", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "D. Eppstein", "label": "D. Eppstein", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "B. Chazelle", "label": "B. Chazelle", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "On Linear-Time Deterministic Algorithms", "label": "On Linear-Time Deterministic Algorithms", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "J. Matou\u02c7sek", "label": "J. Matou\u02c7sek", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Quasiconvex Programming", "label": "Quasiconvex Programming", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Optimization Technique", "label": "Optimization Technique", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "optimization algorithms", "label": "optimization algorithms", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "core theme", "label": "core theme", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "RANSA algorithm", "label": "RANSA algorithm", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "outlier rejection", "label": "outlier rejection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "automated cartography", "label": "automated cartography", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "multiple view geometry", "label": "multiple view geometry", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "central topic", "label": "central topic", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "l\u221e triangulation", "label": "l\u221e triangulation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "outlier handling", "label": "outlier handling", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "H. Li", "label": "H. Li", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "algorithm for l\u221e triangulation", "label": "algorithm for l\u221e triangulation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "geometric optimization", "label": "geometric optimization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "constraints", "label": "constraints", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "C. Olsson, O. Enqvist, and F. Kahl", "label": "C. Olsson, O. Enqvist, and F. Kahl", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "outlier handling in matching", "label": "outlier handling in matching", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "matching", "label": "matching", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "registration", "label": "registration", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "C. Olsson, A. Eriksson, and F. Kahl", "label": "C. Olsson, A. Eriksson, and F. Kahl", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "optimization techniques", "label": "optimization techniques", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "l\u221e-norm problems", "label": "l\u221e-norm problems", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Tat-Jun Chin", "label": "Tat-Jun Chin", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "School of Computer Science, The University of Adelaide", "label": "School of Computer Science, The University of Adelaide", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "University of Adelaide", "label": "University of Adelaide", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Bolei Zhou", "label": "Bolei Zhou", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Fuyuan Hu", "label": "Fuyuan Hu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Zhen Zhang", "label": "Zhen Zhang", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Anton van den Hengel", "label": "Anton van den Hengel", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Chunhua Shen", "label": "Chunhua Shen", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Anders Eriksson", "label": "Anders Eriksson", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "School of Electrical Engineering and Computer Science, Queensland University of Technology", "label": "School of Electrical Engineering and Computer Science, Queensland University of Technology", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Pulak Purkait", "label": "Pulak Purkait", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Julian Straub", "label": "Julian Straub", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Small-Variance Nonparametric Clustering on the Hypsphere", "label": "Small-Variance Nonparametric Clustering on the Hypsphere", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Straub_Small-Variance_Nonparametric_Clustering_2015_CVPR_paper.pdf", "label": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Straub_Small-Variance_Nonparametric_Clustering_2015_CVPR_paper.pdf", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Trevor Campbell", "label": "Trevor Campbell", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Jonathan P. How", "label": "Jonathan P. How", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "John W. Fisher III", "label": "John W. Fisher III", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "fisher@csaill.mit.edu", "label": "fisher@csaill.mit.edu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Straub_Small-Variance_Nonparametric_Clustering_2015_CVPR_paper.pdf", "label": "Straub_Small-Variance_Nonparametric_Clustering_2015_CVPR_paper.pdf", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Small-Variance Nonparametric Clustering on the Hypshere", "label": "Small-Variance Nonparametric Clustering on the Hypshere", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "surface normals", "label": "surface normals", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "distribution of structural regularities", "label": "distribution of structural regularities", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "algorithms", "label": "algorithms", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Bayesian nonparametric von-Mises-Fisher (vMF) mixture distributions", "label": "Bayesian nonparametric von-Mises-Fisher (vMF) mixture distributions", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "geometry of directional data", "label": "geometry of directional data", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "synthetic directional data", "label": "synthetic directional data", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "real 3D surface normals", "label": "real 3D surface normals", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "3D surface normals", "label": "3D surface normals", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "high dimensional directional data", "label": "high dimensional directional data", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "DDP-vMF-means", "label": "DDP-vMF-means", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "temporally evolving cluster structure", "label": "temporally evolving cluster structure", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "streaming data", "label": "streaming data", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "directional data", "label": "directional data", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "unit sphere", "label": "unit sphere", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "DP-vMF-means", "label": "DP-vMF-means", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Dirichlet process (DP) vMF mixture", "label": "Dirichlet process (DP) vMF mixture", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "protein backbone configurations", "label": "protein backbone configurations", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "semantic word vectors", "label": "semantic word vectors", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Algorithms", "label": "Algorithms", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "RGB-D sensors", "label": "RGB-D sensors", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Runtime", "label": "Runtime", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Solution Quality", "label": "Solution Quality", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Surface Normals", "label": "Surface Normals", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Bayesian Nonparametric Clustering", "label": "Bayesian Nonparametric Clustering", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "von-Mises-Fisher Distributions", "label": "von-Mises-Fisher Distributions", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Streaming Data Analysis", "label": "Streaming Data Analysis", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Abramowitz \u0026 Stegun", "label": "Abramowitz \u0026 Stegun", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "mathematical background", "label": "mathematical background", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Neal", "label": "Neal", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Markov chain sampling", "label": "Markov chain sampling", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "DPMMs", "label": "DPMMs", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Jiang, Kulis, \u0026 Jordan", "label": "Jiang, Kulis, \u0026 Jordan", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "theoretical analysis", "label": "theoretical analysis", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Jiang, K., Kulis, B., and Jordan, M.", "label": "Jiang, K., Kulis, B., and Jordan, M.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Bayesian nonparametrics", "label": "Bayesian nonparametrics", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "spherical data", "label": "spherical data", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Latent Dirichlet Allocation", "label": "Latent Dirichlet Allocation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "spherical topic models", "label": "spherical topic models", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "k-means", "label": "k-means", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Directional statistics", "label": "Directional statistics", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Mardia, K. V. and Jupp, P. E.", "label": "Mardia, K. V. and Jupp, P. E.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "John Wiley \u0026 Sons", "label": "John Wiley \u0026 Sons", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Bayesian nonparametric methods", "label": "Bayesian nonparametric methods", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Ferguson, T.", "label": "Ferguson, T.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Ferguson distributions", "label": "Ferguson distributions", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "p\u00b4olya urn schemes", "label": "p\u00b4olya urn schemes", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Dirichlet processes", "label": "Dirichlet processes", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Encyclopedia of Machine Learning", "label": "Encyclopedia of Machine Learning", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Bayesian analysis", "label": "Bayesian analysis", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "nonparametric problems", "label": "nonparametric problems", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Dingwen Zhang", "label": "Dingwen Zhang", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Co-Saliency Detection via Looking Deep and Wide", "label": "Co-Saliency Detection via Looking Deep and Wide", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "cvpr_papers/Zhang_Co-Saliency_Detection_via_2015_CVPR_paper.pdf", "label": "cvpr_papers/Zhang_Co-Saliency_Detection_via_2015_CVPR_paper.pdf", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Junwei Han", "label": "Junwei Han", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Chao Li", "label": "Chao Li", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Jingdong Wang", "label": "Jingdong Wang", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "jingdw@microsoft.com", "label": "jingdw@microsoft.com", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "co-saliency detection", "label": "co-saliency detection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "video foreground extraction", "label": "video foreground extraction", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "surveillance", "label": "surveillance", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "image retrieval", "label": "image retrieval", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "network", "label": "network", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "representation of co-salient objects", "label": "representation of co-salient objects", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "similarity value", "label": "similarity value", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "small target data set", "label": "small target data set", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "large data set", "label": "large data set", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "neighbors", "label": "neighbors", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "common background regions", "label": "common background regions", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "co-salience scores", "label": "co-salience scores", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "intra-image contrast", "label": "intra-image contrast", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "intra-group consistency", "label": "intra-group consistency", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "window-level co-salience scores", "label": "window-level co-salience scores", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "superpixel-level co-salience maps", "label": "superpixel-level co-salience maps", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "foreground region agreement strategy", "label": "foreground region agreement strategy", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Bayesian formulation", "label": "Bayesian formulation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "l-level co-saliency maps", "label": "l-level co-saliency maps", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "foreground region agreement", "label": "foreground region agreement", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "state-of-the-art hashing methods", "label": "state-of-the-art hashing methods", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "current state of the art", "label": "current state of the art", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "robust object discovery", "label": "robust object discovery", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "state-of-the-arts", "label": "state-of-the-arts", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "fluorescence microscopy cell images", "label": "fluorescence microscopy cell images", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "UCSD pedestrians", "label": "UCSD pedestrians", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "small animals", "label": "small animals", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "insects", "label": "insects", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Visual Object Tracking", "label": "Visual Object Tracking", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "state-of-the-art trackers", "label": "state-of-the-art trackers", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Co-salient object detection", "label": "Co-salient object detection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "multiple images", "label": "multiple images", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Unified approach", "label": "Unified approach", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "low rank matrix recovery", "label": "low rank matrix recovery", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "iCoseg", "label": "iCoseg", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "intelligent scribble guidance", "label": "intelligent scribble guidance", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Co-salience detection", "label": "Co-salience detection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Image Group Consistency", "label": "Image Group Consistency", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "image recognition accuracy", "label": "image recognition accuracy", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "complex", "label": "complex", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "time-consuming", "label": "time-consuming", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Pose Estimation", "label": "Pose Estimation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Song_Joint_Multi-Feature_Spatial_2015_CVPR_paper", "label": "Song_Joint_Multi-Feature_Spatial_2015_CVPR_paper", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Deep Convolutional Networks", "label": "Deep Convolutional Networks", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "ImageNet classification", "label": "ImageNet classification", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "deep learning", "label": "deep learning", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Bayesian Formulation", "label": "Bayesian Formulation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "discriminative regional feature integration", "label": "discriminative regional feature integration", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Visual Attention", "label": "Visual Attention", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "co-salience detection", "label": "co-salience detection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Binary Linear Integer Programming", "label": "Binary Linear Integer Programming", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "alient object detection", "label": "alient object detection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "CVPR (Conference)", "label": "CVPR (Conference)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Xie, Y.", "label": "Xie, Y.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Bayesian salience", "label": "Bayesian salience", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "low and mid level cues", "label": "low and mid level cues", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Han, J.", "label": "Han, J.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "object-oriented visual salieny detection framework", "label": "object-oriented visual salieny detection framework", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "sparse coding representations", "label": "sparse coding representations", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Rubinstein, M.", "label": "Rubinstein, M.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "joint object discovery", "label": "joint object discovery", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "internet images", "label": "internet images", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Jiang, H.", "label": "Jiang, H.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "salient object segmentation", "label": "salient object segmentation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "context prior", "label": "context prior", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "IEEE Trans. Image Process.", "label": "IEEE Trans. Image Process.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Image Processing", "label": "Image Processing", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Stereo/Inpainting", "label": "Stereo/Inpainting", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "is_author_of", "label": "is_author_of", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Cong Zhang", "label": "Cong Zhang", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Hongsheng Li", "label": "Hongsheng Li", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Xiaogang Wang", "label": "Xiaogang Wang", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Xiaokang Yang", "label": "Xiaokang Yang", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Cross-Scene Crowd Counting", "label": "Cross-Scene Crowd Counting", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Institute of Image Communication and Network Engineering, Shanghai Jiao Tong University", "label": "Institute of Image Communication and Network Engineering, Shanghai Jiao Tong University", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Department of Electronic Engineering, The Chinese University of Hong Kong", "label": "Department of Electronic Engineering, The Chinese University of Hong Kong", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Saliency Detection by Multi-Context Deep Learning", "label": "Saliency Detection by Multi-Context Deep Learning", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Department of Electronic Engineering", "label": "Department of Electronic Engineering", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "hsli@ee.cuhk.edu.hk", "label": "hsli@ee.cuhk.edu.hk", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Saliency Detection by Multi-Content Deep Learning", "label": "Saliency Detection by Multi-Content Deep Learning", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Shenzhen Institutes of Advanced Technology", "label": "Shenzhen Institutes of Advanced Technology", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Deeply Learned Attributes", "label": "Deeply Learned Attributes", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Chinese University of Hong Kong", "label": "Chinese University of Hong Kong", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "xgwang@ee.cuhk.edu.hk", "label": "xgwang@ee.cuhk.edu.hk", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "xk yang@sjtu.edu.cn", "label": "xk yang@sjtu.edu.cn", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Automatic salient object segmentation", "label": "Automatic salient object segmentation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Self-Adaptively Weighted Co-Saliency Detection", "label": "Self-Adaptively Weighted Co-Saliency Detection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Zhang_Cross-Scene_Crowd_Counting_2015_CVPR_paper.pdf", "label": "Zhang_Cross-Scene_Crowd_Counting_2015_CVPR_paper.pdf", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Cross-scene crowd counting", "label": "Cross-scene crowd counting", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "no laborious data annotation", "label": "no laborious data annotation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "existing crowd counting methods", "label": "existing crowd counting methods", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "significant performance drop", "label": "significant performance drop", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "crowd density", "label": "crowd density", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "object detection benchmarks", "label": "object detection benchmarks", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "switchable learning approach", "label": "switchable learning approach", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "better local optimum", "label": "better local optimum", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "data-driven method", "label": "data-driven method", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "fine-tune CNN model", "label": "fine-tune CNN model", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "CNN model", "label": "CNN model", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "trained", "label": "trained", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Crowd Counting", "label": "Crowd Counting", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Deep Convolutional Neural Networks (CNNs)", "label": "Deep Convolutional Neural Networks (CNNs)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "object counting", "label": "object counting", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Chen et al. (2013)", "label": "Chen et al. (2013)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Dimensionality Reduction", "label": "Dimensionality Reduction", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "cumulative attribute space", "label": "cumulative attribute space", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "crowd density estimation", "label": "crowd density estimation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Lempitsky \u0026 Zisserman (2010)", "label": "Lempitsky \u0026 Zisserman (2010)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Chen et al. (2012)", "label": "Chen et al. (2012)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "feature mining", "label": "feature mining", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "localized crowd counting", "label": "localized crowd counting", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Loy et al. (2012) research", "label": "Loy et al. (2012) research", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "An et al. (2007) research", "label": "An et al. (2007) research", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "kernel ridge regression", "label": "kernel ridge regression", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "vision tasks", "label": "vision tasks", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Kai et al. (2014) research", "label": "Kai et al. (2014) research", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "fully convolutional network", "label": "fully convolutional network", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "crowd segmentation", "label": "crowd segmentation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "neural network", "label": "neural network", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Kong et al. (2006) research", "label": "Kong et al. (2006) research", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "viewpoint invariance", "label": "viewpoint invariance", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Kong et al. (2006)", "label": "Kong et al. (2006)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "viewpoint invariance in crowd counting", "label": "viewpoint invariance in crowd counting", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "practical consideration", "label": "practical consideration", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Jing et al. (2015)", "label": "Jing et al. (2015)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "deep learning for attribute extraction", "label": "deep learning for attribute extraction", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "attribute extraction", "label": "attribute extraction", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "crowd scene understanding", "label": "crowd scene understanding", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Fiaschi et al. (2012)", "label": "Fiaschi et al. (2012)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "regression forest for counting", "label": "regression forest for counting", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "neural networks", "label": "neural networks", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Loy et al. (2013)", "label": "Loy et al. (2013)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "semi-supervised learning for crowd counting", "label": "semi-supervised learning for crowd counting", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "transfer learning for crowd counting", "label": "transfer learning for crowd counting", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "ICPR", "label": "ICPR", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "ICCV", "label": "ICCV", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Class-specific material categorisation", "label": "Class-specific material categorisation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Computer Vision Conference", "label": "Computer Vision Conference", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "1841\u20131848", "label": "1841\u20131848", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Ground truth dataset", "label": "Ground truth dataset", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Gong, S.", "label": "Gong, S.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "From semi-supervised to transfer counting of crowds", "label": "From semi-supervised to transfer counting of crowds", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Xiang, T.", "label": "Xiang, T.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Lowe, D. G.", "label": "Lowe, D. G.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Distinctive image features from scale-invariant keypoints", "label": "Distinctive image features from scale-invariant keypoints", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "feature extraction", "label": "feature extraction", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "SIFT features", "label": "SIFT features", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "crowd analysis", "label": "crowd analysis", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "image representation techniques", "label": "image representation techniques", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "keypoints", "label": "keypoints", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "IJCV, 60:91\u2013110, 2004", "label": "IJCV, 60:91\u2013110, 2004", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "image processing tasks", "label": "image processing tasks", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "key component", "label": "key component", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "distinctive image features", "label": "distinctive image features", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "feature detection", "label": "feature detection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "feature matching", "label": "feature matching", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Yongzhen Huang", "label": "Yongzhen Huang", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Deep SemanticRanking Based Hashing", "label": "Deep SemanticRanking Based Hashing", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Center for Research on Intelligent Perception and Computing", "label": "Center for Research on Intelligent Perception and Computing", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Liang Wang", "label": "Liang Wang", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Deep Semantic Ranking Based Hashing", "label": "Deep Semantic Ranking Based Hashing", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Multi-Label Image Retrieval", "label": "Multi-Label Image Retrieval", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Tieniu Tan", "label": "Tieniu Tan", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Institute of Image Communication and Network Engineering", "label": "Institute of Image Communication and Network Engineering", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Shanghai Jiao Tong University", "label": "Shanghai Jiao Tong University", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "deep hash functions", "label": "deep hash functions", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "hand-crafted features", "label": "hand-crafted features", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "ranking list", "label": "ranking list", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "multilevel similarity information", "label": "multilevel similarity information", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "semantic representation", "label": "semantic representation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "hash codes", "label": "hash codes", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Deep Hash Functions", "label": "Deep Hash Functions", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Similarity Information", "label": "Similarity Information", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Proposed Approach", "label": "Proposed Approach", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Hashing Methods", "label": "Hashing Methods", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "ImageNet Classification", "label": "ImageNet Classification", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Deep Convolutional Ranking", "label": "Deep Convolutional Ranking", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Multi-label Image Annotation", "label": "Multi-label Image Annotation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Iterative Quantization", "label": "Iterative Quantization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Binary Codes", "label": "Binary Codes", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Image Retrieval", "label": "Image Retrieval", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Hashing", "label": "Hashing", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Krizhevsky", "label": "Krizhevsky", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Hinton", "label": "Hinton", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Imaginet classification with deep convolutional neural networks", "label": "Imaginet classification with deep convolutional neural networks", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Gong", "label": "Gong", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Deep convolutional ranking", "label": "Deep convolutional ranking", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "A Maximum Entropy Feature Descriptor", "label": "A Maximum Entropy Feature Descriptor", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "sequential subset selection", "label": "sequential subset selection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Perronnin", "label": "Perronnin", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Iterative quantization", "label": "Iterative quantization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "binary codes", "label": "binary codes", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "each bit", "label": "each bit", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Norouzi", "label": "Norouzi", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Hamming distance metric learning", "label": "Hamming distance metric learning", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Minimal loss hashing", "label": "Minimal loss hashing", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "scalable image retrieval", "label": "scalable image retrieval", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "*CVPR*", "label": "*CVPR*", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "techniques", "label": "techniques", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Torralba", "label": "Torralba", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Small codes", "label": "Small codes", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Learning to Predict Where Humans Look", "label": "Learning to Predict Where Humans Look", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "image databases", "label": "image databases", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "multilabel image annotation", "label": "multilabel image annotation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "multilable image annotation", "label": "multilable image annotation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Gong, Y.", "label": "Gong, Y.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Krizhevsky, A.", "label": "Krizhevsky, A.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "One weird trick", "label": "One weird trick", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "ImageNet classification with deep convolutional neural networks", "label": "ImageNet classification with deep convolutional neural networks", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Sutskever, I.", "label": "Sutskever, I.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Hinton, G. E.", "label": "Hinton, G. E.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "parallelizing convolutional neural networks", "label": "parallelizing convolutional neural networks", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "compact binary codes", "label": "compact binary codes", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "loss", "label": "loss", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Lin, G.", "label": "Lin, G.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Optimizing ranking measures", "label": "Optimizing ranking measures", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "compact binary code learning", "label": "compact binary code learning", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Fang Zhao", "label": "Fang Zhao", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Institute of Automation", "label": "Institute of Automation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Center for Biometrics and Security Research", "label": "Center for Biometrics and Security Research", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "National Laboratory of Pattern Recognition", "label": "National Laboratory of Pattern Recognition", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "David Perra", "label": "David Perra", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Adaptive Eye-Camera Calibration", "label": "Adaptive Eye-Camera Calibration", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Google Inc.", "label": "Google Inc.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Rohit Kumar Gupta", "label": "Rohit Kumar Gupta", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "The University of North Carolina at Chapel Hill", "label": "The University of North Carolina at Chapel Hill", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "rkgupta@cs.unc.edu", "label": "rkgupta@cs.unc.edu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "fang.zhao@nlpr.ia.ac.cn", "label": "fang.zhao@nlpr.ia.ac.cn", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Perra_Adaptive_Eye-Camera_Calibration_2015_CVPR_paper.pdf", "label": "Perra_Adaptive_Eye-Camera_Calibration_2015_CVPR_paper.pdf", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "calibration scheme", "label": "calibration scheme", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "globally optimal model", "label": "globally optimal model", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "changes in calibration", "label": "changes in calibration", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "locally optimal eye-device transformation", "label": "locally optimal eye-device transformation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "local window of previous frames", "label": "local window of previous frames", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "continuous", "label": "continuous", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "locally optimal", "label": "locally optimal", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "state of the art systems", "label": "state of the art systems", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "environment", "label": "environment", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "calibration schemes", "label": "calibration schemes", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "per-user basis", "label": "per-user basis", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "user\u2019s environment", "label": "user\u2019s environment", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "eye-device transformation", "label": "eye-device transformation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "proposed calibration scheme", "label": "proposed calibration scheme", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "existing state of the art systems", "label": "existing state of the art systems", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "less restrictive", "label": "less restrictive", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Alnajar et al.", "label": "Alnajar et al.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Calibration-free gaze estimation", "label": "Calibration-free gaze estimation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "human gaze patterns", "label": "human gaze patterns", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Chen and Ji", "label": "Chen and Ji", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Probabilistic gaze estimation", "label": "Probabilistic gaze estimation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "active personal calibration", "label": "active personal calibration", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Corno et al.", "label": "Corno et al.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "cost-effective solution", "label": "cost-effective solution", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "eye-gaze assistive technology", "label": "eye-gaze assistive technology", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "eye-camera calibration", "label": "eye-camera calibration", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "gaze tracking", "label": "gaze tracking", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "F. Corno", "label": "F. Corno", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "IEEE International Conference on Multimedia and Expo", "label": "IEEE International Conference on Multimedia and Expo", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "research on assistive technology", "label": "research on assistive technology", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "E. Guestrin", "label": "E. Guestrin", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "remote gaze estimation", "label": "remote gaze estimation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "pupil center", "label": "pupil center", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "D. Hansen", "label": "D. Hansen", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "models for eyes and gaze", "label": "models for eyes and gaze", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "J. Harel", "label": "J. Harel", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "graph-based visual saliency", "label": "graph-based visual saliency", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Graph-based visual salience", "label": "Graph-based visual salience", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "neural processing method", "label": "neural processing method", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "X. Hou", "label": "X. Hou", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "image signature", "label": "image signature", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "sparse salient regions", "label": "sparse salient regions", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "IEEE Transactions on Biomedical Engineering", "label": "IEEE Transactions on Biomedical Engineering", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "research on remote gaze estimation", "label": "research on remote gaze estimation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "U. Lahiri", "label": "U. Lahiri", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Neural Systems and Rehabilitation Engineering, IEEE Transactions on", "label": "Neural Systems and Rehabilitation Engineering, IEEE Transactions on", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Virtual Rehabilitation (ICVR)", "label": "Virtual Rehabilitation (ICVR)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "children with autism", "label": "children with autism", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "social communication", "label": "social communication", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "R. Kumar", "label": "R. Kumar", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Computer Vision and Pattern Recognition", "label": "Computer Vision and Pattern Recognition", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Jan-Micheal Frahm", "label": "Jan-Micheal Frahm", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "The University of North Carolian at Chapel Hill", "label": "The University of North Carolian at Chapel Hill", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "jmf@cs.unc.edu", "label": "jmf@cs.unc.edu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Nianuan Jiang", "label": "Nianuan Jiang", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Direct Structure Estimation for 3D Reconstruction", "label": "Direct Structure Estimation for 3D Reconstruction", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Advanced Digital Sciences Center", "label": "Advanced Digital Sciences Center", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Advanced Digital Sciences Center, Singapore", "label": "Advanced Digital Sciences Center, Singapore", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Wen-Yan Lin", "label": "Wen-Yan Lin", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Direct Structure Estimated for 3D Reconstruction", "label": "Direct Structure Estimated for 3D Reconstruction", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Minh N. Do", "label": "Minh N. Do", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Jiangbo Lu", "label": "Jiangbo Lu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Jiang_Direct_Structure_Estimation_2015_CVPR_paper.pdf", "label": "Jiang_Direct_Structure_Estimation_2015_CVPR_paper.pdf", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Structure from Motion (SFM)", "label": "Structure from Motion (SFM)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "camera pose estimation", "label": "camera pose estimation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Euclidean Rigidity", "label": "Euclidean Rigidity", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "scene reconstruction", "label": "scene reconstruction", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Homography Estimation", "label": "Homography Estimation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "visual SLAM", "label": "visual SLAM", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "scene structure recovery", "label": "scene structure recovery", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Direct StructureEstimation (DSE)", "label": "Direct StructureEstimation (DSE)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "homography estimation", "label": "homography estimation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Direct Structure Estimation (DSE)", "label": "Direct Structure Estimation (DSE)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "formulation for scene structure recovery", "label": "formulation for scene structure recovery", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "recovering scene structure", "label": "recovering scene structure", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "recovering camera poses", "label": "recovering camera poses", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "scene structure", "label": "scene structure", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "sideway motion", "label": "sideway motion", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "planar or general man-made scenes", "label": "planar or general man-made scenes", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Camera Pose Estimation", "label": "Camera Pose Estimation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Non-Rigid Structure from Motion", "label": "Non-Rigid Structure from Motion", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "D. Nist\u00e9r", "label": "D. Nist\u00e9r", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "solution to five-point relative pose problem", "label": "solution to five-point relative pose problem", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "D. G. Aliaga", "label": "D. G. Aliaga", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "simplifying reconstruction of 3d models", "label": "simplifying reconstruction of 3d models", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "K. S. Arun", "label": "K. S. Arun", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "least-squares fitting of two 3-d point sets", "label": "least-squares fitting of two 3-d point sets", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "D. Crandall", "label": "D. Crandall", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "discrete-continuous optimization", "label": "discrete-continuous optimization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "D. W. Eggert", "label": "D. W. Eggert", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "comparison of four major algorithms", "label": "comparison of four major algorithms", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Estimating 3-d rigid body transformations", "label": "Estimating 3-d rigid body transformations", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "least-squares fitting", "label": "least-squares fitting", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "3d point set alignment", "label": "3d point set alignment", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "M. A. Fischler", "label": "M. A. Fischler", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Random sample consensus", "label": "Random sample consensus", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Communications of the ACM", "label": "Communications of the ACM", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Singapore", "label": "Singapore", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Structure from motion", "label": "Structure from motion", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "R. Hartley", "label": "R. Hartley", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "In defense of the eight-point algorithm", "label": "In defense of the eight-point algorithm", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "H. Isack", "label": "H. Isack", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Energy-based geometric multi-model fitting", "label": "Energy-based geometric multi-model fitting", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "N. Jiang", "label": "N. Jiang", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "A global linear method for camera pose registration", "label": "A global linear method for camera pose registration", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Stefan Roth", "label": "Stefan Roth", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Richter_Discriminative_Shape_From_2015_CVPR_supplemental", "label": "Richter_Discriminative_Shape_From_2015_CVPR_supplemental", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Department of Computer Science, TU Darmstadt", "label": "Department of Computer Science, TU Darmstadt", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "TU Darmstadt", "label": "TU Darmstadt", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Discriminaitve Shape from Shading", "label": "Discriminaitve Shape from Shading", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Discriminative Shape from Shading", "label": "Discriminative Shape from Shading", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Shape from shading method", "label": "Shape from shading method", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "other approaches", "label": "other approaches", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "local context", "label": "local context", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "learning framework", "label": "learning framework", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "improved reconstructions", "label": "improved reconstructions", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "smooth local context", "label": "smooth local context", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Results", "label": "Results", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "real images", "label": "real images", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Dataset", "label": "Dataset", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "ground truth dataset", "label": "ground truth dataset", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "real objects", "label": "real objects", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Real Objects", "label": "Real Objects", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Shape from Shading", "label": "Shape from Shading", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Surface Reconstruction", "label": "Surface Reconstruction", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Illumination Estimation", "label": "Illumination Estimation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Local and Global Context", "label": "Local and Global Context", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Machine Learning", "label": "Machine Learning", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "research area", "label": "research area", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Zhang_Light_Field_From_2015_CVPR_paper", "label": "Zhang_Light_Field_From_2015_CVPR_paper", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "RGB-D Cameras", "label": "RGB-D Cameras", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Barron, J. T., \u0026 Malik, J.", "label": "Barron, J. T., \u0026 Malik, J.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Color Constancy", "label": "Color Constancy", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Johnson, M. K., \u0026 Adelson, E. H.", "label": "Johnson, M. K., \u0026 Adelson, E. H.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Shape Estimation", "label": "Shape Estimation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Natural Illumination", "label": "Natural Illumination", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Johnson \u0026 Adelson (2011) paper", "label": "Johnson \u0026 Adelson (2011) paper", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Jacobs, D. W.", "label": "Jacobs, D. W.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Basri, R.", "label": "Basri, R.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Stephan R. Richter", "label": "Stephan R. Richter", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Andr\u00e1s B\u00f3dis-Sz\u0151m\u0151ru", "label": "Andr\u00e1s B\u00f3dis-Sz\u0151m\u0151ru", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Superpixel Meshes", "label": "Superpixel Meshes", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "surface reconstruction", "label": "surface reconstruction", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "edges", "label": "edges", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Hayko Riemenschneider", "label": "Hayko Riemenschneider", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Superpixel Meses", "label": "Superpixel Meses", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "ETH Zurich, Computer Vision Lab", "label": "ETH Zurich, Computer Vision Lab", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Luc Van Gool", "label": "Luc Van Gool", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "PSI-VISICS, KU Leuven", "label": "PSI-VISICS, KU Leuven", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Metric imitation by manifold transfer", "label": "Metric imitation by manifold transfer", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "VISICS, ESAT/PSI, KU Leuven", "label": "VISICS, ESAT/PSI, KU Leuven", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Computer Vision Lab, ETH Zurich", "label": "Computer Vision Lab, ETH Zurich", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Privacy Preserving Optics for Miniature Vision Sensors", "label": "Privacy Preserving Optics for Miniature Vision Sensors", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Bodis-Szomoru_Superpixel_Meshes_for_2015_CVPR_supplemental.pdf", "label": "Bodis-Szomoru_Superpixel_Meshes_for_2015_CVPR_supplemental.pdf", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Multi-View-Stereo methods", "label": "Multi-View-Stereo methods", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "highest detail", "label": "highest detail", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "surface reconstruction method", "label": "surface reconstruction method", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "image edges", "label": "image edges", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "second-order smoothness constraints", "label": "second-order smoothness constraints", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "meshes", "label": "meshes", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "classic MVS surfaces", "label": "classic MVS surfaces", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "edge-aligned", "label": "edge-aligned", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "compact", "label": "compact", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "image gradients", "label": "image gradients", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Ground Control Points", "label": "Ground Control Points", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "GCPs", "label": "GCPs", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "LiDAR", "label": "LiDAR", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "RGB-D", "label": "RGB-D", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Structure-from-Motion (SfM) points", "label": "Structure-from-Motion (SfM) points", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "renderings", "label": "renderings", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "lightweight", "label": "lightweight", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "per-face flat", "label": "per-face flat", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Didyk et.al", "label": "Didyk et.al", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "surface quality", "label": "surface quality", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Superpixels", "label": "Superpixels", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Zhang_Light_Light_Field_From_2015_CVPR_paper", "label": "Zhang_Light_Light_Field_From_2015_CVPR_paper", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Structure-from-Motion (SfM)", "label": "Structure-from-Motion (SfM)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Mesh Generation", "label": "Mesh Generation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Edge-Preerving Methods", "label": "Edge-Preerving Methods", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Andr\u00b4as B\u00b4odis-Szomor\u00b4u", "label": "Andr\u00b4as B\u00b4odis-Szomor\u00b4u", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "intermediate views", "label": "intermediate views", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "light field synthesis", "label": "light field synthesis", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Depth Estimation", "label": "Depth Estimation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "pixel-coordinates", "label": "pixel-coordinates", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "failure to enforce consistency constraints", "label": "failure to enforce consistency constraints", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "View Synthesis", "label": "View Synthesis", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Light Field Reconstruction", "label": "Light Field Reconstruction", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Disparity Refinement", "label": "Disparity Refinement", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Iterative View Generation", "label": "Iterative View Generation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Jan Hosang", "label": "Jan Hosang", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Mohamed Omran", "label": "Mohamed Omran", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "CifarNet", "label": "CifarNet", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "AlexNet", "label": "AlexNet", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Krizhevsky, A., Sutskever, I., and Hinton, G. E.", "label": "Krizhevsky, A., Sutskever, I., and Hinton, G. E.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "re-identification models", "label": "re-identification models", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "deep convolutional neural network", "label": "deep convolutional neural network", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "breakthrough performance", "label": "breakthrough performance", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "research in intrinsic image decomposition", "label": "research in intrinsic image decomposition", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "subsequent research", "label": "subsequent research", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "filter size", "label": "filter size", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "layer width", "label": "layer width", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "learning rate policies", "label": "learning rate policies", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "pedestrian heights", "label": "pedestrian heights", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Calttech dataset", "label": "Calttech dataset", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "benchmark", "label": "benchmark", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "vision", "label": "vision", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "robotics", "label": "robotics", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "vision and robotics", "label": "vision and robotics", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "transferability", "label": "transferability", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "neural network training", "label": "neural network training", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "parameter choices", "label": "parameter choices", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "parameter optimization", "label": "parameter optimization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "optimal results", "label": "optimal results", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Pedestrian Detection", "label": "Pedestrian Detection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Neural Network Training", "label": "Neural Network Training", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Parameter Optimization", "label": "Parameter Optimization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Dataset Analysis", "label": "Dataset Analysis", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Transfer Learning", "label": "Transfer Learning", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Benenson, R.", "label": "Benenson, R.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "rodrigo.benenson@mpi-inf.mpg.de", "label": "rodrigo.benenson@mpi-inf.mpg.de", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "informatics", "label": "informatics", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Omran, M.", "label": "Omran, M.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "mohamed.omran@mpi-inf.mpg.de", "label": "mohamed.omran@mpi-inf.mpg.de", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Schiele, B.", "label": "Schiele, B.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "unspecified", "label": "unspecified", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Hosang, J.", "label": "Hosang, J.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "jan.hosang@mpi-inf.mpg.de", "label": "jan.hosang@mpi-inf.mpg.de", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Kiyoshi Matsuo", "label": "Kiyoshi Matsuo", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Depth Image Enhancement Using Local Tangent Plane Approximations", "label": "Depth Image Enhancement Using Local Tangent Plane Approximations", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Hokuyo Automatic Co., LTD.", "label": "Hokuyo Automatic Co., LTD.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Yoshimitsu Aoki", "label": "Yoshimitsu Aoki", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Keio University", "label": "Keio University", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "consumer RGB-D cameras", "label": "consumer RGB-D cameras", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "handling local geometries", "label": "handling local geometries", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "calculation of local tangents", "label": "calculation of local tangents", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "depth image enhancement", "label": "depth image enhancement", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "local geometries", "label": "local geometries", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Local Tangent Planes", "label": "Local Tangent Planes", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Depth Image Enhancement", "label": "Depth Image Enhancement", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Noise Reduction", "label": "Noise Reduction", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Completion Rate", "label": "Completion Rate", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Asus Xtion Pro Live", "label": "Asus Xtion Pro Live", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Kim et al. (2013)", "label": "Kim et al. (2013)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "joint intensity and depth analysis model", "label": "joint intensity and depth analysis model", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Kim et al. (2014)", "label": "Kim et al. (2014)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "depth map upsampling", "label": "depth map upsampling", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Lee et al.", "label": "Lee et al.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Journal of Signal Processing Systems", "label": "Journal of Signal Processing Systems", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "depth map upsampling method", "label": "depth map upsampling method", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "misalignment of depth and color boundaries", "label": "misalignment of depth and color boundaries", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Kopf et al.", "label": "Kopf et al.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Joint bilateral upsampling", "label": "Joint bilateral upsampling", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "upsampling method", "label": "upsampling method", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Li et al.", "label": "Li et al.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Joint example-based depth map super-resolution", "label": "Joint example-based depth map super-resolution", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "depth map resolution", "label": "depth map resolution", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Lu et al.", "label": "Lu et al.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Depth enhancement", "label": "Depth enhancement", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "low-rank matrix completion", "label": "low-rank matrix completion", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Joint geodesic up-sampling", "label": "Joint geodesic up-sampling", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "depth images", "label": "depth images", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "S. Lu", "label": "S. Lu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Depth enhancement via low-rank matrix completion", "label": "Depth enhancement via low-rank matrix completion", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "CVPR 2014", "label": "CVPR 2014", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "D. Scharstein", "label": "D. Scharstein", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Learning conditional random fields for stereo", "label": "Learning conditional random fields for stereo", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "J. Papon", "label": "J. Papon", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Point cloud video object segmentation", "label": "Point cloud video object segmentation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "IROS 2013", "label": "IROS 2013", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Sean Bell", "label": "Sean Bell", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Material Recognition in the Wild", "label": "Material Recognition in the Wild", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Material Recognition in 2015 CVPR paper", "label": "Material Recognition in 2015 CVPR paper", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Materials in Context Database", "label": "Materials in Context Database", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "sbell@cs.cornell.edu", "label": "sbell@cs.cornell.edu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Paul Upchurch", "label": "Paul Upchurch", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Bell_Material_Detection_in_2015_CVPR_paper", "label": "Bell_Material_Detection_in_2015_CVPR_paper", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "paulu@cs.cornell.edu", "label": "paulu@cs.cornell.edu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Noah Snavely", "label": "Noah Snavely", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "snavely@cs.cornell.edu", "label": "snavely@cs.cornell.edu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "large, well-sampled datasets", "label": "large, well-sampled datasets", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "material recognition", "label": "material recognition", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "rich surface texture", "label": "rich surface texture", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "lighting conditions", "label": "lighting conditions", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "MINC", "label": "MINC", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "large-scale", "label": "large-scale", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "open", "label": "open", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "material classification", "label": "material classification", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "material segmentation", "label": "material segmentation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "patch-based classification", "label": "patch-based classification", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "85.2% mean class accuracy", "label": "85.2% mean class accuracy", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "full image segmentation", "label": "full image segmentation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Dataset Creation", "label": "Dataset Creation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "G. Patterson et al.", "label": "G. Patterson et al.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "The SUN Attribute Database", "label": "The SUN Attribute Database", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Deeper Scene Understanding", "label": "Deeper Scene Understanding", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "S. Bell et al.", "label": "S. Bell et al.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "OpenSurposes", "label": "OpenSurposes", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "OpenSurfaces", "label": "OpenSurfaces", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "richly annotated catalog", "label": "richly annotated catalog", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "X. Qi et al.", "label": "X. Qi et al.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Pairwise rotation invariant co-occurrence local binary pattern", "label": "Pairwise rotation invariant co-occurrence local binary pattern", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "B. Caputo et al.", "label": "B. Caputo et al.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Class-speci\ufb01c material categorisation", "label": "Class-speci\ufb01c material categorisation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "image processing", "label": "image processing", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "ImageNet Large Scale Visual Recognition Challenge", "label": "ImageNet Large Scale Visual Recognition Challenge", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "visual recognition", "label": "visual recognition", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "ACM Transactions on Graphics (TOG)", "label": "ACM Transactions on Graphics (TOG)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Re\ufb02ectence and texture of real-world surfaces", "label": "Re\ufb02ectence and texture of real-world surfaces", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "LabelMe", "label": "LabelMe", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Re\ufb02ectance", "label": "Re\ufb02ectance", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "real-world surfaces", "label": "real-world surfaces", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "texture", "label": "texture", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Describing textures in the wild", "label": "Describing textures in the wild", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "TOG", "label": "TOG", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Graphics", "label": "Graphics", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Pascal VOC Challenge", "label": "Pascal VOC Challenge", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Visual Object Classes", "label": "Visual Object Classes", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Canada", "label": "Canada", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Wonmin Byeon", "label": "Wonmin Byeon", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Scene Labeling with LSTM Recurrent Neural Networks", "label": "Scene Labeling with LSTM Recurrent Neural Networks", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "German Research Center for Arti\ufb01cial Intelligence (DFKI)", "label": "German Research Center for Arti\ufb01cial Intelligence (DFKI)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Thomas M. Breuel", "label": "Thomas M. Breuel", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "University of Kaiserslautern", "label": "University of Kaiserslautern", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Federico Raue", "label": "Federico Raue", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Byeon_Scene_Labeling_With_2015_CVPR_paper.pdf", "label": "Byeon_Scene_Labeling_With_2015_CVPR_paper.pdf", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Accurate scene labeling", "label": "Accurate scene labeling", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "image understanding", "label": "image understanding", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "LSTM recurrent neural networks", "label": "LSTM recurrent neural networks", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "benchmark datasets", "label": "benchmark datasets", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Networks", "label": "Networks", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "raw RGB values", "label": "raw RGB values", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "complex scene images", "label": "complex scene images", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Thalaiyasingam Ajanthan", "label": "Thalaiyasingam Ajanthan", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Iteratively Reweighted Graph Cut", "label": "Iteratively Reweighted Graph Cut", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Australian National University", "label": "Australian National University", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Multi-label MRFs", "label": "Multi-label MRFs", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Multi-label Markov Random Fields (MRFs)", "label": "Multi-label Markov Random Fields (MRFs)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "piecewise constant model assumption", "label": "piecewise constant model assumption", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "flow field continuity constraint", "label": "flow field continuity constraint", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "MRFs", "label": "MRFs", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Non-convex Priors", "label": "Non-convex Priors", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Multi-label Markov Random Fields", "label": "Multi-label Markov Random Fields", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Graph Cut Optimization", "label": "Graph Cut Optimization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Ishikawa, H.", "label": "Ishikawa, H.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Exact optimization for Markov random fields with convex priors", "label": "Exact optimization for Markov random fields with convex priors", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "MRF optimization", "label": "MRF optimization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Boykov, Y.", "label": "Boykov, Y.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Fast approximate energy minimization via graph cuts", "label": "Fast approximate energy minimization via graph cuts", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "graph cut methods", "label": "graph cut methods", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Fast approximate energy minimization", "label": "Fast approximate energy minimization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "graph cuts", "label": "graph cuts", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Kolmogorov, V.", "label": "Kolmogorov, V.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Convergent tree-reweighted message passing", "label": "Convergent tree-reweighted message passing", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "reweighted message passing", "label": "reweighted message passing", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "energy minimization", "label": "energy minimization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Iteratively Reweighted Algorithms", "label": "Iteratively Reweighted Algorithms", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "pattern analysis", "label": "pattern analysis", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "energy minimization techniques", "label": "energy minimization techniques", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "smoothness-based priors", "label": "smoothness-based priors", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "inference techniques", "label": "inference techniques", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Markov random fields", "label": "Markov random fields", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "energy minimization methods", "label": "energy minimization methods", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Pattern Analysis and Machine Intelligence", "label": "Pattern Analysis and Machine Intelligence", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "ds with smoothness-based priors", "label": "ds with smoothness-based priors", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Large Displacement Optical Flow", "label": "Large Displacement Optical Flow", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Boykov et al.", "label": "Boykov et al.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "An experimental comparison", "label": "An experimental comparison", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI)", "label": "IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "min-cut/max-flow algorithms", "label": "min-cut/max-flow algorithms", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Pock et al.", "label": "Pock et al.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "convex formulation", "label": "convex formulation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Computer Vision\u2013ECCV 2008", "label": "Computer Vision\u2013ECCV 2008", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "multi-label problems", "label": "multi-label problems", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "H. f.", "label": "H. f.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Kappes et al.", "label": "Kappes et al.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Scharstein \u0026 Szeliski", "label": "Scharstein \u0026 Szeliski", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "dense two-frame stereo correspondence algorithms", "label": "dense two-frame stereo correspondence algorithms", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "understanding", "label": "understanding", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "stereo correspondence algorithms", "label": "stereo correspondence algorithms", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Vekler", "label": "Vekler", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Multi-label moves for mrfs", "label": "Multi-label moves for mrfs", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "truncated convex priors", "label": "truncated convex priors", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Mathieu Salzmann", "label": "Mathieu Salzmann", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Riemannian Coding and Dictionary Learning", "label": "Riemannian Coding and Dictionary Learning", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "NICITA", "label": "NICITA", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Hongdong Li", "label": "Hongdong Li", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Dense, Accurate Optical Flow Estimation", "label": "Dense, Accurate Optical Flow Estimation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Research School of Engineering", "label": "Research School of Engineering", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "The Australian National University (ANU)", "label": "The Australian National University (ANU)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "NICTA", "label": "NICTA", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Rui Zhao", "label": "Rui Zhao", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Multi-Context Deep Learning", "label": "Multi-Context Deep Learning", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Wanli Ouyang", "label": "Wanli Ouyang", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "wlouyang@ee.cuhk.edu.hk", "label": "wlouyang@ee.cuhk.edu.hk", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Image salience detection", "label": "Image salience detection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "highlight visually salient regions", "label": "highlight visually salient regions", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Conventional approaches", "label": "Conventional approaches", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "salient objects in low-contrast backgrounds", "label": "salient objects in low-contrast backgrounds", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "multi-context deep learning framework", "label": "multi-context deep learning framework", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "global context", "label": "global context", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "probabilistic inference", "label": "probabilistic inference", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Task-specific pre-training scheme", "label": "Task-specific pre-training scheme", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Frequency-tuned salient region detection", "label": "Frequency-tuned salient region detection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Training products of experts", "label": "Training products of experts", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Neural computation", "label": "Neural computation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Saliency detection", "label": "Saliency detection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Category-independent object-level saliency detection", "label": "Category-independent object-level saliency detection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Graph-based visual saliency", "label": "Graph-based visual saliency", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Multi-Context Modeling", "label": "Multi-Context Modeling", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "bootstrap learning", "label": "bootstrap learning", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Bootstrap Learning", "label": "Bootstrap Learning", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Deep Learning Features", "label": "Deep Learning Features", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Multiscale Analysis", "label": "Multiscale Analysis", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "ik", "label": "ik", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "arXiv", "label": "arXiv", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "A. Borji", "label": "A. Borji", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Boosting bottom-up and top-down visual features", "label": "Boosting bottom-up and top-down visual features", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "visual attention modeling", "label": "visual attention modeling", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "M.-M. Cheng", "label": "M.-M. Cheng", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Global contrast based salient region detection", "label": "Global contrast based salient region detection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "TRAMI", "label": "TRAMI", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "R. Mairon", "label": "R. Mairon", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "A closer look at context", "label": "A closer look at context", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Advanced Technology", "label": "Advanced Technology", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Baohua Li", "label": "Baohua Li", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Subspace Clustering by Mixture of Gaussian Regression", "label": "Subspace Clustering by Mixture of Gaussian Regression", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Dalian University of Technology", "label": "Dalian University of Technology", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Ying Zhang", "label": "Ying Zhang", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Dalian\u003c0xC2\u003e\u003c0xA0\u003eUniversity of Technology", "label": "Dalian\u003c0xC2\u003e\u003c0xA0\u003eUniversity of Technology", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Zhouchen Lin", "label": "Zhouchen Lin", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Subclone Clustering by Mixture of Gaussian Regression", "label": "Subclone Clustering by Mixture of Gaussian Regression", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "School of EECS", "label": "School of EECS", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Huchuan Lu", "label": "Huchuan Lu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Salient Object Detectio", "label": "Salient Object Detectio", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Deep Networks for Saliency Detection", "label": "Deep Networks for Saliency Detection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Deep Networks for Salience Detected", "label": "Deep Networks for Salience Detected", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Subspace clustering", "label": "Subspace clustering", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "multi-subspace representation", "label": "multi-subspace representation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "high-dimensional space", "label": "high-dimensional space", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Existing methods", "label": "Existing methods", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "norms", "label": "norms", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "MoG Regression", "label": "MoG Regression", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "subspace clustering", "label": "subspace clustering", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "affinity matrix", "label": "affinity matrix", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "clustering performance", "label": "clustering performance", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "noise distributions", "label": "noise distributions", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Mixture of Gausians (MoG)", "label": "Mixture of Gausians (MoG)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Noise Modeling", "label": "Noise Modeling", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Subspace Clustering", "label": "Subspace Clustering", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "state-of-the-art subspace clustering methods", "label": "state-of-the-art subspace clustering methods", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "K-plane clustering", "label": "K-plane clustering", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "segmentation", "label": "segmentation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Journal of Global Optimization", "label": "Journal of Global Optimization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "geo-referenced images", "label": "geo-referenced images", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "hierarchical", "label": "hierarchical", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "graph-based", "label": "graph-based", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "object identification", "label": "object identification", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "motion analysis", "label": "motion analysis", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "motion segmentation", "label": "motion segmentation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "various types of motion", "label": "various types of motion", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "broad framework", "label": "broad framework", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "referenced papers", "label": "referenced papers", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Mixture of Gaussian Regression", "label": "Mixture of Gaussian Regression", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Affinity Matrix Construction", "label": "Affinity Matrix Construction", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Spectral clustering", "label": "Spectral clustering", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "tutorial", "label": "tutorial", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Approaches", "label": "Approaches", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Pattern Analysis", "label": "Pattern Analysis", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Wright et al.", "label": "Wright et al.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "face recognition approach", "label": "face recognition approach", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "EM algorithm", "label": "EM algorithm", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Convergence properties", "label": "Convergence properties", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Gaussian mixtures", "label": "Gaussian mixtures", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Motion segmentation", "label": "Motion segmentation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Framework", "label": "Framework", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Optical flow estimation", "label": "Optical flow estimation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Types of motion", "label": "Types of motion", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Image Quality", "label": "Image Quality", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "robust PCA", "label": "robust PCA", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Face Detection", "label": "Face Detection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Haoxiang Li", "label": "Haoxiang Li", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Stevens Institute of Technology", "label": "Stevens Institute of Technology", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Hierarchical-PEP Model", "label": "Hierarchical-PEP Model", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Zhe Lin", "label": "Zhe Lin", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Jonathan Brandt", "label": "Jonathan Brandt", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Gang Hua", "label": "Gang Hua", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "{ghua}@steverns.edu", "label": "{ghua}@steverns.edu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Face detection", "label": "Face detection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "large visual variations", "label": "large visual variations", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "large search space", "label": "large search space", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Advanced models", "label": "Advanced models", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "visual variations", "label": "visual variations", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "computationally expensive", "label": "computationally expensive", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Paper", "label": "Paper", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "CNN cascade architecture", "label": "CNN cascade architecture", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "subgraph matching formulation", "label": "subgraph matching formulation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "conflicting demands", "label": "conflicting demands", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Cascade", "label": "Cascade", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "background regions", "label": "background regions", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "CNN-based calibration stage", "label": "CNN-based calibration stage", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "localization effectiveness", "label": "localization effectiveness", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "GPU", "label": "GPU", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Cascade Architecture", "label": "Cascade Architecture", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "LeCun", "label": "LeCun", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Convolutional networks", "label": "Convolutional networks", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "speech", "label": "speech", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Rowley", "label": "Rowley", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Neural network-based face detection", "label": "Neural network-based face detection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Felzenszwalb", "label": "Felzenszwalb", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "P. F.", "label": "P. F.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Object detection with discriminatatively trained part-based models", "label": "Object detection with discriminatatively trained part-based models", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "IEEE Trans. Pattern Anal. Mach. Intell", "label": "IEEE Trans. Pattern Anal. Mach. Intell", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Jain, V.", "label": "Jain, V.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Fddb: A benchmark for face detection in unconstrained settings", "label": "Fddb: A benchmark for face detection in unconstrained settings", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "University of Massachusetts, Amherst", "label": "University of Massachusetts, Amherst", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "arXiv preprint arXiv:1311.2524", "label": "arXiv preprint arXiv:1311.2524", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Advances in neural information processing systems", "label": "Advances in neural information processing systems", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Jia, Y.", "label": "Jia, Y.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Caffe: Convolutional architecture for fast feature embedding", "label": "Caffe: Convolutional architecture for fast feature embedding", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "helhamer, E.", "label": "helhamer, E.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Vaillant, R.", "label": "Vaillant, R.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "object localization approach", "label": "object localization approach", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Yang, B.", "label": "Yang, B.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "multi-view face detection method", "label": "multi-view face detection method", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Na Tong", "label": "Na Tong", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Xiang Ruan", "label": "Xiang Ruan", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "OMRON Corporation", "label": "OMRON Corporation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Deep Networks for Salience Detection", "label": "Deep Networks for Salience Detection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Ming-Hsuan Yang", "label": "Ming-Hsuan Yang", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "University of California at Merced", "label": "University of California at Merced", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "JOTS", "label": "JOTS", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Adaptive Region Pooling for Object Detection", "label": "Adaptive Region Pooling for Object Detection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "UC Merced", "label": "UC Merced", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Tong_Salient_Object_Detection_2015_CVPR_paper.pdf", "label": "Tong_Salient_Object_Detection_2015_CVPR_paper.pdf", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "bootstrap learning algorithm", "label": "bootstrap learning algorithm", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "weak models", "label": "weak models", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "strong models", "label": "strong models", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "state-of-the-art salience detection methods", "label": "state-of-the-art salience detection methods", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "weak salience map", "label": "weak salience map", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "image priors", "label": "image priors", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "training samples", "label": "training samples", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "strong classi\ufb01er", "label": "strong classi\ufb01er", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "salient pixels", "label": "salient pixels", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "input image", "label": "input image", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "multiscale salience maps", "label": "multiscale salience maps", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "bootstrap learning approach", "label": "bootstrap learning approach", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "signi\ufb01cant improvement", "label": "signi\ufb01cant improvement", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Salieny Detection Methods", "label": "Salieny Detection Methods", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Bootstrap Learning Approach", "label": "Bootstrap Learning Approach", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Bottom-up Salieny Models", "label": "Bottom-up Salieny Models", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Li et al. (2014)", "label": "Li et al. (2014)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Salient Object Segmentation", "label": "Salient Object Segmentation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Achanta et al. (2009)", "label": "Achanta et al. (2009)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Frequency-tuned Salient Region Detection", "label": "Frequency-tuned Salient Region Detection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Movahedi \u0026 Elder (2010)", "label": "Movahedi \u0026 Elder (2010)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Performance Measures", "label": "Performance Measures", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Achanta et al. (2010)", "label": "Achanta et al. (2010)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Slic Superpixels", "label": "Slic Superpixels", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Achanta, R.", "label": "Achanta, R.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Ojala, T.", "label": "Ojala, T.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Multiresolution gray-scale texture classification", "label": "Multiresolution gray-scale texture classification", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Bach, F. R.", "label": "Bach, F. R.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Multiple kernel learning", "label": "Multiple kernel learning", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "SMO algorithm", "label": "SMO algorithm", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Perazzi, F.", "label": "Perazzi, F.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Saliency filters", "label": "Saliency filters", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Borji, A.", "label": "Borji, A.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Salient object detection benchmark", "label": "Salient object detection benchmark", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Salient Object Detection: A Benchmark", "label": "Salient Object Detection: A Benchmark", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "authored", "label": "authored", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Rahtu, E.", "label": "Rahtu, E.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Salient object segmentation", "label": "Salient object segmentation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "performance measures", "label": "performance measures", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "feedforward multilayer network", "label": "feedforward multilayer network", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "primitive saliency dictionary", "label": "primitive saliency dictionary", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Local binary patterns", "label": "Local binary patterns", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "J. Heikkil\u00e4", "label": "J. Heikkil\u00e4", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Segmenting salient objects from images and videos", "label": "Segmenting salient objects from images and videos", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Kaiming He", "label": "Kaiming He", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "A Geodesic-Prepreserving Method for Image Warping", "label": "A Geodesic-Prepreserving Method for Image Warping", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "A Geolesic-Preerving Method for Image Warping", "label": "A Geolesic-Preerving Method for Image Warping", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Convolutional Neural Networks at Constained Time Cost", "label": "Convolutional Neural Networks at Constained Time Cost", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "architecture", "label": "architecture", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "competitive accuracy", "label": "competitive accuracy", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "20% faster than AlexNet", "label": "20% faster than AlexNet", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "application domain", "label": "application domain", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "69.6% average accuracy", "label": "69.6% average accuracy", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "PAS-CAL VOC 2012 test set", "label": "PAS-CAL VOC 2012 test set", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "accuracy improvements", "label": "accuracy improvements", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "factors", "label": "factors", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "ImageNet dataset", "label": "ImageNet dataset", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "11.8% top-5 error", "label": "11.8% top-5 error", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Offline Training", "label": "Offline Training", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Time Constraints", "label": "Time Constraints", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Accuracy Improvements", "label": "Accuracy Improvements", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Factors", "label": "Factors", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Limited Time Budget", "label": "Limited Time Budget", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Computer-Aided Design application", "label": "Computer-Aided Design application", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "recurrent convolutional", "label": "recurrent convolutional", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "large-scale visual learning", "label": "large-scale visual learning", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "ImageNet Dataset", "label": "ImageNet Dataset", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Computer Vision Tasks", "label": "Computer Vision Tasks", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Image Database", "label": "Image Database", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Deng et al. (2009)", "label": "Deng et al. (2009)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "ImageNet", "label": "ImageNet", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Layer Replacement", "label": "Layer Replacement", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Architecture Optimization", "label": "Architecture Optimization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "hierarchical image database", "label": "hierarchical image database", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Hierarchical Image Database", "label": "Hierarchical Image Database", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "ILSVRC2012", "label": "ILSVRC2012", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "large dataset", "label": "large dataset", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "training computer vision models", "label": "training computer vision models", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "hierarchical image", "label": "hierarchical image", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "very deep convolutional networks", "label": "very deep convolutional networks", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "development", "label": "development", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "network architecture", "label": "network architecture", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "power of deep learning", "label": "power of deep learning", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "image classification", "label": "image classification", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "rich feature hierarchies", "label": "rich feature hierarchies", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Malik et al. (2014)", "label": "Malik et al. (2014)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Zeiler et al. (2014)", "label": "Zeiler et al. (2014)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "convolutional neural networks", "label": "convolutional neural networks", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "interpretability", "label": "interpretability", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Eigen et al. (2013)", "label": "Eigen et al. (2013)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "deep architectures", "label": "deep architectures", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "recursive convolutional networks", "label": "recursive convolutional networks", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Chatfield et al. (2014)", "label": "Chatfield et al. (2014)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "convolutional networks", "label": "convolutional networks", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "details", "label": "details", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "output of last layer", "label": "output of last layer", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Return of the devil in the details", "label": "Return of the devil in the details", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Overfeat", "label": "Overfeat", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "integrated recognition, localization, and detection system", "label": "integrated recognition, localization, and detection system", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Going deeper with convolutions", "label": "Going deeper with convolutions", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "convolutions", "label": "convolutions", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Lei Zhang", "label": "Lei Zhang", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Reweighted Laplace Prior Based Hyperspectral Compressive Sensing", "label": "Reweighted Laplace Prior Based Hyperspectral Compressive Sensing", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Yanning Zhang", "label": "Yanning Zhang", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Zhang_Reweighted_Laplace_Prior_2015_CVPR_supplemental.pdf", "label": "Zhang_Reweighted_Laplace_Prior_2015_CVPR_supplemental.pdf", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Chunna Tian", "label": "Chunna Tian", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Reweighted Laplace Prior Based Hyperspectra Compressives Sensing", "label": "Reweighted Laplace Prior Based Hyperspectra Compressives Sensing", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Zhang_Reweighted_Lapless_Prior_2015_CVPR_supplemental.pdf", "label": "Zhang_Reweighted_Lapless_Prior_2015_CVPR_supplemental.pdf", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Fei Li", "label": "Fei Li", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Reweighted Laplace Prior Based Hyperspectral Compressives Sensing", "label": "Reweighted Laplace Prior Based Hyperspectral Compressives Sensing", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Wei Wei", "label": "Wei Wei", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Hyperspectral Compressives Sensing", "label": "Hyperspectral Compressives Sensing", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Laplace Prior", "label": "Laplace Prior", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "hyperspectral compressive sensing method", "label": "hyperspectral compressive sensing method", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "reweighted Laplace prior", "label": "reweighted Laplace prior", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "optimization procedure", "label": "optimization procedure", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "matrix algebra manipulations", "label": "matrix algebra manipulations", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "conjugate functions", "label": "conjugate functions", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "sparsity learning over \u03b3", "label": "sparsity learning over \u03b3", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "noise estimation over \u03bb", "label": "noise estimation over \u03bb", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "non-convex optimization problems", "label": "non-convex optimization problems", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Hyberspectral Compressive Sensing", "label": "Hyberspectral Compressive Sensing", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Reweighted Laplace Prior", "label": "Reweighted Laplace Prior", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "optimization technique", "label": "optimization technique", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Sparsity Learning", "label": "Sparsity Learning", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Noise Estimation", "label": "Noise Estimation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Fang_Collaborative_Feature_Learning_2015_CVPR_paper", "label": "Fang_Collaborative_Feature_Learning_2015_CVPR_paper", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Paper Abstract", "label": "Paper Abstract", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "readable text", "label": "readable text", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Research", "label": "Research", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Data Encoding/Decoding", "label": "Data Encoding/Decoding", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Text Corruption/Error Correction", "label": "Text Corruption/Error Correction", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Information Retrieval", "label": "Information Retrieval", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Salience", "label": "Salience", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Material Metamers", "label": "Material Metamers", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Visual Inference tasks", "label": "Visual Inference tasks", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Victor Escorcia", "label": "Victor Escorcia", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "On the Relationship between Visual Attributes and Convolutional Networks", "label": "On the Relationship between Visual Attributes and Convolutional Networks", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Escorcia_On_the_Relationship_2015_CVPR_paper.pdf", "label": "Escorcia_On_the_Relationship_2015_CVPR_paper.pdf", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "King Abdullah University of Science and Technology", "label": "King Abdullah University of Science and Technology", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Juan Carlos Niebles", "label": "Juan Carlos Niebles", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Universidad del Norte", "label": "Universidad del Norte", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Bernard Ghanem", "label": "Bernard Ghanem", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "\u21130TV: A New Method", "label": "\u21130TV: A New Method", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "King Abdullah University of Science and Technology (KAUST)", "label": "King Abdullah University of Science and Technology (KAUST)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "bernard.ghanem@kust.edu.sa", "label": "bernard.ghanem@kust.edu.sa", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Convolutional Networks", "label": "Convolutional Networks", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Visual Attributes", "label": "Visual Attributes", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "conv-net based object recognition", "label": "conv-net based object recognition", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "/mnt/DATA/Glucomaa/LLM/Ollama_pdf_handle/cvpr_papers", "label": "/mnt/DATA/Glucomaa/LLM/Ollama_pdf_handle/cvpr_papers", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "conv-nets", "label": "conv-nets", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "abstract concepts", "label": "abstract concepts", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "semantic visual attributes", "label": "semantic visual attributes", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "objects in images", "label": "objects in images", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "object description", "label": "object description", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Attribute Centric Nodes (ACNs)", "label": "Attribute Centric Nodes (ACNs)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "visual attribute representation", "label": "visual attribute representation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "discrimination", "label": "discrimination", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "conv-net nodes", "label": "conv-net nodes", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "information", "label": "information", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "layers", "label": "layers", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "sparsely distributed", "label": "sparsely distributed", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "unevenly distributed", "label": "unevenly distributed", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "visual attribute representation and discrimination", "label": "visual attribute representation and discrimination", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Zero-Shot Object Recognition", "label": "Zero-Shot Object Recognition", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Semantic Manifold Distance", "label": "Semantic Manifold Distance", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Zhenyong Fu", "label": "Zhenyong Fu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Queen Mary, University of London", "label": "Queen Mary, University of London", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Tao Xiang", "label": "Tao Xiang", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "QueenMary, University of London", "label": "QueenMary, University of London", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Elyor Kodirov", "label": "Elyor Kodirov", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Zero-Shot Object Research", "label": "Zero-Shot Object Research", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Shaogang Gong", "label": "Shaogang Gong", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Zero-shot learning", "label": "Zero-shot learning", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "recognise objects", "label": "recognise objects", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "knowledge transfer", "label": "knowledge transfer", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "existing works", "label": "existing works", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "similarity", "label": "similarity", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "semantic embedding space", "label": "semantic embedding space", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "distance metrics", "label": "distance metrics", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "intrinsic structure", "label": "intrinsic structure", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "semantic categories", "label": "semantic categories", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "semantic manifold distance", "label": "semantic manifold distance", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "AwA datasets", "label": "AwA datasets", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "proposed model", "label": "proposed model", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "ZSL algorithms", "label": "ZSL algorithms", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "semantic manifold", "label": "semantic manifold", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "AMP", "label": "AMP", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Label-embedding", "label": "Label-embedding", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Akata et al. (2013)", "label": "Akata et al. (2013)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "attribute-based classification", "label": "attribute-based classification", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Single-example learning", "label": "Single-example learning", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Bart \u0026 Ullman (2005)", "label": "Bart \u0026 Ullman (2005)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Cluster kernels", "label": "Cluster kernels", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Chapelle et al. (2002)", "label": "Chapelle et al. (2002)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Zero-shot learning (ZSL)", "label": "Zero-shot learning (ZSL)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Chapelle", "label": "Chapelle", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Weston", "label": "Weston", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Sch\u00f6lkopf", "label": "Sch\u00f6lkopf", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Deng", "label": "Deng", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Ding", "label": "Ding", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Learning higher-order graph structure", "label": "Learning higher-order graph structure", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Label relation graphs", "label": "Label relation graphs", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Dong", "label": "Dong", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Frome", "label": "Frome", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Devise", "label": "Devise", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "embedding model", "label": "embedding model", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "visual-semantic embedding", "label": "visual-semantic embedding", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Transductive multi-view embedding", "label": "Transductive multi-view embedding", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "zero-shot recognition", "label": "zero-shot recognition", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Zero shot recognition with unreliable attributes", "label": "Zero shot recognition with unreliable attributes", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Efficient estimation of word representations", "label": "Efficient estimation of word representations", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Sakrapee Paisitkriangkrai", "label": "Sakrapee Paisitkriangkrai", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Learning to rank in person re-identi\ufb01cation with metric ensembles", "label": "Learning to rank in person re-identi\ufb01cation with metric ensembles", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "The University of Adelaide", "label": "The University of Adelaide", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Australian Centre for Robotic Vision", "label": "Australian Centre for Robotic Vision", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Paisitkriangrai_Learning_to_Rank_2015_CVPR_paper.pdf", "label": "Paisitkriangrai_Learning_to_Rank_2015_CVPR_paper.pdf", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "The University of Adelaide, Australia", "label": "The University of Adelaide, Australia", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Supervised Discrete Hashing", "label": "Supervised Discrete Hashing", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Learning graph structure...", "label": "Learning graph structure...", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Robust Multiple Homography Estimation", "label": "Robust Multiple Homography Estimation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "not adaptable", "label": "not adaptable", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "relative distance", "label": "relative distance", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "average rank-k recognition rate", "label": "average rank-k recognition rate", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "RMS methods", "label": "RMS methods", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "rank-1 recognition rates", "label": "rank-1 recognition rates", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Ensemble-based approaches", "label": "Ensemble-based approaches", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "linear metrics", "label": "linear metrics", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "non-linear metrics", "label": "non-linear metrics", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Similarity metric", "label": "Similarity metric", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Person Re-Identification", "label": "Person Re-Identification", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Gong et al. (2014)", "label": "Gong et al. (2014)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Gong, S., Crisitan, M., Yan, S., and Loy, C. C.", "label": "Gong, S., Crisitan, M., Yan, S., and Loy, C. C.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "solution frameworks", "label": "solution frameworks", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "support vector method", "label": "support vector method", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Joachims, T.", "label": "Joachims, T.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "re-identification systems", "label": "re-identification systems", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Imaginet classification", "label": "Imaginet classification", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "metric learning", "label": "metric learning", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "scalability", "label": "scalability", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "distance metric learning", "label": "distance metric learning", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "computational challenges", "label": "computational challenges", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "paper (Felzenszwalb et al., 2010)", "label": "paper (Felzenszwalb et al., 2010)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "part-based model", "label": "part-based model", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "shape and appearance information", "label": "shape and appearance information", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "robust re-identification", "label": "robust re-identification", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "kernel methods", "label": "kernel methods", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "geodesic metric spaces", "label": "geodesic metric spaces", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "geodesic Laplacian kernels", "label": "geodesic Laplacian kernels", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Adelaide, Australia", "label": "Adelaide, Australia", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Dengxin Dai", "label": "Dengxin Dai", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "efficient vision applications", "label": "efficient vision applications", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Till Kroeger", "label": "Till Kroeger", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Radu Timofte", "label": "Radu Timofte", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Metric Imitation", "label": "Metric Imitation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "improve performance", "label": "improve performance", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "manifold structure", "label": "manifold structure", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "instance-based object retrieval", "label": "instance-based object retrieval", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "image clustering", "label": "image clustering", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "category-based image retrieval", "label": "category-based image retrieval", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "better performance", "label": "better performance", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "vision applications", "label": "vision applications", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "GIST features", "label": "GIST features", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "target features", "label": "target features", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "SIFT-llc", "label": "SIFT-llc", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "source features", "label": "source features", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "object-bank", "label": "object-bank", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "CNN features", "label": "CNN features", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Metric Imitation (MI)", "label": "Metric Imitation (MI)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "original target features", "label": "original target features", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Bosch, A.", "label": "Bosch, A.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Image classification using random forests and ferns", "label": "Image classification using random forests and ferns", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Chatfield, K.", "label": "Chatfield, K.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Dai, D.", "label": "Dai, D.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Ensemble partitioning", "label": "Ensemble partitioning", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "llc", "label": "llc", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Object Retrieval", "label": "Object Retrieval", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Object-bank (OB)", "label": "Object-bank (OB)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Image Clustering", "label": "Image Clustering", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Dana et al.", "label": "Dana et al.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Reflectance and texture of real-world surfaces", "label": "Reflectance and texture of real-world surfaces", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "*ACM Trans. Graph.*", "label": "*ACM Trans. Graph.*", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Fei-Fei et al.", "label": "Fei-Fei et al.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Learning generative visual models", "label": "Learning generative visual models", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Workshop on Generative-Model Based Vision", "label": "Workshop on Generative-Model Based Vision", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Lazebnik et al.", "label": "Lazebnik et al.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Spatial pyramid matching", "label": "Spatial pyramid matching", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "recognizing natural scene categories", "label": "recognizing natural scene categories", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Li \u0026 Fei-Fei", "label": "Li \u0026 Fei-Fei", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "What, where and who?", "label": "What, where and who?", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Li, L.-J.", "label": "Li, L.-J.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Li et al. (2010)", "label": "Li et al. (2010)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Object bank", "label": "Object bank", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "scene classification", "label": "scene classification", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Li, L.-J. et al. (2010)", "label": "Li, L.-J. et al. (2010)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "*NIPS*", "label": "*NIPS*", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Nist\u00b4er, D. \u0026 Stew\u00b4enius, H. (2006)", "label": "Nist\u00b4er, D. \u0026 Stew\u00b4enius, H. (2006)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Vocabulary tree", "label": "Vocabulary tree", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "scalable recognition", "label": "scalable recognition", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Oliva, A. \u0026 Torralba, A. (2001)", "label": "Oliva, A. \u0026 Torralba, A. (2001)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "*IJCV*", "label": "*IJCV*", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Spatial envelope", "label": "Spatial envelope", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "shape of the scene", "label": "shape of the scene", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Tuytelaars, T. et al. (2009)", "label": "Tuytelaars, T. et al. (2009)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "*IJCLP*", "label": "*IJCLP*", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Object discovery", "label": "Object discovery", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "unsupervised", "label": "unsupervised", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Wang, J. et al. (2010)", "label": "Wang, J. et al. (2010)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Locality-constrained linear coding", "label": "Locality-constrained linear coding", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "ETH Zurich", "label": "ETH Zurich", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Francesco Pittaluca", "label": "Francesco Pittaluca", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Sanjeev J. Koppal", "label": "Sanjeev J. Koppal", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Pittaluga_Privacy_Preserving_Optics_2015_CVPR_supplemental.pdf", "label": "Pittaluga_Privacy_Preserving_Optics_2015_CVPR_supplemental.pdf", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Privacy Preseving Optics for Miniature Vision Sensors", "label": "Privacy Preseving Optics for Miniature Vision Sensors", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "privacy-preserving optics", "label": "privacy-preserving optics", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "minature vision sensors", "label": "minature vision sensors", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "impact of defocusing optics", "label": "impact of defocusing optics", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "performance of face recognition algorithms", "label": "performance of face recognition algorithms", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "FLIR One thermal sensor", "label": "FLIR One thermal sensor", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Kinect time-of-flight sensor", "label": "Kinect time-of-flight sensor", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Angular support derivation", "label": "Angular support derivation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "face recognition rates", "label": "face recognition rates", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "determining angular support", "label": "determining angular support", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Privacy-preserving optics", "label": "Privacy-preserving optics", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Facial images", "label": "Facial images", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "face recognition evaluation", "label": "face recognition evaluation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "CSU face identification evaluation system", "label": "CSU face identification evaluation system", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Face identification", "label": "Face identification", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Sensor positioning", "label": "Sensor positioning", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Bolme, D. S. et al.", "label": "Bolme, D. S. et al.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Newton, E. et al.", "label": "Newton, E. et al.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Privacy-preserving techniques", "label": "Privacy-preserving techniques", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Phillips, P. J. et al.", "label": "Phillips, P. J. et al.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Feret evaluation methodology", "label": "Feret evaluation methodology", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Face recognition evaluation", "label": "Face recognition evaluation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Park, Min-Gyu", "label": "Park, Min-Gyu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Leveraging Stereo Matching with Learning-based Con\ufb01dence Measures", "label": "Leveraging Stereo Matching with Learning-based Con\ufb01dence Measures", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "University of Florida", "label": "University of Florida", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Yoon, Kuk-Jin", "label": "Yoon, Kuk-Jin", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Leverging Stereo Matching with Learning-based Con\ufb01dence Measures", "label": "Leverging Stereo Matching with Learning-based Con\ufb01dence Measures", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Pittaluga, Francesco", "label": "Pittaluga, Francesco", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "P. J. The furet evaluation methodology", "label": "P. J. The furet evaluation methodology", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Random Forests", "label": "Random Forests", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Breiman, L.", "label": "Breiman, L.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Random forests", "label": "Random forests", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "real time 3d face analysis", "label": "real time 3d face analysis", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Koppal, Sanjeev J.", "label": "Koppal, Sanjeev J.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Breiman", "label": "Breiman", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "stereo confidence metric", "label": "stereo confidence metric", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "key aspect of stereo vision", "label": "key aspect of stereo vision", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "stereo vision", "label": "stereo vision", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Egnal", "label": "Egnal", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Hirschm\u00fcller", "label": "Hirschm\u00fcller", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "mutual information", "label": "mutual information", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "stereo processing", "label": "stereo processing", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "confidence measures", "label": "confidence measures", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Hu, X.", "label": "Hu, X.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "quantitative evaluation", "label": "quantitative evaluation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "dense matching", "label": "dense matching", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "complex scenes", "label": "complex scenes", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Manduchi, R.", "label": "Manduchi, R.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "distinctiveness maps", "label": "distinctiveness maps", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "image matching", "label": "image matching", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "determining viewpoint", "label": "determining viewpoint", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "viewpoint", "label": "viewpoint", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "coarse pose", "label": "coarse pose", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "keypoint prediction", "label": "keypoint prediction", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "finer details", "label": "finer details", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "constrained setting", "label": "constrained setting", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "bounding boxes", "label": "bounding boxes", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "detection setting", "label": "detection setting", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "viewpoint estimates", "label": "viewpoint estimates", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "keypoint predictions", "label": "keypoint predictions", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Tulsiani_Viewpoints_and_Keypoints_2015_CVPR_paper", "label": "Tulsiani_Viewpoints_and_Keypoints_2015_CVPR_paper", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Shubham Tulsiani", "label": "Shubham Tulsiani", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Jitendra Malik", "label": "Jitendra Malik", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "shubhtuls@eecs.berkeley.edu", "label": "shubhtuls@eecs.berkeley.edu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "malik@eecs.berkeley.edu", "label": "malik@eecs.berkeley.edu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Category-Speci\ufb01c Object Reconstruction from a Single Image", "label": "Category-Speci\ufb01c Object Reconstruction from a Single Image", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "effort", "label": "effort", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "future efforts", "label": "future efforts", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "analysis", "label": "analysis", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "error modes", "label": "error modes", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "effect", "label": "effect", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Keypoint Prediction", "label": "Keypoint Prediction", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Viewpoint Prediction", "label": "Viewpoint Prediction", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Refraction", "label": "Refraction", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Abed Malti", "label": "Abed Malti", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Malti_A_Linear_Least-Squares_2015_CVPR_paper", "label": "Malti_A_Linear_Least-Squares_2015_CVPR_paper", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Fluminance/INRIA", "label": "Fluminance/INRIA", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Fuminance/INRIA", "label": "Fuminance/INRIA", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Adrien Bartoli", "label": "Adrien Bartoli", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Maiti_A_Linear_Least-Squares_2015_CVPR_paper", "label": "Maiti_A_Linear_Least-Squares_2015_CVPR_paper", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "ALCoV/ISIT", "label": "ALCoV/ISIT", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Richard Hartley", "label": "Richard Hartley", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Shape-from-Template methods", "label": "Shape-from-Template methods", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "balancing accuracy, speed, and robustness", "label": "balancing accuracy, speed, and robustness", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "non-linear optimization", "label": "non-linear optimization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "existing approach", "label": "existing approach", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Kalman filtering", "label": "Kalman filtering", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "proposed solution", "label": "proposed solution", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "mechanical constraints", "label": "mechanical constraints", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "finite element methods", "label": "finite element methods", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "accurate reconstruction", "label": "accurate reconstruction", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "efficient reconstruction", "label": "efficient reconstruction", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "linear least-squares SfT method", "label": "linear least-squares SfT method", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "surface", "label": "surface", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "deformation", "label": "deformation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Shape-from-Template (SfT)", "label": "Shape-from-Template (SfT)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "elastic deformations", "label": "elastic deformations", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "linear least-squares estimation", "label": "linear least-squares estimation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "fully linear least-squares SfT method", "label": "fully linear least-squares SfT method", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Finite Element Methods (FEM)", "label": "Finite Element Methods (FEM)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "non-rigid EKF monocular SLAM", "label": "non-rigid EKF monocular SLAM", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "sequential bayesian non-rigid structure from motion", "label": "sequential bayesian non-rigid structure from motion", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Agudo, A.", "label": "Agudo, A.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "FEM models to code non-rigid EKF monocular SLAM", "label": "FEM models to code non-rigid EKF monocular SLAM", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Finite element based sequential bayesian non-rigid structure from motion", "label": "Finite element based sequential bayesian non-rigid structure from motion", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "frequently", "label": "frequently", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "FEM models", "label": "FEM models", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Agudo et al.", "label": "Agudo et al.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "related work", "label": "related work", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "structure from motion", "label": "structure from motion", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Bartoli et al.", "label": "Bartoli et al.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "methodology", "label": "methodology", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Salzmann and Urtasun", "label": "Salzmann and Urtasun", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Moreno-Noguer and Porta", "label": "Moreno-Noguer and Porta", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "shape recovery", "label": "shape recovery", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Finite Element Methods", "label": "Finite Element Methods", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Chaskalovic", "label": "Chaskalovic", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Salzmann and Urutasun", "label": "Salzmann and Urutasun", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "3D reconstruction", "label": "3D reconstruction", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "ape recovery", "label": "ape recovery", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Chaskaloric", "label": "Chaskaloric", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Finite Elements Methods for Engineering Sciences", "label": "Finite Elements Methods for Engineering Sciences", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "background knowledge", "label": "background knowledge", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Reconstructing sharply folding surfaces", "label": "Reconstructing sharply folding surfaces", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Salzmann, M., and Fua, P.", "label": "Salzmann, M., and Fua, P.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Linear local models for monocular reconstruction", "label": "Linear local models for monocular reconstruction", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Matthews, I., and Baker, S.", "label": "Matthews, I., and Baker, S.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Active appearance models revisited", "label": "Active appearance models revisited", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "modeling techniques", "label": "modeling techniques", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Riemannian Coding", "label": "Riemannian Coding", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Kernels", "label": "Kernels", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Mehrtash Harandi", "label": "Mehrtash Harandi", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "covariance descriptors", "label": "covariance descriptors", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Riemannian manifolds", "label": "Riemannian manifolds", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "normalized histograms", "label": "normalized histograms", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "linear subspaces", "label": "linear subspaces", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Riemannianmanifolds", "label": "Riemannianmanifolds", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "2D shape outlines", "label": "2D shape outlines", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "existing solutions", "label": "existing solutions", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "dedicated to specific manifolds", "label": "dedicated to specific manifolds", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "optimization problems", "label": "optimization problems", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "difficult to solve", "label": "difficult to solve", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "kernel-based counterpart", "label": "kernel-based counterpart", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "generalization beyond sparse coding", "label": "generalization beyond sparse coding", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "kernel parameters", "label": "kernel parameters", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Riemannian coding framework", "label": "Riemannian coding framework", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "learning of kernel parameters", "label": "learning of kernel parameters", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "dictionary learning", "label": "dictionary learning", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "sparse coding", "label": "sparse coding", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "flat data", "label": "flat data", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "coding schemes", "label": "coding schemes", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "efficient solutions", "label": "efficient solutions", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Riemannian Manifolds", "label": "Riemannian Manifolds", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Dictionary Learning", "label": "Dictionary Learning", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Coding Theory", "label": "Coding Theory", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Mehrtas Harandi", "label": "Mehrtas Harandi", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Yuting Zhang", "label": "Yuting Zhang", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Improving Object Detection", "label": "Improving Object Detection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Department of Computer Science, Zhejiang University", "label": "Department of Computer Science, Zhejiang University", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Kihyuk Sohn", "label": "Kihyuk Sohn", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Improving ObjectDetection", "label": "Improving ObjectDetection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Department of Electrical Engineering and Computer Science, University of Michigan", "label": "Department of Electrical Engineering and Computer Science, University of Michigan", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Ruben Villegas", "label": "Ruben Villegas", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Gang Pan", "label": "Gang Pan", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Honglak Lee", "label": "Honglak Lee", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "inaccurate localization", "label": "inaccurate localization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "major source of error", "label": "major source of error", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "search algorithm", "label": "search algorithm", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Bayesian optimization", "label": "Bayesian optimization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "candidate regions", "label": "candidate regions", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "localization inaccuracy", "label": "localization inaccuracy", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "methods", "label": "methods", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "baseline method", "label": "baseline method", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "prior over human poses", "label": "prior over human poses", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "object search", "label": "object search", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "proposed methods", "label": "proposed methods", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "two methods", "label": "two methods", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "complementary", "label": "complementary", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "combined methods", "label": "combined methods", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Bayesian Optimization", "label": "Bayesian Optimization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Structured Prediction (Structured SVM)", "label": "Structured Prediction (Structured SVM)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Localization Accuracy", "label": "Localization Accuracy", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Deep Networks", "label": "Deep Networks", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Greedy Layer-Wise Training", "label": "Greedy Layer-Wise Training", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Local Estimation", "label": "Local Estimation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Global Search", "label": "Global Search", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Local Binary Patterns", "label": "Local Binary Patterns", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Representation Learning", "label": "Representation Learning", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Bengio, Y.", "label": "Bengio, Y.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Learning deep architectures for AI", "label": "Learning deep architectures for AI", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Girschick, R.", "label": "Girschick, R.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Everingham, M.", "label": "Everingham, M.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "VOC2007", "label": "VOC2007", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "PASCAL VOC challenge description", "label": "PASCAL VOC challenge description", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Visual Object Classes Challenge", "label": "Visual Object Classes Challenge", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Pascal Network", "label": "Pascal Network", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Deng, J.", "label": "Deng, J.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Donahue, J.", "label": "Donahue, J.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "DeCAF", "label": "DeCAF", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Visual Recognition", "label": "Visual Recognition", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "CoRR", "label": "CoRR", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Erhan", "label": "Erhan", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Erhan, D.", "label": "Erhan, D.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Dongping Li", "label": "Dongping Li", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Zhejiang University", "label": "Zhejiang University", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Department of Electrical Engineering and Computer Science", "label": "Department of Electrical Engineering and Computer Science", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "University of Michigan", "label": "University of Michigan", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Image Warping", "label": "Image Warping", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Kun Zhou", "label": "Kun Zhou", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "honglak@umich.edu", "label": "honglak@umich.edu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Li_A_Geodesic-Preserving_Method_2015_CVPR_supplemental.pdf", "label": "Li_A_Geodesic-Preserving_Method_2015_CVPR_supplemental.pdf", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "high-quality warped images", "label": "high-quality warped images", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "maintain shape", "label": "maintain shape", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "distortions", "label": "distortions", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "core of method", "label": "core of method", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "preserving geodesic distances", "label": "preserving geodesic distances", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "local smoothness", "label": "local smoothness", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "unwanted artifacts", "label": "unwanted artifacts", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "shape preservation terms", "label": "shape preservation terms", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "boundary preservation terms", "label": "boundary preservation terms", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "geodesic preservation terms", "label": "geodesic preservation terms", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Shape and Boundary Preservation", "label": "Shape and Boundary Preservation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Energy Minimization", "label": "Energy Minimization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Gausss-Newton Method", "label": "Gausss-Newton Method", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Zhang, G. et al.", "label": "Zhang, G. et al.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "A shape-preserivng approach to image resizing", "label": "A shape-preserivng approach to image resizing", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Rotation matrix R\u03b8,\u03c6", "label": "Rotation matrix R\u03b8,\u03c6", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Eqn. (1)", "label": "Eqn. (1)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Shape-preserving term ES(V)", "label": "Shape-preserving term ES(V)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Eqn. (7)", "label": "Eqn. (7)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "shape-preserving approach", "label": "shape-preserving approach", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Eqn. (7) - Shape-preserving term ES(V)", "label": "Eqn. (7) - Shape-preserving term ES(V)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "local smoothness preservation", "label": "local smoothness preservation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Eqn. (4) - Local smoothness preservation EC(V)", "label": "Eqn. (4) - Local smoothness preservation EC(V)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Eqn. (5) - Combined energy function E(V)", "label": "Eqn. (5) - Combined energy function E(V)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "overall energy function", "label": "overall energy function", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Q", "label": "Q", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "orthogonal matrices", "label": "orthogonal matrices", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "mathematical concept", "label": "mathematical concept", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "mathematics", "label": "mathematics", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "E(V)", "label": "E(V)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "various terms", "label": "various terms", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Antonio Agudo", "label": "Antonio Agudo", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Simultaneous Pose and Non-Rigid Shape with Particle Dynamics", "label": "Simultaneous Pose and Non-Rigid Shape with Particle Dynamics", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Instituto de Invesigaci\u00b4on en Ingenier\u00b4\u0131a de Arag\u00b4on (I3A)", "label": "Instituto de Invesigaci\u00b4on en Ingenier\u00b4\u0131a de Arag\u00b4on (I3A)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Francesc Moreno-Noguer", "label": "Francesc Moreno-Noguer", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Institut de Rob`otica i Inform`atica Industrial (CSI-UPC)", "label": "Institut de Rob`otica i Inform`atica Industrial (CSI-UPC)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Simultaneous Pose and Non-Rigid Shape", "label": "Simultaneous Pose and Non-Rigid Shape", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "particle dynamics", "label": "particle dynamics", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "geodesic preservation", "label": "geodesic preservation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "non-rigid shape", "label": "non-rigid shape", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "simultaneous pose", "label": "simultaneous pose", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "camera pose", "label": "camera pose", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "absolute pose problem", "label": "absolute pose problem", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "camera position", "label": "camera position", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "camera orientation", "label": "camera orientation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "translational velocity", "label": "translational velocity", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "angular velocity", "label": "angular velocity", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "spatiotemporal filters", "label": "spatiotemporal filters", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "metric-learning framework", "label": "metric-learning framework", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "ensemble of particles", "label": "ensemble of particles", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "annotation proposals", "label": "annotation proposals", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "particle", "label": "particle", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Newton\u2019s second law of motion", "label": "Newton\u2019s second law of motion", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "dynamic model", "label": "dynamic model", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "bundle adjustment framework", "label": "bundle adjustment framework", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "validation", "label": "validation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "real video sequences", "label": "real video sequences", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "motion", "label": "motion", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "articulated", "label": "articulated", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "non-rigid", "label": "non-rigid", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "shapes", "label": "shapes", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "discontinuous", "label": "discontinuous", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "batch methods", "label": "batch methods", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "System", "label": "System", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Competing Batch Methods", "label": "Competing Batch Methods", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Uncanny Valley Effect", "label": "Uncanny Valley Effect", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Qualitative Evaluations", "label": "Qualitative Evaluations", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Particle Dynamics", "label": "Particle Dynamics", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Bundle Adjustment", "label": "Bundle Adjustment", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Monocular Video Analysis", "label": "Monocular Video Analysis", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Instituto de Invesigaci\u00b4on en Ingenier\u00b4\u0131a de Arag\u00b4on (I3a)", "label": "Instituto de Invesigaci\u00b4on en Ingenier\u00b4\u0131a de Arag\u00b4on (I3a)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Universidad de Zaragoza", "label": "Universidad de Zaragoza", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Simone Frintrop", "label": "Simone Frintrop", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Rheinische Friedrich-Wilhelms-Universit\u00a8at Bonn", "label": "Rheinische Friedrich-Wilhelms-Universit\u00a8at Bonn", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Rheinische Friedrich-Wilhelms-Universit\u00e4t Bonn", "label": "Rheinische Friedrich-Wilhelms-Universit\u00e4t Bonn", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "frintrop@iai.uni-bonn.de", "label": "frintrop@iai.uni-bonn.de", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Thomas Werner", "label": "Thomas Werner", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Germ\u00e1n M. Garc\u00eda", "label": "Germ\u00e1n M. Garc\u00eda", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Itti et al.", "label": "Itti et al.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "adaptations", "label": "adaptations", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "scale-space structure", "label": "scale-space structure", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "twin pyramid", "label": "twin pyramid", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "adaptation", "label": "adaptation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "VOCUS2", "label": "VOCUS2", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "ration framework", "label": "ration framework", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "segment-based salience maps", "label": "segment-based salience maps", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "modern applications", "label": "modern applications", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "revisiting foundational approaches", "label": "revisiting foundational approaches", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "t-based salience maps", "label": "t-based salience maps", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Saliency Models", "label": "Saliency Models", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Itti Model", "label": "Itti Model", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "A cognitive approach for object discovery", "label": "A cognitive approach for object discovery", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Discriminant salieny", "label": "Discriminant salieny", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "TPAMI", "label": "TPAMI", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "State-of-the-art in visual attention modeling", "label": "State-of-the-art in visual attention modeling", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Scale-space representation", "label": "Scale-space representation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "asri", "label": "asri", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "L. Itti", "label": "L. Itti", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "salienicy-based visual attention model", "label": "salienicy-based visual attention model", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "N. D. B. Bruce", "label": "N. D. B. Bruce", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "information theoretic approach", "label": "information theoretic approach", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "L. Hurvich", "label": "L. Hurvich", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "opponent-process theory", "label": "opponent-process theory", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Bonn", "label": "Bonn", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Jiaolong Yang", "label": "Jiaolong Yang", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Beijing Lab of Intelligent Information Technology", "label": "Beijing Lab of Intelligent Information Technology", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Yang_Dense_Accurate_Optical_2015_CVPR_paper", "label": "Yang_Dense_Accurate_Optical_2015_CVPR_paper", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "multi-model fitting scheme", "label": "multi-model fitting scheme", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "KITTI", "label": "KITTI", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "MPI Sintel", "label": "MPI Sintel", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Optical Flow", "label": "Optical Flow", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Scene Flow", "label": "Scene Flow", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Piecewise parametric models", "label": "Piecewise parametric models", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Energy minimization", "label": "Energy minimization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Homography transformation", "label": "Homography transformation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Baker et al. (2011)", "label": "Baker et al. (2011)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "database and evaluation methodology", "label": "database and evaluation methodology", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Bao et al. (2014)", "label": "Bao et al. (2014)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Fast edge-preserving patchmatch", "label": "Fast edge-preserving patchmatch", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Barnes et al. (2009)", "label": "Barnes et al. (2009)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "PatchMatch", "label": "PatchMatch", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "structural image editing", "label": "structural image editing", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Piecewise image registration", "label": "Piecewise image registration", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "label": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Multiway cut", "label": "Multiway cut", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "stereo and motion", "label": "stereo and motion", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "slanted surfaces", "label": "slanted surfaces", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "robust estimation", "label": "robust estimation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "multiple motions", "label": "multiple motions", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "parametric flow fields", "label": "parametric flow fields", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "piecewise-smooth flow fields", "label": "piecewise-smooth flow fields", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Black", "label": "Black", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Anandan", "label": "Anandan", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Black, M. J.", "label": "Black, M. J.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "The robust estimation of multiple motions", "label": "The robust estimation of multiple motions", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Estimating optical flow in segmented images", "label": "Estimating optical flow in segmented images", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Anandan, P.", "label": "Anandan, P.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Jepson, A. D.", "label": "Jepson, A. D.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Bleyer, M.", "label": "Bleyer, M.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "PatchMatch Stereo", "label": "PatchMatch Stereo", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Rhemann, C.", "label": "Rhemann, C.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Rother, C.", "label": "Rother, C.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "PatchMatch Stere", "label": "PatchMatch Stere", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Braux-Zin et al.", "label": "Braux-Zin et al.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "A general dense image matching framework", "label": "A general dense image matching framework", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Thomas Mauthner", "label": "Thomas Mauthner", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Encoding Based Saliency Detection for Videos and Images", "label": "Encoding Based Saliency Detection for Videos and Images", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Horst Possegger", "label": "Horst Possegger", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Mauthner_Encoding_Based_Saliency_2015_CVPR_paper", "label": "Mauthner_Encoding_Based_Saliency_2015_CVPR_paper", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Georg Waltner", "label": "Georg Waltner", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Mauthner_Encoding_Based_Salieney_2015_CVPR_paper", "label": "Mauthner_Encoding_Based_Salieney_2015_CVPR_paper", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "waltner@icg.tugraz.at", "label": "waltner@icg.tugraz.at", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Horst Bischof", "label": "Horst Bischof", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Mauthne_Encoding_Based_Salieney_2015_CVPR_paper", "label": "Mauthne_Encoding_Based_Salieney_2015_CVPR_paper", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "bischof@icg.tugraz.at", "label": "bischof@icg.tugraz.at", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "eye-traking data", "label": "eye-traking data", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "reliance on human gaze", "label": "reliance on human gaze", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "bias", "label": "bias", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "joint feature distributions", "label": "joint feature distributions", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "salience computation", "label": "salience computation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Gestalt principle of figure-ground segregation", "label": "Gestalt principle of figure-ground segregation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "re-ground segregation", "label": "re-ground segregation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Video Salience Detection", "label": "Video Salience Detection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Gestalt Principles", "label": "Gestalt Principles", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Encoding Methods", "label": "Encoding Methods", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Human Activity Recognition", "label": "Human Activity Recognition", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Itti, L.", "label": "Itti, L.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "model of salience-based visual attention", "label": "model of salience-based visual attention", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Alexe, B.", "label": "Alexe, B.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "What is an object?", "label": "What is an object?", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Liu, T.", "label": "Liu, T.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Learning to Detect A Salient Object", "label": "Learning to Detect A Salient Object", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Johansson, G.", "label": "Johansson, G.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "model for analysis of biological motion", "label": "model for analysis of biological motion", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Gorelick, L.", "label": "Gorelick, L.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Actions as Space-Time Shapes", "label": "Actions as Space-Time Shapes", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Gorelick", "label": "Gorelick", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Blank", "label": "Blank", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Shechtman", "label": "Shechtman", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Irani", "label": "Irani", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Basri", "label": "Basri", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Borji", "label": "Borji", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Sihte", "label": "Sihte", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Salient Object Determination: A Benchmark", "label": "Salient Object Determination: A Benchmark", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Itti", "label": "Itti", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Guo", "label": "Guo", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Spatio-temporal Saliency detection", "label": "Spatio-temporal Saliency detection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Zhang", "label": "Zhang", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Judd", "label": "Judd", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Ehinger", "label": "Ehinger", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Durand", "label": "Durand", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Harel", "label": "Harel", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Graph-based Visual Saliency", "label": "Graph-based Visual Saliency", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Koch", "label": "Koch", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Perona", "label": "Perona", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Caltech-256 object category dataset", "label": "Caltech-256 object category dataset", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Rahtu", "label": "Rahtu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Segmenting Salient Objects from Images and Videos", "label": "Segmenting Salient Objects from Images and Videos", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Kannala", "label": "Kannala", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Salo", "label": "Salo", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Heikkil\u00a8a", "label": "Heikkil\u00a8a", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Mauthner", "label": "Mauthner", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Institute for Computer Graphics and Vision, Graz University of Technology", "label": "Institute for Computer Graphics and Vision, Graz University of Technology", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Possegger", "label": "Possegger", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Waltner", "label": "Waltner", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "possegger@icg.tugraz.at", "label": "possegger@icg.tugraz.at", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Guanbin Li", "label": "Guanbin Li", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Visual Saliency Based on Multiscale Deep Features", "label": "Visual Saliency Based on Multiscale Deep Features", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Yizhou Yu", "label": "Yizhou Yu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Multiple Scales", "label": "Multiple Scales", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "MDF approach", "label": "MDF approach", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Salience maps", "label": "Salience maps", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "accurate salience maps", "label": "accurate salience maps", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Spectral residual approach", "label": "Spectral residual approach", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Salience detection", "label": "Salience detection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Salient object detection", "label": "Salient object detection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Yuan et al. (2013)", "label": "Yuan et al. (2013)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Perazzi et al. (2012)", "label": "Perazzi et al. (2012)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Wei et al. (2012)", "label": "Wei et al. (2012)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Yan et al. (2013)", "label": "Yan et al. (2013)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Yang et al. (2013)", "label": "Yang et al. (2013)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Salient region detection", "label": "Salient region detection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Salience filters", "label": "Salience filters", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Zhu et al. (2014)", "label": "Zhu et al. (2014)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Sun et al. (2014)", "label": "Sun et al. (2014)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "discriminative manifold-based approach", "label": "discriminative manifold-based approach", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Minsu Cho", "label": "Minsu Cho", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Unsupervised Object Discovery and Localization", "label": "Unsupervised Object Discovery and Localization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Inria", "label": "Inria", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "bottom-up region proposals", "label": "bottom-up region proposals", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "object discovery", "label": "object discovery", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "object localization", "label": "object localization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "part-based matching", "label": "part-based matching", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "mixed-class datasets", "label": "mixed-class datasets", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Unsupervised Object Localization", "label": "Unsupervised Object Localization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "setting", "label": "setting", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "fully unsupervised", "label": "fully unsupervised", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "region proposals", "label": "region proposals", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "candidate bounding boxes", "label": "candidate bounding boxes", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "correspondence", "label": "correspondence", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "probabilistic Hough transform", "label": "probabilistic Hough transform", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Hough transform", "label": "Hough transform", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "candidate correspondence", "label": "candidate correspondence", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "dominant objects", "label": "dominant objects", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "comparing scores", "label": "comparing scores", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "selecting regions", "label": "selecting regions", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "regions", "label": "regions", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "candidate objects", "label": "candidate objects", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "standard benchmarks", "label": "standard benchmarks", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "rd benchmarks", "label": "rd benchmarks", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Probabiltistic Hough transform", "label": "Probabiltistic Hough transform", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "multiple object detection", "label": "multiple object detection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Alexe et al.", "label": "Alexe et al.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Measuring the object-ness of image windows", "label": "Measuring the object-ness of image windows", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Ballard", "label": "Ballard", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Generalizing the Hough transform", "label": "Generalizing the Hough transform", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Joulin et al.", "label": "Joulin et al.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Discriminative clustering for image co-segmentation", "label": "Discriminative clustering for image co-segmentation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Cho et al.", "label": "Cho et al.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Learning graphs to match", "label": "Learning graphs to match", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Proceedings of the IEEE International Conference on Computer Vision", "label": "Proceedings of the IEEE International Conference on Computer Vision", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Unsupervised object localization", "label": "Unsupervised object localization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Learning Graphs", "label": "Learning Graphs", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Efficient Image Localization", "label": "Efficient Image Localization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Pictoiral Structures", "label": "Pictoiral Structures", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "IJCV", "label": "IJCV", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Architectural modeling", "label": "Architectural modeling", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Dynamic textures", "label": "Dynamic textures", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Dynamic texture detection based on motion analysis", "label": "Dynamic texture detection based on motion analysis", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Ijaz Akhter", "label": "Ijaz Akhter", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Pose-Conditioned Joint Angle Limits", "label": "Pose-Conditioned Joint Angle Limits", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Max Planck Institute for Intelligent Systems", "label": "Max Planck Institute for Intelligent Systems", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "ijaz.akhter@tuebingen.mpg.de", "label": "ijaz.akhter@tuebingen.mpg.de", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Akhter_Pose-Conditioned_Joint_Angle_2015_CVPR_paper.pdf", "label": "Akhter_Pose-Conditioned_Joint_Angle_2015_CVPR_paper.pdf", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Michael J. Black", "label": "Michael J. Black", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Pose-Conditioning Joint Angle Limits", "label": "Pose-Conditioning Joint Angle Limits", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "3D human pose estimation", "label": "3D human pose estimation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "analysis of people in images and video", "label": "analysis of people in images and video", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "joint limits", "label": "joint limits", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "pose", "label": "pose", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "motion capture dataset", "label": "motion capture dataset", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "range of human poses", "label": "range of human poses", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "2D joint locations", "label": "2D joint locations", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "detections", "label": "detections", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Leeds sports pose dataset", "label": "Leeds sports pose dataset", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "he-art results", "label": "he-art results", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "2D to 3D pose estimation", "label": "2D to 3D pose estimation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "superior results", "label": "superior results", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "automatic detections", "label": "automatic detections", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Andriluka, M. et al. (2010)", "label": "Andriluka, M. et al. (2010)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "monocular 3D pose estimation and tracking", "label": "monocular 3D pose estimation and tracking", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Barr`on, C. \u0026 Kakadiaris, I. (2001)", "label": "Barr`on, C. \u0026 Kakadiaris, I. (2001)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "estimating anthropometry and pose", "label": "estimating anthropometry and pose", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "BenAbdelkader, C. \u0026 Yacoob, Y. (2008)", "label": "BenAbdelkader, C. \u0026 Yacoob, Y. (2008)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "statistical estimation of human anthropometry", "label": "statistical estimation of human anthropometry", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Human Pose Reconstruction", "label": "Human Pose Reconstruction", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Motion Capture Data", "label": "Motion Capture Data", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Prior Models", "label": "Prior Models", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Bourdev \u0026 Malik", "label": "Bourdev \u0026 Malik", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "International Conference on Computer Vision", "label": "International Conference on Computer Vision", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Poselets", "label": "Poselets", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "body part detector", "label": "body part detector", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "3D human pose annotations", "label": "3D human pose annotations", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Chen, Nie, \u0026 Ji", "label": "Chen, Nie, \u0026 Ji", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Guan et al.", "label": "Guan et al.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Int. Conf. on Computer Vision (ICCV)", "label": "Int. Conf. on Computer Vision (ICCV)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Estimating human shape", "label": "Estimating human shape", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "single image", "label": "single image", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Grochow et al.", "label": "Grochow et al.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Style-based inverse kinematics", "label": "Style-based inverse kinematics", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "human anthropometry", "label": "human anthropometry", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "single uncalibrated image", "label": "single uncalibrated image", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "three-dimensional multivariate model", "label": "three-dimensional multivariate model", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Clinical Biomechanics", "label": "Clinical Biomechanics", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Herda et al.", "label": "Herda et al.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Hierarchical implicit surface joint limits", "label": "Hierarchical implicit surface joint limits", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Lin et al.", "label": "Lin et al.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "sketching interface", "label": "sketching interface", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Microsoft COCO: Common objects in context", "label": "Microsoft COCO: Common objects in context", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "IEEE Transactions on Visualization and Computer Graphics", "label": "IEEE Transactions on Visualization and Computer Graphics", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Tuebingen", "label": "Tuebingen", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Ran Tao", "label": "Ran Tao", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Arnold W.M. Smeulders", "label": "Arnold W.M. Smeulders", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Tao_Attributes_and_Categories_2015_CVPR_paper", "label": "Tao_Attributes_and_Categories_2015_CVPR_paper", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "instance search methods", "label": "instance search methods", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "arbitrary 3D objects", "label": "arbitrary 3D objects", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "shoes", "label": "shoes", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "category-specific attributes", "label": "category-specific attributes", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "appearance variations", "label": "appearance variations", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "category-level information", "label": "category-level information", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "combination", "label": "combination", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "approaches relying on low-level features", "label": "approaches relying on low-level features", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "representing query image", "label": "representing query image", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "query image", "label": "query image", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "robust representation", "label": "robust representation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Appearance Variation", "label": "Appearance Variation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "distinction from similar instances", "label": "distinction from similar instances", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "distinction", "label": "distinction", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "similar instances", "label": "similar instances", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Generic Instance Search", "label": "Generic Instance Search", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Core Challenge", "label": "Core Challenge", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Attribute Representation", "label": "Attribute Representation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Large Scale Visual Recognition Challenge", "label": "Large Scale Visual Recognition Challenge", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Attribute Transfer", "label": "Attribute Transfer", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "detecting unseen object classes", "label": "detecting unseen object classes", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Multiple queries", "label": "Multiple queries", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "large scale specific object retrieval", "label": "large scale specific object retrieval", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Aranandjelovic", "label": "Aranandjelovic", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Zisserman", "label": "Zisserman", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Naphade", "label": "Naphade", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "concept ontology", "label": "concept ontology", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "IEEE MultiMedia", "label": "IEEE MultiMedia", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Perdoch", "label": "Perdoch", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "efficient representation", "label": "efficient representation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Farhad", "label": "Farhad", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "describing objects", "label": "describing objects", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "ISLA", "label": "ISLA", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "object attributes", "label": "object attributes", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Recognition algorithms", "label": "Recognition algorithms", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "last layer output", "label": "last layer output", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "spatially coarse", "label": "spatially coarse", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "earlier layers", "label": "earlier layers", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "precise in localization", "label": "precise in localization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "semantics", "label": "semantics", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "hypercolumn", "label": "hypercolumn", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "vector of activations", "label": "vector of activations", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "pixel", "label": "pixel", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "CNN units", "label": "CNN units", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "hypercolumns", "label": "hypercolumns", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "pixel descriptors", "label": "pixel descriptors", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "simultaneous detection", "label": "simultaneous detection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "localization task", "label": "localization task", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "keypoint localization", "label": "keypoint localization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "part labeling", "label": "part labeling", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Keypoint Localization", "label": "Keypoint Localization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Spatial Pyramid Pooling", "label": "Spatial Pyramid Pooling", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Multi-scale Feature Integration", "label": "Multi-scale Feature Integration", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Arbel\u00e1ez, P.", "label": "Arbel\u00e1ez, P.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Multiscale Combinatorial Grouping", "label": "Multiscale Combinatorial Grouping", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "He, K.", "label": "He, K.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Barron, J. T.", "label": "Barron, J. T.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Volumetric Semantic Segmentation", "label": "Volumetric Semantic Segmentation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Hypercolumn Representation", "label": "Hypercolumn Representation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Barron et al.", "label": "Barron et al.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Hubel \u0026 Wiesel", "label": "Hubel \u0026 Wiesel", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "The Journal of Physiology", "label": "The Journal of Physiology", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Bo \u0026 Fowlakes", "label": "Bo \u0026 Fowlakes", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Ionescu et al.", "label": "Ionescu et al.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Jones \u0026 Malik", "label": "Jones \u0026 Malik", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Koenderink \u0026 van Doorn", "label": "Koenderink \u0026 van Doorn", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Biological Cybernetics", "label": "Biological Cybernetics", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Volumetric semantic segmentation", "label": "Volumetric semantic segmentation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "pyramid context features", "label": "pyramid context features", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Visual cortex", "label": "Visual cortex", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Hubel \u0026 Wiesel\u0027s work", "label": "Hubel \u0026 Wiesel\u0027s work", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Pedestrian parsing", "label": "Pedestrian parsing", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "shape-based methods", "label": "shape-based methods", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Biological cybernetics", "label": "Biological cybernetics", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "visual system", "label": "visual system", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Malik", "label": "Malik", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Xie", "label": "Xie", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "DeepShape", "label": "DeepShape", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "shape descriptor", "label": "shape descriptor", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "3D shape matching and retrieval", "label": "3D shape matching and retrieval", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "multiple discriminative auto-encoders", "label": "multiple discriminative auto-encoders", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "3D model", "label": "3D model", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "multiscale shape distribution", "label": "multiscale shape distribution", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "auto-encoder", "label": "auto-encoder", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "discriminative deep auto-encoder", "label": "discriminative deep auto-encoder", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "shape distribution", "label": "shape distribution", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Fisher discrimination criterion", "label": "Fisher discrimination criterion", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "neurons", "label": "neurons", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "hidden layer neurons", "label": "hidden layer neurons", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "geometric variations", "label": "geometric variations", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Proposed Method", "label": "Proposed Method", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Agathos et al. (2009)", "label": "Agathos et al. (2009)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Retrieval of 3D articulated objects", "label": "Retrieval of 3D articulated objects", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Assfalg et al. (2007)", "label": "Assfalg et al. (2007)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Content-based retrieval of 3D objects", "label": "Content-based retrieval of 3D objects", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Belongie et al. (2000)", "label": "Belongie et al. (2000)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Shape context", "label": "Shape context", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Shape Context", "label": "Shape Context", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "shape matching and object recognition", "label": "shape matching and object recognition", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Geometric Feature Learning", "label": "Geometric Feature Learning", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Deep Auto-encoders", "label": "Deep Auto-encoders", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Ric variations", "label": "Ric variations", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "SHREC\u002710 Shape dataset", "label": "SHREC\u002710 Shape dataset", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "shape matching", "label": "shape matching", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Deep Architectures", "label": "Deep Architectures", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "AI", "label": "AI", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Shape Google", "label": "Shape Google", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "geometric words", "label": "geometric words", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Isometry-invariant distances", "label": "Isometry-invariant distances", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Bronstein et al. (2006)", "label": "Bronstein et al. (2006)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Gromov-Hausdorff framework", "label": "Gromov-Hausdorff framework", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "non-rigid shape matching", "label": "non-rigid shape matching", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Diffusion geometry", "label": "Diffusion geometry", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "topologically-robust matching", "label": "topologically-robust matching", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Bronstein et al. (2011)", "label": "Bronstein et al. (2011)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Mahmoudi, M.", "label": "Mahmoudi, M.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "A Gromov-Hausdorff framework", "label": "A Gromov-Hausdorff framework", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Chen, D.-Y.", "label": "Chen, D.-Y.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "On visual similarity based 3D model retrieval", "label": "On visual similarity based 3D model retrieval", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "3D model retrieval", "label": "3D model retrieval", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Chen, X.", "label": "Chen, X.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "A benchmark for 3D mesh segmentation", "label": "A benchmark for 3D mesh segmentation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "A benchmark", "label": "A benchmark", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "3D mesh segmentation", "label": "3D mesh segmentation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "De Goes, F.", "label": "De Goes, F.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "A hierarchical segmentation", "label": "A hierarchical segmentation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "articulated bodies", "label": "articulated bodies", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Jin Xie", "label": "Jin Xie", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "New York University Abu Dhabi", "label": "New York University Abu Dhabi", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "jin.xie@nyu.edu", "label": "jin.xie@nyu.edu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Yi Fang", "label": "Yi Fang", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "yfang@nyu.edu", "label": "yfang@nyu.edu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Fan Zhu", "label": "Fan Zhu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Department of Electrical and Computer Engineering", "label": "Department of Electrical and Computer Engineering", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Department of Electrical and ComputerEngineering", "label": "Department of Electrical and ComputerEngineering", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Edward Wong", "label": "Edward Wong", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Polytechnic School of Engineering", "label": "Polytechnic School of Engineering", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "New York University", "label": "New York University", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Bingbing Ni", "label": "Bingbing Ni", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "ADSC Singapore", "label": "ADSC Singapore", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Pierre Moulin", "label": "Pierre Moulin", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Department of ECE", "label": "Department of ECE", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "golf", "label": "golf", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Action", "label": "Action", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "punch", "label": "punch", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Fisher vector", "label": "Fisher vector", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Ni_Motion_Part_Regularization_2015_CVPR_paper", "label": "Ni_Motion_Part_Regularization_2015_CVPR_paper", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Motion Part Regularization framework", "label": "Motion Part Regularization framework", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "action recognition", "label": "action recognition", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "dense trajectories", "label": "dense trajectories", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "3D natural scenes", "label": "3D natural scenes", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "depth cameras", "label": "depth cameras", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "discriminativeness weighted Fisher vector representation", "label": "discriminativeness weighted Fisher vector representation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "traditional Fisher vector", "label": "traditional Fisher vector", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "sparse selection of trajectory groups", "label": "sparse selection of trajectory groups", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "action class discriminative term", "label": "action class discriminative term", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "discriminative term", "label": "discriminative term", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "auxiliary variables", "label": "auxiliary variables", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Optimization Algorithm", "label": "Optimization Algorithm", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Motion Part", "label": "Motion Part", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Discriminative Weights", "label": "Discriminative Weights", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Dense Trajectories", "label": "Dense Trajectories", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Wang et al. (2011)", "label": "Wang et al. (2011)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Behavior Recognition", "label": "Behavior Recognition", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Spatio-Temporal Grouping", "label": "Spatio-Temporal Grouping", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "LIBSVM", "label": "LIBSVM", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "library", "label": "library", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Automatic Annotation", "label": "Automatic Annotation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Human Actions", "label": "Human Actions", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "O. Duchenne", "label": "O. Duchenne", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Automatic annotation of human actions in video", "label": "Automatic annotation of human actions in video", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "I. Laptev", "label": "I. Laptev", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "J. Sivic", "label": "J. Sivic", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "J. Ponce", "label": "J. Ponce", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "H. Wang", "label": "H. Wang", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Action recognition with improved trajectories", "label": "Action recognition with improved trajectories", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "C. Schmid", "label": "C. Schmid", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Multi-fold mil training", "label": "Multi-fold mil training", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "P. Felzenszwalb", "label": "P. Felzenszwalb", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Object detection with discriminatively trained part based models", "label": "Object detection with discriminatively trained part based models", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "R. Girshick", "label": "R. Girshick", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Efficient regression of general-activity human poses", "label": "Efficient regression of general-activity human poses", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "D. McAlleser", "label": "D. McAlleser", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "D. Ramanan", "label": "D. Ramanan", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Object description with discriminatively trained part based models", "label": "Object description with discriminatively trained part based models", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "J. Wang", "label": "J. Wang", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Mining actionlet ensemble for action recognition with depth cameras", "label": "Mining actionlet ensemble for action recognition with depth cameras", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Z. Liu", "label": "Z. Liu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Y. Wu", "label": "Y. Wu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "J. Yuan", "label": "J. Yuan", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "M. Jain", "label": "M. Jain", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Better exploiting motion for better action recognition", "label": "Better exploiting motion for better action recognition", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "H. Jegou", "label": "H. Jegou", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "P. Bouthemy", "label": "P. Bouthemy", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Y.-G. Jiang", "label": "Y.-G. Jiang", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Trajectory-based modeling of human actions with motion reference points", "label": "Trajectory-based modeling of human actions with motion reference points", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Q. Dai", "label": "Q. Dai", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "X. Xue", "label": "X. Xue", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "W. Liu", "label": "W. Liu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "C.-W. Ngo", "label": "C.-W. Ngo", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Pierre Bounameaux", "label": "Pierre Bounameaux", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "gaze correction solutions", "label": "gaze correction solutions", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "additional hardware", "label": "additional hardware", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "pixel replacement operations", "label": "pixel replacement operations", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "eyes", "label": "eyes", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Monocular Gaze Correction", "label": "Monocular Gaze Correction", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Localized Pixel Replacement", "label": "Localized Pixel Replacement", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Amit", "label": "Amit", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Shape Quantization", "label": "Shape Quantization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Doll\u00e1r", "label": "Doll\u00e1r", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Structured Forests", "label": "Structured Forests", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Fast Edge Detection", "label": "Fast Edge Detection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Fanelli", "label": "Fanelli", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Random Forests for 3D Face Analysis", "label": "Random Forests for 3D Face Analysis", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Fanelli, G.", "label": "Fanelli, G.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Random forests for real time 3d face analysis", "label": "Random forests for real time 3d face analysis", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Gaze correction", "label": "Gaze correction", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "single webcam", "label": "single webcam", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Gall, J.", "label": "Gall, J.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Class-specific hough forests for object detection", "label": "Class-specific hough forests for object detection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Hough forests", "label": "Hough forests", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Jones, A.", "label": "Jones, A.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Achieving eye contact in a one-to-many 3D video teleconferecing system", "label": "Achieving eye contact in a one-to-many 3D video teleconferecing system", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "eye contact", "label": "eye contact", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "3D video teleconferencing system", "label": "3D video teleconferencing system", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Kazemi, V.", "label": "Kazemi, V.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "One milliseccond face alignment with an ensemble of regression trees", "label": "One milliseccond face alignment with an ensemble of regression trees", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "One milliseccond face alignment", "label": "One milliseccond face alignment", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Sullivan, J.", "label": "Sullivan, J.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "face alignment", "label": "face alignment", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Kuster, C.", "label": "Kuster, C.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Ren, S.", "label": "Ren, S.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Face alignment", "label": "Face alignment", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Cao, X.", "label": "Cao, X.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "practical transfer learning algorithm", "label": "practical transfer learning algorithm", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Kononenko, Daniil", "label": "Kononenko, Daniil", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Skolkovo Institute of Science and Technology (Skoltech)", "label": "Skolkovo Institute of Science and Technology (Skoltech)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Lempitsky, Victor", "label": "Lempitsky, Victor", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Skolkovo Institute of Science and Technology (Skeltech)", "label": "Skolkovo Institute of Science and Technology (Skeltech)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Zhao, Kaili", "label": "Zhao, Kaili", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Joint Patch and Multi-label Learning", "label": "Joint Patch and Multi-label Learning", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Facial Action Unit Detection", "label": "Facial Action Unit Detection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Zhao_Joint_Patch_and_2015_CVPR_paper.pdf", "label": "Zhao_Joint_Patch_and_2015_CVPR_paper.pdf", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Facial Action Coding System", "label": "Facial Action Coding System", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "describing facial movements", "label": "describing facial movements", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "human face", "label": "human face", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "facial action units", "label": "facial action units", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "facial action unit detection", "label": "facial action unit detection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Action Units", "label": "Action Units", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "JPML", "label": "JPML", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Multi-label Learning", "label": "Multi-label Learning", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "highest average F1 scores", "label": "highest average F1 scores", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "CK+", "label": "CK+", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "BP4D", "label": "BP4D", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Machine learning", "label": "Machine learning", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "facial expression recognition", "label": "facial expression recognition", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Support-vector networks", "label": "Support-vector networks", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "FACIAL Action Coding System (FACS)", "label": "FACIAL Action Coding System (FACS)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Action Unit (AU) Detection", "label": "Action Unit (AU) Detection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Patch Learning", "label": "Patch Learning", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Alternating direction method of multipliers", "label": "Alternating direction method of multipliers", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Alternating Direction Method of Multipliers", "label": "Alternating Direction Method of Multipliers", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Affective Computing", "label": "Affective Computing", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Facial Action Unit Event Detection", "label": "Facial Action Unit Event Detection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "cascade of tasks", "label": "cascade of tasks", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "X. Ding", "label": "X. Ding", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "P. Ekman", "label": "P. Ekman", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "F. De la Torre", "label": "F. De la Torre", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Intraface", "label": "Intraface", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Selective transfer machine", "label": "Selective transfer machine", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "J. C. Hager", "label": "J. C. Hager", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "personalization in facial action unit detection", "label": "personalization in facial action unit detection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "W.-S. Chu", "label": "W.-S. Chu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "J. F. Cohn", "label": "J. F. Cohn", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Facing imbalanced data", "label": "Facing imbalanced data", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "imbalanced datasets", "label": "imbalanced datasets", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "L. A. Jeni", "label": "L. A. Jeni", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Data-free prior model", "label": "Data-free prior model", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "data-free approach", "label": "data-free approach", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "facial action unit recognition", "label": "facial action unit recognition", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Y. Li", "label": "Y. Li", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Y. Zhao", "label": "Y. Zhao", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "School of Comm. and Info. Engineering", "label": "School of Comm. and Info. Engineering", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Beijing University of Posts and Telecom.", "label": "Beijing University of Posts and Telecom.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "G. Littlewort", "label": "G. Littlewort", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Dynamics of facial expression", "label": "Dynamics of facial expression", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "AU-cascades", "label": "AU-cascades", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "action unit detection", "label": "action unit detection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Wen-Sheng Chu", "label": "Wen-Sheng Chu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Robotics Institute", "label": "Robotics Institute", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Fernando De la Torre", "label": "Fernando De la Torre", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Jeffrey F. Cohn", "label": "Jeffrey F. Cohn", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Robotic Institute", "label": "Robotic Institute", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Honggang Zhang", "label": "Honggang Zhang", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Beijing University of Posts and Telecom", "label": "Beijing University of Posts and Telecom", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "TVSum", "label": "TVSum", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Web Videos", "label": "Web Videos", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Titles", "label": "Titles", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "video summarization framework", "label": "video summarization framework", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "title-based image search results", "label": "title-based image search results", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "superior quality summaries", "label": "superior quality summaries", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "summaries", "label": "summaries", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "superior", "label": "superior", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Yale Song", "label": "Yale Song", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Yahoo Labs", "label": "Yahoo Labs", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "yalessong@yahoo-inc.com", "label": "yalessong@yahoo-inc.com", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Jordi Vallmitjana", "label": "Jordi Vallmitjana", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Amanda Stent", "label": "Amanda Stent", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Alejandro Jaimes", "label": "Alejandro Jaimes", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Song_TVSum_Summarizing_Web_2015_CVPR_paper.pdf", "label": "Song_TVSum_Summarizing_Web_2015_CVPR_paper.pdf", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Video summarization", "label": "Video summarization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "need for prior knowledge", "label": "need for prior knowledge", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "video titles", "label": "video titles", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "descriptive", "label": "descriptive", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "co-archetypal analysis", "label": "co-archetypal analysis", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "novel technique", "label": "novel technique", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "visual concepts", "label": "visual concepts", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "video and images", "label": "video and images", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "TVSum50", "label": "TVSum50", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "benchmark dataset", "label": "benchmark dataset", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "image search results", "label": "image search results", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "noise and variance", "label": "noise and variance", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Co-Archetypal Analysis", "label": "Co-Archetypal Analysis", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "analysis method", "label": "analysis method", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Canonical Visual Concepts", "label": "Canonical Visual Concepts", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "concept", "label": "concept", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "M. Basseville", "label": "M. Basseville", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Detection of abrupt changes", "label": "Detection of abrupt changes", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "A. Beck", "label": "A. Beck", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "shrinkage-thresholding algorithm", "label": "shrinkage-thresholding algorithm", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "K. Bleakley", "label": "K. Bleakley", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "group fused lasso", "label": "group fused lasso", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Y. Chen", "label": "Y. Chen", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "archetypal analysis", "label": "archetypal analysis", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "S. Fidler", "label": "S. Fidler", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "sentence is worth a thousand pixels", "label": "sentence is worth a thousand pixels", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "A sentence is worth a thousand pixels", "label": "A sentence is worth a thousand pixels", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Airal", "label": "Airal", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Fast and robust archetypal analysis", "label": "Fast and robust archetypal analysis", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "M. Gygli", "label": "M. Gygli", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Creating summaries from user videos", "label": "Creating summaries from user videos", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Y. Jia", "label": "Y. Jia", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Visual concept learning", "label": "Visual concept learning", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Y. J. Lee", "label": "Y. J. Lee", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Discovering important people and objects", "label": "Discovering important people and objects", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Object-graphs for context-aware category discovery", "label": "Object-graphs for context-aware category discovery", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "L. Li", "label": "L. Li", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Video summarization via transferrable structured learning", "label": "Video summarization via transferrable structured learning", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "WWW", "label": "WWW", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "D. Lin", "label": "D. Lin", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Retrieving videos", "label": "Retrieving videos", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "complex textual queries", "label": "complex textual queries", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "research institution", "label": "research institution", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "isual semantic search", "label": "isual semantic search", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "video retrieval", "label": "video retrieval", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Tianjun Xiao", "label": "Tianjun Xiao", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "The Application of Two-level Attention Models", "label": "The Application of Two-level Attention Models", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Institute of Computer Science and Technologies", "label": "Institute of Computer Science and Technologies", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Deep Convolutional Neural Network", "label": "Deep Convolutional Neural Network", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Yichong Xu", "label": "Yichong Xu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Kuiyuan Yang", "label": "Kuiyuan Yang", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Jiaxing Zhang", "label": "Jiaxing Zhang", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Yuxin Peng", "label": "Yuxin Peng", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "The Application of Two-level Attack Models", "label": "The Application of Two-level Attack Models", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Institute of Computer Science and Technology", "label": "Institute of Computer Science and Technology", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Zheng Zhang", "label": "Zheng Zhang", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "New York University Shanghai", "label": "New York University Shanghai", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Fine-grained classification", "label": "Fine-grained classification", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "subtle differences between categories", "label": "subtle differences between categories", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "pipeline", "label": "pipeline", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "bottom-up attention", "label": "bottom-up attention", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "object-level top-down attention", "label": "object-level top-down attention", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "part-level top-down attention", "label": "part-level top-down attention", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Fine-grained image classification", "label": "Fine-grained image classification", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Visual attention models", "label": "Visual attention models", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Deep convolutional neural networks", "label": "Deep convolutional neural networks", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Weak supervision", "label": "Weak supervision", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Geodesic Exponential Kernel", "label": "Geodesic Exponential Kernel", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "curvature and linearity conflict", "label": "curvature and linearity conflict", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Aasa Feragen", "label": "Aasa Feragen", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "DIKU, University of Copenhagen", "label": "DIKU, University of Copenhagen", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Fran\u00e7ois Lauze", "label": "Fran\u00e7ois Lauze", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Feragen", "label": "Feragen", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "S\u00f8ren Hauberg", "label": "S\u00f8ren Hauberg", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "DTU Compute", "label": "DTU Compute", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Gaussian kernel", "label": "Gaussian kernel", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "positive definite kernel", "label": "positive definite kernel", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "flat space", "label": "flat space", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "geodesic Gaussian kernel", "label": "geodesic Gaussian kernel", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Riemannian manifold is Euclidean", "label": "Riemannian manifold is Euclidean", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "geodesic Laplacian kernel", "label": "geodesic Laplacian kernel", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "positive de\ufb01niteness", "label": "positive de\ufb01niteness", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "curved spaces", "label": "curved spaces", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "spheres", "label": "spheres", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "hyperbolic spaces", "label": "hyperbolic spaces", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "kernel", "label": "kernel", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "spaces", "label": "spaces", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "conditionally negative de\ufb01nite distances", "label": "conditionally negative de\ufb01nite distances", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "theoretical results", "label": "theoretical results", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "empirically", "label": "empirically", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Gaussian kernels", "label": "Gaussian kernels", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Laplacian kernels", "label": "Laplacian kernels", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "M. Alamgir and U. von Luxburg", "label": "M. Alamgir and U. von Luxburg", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Shortest path distance in random k-nearest neighbor graphs", "label": "Shortest path distance in random k-nearest neighbor graphs", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "N. Dalal and B. Triggs", "label": "N. Dalal and B. Triggs", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Histograms of oriented gradients for human detection", "label": "Histograms of oriented gradients for human detection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "S. Amar\u00ed and H. Nagaoka", "label": "S. Amar\u00ed and H. Nagaoka", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Methods of information geometry", "label": "Methods of information geometry", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Arsigny et al.", "label": "Arsigny et al.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Fast and simple calculus on tensors", "label": "Fast and simple calculus on tensors", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "MICCAI", "label": "MICCAI", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Feragen et al.", "label": "Feragen et al.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Means in spaces of tree-like shapes", "label": "Means in spaces of tree-like shapes", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Scalable kernels for graphs", "label": "Scalable kernels for graphs", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Bekka and de la Harple", "label": "Bekka and de la Harple", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Kazhdan\u2019s Property (T)", "label": "Kazhdan\u2019s Property (T)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "mathematical_property", "label": "mathematical_property", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "New Mathematical Monographs", "label": "New Mathematical Monographs", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Bridson and Hae\ufb02iger", "label": "Bridson and Hae\ufb02iger", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Metric spaces of non-positive curvature", "label": "Metric spaces of non-positive curvature", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Ganzhao Yuan", "label": "Ganzhao Yuan", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "South China University of Technology (SCUT)", "label": "South China University of Technology (SCUT)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "yuan Ganzhao@gmail.com", "label": "yuan Ganzhao@gmail.com", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Image Restoration", "label": "Image Restoration", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Total Variation", "label": "Total Variation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "PADMM", "label": "PADMM", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Yuan_L0TV_A_New_2015_CVPR_paper", "label": "Yuan_L0TV_A_New_2015_CVPR_paper", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Impulse Noise", "label": "Impulse Noise", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "TV-based restoration problem", "label": "TV-based restoration problem", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "\u21130-norm data fidelity", "label": "\u21130-norm data fidelity", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "MPEC", "label": "MPEC", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Ejaz Ahmed", "label": "Ejaz Ahmed", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "An Improved Deep Learning Architecture", "label": "An Improved Deep Learning Architecture", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Michael Jones", "label": "Michael Jones", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Mitsubishi Electric Research Labs", "label": "Mitsubishi Electric Research Labs", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Tim K. Marks", "label": "Tim K. Marks", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "same person", "label": "same person", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "layer", "label": "layer", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "cross-input neighborhood differences", "label": "cross-input neighborhood differences", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "local relationships", "label": "local relationships", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "mid-level features", "label": "mid-level features", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "patch summary features", "label": "patch summary features", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "layer of patch summary features", "label": "layer of patch summary features", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "high-level summary", "label": "high-level summary", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "CUHK03", "label": "CUHK03", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "CUHK01", "label": "CUHK01", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "medium-sized data set", "label": "medium-sized data set", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "VIPeR", "label": "VIPeR", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "small data set", "label": "small data set", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "initial training", "label": "initial training", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "fine-tuning", "label": "fine-tuning", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Deep Convolutional Architecture", "label": "Deep Convolutional Architecture", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Similarity Metric Learning", "label": "Similarity Metric Learning", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Neighborhood Difference Layer", "label": "Neighborhood Difference Layer", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Metric Learning", "label": "Metric Learning", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Distribution Divergence", "label": "Distribution Divergence", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "[Li, W., \u0026 Wang, X. (2013)]", "label": "[Li, W., \u0026 Wang, X. (2013)]", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Li, Z.", "label": "Li, Z.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Learning locally-adaptive decision functions", "label": "Learning locally-adaptive decision functions", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Bazzani, L.", "label": "Bazzani, L.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Multiple-shot person re-identi\ufb01cation", "label": "Multiple-shot person re-identi\ufb01cation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "chromatic analyses", "label": "chromatic analyses", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Bottou, L.", "label": "Bottou, L.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Stochastic gradient tricks", "label": "Stochastic gradient tricks", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Davis, J. V.", "label": "Davis, J. V.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Information-theoretic metric learning", "label": "Information-theoretic metric learning", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Farenzena, M.", "label": "Farenzena, M.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Person re-identi\ufb01cation by symmetry-driven accumulation", "label": "Person re-identi\ufb01cation by symmetry-driven accumulation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Person re-identi\ufb01cation", "label": "Person re-identi\ufb01cation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "local features", "label": "local features", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "local contrast", "label": "local contrast", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "shape information", "label": "shape information", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "McAllester, D.", "label": "McAllester, D.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Vassileios Balntas", "label": "Vassileios Balntas", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "BOLD", "label": "BOLD", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "University of Surrey, UK", "label": "University of Surrey, UK", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Binary Online Learned Descriptor", "label": "Binary Online Learned Descriptor", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Lilian Tang", "label": "Lilian Tang", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Krystian Mikolajczyk", "label": "Krystian Mikolajczyk", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "image patch", "label": "image patch", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "test results", "label": "test results", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "subset of robust tests", "label": "subset of robust tests", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "per-patch optimization", "label": "per-patch optimization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "global optimization", "label": "global optimization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Masked Hamming distance", "label": "Masked Hamming distance", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "tests", "label": "tests", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Per-patch optimization", "label": "Per-patch optimization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "D. G. Lowe", "label": "D. G. Lowe", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Local descriptors", "label": "Local descriptors", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "IEEE TPAMI, 27(10):1615\u20131630, 2005", "label": "IEEE TPAMI, 27(10):1615\u20131630, 2005", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "SURF descriptor", "label": "SURF descriptor", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "ECCV, 2006", "label": "ECCV, 2006", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "G. H. M. Brown and S. Winder", "label": "G. H. M. Brown and S. Winder", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Local image descriptors", "label": "Local image descriptors", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Discriminative learning", "label": "Discriminative learning", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "IEEE TPAMI, 33(1):43\u201357, 2010", "label": "IEEE TPAMI, 33(1):43\u201357, 2010", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Binary descriptors", "label": "Binary descriptors", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Image matching", "label": "Image matching", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Online descriptor optimization", "label": "Online descriptor optimization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "discriminative learning", "label": "discriminative learning", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "interest point detection", "label": "interest point detection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "K. Mikolajczyk and C. Schmid", "label": "K. Mikolajczyk and C. Schmid", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "tracking applications", "label": "tracking applications", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Struck", "label": "Struck", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Tracking-learning-detection", "label": "Tracking-learning-detection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "integration", "label": "integration", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "tracking", "label": "tracking", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "keypoint recognition", "label": "keypoint recognition", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "random ferns", "label": "random ferns", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "M. Ozuysal", "label": "M. Ozuysal", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Fast keypoint recognition method", "label": "Fast keypoint recognition method", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "topological constraints", "label": "topological constraints", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "deformable objects", "label": "deformable objects", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "occluded objects", "label": "occluded objects", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "dynamic graph", "label": "dynamic graph", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "IEEE TPAMI", "label": "IEEE TPAMI", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "annotation of pictures", "label": "annotation of pictures", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "ORB", "label": "ORB", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "SIFT", "label": "SIFT", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "SURF", "label": "SURF", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Bay, H.", "label": "Bay, H.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "V. L. T. Trzcinski", "label": "V. L. T. Trzcinski", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Boosting Binary Keypoint Descriptors", "label": "Boosting Binary Keypoint Descriptors", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Jiajun Wu", "label": "Jiajun Wu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Deep Multiple Instance Learning for Image Classi\ufb01cation and Auto-Annotation", "label": "Deep Multiple Instance Learning for Image Classi\ufb01cation and Auto-Annotation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Learning Algorithm", "label": "Learning Algorithm", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Yinan Yu", "label": "Yinan Yu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Institute of Deep Learning", "label": "Institute of Deep Learning", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Kai Yu", "label": "Kai Yu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Deep Multiple instance Learning", "label": "Deep Multiple instance Learning", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Wu_Deep_Multiple_Instance_2015_CVPR_paper.pdf", "label": "Wu_Deep_Multiple_Instance_2015_CVPR_paper.pdf", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Deep learning", "label": "Deep learning", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "tremendous improvements", "label": "tremendous improvements", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "object proposals", "label": "object proposals", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "instance sets", "label": "instance sets", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "text annotations", "label": "text annotations", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "systems", "label": "systems", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "MIL property", "label": "MIL property", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "deep learning strategies", "label": "deep learning strategies", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "reasonable", "label": "reasonable", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "extraction", "label": "extraction", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "little supervision", "label": "little supervision", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Region-keyword pairs", "label": "Region-keyword pairs", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Multiple Instance Learning (MIL)", "label": "Multiple Instance Learning (MIL)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Andrews et al.", "label": "Andrews et al.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Support vector machines", "label": "Support vector machines", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Li \u0026 Wang", "label": "Li \u0026 Wang", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "computerized annotation", "label": "computerized annotation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "pictures", "label": "pictures", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Barnard et al.", "label": "Barnard et al.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Image Annotation", "label": "Image Annotation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Barnard", "label": "Barnard", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Chen", "label": "Chen", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Hierarchical matching", "label": "Hierarchical matching", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "University of California, Los Angeles", "label": "University of California, Los Angeles", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Li, Q.", "label": "Li, Q.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Harvesting mid-level visual concepts", "label": "Harvesting mid-level visual concepts", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Imaginet", "label": "Imaginet", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "hierarchical images", "label": "hierarchical images", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Computer Vision and Pattern Recognition, 2009", "label": "Computer Vision and Pattern Recognition, 2009", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Rochan", "label": "Rochan", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Mrigank Rochan", "label": "Mrigank Rochan", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "University of Manitoba", "label": "University of Manitoba", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Chang Huang", "label": "Chang Huang", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "training images", "label": "training images", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "object bounding boxes", "label": "object bounding boxes", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "weakly labeled data", "label": "weakly labeled data", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "YouTube videos", "label": "YouTube videos", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "user-generated tags", "label": "user-generated tags", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "image search", "label": "image search", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "weakly labeled images", "label": "weakly labeled images", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "object category", "label": "object category", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "local density map", "label": "local density map", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Lampert et al. (2009)", "label": "Lampert et al. (2009)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "unseen object classes", "label": "unseen object classes", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "K. Grauman", "label": "K. Grauman", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "R. G. Cinbis", "label": "R. G. Cinbis", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "J. Verbeek", "label": "J. Verbeek", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "T. Mikolov", "label": "T. Mikolov", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Distributed representations of words and phrases", "label": "Distributed representations of words and phrases", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "I. Sutskever", "label": "I. Sutskever", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "M. H. Nguyen", "label": "M. H. Nguyen", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Weakly supervised discrimiative localization", "label": "Weakly supervised discrimiative localization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "A. Papazoglou", "label": "A. Papazoglou", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Fast object segmentation", "label": "Fast object segmentation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "IEEE International Conference on Computer Vision", "label": "IEEE International Conference on Computer Vision", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "V. Ferrari", "label": "V. Ferrari", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Object-graphs", "label": "Object-graphs", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "context-aware category discovery", "label": "context-aware category discovery", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Distributed representations", "label": "Distributed representations", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "compositionality", "label": "compositionality", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Graph structured sparsity model", "label": "Graph structured sparsity model", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "A. Prest", "label": "A. Prest", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Learning object class detectors", "label": "Learning object class detectors", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "M. Rohrbach", "label": "M. Rohrbach", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Evaluating knowledge transfer", "label": "Evaluating knowledge transfer", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "What helps where", "label": "What helps where", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "IEEE Conference on Computer Vision and Pattern Recognition", "label": "IEEE Conference on Computer Vision and Pattern Recognition", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition", "label": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Yang Wang", "label": "Yang Wang", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Mriganka Rochan", "label": "Mriganka Rochan", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "mrochan@cs.umanitoba.ca", "label": "mrochan@cs.umanitoba.ca", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "rigank Rochan", "label": "rigank Rochan", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Wei Liu", "label": "Wei Liu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Towards 3D Object Detection", "label": "Towards 3D Object Detection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Dep. of Cognitive Science", "label": "Dep. of Cognitive Science", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Xiamen University", "label": "Xiamen University", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "IBM Research", "label": "IBM Research", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Rongrong Ji", "label": "Rongrong Ji", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Dep. of Cognitive Space", "label": "Dep. of Cognitive Space", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Shaozi Li", "label": "Shaozi Li", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Towards 3D ObjectDetection", "label": "Towards 3D ObjectDetection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "ywang@cs.umanitoba.ca", "label": "ywang@cs.umanitoba.ca", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "accurate detection algorithm", "label": "accurate detection algorithm", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "RGB and depth modalities", "label": "RGB and depth modalities", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "correlated", "label": "correlated", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "cross-modality deep learning framework", "label": "cross-modality deep learning framework", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "deep Boltzmann Machines", "label": "deep Boltzmann Machines", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "existing datasets", "label": "existing datasets", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "models", "label": "models", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "cross-modality features", "label": "cross-modality features", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "RGBD data", "label": "RGBD data", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "compositional", "label": "compositional", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "backpropagation", "label": "backpropagation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "temporal dynamics", "label": "temporal dynamics", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "convolutional perceptual representations", "label": "convolutional perceptual representations", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "distinct advantages", "label": "distinct advantages", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "k", "label": "k", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Semantic labeling", "label": "Semantic labeling", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "3d point clouds", "label": "3d point clouds", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Learning rich features", "label": "Learning rich features", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "RGBD images", "label": "RGBD images", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "2012", "label": "2012", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Efficient 3d scene labeling", "label": "Efficient 3d scene labeling", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "field-s of trees", "label": "field-s of trees", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "geNet", "label": "geNet", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "O. Kahler", "label": "O. Kahler", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "N. Srivastava", "label": "N. Srivastava", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Multimodal learning", "label": "Multimodal learning", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "K. Lai", "label": "K. Lai", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Detection-based object labeling", "label": "Detection-based object labeling", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "IEEE International Conference on Robotics and Automation", "label": "IEEE International Conference on Robotics and Automation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "L. Bo", "label": "L. Bo", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Unsupervised feature learning", "label": "Unsupervised feature learning", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "rgb-d based object recognition", "label": "rgb-d based object recognition", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "X. Xiong", "label": "X. Xiong", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "3-d scene analysis", "label": "3-d scene analysis", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "sequenced predictions", "label": "sequenced predictions", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "A. Wang", "label": "A. Wang", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Multi-modal unsupervised feature learning", "label": "Multi-modal unsupervised feature learning", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "rgb-d scene labeling", "label": "rgb-d scene labeling", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Abhishek Sharma", "label": "Abhishek Sharma", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Deep Hierarchical Parsing", "label": "Deep Hierarchical Parsing", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Sharma_Deep_Hierarial_Parsing_2015_CVPR_paper", "label": "Sharma_Deep_Hierarial_Parsing_2015_CVPR_paper", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Computer Science Department", "label": "Computer Science Department", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "bhokaal@cs.umd.edu", "label": "bhokaal@cs.umd.edu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Oncel Tuzel", "label": "Oncel Tuzel", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "MERL", "label": "MERL", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "David W. Jacobs", "label": "David W. Jacobs", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Sliding shapes", "label": "Sliding shapes", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "improvements to RCPN", "label": "improvements to RCPN", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "RCPN", "label": "RCPN", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "deep feed-forward neural network", "label": "deep feed-forward neural network", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "bypass error paths", "label": "bypass error paths", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "contextual propagation", "label": "contextual propagation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "modifications", "label": "modifications", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "classification loss", "label": "classification loss", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "tree-style MRF", "label": "tree-style MRF", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "random parse trees", "label": "random parse trees", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "hierarchical dependencies", "label": "hierarchical dependencies", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Tree-Style MRF", "label": "Tree-Style MRF", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Modifications", "label": "Modifications", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Deep Neural Networks", "label": "Deep Neural Networks", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Image Alignment", "label": "Image Alignment", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Regions", "label": "Regions", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Parts", "label": "Parts", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Recursive Context Propagation Network (RCPN)", "label": "Recursive Context Propagation Network (RCPN)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Contextual Propagation", "label": "Contextual Propagation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Socher et al. (2011)", "label": "Socher et al. (2011)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Recursive Neural Networks", "label": "Recursive Neural Networks", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Farabet et al. (2013)", "label": "Farabet et al. (2013)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "scene labeling", "label": "scene labeling", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Fergus and Eigen (2012)", "label": "Fergus and Eigen (2012)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "image parsing", "label": "image parsing", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Markov Random Fields (MRF)", "label": "Markov Random Fields (MRF)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Najman", "label": "Najman", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Learning hierarchical features for scene labeling", "label": "Learning hierarchical features for scene labeling", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "IEEE TPAM", "label": "IEEE TPAM", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "R. Fergus", "label": "R. Fergus", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Nonparametric image parsing", "label": "Nonparametric image parsing", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "IEEE CVPR", "label": "IEEE CVPR", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "A. Torralba", "label": "A. Torralba", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Context-based vision system", "label": "Context-based vision system", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "P. H. O. Pinheiro", "label": "P. H. O. Pinheiro", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Recurrent convolutional neural networks", "label": "Recurrent convolutional neural networks", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "ICML", "label": "ICML", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "A. Sharma", "label": "A. Sharma", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Recursive context propagation network", "label": "Recursive context propagation network", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "J. Tighe", "label": "J. Tighe", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Finding things", "label": "Finding things", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Superparsing", "label": "Superparsing", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "R. Mottaghi", "label": "R. Mottaghi", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Analyzing semantic segmentation", "label": "Analyzing semantic segmentation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Bell et al.", "label": "Bell et al.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Junlin Hu", "label": "Junlin Hu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Deep Transfer Metric Learning", "label": "Deep Transfer Metric Learning", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "School of Electrical and Electronic Engineering", "label": "School of Electrical and Electronic Engineering", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "jhu007@e.ntu.edu.sg", "label": "jhu007@e.ntu.edu.sg", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Jiwen Lu", "label": "Jiwen Lu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Nanyang Technological University", "label": "Nanyang Technological University", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "jiwen.lu@adsc.com.sg", "label": "jiwen.lu@adsc.com.sg", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "metric learning methods", "label": "metric learning methods", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "similar scenarios", "label": "similar scenarios", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "real-world visual recognition applications", "label": "real-world visual recognition applications", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "DTML method", "label": "DTML method", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "discriminative knowledge", "label": "discriminative knowledge", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "inter-class variations", "label": "inter-class variations", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "intra-class variations", "label": "intra-class variations", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "distribution divergence", "label": "distribution divergence", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "DSTML method", "label": "DSTML method", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "outputs of hidden and top layers", "label": "outputs of hidden and top layers", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "deeply supervised transfer metric learning", "label": "deeply supervised transfer metric learning", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "transfer metric learning method", "label": "transfer metric learning method", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "cross-dataset tasks", "label": "cross-dataset tasks", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "vergence", "label": "vergence", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "source and target domains", "label": "source and target domains", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "hidden layers", "label": "hidden layers", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "top layers", "label": "top layers", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Deep Transfer Metric Learning (DTML)", "label": "Deep Transfer Metric Learning (DTML)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Cross-Domain Visual Recognition", "label": "Cross-Domain Visual Recognition", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Face Description", "label": "Face Description", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Learning Deep Architectures", "label": "Learning Deep Architectures", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Predictive Structures", "label": "Predictive Structures", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Multiple Tasks", "label": "Multiple Tasks", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Journal of Machine Learning Research", "label": "Journal of Machine Learning Research", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Foundations and Trends in Machine Learning", "label": "Foundations and Trends in Machine Learning", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "ACM Multimedia", "label": "ACM Multimedia", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "ACM", "label": "ACM", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Graph-based search methods", "label": "Graph-based search methods", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Chen, D.", "label": "Chen, D.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Bayesian face revisited", "label": "Bayesian face revisited", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "joint formulation", "label": "joint formulation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Duan, L.", "label": "Duan, L.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Domain transfer SVM", "label": "Domain transfer SVM", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Conference on Computer Vision and Pattern Recognition", "label": "Conference on Computer Vision and Pattern Recognition", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Gray \u0026 Tao (2008)", "label": "Gray \u0026 Tao (2008)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Gretton et al. (2006)", "label": "Gretton et al. (2006)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Neural Information Processing Systems", "label": "Neural Information Processing Systems", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Hinton et al. (2006)", "label": "Hinton et al. (2006)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Neural Computation", "label": "Neural Computation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Huang et al. (2012)", "label": "Huang et al. (2012)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Yap-Peng Tan", "label": "Yap-Peng Tan", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "eyptan@ntu.edu.sg", "label": "eyptan@ntu.edu.sg", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Takuya Narihira", "label": "Takuya Narihira", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Learning Lightness from Human Judgement", "label": "Learning Lightness from Human Judgement", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Michael Maire", "label": "Michael Maire", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "TTI Chicago", "label": "TTI Chicago", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Stella X. Yu", "label": "Stella X. Yu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "UC Berkeley", "label": "UC Berkeley", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "ICSI", "label": "ICSI", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Narihira_Learning_Lightness_From_2015_CVPR_paper.pdf", "label": "Narihira_Learning_Lightness_From_2015_CVPR_paper.pdf", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Narihira_Learning_Lightness_From_2015_CVPR_paper", "label": "Narihira_Learning_Lightness_From_2015_CVPR_paper", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "inferring lightness", "label": "inferring lightness", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "perceived re\ufb02ectance", "label": "perceived re\ufb02ectance", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "classic methods", "label": "classic methods", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "re\ufb02ectance and shading components", "label": "re\ufb02ectance and shading components", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "lightness perception", "label": "lightness perception", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "deep networks", "label": "deep networks", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "global saliency cues", "label": "global saliency cues", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "local lightness model", "label": "local lightness model", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "on-par with global lightness model", "label": "on-par with global lightness model", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "on-par performance", "label": "on-par performance", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "state-of-the-art global lightness model", "label": "state-of-the-art global lightness model", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "ld dataset", "label": "ld dataset", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "global lightness model", "label": "global lightness model", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "shading/re\ufb02ectance priors", "label": "shading/re\ufb02ectance priors", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "dense conditional random field formulation", "label": "dense conditional random field formulation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "shading/reflectance priors", "label": "shading/reflectance priors", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "multiple priors", "label": "multiple priors", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "simultaneous reasoning", "label": "simultaneous reasoning", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "pairs of pixels", "label": "pairs of pixels", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "cognitive neurosciences", "label": "cognitive neurosciences", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Lightness perception and lightness illusions", "label": "Lightness perception and lightness illusions", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "foundational work", "label": "foundational work", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "feature learning", "label": "feature learning", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "H. G. Barrow", "label": "H. G. Barrow", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Recovering intrinsic scene characteristics from images", "label": "Recovering intrinsic scene characteristics from images", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "intrinsic image algorithms", "label": "intrinsic image algorithms", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "baseline evaluations", "label": "baseline evaluations", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "relative reflectance", "label": "relative reflectance", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "human judgment data", "label": "human judgment data", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Computer Vision Systems", "label": "Computer Vision Systems", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "scene characteristics from images", "label": "scene characteristics from images", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Retinex theory", "label": "Retinex theory", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "E. H. Land and J. J. McCann", "label": "E. H. Land and J. J. McCann", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "brightness perception", "label": "brightness perception", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "intrinsic image recovery", "label": "intrinsic image recovery", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "determining lightness from an image", "label": "determining lightness from an image", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "M. Tappen, W. Freeman, and E. Adelson", "label": "M. Tappen, W. Freeman, and E. Adelson", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 2005", "label": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 2005", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "R. Grosse", "label": "R. Grosse", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "International Conference on Computer Vision, 2009", "label": "International Conference on Computer Vision, 2009", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Y. Tang", "label": "Y. Tang", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Deep Lambertian networks", "label": "Deep Lambertian networks", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Lambertian reflectance models", "label": "Lambertian reflectance models", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "deep learning approaches", "label": "deep learning approaches", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "International Conference on Machine Learning, 2012", "label": "International Conference on Machine Learning, 2012", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Lambertian networks", "label": "Lambertian networks", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Takaya Narihira", "label": "Takaya Narihira", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Sony Corp.", "label": "Sony Corp.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "face recognition", "label": "face recognition", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Labelled faces in the wild", "label": "Labelled faces in the wild", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Hierarchical-PEP model", "label": "Hierarchical-PEP model", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Probabilistic Elastic Part (PEP) model", "label": "Probabilistic Elastic Part (PEP) model", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Deep Hierarchical Architectures", "label": "Deep Hierarchical Architectures", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Fine-grained Structures", "label": "Fine-grained Structures", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Supervised Information", "label": "Supervised Information", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Face Image", "label": "Face Image", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Face Parts", "label": "Face Parts", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Detail Level", "label": "Detail Level", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Face Part Representations", "label": "Face Part Representations", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Layer", "label": "Layer", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Dimensionality", "label": "Dimensionality", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Face Representation", "label": "Face Representation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Invariant", "label": "Invariant", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "face parts", "label": "face parts", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "supervised information", "label": "supervised information", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "face recognition challenge", "label": "face recognition challenge", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Ahonen, T.", "label": "Ahonen, T.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Face recognition with local binary patterns", "label": "Face recognition with local binary patterns", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Grauman, K.", "label": "Grauman, K.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "The pyramid match kernel", "label": "The pyramid match kernel", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Hu, J.", "label": "Hu, J.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Discriminative deep metric learning", "label": "Discriminative deep metric learning", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Large margin multi-metric learning", "label": "Large margin multi-metric learning", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Eigenfaces", "label": "Eigenfaces", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Fisherfaces", "label": "Fisherfaces", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "linear projection", "label": "linear projection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Unsupervised joint alignment", "label": "Unsupervised joint alignment", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "complex images", "label": "complex images", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Labeled faces in the wild", "label": "Labeled faces in the wild", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "reporting procedures", "label": "reporting procedures", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "kinship verification", "label": "kinship verification", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Asian Conference on Computer Vision (ACCV)", "label": "Asian Conference on Computer Vision (ACCV)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition", "label": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Lei, Z.", "label": "Lei, Z.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "discriminant face descriptor", "label": "discriminant face descriptor", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Simonyan, K.", "label": "Simonyan, K.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Deep fisher networks", "label": "Deep fisher networks", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Very deep convolutional networks", "label": "Very deep convolutional networks", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Yu Kong", "label": "Yu Kong", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Bilinear Heterogeneous Information Machine", "label": "Bilinear Heterogeneous Information Machine", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "low-rank bilinear classification", "label": "low-rank bilinear classification", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Kong_Bilinear_Heterogeneous_Information_2015_CVPR_paper.pdf", "label": "Kong_Bilinear_Heterogeneous_Information_2015_CVPR_paper.pdf", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "RGB-D Action Recognition", "label": "RGB-D Action Recognition", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Yun Fu", "label": "Yun Fu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "yunfu@ece.neu.edu", "label": "yunfu@ece.neu.edu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "space", "label": "space", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "learned", "label": "learned", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "knowledge", "label": "knowledge", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "shared", "label": "shared", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "low-rank classifier", "label": "low-rank classifier", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "eter", "label": "eter", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "RGB data are missing", "label": "RGB data are missing", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "depth data are missing", "label": "depth data are missing", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Argyriou et al. (2008)", "label": "Argyriou et al. (2008)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Bo et al. (2011)", "label": "Bo et al. (2011)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "kernel descriptors", "label": "kernel descriptors", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Do and Artieres (2009)", "label": "Do and Artieres (2009)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Artieres et al.", "label": "Artieres et al.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Had\ufb01eld and Bowden", "label": "Had\ufb01eld and Bowden", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Ji et al.", "label": "Ji et al.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "3D Convolutional Neural Networks", "label": "3D Convolutional Neural Networks", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Kobayashi", "label": "Kobayashi", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "modeling complex interactions", "label": "modeling complex interactions", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "spatio-temporal depth cuboid similarity feature", "label": "spatio-temporal depth cuboid similarity feature", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "activity recognition", "label": "activity recognition", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "information bottleneck method", "label": "information bottleneck method", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "feature selection", "label": "feature selection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "representation learning", "label": "representation learning", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "depth camera", "label": "depth camera", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "depth sequences", "label": "depth sequences", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "HON4D", "label": "HON4D", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "oriented 4D normals", "label": "oriented 4D normals", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "information bottlenecks", "label": "information bottlenecks", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Sebastian Haner", "label": "Sebastian Haner", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Absolute Pose for Cameras", "label": "Absolute Pose for Cameras", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Centre for Mathematical Sciences", "label": "Centre for Mathematical Sciences", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Lund University", "label": "Lund University", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Centre for Mathematical Sciences, Lund University", "label": "Centre for Mathematical Sciences, Lund University", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "haner@maths.lth.se", "label": "haner@maths.lth.se", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "refractive interfaces", "label": "refractive interfaces", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Kalle \u02daAstr\u00a8om", "label": "Kalle \u02daAstr\u00a8om", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Workshop on Omnidirectional Vision", "label": "Workshop on Omnidirectional Vision", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "kalle@maths.lth.se", "label": "kalle@maths.lth.se", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Yukong", "label": "Yukong", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "yukong@ece.neu.edu", "label": "yukong@ece.neu.edu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "perspective camera", "label": "perspective camera", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "scene", "label": "scene", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "rigidity", "label": "rigidity", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "refractive plane", "label": "refractive plane", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "transparent media", "label": "transparent media", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "solvers", "label": "solvers", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "2D cases", "label": "2D cases", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "minimal", "label": "minimal", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "synthetic data", "label": "synthetic data", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "real data", "label": "real data", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Snell\u2019s law", "label": "Snell\u2019s law", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "false solutions", "label": "false solutions", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "complexity of problem", "label": "complexity of problem", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "pose estimates", "label": "pose estimates", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "explicitly modelling refraction", "label": "explicitly modelling refraction", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Refractive Interfaces Modelling", "label": "Refractive Interfaces Modelling", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Refractive Interfaces", "label": "Refractive Interfaces", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Snell\u0027s Law", "label": "Snell\u0027s Law", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Structure-and-Motion", "label": "Structure-and-Motion", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Polynomial Equation Solving", "label": "Polynomial Equation Solving", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Agrawal et al. (2012)", "label": "Agrawal et al. (2012)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Multi-layer Flat Refractive Geometry Theory", "label": "Multi-layer Flat Refractive Geometry Theory", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Byr\u00a8od et al. (2009)", "label": "Byr\u00a8od et al. (2009)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Multi-layer Flat Refractive Geometry", "label": "Multi-layer Flat Refractive Geometry", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Pose Estimation Accuracy", "label": "Pose Estimation Accuracy", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Ideals, Varieties, and Algorithms", "label": "Ideals, Varieties, and Algorithms", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "computational algebraic geometry", "label": "computational algebraic geometry", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "commutative algebra", "label": "commutative algebra", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "\u02daAstr\u00a8om, Kuang, \u0026 Ask", "label": "\u02daAstr\u00a8om, Kuang, \u0026 Ask", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "polynomial equation solving optimization", "label": "polynomial equation solving optimization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "p-fold symmetries", "label": "p-fold symmetries", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Chari \u0026 Sturm", "label": "Chari \u0026 Sturm", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "multi-view geometry", "label": "multi-view geometry", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Chari, V.", "label": "Chari, V.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Multi-view geometry of the refractive plane", "label": "Multi-view geometry of the refractive plane", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "British Machine Vision Conference", "label": "British Machine Vision Conference", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Sturm, P. F.", "label": "Sturm, P. F.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Fitzgibbon, A. W.", "label": "Fitzgibbon, A. W.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Simultaneous linear estimation", "label": "Simultaneous linear estimation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "geometric estimation", "label": "geometric estimation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "lens distortion", "label": "lens distortion", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Kuang, Y.", "label": "Kuang, Y.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "polynomial solvers", "label": "polynomial solvers", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "chmid", "label": "chmid", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "European Conference on ComputerVision", "label": "European Conference on ComputerVision", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Lecture Notes in Computer Science", "label": "Lecture Notes in Computer Science", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "polynomial solver optimization", "label": "polynomial solver optimization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Computer Vision - ECCV 2008", "label": "Computer Vision - ECCV 2008", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "5304", "label": "5304", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Nist\u00e9r", "label": "Nist\u00e9r", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "generalized 3-point pose problem", "label": "generalized 3-point pose problem", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Stew\u00e9nius", "label": "Stew\u00e9nius", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "generalized relative pose problems", "label": "generalized relative pose problems", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Kukelova", "label": "Kukelova", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "polynomial eigenvalue solutions", "label": "polynomial eigenvalue solutions", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "minimal problems", "label": "minimal problems", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "R6P", "label": "R6P", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Albl_R6P_-_Rolling_2015_CVPR_paper", "label": "Albl_R6P_-_Rolling_2015_CVPR_paper", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "polynomial solutions", "label": "polynomial solutions", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Cenek Albl", "label": "Cenek Albl", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Czech Technical University in Prague", "label": "Czech Technical University in Prague", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Zuana Kukelova", "label": "Zuana Kukelova", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Albi_R6P_-_Rolling_2015_CVPR_paper", "label": "Albi_R6P_-_Rolling_2015_CVPR_paper", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Microsoft Research Ltd", "label": "Microsoft Research Ltd", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Tomas Pajdla", "label": "Tomas Pajdla", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "rolling shutter", "label": "rolling shutter", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "digital cameras", "label": "digital cameras", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "camera model", "label": "camera model", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "polynomial solver", "label": "polynomial solver", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "linear approximation", "label": "linear approximation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "6 degrees", "label": "6 degrees", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "identity rotation", "label": "identity rotation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "P3P algorithm", "label": "P3P algorithm", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "estimate camera orientation", "label": "estimate camera orientation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "camera rotation matrix", "label": "camera rotation matrix", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "camera rotation velocity", "label": "camera rotation velocity", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "30deg/frame", "label": "30deg/frame", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Ithm", "label": "Ithm", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "identity", "label": "identity", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "2%", "label": "2%", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "orientation error", "label": "orientation error", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "0.5 degrees", "label": "0.5 degrees", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "half a degree", "label": "half a degree", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Rolling Shutter Cameras", "label": "Rolling Shutter Cameras", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Absolute Pose Problem", "label": "Absolute Pose Problem", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Polynomial Solvers", "label": "Polynomial Solvers", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Linearized Camera Models", "label": "Linearized Camera Models", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "relative position error", "label": "relative position error", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "RANSAC", "label": "RANSAC", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "robust model fitting", "label": "robust model fitting", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Haralick", "label": "Haralick", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Hedborg", "label": "Hedborg", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "RANAC", "label": "RANAC", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "rolling shutter video", "label": "rolling shutter video", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "inertial measurements", "label": "inertial measurements", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "motion estimation", "label": "motion estimation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "rolling shutter data", "label": "rolling shutter data", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "bundle adjustment", "label": "bundle adjustment", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "C. Jia and B. L. Evans", "label": "C. Jia and B. L. Evans", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "video analysis", "label": "video analysis", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "ter video recti\ufb01cation", "label": "ter video recti\ufb01cation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "parallel tracking and mapping", "label": "parallel tracking and mapping", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "ISMAR \u201909", "label": "ISMAR \u201909", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "EEE International Symposium on Mixed and augmented Reality", "label": "EEE International Symposium on Mixed and augmented Reality", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Z. Kukelova", "label": "Z. Kukelova", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Singly-bordered block-diagonal form", "label": "Singly-bordered block-diagonal form", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Automatic generator of minimal problem solvers", "label": "Automatic generator of minimal problem solvers", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "salient regions", "label": "salient regions", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "efficient solvers", "label": "efficient solvers", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "ECCV 2008", "label": "ECCV 2008", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Proceedings", "label": "Proceedings", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Cox", "label": "Cox", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Using Algebraic Geometry", "label": "Using Algebraic Geometry", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Springer", "label": "Springer", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Line Drawing Interpretation", "label": "Line Drawing Interpretation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Sridhar", "label": "Sridhar", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Fast and Robust Hand Tracking", "label": "Fast and Robust Hand Tracking", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Theobalt", "label": "Theobalt", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Detection-Guided Optimization", "label": "Detection-Guided Optimization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Baak et al.", "label": "Baak et al.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "full body pose reconstruction", "label": "full body pose reconstruction", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Ballan et al.", "label": "Ballan et al.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "motion capture of hands", "label": "motion capture of hands", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Bhattacharyya", "label": "Bhattacharyya", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "measure of divergence", "label": "measure of divergence", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Criminisi and Shotton", "label": "Criminisi and Shotton", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Decision forests", "label": "Decision forests", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Srinath Srilhar", "label": "Srinath Srilhar", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "A. Criminisi", "label": "A. Criminisi", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Decision forests for computer vision", "label": "Decision forests for computer vision", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Learning to be a depth camera", "label": "Learning to be a depth camera", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Geodesic image and video editing", "label": "Geodesic image and video editing", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "J. Shotton", "label": "J. Shotton", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "S. R. Fanello", "label": "S. R. Fanello", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "H. Hamer", "label": "H. Hamer", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Tracking a hand manipulating an object", "label": "Tracking a hand manipulating an object", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "C. Keskin", "label": "C. Keskin", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Real time hand pose estimation", "label": "Real time hand pose estimation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "ICCV Workshops", "label": "ICCV Workshops", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "F. Kirac", "label": "F. Kirac", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Y. Kara", "label": "Y. Kara", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "L. Akarun", "label": "L. Akarun", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Srinath Sridhar", "label": "Srinath Sridhar", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Antti Oulasvirta", "label": "Antti Oulasvirta", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Aalto University", "label": "Aalto University", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Fumin Shen", "label": "Fumin Shen", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "University of Electronic Science and Technology of China", "label": "University of Electronic Science and Technology of China", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Heng Tao Shen", "label": "Heng Tao Shen", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "The University of Queensland", "label": "The University of Queensland", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Supervised Discrete Hanning (SDH)", "label": "Supervised Discrete Hanning (SDH)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "hashing framework", "label": "hashing framework", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "linear classification", "label": "linear classification", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "handling discrete constraints", "label": "handling discrete constraints", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "NP-hard optimization problems", "label": "NP-hard optimization problems", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "objective", "label": "objective", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "introducing an auxiliary variable and regularization algorithm", "label": "introducing an auxiliary variable and regularization algorithm", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "cyclic coordinate descent", "label": "cyclic coordinate descent", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "regularization sub-problem", "label": "regularization sub-problem", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "SDH", "label": "SDH", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "high-quality discrete solutions", "label": "high-quality discrete solutions", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "handling of massive datasets", "label": "handling of massive datasets", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "DH", "label": "DH", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "p-stable distributions", "label": "p-stable distributions", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "hashing technique", "label": "hashing technique", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "bilinear projections", "label": "bilinear projections", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "binary code learning", "label": "binary code learning", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Belkin, M., \u0026 Niyogi, P.", "label": "Belkin, M., \u0026 Niyogi, P.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "foundational paper", "label": "foundational paper", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Datar, N., et al.", "label": "Datar, N., et al.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Gong, Y., et al.", "label": "Gong, Y., et al.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Rowley et al. (2013) paper", "label": "Rowley et al. (2013) paper", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Weiss et al. (2008) paper", "label": "Weiss et al. (2008) paper", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "spectral hashing", "label": "spectral hashing", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "contribution to field", "label": "contribution to field", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Gong et al. (2013) paper", "label": "Gong et al. (2013) paper", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "iterative quantization approach", "label": "iterative quantization approach", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "procustean approach", "label": "procustean approach", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Kulis \u0026 Darrell (2009) paper", "label": "Kulis \u0026 Darrell (2009) paper", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "method for learning to hash", "label": "method for learning to hash", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "binary reconstructive embeddings", "label": "binary reconstructive embeddings", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Kulis \u0026 Darrell (2009)", "label": "Kulis \u0026 Darrell (2009)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "binary reconstructive embeddings hashing method", "label": "binary reconstructive embeddings hashing method", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Liu, Wang, Kumar, \u0026 Chang (2011)", "label": "Liu, Wang, Kumar, \u0026 Chang (2011)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "hashing techniques", "label": "hashing techniques", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "graph structures", "label": "graph structures", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "efficient similarity search", "label": "efficient similarity search", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Wang, Kumar, \u0026 Chang (2012)", "label": "Wang, Kumar, \u0026 Chang (2012)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "hashing", "label": "hashing", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "semi-supervised setting", "label": "semi-supervised setting", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Shen \u0026 Hao (2011)", "label": "Shen \u0026 Hao (2011)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "learning and classification", "label": "learning and classification", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Norouzi \u0026 Blei (2011)", "label": "Norouzi \u0026 Blei (2011)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "minimal loss hashing", "label": "minimal loss hashing", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Blei", "label": "Blei", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Age Invariant Face Recognition", "label": "Age Invariant Face Recognition", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Li, Zhifeng", "label": "Li, Zhifeng", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Nonparametric Discriminent Analysis for Face Recognition", "label": "Nonparametric Discriminent Analysis for Face Recognition", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Tao, Dacheng", "label": "Tao, Dacheng", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Liu, Jianzhang", "label": "Liu, Jianzhang", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Li, Xuelong", "label": "Li, Xuelong", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "maximum entropy feature descriptor", "label": "maximum entropy feature descriptor", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "microstructure", "label": "microstructure", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "discrete codes", "label": "discrete codes", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "feature descriptor", "label": "feature descriptor", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "sampling", "label": "sampling", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "discriminatory information", "label": "discriminatory information", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "identity factor analysis", "label": "identity factor analysis", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "probability of same identity", "label": "probability of same identity", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "MORPH dataset", "label": "MORPH dataset", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "FGNET dataset", "label": "FGNET dataset", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "MORPH", "label": "MORPH", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "face aging dataset", "label": "face aging dataset", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "FGNET", "label": "FGNET", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Age Invariant Face Recognition (AIFR)", "label": "Age Invariant Face Recognition (AIFR)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Age Incompliant Face Recognition (AIFR)", "label": "Age Incompliant Face Recognition (AIFR)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Maximum Entropy Feature Descriptor (MEFD)", "label": "Maximum Entropy Feature Descriptor (MEFD)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Identity Factor Analysis (IFA)", "label": "Identity Factor Analysis (IFA)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Wang, Xiaogang", "label": "Wang, Xiaogang", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "A unified framework for subspace face recognition", "label": "A unified framework for subspace face recognition", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "IEEE Trans. Pattern Anal. Mach. Intell.", "label": "IEEE Trans. Pattern Anal. Mach. Intell.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Li, Unsang", "label": "Li, Unsang", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "A discriminative model for age invariant face recognition", "label": "A discriminative model for age invariant face recognition", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "IEEE Transactions on Information Forensics and Security", "label": "IEEE Transactions on Information Forensics and Security", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Zhifeng Li", "label": "Zhifeng Li", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Park, Unsav", "label": "Park, Unsav", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Yiying Tong", "label": "Yiying Tong", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Belhumeur, Peter N.", "label": "Belhumeur, Peter N.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Gong, D.", "label": "Gong, D.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "ICCV 2013", "label": "ICCV 2013", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Huang, G.B.", "label": "Huang, G.B.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "face recognition in unconstrained environments", "label": "face recognition in unconstrained environments", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Shenzhen Key Lab of Computer Vision and Pattern Recogniton", "label": "Shenzhen Key Lab of Computer Vision and Pattern Recogniton", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Technical Report 07-49", "label": "Technical Report 07-49", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Random sampling LDA", "label": "Random sampling LDA", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "DiHong Gong", "label": "DiHong Gong", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Shenzhen Key Lab of Computer Vision and Pattern Recognition", "label": "Shenzhen Key Lab of Computer Vision and Pattern Recognition", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Dihong Gong", "label": "Dihong Gong", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Dacheng Tao", "label": "Dacheng Tao", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "University of Technology, Sydney", "label": "University of Technology, Sydney", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Jianzhuang Liu", "label": "Jianzhuang Liu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Dept. of Information Engineering", "label": "Dept. of Information Engineering", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Huawei Technologies Co. Ltd.", "label": "Huawei Technologies Co. Ltd.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Xuelong Li", "label": "Xuelong Li", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Xi\u0027an Institute of Optics and Precision Mechanics", "label": "Xi\u0027an Institute of Optics and Precision Mechanics", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Sayed Hossein Khatoonabadi", "label": "Sayed Hossein Khatoonabadi", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Simon Fraser University", "label": "Simon Fraser University", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Sayed Hosheen Khatoonabadi", "label": "Sayed Hosheen Khatoonabadi", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "How Many Bits Does It Take for a Stimulus to Be Salient?", "label": "How Many Bits Does It Take for a Stimulus to Be Salient?", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Nuno Vasconcelos", "label": "Nuno Vasconcelos", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "University of California, San Diego", "label": "University of California, San Diego", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Yuifeng Shan", "label": "Yuifeng Shan", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Khatoonabadi_How_Many_Bits_2015_CVPR_paper.pdf", "label": "Khatoonabadi_How_Many_Bits_2015_CVPR_paper.pdf", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "xuelong_li@opt.ac.cn", "label": "xuelong_li@opt.ac.cn", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Stimulus", "label": "Stimulus", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "computational models", "label": "computational models", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "early approaches", "label": "early approaches", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "center-surround filters", "label": "center-surround filters", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "recent works", "label": "recent works", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "general computational principles", "label": "general computational principles", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "bits required by video compressor", "label": "bits required by video compressor", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "predictive power", "label": "predictive power", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "brain", "label": "brain", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "universal compression device", "label": "universal compression device", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Fixation Prediction", "label": "Fixation Prediction", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "view of the brain", "label": "view of the brain", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Agarwal et al. (2003)", "label": "Agarwal et al. (2003)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "compressed MPEG domain", "label": "compressed MPEG domain", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Hou and Zhang (2007)", "label": "Hou and Zhang (2007)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "spectral residual approach", "label": "spectral residual approach", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "salience detection method", "label": "salience detection method", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Helbing and Molnar (1995)", "label": "Helbing and Molnar (1995)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "pedestrian dynamics", "label": "pedestrian dynamics", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "EE CVPR\u201907", "label": "EE CVPR\u201907", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Agarwal, G.", "label": "Agarwal, G.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Anstis, S. M.", "label": "Anstis, S. M.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "perception of apparent movement", "label": "perception of apparent movement", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Attneave, F.", "label": "Attneave, F.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Informational aspects of visual perception", "label": "Informational aspects of visual perception", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Barlow, H.", "label": "Barlow, H.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Cerebral cortex as a model builder", "label": "Cerebral cortex as a model builder", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Redundancy reduction revisited", "label": "Redundancy reduction revisited", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Besag, J.", "label": "Besag, J.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Spatial interaction", "label": "Spatial interaction", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Royal Statistical Society", "label": "Royal Statistical Society", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Series B", "label": "Series B", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "36", "label": "36", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "192\u2013236", "label": "192\u2013236", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Ivan V. Bajic", "label": "Ivan V. Bajic", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Limin Wang", "label": "Limin Wang", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Action Recognition with Trajectory-Pooled Deep-Convolutional Descriptors", "label": "Action Recognition with Trajectory-Pooled Deep-Convolutional Descriptors", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Department of Information Engineering", "label": "Department of Information Engineering", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "07wanglimin@gmail.com", "label": "07wanglimin@gmail.com", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Yu Qiao", "label": "Yu Qiao", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "researcher", "label": "researcher", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "CAS", "label": "CAS", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Xiaoou Tang", "label": "Xiaoou Tang", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Action Recognized with Trajectory-Pooled Deep-Convolutional Descriptors", "label": "Action Recognized with Trajectory-Pooled Deep-Convolutional Descriptors", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "TDD", "label": "TDD", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "video representation", "label": "video representation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "trajectory-constrained pooling", "label": "trajectory-constrained pooling", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "feature maps", "label": "feature maps", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "normalization methods", "label": "normalization methods", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "TDDs", "label": "TDDs", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "deep-learned features", "label": "deep-learned features", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "HMD-B51", "label": "HMD-B51", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "UCF101", "label": "UCF101", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Human Action Recognition", "label": "Human Action Recognition", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Deep Convolutional Descriptors", "label": "Deep Convolutional Descriptors", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Trajectory-Constrained Pooling", "label": "Trajectory-Constrained Pooling", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Multi-view super vector", "label": "Multi-view super vector", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Convolutional Nets", "label": "Convolutional Nets", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Aggarwal, J. K., \u0026 Ryoo, M. S.", "label": "Aggarwal, J. K., \u0026 Ryoo, M. S.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Human activity analysis review", "label": "Human activity analysis review", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Bay, H., Tuytelaars, T., \u0026 Van Gool, L. J.", "label": "Bay, H., Tuytelaars, T., \u0026 Van Gool, L. J.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "SURF description", "label": "SURF description", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Karpathy et al.", "label": "Karpathy et al.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "HMDB", "label": "HMDB", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "video database", "label": "video database", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "human motion recognition", "label": "human motion recognition", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "T", "label": "T", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Jing Shao", "label": "Jing Shao", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Kai Kang", "label": "Kai Kang", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Chen Change Loy", "label": "Chen Change Loy", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Crowded scene understanding", "label": "Crowded scene understanding", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Deep model", "label": "Deep model", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "appearance features", "label": "appearance features", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "motion features", "label": "motion features", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Crowd motion channels", "label": "Crowd motion channels", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "deep model", "label": "deep model", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "generic properties of crowd systems", "label": "generic properties of crowd systems", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "WWW Crowd dataset", "label": "WWW Crowd dataset", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "10,000 videos", "label": "10,000 videos", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "8,257 crowded scenes", "label": "8,257 crowded scenes", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "cross-scene attribute recognition", "label": "cross-scene attribute recognition", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Attribute set", "label": "Attribute set", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "94 attributes", "label": "94 attributes", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Deep models", "label": "Deep models", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "significant performance improvements", "label": "significant performance improvements", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "deep models", "label": "deep models", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "feature-based baselines", "label": "feature-based baselines", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "deeply learned features", "label": "deeply learned features", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "multi-task learning", "label": "multi-task learning", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "attribute recognition", "label": "attribute recognition", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "cross-scene", "label": "cross-scene", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "crowd-related features", "label": "crowd-related features", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "baselines", "label": "baselines", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "feature-based", "label": "feature-based", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Deep learning models", "label": "Deep learning models", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "crowd motion channels", "label": "crowd motion channels", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Ali and Shah (2007)", "label": "Ali and Shah (2007)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "crowd flow segmentation", "label": "crowd flow segmentation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Ali and Shah (2008)", "label": "Ali and Shah (2008)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "tracking in high density crowd scenes", "label": "tracking in high density crowd scenes", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Andrade, Blunsden, and Fisher (2006)", "label": "Andrade, Blunsden, and Fisher (2006)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "event detection in crowd scenes", "label": "event detection in crowd scenes", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Chan and Vasconcelos (2008)", "label": "Chan and Vasconcelos (2008)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "eye tracking prior", "label": "eye tracking prior", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "dynamic textures", "label": "dynamic textures", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "oring", "label": "oring", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "CVPR 2008", "label": "CVPR 2008", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Vasconcelos, N.", "label": "Vasconcelos, N.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Dalal, N.", "label": "Dalal, N.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Triggers, B.", "label": "Triggers, B.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Farhad, A.", "label": "Farhad, A.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Describing objects by their attributes", "label": "Describing objects by their attributes", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Hospedales, T.", "label": "Hospedales, T.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "A markov clustering topic model", "label": "A markov clustering topic model", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Kang, K.", "label": "Kang, K.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Fully convolutional neural networks", "label": "Fully convolutional neural networks", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Wang, X.", "label": "Wang, X.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "dominant visual tracks", "label": "dominant visual tracks", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Object Extraction", "label": "Object Extraction", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "object search algorithm", "label": "object search algorithm", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "object boundaries", "label": "object boundaries", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "grabcut segmentation", "label": "grabcut segmentation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Intriligator \u0026 Cavanagh", "label": "Intriligator \u0026 Cavanagh", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Cognitive psychology article", "label": "Cognitive psychology article", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Itti, Koch, \u0026 Niebur", "label": "Itti, Koch, \u0026 Niebur", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "salience-based visual attention model", "label": "salience-based visual attention model", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "rapid scene analysis", "label": "rapid scene analysis", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "scene analysis", "label": "scene analysis", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Judd, Ehinger, Durand, \u0026 Torralba", "label": "Judd, Ehinger, Durand, \u0026 Torralba", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "human gaze prediction", "label": "human gaze prediction", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Video Segmentation", "label": "Video Segmentation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Energy Function Optimization", "label": "Energy Function Optimization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Judd et al.", "label": "Judd et al.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Learning to predict where humans look", "label": "Learning to predict where humans look", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Computer Vision, 2009 IEEE 12th international conference on", "label": "Computer Vision, 2009 IEEE 12th international conference on", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Karthikeyan et al. (2012)", "label": "Karthikeyan et al. (2012)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Uni\ufb01ed probabilistic framework", "label": "Uni\ufb01ed probabilistic framework", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Borji \u0026 Itti", "label": "Borji \u0026 Itti", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Borji, Sihite, \u0026 Itti", "label": "Borji, Sihite, \u0026 Itti", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Salient object detection: A benchmark", "label": "Salient object detection: A benchmark", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Computer Vision\u2013ECCV 2012", "label": "Computer Vision\u2013ECCV 2012", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "pages 414\u2013429", "label": "pages 414\u2013429", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Karthikeyan et al. (2013)", "label": "Karthikeyan et al. (2013)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Learning top-down scene context", "label": "Learning top-down scene context", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "ICIP, IEEE", "label": "ICIP, IEEE", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Karthikeyan, S.", "label": "Karthikeyan, S.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "University of California Santa Barbara", "label": "University of California Santa Barbara", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "{karthikeyan, thuyen, manj}@ece.ucsb.edu", "label": "{karthikeyan, thuyen, manj}@ece.ucsb.edu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "B.S. Manjunath", "label": "B.S. Manjunath", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Thuyen Ngo", "label": "Thuyen Ngo", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Miguel Eckstein", "label": "Miguel Eckstein", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "eckstein@psych.ucsb.edu", "label": "eckstein@psych.ucsb.edu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Felzenszwalb, P.", "label": "Felzenszwalb, P.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Eckstein, M. P.", "label": "Eckstein, M. P.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Visual search: A retrospective", "label": "Visual search: A retrospective", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Shervin Ardeshir", "label": "Shervin Ardeshir", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Geo-Semantic Segmentation", "label": "Geo-Semantic Segmentation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Ko\ufb01 Malcolm Collins-Sibley", "label": "Ko\ufb01 Malcolm Collins-Sibley", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Mubarak Shah", "label": "Mubarak Shah", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "University of  Central Florida", "label": "University of  Central Florida", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "shah@crcv.ucf.edu", "label": "shah@crcv.ucf.edu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "GIS databases", "label": "GIS databases", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "GIS projections alignment", "label": "GIS projections alignment", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "building locations", "label": "building locations", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "street locations", "label": "street locations", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "projections", "label": "projections", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "GPS errors", "label": "GPS errors", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "camera parameter inaccuracies", "label": "camera parameter inaccuracies", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "random walks", "label": "random walks", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "global transformations", "label": "global transformations", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "fast and efficient projections", "label": "fast and efficient projections", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "alignment", "label": "alignment", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "image processing technique", "label": "image processing technique", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "segmentations", "label": "segmentations", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "geo-references", "label": "geo-references", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "addresses", "label": "addresses", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "geo-locations", "label": "geo-locations", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Geo-semantic Segmentation", "label": "Geo-semantic Segmentation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Random Walks", "label": "Random Walks", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Global Transformations", "label": "Global Transformations", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Semantically Segmented Images", "label": "Semantically Segmented Images", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "GIS Data Integration", "label": "GIS Data Integration", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Iterative Data Fusion", "label": "Iterative Data Fusion", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Geo-references", "label": "Geo-references", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Addresses", "label": "Addresses", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "P. Zhao et al. [17]", "label": "P. Zhao et al. [17]", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Rectilinear parsing", "label": "Rectilinear parsing", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "O. Teboul et al. [10]", "label": "O. Teboul et al. [10]", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Segmentation of building facades", "label": "Segmentation of building facades", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "G. J. Brostow et al. [2]", "label": "G. J. Brostow et al. [2]", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Segmentation and recognition", "label": "Segmentation and recognition", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "EE", "label": "EE", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "2010", "label": "2010", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Brostow", "label": "Brostow", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Segmentation and recognition using structure from motion point clouds", "label": "Segmentation and recognition using structure from motion point clouds", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "He", "label": "He", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Multiscale conditional random fields for image labeling", "label": "Multiscale conditional random fields for image labeling", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "CVPR 2004", "label": "CVPR 2004", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Liu", "label": "Liu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Saturation-Preerving Specular Reflection Separation", "label": "Saturation-Preerving Specular Reflection Separation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "M\u00fcller", "label": "M\u00fcller", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Procedural modeling of buildings", "label": "Procedural modeling of buildings", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Musialski", "label": "Musialski", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Interactive coherence-based fac\u00b8ade modeling", "label": "Interactive coherence-based fac\u00b8ade modeling", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Computer Graphics Forum", "label": "Computer Graphics Forum", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "dings", "label": "dings", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Ardeshir", "label": "Ardeshir", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "University of Central Florida", "label": "University of Central Florida", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Gis-assisted object detection", "label": "Gis-assisted object detection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Lerma", "label": "Lerma", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Hoiem", "label": "Hoiem", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Automatic photo popup", "label": "Automatic photo popup", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Collins-Sibley", "label": "Collins-Sibley", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Northeaster University", "label": "Northeaster University", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Shah", "label": "Shah", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Huang", "label": "Huang", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Bayesian Inference", "label": "Bayesian Inference", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Chao-Tsung Huang", "label": "Chao-Tsung Huang", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Bayesian Inference for Neighborhood Filters", "label": "Bayesian Inference for Neighborhood Filters", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "collins-sibley.k@husky.neu.edu", "label": "collins-sibley.k@husky.neu.edu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Denoising", "label": "Denoising", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Huang_Bayesian_Infrenence_for_2015_CVPR_paper.pdf", "label": "Huang_Bayesian_Infrenence_for_2015_CVPR_paper.pdf", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Range-weighted neighborhood filters", "label": "Range-weighted neighborhood filters", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "edge-preserving denoising", "label": "edge-preserving denoising", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "limited theoretical understanding", "label": "limited theoretical understanding", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "unified empirical Bayesian framework", "label": "unified empirical Bayesian framework", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "filters", "label": "filters", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "range variance", "label": "range variance", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "accurate estimation", "label": "accurate estimation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "neighborhood noise model", "label": "neighborhood noise model", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Yaroslavsky, bilateral, and modified non-local means filters", "label": "Yaroslavsky, bilateral, and modified non-local means filters", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "EM+ algorithm", "label": "EM+ algorithm", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "color-image denoising", "label": "color-image denoising", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "model\u0027s effectiveness", "label": "model\u0027s effectiveness", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "recursive fitting", "label": "recursive fitting", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Paris, S.", "label": "Paris, S.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Bilateral filtering", "label": "Bilateral filtering", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Buades, A.", "label": "Buades, A.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "image denoising algorithms review", "label": "image denoising algorithms review", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "SIAM Journal on Multi-scale Modeling and Simulation", "label": "SIAM Journal on Multi-scale Modeling and Simulation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Chatterjee, P.", "label": "Chatterjee, P.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Patch-based near-optimal image denoising", "label": "Patch-based near-optimal image denoising", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Peng, H.", "label": "Peng, H.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Bilateral kernel parameter optimization", "label": "Bilateral kernel parameter optimization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Multispectral image denoising", "label": "Multispectral image denoising", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "International Conference on Image Processing", "label": "International Conference on Image Processing", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "vector bilateral filter", "label": "vector bilateral filter", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "image filtering technique", "label": "image filtering technique", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Vector bilateral filter", "label": "Vector bilateral filter", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Image denoising", "label": "Image denoising", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "scale mixtures of gausians", "label": "scale mixtures of gausians", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Bilateral filter", "label": "Bilateral filter", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Chao-Tsun Huang", "label": "Chao-Tsun Huang", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "National Tsing Hua University", "label": "National Tsing Hua University", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Lijun Wang", "label": "Lijun Wang", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Wang_Deep_Networks_for_2015_CVPR_paper", "label": "Wang_Deep_Networks_for_2015_CVPR_paper", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Wang_Deep_Networks_for_2015_CVPR_paper.pdf", "label": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Wang_Deep_Networks_for_2015_CVPR_paper.pdf", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Salience Detection", "label": "Salience Detection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Sparse Coding", "label": "Sparse Coding", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "DNN-L", "label": "DNN-L", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "object concepts", "label": "object concepts", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "global contrast", "label": "global contrast", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "local patch features", "label": "local patch features", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "salience value", "label": "salience value", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "DNN-G", "label": "DNN-G", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "salient score", "label": "salient score", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "salient object regions", "label": "salient object regions", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "weighted sum", "label": "weighted sum", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "saliency map", "label": "saliency map", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "object region", "label": "object region", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "saliency score", "label": "saliency score", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "global features", "label": "global features", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Algorithm", "label": "Algorithm", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Salient Region Detection", "label": "Salient Region Detection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "field", "label": "field", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Video Object Segmentation", "label": "Video Object Segmentation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "CVPR, 2012", "label": "CVPR, 2012", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "R. Achanta et al.", "label": "R. Achanta et al.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "J. Carreira and C. Sminchisescu", "label": "J. Carreira and C. Sminchisescu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Constrained parametric min-cuts", "label": "Constrained parametric min-cuts", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "automatic object segmentation", "label": "automatic object segmentation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "min-cut approach", "label": "min-cut approach", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Object Candidate Regions", "label": "Object Candidate Regions", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "tric min-cuts", "label": "tric min-cuts", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "visual salience", "label": "visual salience", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Itti, Koch, and Niebur (1998)", "label": "Itti, Koch, and Niebur (1998)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "computational model", "label": "computational model", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "saliency detection", "label": "saliency detection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "absorbing Markov chain", "label": "absorbing Markov chain", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Selective search", "label": "Selective search", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "generating object proposals", "label": "generating object proposals", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "preprocessing step", "label": "preprocessing step", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "salient object detection pipelines", "label": "salient object detection pipelines", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "simultaneous detection and segmentation", "label": "simultaneous detection and segmentation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "related task", "label": "related task", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Bayesian models", "label": "Bayesian models", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "ICC paper", "label": "ICC paper", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "ECCV paper", "label": "ECCV paper", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "ICIP paper", "label": "ICIP paper", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Bayesian model", "label": "Bayesian model", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Abhishek Kar", "label": "Abhishek Kar", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "akar@eecs.berkeley.edu", "label": "akar@eecs.berkeley.edu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Jo\u02dcao Carreira", "label": "Jo\u02dcao Carreira", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "carreira@eecs.berkeley.edu", "label": "carreira@eecs.berkeley.edu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Shubham Tulisiani", "label": "Shubham Tulisiani", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Thorsten Beier", "label": "Thorsten Beier", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Fusion Moves for Correlation Clustering", "label": "Fusion Moves for Correlation Clustering", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "University of Heidelberg", "label": "University of Heidelberg", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "thorsten.beier@iwr.uni-heidelberg.de", "label": "thorsten.beier@iwr.uni-heidelberg.de", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Beier_Fusion_Moves_for_2015_CVPR_supplemental", "label": "Beier_Fusion_Moves_for_2015_CVPR_supplemental", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "University of Heidelberg (Iwr)", "label": "University of Heidelberg (Iwr)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Beier_Fusion_Moves_for_2015_CVPR_supplemental.pdf", "label": "Beier_Fusion_Moves_for_2015_CVPR_supplemental.pdf", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Fred A. Hamprecht", "label": "Fred A. Hamprecht", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "fred.hamprecht@iwr.uni-heidelberg.de", "label": "fred.hamprecht@iwr.uni-heidelberg.de", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "J\u00f6rg H. Kappes", "label": "J\u00f6rg H. Kappes", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Math", "label": "Math", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "kappes@math.uni-heidelberg.de", "label": "kappes@math.uni-heidelberg.de", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Hossein Rahmani", "label": "Hossein Rahmani", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "The University of Western Australia", "label": "The University of Western Australia", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Learning a Non-linear Knowledge Transfer Model for Cross-View Action Recognition", "label": "Learning a Non-linear Knowledge Transfer Model for Cross-View Action Recognition", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Rahmani_Learning_a_Non-Linear_2015_CVPR_paper.pdf", "label": "Rahmani_Learning_a_Non-Linear_2015_CVPR_paper.pdf", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Hossein Rahman\u0131", "label": "Hossein Rahman\u0131", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "hossein@csse.uwa.edu.au", "label": "hossein@csse.uwa.edu.au", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Ajmal Mian", "label": "Ajmal Mian", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "The University of Western Canada", "label": "The University of Western Canada", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "ajmal.mian@uwa.edu.au", "label": "ajmal.mian@uwa.edu.au", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Convex Multi-task Cluster Learning", "label": "Convex Multi-task Cluster Learning", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Laplacian Eigenmaps", "label": "Laplacian Eigenmaps", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "SKMTL problem", "label": "SKMTL problem", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "jointly convex", "label": "jointly convex", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "tasks", "label": "tasks", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "benchmarks", "label": "benchmarks", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "progress", "label": "progress", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "sparse structure", "label": "sparse structure", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "settings beyond computer vision", "label": "settings beyond computer vision", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Sparse Kernel Multi-task Learning", "label": "Sparse Kernel Multi-task Learning", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Joint Convexity", "label": "Joint Convexity", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Cluster Multi-task Learning", "label": "Cluster Multi-task Learning", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Robotics", "label": "Robotics", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Sarcos dataset", "label": "Sarcos dataset", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "ill-solved problem", "label": "ill-solved problem", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Zygmunt L. Szpak", "label": "Zygmunt L. Szpak", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Wojciech Chojnacki", "label": "Wojciech Chojnacki", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Szpak_Robust_Multiple_Homography_2015_CVPR_paper.pdf", "label": "Szpak_Robust_Multiple_Homography_2015_CVPR_paper.pdf", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "consistency constraints", "label": "consistency constraints", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "homographies", "label": "homographies", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "new constraints", "label": "new constraints", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "views", "label": "views", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "epipolar geometries", "label": "epipolar geometries", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "inconsistent", "label": "inconsistent", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "robust multi-structure estimation methods", "label": "robust multi-structure estimation methods", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "enforcing constraints on homography matrices", "label": "enforcing constraints on homography matrices", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "multi-structure estimation methods", "label": "multi-structure estimation methods", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "enforcing constraints", "label": "enforcing constraints", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "homography matrices", "label": "homography matrices", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "new generation", "label": "new generation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "critiques", "label": "critiques", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Robust Multi-Structure Estimation", "label": "Robust Multi-Structure Estimation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "constraints on homography matrices", "label": "constraints on homography matrices", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Homography Matrices", "label": "Homography Matrices", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Projective Geometry", "label": "Projective Geometry", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Epiopolar Geometry", "label": "Epiopolar Geometry", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Baker, S., Datta, A., and Kanade, T.", "label": "Baker, S., Datta, A., and Kanade, T.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Parameterizing homographies", "label": "Parameterizing homographies", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "tech. rep. CMU-RI-TR-06-11", "label": "tech. rep. CMU-RI-TR-06-11", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Bernstein, D. S.", "label": "Bernstein, D. S.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Matrix Mathematics", "label": "Matrix Mathematics", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Chen, P., and Suter, D.", "label": "Chen, P., and Suter, D.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Rank constraints for homographies", "label": "Rank constraints for homographies", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Chen, P.", "label": "Chen, P.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Rank constraints", "label": "Rank constraints", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Suter, D.", "label": "Suter, D.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Chojnacki, W.", "label": "Chojnacki, W.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Multiple homography estimation", "label": "Multiple homography estimation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Dimensionality result", "label": "Dimensionality result", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Szpak, Z.", "label": "Szpak, Z.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "van den Hengel, A.", "label": "van den Hengel, A.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Fouhey, D. F.", "label": "Fouhey, D. F.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Multiple plane detection", "label": "Multiple plane detection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "J-linkage", "label": "J-linkage", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Scharstein, D.", "label": "Scharstein, D.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Briggs, A. J.", "label": "Briggs, A. J.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Goldberger, J.", "label": "Goldberger, J.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Camera projection matrices", "label": "Camera projection matrices", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Goldberger", "label": "Goldberger", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Reconstructing camera projection matrices", "label": "Reconstructing camera projection matrices", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Irving", "label": "Irving", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Integers, Polynomials, and Rings", "label": "Integers, Polynomials, and Rings", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Szpak", "label": "Szpak", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Chojnicki", "label": "Chojnicki", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "van den Hengel", "label": "van den Hengel", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Yuan", "label": "Yuan", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Zheng", "label": "Zheng", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Wu", "label": "Wu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "University of Delaware", "label": "University of Delaware", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Robust Regression on Image Manifolds", "label": "Robust Regression on Image Manifolds", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "nianyi@eecis.udel.edu", "label": "nianyi@eecis.udel.edu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Yuanliu Liu", "label": "Yuanliu Liu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Saturation-Preerving Specular Reflection Paper", "label": "Saturation-Preerving Specular Reflection Paper", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Institute of Artificial AI and Robotics", "label": "Institute of Artificial AI and Robotics", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Institute of Artificial Intelligence and Robotics", "label": "Institute of Artificial Intelligence and Robotics", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Zejian Yuan", "label": "Zejian Yuan", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Nanning Zheng", "label": "Nanning Zheng", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Yang Wu", "label": "Yang Wu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Nara Institute of Science and Technology", "label": "Nara Institute of Science and Technology", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Reflection", "label": "Reflection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Specular Reflection", "label": "Specular Reflection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Saturation-Preserving Specular Reflection Paper", "label": "Saturation-Preserving Specular Reflection Paper", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Linear Programming", "label": "Linear Programming", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Diffuse Reflection", "label": "Diffuse Reflection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Liu_Saturation-Preerving Specular Reflection Paper", "label": "Liu_Saturation-Preerving Specular Reflection Paper", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Liu_Saturation-Preerving_Specular_Reflection_2015_CVPR_paper.pdf", "label": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Liu_Saturation-Preerving_Specular_Reflection_2015_CVPR_paper.pdf", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Specular reflection", "label": "Specular reflection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "saturation of surface colors", "label": "saturation of surface colors", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "decreased saturation", "label": "decreased saturation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "confusion with other colors", "label": "confusion with other colors", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Traditional methods", "label": "Traditional methods", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "hue-saturation ambiguity", "label": "hue-saturation ambiguity", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Specular-free images", "label": "Specular-free images", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "oversaturated", "label": "oversaturated", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "This paper", "label": "This paper", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "two-step approach", "label": "two-step approach", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "over-saturated specular-free image", "label": "over-saturated specular-free image", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "global chromaticity propagation", "label": "global chromaticity propagation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Saturation", "label": "Saturation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "piecewise constancy of diffuse chromaticity", "label": "piecewise constancy of diffuse chromaticity", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "spatial sparsity/smoothness of specular reflection", "label": "spatial sparsity/smoothness of specular reflection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "achieved by increasing", "label": "achieved by increasing", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "linear programming", "label": "linear programming", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "increase achromatic component", "label": "increase achromatic component", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "diffuse chromaticity", "label": "diffuse chromaticity", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "chromaticity", "label": "chromaticity", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "spatial sparsity", "label": "spatial sparsity", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "surface colors", "label": "surface colors", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "achromatic component", "label": "achromatic component", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "saturation", "label": "saturation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Diffuse Chromaticity", "label": "Diffuse Chromaticity", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "surface color saturation", "label": "surface color saturation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "reflection component separation", "label": "reflection component separation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Shafer, S.", "label": "Shafer, S.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Artusi, A. et al.", "label": "Artusi, A. et al.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "survey of specular removal methods", "label": "survey of specular removal methods", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Hue-Saturation Ambiguity", "label": "Hue-Saturation Ambiguity", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Chromaticity Propagation", "label": "Chromaticity Propagation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Diffuse and specular interface reflections", "label": "Diffuse and specular interface reflections", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Gonzalez \u0026 Woods", "label": "Gonzalez \u0026 Woods", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Digital Image Processing", "label": "Digital Image Processing", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Land \u0026 McCann", "label": "Land \u0026 McCann", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "retinex theory", "label": "retinex theory", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Kim et al.", "label": "Kim et al.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "dark channel prior", "label": "dark channel prior", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "specular reflection separation", "label": "specular reflection separation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Mallick et al.", "label": "Mallick et al.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "specular surfaces", "label": "specular surfaces", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "color information", "label": "color information", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "S. P.", "label": "S. P.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "reconstructing specular surfaces", "label": "reconstructing specular surfaces", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Zickler", "label": "Zickler", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "T.", "label": "T.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "P. N. Belhumeur", "label": "P. N. Belhumeur", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "D. J. Kriegman", "label": "D. J. Kriegman", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "P., Zickler, T., Belhumeur, P. N., \u0026 Kriegman, D. J.", "label": "P., Zickler, T., Belhumeur, P. N., \u0026 Kriegman, D. J.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Lin, S., \u0026 Shum, H.-Y.", "label": "Lin, S., \u0026 Shum, H.-Y.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "separation of diffuse and specular reflection", "label": "separation of diffuse and specular reflection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Tan, R. T., Nishino, K., \u0026 Ikeuchi, K.", "label": "Tan, R. T., Nishino, K., \u0026 Ikeuchi, K.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Mallick, S. P., Zickler, T., Kriegman, D. J., \u0026 Belhumeur, P. N.", "label": "Mallick, S. P., Zickler, T., Kriegman, D. J., \u0026 Belhumeur, P. N.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "PDE approach", "label": "PDE approach", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "specular removal", "label": "specular removal", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "diffuse reflection", "label": "diffuse reflection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Wuyuan Xie", "label": "Wuyuan Xie", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Photometric Stereo with Near Point Lighting", "label": "Photometric Stereo with Near Point Lighting", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Chengkai Dai", "label": "Chengkai Dai", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Charlie C. L. Wang", "label": "Charlie C. L. Wang", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Xie_Photometric_Stereo_With_2015_CVPR_paper.pdf", "label": "Xie_Photometric_Stereo_With_2015_CVPR_paper.pdf", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "near point lighting", "label": "near point lighting", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "nonlinear relationship", "label": "nonlinear relationship", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "local surface normals", "label": "local surface normals", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "light source positions", "label": "light source positions", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "facet position", "label": "facet position", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "facet orientation", "label": "facet orientation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "local projection", "label": "local projection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "global blending", "label": "global blending", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Photometric Stereo", "label": "Photometric Stereo", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Nonlinear Optimization", "label": "Nonlinear Optimization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "S. Barsky", "label": "S. Barsky", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "4-source photometric stereo technique", "label": "4-source photometric stereo technique", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "highlights and shadows", "label": "highlights and shadows", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "D. Nehab", "label": "D. Nehab", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Efficiently combining positions and normals", "label": "Efficiently combining positions and normals", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "precise 3d geometry", "label": "precise 3d geometry", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "A. Hertzmann", "label": "A. Hertzmann", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Example-based photometric stereo", "label": "Example-based photometric stereo", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "shape", "label": "shape", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Shape and spatially-ranging brdfs", "label": "Shape and spatially-ranging brdfs", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Mesh Deformation", "label": "Mesh Deformation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Near Point Lighting", "label": "Near Point Lighting", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Photometric stereo", "label": "Photometric stereo", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Computer Vision Workshops (ICCV Workshops)", "label": "Computer Vision Workshops (ICCV Workshops)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "point light sources", "label": "point light sources", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Shape", "label": "Shape", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Proceedings of the Fifth Eurographics Symposium on Geometry Processing", "label": "Proceedings of the Fifth Eurographics Symposium on Geometry Processing", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Surface orientation", "label": "Surface orientation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "photometric method", "label": "photometric method", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Surface modeling", "label": "Surface modeling", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "as-rigid-as-possible surface modeling", "label": "as-rigid-as-possible surface modeling", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Discrete geometry", "label": "Discrete geometry", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "BRDF", "label": "BRDF", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "urographics Symposium on Geometry Processing", "label": "urographics Symposium on Geometry Processing", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "2007", "label": "2007", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "S. Bouaziz", "label": "S. Bouaziz", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "existing binary codes learning methods", "label": "existing binary codes learning methods", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "single linear projection", "label": "single linear projection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "hierarchical non-linear transformations", "label": "hierarchical non-linear transformations", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "nonlinear relationship of samples", "label": "nonlinear relationship of samples", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "real-valued feature descriptor", "label": "real-valued feature descriptor", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "different bits", "label": "different bits", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "independent", "label": "independent", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "each other", "label": "each other", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "nary vector", "label": "nary vector", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "learned binary codes", "label": "learned binary codes", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "discriminative power", "label": "discriminative power", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Deep Hashing", "label": "Deep Hashing", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Binary Codes Learning", "label": "Binary Codes Learning", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Large-Scale Visual Search", "label": "Large-Scale Visual Search", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "maximize inter-class variations", "label": "maximize inter-class variations", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "minimize intra-class variations", "label": "minimize intra-class variations", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Hashing Functions", "label": "Hashing Functions", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Deep HHashing", "label": "Deep HHashing", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Andoni \u0026 Indyk (2006)", "label": "Andoni \u0026 Indyk (2006)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Near-optimal Hashing Algorithms", "label": "Near-optimal Hashing Algorithms", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Approximate Nearest Neighbor Search", "label": "Approximate Nearest Neighbor Search", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Gong et al. (2012)", "label": "Gong et al. (2012)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Angular Quantization-based Binary Codes", "label": "Angular Quantization-based Binary Codes", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Fast Similarity Search", "label": "Fast Similarity Search", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Hinton \u0026 Salakhutdinov (2006)", "label": "Hinton \u0026 Salakhutdinov (2006)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Reducing Data Dimensionality", "label": "Reducing Data Dimensionality", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Neural Networks", "label": "Neural Networks", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "data dimensionality", "label": "data dimensionality", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Object Recognition", "label": "Object Recognition", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Tiny Images", "label": "Tiny Images", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "nonparametric object recognition", "label": "nonparametric object recognition", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Hash Bit Selection", "label": "Hash Bit Selection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "unified solution", "label": "unified solution", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Shift-invariant kernels", "label": "Shift-invariant kernels", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "locality-sensitive binary codes", "label": "locality-sensitive binary codes", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Minimal Loss Hashing", "label": "Minimal Loss Hashing", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Science", "label": "Science", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Torralba, A.", "label": "Torralba, A.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "80 million tiny images", "label": "80 million tiny images", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "*PAM*I", "label": "*PAM*I", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Wang, J.", "label": "Wang, J.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Venice Erin Liong", "label": "Venice Erin Liong", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Gang Wang", "label": "Gang Wang", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Jie Zhou", "label": "Jie Zhou", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Dongyoon Han", "label": "Dongyoon Han", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "jzhou@tsinghua.edu.cn", "label": "jzhou@tsinghua.edu.cn", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Han_Unpublished_Simultaneous_Orthogonal_2015_CVPR_paper", "label": "Han_Unpublished_Simultaneous_Orthogonal_2015_CVPR_paper", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Han_Unsupervised_Simultaneous_Orthogonal_2015_CVPR_paper", "label": "Han_Unsupervised_Simultaneous_Orthogonal_2015_CVPR_paper", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "feature selection methods", "label": "feature selection methods", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "supervised and unsupervised feature selection methods", "label": "supervised and unsupervised feature selection methods", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "SOCFS", "label": "SOCFS", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "unsupervised feature selection method", "label": "unsupervised feature selection method", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "feature selection on unlabeled data", "label": "feature selection on unlabeled data", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "regularized regression-based formulation", "label": "regularized regression-based formulation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "real world datasets", "label": "real world datasets", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "target matrix", "label": "target matrix", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "latent cluster centers", "label": "latent cluster centers", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "projection matrix", "label": "projection matrix", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "discriminative features", "label": "discriminative features", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "sparse nature", "label": "sparse nature", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Nie et al.", "label": "Nie et al.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "feature selection via joint l2,1-norms minimization", "label": "feature selection via joint l2,1-norms minimization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Nene et al.", "label": "Nene et al.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Columbia object image library (coil-20)", "label": "Columbia object image library (coil-20)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "CCUCS-005-96", "label": "CCUCS-005-96", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Yang et al.", "label": "Yang et al.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "l2,1-norm regularized discriminative feature selection", "label": "l2,1-norm regularized discriminative feature selection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Qian and Zhai", "label": "Qian and Zhai", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Robust unsupervised feature selection", "label": "Robust unsupervised feature selection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "IJCAI", "label": "IJCAI", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Sch\u00a8onemann", "label": "Sch\u00a8onemann", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "generalized solution of the orthogonal Procustes problem", "label": "generalized solution of the orthogonal Procustes problem", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Zhao and Liu", "label": "Zhao and Liu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Spectral feature selection", "label": "Spectral feature selection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Procrustes problem", "label": "Procrustes problem", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Psychometrika", "label": "Psychometrika", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Zhao, Z.", "label": "Zhao, Z.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Samaria, F. S.", "label": "Samaria, F. S.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "stochastic model", "label": "stochastic model", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Jianming Zhang", "label": "Jianming Zhang", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Boston University", "label": "Boston University", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Shugao Ma", "label": "Shugao Ma", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Mehrnooush Sameki", "label": "Mehrnooush Sameki", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Stan Sclaroff", "label": "Stan Sclaroff", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Margrit Betke", "label": "Margrit Betke", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Radom\u00edr M\u011bch", "label": "Radom\u00edr M\u011bch", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "People", "label": "People", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "subitizing", "label": "subitizing", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Salient Object Subitizing (SOS)", "label": "Salient Object Subitizing (SOS)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "predict existence and number", "label": "predict existence and number", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Crowd Sourcing", "label": "Crowd Sourcing", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Holistic Image Analysis", "label": "Holistic Image Analysis", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Computer Vision Applications", "label": "Computer Vision Applications", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "robot vision", "label": "robot vision", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Mehrnoosh Sameki", "label": "Mehrnoosh Sameki", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Radom\u00b4\u0131r M\u02d8ech", "label": "Radom\u00b4\u0131r M\u02d8ech", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Richter_Discriminaitve_Shape_From_2015_CVPR_paper.pdf", "label": "Richter_Discriminaitve_Shape_From_2015_CVPR_paper.pdf", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Estimating surface normals", "label": "Estimating surface normals", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "challenging problem", "label": "challenging problem", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "under-constrained problem", "label": "under-constrained problem", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Simplifying assumptions", "label": "Simplifying assumptions", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "directional lighting", "label": "directional lighting", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "known reflectance maps", "label": "known reflectance maps", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "regression forests", "label": "regression forests", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Von Mises-Fisher distributions", "label": "Von Mises-Fisher distributions", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "textons", "label": "textons", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "novel silhouette features", "label": "novel silhouette features", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "uncalibrated illumination", "label": "uncalibrated illumination", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "pixel-independent prediction", "label": "pixel-independent prediction", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "efficient estimation", "label": "efficient estimation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Discrimiative Learning", "label": "Discrimiative Learning", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "J. T. Barron", "label": "J. T. Barron", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Color constancy, intrinsic images, and shape estimation", "label": "Color constancy, intrinsic images, and shape estimation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Shape, albedo, and illumination from a single image", "label": "Shape, albedo, and illumination from a single image", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "J. Ben-Arie", "label": "J. Ben-Arie", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "A neural network approach", "label": "A neural network approach", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "L. Breiman", "label": "L. Breiman", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "ShapeCollage", "label": "ShapeCollage", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "example-based methods", "label": "example-based methods", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "occlusion-aware shape interpretation", "label": "occlusion-aware shape interpretation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Image-to-geometry registration", "label": "Image-to-geometry registration", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Modeling data", "label": "Modeling data", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "directional distributions", "label": "directional distributions", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Dispersion", "label": "Dispersion", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "P. Roy. Soc. Lond. B", "label": "P. Roy. Soc. Lond. B", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Floating scale reconstruction", "label": "Floating scale reconstruction", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "SIGGRAPH", "label": "SIGGRAPH", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Debevec et al.", "label": "Debevec et al.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Adelson", "label": "Adelson", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "W. T. Freeman", "label": "W. T. Freeman", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Jean-Dominique FAVREAU", "label": "Jean-Dominique FAVREAU", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "INRIA Sophia-Antippolis", "label": "INRIA Sophia-Antippolis", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "3D Scene Understanding", "label": "3D Scene Understanding", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Adrien Bousseau", "label": "Adrien Bousseau", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "INRIA Sophia-Antipollis", "label": "INRIA Sophia-Antipollis", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "INRIAL Sophia-Antipolis", "label": "INRIAL Sophia-Antipolis", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "line drawings", "label": "line drawings", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "imaginary objects", "label": "imaginary objects", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "photographs", "label": "photographs", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "desired scene", "label": "desired scene", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "undesired reflections", "label": "undesired reflections", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "computer vision algorithms", "label": "computer vision algorithms", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "limited support", "label": "limited support", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "multi-view stereo algorithms", "label": "multi-view stereo algorithms", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "real-world scenes", "label": "real-world scenes", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "line-drawing interpretation algorithms", "label": "line-drawing interpretation algorithms", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "contextual awareness", "label": "contextual awareness", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "orientation", "label": "orientation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "real world", "label": "real world", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "new orientation", "label": "new orientation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "creation of new structures", "label": "creation of new structures", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "unknown orientation", "label": "unknown orientation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Computer-Aided Design", "label": "Computer-Aided Design", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Furniture Design", "label": "Furniture Design", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "O-snap", "label": "O-snap", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "optimization-based snapping method", "label": "optimization-based snapping method", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "M. Arikan", "label": "M. Arikan", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "H. Barrow", "label": "H. Barrow", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Y. Boykov", "label": "Y. Boykov", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "energy minimization algorithms", "label": "energy minimization algorithms", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "A.-L. Chauve", "label": "A.-L. Chauve", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "3D reconstruction methods", "label": "3D reconstruction methods", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Multi-View Stereo Reconstruction", "label": "Multi-View Stereo Reconstruction", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Manhattan-world stereo", "label": "Manhattan-world stereo", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Wiley-ISTE", "label": "Wiley-ISTE", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Stochastic geometry for image analysis", "label": "Stochastic geometry for image analysis", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Florent LAFARGE", "label": "Florent LAFARGE", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "INRIA Sophia-Antipolis", "label": "INRIA Sophia-Antipolis", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Bis Publishers", "label": "Bis Publishers", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": " Sketching: The Basics", "label": " Sketching: The Basics", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "jean-dominuque.favreau@inria.fr", "label": "jean-dominuque.favreau@inria.fr", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Olga Russakovsky", "label": "Olga Russakovsky", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Best of both worlds", "label": "Best of both worlds", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Stanford University", "label": "Stanford University", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Li-Jia Li", "label": "Li-Jia Li", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Snapchat", "label": "Snapchat", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Li Fei-Fei", "label": "Li Fei-Fei", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Russakovsky_Best_of_Both_2015_CVPR_paper.pdf", "label": "Russakovsky_Best_of_Both_2015_CVPR_paper.pdf", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "jean-dominique.favreau@inria.fr", "label": "jean-dominique.favreau@inria.fr", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "jean-dominique.favreau", "label": "jean-dominique.favreau", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "florent.lafarge@inria.fr", "label": "florent.lafarge@inria.fr", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "adrien.bousseau@inria.fr", "label": "adrien.bousseau@inria.fr", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "manual annotation", "label": "manual annotation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "quite expensive", "label": "quite expensive", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "crowd engineering innovations", "label": "crowd engineering innovations", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "automatic object detectors", "label": "automatic object detectors", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "at most a few objects per image", "label": "at most a few objects per image", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "human feedback", "label": "human feedback", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "feedback", "label": "feedback", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Visesh Chari", "label": "Visesh Chari", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "On Pairwise Costs for Network Flow Multi-Object Tracking", "label": "On Pairwise Costs for Network Flow Multi-Object Tracking", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "INRIA", "label": "INRIA", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Ecole Normale Sup\u00b4erieure", "label": "Ecole Normale Sup\u00b4erieure", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Ivan Laptev", "label": "Ivan Laptev", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Josef Sivic", "label": "Josef Sivic", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Chari_On_Pairwise_Costs_2015_CVPR_supplemental", "label": "Chari_On_Pairwise_Costs_2015_CVPR_supplemental", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Multi-object Tracking", "label": "Multi-object Tracking", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Network Flow Optimization", "label": "Network Flow Optimization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "dependencies among tracks", "label": "dependencies among tracks", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Pairwise Costs", "label": "Pairwise Costs", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "object detector failures", "label": "object detector failures", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "min-cost network flow framework", "label": "min-cost network flow framework", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Convex Relaxation", "label": "Convex Relaxation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "efficient rounding heuristic", "label": "efficient rounding heuristic", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "pairwise costs", "label": "pairwise costs", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "real-world video sequences", "label": "real-world video sequences", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "recent tracking methods", "label": "recent tracking methods", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Tracking-by-Detection", "label": "Tracking-by-Detection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "TILDE", "label": "TILDE", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Temporally Invariant Learned Detector", "label": "Temporally Invariant Learned Detector", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Yannick Verdie", "label": "Yannick Verdie", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Computer Vision Laboratory, EPFL", "label": "Computer Vision Laboratory, EPFL", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Kwang Moo Yi", "label": "Kwang Moo Yi", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Pascal Fua", "label": "Pascal Fua", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Vincent Lepetit", "label": "Vincent Lepetit", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Simon Lacoste-Julien", "label": "Simon Lacoste-Julien", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "INRIO", "label": "INRIO", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Cortes, C.", "label": "Cortes, C.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Support-Vector Networks", "label": "Support-Vector Networks", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Vapnik, V.", "label": "Vapnik, V.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Harris, C.", "label": "Harris, C.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Combined Corner and Edge Detector", "label": "Combined Corner and Edge Detector", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Stephens, M.", "label": "Stephens, M.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "C.", "label": "C.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Corner Detector", "label": "Corner Detector", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Hinging Hyperplanes", "label": "Hinging Hyperplanes", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "IEEE Transactions on Information Theory", "label": "IEEE Transactions on Information Theory", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Gradient-Based Learning", "label": "Gradient-Based Learning", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Document Recognition", "label": "Document Recognition", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Af\ufb01ne Region Detectors", "label": "Af\ufb01ne Region Detectors", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Mikolajczyk, K.", "label": "Mikolajczyk, K.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "A Comparison of Af\ufb01ne Region Detectors", "label": "A Comparison of Af\ufb01ne Region Detectors", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "rs", "label": "rs", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Zisserma", "label": "Zisserma", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Mata", "label": "Mata", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Schaffalitzky", "label": "Schaffalitzky", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Kadir", "label": "Kadir", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Van Gool", "label": "Van Gool", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "A Review of Af\ufb01ne Region Detectors", "label": "A Review of Af\ufb01ne Region Detectors", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Dollar", "label": "Dollar", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Supervised Learning of Edges and Object Boundaries", "label": "Supervised Learning of Edges and Object Boundaries", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Tu", "label": "Tu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Belongie", "label": "Belongie", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Rosten", "label": "Rosten", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Machine Learning for High-Speed Corner Detection", "label": "Machine Learning for High-Speed Corner Detection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Drummond", "label": "Drummond", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Lowe", "label": "Lowe", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Distinctive Image Features from Scale-Invariant Keypoints", "label": "Distinctive Image Features from Scale-Invariant Keypoints", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Fan", "label": "Fan", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Chang", "label": "Chang", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Hsieh", "label": "Hsieh", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Lin", "label": "Lin", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Verdie", "label": "Verdie", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "EPFL", "label": "EPFL", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Yi", "label": "Yi", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Vincent Le Petit", "label": "Vincent Le Petit", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Joint Online Tracking and Segmentation", "label": "Joint Online Tracking and Segmentation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Video Segmentation task", "label": "Video Segmentation task", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Multi-part tracking", "label": "Multi-part tracking", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "SegTrack database", "label": "SegTrack database", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "SegTrack v2 database", "label": "SegTrack v2 database", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Longyin Wen", "label": "Longyin Wen", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "NLPR, Institute of Automation, Chinese Academy of Sciences", "label": "NLPR, Institute of Automation, Chinese Academy of Sciences", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Dawei Du", "label": "Dawei Du", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "JETS", "label": "JETS", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "SCCE, University of Chinese Academy of Sciences", "label": "SCCE, University of Chinese Academy of Sciences", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Zhen Lei", "label": "Zhen Lei", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "High-Fidelity Pose and Expression Normalization for Face Recognition in the Wild", "label": "High-Fidelity Pose and Expression Normalization for Face Recognition in the Wild", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "High-Fidelity Pose and Expression Normalization for FaceRecognition in the Wild", "label": "High-Fidelity Pose and Expression Normalization for FaceRecognition in the Wild", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Stan Z. Li", "label": "Stan Z. Li", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "NLPR", "label": "NLPR", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "szli@nlpr.ia.ac.cn", "label": "szli@nlpr.ia.ac.cn", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Wen_JOTS_Joint_Online_2015_CVPR_paper.pdf", "label": "Wen_JOTS_Joint_Online_2015_CVPR_paper.pdf", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Tracking and Segmentation stages", "label": "Tracking and Segmentation stages", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "RANSA-style approach", "label": "RANSA-style approach", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Multi-part Models", "label": "Multi-part Models", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Tracking and Segmentation", "label": "Tracking and Segmentation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "multi-target tracking", "label": "multi-target tracking", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Vasconcelos", "label": "Vasconcelos", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "tracking deformable and occluded objects", "label": "tracking deformable and occluded objects", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "S. Z. Li", "label": "S. Z. Li", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Delong", "label": "Delong", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "optimization method", "label": "optimization method", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Philipp Kr\u00e4henbuhl", "label": "Philipp Kr\u00e4henbuhl", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Learning to Propose Objects", "label": "Learning to Propose Objects", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Vladlen Koltun", "label": "Vladlen Koltun", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Krahenbuhl_Learning_to_Propos_2015_CVPR_paper.pdf", "label": "Krahenbuhl_Learning_to_Propos_2015_CVPR_paper.pdf", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "zlei", "label": "zlei", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "ensemble", "label": "ensemble", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "jointly", "label": "jointly", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "elementary image features", "label": "elementary image features", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "rapid image analysis", "label": "rapid image analysis", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "composition", "label": "composition", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "parameters", "label": "parameters", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "ensemble training", "label": "ensemble training", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "sequence of uncapacitated facility location problems", "label": "sequence of uncapacitated facility location problems", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "procedure", "label": "procedure", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "size of the ensemble", "label": "size of the ensemble", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "cliques", "label": "cliques", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "ensembles", "label": "ensembles", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "presented approach", "label": "presented approach", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "prior object proposal algorithms", "label": "prior object proposal algorithms", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "lowest running time", "label": "lowest running time", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "bottom-up segmentation model", "label": "bottom-up segmentation model", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "generally applicable model", "label": "generally applicable model", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "trained ensembles", "label": "trained ensembles", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Ensemble Methods", "label": "Ensemble Methods", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "running time", "label": "running time", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Arbel\u00e1ez et al. (2012)", "label": "Arbel\u00e1ez et al. (2012)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Semantic segmentation using regions and parts", "label": "Semantic segmentation using regions and parts", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Carreira et al.", "label": "Carreira et al.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Free-form region description with second-order pooling", "label": "Free-form region description with second-order pooling", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Microsoft COCO", "label": "Microsoft COCO", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "common objects", "label": "common objects", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Objectness", "label": "Objectness", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Structured forests", "label": "Structured forests", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "BING", "label": "BING", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Geometry of cuts and metrics", "label": "Geometry of cuts and metrics", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Aravindh Mahendran", "label": "Aravindh Mahendran", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Understanding Deep Image Representations", "label": "Understanding Deep Image Representations", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Understanding Deep Image Representations by Inverting Them", "label": "Understanding Deep Image Representations by Inverting Them", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Intel Labs", "label": "Intel Labs", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Andrea Vedaldi", "label": "Andrea Vedaldi", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Deep Image Representations", "label": "Deep Image Representations", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Understanding Deep Image Presentations by Inverting Them", "label": "Understanding Deep Image Presentations by Inverting Them", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Mahendran_Understanding_Deep_Image_2015_CVPR_paper.pdf", "label": "Mahendran_Understanding_Deep_Image_2015_CVPR_paper.pdf", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Image Representations", "label": "Image Representations", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "geometric and photometric invariance", "label": "geometric and photometric invariance", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Bishop", "label": "Bishop", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Neural Networks for Pattern Recognition", "label": "Neural Networks for Pattern Recognition", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "3D textons", "label": "3D textons", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "SIFT detector", "label": "SIFT detector", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "open-source implementation", "label": "open-source implementation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Zeiler \u0026 Fergus", "label": "Zeiler \u0026 Fergus", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "visualizing convolutional networks", "label": "visualizing convolutional networks", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Hinton \u0026 Salakhutdinov", "label": "Hinton \u0026 Salakhutdinov", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Wang et al.", "label": "Wang et al.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "locality-constrained linear coding", "label": "locality-constrained linear coding", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Arvindh Mahendran", "label": "Arvindh Mahendran", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "University of Oxford", "label": "University of Oxford", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Yan Xia", "label": "Yan Xia", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Sparse Projections", "label": "Sparse Projections", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "University of Science and Technology of China", "label": "University of Science and Technology of China", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Computationally Bounded Retrieval", "label": "Computationally Bounded Retrieval", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "number of parameters", "label": "number of parameters", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "overfitting", "label": "overfitting", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "sparse projection matrix", "label": "sparse projection matrix", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "reduction in computational cost", "label": "reduction in computational cost", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "ITQ", "label": "ITQ", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "rix", "label": "rix", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "high-dimensional binary encoding", "label": "high-dimensional binary encoding", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Agrawal et al. (2014)", "label": "Agrawal et al. (2014)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Foundational context", "label": "Foundational context", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Local Scale-Invariant Features", "label": "Local Scale-Invariant Features", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Fan et al. (2008)", "label": "Fan et al. (2008)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Liblinear", "label": "Liblinear", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "approximate nearest neighbor search", "label": "approximate nearest neighbor search", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "many applications", "label": "many applications", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "sparse approximation techniques", "label": "sparse approximation techniques", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "product quantization", "label": "product quantization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "atomic decomposition", "label": "atomic decomposition", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "basis pursuit", "label": "basis pursuit", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "hashing algorithms", "label": "hashing algorithms", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "ear-optimal hashing algorithms", "label": "ear-optimal hashing algorithms", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "FOCS", "label": "FOCS", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "locality-sensitive hashing", "label": "locality-sensitive hashing", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Symposium on Computational Geometry", "label": "Symposium on Computational Geometry", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Procrustes analysis", "label": "Procrustes analysis", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "finding optimal transformation", "label": "finding optimal transformation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Procrustes problems", "label": "Procrustes problems", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Oxford University Press", "label": "Oxford University Press", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Huazhu Fu", "label": "Huazhu Fu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Dong Xu", "label": "Dong Xu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Stephen Lin", "label": "Stephen Lin", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Jiang Liu", "label": "Jiang Liu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Object-based RGBD Image Co-segmentation with Mutex Constraint", "label": "Object-based RGBD Image Co-segmentation with Mutex Constraint", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Object-based RGBD Image Co-segmentation with Mutux Constraint", "label": "Object-based RGBD Image Co-segmentation with Mutux Constraint", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "identification of similar foreground objects", "label": "identification of similar foreground objects", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "co-segmentation", "label": "co-segmentation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "fully-connected graph structure", "label": "fully-connected graph structure", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "graph structure", "label": "graph structure", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "mutex constraints", "label": "mutex constraints", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "improper solutions", "label": "improper solutions", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "object-based RGBD co-segmentation", "label": "object-based RGBD co-segmentation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "related methods", "label": "related methods", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "segmentation accuracy", "label": "segmentation accuracy", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "RGBD co-segmentation", "label": "RGBD co-segmentation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "related techniques", "label": "related techniques", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "RGB co-segmentation techniques", "label": "RGB co-segmentation techniques", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "RGB images", "label": "RGB images", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Depth maps", "label": "Depth maps", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Object-Based Methods", "label": "Object-Based Methods", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Mutual Exclusion Constraints", "label": "Mutual Exclusion Constraints", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Co-Saliency Maps", "label": "Co-Saliency Maps", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Graph Formulation", "label": "Graph Formulation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Multi-Plane Block-Coordinate Frank-Wilfe Algorithm", "label": "Multi-Plane Block-Coordinate Frank-Wilfe Algorithm", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Structural SVMs", "label": "Structural SVMs", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "max-Oracle", "label": "max-Oracle", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Neel Shah", "label": "Neel Shah", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "IST Austria", "label": "IST Austria", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Vladimir Kolmogorov", "label": "Vladimir Kolmogorov", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Chris H. Lampert", "label": "Chris H. Lampert", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "IST Australia", "label": "IST Australia", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Structural Support Vector Machines", "label": "Structural Support Vector Machines", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "structured computer vision tasks", "label": "structured computer vision tasks", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Max-Oracle", "label": "Max-Oracle", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Frank-Wolfe Algorithm", "label": "Frank-Wolfe Algorithm", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Block-Coordinate Methods", "label": "Block-Coordinate Methods", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Frank-Wolfe algorithm", "label": "Frank-Wolfe algorithm", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "training SSVMs", "label": "training SSVMs", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "caching mechanism", "label": "caching mechanism", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "criterion", "label": "criterion", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "call max-oracle", "label": "call max-oracle", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "max-oracle", "label": "max-oracle", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "bottleneck", "label": "bottleneck", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "PIVOT-BOEM", "label": "PIVOT-BOEM", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Correlation Clustering", "label": "Correlation Clustering", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "HC", "label": "HC", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "CGC", "label": "CGC", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Fusion Moves", "label": "Fusion Moves", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Experimental Analysis", "label": "Experimental Analysis", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Anytime Algorithms", "label": "Anytime Algorithms", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Anytime Behavior", "label": "Anytime Behavior", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Progressive Improvement", "label": "Progressive Improvement", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Dataset Performance", "label": "Dataset Performance", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Instances", "label": "Instances", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Mohammadreza Mostajabi", "label": "Mohammadreza Mostajabi", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Feedforward Semantic Segmentation", "label": "Feedforward Semantic Segmentation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Toyota Technological Institute at Chicago", "label": "Toyota Technological Institute at Chicago", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Payman Yadollahpour", "label": "Payman Yadollahpour", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Gregory Shakhnarovich", "label": "Gregory Shakhnarovich", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Mostajabi_Feedforward_Semantic_Segmentation_2015_CVPR_paper", "label": "Mostajabi_Feedforward_Semantic_Segmentation_2015_CVPR_paper", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "University of Heidelberg (Department of Mathematics)", "label": "University of Heidelberg (Department of Mathematics)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Mostajabi_Feedforwad_Semantic_Segmentation_2015_CVPR_paper", "label": "Mostajabi_Feedforwad_Semantic_Segmentation_2015_CVPR_paper", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "feed-forward architecture", "label": "feed-forward architecture", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "image elements", "label": "image elements", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "rich feature representations", "label": "rich feature representations", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "nested regions", "label": "nested regions", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "zoom-out", "label": "zoom-out", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "superpixel", "label": "superpixel", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Chen et al.", "label": "Chen et al.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Semantic image segmentation with deep convolutional nets and fully connected crfs", "label": "Semantic image segmentation with deep convolutional nets and fully connected crfs", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Long et al.", "label": "Long et al.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Fully convolutional networks for semantic segmentation", "label": "Fully convolutional networks for semantic segmentation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Associative hierarchical CRFs for object class image segmentation", "label": "Associative hierarchical CRFs for object class image segmentation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Carreira and Sminchisescu", "label": "Carreira and Sminchisescu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "CPMC: Automatic object segmentation using constrained parametric min-cuts", "label": "CPMC: Automatic object segmentation using constrained parametric min-cuts", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "CPMC", "label": "CPMC", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Carreira, J.", "label": "Carreira, J.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Sminchisescu, C.", "label": "Sminchisescu, C.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Zisserma, A.", "label": "Zisserma, A.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Fr\u00b4edo Durand", "label": "Fr\u00b4edo Durand", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Reflection Removal using Ghosting Cues", "label": "Reflection Removal using Ghosting Cues", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Ghosting Cues", "label": "Ghosting Cues", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "MIT CSAIL", "label": "MIT CSAIL", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "YiChang Shih", "label": "YiChang Shih", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Dilip Krishnan", "label": "Dilip Krishnan", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Google Research", "label": "Google Research", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "William T. Freeman", "label": "William T. Freeman", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Ghosting Cunes", "label": "Ghosting Cunes", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "billf@mit.edu", "label": "billf@mit.edu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Shih_Reflection_Removal_Using_2015_CVPR_paper", "label": "Shih_Reflection_Removal_Using_2015_CVPR_paper", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "synthetic inputs", "label": "synthetic inputs", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "real-world inputs", "label": "real-world inputs", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "layer separation", "label": "layer separation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "ill-posed problem", "label": "ill-posed problem", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "ghosting cues", "label": "ghosting cues", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "asymmetry", "label": "asymmetry", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "barely perceptible", "label": "barely perceptible", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "shifted double reflections", "label": "shifted double reflections", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "ghosted reflection", "label": "ghosted reflection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "double-impulse convolution kernel", "label": "double-impulse convolution kernel", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "ghosted reflection components", "label": "ghosted reflection components", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "relative attenuation", "label": "relative attenuation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Reflection Removal", "label": "Reflection Removal", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Soonmin Hwang", "label": "Soonmin Hwang", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Multispectral Pedestrian Detection", "label": "Multispectral Pedestrian Detection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Benchmark Dataset", "label": "Benchmark Dataset", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "pedestrian datasets", "label": "pedestrian datasets", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "color channel", "label": "color channel", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "thermal channel", "label": "thermal channel", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "multispectral pedestrian dataset", "label": "multispectral pedestrian dataset", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "color-thermal image pairs", "label": "color-thermal image pairs", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "previous color-based datasets", "label": "previous color-based datasets", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "dense annotations", "label": "dense annotations", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "image type", "label": "image type", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "multispectral ACF", "label": "multispectral ACF", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "aggregated channel features (ACF)", "label": "aggregated channel features (ACF)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "average miss rate of ACF", "label": "average miss rate of ACF", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "spectral ACF", "label": "spectral ACF", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "aggregated channel features", "label": "aggregated channel features", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "image feature", "label": "image feature", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Multispectral ACF", "label": "Multispectral ACF", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "average miss rate", "label": "average miss rate", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "15%", "label": "15%", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "miss rate", "label": "miss rate", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "breakthrough in pedestrian detection", "label": "breakthrough in pedestrian detection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "RGBD-Fusion", "label": "RGBD-Fusion", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "real-time", "label": "real-time", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "high precision", "label": "high precision", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Jaesik Park", "label": "Jaesik Park", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Namil Kim", "label": "Namil Kim", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Roy Or", "label": "Roy Or", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Technion, Israel Institute of Technology", "label": "Technion, Israel Institute of Technology", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "RGB-D scanners", "label": "RGB-D scanners", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "subtle details", "label": "subtle details", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Depth map enhancement", "label": "Depth map enhancement", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "lighting model", "label": "lighting model", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "natural scene illumination", "label": "natural scene illumination", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "shape from shading-like technique", "label": "shape from shading-like technique", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "detailed geometry", "label": "detailed geometry", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "evidence", "label": "evidence", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "improvement in depth", "label": "improvement in depth", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Shape from shading", "label": "Shape from shading", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Lighting models", "label": "Lighting models", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Computer Vision, Graphics, and Image Processing", "label": "Computer Vision, Graphics, and Image Processing", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Real-time processing", "label": "Real-time processing", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Lambertian reflectance", "label": "Lambertian reflectance", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Bayesian nonparametric intrinsic image decomposition", "label": "Bayesian nonparametric intrinsic image decomposition", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Grosse, R.", "label": "Grosse, R.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Ground-truth dataset", "label": "Ground-truth dataset", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Han, Y.", "label": "Han, Y.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "High quality shape", "label": "High quality shape", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "RGB-D image", "label": "RGB-D image", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Horn, B. K.", "label": "Horn, B. K.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "PhD thesis", "label": "PhD thesis", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "The variational approach", "label": "The variational approach", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Horn \u0026 Brooks", "label": "Horn \u0026 Brooks", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "The variational approach to shape from shading", "label": "The variational approach to shape from shading", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Johnson \u0026 Adelison", "label": "Johnson \u0026 Adelison", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Guy Rosman", "label": "Guy Rosman", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Computer Science and Artificial Intelligence Lab, MIT", "label": "Computer Science and Artificial Intelligence Lab, MIT", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Aaron Wetzler", "label": "Aaron Wetzler", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Ron Kimmel", "label": "Ron Kimmel", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Alfred M. Bruckstein", "label": "Alfred M. Bruckstein", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Xiangyu Zhu", "label": "Xiangyu Zhu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Junnie Yan", "label": "Junnie Yan", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Junjie Yan", "label": "Junjie Yan", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Center for Biomatrics and Security Research", "label": "Center for Biomatrics and Security Research", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "jjyan@nlpr.ia.ac.cn", "label": "jjyan@nlpr.ia.ac.cn", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Dong Yi", "label": "Dong Yi", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "freddy@cs.technion.ac.il", "label": "freddy@cs.technion.ac.il", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Technion - Israel Institute of Technology", "label": "Technion - Israel Institute of Technology", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Zhu_High-Fidelity_Pose_and_2015_CVPR_paper.pdf", "label": "Zhu_High-Fidelity_Pose_and_2015_CVPR_paper.pdf", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "face recognition performance", "label": "face recognition performance", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "pose variations", "label": "pose variations", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "HPEN method", "label": "HPEN method", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "3D Morphable Model", "label": "3D Morphable Model", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "face images", "label": "face images", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "landmark marching", "label": "landmark marching", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "3DMM fitting", "label": "3DMM fitting", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "3D meshing", "label": "3D meshing", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "frontal pose", "label": "frontal pose", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "neutral expression", "label": "neutral expression", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "inpainting", "label": "inpainting", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Multi-PIE", "label": "Multi-PIE", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Amberg, B.", "label": "Amberg, B.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Optimal step non-rigid icp algorithms for surface registration", "label": "Optimal step non-rigid icp algorithms for surface registration", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Romdhani, S.", "label": "Romdhani, S.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Vetter, T.", "label": "Vetter, T.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Chai, X.", "label": "Chai, X.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Shan, S.", "label": "Shan, S.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Gao, W.", "label": "Gao, W.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Locably linear regression for pose-invariant face recognition", "label": "Locably linear regression for pose-invariant face recognition", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Arashloo, S. R.", "label": "Arashloo, S. R.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Pose-invariant face matching using MRF energy minimization framework", "label": "Pose-invariant face matching using MRF energy minimization framework", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Kittler, J.", "label": "Kittler, J.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Chan, C. H.", "label": "Chan, C. H.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Tahir, M. A.", "label": "Tahir, M. A.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "robust component-based face recognition", "label": "robust component-based face recognition", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "component-based face recognition", "label": "component-based face recognition", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "kernel fusion", "label": "kernel fusion", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "multiple descriptors", "label": "multiple descriptors", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Chen, D. (2012)", "label": "Chen, D. (2012)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "high-dimensional feature", "label": "high-dimensional feature", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "efficient compression", "label": "efficient compression", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Asthana, A. (2013)", "label": "Asthana, A. (2013)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "discriminative response map fitting", "label": "discriminative response map fitting", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "constrained local models", "label": "constrained local models", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "2013 IEEE Conference on Computer Vision and Pattern Recognition", "label": "2013 IEEE Conference on Computer Vision and Pattern Recognition", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "high-dimensional feature compression", "label": "high-dimensional feature compression", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Cheng, S.", "label": "Cheng, S.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Robust discriminative response map fitting", "label": "Robust discriminative response map fitting", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Barkan, O.", "label": "Barkan, O.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Fast high dimensional vector multiplication face recognition", "label": "Fast high dimensional vector multiplication face recognition", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Weill, J.", "label": "Weill, J.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Wolf, L.", "label": "Wolf, L.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Aronowitz, H.", "label": "Aronowitz, H.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Parsing Occluded People", "label": "Parsing Occluded People", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Xianjie Chen", "label": "Xianjie Chen", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Alan Yuille", "label": "Alan Yuille", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "tree structure", "label": "tree structure", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "connected subtree", "label": "connected subtree", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "flexible composition", "label": "flexible composition", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "inference", "label": "inference", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "search over models", "label": "search over models", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "part sharing", "label": "part sharing", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "twice as many", "label": "twice as many", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "searching", "label": "searching", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "entire object", "label": "entire object", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "standard benchmarked dataset", "label": "standard benchmarked dataset", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "best", "label": "best", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "modeling", "label": "modeling", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "occlusion", "label": "occlusion", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Graphical models", "label": "Graphical models", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Yuilie, A.", "label": "Yuilie, A.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Ferrari, V.", "label": "Ferrari, V.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Marin-Jimenez, M.", "label": "Marin-Jimenez, M.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "human pose estimation", "label": "human pose estimation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "progressive search space reduction", "label": "progressive search space reduction", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "support-vector networks", "label": "support-vector networks", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Cortes", "label": "Cortes", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Dalal", "label": "Dalal", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Triggs", "label": "Triggs", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Sapp", "label": "Sapp", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Adaptive pose priors", "label": "Adaptive pose priors", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Jordan", "label": "Jordan", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Taskar", "label": "Taskar", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Yuille", "label": "Yuille", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Alabort-i-Medina", "label": "Alabort-i-Medina", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "deformable models", "label": "deformable models", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "degrees of freedom", "label": "degrees of freedom", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Face Alignment", "label": "Face Alignment", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Holistic Deformable Models", "label": "Holistic Deformable Models", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Parts-Based Deformable Models", "label": "Parts-Based Deformable Models", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Deformable Models", "label": "Deformable Models", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Active Appearance Models", "label": "Active Appearance Models", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "T. F. Cootes, G. J. Edwards, and C. J. Taylor (2001)", "label": "T. F. Cootes, G. J. Edwards, and C. J. Taylor (2001)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Active Shape Models", "label": "Active Shape Models", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "T. F. Cootes, C. J. Taylor, D. H. Cooper, and J. Graham (1995)", "label": "T. F. Cootes, C. J. Taylor, D. H. Cooper, and J. Graham (1995)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Lucas-Kanade Method", "label": "Lucas-Kanade Method", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Bayesian Active Appearance Models", "label": "Bayesian Active Appearance Models", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "J. Alabort-i-Medina and S. Zafeiriou (2014)", "label": "J. Alabort-i-Medina and S. Zafeiriou (2014)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Conference on Computer Vision and Pattern Recognition (CVPR)", "label": "Conference on Computer Vision and Pattern Recognition (CVPR)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "2014", "label": "2014", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Lucas-Kanade", "label": "Lucas-Kanade", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "International Journal of Computer Vision (IJCR)", "label": "International Journal of Computer Vision (IJCR)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "unifying framework", "label": "unifying framework", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "X. Cao", "label": "X. Cao", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Face alignment by explicit shape regression", "label": "Face alignment by explicit shape regression", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "G. Papandreou", "label": "G. Papandreou", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Adaptive and constrained algorithms", "label": "Adaptive and constrained algorithms", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "inverse compositional active appearance model fitting", "label": "inverse compositional active appearance model fitting", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "A. Asthana", "label": "A. Asthana", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "S. Zafeiriou", "label": "S. Zafeiriou", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Conference on Computer Vision and Pattern Reduction (CVPR)", "label": "Conference on Computer Vision and Pattern Reduction (CVPR)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "J. Sragih", "label": "J. Sragih", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Joan Alabort-i-Medina", "label": "Joan Alabort-i-Medina", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Imperial College London", "label": "Imperial College London", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "ja310@imperial.ac.uk", "label": "ja310@imperial.ac.uk", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Stefanos Zafeiriou", "label": "Stefanos Zafeiriou", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Department of Computing", "label": "Department of Computing", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "s.zafeiriou@imperial.ac.uk", "label": "s.zafeiriou@imperial.ac.uk", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Jia Xu", "label": "Jia Xu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Gaze-Enabled Egocentric Video Summarization", "label": "Gaze-Enabled Egocentric Video Summarization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "University of Wisconsin-Madison", "label": "University of Wisconsin-Madison", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Lopamudra Mukherjee", "label": "Lopamudra Mukherjee", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "University of Wisconsin-Whitewater", "label": "University of Wisconsin-Whitewater", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Yin Li", "label": "Yin Li", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Jamieson Warner", "label": "Jamieson Warner", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "James M. Rehg", "label": "James M. Rehg", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Vikas Singh", "label": "Vikas Singh", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "egocentric videos", "label": "egocentric videos", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "compact representation", "label": "compact representation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "unique challenges", "label": "unique challenges", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "gaze tracking information", "label": "gaze tracking information", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "summarization", "label": "summarization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "frame comparison", "label": "frame comparison", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "personalized summaries", "label": "personalized summaries", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "summarization model", "label": "summarization model", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "submodular function maximization", "label": "submodular function maximization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Egocentric Video Summarization", "label": "Egocentric Video Summarization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Submodular Function Maximization", "label": "Submodular Function Maximization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Multilinear Relaxation", "label": "Multilinear Relaxation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Personalized Summarization", "label": "Personalized Summarization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Almeida et al.", "label": "Almeida et al.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "VISON", "label": "VISON", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Online Applications", "label": "Online Applications", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Submodular Maximization", "label": "Submodular Maximization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Partition Matroid", "label": "Partition Matroid", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Filmus \u0026 Ward", "label": "Filmus \u0026 Ward", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Combinatorial Algorithm", "label": "Combinatorial Algorithm", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Fujishige", "label": "Fujishige", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Submodular Functions and Optimization", "label": "Submodular Functions and Optimization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Submodular functions and optimization", "label": "Submodular functions and optimization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Gaze Tracking", "label": "Gaze Tracking", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Wearable Cameras", "label": "Wearable Cameras", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Ward", "label": "Ward", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Iyer", "label": "Iyer", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Krause", "label": "Krause", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "information gathering", "label": "information gathering", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Xu", "label": "Xu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Andreas Geiger", "label": "Andreas Geiger", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Object Scene Flow for Autonomous Vehicles", "label": "Object Scene Flow for Autonomous Vehicles", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "MPI Tubingen", "label": "MPI Tubingen", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Menze_Object_Scene_Flow_2015_CVPR_supplemental.pdf", "label": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Menze_Object_Scene_Flow_2015_CVPR_supplemental.pdf", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Moritz Menze", "label": "Moritz Menze", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Menze_Object_Scene_Flow_2015_CVPR_supplemental", "label": "Menze_Object_Scene_Flow_2015_CVPR_supplemental", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "supplementary document", "label": "supplementary document", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "additional descriptions", "label": "additional descriptions", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "visualizations", "label": "visualizations", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "scene flow ground truth", "label": "scene flow ground truth", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "model parameters", "label": "model parameters", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "model sensitivity", "label": "model sensitivity", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "KITTI stereo", "label": "KITTI stereo", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "stereo", "label": "stereo", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "novel", "label": "novel", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "sphere sequence", "label": "sphere sequence", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Brox, T. \u0026 Malik, J.", "label": "Brox, T. \u0026 Malik, J.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Variational Motion Estimation", "label": "Variational Motion Estimation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Hirschmueller, H.", "label": "Hirschmueller, H.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Stereo Processing", "label": "Stereo Processing", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Scene Flow Datasets", "label": "Scene Flow Datasets", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Quantitative Results", "label": "Quantitative Results", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Qualitative Results", "label": "Qualitative Results", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Stereo processing", "label": "Stereo processing", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Scene flow estimation", "label": "Scene flow estimation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "growing correspondence seeds", "label": "growing correspondence seeds", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "piecewise rigid scene flow", "label": "piecewise rigid scene flow", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "variational method", "label": "variational method", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Cech et al. (2011)", "label": "Cech et al. (2011)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Vogel et al. (2013)", "label": "Vogel et al. (2013)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Huguet \u0026 Devernay (2007)", "label": "Huguet \u0026 Devernay (2007)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "scene flow estimation", "label": "scene flow estimation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "ICVV", "label": "ICVV", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Semiglobal matching", "label": "Semiglobal matching", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Mutual information", "label": "Mutual information", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "stereo sequences", "label": "stereo sequences", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Sun, Roth, \u0026 Black (2013)", "label": "Sun, Roth, \u0026 Black (2013)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "optical flow estimation", "label": "optical flow estimation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Hornacek, Fitzgibbon, \u0026 Rother (2014)", "label": "Hornacek, Fitzgibbon, \u0026 Rother (2014)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "SphereFlow", "label": "SphereFlow", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Valgaerts et al. (2010)", "label": "Valgaerts et al. (2010)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "motion and geometry estimation", "label": "motion and geometry estimation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Kert et al. (2010)", "label": "Kert et al. (2010)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Wedel et al. (2008)", "label": "Wedel et al. (2008)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "scene flow computation", "label": "scene flow computation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Geiger et al. (2011)", "label": "Geiger et al. (2011)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "3D reconstruction techniques", "label": "3D reconstruction techniques", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Menz", "label": "Menz", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Leibniz Universit\u00a8at Hannover", "label": "Leibniz Universit\u00a8at Hannover", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Tal Hassner", "label": "Tal Hassner", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Effective Face Frontalization", "label": "Effective Face Frontalization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Shai Harel", "label": "Shai Harel", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Eran Paz", "label": "Eran Paz", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Roee Enbar", "label": "Roee Enbar", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Hassner_Effective_Face_Frontalization_2015_CVPR_paper.pdf", "label": "Hassner_Effective_Face_Frontalization_2015_CVPR_paper.pdf", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "face recognition systems", "label": "face recognition systems", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "unconstrained images", "label": "unconstrained images", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "varying poses", "label": "varying poses", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "varying expressions", "label": "varying expressions", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "varying lighting", "label": "varying lighting", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Fronalization", "label": "Fronalization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "problem of varying poses", "label": "problem of varying poses", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "problem of varying lighting", "label": "problem of varying lighting", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "previous methods", "label": "previous methods", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "estimating 3D facial shapes", "label": "estimating 3D facial shapes", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "this paper", "label": "this paper", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "simpler approach", "label": "simpler approach", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "visual common sense", "label": "visual common sense", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "single 3D surface", "label": "single 3D surface", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "frontal views", "label": "frontal views", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "aesthetically pleasing", "label": "aesthetically pleasing", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "gender estimation", "label": "gender estimation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Face frontalization", "label": "Face frontalization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "aesthetically pleasing frontal views", "label": "aesthetically pleasing frontal views", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "AI systems", "label": "AI systems", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "factual question answering", "label": "factual question answering", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "common sense reasoning", "label": "common sense reasoning", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "semantic knowledge", "label": "semantic knowledge", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Xiao Lin", "label": "Xiao Lin", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "linxiao@vt.edu", "label": "linxiao@vt.edu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Visual Common Sense", "label": "Visual Common Sense", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "AI Reasoning", "label": "AI Reasoning", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Visual Paraphasing", "label": "Visual Paraphasing", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Hsu_Robust_Image_Alignment_2015_CVPR_paper.pdf", "label": "Hsu_Robust_Image_Alignment_2015_CVPR_paper.pdf", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "process", "label": "process", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "data analysis", "label": "data analysis", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Data Analysis", "label": "Data Analysis", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Parameter Estimation", "label": "Parameter Estimation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Optimization", "label": "Optimization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "System Dynamics", "label": "System Dynamics", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Relationship Modeling", "label": "Relationship Modeling", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Rock_Compleeting_3D_Object_2015_CVPR_paper.pdf", "label": "Rock_Compleeting_3D_Object_2015_CVPR_paper.pdf", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Jason Rock", "label": "Jason Rock", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Justin Thorsten", "label": "Justin Thorsten", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "JunYoung Gwak", "label": "JunYoung Gwak", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Daeyun Shin", "label": "Daeyun Shin", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "3D Shape Reconstruction", "label": "3D Shape Reconstruction", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "View-Based Matching", "label": "View-Based Matching", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Shape Completion", "label": "Shape Completion", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "3D Model Synthesis", "label": "3D Model Synthesis", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Symmetry Transfer", "label": "Symmetry Transfer", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Tanmay Gupta", "label": "Tanmay Gupta", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Liu_Data-Driven_Sparsity-Based_Restoration_2015_CVPR_paper.pdf", "label": "Liu_Data-Driven_Sparsity-Based_Restoration_2015_CVPR_paper.pdf", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Rui Caseiro", "label": "Rui Caseiro", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Beyond the Shortest Path", "label": "Beyond the Shortest Path", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Institute of Systems and Robotics - University of Coimbra", "label": "Institute of Systems and Robotics - University of Coimbra", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "domain adaptation", "label": "domain adaptation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "spline flow", "label": "spline flow", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Pedro Martins", "label": "Pedro Martins", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Jorge Batista", "label": "Jorge Batista", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Institute of Sistemas and Robotics - University of Coimbra", "label": "Institute of Sistemas and Robotics - University of Coimbra", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "improve_performance", "label": "improve_performance", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "domain adaptation paradigm", "label": "domain adaptation paradigm", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "shortest path", "label": "shortest path", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "geodesic curve", "label": "geodesic curve", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "modeling complex domain shifts", "label": "modeling complex domain shifts", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "use of multiple datasets", "label": "use of multiple datasets", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "spline curves", "label": "spline curves", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "integration of multiple source domains", "label": "integration of multiple source domains", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "domain shifts", "label": "domain shifts", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "improved performance", "label": "improved performance", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "rolling maps", "label": "rolling maps", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Domain Adaptation", "label": "Domain Adaptation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Subspace Representation", "label": "Subspace Representation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Baktas et al. (2013)", "label": "Baktas et al. (2013)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Domain Invariant Projection", "label": "Domain Invariant Projection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Gopalan et al. (2013)", "label": "Gopalan et al. (2013)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "location recognition", "label": "location recognition", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Gopalan et al. (2011)", "label": "Gopalan et al. (2011)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Unsupervised approach", "label": "Unsupervised approach", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Carreira et al. (2012)", "label": "Carreira et al. (2012)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Caseiro et al. (2010)", "label": "Caseiro et al. (2010)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "cast shadows", "label": "cast shadows", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Gopalan et al. (2014)", "label": "Gopalan et al. (2014)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "intermediate data representations", "label": "intermediate data representations", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Spline Flow", "label": "Spline Flow", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Grasmannn Manifold", "label": "Grasmannn Manifold", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Rolling Maps", "label": "Rolling Maps", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Gopalan", "label": "Gopalan", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Unsupervised adaptation across domain shifts by generating intermediate data representations", "label": "Unsupervised adaptation across domain shifts by generating intermediate data representations", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Li", "label": "Li", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Griffin", "label": "Griffin", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Holub", "label": "Holub", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Cal tech-256 object category dataset", "label": "Cal tech-256 object category dataset", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Caseiro", "label": "Caseiro", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "non-parametric riemannian framework", "label": "non-parametric riemannian framework", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Henriques", "label": "Henriques", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Martins", "label": "Martins", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Batista", "label": "Batista", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Hays", "label": "Hays", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Im2gps", "label": "Im2gps", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Efroos", "label": "Efroos", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Pan", "label": "Pan", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Jo\u00e3o F. Henriques", "label": "Jo\u00e3o F. Henriques", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Sparse Composite Quantization", "label": "Sparse Composite Quantization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "quantization techniques", "label": "quantization techniques", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Zhang_Sparse_Composite_Quantization_2015_CVPR_paper", "label": "Zhang_Sparse_Composite_Quantization_2015_CVPR_paper", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "sparse composite quantization", "label": "sparse composite quantization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "competitive search accuracy", "label": "competitive search accuracy", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Cartesian k-means", "label": "Cartesian k-means", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "composite quantization", "label": "composite quantization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "sparse dictionaries", "label": "sparse dictionaries", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "distance table computation time", "label": "distance table computation time", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "variation", "label": "variation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "CVPR (2013)", "label": "CVPR (2013)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "distance table computation", "label": "distance table computation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "time", "label": "time", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "distance evaluation", "label": "distance evaluation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "1M", "label": "1M", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "1B", "label": "1B", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "search times", "label": "search times", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "faster", "label": "faster", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "SIFTS", "label": "SIFTS", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "ANN", "label": "ANN", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "nearest neighbor search", "label": "nearest neighbor search", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Product Quantization", "label": "Product Quantization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "for nearest neighbor search", "label": "for nearest neighbor search", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Babenko and Lempitsky (2012)", "label": "Babenko and Lempitsky (2012)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "inverted multi-index", "label": "inverted multi-index", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Babenko and Lempitsky (2014)", "label": "Babenko and Lempitsky (2014)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "bilayer product quantization", "label": "bilayer product quantization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "billion-scale approximate nearest neighbors", "label": "billion-scale approximate nearest neighbors", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "large-scale applications", "label": "large-scale applications", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "High-Dimensional Data", "label": "High-Dimensional Data", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "approximate nearest neighbors", "label": "approximate nearest neighbors", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "vocabulary trees", "label": "vocabulary trees", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Hamming embedding", "label": "Hamming embedding", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "large scale image search", "label": "large scale image search", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "semi-supervised hashing", "label": "semi-supervised hashing", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "large-scale search", "label": "large-scale search", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Re-ranking strategies", "label": "Re-ranking strategies", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "search", "label": "search", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Guo-Jun Qi", "label": "Guo-Jun Qi", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Jinhui Tang", "label": "Jinhui Tang", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Unknown", "label": "Unknown", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Nanjing University of Science and Technology", "label": "Nanjing University of Science and Technology", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Ting Zhang", "label": "Ting Zhang", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Jingding Wang", "label": "Jingding Wang", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Ioannis Gkioulekalas", "label": "Ioannis Gkioulekalas", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Gkiooulekas_On_the_Appearance_2015_CVPR_paper.pdf", "label": "Gkiooulekas_On_the_Appearance_2015_CVPR_paper.pdf", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Edges in images", "label": "Edges in images", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Edges in opaque objects", "label": "Edges in opaque objects", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Edges", "label": "Edges", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Discontinuity in surface orientation", "label": "Discontinuity in surface orientation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Authors", "label": "Authors", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Edge patterns", "label": "Edge patterns", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Light Transport", "label": "Light Transport", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Simulations", "label": "Simulations", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Scattering parameters", "label": "Scattering parameters", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Shape estimation", "label": "Shape estimation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Material estimation", "label": "Material estimation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Jensen et al. (2001)", "label": "Jensen et al. (2001)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Wave Propagation", "label": "Wave Propagation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Ishimaru (1978)", "label": "Ishimaru (1978)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Stereo Reconstruction", "label": "Stereo Reconstruction", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Human Perception", "label": "Human Perception", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Adelson (2001)", "label": "Adelson (2001)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Machine Vision", "label": "Machine Vision", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Rendering", "label": "Rendering", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Reconstruction", "label": "Reconstruction", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Translucent Objects", "label": "Translucent Objects", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Material Estimation", "label": "Material Estimation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Material Metameters", "label": "Material Metameters", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "light transport", "label": "light transport", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "translucient materials", "label": "translucient materials", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "rendering", "label": "rendering", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "accurate light transport", "label": "accurate light transport", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "translucent materials", "label": "translucent materials", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "translucent appearance", "label": "translucent appearance", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "translucency", "label": "translucency", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "phase functions", "label": "phase functions", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "reconstruction", "label": "reconstruction", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "accurate rendering", "label": "accurate rendering", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "phase function", "label": "phase function", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "photon diffusion", "label": "photon diffusion", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Open-surfaces catalog", "label": "Open-surfaces catalog", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "surface appearances", "label": "surface appearances", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "evaluation", "label": "evaluation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "hybrid multi-camera and marker-based capture dataset", "label": "hybrid multi-camera and marker-based capture dataset", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "ACM Transactions on Graphics", "label": "ACM Transactions on Graphics", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "research on phase function", "label": "research on phase function", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Bala. Open-surfaces", "label": "Bala. Open-surfaces", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Journal of Vision", "label": "Journal of Vision", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "research on translucency", "label": "research on translucency", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "ACM SIGGRAPH", "label": "ACM SIGGRAPH", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "research on rendering techniques", "label": "research on rendering techniques", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "dataset of surface appearances", "label": "dataset of surface appearances", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Gkiouslekas et al.", "label": "Gkiouslekas et al.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "phase function in translucent appearance", "label": "phase function in translucent appearance", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "materials in context database", "label": "materials in context database", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "translucent objects", "label": "translucent objects", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Mingkui Tan", "label": "Mingkui Tan", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Tan_Learning_Graph_Structure_2015_CVPR_paper.pdf", "label": "Tan_Learning_Graph_Structure_2015_CVPR_paper.pdf", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Qinfeng Shi", "label": "Qinfeng Shi", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Multi-label image classification", "label": "Multi-label image classification", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Classification performance", "label": "Classification performance", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Probablistic Graphical Models", "label": "Probablistic Graphical Models", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Probabilistic Graphical Models", "label": "Probabilistic Graphical Models", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Label dependency", "label": "Label dependency", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Graphical model structure", "label": "Graphical model structure", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Heuristic methods", "label": "Heuristic methods", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Limited information", "label": "Limited information", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Problem", "label": "Problem", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Max-margin framework", "label": "Max-margin framework", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Convex programming problem", "label": "Convex programming problem", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "learning", "label": "learning", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Procedure", "label": "Procedure", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Set of cliques", "label": "Set of cliques", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Graph structure learning", "label": "Graph structure learning", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Clique generation", "label": "Clique generation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Boutell et al. (2004)", "label": "Boutell et al. (2004)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Bradley \u0026 Guestrin (2010)", "label": "Bradley \u0026 Guestrin (2010)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "tree conditional random fields", "label": "tree conditional random fields", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Bucak et al. (2009)", "label": "Bucak et al. (2009)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "multi-label ranking", "label": "multi-label ranking", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "R.", "label": "R.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Efficient multi-label ranking", "label": "Efficient multi-label ranking", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Jain, A. K.", "label": "Jain, A. K.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Cai, X.", "label": "Cai, X.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Chow, C.", "label": "Chow, C.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Approximating discrete probability distributions", "label": "Approximating discrete probability distributions", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Liu, C.", "label": "Liu, C.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Dembczy\u0144ski, K.", "label": "Dembczy\u0144ski, K.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Label dependence and loss minimization", "label": "Label dependence and loss minimization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Waegeman, W.", "label": "Waegeman, W.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Analysis of chaining", "label": "Analysis of chaining", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Cheng, W.", "label": "Cheng, W.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "H\u00fcllermeier, E.", "label": "H\u00fcllermeier, E.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Dembczynski, K.", "label": "Dembczynski, K.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "European Conference on Artificial Intelligence", "label": "European Conference on Artificial Intelligence", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Dembczynski", "label": "Dembczynski", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "analysis of chaining", "label": "analysis of chaining", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Everingham", "label": "Everingham", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "PASUAL Visual Object Classes Challenge 2012", "label": "PASUAL Visual Object Classes Challenge 2012", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "ConceptLearner", "label": "ConceptLearner", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "MIT", "label": "MIT", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "scalable approach", "label": "scalable approach", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "promising performance", "label": "promising performance", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "fully supervised methods", "label": "fully supervised methods", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "weakly supervised methods", "label": "weakly supervised methods", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Vignesh Jagadeesh", "label": "Vignesh Jagadeesh", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "eBay Research Labs", "label": "eBay Research Labs", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Zhou_ConceptLearner_Discovering_Visual_2015_CVPR_paper.pdf", "label": "Zhou_ConceptLearner_Discovering_Visual_2015_CVPR_paper.pdf", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Charles Sturt University", "label": "Charles Sturt University", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Junbin Gao", "label": "Junbin Gao", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "computer vision recognition systems", "label": "computer vision recognition systems", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "visual knowledge", "label": "visual knowledge", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "fully labeled data", "label": "fully labeled data", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "expensive", "label": "expensive", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "visual concept detectors", "label": "visual concept detectors", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "automatically", "label": "automatically", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "image region-level detection", "label": "image region-level detection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "learned concepts", "label": "learned concepts", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "scene recognition", "label": "scene recognition", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "domain-specific supervision", "label": "domain-specific supervision", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "automatic attribute discovery", "label": "automatic attribute discovery", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "noisy web data", "label": "noisy web data", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Im2text", "label": "Im2text", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "captioned photographs", "label": "captioned photographs", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "object detectors", "label": "object detectors", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Deng et al.", "label": "Deng et al.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Places database", "label": "Places database", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Zhou et al.", "label": "Zhou et al.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Piction", "label": "Piction", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "human faces", "label": "human faces", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Srihari", "label": "Srihari", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Divvala et al.", "label": "Divvala et al.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "webly-supervised visual concept learning", "label": "webly-supervised visual concept learning", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Tang et al.", "label": "Tang et al.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "AAAI", "label": "AAAI", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "AAAI Press", "label": "AAAI Press", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "The MIT Press", "label": "The MIT Press", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Divvala, S. K.", "label": "Divvala, S. K.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Robinson Piramuthu", "label": "Robinson Piramuthu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Yumin Suh", "label": "Yumin Suh", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Subgraph Matching using Compactness Prior", "label": "Subgraph Matching using Compactness Prior", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Seoul National University", "label": "Seoul National University", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Kamil Adamczewski", "label": "Kamil Adamczewski", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Suh_Subgraph_Matching_Using_2015_CVPR_paper.pdf", "label": "Suh_Subgraph_Matching_Using_2015_CVPR_paper.pdf", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Feature correspondence", "label": "Feature correspondence", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "computer vision applications", "label": "computer vision applications", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "2009 IEEE 12th International Conference on Computer Vision", "label": "2009 IEEE 12th International Conference on Computer Vision", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Graph matching", "label": "Graph matching", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Graph matching algorithms", "label": "Graph matching algorithms", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "precision", "label": "precision", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Solutions", "label": "Solutions", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Subgraph matching formulation", "label": "Subgraph matching formulation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "compactness prior", "label": "compactness prior", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Meta-algorithm", "label": "Meta-algorithm", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Markov chain Monte Carlo", "label": "Markov chain Monte Carlo", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Formulation and algorithm", "label": "Formulation and algorithm", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "baseline performance", "label": "baseline performance", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Cho, M.", "label": "Cho, M.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Reweighted random walks", "label": "Reweighted random walks", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "max-pooling strategy", "label": "max-pooling strategy", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Computer Vision\u2013ECCV 2010", "label": "Computer Vision\u2013ECCV 2010", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Progressive graph matching", "label": "Progressive graph matching", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on", "label": "Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Alahari, K.", "label": "Alahari, K.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Ponce, J.", "label": "Ponce, J.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Lee, Kyoung Mu", "label": "Lee, Kyoung Mu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "progressive graph matching", "label": "progressive graph matching", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Adamczewski, Kamil", "label": "Adamczewski, Kamil", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Suh, Yumin", "label": "Suh, Yumin", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Duchenne, O.", "label": "Duchenne, O.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "tensor-based algorithm", "label": "tensor-based algorithm", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Gilks, W. R.", "label": "Gilks, W. R.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Markov chain monte carlo", "label": "Markov chain monte carlo", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Cour, T.", "label": "Cour, T.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "balanced graph matching", "label": "balanced graph matching", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Guancong Zhang", "label": "Guancong Zhang", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Good Features to Track for Visual SLAM", "label": "Good Features to Track for Visual SLAM", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Zhang_Good_Features_to_2015_CVPR_paper.pdf", "label": "Zhang_Good_Features_to_2015_CVPR_paper.pdf", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Patricio A. Vela", "label": "Patricio A. Vela", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "pvela@gatech.edu", "label": "pvela@gatech.edu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "measured features", "label": "measured features", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "accurate localization", "label": "accurate localization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "observability of SLAM", "label": "observability of SLAM", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "existing SLAM systems", "label": "existing SLAM systems", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "localization accuracy", "label": "localization accuracy", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "estimation utility", "label": "estimation utility", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "observability indices", "label": "observability indices", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "incremental singular value decomposition (SVD)", "label": "incremental singular value decomposition (SVD)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "greedy selection", "label": "greedy selection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "incremental singular value decomposition", "label": "incremental singular value decomposition", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "submodular", "label": "submodular", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "near-optimal", "label": "near-optimal", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "SLAM", "label": "SLAM", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "SfM", "label": "SfM", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "temporal observability indices", "label": "temporal observability indices", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "synthetic experiments", "label": "synthetic experiments", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "improved localization accuracy", "label": "improved localization accuracy", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "SLAM experiments", "label": "SLAM experiments", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "improved data association", "label": "improved data association", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Visual SLAM", "label": "Visual SLAM", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Data Association", "label": "Data Association", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Incremental SVD", "label": "Incremental SVD", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Observability Analysis", "label": "Observability Analysis", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "map_building", "label": "map_building", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "MonoSLAM", "label": "MonoSLAM", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "real-time SLAM", "label": "real-time SLAM", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "LSD-SLAM", "label": "LSD-SLAM", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "direct monocular SLAM", "label": "direct monocular SLAM", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Slam_Algorithm", "label": "Slam_Algorithm", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Covariance recovery", "label": "Covariance recovery", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "data association", "label": "data association", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Feature Selection", "label": "Feature Selection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Andrade-Cetto and Sanfeliu", "label": "Andrade-Cetto and Sanfeliu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "partial observability", "label": "partial observability", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Kaess and Dellaert", "label": "Kaess and Dellaert", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "covariance recovery", "label": "covariance recovery", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Davison et al.", "label": "Davison et al.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Engel et al.", "label": "Engel et al.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Machine Intelligence", "label": "Machine Intelligence", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "iSAM2", "label": "iSAM2", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Bayes Tree", "label": "Bayes Tree", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Incremental Smoothing", "label": "Incremental Smoothing", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Slam", "label": "Slam", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Active search", "label": "Active search", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Live dense reconstruction", "label": "Live dense reconstruction", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Single moving camera", "label": "Single moving camera", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Zheng Ma", "label": "Zheng Ma", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "School of ECR", "label": "School of ECR", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "zhanggc@gatech.edu", "label": "zhanggc@gatech.edu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "City University of Hong Kong", "label": "City University of Hong Kong", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Lei Yu", "label": "Lei Yu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Antoni B. Chan", "label": "Antoni B. Chan", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Georgia Tech", "label": "Georgia Tech", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "School of ECE", "label": "School of ECE", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Ma_Small_Instance_Detection_2015_CVPR_paper", "label": "Ma_Small_Instance_Detection_2015_CVPR_paper", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Small Instance Detection", "label": "Small Instance Detection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Integer Programming", "label": "Integer Programming", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Object Density Maps", "label": "Object Density Maps", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "pedestrians", "label": "pedestrians", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "cells", "label": "cells", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "partially-occluding small instances", "label": "partially-occluding small instances", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "2D integer programming", "label": "2D integer programming", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "recover object instance locations", "label": "recover object instance locations", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "ROI counts", "label": "ROI counts", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "density map", "label": "density map", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "object instances", "label": "object instances", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Bayesian regression", "label": "Bayesian regression", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Crowd counting", "label": "Crowd counting", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "multiple local features", "label": "multiple local features", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "IEEE Conf. Computer Vision and Pattern Recognition", "label": "IEEE Conf. Computer Vision and Pattern Recognition", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "IEEE Trans. on Image Processing", "label": "IEEE Trans. on Image Processing", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Digital Image Computing: Techniques and Applications", "label": "Digital Image Computing: Techniques and Applications", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Crowd counting using multiple local features", "label": "Crowd counting using multiple local features", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Human detection", "label": "Human detection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Sridharan (2009)", "label": "Sridharan (2009)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Lowe (2004)", "label": "Lowe (2004)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Chan (2008)", "label": "Chan (2008)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "privacy concerns", "label": "privacy concerns", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "crowd monitoring", "label": "crowd monitoring", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Zhao (2003)", "label": "Zhao (2003)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Bayesian segmentation", "label": "Bayesian segmentation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "crowded scenes", "label": "crowded scenes", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "R. ia", "label": "R. ia", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Lemtipsky, V.", "label": "Lemtipsky, V.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "PASCAL VOC challenge", "label": "PASCAL VOC challenge", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Mohammad Rastegari", "label": "Mohammad Rastegari", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Computationally Bound Retrieval", "label": "Computationally Bound Retrieval", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Rastegari_Computationally_Bounded_Retrieval_2015_CVPR_paper.pdf", "label": "Rastegari_Computationally_Bounded_Retrieval_2015_CVPR_paper.pdf", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Rastegari_Computationality_Bounded_Retrieval_2015_CVPR_paper", "label": "Rastegari_Computationality_Bounded_Retrieval_2015_CVPR_paper", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "large image databases", "label": "large image databases", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "efficient retrieval challenging", "label": "efficient retrieval challenging", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "high dimensional data", "label": "high dimensional data", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "retrieval challenge", "label": "retrieval challenge", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "hashing methods", "label": "hashing methods", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "accuracy for speed", "label": "accuracy for speed", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "bit correlation", "label": "bit correlation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "iterative scheme", "label": "iterative scheme", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Near-optimal Hasing Algorithms", "label": "Near-optimal Hasing Algorithms", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Approximate Nearest Neighbor", "label": "Approximate Nearest Neighbor", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "High Dimensions", "label": "High Dimensions", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Datar et al. (2004)", "label": "Datar et al. (2004)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Locality-Sensitive Hashing Scheme", "label": "Locality-Sensitive Hashing Scheme", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "P-stable Distributions", "label": "P-stable Distributions", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Methods", "label": "Methods", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Speed-up", "label": "Speed-up", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Gong et al. (2013)", "label": "Gong et al. (2013)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Computer Vision and Pattern Recognition (CVPR), 2013", "label": "Computer Vision and Pattern Recognition (CVPR), 2013", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Gong \u0026 Lazebnik (2011)", "label": "Gong \u0026 Lazebnik (2011)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "2011 IEEE Conference on Computer Vision and Pattern Recognition", "label": "2011 IEEE Conference on Computer Vision and Pattern Recognition", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "iterative quantization", "label": "iterative quantization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "J\u00e9goeu et al. (2009)", "label": "J\u00e9goeu et al. (2009)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Searching with quantization", "label": "Searching with quantization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "short codes", "label": "short codes", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "J\u00e9gou et al (2009)", "label": "J\u00e9gou et al (2009)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Krizhevskya et al (2012)", "label": "Krizhevskya et al (2012)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Majia et al (2013)", "label": "Majia et al (2013)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "efficient classification", "label": "efficient classification", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Norouzia et al (2011)", "label": "Norouzia et al (2011)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Wu_New_Insights_Into_2015_CVPR_paper.pdf", "label": "Wu_New_Insights_Into_2015_CVPR_paper.pdf", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Graph-based computer vision applications", "label": "Graph-based computer vision applications", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "similarity metrics", "label": "similarity metrics", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "pairwise similarity", "label": "pairwise similarity", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "vertices", "label": "vertices", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "(L + \u03b1\u039b)\u22121", "label": "(L + \u03b1\u039b)\u22121", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "graph Laplacian", "label": "graph Laplacian", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "positive diagonal matrix", "label": "positive diagonal matrix", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "graph topology", "label": "graph topology", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "regularizer", "label": "regularizer", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "cluster density", "label": "cluster density", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "\u039b", "label": "\u039b", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "choices", "label": "choices", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "complementary behaviors", "label": "complementary behaviors", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Paper (1999)", "label": "Paper (1999)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Pagerank Citation Ranking", "label": "Pagerank Citation Ranking", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "bring order to web", "label": "bring order to web", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Chung (1997)", "label": "Chung (1997)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Spectral Graph Theory", "label": "Spectral Graph Theory", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Graph Topology", "label": "Graph Topology", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Andersen et al. (2006)", "label": "Andersen et al. (2006)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Pagerank Vectors", "label": "Pagerank Vectors", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Local Graph Partitioning", "label": "Local Graph Partitioning", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Belkin \u0026 Niyogi (2001)", "label": "Belkin \u0026 Niyogi (2001)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Shi \u0026 Malik (2000)", "label": "Shi \u0026 Malik (2000)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Laplacianfaces", "label": "Laplacianfaces", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Random walks", "label": "Random walks", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Wu, X.-M.", "label": "Wu, X.-M.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Electrical Engineering", "label": "Electrical Engineering", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "graph-based learning", "label": "graph-based learning", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Chang, S.-F.", "label": "Chang, S.-F.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Wenguan Wang", "label": "Wenguan Wang", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Saliency-Aware Geodesic Video Object Segmentation", "label": "Saliency-Aware Geodesic Video Object Segmentation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Jianbing Shen", "label": "Jianbing Shen", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Fatih Porikli", "label": "Fatih Porikli", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "NICTA Australia", "label": "NICTA Australia", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Saliency-Aware Geosesic Video Object Segmentation", "label": "Saliency-Aware Geosesic Video Object Segmentation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "geodesic distance", "label": "geodesic distance", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "spatiotemporal salience maps", "label": "spatiotemporal salience maps", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "spatially and temporally coherent object segmentation", "label": "spatially and temporally coherent object segmentation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Geos image segmentation", "label": "Geos image segmentation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "ECCV, 2008", "label": "ECCV, 2008", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "geodesic image segmentation approach", "label": "geodesic image segmentation approach", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Motion Boundaries", "label": "Motion Boundaries", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Geos", "label": "Geos", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Geodesic graph cut", "label": "Geodesic graph cut", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "interactive image segmentation", "label": "interactive image segmentation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Grabcut", "label": "Grabcut", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "interactive foreground extraction", "label": "interactive foreground extraction", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "iterated graph cuts", "label": "iterated graph cuts", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Object segmentation", "label": "Object segmentation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "trajectory analysis", "label": "trajectory analysis", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Video object segmentation", "label": "Video object segmentation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "W. Brendel and S. Todorovic", "label": "W. Brendel and S. Todorovic", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "geodesic methods", "label": "geodesic methods", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "J. Carreira", "label": "J. Carreira", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "W. Brendel", "label": "W. Brendel", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Video object segmentation by tracking regions", "label": "Video object segmentation by tracking regions", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "D. Tsai", "label": "D. Tsai", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Motion coherent tracking using multi-label mrf optimization", "label": "Motion coherent tracking using multi-label mrf optimization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Tali Dekel", "label": "Tali Dekel", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Best-Buddies Similarity", "label": "Best-Buddies Similarity", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "unconstrained environments", "label": "unconstrained environments", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Best-Buddies Similarity (BBS)", "label": "Best-Buddies Similarity (BBS)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "counting Best-Buddie Pairs (BBPs)", "label": "counting Best-Buddie Pairs (BBPs)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "geometric deformations", "label": "geometric deformations", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Best-Buddies Similarity (BSS)", "label": "Best-Buddies Similarity (BSS)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "parameter-free", "label": "parameter-free", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Best-Buddie Pairs (BBPs)", "label": "Best-Buddie Pairs (BBPs)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "pairs of points", "label": "pairs of points", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "BBS", "label": "BBS", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "non-rigid object tracking", "label": "non-rigid object tracking", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "consistent success", "label": "consistent success", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Similarity Measures", "label": "Similarity Measures", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Outlier Robustness", "label": "Outlier Robustness", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Geometric Deformations", "label": "Geometric Deformations", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Comaniciu, D. et al.", "label": "Comaniciu, D. et al.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "mean shift tracking", "label": "mean shift tracking", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Rubner, Y. et al.", "label": "Rubner, Y. et al.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Earth Mover\u0027s Distance", "label": "Earth Mover\u0027s Distance", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "metric for image retrieval", "label": "metric for image retrieval", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "image comparison", "label": "image comparison", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Rubner et al. (2000)", "label": "Rubner et al. (2000)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Pele et al. (2008)", "label": "Pele et al. (2008)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "robust pattern matching", "label": "robust pattern matching", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Simaov et al. (2008)", "label": "Simaov et al. (2008)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "summarizing visual data", "label": "summarizing visual data", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "similarity measures", "label": "similarity measures", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Hel-Or et al. (2014)", "label": "Hel-Or et al. (2014)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "photometric invariant template matching", "label": "photometric invariant template matching", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "matching technique", "label": "matching technique", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Tian et al. (2012)", "label": "Tian et al. (2012)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "estimating nonrigid image distortions", "label": "estimating nonrigid image distortions", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "nonrigid image distortions", "label": "nonrigid image distortions", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "image estimation", "label": "image estimation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Tian \u0026 Narasimhan (2012)", "label": "Tian \u0026 Narasimhan (2012)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Korman et al. (2013)", "label": "Korman et al. (2013)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "fast affine template matching algorithm", "label": "fast affine template matching algorithm", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Wu et al. (2013)", "label": "Wu et al. (2013)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "online object tracking benchmark", "label": "online object tracking benchmark", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Olson (2002)", "label": "Olson (2002)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "maximum-likelihood image matching", "label": "maximum-likelihood image matching", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Michael Rubinstein", "label": "Michael Rubinstein", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Shai Avidan", "label": "Shai Avidan", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Tel Aviv University", "label": "Tel Aviv University", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "avidan@eng.tau.ac.il", "label": "avidan@eng.tau.ac.il", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Nianyi Li", "label": "Nianyi Li", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "A Weighted Sparse Coding Framework for Saliency Detection", "label": "A Weighted Sparse Coding Framework for Saliency Detection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "A Weighted Sparse Coding Framework", "label": "A Weighted Sparse Coding Framework", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Bilin Sun", "label": "Bilin Sun", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Jingyi", "label": "Jingyi", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Saliency Detection", "label": "Saliency Detection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Jingyi Yu", "label": "Jingyi Yu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Li_A_Weighted_Sparse_2015_CVPR_paper", "label": "Li_A_Weighted_Sparse_2015_CVPR_paper", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "pdf", "label": "pdf", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "salience detection", "label": "salience detection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "high-dimensional datasets", "label": "high-dimensional datasets", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "dictionaries", "label": "dictionaries", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "data-speci\ufb01c features", "label": "data-speci\ufb01c features", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "dictionary", "label": "dictionary", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Liu et al. (2011)", "label": "Liu et al. (2011)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Achanta et al. (2012)", "label": "Achanta et al. (2012)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Borji \u0026 Itti (2012)", "label": "Borji \u0026 Itti (2012)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Patch Rarities", "label": "Patch Rarities", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Borji, Sihite, \u0026 Itti (2012)", "label": "Borji, Sihite, \u0026 Itti (2012)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Salient Object Detection Benchmark", "label": "Salient Object Detection Benchmark", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Reynolds \u0026 Desimone", "label": "Reynolds \u0026 Desimone", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "V4", "label": "V4", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "attention", "label": "attention", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Neuron", "label": "Neuron", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Nothdurft", "label": "Nothdurft", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "additivity across dimensions", "label": "additivity across dimensions", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "salience from feature contrast", "label": "salience from feature contrast", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Perazzi et al.", "label": "Perazzi et al.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "salience filters", "label": "salience filters", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "contrast based filtering", "label": "contrast based filtering", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Cheng et al.", "label": "Cheng et al.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Borji et al.", "label": "Borji et al.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "cal and global patch rarities", "label": "cal and global patch rarities", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Movahedi \u0026 Elder", "label": "Movahedi \u0026 Elder", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Reynolds \u0026 Desimnone", "label": "Reynolds \u0026 Desimnone", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Itti \u0026 Koch", "label": "Itti \u0026 Koch", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Nature Reviews Neuroscience", "label": "Nature Reviews Neuroscience", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Reynolds", "label": "Reynolds", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Souvenir", "label": "Souvenir", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Sun", "label": "Sun", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "sunbilin@eecis.udel.edu", "label": "sunbilin@eecis.udel.edu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Yu", "label": "Yu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "yu@eecis.udel.edu", "label": "yu@eecis.udel.edu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "non-parametric", "label": "non-parametric", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "ordered labels", "label": "ordered labels", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "80%", "label": "80%", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "associated images", "label": "associated images", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Ordered Labels", "label": "Ordered Labels", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Ordinal Data", "label": "Ordinal Data", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Label Denoising", "label": "Label Denoising", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "accuracy of image labels", "label": "accuracy of image labels", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Building Rome in a day", "label": "Building Rome in a day", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "IEEE International Conference on Computer Visions", "label": "IEEE International Conference on Computer Visions", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "R. C. Bolles", "label": "R. C. Bolles", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "C.-C. Chang", "label": "C.-C. Chang", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "C.-J. Lin", "label": "C.-J. Lin", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "USAC", "label": "USAC", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "R. Raguram", "label": "R. Raguram", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "l-curve", "label": "l-curve", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Analysis of discrete ill-posed problems", "label": "Analysis of discrete ill-posed problems", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "P. C. Hansen", "label": "P. C. Hansen", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Internet photo collections", "label": "Internet photo collections", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Modeling the world", "label": "Modeling the world", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "N. Snavely", "label": "N. Snavely", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "SIAM review", "label": "SIAM review", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Kai Han", "label": "Kai Han", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "A Fixed Viewpoint Approach", "label": "A Fixed Viewpoint Approach", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "The University of Hokkaido", "label": "The University of Hokkaido", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Kwan-Yee K. Wong", "label": "Kwan-Yee K. Wong", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Miaomiao Liu", "label": "Miaomiao Liu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "A Fixed View Point Approach", "label": "A Fixed View Point Approach", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "CECS, ANU", "label": "CECS, ANU", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "refractive photo-light-path", "label": "refractive photo-light-path", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "M. Ben-Ezra and S. K. Nayr", "label": "M. Ben-Ezra and S. K. Nayr", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "transparency analysis", "label": "transparency analysis", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "G. Eren et al.", "label": "G. Eren et al.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "shape estimation", "label": "shape estimation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "local surface heating", "label": "local surface heating", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Fischler and Bolles", "label": "Fischler and Bolles", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Hata et al.", "label": "Hata et al.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "genetic algorithm", "label": "genetic algorithm", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "shape extraction", "label": "shape extraction", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Ihrke et al. (2005)", "label": "Ihrke et al. (2005)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "geometry reconstruction", "label": "geometry reconstruction", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "dynamic environments", "label": "dynamic environments", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Ihrke et al. (2008)", "label": "Ihrke et al. (2008)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Transparent objects", "label": "Transparent objects", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Benjamin Allain", "label": "Benjamin Allain", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "An Efficient Volumetric Framework for Shape Tracking", "label": "An Efficient Volumetric Framework for Shape Tracking", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Inria Grenoble Rh\u02c6one-Alpes - LJK", "label": "Inria Grenoble Rh\u02c6one-Alpes - LJK", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Inria Grenoble Rh\u02c6one- Alpes", "label": "Inria Grenoble Rh\u02c6one- Alpes", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "LJK", "label": "LJK", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Allain_An_Efficient_Volumetric_2015_CVPR_paper.pdf", "label": "Allain_An_Efficient_Volumetric_2015_CVPR_paper.pdf", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Jean-S\u00e9batian Franco", "label": "Jean-S\u00e9batian Franco", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Edmond Boyer", "label": "Edmond Boyer", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "observations define several feasible surfaces", "label": "observations define several feasible surfaces", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "this work", "label": "this work", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "volumetric shape parametrization", "label": "volumetric shape parametrization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Centroidal Voronoi Tesselations (CVT)", "label": "Centroidal Voronoi Tesselations (CVT)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "improved precision and robustness", "label": "improved precision and robustness", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "volumetric shape tracking", "label": "volumetric shape tracking", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Volumetric Shape Tracking", "label": "Volumetric Shape Tracking", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Centroidal Voronoi Tesselations", "label": "Centroidal Voronoi Tesselations", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Dynamic Shape Capture", "label": "Dynamic Shape Capture", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Motion Estimation", "label": "Motion Estimation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Surface-based Methods", "label": "Surface-based Methods", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Volume-based Methods", "label": "Volume-based Methods", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Alexa et al. (2000)", "label": "Alexa et al. (2000)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "As-rigid-as-possible shape interpolation", "label": "As-rigid-as-possible shape interpolation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Allain et al. (2014)", "label": "Allain et al. (2014)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "On mean pose and variability of 3d deformable models", "label": "On mean pose and variability of 3d deformable models", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Ballan \u0026 Cortelazzo (2008)", "label": "Ballan \u0026 Cortelazzo (2008)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Marker-less motion capture of skinned models", "label": "Marker-less motion capture of skinned models", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Bishop (2006)", "label": "Bishop (2006)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Pattern Recognition and Machine Learning", "label": "Pattern Recognition and Machine Learning", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Botsu et al. (2007)", "label": "Botsu et al. (2007)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Adaptive space deformations based on rigid cells", "label": "Adaptive space deformations based on rigid cells", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Comput. Graph. Forum", "label": "Comput. Graph. Forum", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Botsu, M.", "label": "Botsu, M.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Cagniart, C.", "label": "Cagniart, C.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Free-form mesh tracking: a patch-based approach", "label": "Free-form mesh tracking: a patch-based approach", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Probabilistic deformable surface tracking from multiple videos", "label": "Probabilistic deformable surface tracking from multiple videos", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "de Aguiar, E.", "label": "de Aguiar, E.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Performance capture from sparse multi-view video", "label": "Performance capture from sparse multi-view video", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "de Aguliar, E.", "label": "de Aguliar, E.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Marker-less deformable mesh tracking for human shape and motion capture", "label": "Marker-less deformable mesh tracking for human shape and motion capture", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Benjamin Allaine", "label": "Benjamin Allaine", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "firstname.lastname@inria.fr", "label": "firstname.lastname@inria.fr", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Maximum likelihood", "label": "Maximum likelihood", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "em algorithm", "label": "em algorithm", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Maximum likelihood from incomplete data", "label": "Maximum likelihood from incomplete data", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Journal of the Royal Statistical Society, series B", "label": "Journal of the Royal Statistical Society, series B", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Maximum likelihood from incomplete data via the em algorithm", "label": "Maximum likelihood from incomplete data via the em algorithm", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Jean-S\u00e9bastien Franco", "label": "Jean-S\u00e9bastien Franco", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Inria Grenoble Rh\u02c6one-Alpes - LHK", "label": "Inria Grenoble Rh\u02c6one-Alpes - LHK", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Yi-Hsuan Tsai", "label": "Yi-Hsuan Tsai", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Tsai_Adaptive_Region_Pooling_2015_CVPR_supplemental.pdf", "label": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Tsai_Adaptive_Region_Pooling_2015_CVPR_supplemental.pdf", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Onur C. Hamsici", "label": "Onur C. Hamsici", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Qualcomm Research", "label": "Qualcomm Research", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Tsai_Adaptive_Region_Pooling_2015_CVPR_supplemental", "label": "Tsai_Adaptive_Region_Pooling_2015_CVPR_supplemental", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "discriminative object parts", "label": "discriminative object parts", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "challenging conditions", "label": "challenging conditions", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Adaptive Region Processing", "label": "Adaptive Region Processing", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "detected objects", "label": "detected objects", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "ARP", "label": "ARP", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "ESVM", "label": "ESVM", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Ensemble of exemplar-svms", "label": "Ensemble of exemplar-svms", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Keypoint Transfer", "label": "Keypoint Transfer", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Object Parts Discovery", "label": "Object Parts Discovery", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Long-term Recurrent Convolutional Networks", "label": "Long-term Recurrent Convolutional Networks", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Visual Description", "label": "Visual Description", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Jeff Donahue", "label": "Jeff Donahue", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Lisa Anne Hendricks", "label": "Lisa Anne Hendricks", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Sergio Guadarrama", "label": "Sergio Guadarrama", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Donahue_Long-Term_Recurrent_Convolutional_2015_CVPR_paper", "label": "Donahue_Long-Term_Recurrent_Convolutional_2015_CVPR_paper", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Visual Features", "label": "Visual Features", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Predictions", "label": "Predictions", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Visual Input", "label": "Visual Input", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Marcus Rohrbach", "label": "Marcus Rohrbach", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Subhashini Venugopalan", "label": "Subhashini Venugopalan", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "UT Austin", "label": "UT Austin", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Kate Saenko", "label": "Kate Saenko", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "UMass Lowell", "label": "UMass Lowell", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Trevor Darrell", "label": "Trevor Darrell", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Models", "label": "Models", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "recurrent", "label": "recurrent", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "temporally deep", "label": "temporally deep", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "video recognition tasks", "label": "video recognition tasks", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "image description", "label": "image description", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "video narration challenges", "label": "video narration challenges", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "recurrent convolutional models", "label": "recurrent convolutional models", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "doubly deep", "label": "doubly deep", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "complex target concepts", "label": "complex target concepts", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "limited training data", "label": "limited training data", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "network state updates", "label": "network state updates", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "long-term dependencies", "label": "long-term dependencies", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "long-term RNN models", "label": "long-term RNN models", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "variable length outputs", "label": "variable length outputs", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "complex temporal dynamics", "label": "complex temporal dynamics", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "recurrent long-term models", "label": "recurrent long-term models", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "visual convnet models", "label": "visual convnet models", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Recurrent Convolutional Networks (LRCNs)", "label": "Recurrent Convolutional Networks (LRCNs)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Video Recognition", "label": "Video Recognition", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Long-Term Dependencies", "label": "Long-Term Dependencies", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "state-of-the-art models", "label": "state-of-the-art models", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "separately optimized", "label": "separately optimized", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Action Classification", "label": "Action Classification", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Long short-term memory recurrent neural networks", "label": "Long short-term memory recurrent neural networks", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "soccer videos", "label": "soccer videos", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Multimodal neural language models", "label": "Multimodal neural language models", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "visual-semantic embeddings", "label": "visual-semantic embeddings", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Unifying visual-semantic embeddings", "label": "Unifying visual-semantic embeddings", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Video in sentences out", "label": "Video in sentences out", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "UAI", "label": "UAI", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "High accuracy optical flow estimation", "label": "High accuracy optical flow estimation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "3D convolutional neural networks", "label": "3D convolutional neural networks", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Generating sequences", "label": "Generating sequences", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "arXiv preprint arXiv:1308.0850", "label": "arXiv preprint arXiv:1308.0850", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Recurrent neural networks", "label": "Recurrent neural networks", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "generating sequences", "label": "generating sequences", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "J. Deng", "label": "J. Deng", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "W. Dong", "label": "W. Dong", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "R. Socher", "label": "R. Socher", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "L.-J. Li", "label": "L.-J. Li", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "K. Li", "label": "K. Li", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "L. Fei-Fei", "label": "L. Fei-Fei", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "A. Frome", "label": "A. Frome", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "ubhashini Venugopalan", "label": "ubhashini Venugopalan", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "SOM", "label": "SOM", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Semantic Obviousness Metric", "label": "Semantic Obviousness Metric", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Image Quality Assessment", "label": "Image Quality Assessment", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Peng Zhang", "label": "Peng Zhang", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "pzhangoo@mail.ustc.edu.cn", "label": "pzhangoo@mail.ustc.edu.cn", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Wengang Zhou", "label": "Wengang Zhou", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "zhwg@ustc.edu.cn", "label": "zhwg@ustc.edu.cn", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Lei Wu", "label": "Lei Wu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "wuleibig@gmail.com", "label": "wuleibig@gmail.com", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Houqiang Li", "label": "Houqiang Li", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "lihq@ustc.edu.cn", "label": "lihq@ustc.edu.cn", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Zhang_SOM_Semantic_Obviousness_2015_CVPR_paper.pdf", "label": "Zhang_SOM_Semantic_Obviousness_2015_CVPR_paper.pdf", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Image quality assessment (IQA)", "label": "Image quality assessment (IQA)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "objectively estimate human perception", "label": "objectively estimate human perception", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "no-referece (NR) image quality assessment (IQA) framework", "label": "no-referece (NR) image quality assessment (IQA) framework", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "semantic-level factors", "label": "semantic-level factors", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "human perception of image quality", "label": "human perception of image quality", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "state-of-the-art full-referece IQA (FR-IQA) methods", "label": "state-of-the-art full-referece IQA (FR-IQA) methods", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "NR-IQA algorithms", "label": "NR-IQA algorithms", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "IQA algorithms", "label": "IQA algorithms", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "existing algorithms", "label": "existing algorithms", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "full-referece IQA (FR-IQA) methods", "label": "full-referece IQA (FR-IQA) methods", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "IQA methods", "label": "IQA methods", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Image Quality Assessment (IQA)", "label": "Image Quality Assessment (IQA)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "assessment method", "label": "assessment method", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "No-Reference Image Quality Assessment (NR-IQA)", "label": "No-Reference Image Quality Assessment (NR-IQA)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "IQA method", "label": "IQA method", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "NR-IQA", "label": "NR-IQA", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Damien Teney", "label": "Damien Teney", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Learning Similarity Metrics", "label": "Learning Similarity Metrics", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Dynamic Scene Segmentation", "label": "Dynamic Scene Segmentation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Matthew Brown", "label": "Matthew Brown", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "complex patterns", "label": "complex patterns", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Spatio-temporal filters", "label": "Spatio-temporal filters", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Metric learning", "label": "Metric learning", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Graph-based segmentation", "label": "Graph-based segmentation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Alpert et al. (2007)", "label": "Alpert et al. (2007)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Brox \u0026 Malik (2010)", "label": "Brox \u0026 Malik (2010)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Chan \u0026 Vasconcelos (2008)", "label": "Chan \u0026 Vasconcelos (2008)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Chan \u0026 Vasconcelos (2009)", "label": "Chan \u0026 Vasconcelos (2009)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Corso", "label": "Corso", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "CVPR tutorial on video segmentation", "label": "CVPR tutorial on video segmentation", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "spatio-temporal orientation analysis", "label": "spatio-temporal orientation analysis", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Dynamic texture detection", "label": "Dynamic texture detection", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "S.", "label": "S.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Wu, Y. N.", "label": "Wu, Y. N.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Fazekas, S.", "label": "Fazekas, S.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Amiatz, T.", "label": "Amiatz, T.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Chetverikov, D.", "label": "Chetverikov, D.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Feichtenhofer, C.", "label": "Feichtenhofer, C.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Bags of spacetime energies", "label": "Bags of spacetime energies", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Pinz, A.", "label": "Pinz, A.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Wilides, R.", "label": "Wilides, R.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Teney, Damien", "label": "Teney, Damien", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Brown, Matthew", "label": "Brown, Matthew", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "University of Bath", "label": "University of Bath", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Kit, Dimitry", "label": "Kit, Dimitry", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Hall, Peter", "label": "Hall, Peter", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Li, Yang", "label": "Li, Yang", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Reliable Patch Tracers", "label": "Reliable Patch Tracers", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Zhu, Jianke", "label": "Zhu, Jianke", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Hoi, Steven C.H.", "label": "Hoi, Steven C.H.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "modern trackers", "label": "modern trackers", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "tracking results", "label": "tracking results", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Reliable Patch Tracers (RPT)", "label": "Reliable Patch Tracers (RPT)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "tracking method", "label": "tracking method", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "identify reliable patches", "label": "identify reliable patches", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "reliable patches", "label": "reliable patches", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "tracked effectively", "label": "tracked effectively", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "tracking reliability metric", "label": "tracking reliability metric", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "reliability of patch", "label": "reliability of patch", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "probability model", "label": "probability model", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "distribution of reliable patches", "label": "distribution of reliable patches", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "sequential Monte Carlo framework", "label": "sequential Monte Carlo framework", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "motion trajectories", "label": "motion trajectories", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "reliable patches from background", "label": "reliable patches from background", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "visual object", "label": "visual object", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "cluster", "label": "cluster", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "source code", "label": "source code", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "public", "label": "public", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Adam, A., Rivlin, E., \u0026 Shimshoni, I. (2006)", "label": "Adam, A., Rivlin, E., \u0026 Shimshoni, I. (2006)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Robust fragments-based tracking", "label": "Robust fragments-based tracking", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "integral histogram", "label": "integral histogram", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Lucas, B. D., \u0026 Kanade, T. (1981)", "label": "Lucas, B. D., \u0026 Kanade, T. (1981)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "iterative image registration technique", "label": "iterative image registration technique", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Poling, B., Lerman, G., \u0026 Szlarm, A. (2014)", "label": "Poling, B., Lerman, G., \u0026 Szlarm, A. (2014)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Better feature tracking", "label": "Better feature tracking", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Cai, Z., Wen, L., Yang, J., Lei, Z., \u0026 Li, S. (2012)", "label": "Cai, Z., Wen, L., Yang, J., Lei, Z., \u0026 Li, S. (2012)", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Structured visual tracking", "label": "Structured visual tracking", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Sequential Monte Carlo Methods in Practice", "label": "Sequential Monte Carlo Methods in Practice", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Sequential Monte Carlo Framework", "label": "Sequential Monte Carlo Framework", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Szlarm", "label": "Szlarm", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Cai et al.", "label": "Cai et al.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "ACCV", "label": "ACCV", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Mannning et al.", "label": "Mannning et al.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Introduction to Information Retrieval", "label": "Introduction to Information Retrieval", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Danelljan et al.", "label": "Danelljan et al.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Everingham et al.", "label": "Everingham et al.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "The pascal visual object classes(voc) challenge", "label": "The pascal visual object classes(voc) challenge", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Grundmann et al.", "label": "Grundmann et al.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Yang Li", "label": "Yang Li", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "College of Computer Science", "label": "College of Computer Science", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "liyang89@zju.edu.cn", "label": "liyang89@zju.edu.cn", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Jianke Zhu", "label": "Jianke Zhu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "V.", "label": "V.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Han, M.", "label": "Han, M.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Essa, I.", "label": "Essa, I.", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "SALICON", "label": "SALICON", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "understand and predict visual attention", "label": "understand and predict visual attention", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "collecting large-scale human data", "label": "collecting large-scale human data", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "new possibilities for visual understanding", "label": "new possibilities for visual understanding", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "mouse-contingent paradigm", "label": "mouse-contingent paradigm", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "eye tracker", "label": "eye tracker", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "SALICON dataset", "label": "SALICON dataset", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "human \u0027free-viewing\u0027 data", "label": "human \u0027free-viewing\u0027 data", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "10,000 images", "label": "10,000 images", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Microsoft COCO dataset", "label": "Microsoft COCO dataset", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "ground truth for evaluating salience algorithms", "label": "ground truth for evaluating salience algorithms", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Ming Jiang", "label": "Ming Jiang", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Shengshen Huang", "label": "Shengshen Huang", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Juanyong Duan", "label": "Juanyong Duan", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Qi Zhao", "label": "Qi Zhao", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "fine-grained recognition", "label": "fine-grained recognition", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Fine-grained Recognition", "label": "Fine-grained Recognition", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Xiaoyong Shen", "label": "Xiaoyong Shen", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Localization", "label": "Localization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Classification", "label": "Classification", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Di Lin", "label": "Di Lin", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Cewu Lu", "label": "Cewu Lu", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "Lin_Deep_LAC_Deep_2015_CVPR_paper", "label": "Lin_Deep_LAC_Deep_2015_CVPR_paper", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "eleqiz@nus.edu.sg", "label": "eleqiz@nus.edu.sg", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "fine-grained recognition system", "label": "fine-grained recognition system", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "part localization", "label": "part localization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "valve linkage function", "label": "valve linkage function", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "back-propagation chaining", "label": "back-propagation chaining", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "classification errors", "label": "classification errors", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "alignment errors", "label": "alignment errors", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "update localization", "label": "update localization", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "deep LAC system", "label": "deep LAC system", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "LAC system", "label": "LAC system", "shape": "dot", "size": 10, "type": "entity"}, {"color": "#97c2fc", "id": "fine-grained object data", "label": "fine-grained object data", "shape": "dot", "size": 10, "type": "entity"}]);
                  edges = new vis.DataSet([{"arrows": "to", "chunk_id": "doc_0_chunk_0", "file_type": "json", "from": "Saurabh Singh", "predicate": "is_author_of", "to": "Learning a Sequential Search for Landmarks", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_4", "file_type": "json", "from": "Saurabh Singh", "predicate": "affiliated_with", "to": "University of Illinois, Urbana-Champaign", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_5", "file_type": "json", "from": "Saurabh Singh", "predicate": "affiliated_with", "to": "University of Indiana", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_0", "file_type": "json", "from": "Learning a Sequential Search for Landmarks", "predicate": "is_conference_paper", "to": "CVPR", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_0", "file_type": "json", "from": "Learning a Sequential Search for Landmarks", "predicate": "is_published_in", "to": "2015", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_0", "file_type": "json", "from": "Derek Hoiem", "predicate": "is_author_of", "to": "Learning a Sequential Search for Landmarks", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_4", "file_type": "json", "from": "Derek Hoiem", "predicate": "affiliated_with", "to": "University of Illinois, Urbana-Champaign", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_5", "file_type": "json", "from": "Derek Hoiem", "predicate": "affiliated_with", "to": "University of Indiana", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_615", "file_type": "json", "from": "Derek Hoiem", "predicate": "is_author_of", "to": "Rock_Completing_3D_Object_2015_CVPR_paper.pdf", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_617", "file_type": "json", "from": "Derek Hoiem", "predicate": "affiliation", "to": "University of Illinois at Urbana-Champaign", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_0", "file_type": "json", "from": "David Forsyth", "predicate": "is_author_of", "to": "Learning a Sequential Search for Landmarks", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_4", "file_type": "json", "from": "David Forsyth", "predicate": "affiliated_with", "to": "University of Illinois, Urbana-Champaign", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_5", "file_type": "json", "from": "David Forsyth", "predicate": "affiliated_with", "to": "University of Indiana", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_0", "file_type": "json", "from": "CVPR", "predicate": "is_conference", "to": "Learning a Sequential Search for Landmarks", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_54", "file_type": "json", "from": "CVPR", "predicate": "published", "to": "Benenson, R., Mathias, M., Tuytelaars, T., \u0026 Van Gool, L. (2013)", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_77", "file_type": "json", "from": "CVPR", "predicate": "published", "to": "Bayesian color constancy revisited", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_97", "file_type": "json", "from": "CVPR", "predicate": "published", "to": "Efficient belief propagation", "type": "factual", "width": 2.0}, {"arrows": "to", "chunk_id": "doc_0_chunk_378", "file_type": "json", "from": "CVPR", "predicate": "is_conference_of", "to": "Computer Vision", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_323", "file_type": "json", "from": "CVPR", "predicate": "is_a", "to": "conference", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_177", "file_type": "json", "from": "CVPR", "predicate": "is_publication_venue_for", "to": "reference [3]", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_227", "file_type": "json", "from": "CVPR", "predicate": "presents", "to": "multi-column deep neural networks", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_643", "file_type": "json", "from": "CVPR", "predicate": "is_conference_for", "to": "computer vision", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_312", "file_type": "json", "from": "CVPR", "predicate": "is_conference_for", "to": "Image Co-segmentation", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_312", "file_type": "json", "from": "CVPR", "predicate": "is_conference_for", "to": "Object Detection", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_353", "file_type": "json", "from": "CVPR", "predicate": "is_conference_for", "to": "Visual semantic search", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_367", "file_type": "json", "from": "CVPR", "predicate": "is_conference_for", "to": "person re-identi\ufb01cation", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_473", "file_type": "json", "from": "CVPR", "predicate": "hosts", "to": "Entropy rate superpixel segmentation", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_485", "file_type": "json", "from": "CVPR", "predicate": "publishes", "to": "tric min-cuts paper", "type": "factual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_503", "file_type": "json", "from": "CVPR", "predicate": "hosts", "to": "Beyond lambert", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_513", "file_type": "json", "from": "CVPR", "predicate": "is_publication_venue", "to": "research paper", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_530", "file_type": "json", "from": "CVPR", "predicate": "is_conference_for", "to": "Furukawa et al.", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_676", "file_type": "json", "from": "CVPR", "predicate": "hosts", "to": "constrained parametric min-cuts", "type": "factual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_719", "file_type": "json", "from": "CVPR", "predicate": "published", "to": "Variational layered dynamic textures", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_0", "file_type": "json", "from": "Glaucoma", "predicate": "is_related_to", "to": "Learning a Sequential Search for Landmarks", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_373", "file_type": "json", "from": "Glaucoma", "predicate": "is_analyzed_by", "to": "Deep Multiple Instance Learning", "type": "factual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_0", "file_type": "json", "from": "Sequential Search", "predicate": "is_technique_in", "to": "Learning a Sequential Search for Landmarks", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_0", "file_type": "json", "from": "Landmarks", "predicate": "is_used_in", "to": "Learning a Sequential Search for Landmarks", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_0", "file_type": "json", "from": "Ollama", "predicate": "is_related_to", "to": "Learning a Sequential Search for Landmarks", "type": "conceptual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_1", "file_type": "json", "from": "method", "predicate": "aims_to_find", "to": "landmarks", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_1", "file_type": "json", "from": "method", "predicate": "uses", "to": "appearance", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_1", "file_type": "json", "from": "method", "predicate": "applied_to", "to": "parsing human body layouts", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_1", "file_type": "json", "from": "method", "predicate": "applied_to", "to": "finding landmarks in images of birds", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_1", "file_type": "json", "from": "method", "predicate": "learns", "to": "sequential search", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_2", "file_type": "json", "from": "method", "predicate": "represents", "to": "spatial model", "type": "conceptual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_2", "file_type": "json", "from": "method", "predicate": "displays", "to": "strong performance", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_2", "file_type": "json", "from": "method", "predicate": "addresses", "to": "model problems", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_416", "file_type": "json", "from": "method", "predicate": "achieves", "to": "state-of-the-art results", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_7", "file_type": "json", "from": "method", "predicate": "applicable_to", "to": "contour detection", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_7", "file_type": "json", "from": "method", "predicate": "lacks", "to": "feature engineering", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_30", "file_type": "json", "from": "method", "predicate": "aims_to_achieve", "to": "subpixel-level accuracy", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_30", "file_type": "json", "from": "method", "predicate": "exhibits", "to": "robustness", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_30", "file_type": "json", "from": "method", "predicate": "sparsified_in", "to": "images", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_30", "file_type": "json", "from": "method", "predicate": "separates", "to": "sparse error tensors", "type": "factual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_30", "file_type": "json", "from": "method", "predicate": "addresses", "to": "limitations", "type": "factual", "width": 0.82}, {"arrows": "to", "chunk_id": "doc_0_chunk_30", "file_type": "json", "from": "method", "predicate": "outperforms", "to": "traditional methods", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_663", "file_type": "json", "from": "method", "predicate": "compared_to", "to": "state-of-the-art methods", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_35", "file_type": "json", "from": "method", "predicate": "localizes", "to": "ground-level query images", "type": "factual", "width": 0.87}, {"arrows": "to", "chunk_id": "doc_0_chunk_35", "file_type": "json", "from": "method", "predicate": "matches", "to": "aerial imagery", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_35", "file_type": "json", "from": "method", "predicate": "learns", "to": "feature representation", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_60", "file_type": "json", "from": "method", "predicate": "addresses", "to": "border ownership assignment", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_60", "file_type": "json", "from": "method", "predicate": "utilizes", "to": "Structured Random Forests (SRF)", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_60", "file_type": "json", "from": "method", "predicate": "introduces", "to": "border ownership structure", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_60", "file_type": "json", "from": "method", "predicate": "uses", "to": "shape descriptors", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_60", "file_type": "json", "from": "method", "predicate": "uses", "to": "spectral properties", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_60", "file_type": "json", "from": "method", "predicate": "uses", "to": "semi-global grouping cues", "type": "factual", "width": 0.87}, {"arrows": "to", "chunk_id": "doc_0_chunk_60", "file_type": "json", "from": "method", "predicate": "evaluated_on", "to": "Berkeley Segmentation Dataset (BSDS)", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_60", "file_type": "json", "from": "method", "predicate": "evaluated_on", "to": "NYU Depth V2 dataset", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_60", "file_type": "json", "from": "method", "predicate": "outperforms", "to": "multi-stage approaches", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_65", "file_type": "json", "from": "method", "predicate": "achieves", "to": "scale invariance", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_75", "file_type": "json", "from": "method", "predicate": "is_based_on", "to": "four simple color features", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_75", "file_type": "json", "from": "method", "predicate": "uses", "to": "regression trees", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_81", "file_type": "json", "from": "method", "predicate": "shows", "to": "effectiveness", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_81", "file_type": "json", "from": "method", "predicate": "handles", "to": "high percentage of outliers", "type": "factual", "width": 0.87}, {"arrows": "to", "chunk_id": "doc_0_chunk_90", "file_type": "json", "from": "method", "predicate": "achieves", "to": "16x speedup", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_90", "file_type": "json", "from": "method", "predicate": "compared_to", "to": "Deformable Part Model", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_90", "file_type": "json", "from": "method", "predicate": "demonstrates_applicability_for", "to": "parallel computing", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_93", "file_type": "json", "from": "method", "predicate": "segments", "to": "frame", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_93", "file_type": "json", "from": "method", "predicate": "adjusts", "to": "enhancement of regions", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_95", "file_type": "json", "from": "method", "predicate": "achieves", "to": "high fidelity", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_94", "file_type": "json", "from": "method", "predicate": "achieves", "to": "temporal consistency", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_105", "file_type": "json", "from": "method", "predicate": "applied to", "to": "texture classification", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_105", "file_type": "json", "from": "method", "predicate": "evaluated on", "to": "KTH-TIPS2 dataset", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_105", "file_type": "json", "from": "method", "predicate": "evaluated on", "to": "FMD dataset", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_105", "file_type": "json", "from": "method", "predicate": "evaluated on", "to": "DTD dataset", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_113", "file_type": "json", "from": "method", "predicate": "explores", "to": "convolutional temporal feature pooling architectures", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_113", "file_type": "json", "from": "method", "predicate": "models", "to": "video", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_119", "file_type": "json", "from": "method", "predicate": "tested on", "to": "datasets", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_152", "file_type": "json", "from": "method", "predicate": "handles", "to": "unseen target crowd scene", "type": "conceptual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_152", "file_type": "json", "from": "method", "predicate": "fine-tunes", "to": "trained CNN model", "type": "conceptual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_179", "file_type": "json", "from": "method", "predicate": "performs", "to": "dense depth optimization", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_179", "file_type": "json", "from": "method", "predicate": "removes_need_for", "to": "view pairing", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_179", "file_type": "json", "from": "method", "predicate": "removes_need_for", "to": "stereo depth estimation", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_179", "file_type": "json", "from": "method", "predicate": "allows_for", "to": "per-image paralleization", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_179", "file_type": "json", "from": "method", "predicate": "is_not_specific_to", "to": "SfM points", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_182", "file_type": "json", "from": "method", "predicate": "leverages", "to": "work on depth", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_182", "file_type": "json", "from": "method", "predicate": "incorporates", "to": "synthesized right views", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_187", "file_type": "json", "from": "method", "predicate": "uses", "to": "local tangent planes", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_187", "file_type": "json", "from": "method", "predicate": "is_composed_of", "to": "two steps", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_187", "file_type": "json", "from": "method", "predicate": "demonstrates", "to": "high completion rate", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_187", "file_type": "json", "from": "method", "predicate": "achieves", "to": "lowest errors", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_280", "file_type": "json", "from": "method", "predicate": "demonstrated through", "to": "experimental results", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_283", "file_type": "json", "from": "method", "predicate": "replaces", "to": "geodesic-preserving term", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_293", "file_type": "json", "from": "method", "predicate": "estimates", "to": "optical flow field", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_293", "file_type": "json", "from": "method", "predicate": "revitalizes", "to": "piecewise parametric flow model", "type": "conceptual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_293", "file_type": "json", "from": "method", "predicate": "handles", "to": "homogeneous motions", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_293", "file_type": "json", "from": "method", "predicate": "handles", "to": "complex motions", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_294", "file_type": "json", "from": "method", "predicate": "has_property", "to": "equity constraint", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_295", "file_type": "json", "from": "method", "predicate": "achieves", "to": "top-tier performances", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_295", "file_type": "json", "from": "method", "predicate": "evaluated_on", "to": "Optical flow benchmarks", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_299", "file_type": "json", "from": "method", "predicate": "is", "to": "unsupervised video salieny detection method", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_299", "file_type": "json", "from": "method", "predicate": "supports", "to": "human activity recognition", "type": "factual", "width": 0.89}, {"arrows": "to", "chunk_id": "doc_0_chunk_299", "file_type": "json", "from": "method", "predicate": "supports", "to": "training of activity detection algorithms", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_300", "file_type": "json", "from": "method", "predicate": "evaluated_on", "to": "challenging datasets", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_300", "file_type": "json", "from": "method", "predicate": "demonstrates", "to": "favorable performance", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_314", "file_type": "json", "from": "method", "predicate": "estimates", "to": "3D pose from 2D joint locations", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_314", "file_type": "json", "from": "method", "predicate": "uses", "to": "over-complete dictionary of poses", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_314", "file_type": "json", "from": "method", "predicate": "shows", "to": "good generalization", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_315", "file_type": "json", "from": "method", "predicate": "parameterizes", "to": "body pose", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_315", "file_type": "json", "from": "method", "predicate": "estimates", "to": "3D pose", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_315", "file_type": "json", "from": "method", "predicate": "uses", "to": "over-completes dictionary of poses", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_315", "file_type": "json", "from": "method", "predicate": "avoids", "to": "impossible poses", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_315", "file_type": "json", "from": "method", "predicate": "compared with", "to": "recent work", "type": "factual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_330", "file_type": "json", "from": "method", "predicate": "evaluated_on", "to": "Mcgill dataset", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_410", "file_type": "json", "from": "method", "predicate": "demonstrates", "to": "state-of-the-art performance", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_335", "file_type": "json", "from": "method", "predicate": "utilizes", "to": "optimization algorithm", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_355", "file_type": "json", "from": "method", "predicate": "applies", "to": "visual attention", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_355", "file_type": "json", "from": "method", "predicate": "uses", "to": "deep neural networks", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_355", "file_type": "json", "from": "method", "predicate": "relies_on", "to": "additional annotations", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_362", "file_type": "json", "from": "method", "predicate": "outperforms", "to": "state-of-the-art image restoration methods", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_364", "file_type": "json", "from": "method", "predicate": "aims_to", "to": "learning features", "type": "operational", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_364", "file_type": "json", "from": "method", "predicate": "aims_to", "to": "learning similarity metric", "type": "operational", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_365", "file_type": "json", "from": "method", "predicate": "outperforms", "to": "state of the art", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_365", "file_type": "json", "from": "method", "predicate": "is resistant to", "to": "over-fitting", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_369", "file_type": "json", "from": "method", "predicate": "inspired_by", "to": "linear discriminant embedding", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_369", "file_type": "json", "from": "method", "predicate": "establishes", "to": "binary tests", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_379", "file_type": "json", "from": "method", "predicate": "aims_to", "to": "localize object", "type": "conceptual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_379", "file_type": "json", "from": "method", "predicate": "takes_as_input", "to": "collection of images", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_380", "file_type": "json", "from": "method", "predicate": "outputs", "to": "bounding box", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_380", "file_type": "json", "from": "method", "predicate": "applies_to", "to": "videos", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_380", "file_type": "json", "from": "method", "predicate": "localizes", "to": "object", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_382", "file_type": "json", "from": "method", "predicate": "transfers", "to": "appearance models", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_382", "file_type": "json", "from": "method", "predicate": "targets", "to": "unseen objects", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_410", "file_type": "json", "from": "method", "predicate": "verified_on", "to": "LFW", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_410", "file_type": "json", "from": "method", "predicate": "verified_on", "to": "YouTube Faces", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_416", "file_type": "json", "from": "method", "predicate": "evaluated_on", "to": "RGB-D action datasets", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_416", "file_type": "json", "from": "method", "predicate": "shows", "to": "promising results", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_435", "file_type": "json", "from": "method", "predicate": "addresses", "to": "tracking inaccuracies", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_470", "file_type": "json", "from": "method", "predicate": "segments", "to": "image", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_470", "file_type": "json", "from": "method", "predicate": "projects", "to": "GIS data", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_483", "file_type": "json", "from": "method", "predicate": "presents", "to": "insight", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_484", "file_type": "json", "from": "method", "predicate": "uses", "to": "min-cuts", "type": "factual", "width": 0.91}, {"arrows": "to", "chunk_id": "doc_0_chunk_500", "file_type": "json", "from": "method", "predicate": "demonstrates", "to": "ability to separate specular reflection", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_500", "file_type": "json", "from": "method", "predicate": "preserves", "to": "saturation of underlying surface colors", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_506", "file_type": "json", "from": "method", "predicate": "achieves", "to": "accurate surface shape estimation", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_506", "file_type": "json", "from": "method", "predicate": "demonstrates", "to": "robustness to light source position errors", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_520", "file_type": "json", "from": "method", "predicate": "demonstrates_utility_in", "to": "salient object detection", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_520", "file_type": "json", "from": "method", "predicate": "demonstrates_utility_in", "to": "object proposal applications", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_556", "file_type": "json", "from": "method", "predicate": "leads_to", "to": "better accuracy", "type": "causal", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_556", "file_type": "json", "from": "method", "predicate": "is_faster_than", "to": "dense projections", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_556", "file_type": "json", "from": "method", "predicate": "is_faster_than", "to": "other methods", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_663", "file_type": "json", "from": "method", "predicate": "improves", "to": "accuracy", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_556", "file_type": "json", "from": "method", "predicate": "reduces", "to": "computational cost", "type": "causal", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_562", "file_type": "json", "from": "method", "predicate": "takes_advantage_of", "to": "depth data", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_562", "file_type": "json", "from": "method", "predicate": "handles", "to": "noisy images", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_562", "file_type": "json", "from": "method", "predicate": "utilizes", "to": "depth channel", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_562", "file_type": "json", "from": "method", "predicate": "improves", "to": "detection of object-like regions", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_562", "file_type": "json", "from": "method", "predicate": "provides", "to": "depth-based local features", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_563", "file_type": "json", "from": "method", "predicate": "provides", "to": "comparable performance", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_575", "file_type": "json", "from": "method", "predicate": "requires", "to": "single input image", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_575", "file_type": "json", "from": "method", "predicate": "uses", "to": "Gaussian Mixture Model", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_575", "file_type": "json", "from": "method", "predicate": "demonstrates", "to": "reflection removal", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_580", "file_type": "json", "from": "method", "predicate": "enhances", "to": "depth maps", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_580", "file_type": "json", "from": "method", "predicate": "improves", "to": "visual fidelity", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_580", "file_type": "json", "from": "method", "predicate": "operates", "to": "four orders of magnitude faster", "type": "factual", "width": 0.91}, {"arrows": "to", "chunk_id": "doc_0_chunk_604", "file_type": "json", "from": "method", "predicate": "exhibits", "to": "small loss in performance", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_604", "file_type": "json", "from": "method", "predicate": "limited_by", "to": "runtime", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_604", "file_type": "json", "from": "method", "predicate": "compared_to", "to": "stereo approaches", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_604", "file_type": "json", "from": "method", "predicate": "compared_to", "to": "optical flow approaches", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_604", "file_type": "json", "from": "method", "predicate": "compared_to", "to": "scene flow approaches", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_605", "file_type": "json", "from": "method", "predicate": "provides", "to": "qualitative results", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_605", "file_type": "json", "from": "method", "predicate": "compares_to", "to": "state-of-the-art stereo", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_605", "file_type": "json", "from": "method", "predicate": "compares_to", "to": "optical flow", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_605", "file_type": "json", "from": "method", "predicate": "compares_to", "to": "scene flow", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_605", "file_type": "json", "from": "method", "predicate": "evaluates_on", "to": "scene flow dataset", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_651", "file_type": "json", "from": "method", "predicate": "improves", "to": "localization", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_663", "file_type": "json", "from": "method", "predicate": "has_speed_up", "to": "factor of 100", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_674", "file_type": "json", "from": "method", "predicate": "builds", "to": "global appearance models", "type": "factual", "width": 0.91}, {"arrows": "to", "chunk_id": "doc_0_chunk_674", "file_type": "json", "from": "method", "predicate": "establishes", "to": "dynamic location models", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_674", "file_type": "json", "from": "method", "predicate": "combines", "to": "elements within energy minimization framework", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_674", "file_type": "json", "from": "method", "predicate": "demonstrates", "to": "superiority over existing algorithms", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_679", "file_type": "json", "from": "method", "predicate": "proposes", "to": "novel method", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_689", "file_type": "json", "from": "method", "predicate": "achieves", "to": "superior denois-ing accuracy", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_689", "file_type": "json", "from": "method", "predicate": "handles", "to": "label corruption levels", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_694", "file_type": "json", "from": "method", "predicate": "addresses", "to": "transparent object reconstruction", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_1", "file_type": "json", "from": "landmarks", "predicate": "located_in", "to": "images of objects", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_1", "file_type": "json", "from": "sequential search", "predicate": "localizes", "to": "landmarks", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_1", "file_type": "json", "from": "landmark addition", "predicate": "depends_on", "to": "image", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_146", "file_type": "json", "from": "image", "predicate": "contains", "to": "object proposal windows", "type": "factual", "width": 0.87}, {"arrows": "to", "chunk_id": "doc_0_chunk_1", "file_type": "json", "from": "groups", "predicate": "scored_using", "to": "learned function", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_2", "file_type": "json", "from": "learned function", "predicate": "used to", "to": "expand groups", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_2", "file_type": "json", "from": "landmark group", "predicate": "scored using", "to": "learned function", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_2", "file_type": "json", "from": "scoring function", "predicate": "learned from", "to": "data labelled with landmarks", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_2", "file_type": "json", "from": "scoring function", "predicate": "derived from", "to": "data", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_2", "file_type": "json", "from": "spatial model", "predicate": "models", "to": "kinematics of landmark groups", "type": "conceptual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_2", "file_type": "json", "from": "landmark", "predicate": "part of", "to": "landmark group", "type": "conceptual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_2", "file_type": "json", "from": "initial landmark", "predicate": "dependent on", "to": "image", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_415", "file_type": "json", "from": "data", "predicate": "is", "to": "compressed", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_415", "file_type": "json", "from": "data", "predicate": "is", "to": "projected", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_685", "file_type": "json", "from": "data", "predicate": "is", "to": "2D, 3D and 4D", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_3", "file_type": "json", "from": "Method", "predicate": "represents", "to": "spatial model", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_3", "file_type": "json", "from": "Method", "predicate": "displays", "to": "strong performance", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_82", "file_type": "json", "from": "Method", "predicate": "demonstrates", "to": "effectiveness", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_82", "file_type": "json", "from": "Method", "predicate": "handles", "to": "outliers", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_106", "file_type": "json", "from": "Method", "predicate": "applied_to", "to": "Texture Classification", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_106", "file_type": "json", "from": "Method", "predicate": "evaluated_on", "to": "KTH-TIPS2", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_106", "file_type": "json", "from": "Method", "predicate": "evaluated_on", "to": "FMD", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_106", "file_type": "json", "from": "Method", "predicate": "evaluated_on", "to": "DTD", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_106", "file_type": "json", "from": "Method", "predicate": "outperforms", "to": "State-of-the-art approaches", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_174", "file_type": "json", "from": "Method", "predicate": "adapts_to", "to": "unknown reflectance maps", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_174", "file_type": "json", "from": "Method", "predicate": "reconstructs", "to": "fine detail", "type": "factual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_175", "file_type": "json", "from": "Method", "predicate": "adapts_to", "to": "Reflectance Maps", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_175", "file_type": "json", "from": "Method", "predicate": "reconstructs", "to": "Fine Detail", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_188", "file_type": "json", "from": "Method", "predicate": "demonstrates", "to": "high completion rate", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_188", "file_type": "json", "from": "Method", "predicate": "achieves", "to": "lowest errors", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_188", "file_type": "json", "from": "Method", "predicate": "operates_in", "to": "noisy cases", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_304", "file_type": "json", "from": "Method", "predicate": "leverages", "to": "Deep Features", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_336", "file_type": "json", "from": "Method", "predicate": "achieves", "to": "State-of-the-Art Performance", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_417", "file_type": "json", "from": "Method", "predicate": "improves", "to": "generalization power", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_417", "file_type": "json", "from": "Method", "predicate": "evaluated_on", "to": "RGB-D action datasets", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_417", "file_type": "json", "from": "Method", "predicate": "achieves", "to": "state-of-the-art results", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_417", "file_type": "json", "from": "Method", "predicate": "shows results", "to": "missing RGB data", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_417", "file_type": "json", "from": "Method", "predicate": "shows results", "to": "missing depth data", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_557", "file_type": "json", "from": "Method", "predicate": "more_accurate_than", "to": "Other methods", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_557", "file_type": "json", "from": "Method", "predicate": "speeds_up", "to": "High-Dimensional Binary Encoding", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_564", "file_type": "json", "from": "Method", "predicate": "handles", "to": "noisy images", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_576", "file_type": "json", "from": "Method", "predicate": "requires", "to": "single input image", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_3", "file_type": "json", "from": "Andriluka et al. (2009)", "predicate": "addresses", "to": "People detection", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_3", "file_type": "json", "from": "Andriluka et al. (2009)", "predicate": "addresses", "to": "articulated pose estimation", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_594", "file_type": "json", "from": "articulated pose estimation", "predicate": "is_method_of", "to": "graphical model", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_3", "file_type": "json", "from": "Barto (1998)", "predicate": "introduces", "to": "Reinforcement learning", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_3", "file_type": "json", "from": "Felzenszwalb and Huttenlocher (2005)", "predicate": "proposes", "to": "Pictorial structures", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_3", "file_type": "json", "from": "Fergus et al. (2003)", "predicate": "proposes", "to": "Unsupervised scale-invariant learning", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_3", "file_type": "json", "from": "Doll\u00b4ar et al. (2009)", "predicate": "proposes", "to": "Integral channel features", "type": "factual", "width": 0.91}, {"arrows": "to", "chunk_id": "doc_0_chunk_3", "file_type": "json", "from": "Eichner and Ferrari (2012)", "predicate": "addresses", "to": "collective human pose estimation", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_4", "file_type": "json", "from": "Doll\u00e1r, P.", "predicate": "authored", "to": "Integral channel features", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_4", "file_type": "json", "from": "Doll\u00e1r, P.", "predicate": "presented_at", "to": "BMVC", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_4", "file_type": "json", "from": "Eichner, M.", "predicate": "authored", "to": "Appearance sharing", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_4", "file_type": "json", "from": "Fei-Fei, L.", "predicate": "authored", "to": "One-shot learning", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_4", "file_type": "json", "from": "Fei-Fei, L.", "predicate": "presented_at", "to": "CVPR", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_4", "file_type": "json", "from": "Felzenszwalb, P. F.", "predicate": "authored", "to": "Cascade object detection", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_4", "file_type": "json", "from": "Felzenszwalb, P. F.", "predicate": "presented_at", "to": "CVPR", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_73", "file_type": "json", "from": "Felzenszwalb, P. F.", "predicate": "authored", "to": "Object detection grammar", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_74", "file_type": "json", "from": "Felzenszwalb, P. F.", "predicate": "writes_paper", "to": "Object detection with grammar models", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_593", "file_type": "json", "from": "Felzenszwalb, P. F.", "predicate": "authored", "to": "Object detection with discriminatively trained part-based models", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_594", "file_type": "json", "from": "Felzenszwalb, P. F.", "predicate": "co_author_of", "to": "Huttenlocher, D. P.", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_4", "file_type": "json", "from": "Fergus, R.", "predicate": "authored", "to": "Sparse object category model", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_4", "file_type": "json", "from": "Fergus, R.", "predicate": "presented_at", "to": "CVPR", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_4", "file_type": "json", "from": "Wang, Y.", "predicate": "authored", "to": "Multiple tree models", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_4", "file_type": "json", "from": "Wang, Y.", "predicate": "presented_at", "to": "ECCV", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_149", "file_type": "json", "from": "BMVC", "predicate": "is_conference_of", "to": "Computer Vision", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_323", "file_type": "json", "from": "BMVC", "predicate": "is_a", "to": "conference", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_177", "file_type": "json", "from": "ECCV", "predicate": "is_publication_venue_for", "to": "reference [1]", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_194", "file_type": "json", "from": "ECCV", "predicate": "published", "to": "variant co-occurrence local binary pattern", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_306", "file_type": "json", "from": "ECCV", "predicate": "is_conference_for", "to": "computer vision", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_676", "file_type": "json", "from": "ECCV", "predicate": "hosts", "to": "geospatial image segmentation approach", "type": "factual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_5", "file_type": "json", "from": "Gedas Bertasius", "predicate": "contributed_to", "to": "DeepEdge", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_9", "file_type": "json", "from": "Gedas Bertasius", "predicate": "affiliation", "to": "University of Pennsylvania", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_5", "file_type": "json", "from": "DeepEdge", "predicate": "is_a", "to": "Deep Network", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_5", "file_type": "json", "from": "DeepEdge", "predicate": "performs", "to": "Contour Detection", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_5", "file_type": "json", "from": "DeepEdge", "predicate": "is_for", "to": "Top-Down Contour Detection", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_5", "file_type": "json", "from": "DeepEdge", "predicate": "published_in", "to": "CVPR", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_5", "file_type": "json", "from": "DeepEdge", "predicate": "has_architecture", "to": "Bifurcated Deep Network", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_6", "file_type": "json", "from": "DeepEdge", "predicate": "proposes", "to": "novel method", "type": "conceptual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_5", "file_type": "json", "from": "Lorenzo Torresani", "predicate": "author_of", "to": "DeepEdge", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_9", "file_type": "json", "from": "Lorenzo Torresani", "predicate": "affiliation", "to": "Dartmouth College", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_6", "file_type": "json", "from": "Contour detection", "predicate": "relies_on", "to": "low-level features", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_662", "file_type": "json", "from": "novel method", "predicate": "improves", "to": "speed of image retrieval", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_662", "file_type": "json", "from": "novel method", "predicate": "learns", "to": "computationally bounded sparse projections", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_662", "file_type": "json", "from": "novel method", "predicate": "adds", "to": "orthogonality constraint", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_679", "file_type": "json", "from": "novel method", "predicate": "is_for", "to": "template matching", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_113", "file_type": "json", "from": "state-of-the-art results", "predicate": "achieved_in", "to": "recognition", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_113", "file_type": "json", "from": "state-of-the-art results", "predicate": "achieved_in", "to": "detection", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_113", "file_type": "json", "from": "state-of-the-art results", "predicate": "achieved_in", "to": "retrieval", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_7", "file_type": "json", "from": "contour detection", "predicate": "related_to", "to": "object recognition", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_236", "file_type": "json", "from": "object recognition", "predicate": "based on", "to": "conv-net", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_417", "file_type": "json", "from": "object recognition", "predicate": "related_task_of", "to": "Action Recognition", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_594", "file_type": "json", "from": "object recognition", "predicate": "uses", "to": "pictorial structures", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_7", "file_type": "json", "from": "Arbel\u00e1ez et al. (2011)", "predicate": "published", "to": "Contour detection and hierarchical image segmentation", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_61", "file_type": "json", "from": "Arbel\u00e1ez et al. (2011)", "predicate": "contributed_to", "to": "Hierarchical Image Segmentation", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_7", "file_type": "json", "from": "Lim et al. (2013)", "predicate": "published", "to": "Sketch tokens", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_7", "file_type": "json", "from": "Sketch tokens", "predicate": "related_to", "to": "contour detection", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_7", "file_type": "json", "from": "Long et al. (2014)", "predicate": "published", "to": "Fully convolutional networks", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_7", "file_type": "json", "from": "Fully convolutional networks", "predicate": "used_for", "to": "semantic segmentation", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_7", "file_type": "json", "from": "Malik et al. (2001)", "predicate": "published", "to": "Contour and texture analysis", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_8", "file_type": "json", "from": "Girshick", "predicate": "author_of", "to": "Rich feature hierarchies", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_328", "file_type": "json", "from": "Girshick", "predicate": "is_affiliated_with", "to": "Microsoft Research", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_570", "file_type": "json", "from": "Girshick", "predicate": "authors", "to": "Rich Feature Hierarchies", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_602", "file_type": "json", "from": "Girshick", "predicate": "authored", "to": "feature hierarchies", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_27", "file_type": "json", "from": "Rich feature hierarchies", "predicate": "used_for", "to": "Object detection", "type": "conceptual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_27", "file_type": "json", "from": "Rich feature hierarchies", "predicate": "used_for", "to": "Semantic segmentation", "type": "conceptual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_73", "file_type": "json", "from": "Rich feature hierarchies", "predicate": "presented_at", "to": "IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)", "type": "factual", "width": 0.91}, {"arrows": "to", "chunk_id": "doc_0_chunk_206", "file_type": "json", "from": "Rich feature hierarchies", "predicate": "published_as", "to": "arXiv preprint", "type": "factual", "width": 0.91}, {"arrows": "to", "chunk_id": "doc_0_chunk_593", "file_type": "json", "from": "Rich feature hierarchies", "predicate": "focused_on", "to": "object detection", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_593", "file_type": "json", "from": "Rich feature hierarchies", "predicate": "focused_on", "to": "semantic segmentation", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_8", "file_type": "json", "from": "Donahue", "predicate": "author_of", "to": "Rich feature hierarchies", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_8", "file_type": "json", "from": "Darrell", "predicate": "author_of", "to": "Rich feature hierarchies", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_570", "file_type": "json", "from": "Hariharan", "predicate": "authors", "to": "Hypercolumns", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_328", "file_type": "json", "from": "Hariharan", "predicate": "is_affiliated_with", "to": "University of California, Berkeley", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_570", "file_type": "json", "from": "Hypercolumns", "predicate": "improves", "to": "Object Segmentation", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_8", "file_type": "json", "from": "Arbel\u00e1ez", "predicate": "author_of", "to": "Hypercolumns", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_328", "file_type": "json", "from": "Arbel\u00e1ez", "predicate": "is_affiliated_with", "to": "Universidad de los Andes", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_570", "file_type": "json", "from": "Arbel\u00e1ez", "predicate": "authors", "to": "Semantic Segmentation", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_8", "file_type": "json", "from": "Iandola", "predicate": "author_of", "to": "Densenet", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_8", "file_type": "json", "from": "Jia", "predicate": "author_of", "to": "Caffe", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_240", "file_type": "json", "from": "Jia", "predicate": "authored", "to": "Large-scale object classification", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_8", "file_type": "json", "from": "Caffe", "predicate": "is_a", "to": "Convolutional architecture", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_219", "file_type": "json", "from": "Caffe", "predicate": "is_architecture_for", "to": "fast feature embedding", "type": "conceptual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_408", "file_type": "json", "from": "Caffe", "predicate": "is_a", "to": "deep learning framework", "type": "conceptual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_408", "file_type": "json", "from": "Caffe", "predicate": "has_application_in", "to": "computer vision tasks", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_408", "file_type": "json", "from": "Caffe", "predicate": "used_for", "to": "intrinsic image decomposition", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_8", "file_type": "json", "from": "Girshik", "predicate": "related_to", "to": "Object detection", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_27", "file_type": "json", "from": "Object detection", "predicate": "requires", "to": "Rich feature hierarchies", "type": "conceptual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_367", "file_type": "json", "from": "Object detection", "predicate": "utilizes", "to": "part-based models", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_9", "file_type": "json", "from": "Shelhamer et al.", "predicate": "authored", "to": "Caffe", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_9", "file_type": "json", "from": "Ren et al.", "predicate": "authored", "to": "Scale-invariant contour completion", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_9", "file_type": "json", "from": "Scale-invariant contour completion", "predicate": "uses", "to": "Condition random fields", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_9", "file_type": "json", "from": "Jianbo Shi", "predicate": "affiliation", "to": "University of Pennsylvania", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_9", "file_type": "json", "from": "Wen Wang", "predicate": "author_of", "to": "Discrimi nant Analysis", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_14", "file_type": "json", "from": "Wen Wang", "predicate": "affiliated_with", "to": "Key Laboratory of Intelligent Information Processing", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_15", "file_type": "json", "from": "Wen Wang", "predicate": "member_of", "to": "Institute of Computing Technology", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_15", "file_type": "json", "from": "Wen Wang", "predicate": "has_email", "to": "wen.wang@vipl.ict.ac.cn", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_9", "file_type": "json", "from": "Discrimi nant Analysis", "predicate": "analyzes", "to": "Gaussian Distributions", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_9", "file_type": "json", "from": "Discrimi nant Analysis", "predicate": "applied_to", "to": "Face Recognition", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_9", "file_type": "json", "from": "Ruiping Wang", "predicate": "author_of", "to": "Discrimi nant Analysis", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_10", "file_type": "json", "from": "Ruiping Wang", "predicate": "authored", "to": "Wang_Discriminant_Analysis_on_2015_CVPR_paper", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_14", "file_type": "json", "from": "Ruiping Wang", "predicate": "affiliated_with", "to": "Key Laboratory of Intelligent Information Processing", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_15", "file_type": "json", "from": "Ruiping Wang", "predicate": "member_of", "to": "Institute of Computing Technology", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_15", "file_type": "json", "from": "Ruiping Wang", "predicate": "has_email", "to": "wangruiping@ict.ac.cn", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_12", "file_type": "json", "from": "Face Recognition", "predicate": "uses", "to": "Gaussian Mixture Models", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_12", "file_type": "json", "from": "Face Recognition", "predicate": "uses", "to": "Riemannian Manifold", "type": "factual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_12", "file_type": "json", "from": "Face Recognition", "predicate": "uses", "to": "Discriminant Analysis", "type": "factual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_12", "file_type": "json", "from": "Face Recognition", "predicate": "uses", "to": "Kernel Methods", "type": "factual", "width": 0.7}, {"arrows": "to", "chunk_id": "doc_0_chunk_13", "file_type": "json", "from": "Face Recognition", "predicate": "based_on", "to": "Image Sets", "type": "conceptual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_409", "file_type": "json", "from": "Face Recognition", "predicate": "has_challenge", "to": "Pose variation", "type": "conceptual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_411", "file_type": "json", "from": "Face Recognition", "predicate": "employs", "to": "PEP (Probabilistic Elastic Part) Model", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_411", "file_type": "json", "from": "Face Recognition", "predicate": "is_evaluated_on", "to": "LFW", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_411", "file_type": "json", "from": "Face Recognition", "predicate": "participates_in", "to": "PaSC", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_411", "file_type": "json", "from": "Face Recognition", "predicate": "achieves", "to": "state-of-the-art performance", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_586", "file_type": "json", "from": "Face Recognition", "predicate": "benefits_from", "to": "Poisson Editing", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_586", "file_type": "json", "from": "Face Recognition", "predicate": "improves_performance_with", "to": "3D Morphable Models", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_10", "file_type": "json", "from": "Zhiwu Huang", "predicate": "authored", "to": "Wang_Discribminant_Analysis_on_2015_CVPR_paper", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_15", "file_type": "json", "from": "Zhiwu Huang", "predicate": "member_of", "to": "Institute of Computing Technology", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_15", "file_type": "json", "from": "Zhiwu Huang", "predicate": "has_email", "to": "zhiwu.huang@vipl.ict.ac.cn", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_10", "file_type": "json", "from": "Shiguan Shan", "predicate": "authored", "to": "Wang_Discriminant_Analysis_on_2015_CVPR_paper", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_15", "file_type": "json", "from": "Shiguan Shan", "predicate": "member_of", "to": "Institute of Computing Technology", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_15", "file_type": "json", "from": "Shiguan Shan", "predicate": "has_email", "to": "sgshan@ict.ac.cn", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_10", "file_type": "json", "from": "Xilin Chen", "predicate": "authored", "to": "Wang_Discriminant_Analysis_on_2015_CVPR_paper", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_15", "file_type": "json", "from": "Xilin Chen", "predicate": "member_of", "to": "Institute of Computing Technology", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_15", "file_type": "json", "from": "Xilin Chen", "predicate": "has_email", "to": "xlchen@ict.ac.cn", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_16", "file_type": "json", "from": "Xilin Chen", "predicate": "affiliated_with", "to": "Key Laboratory of Intelligent Information Processing", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_10", "file_type": "json", "from": "DARG", "predicate": "addresses", "to": "face recognition problem", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_10", "file_type": "json", "from": "DARG", "predicate": "is_a", "to": "novel method", "type": "conceptual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_10", "file_type": "json", "from": "DARG", "predicate": "represents", "to": "image sets", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_10", "file_type": "json", "from": "DARG", "predicate": "discriminates", "to": "Gaussian components", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_10", "file_type": "json", "from": "image sets", "predicate": "represented_as", "to": "Gaussian Mixture Models", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_10", "file_type": "json", "from": "Gaussian distributions", "predicate": "lie_on", "to": "Riemannian manifold", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_10", "file_type": "json", "from": "Kernel Discrimiant Analysis", "predicate": "utilizes", "to": "probabilistic kernels", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_10", "file_type": "json", "from": "probabilistic kernels", "predicate": "encode", "to": "geometry", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_11", "file_type": "json", "from": "proposed method", "predicate": "evaluated_on", "to": "face recognition databases", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_11", "file_type": "json", "from": "proposed method", "predicate": "demonstrates", "to": "superior performance", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_39", "file_type": "json", "from": "proposed method", "predicate": "outperforms", "to": "state-of-the-art algorithms", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_39", "file_type": "json", "from": "proposed method", "predicate": "evaluated_on", "to": "MultiPIE dataset", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_294", "file_type": "json", "from": "proposed method", "predicate": "handles", "to": "homogeneous motions", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_294", "file_type": "json", "from": "proposed method", "predicate": "handles", "to": "complex motions", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_294", "file_type": "json", "from": "proposed method", "predicate": "achieves", "to": "top-tier performances", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_294", "file_type": "json", "from": "proposed method", "predicate": "performs_better_than", "to": "state of the art", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_616", "file_type": "json", "from": "proposed method", "predicate": "retrieves", "to": "similar 3D models", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_616", "file_type": "json", "from": "proposed method", "predicate": "transfers", "to": "symmetries", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_11", "file_type": "json", "from": "face recognition databases", "predicate": "are", "to": "challenging", "type": "factual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_11", "file_type": "json", "from": "superior performance", "predicate": "compared_to", "to": "state-of-the-art approaches", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_625", "file_type": "json", "from": "superior performance", "predicate": "compared_to", "to": "existing methods", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_11", "file_type": "json", "from": "Russian components", "predicate": "from", "to": "different subjects", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_11", "file_type": "json", "from": "prior probabilities", "predicate": "incorporated_in", "to": "Russian components", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_273", "file_type": "json", "from": "Kernel Methods", "predicate": "is_topic_of", "to": "cvpr_papers", "type": "conceptual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_12", "file_type": "json", "from": "Aranndi et al. (2005)", "predicate": "published", "to": "Face recognition with image sets using manifold density divergence", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_12", "file_type": "json", "from": "Face recognition with image sets using manifold density divergence", "predicate": "addresses", "to": "Face Recognition", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_12", "file_type": "json", "from": "Amar \u0026 Nagaoka (2000)", "predicate": "authored", "to": "Methods of Information Geometry", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_12", "file_type": "json", "from": "Methods of Information Geometry", "predicate": "describes", "to": "Information Geometry", "type": "conceptual", "width": 0.82}, {"arrows": "to", "chunk_id": "doc_0_chunk_12", "file_type": "json", "from": "Chan et al. (2004)", "predicate": "developed", "to": "Probabilistic Kernels", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_13", "file_type": "json", "from": "Probabilistic Kernels", "predicate": "based_on", "to": "Information Divergence", "type": "conceptual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_12", "file_type": "json", "from": "Probabilistic KernELS", "predicate": "based on", "to": "Information Divergence", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_13", "file_type": "json", "from": "Chan, A. B.", "predicate": "authored", "to": "Probabilistic Kernels", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_463", "file_type": "json", "from": "Chan, A. B.", "predicate": "authored", "to": "Modeling, clustering, and segmenting video", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_13", "file_type": "json", "from": "Moreno, P. J.", "predicate": "co-authored", "to": "Probabilistic Kernels", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_13", "file_type": "json", "from": "Cevikalp, H.", "predicate": "authored", "to": "Face Recognition", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_13", "file_type": "json", "from": "Triggs, B.", "predicate": "co-authored", "to": "Face Recognition", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_13", "file_type": "json", "from": "Image Sets Alignment", "predicate": "for", "to": "Video-based Face Recognition", "type": "conceptual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_13", "file_type": "json", "from": "Cui, Z.", "predicate": "authored", "to": "Image Sets Alignment", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_13", "file_type": "json", "from": "Grassmann Discriminant Analysis", "predicate": "provides", "to": "Subspace-based Learning", "type": "conceptual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_13", "file_type": "json", "from": "Hamm, J.", "predicate": "authored", "to": "Grassmann Discriminant Analysis", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_13", "file_type": "json", "from": "Lee, D. D.", "predicate": "co-authored", "to": "Grassmann Discriminant Analysis", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_13", "file_type": "json", "from": "Sparse Approximated Nearest Points", "predicate": "used_for", "to": "Image Set Classification", "type": "conceptual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_13", "file_type": "json", "from": "Hu, Y.", "predicate": "authored", "to": "Sparse Approximated Nearest Points", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_14", "file_type": "json", "from": "Hu, Y.", "predicate": "authored", "to": "Sparse approximated nearest points", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_14", "file_type": "json", "from": "Hu, Y.", "predicate": "presented_at", "to": "IEEE International Conference on Computer Vision and Pattern Recognized (CVPR)", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_14", "file_type": "json", "from": "Harandi, M. T.", "predicate": "authored", "to": "Grasmannian kernels", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_14", "file_type": "json", "from": "Jayasumana, S.", "predicate": "presented_at", "to": "IEEE Computer Society on Computer Vision and Pattern Recognition (CVPR)", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_14", "file_type": "json", "from": "Kim, M.", "predicate": "authored", "to": "Face tracking and recognition", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_14", "file_type": "json", "from": "Kim, M.", "predicate": "presented_at", "to": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_16", "file_type": "json", "from": "Key Laboratory of Intelligent Information Processing", "predicate": "is_part_of", "to": "Chinese Academy of Sciences", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_16", "file_type": "json", "from": "Key Laboratory of Intelligent Information Processing", "predicate": "located_in", "to": "Institute of Computing Technology", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_450", "file_type": "json", "from": "Chinese Academy of Sciences", "predicate": "is_affiliated_with", "to": "hanics", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_544", "file_type": "json", "from": "Chinese Academy of Sciences", "predicate": "located_in", "to": "Beijing", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_15", "file_type": "json", "from": "Institute of Computing Technology", "predicate": "part_of", "to": "Chinese Academy of Sciences", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_16", "file_type": "json", "from": "Super-resolution Person Re-identi\ufb01cation", "predicate": "is_publication_of", "to": "Xiao-Yuan Jing", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_16", "file_type": "json", "from": "Super-resolution Person Re-identi\ufb01cation", "predicate": "is_publication_of", "to": "Xiaoke Zhu", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_16", "file_type": "json", "from": "Super-resolution Person Re-identi\ufb01cation", "predicate": "is_publication_of", "to": "Fei Wu", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_16", "file_type": "json", "from": "Super-resolution Person Re-identi\ufb01cation", "predicate": "is_publication_of", "to": "Xinge You", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_16", "file_type": "json", "from": "Super-resolution Person Re-identi\ufb01cation", "predicate": "is_publication_of", "to": "Qinglong Liu", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_16", "file_type": "json", "from": "Super-resolution Person Re-identi\ufb01cation", "predicate": "is_publication_of", "to": "Dong Yue", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_16", "file_type": "json", "from": "Super-resolution Person Re-identi\ufb01cation", "predicate": "is_publication_of", "to": "Ruimin Hu", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_16", "file_type": "json", "from": "Super-resolution Person Re-identi\ufb01cation", "predicate": "is_publication_of", "to": "Baowen Xu", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_16", "file_type": "json", "from": "Super-resolution Person Re-identi\ufb01cation", "predicate": "presented_at", "to": "CVPR", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_21", "file_type": "json", "from": "Xiao-Yuan Jing", "predicate": "affiliated_with", "to": "State Key Laboratory of Software Engineering", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_21", "file_type": "json", "from": "Xiaoke Zhu", "predicate": "affiliated_with", "to": "State Key Laboratory of Software Engineering", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_21", "file_type": "json", "from": "Fei Wu", "predicate": "affiliated_with", "to": "State Key Laboratory of Software Engineering", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_22", "file_type": "json", "from": "Xinge You", "predicate": "affiliated_with", "to": "Huazhong University of Science and Technology", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_22", "file_type": "json", "from": "Qinglong Liu", "predicate": "affiliated_with", "to": "State Key Laboratory of Software Engineering", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_22", "file_type": "json", "from": "Dong Yue", "predicate": "affiliated_with", "to": "Nanjing University of Posts and Telecommunications", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_22", "file_type": "json", "from": "Ruimin Hu", "predicate": "affiliated_with", "to": "National Engineering Research Center for Multimedia Software", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_22", "file_type": "json", "from": "Baowen Xu", "predicate": "affiliated_with", "to": "State Key Laboratory of Software Engineering", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_23", "file_type": "json", "from": "Baowen Xu", "predicate": "affiliated_with", "to": "School of Computer", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_16", "file_type": "json", "from": "Jing_Super-Resolution_Person_Re-Identification_2015_CVPR_paper.pdf", "predicate": "is_file_of", "to": "Super-resolution Person Re-identi\ufb01cation", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_17", "file_type": "json", "from": "Person re-identification", "predicate": "is_important_in", "to": "surveillance applications", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_17", "file_type": "json", "from": "Person re-identification", "predicate": "is_important_in", "to": "forensics applications", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_19", "file_type": "json", "from": "Person re-identification", "predicate": "uses", "to": "semi-coupled dictionaries", "type": "factual", "width": 0.89}, {"arrows": "to", "chunk_id": "doc_0_chunk_18", "file_type": "json", "from": "SLD2L", "predicate": "has_purpose", "to": "converting LR probe image features", "type": "conceptual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_19", "file_type": "json", "from": "SLD2L", "predicate": "demonstrates", "to": "effectiveness", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_330", "file_type": "json", "from": "effectiveness", "predicate": "relates_to", "to": "3D shape matching", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_330", "file_type": "json", "from": "effectiveness", "predicate": "relates_to", "to": "3D shape retrieval", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_388", "file_type": "json", "from": "effectiveness", "predicate": "of", "to": "proposed approach", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_18", "file_type": "json", "from": "discriminant term", "predicate": "ensures", "to": "converted features are far from different-person HR gallery features", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_19", "file_type": "json", "from": "low-rank regularization", "predicate": "characterizes", "to": "intrinsic feature space", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_18", "file_type": "json", "from": "low-rank regularization", "predicate": "applied_to", "to": "HR images", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_18", "file_type": "json", "from": "low-rank regularization", "predicate": "applied_to", "to": "LR images", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_19", "file_type": "json", "from": "HR images", "predicate": "is_type_of", "to": "image", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_19", "file_type": "json", "from": "LR images", "predicate": "is_type_of", "to": "image", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_18", "file_type": "json", "from": "HR gallery images", "predicate": "has_feature", "to": "features", "type": "conceptual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_274", "file_type": "json", "from": "features", "predicate": "are", "to": "discriminative for categorization", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_713", "file_type": "json", "from": "features", "predicate": "measures", "to": "semantic obviousness", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_713", "file_type": "json", "from": "features", "predicate": "discovers", "to": "local characteristics", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_18", "file_type": "json", "from": "LR probe images", "predicate": "has_feature", "to": "features", "type": "conceptual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_18", "file_type": "json", "from": "public datasets", "predicate": "demonstrates", "to": "SLD2L", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_19", "file_type": "json", "from": "HR gallery", "predicate": "features", "to": "low-rank regularization", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_19", "file_type": "json", "from": "Super-resolution person re-identification", "predicate": "is_topic_of", "to": "research", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_299", "file_type": "json", "from": "research", "predicate": "focuses_on", "to": "predicting human gaze", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_374", "file_type": "json", "from": "research", "predicate": "is in", "to": "early stage", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_596", "file_type": "json", "from": "research", "predicate": "is_area_of", "to": "computer vision", "type": "conceptual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_19", "file_type": "json", "from": "Low-rank discriminant dictionary learning", "predicate": "is_technique", "to": "machine learning", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_19", "file_type": "json", "from": "Semi-coupled dictionaries", "predicate": "is_method", "to": "person re-identification", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_243", "file_type": "json", "from": "person re-identification", "predicate": "aims_to", "to": "match pedestrian images", "type": "conceptual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_243", "file_type": "json", "from": "person re-identification", "predicate": "remains", "to": "challenging", "type": "factual", "width": 0.5}, {"arrows": "to", "chunk_id": "doc_0_chunk_247", "file_type": "json", "from": "person re-identification", "predicate": "uses", "to": "Mahalanobis distance", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_397", "file_type": "json", "from": "person re-identification", "predicate": "is", "to": "cross-dataset task", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_19", "file_type": "json", "from": "Feature representation learning", "predicate": "is_field", "to": "machine learning", "type": "conceptual", "width": 0.87}, {"arrows": "to", "chunk_id": "doc_0_chunk_19", "file_type": "json", "from": "Bak et al. (2010)", "predicate": "researches", "to": "Person re-identification", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_19", "file_type": "json", "from": "Bedagkar-Gala \u0026 Shah (2014)", "predicate": "surveys", "to": "person re-identi\ufb01cation approaches", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_19", "file_type": "json", "from": "Liu et al. (2014)", "predicate": "proposes", "to": "semi-supervised coupled dictionary learning", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_20", "file_type": "json", "from": "Liu, X.", "predicate": "authored", "to": "Semi-supervised coupled dictionary learning for person re-identi\ufb01cation", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_20", "file_type": "json", "from": "Semi-supervised coupled dictionary learning for person re-identi\ufb01cation", "predicate": "presented_at", "to": "CVPR, IEEE Conference on", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_20", "file_type": "json", "from": "Ma, L.", "predicate": "authored", "to": "Sparse representation for face recognition based on discriminative low-rank dictionary learning", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_20", "file_type": "json", "from": "Ma, L.", "predicate": "authored", "to": "Person re-identi\ufb01cation over camera networks using multi-task distance metric learning", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_21", "file_type": "json", "from": "Ma, L.", "predicate": "authored", "to": "Person re-identi\ufb01cation over camera networks", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_20", "file_type": "json", "from": "Gray, D.", "predicate": "authored", "to": "Evaluating appearance models for recognition, reacquisition, and tracking", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_20", "file_type": "json", "from": "Gray, D.", "predicate": "authored", "to": "Viewpoint invariant pedestrian recognition with an ensemble of localized features", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_399", "file_type": "json", "from": "Gray, D.", "predicate": "authored", "to": "Viewpoint invariant pedestrian recognition", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_20", "file_type": "json", "from": "Evaluating appearance models for recognition, reacquisition, and tracking", "predicate": "presented_at", "to": "Performance Evaluation of Tracking and Surveillance, IEEE workshop on", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_20", "file_type": "json", "from": "Viewpoint invariant pedestrian recognition with an ensemble of localized features", "predicate": "presented_at", "to": "ECCV", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_20", "file_type": "json", "from": "Person re-identi\ufb01cation over camera networks using multi-task distance metric learning", "predicate": "published_in", "to": "Image Processing, IEEE Transactions on", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_21", "file_type": "json", "from": "Image Processing, IEEE Transactions on", "predicate": "publishes", "to": "Person re-identi\ufb01cation over camera networks", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_21", "file_type": "json", "from": "Image Processing, IEEE Transactions on", "predicate": "publishes", "to": "Image super-resolution via sparse representation", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_21", "file_type": "json", "from": "Hirzer, M.", "predicate": "authored", "to": "Person re-identi\ufb01cation by descriptive and discriminative classification", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_21", "file_type": "json", "from": "Zheng, W.-S.", "predicate": "authored", "to": "Reidenti\ufb01cation by relative distance comparison", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_23", "file_type": "json", "from": "State Key Laboratory of Software Engineering", "predicate": "affiliated_with", "to": "School of Computer", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_21", "file_type": "json", "from": "Image Analysis", "predicate": "publishes", "to": "Person re-identi\ufb01cation by descriptive and discriminative classification", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_21", "file_type": "json", "from": "Pattern Analysis and Machine Intelligence, IEEE Transactions on", "predicate": "publishes", "to": "Reidenti\ufb01cation by relative distance comparison", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_22", "file_type": "json", "from": "re Engineering", "predicate": "located_in", "to": "School of Computer", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_23", "file_type": "json", "from": "School of Computer", "predicate": "part_of", "to": "Wuhan University", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_23", "file_type": "json", "from": "Wuhan University", "predicate": "located_in", "to": "China", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_22", "file_type": "json", "from": "School of Electronic Information and Communications", "predicate": "part_of", "to": "Huazhong University of Science and Technology", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_22", "file_type": "json", "from": "National Engineering Research Center for Multimedia Software", "predicate": "part_of", "to": "School of Computer", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_23", "file_type": "json", "from": "Ronan Collobert", "predicate": "author_of", "to": "From Image-level to Pixel-level Labeling", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_29", "file_type": "json", "from": "Ronan Collobert", "predicate": "affiliated_with", "to": "Facebook AI Research", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_28", "file_type": "json", "from": "Ronan Collobert", "predicate": "email", "to": "ronan@coltobert.com", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_29", "file_type": "json", "from": "Ronan Collobert", "predicate": "works_at", "to": "Menlo Park", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_23", "file_type": "json", "from": "From Image-level to Pixel-level Labeling", "predicate": "published_in", "to": "CVPR", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_23", "file_type": "json", "from": "Multimedia Software", "predicate": "developed_by", "to": "School of Computer", "type": "factual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_24", "file_type": "json", "from": "We", "predicate": "are_interested_in", "to": "object segmentation", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_24", "file_type": "json", "from": "object segmentation", "predicate": "requires", "to": "object class information", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_485", "file_type": "json", "from": "object segmentation", "predicate": "related_to", "to": "salient object detection", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_717", "file_type": "json", "from": "object segmentation", "predicate": "is_part_of", "to": "object and motion segmentation", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_24", "file_type": "json", "from": "weakly supervised segmentation task", "predicate": "fits_framework", "to": "Multiple Instance Learning (MIL) framework", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_24", "file_type": "json", "from": "training image", "predicate": "has", "to": "pixel corresponding to image class label", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_24", "file_type": "json", "from": "segmentation task", "predicate": "is_rewritten_as", "to": "inferring pixels belonging to class of object", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_24", "file_type": "json", "from": "model", "predicate": "is_based_on", "to": "Convolutional Neural Network", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_24", "file_type": "json", "from": "model", "predicate": "is_constrained_during", "to": "training", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_24", "file_type": "json", "from": "model", "predicate": "discriminates", "to": "pixels", "type": "factual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_25", "file_type": "json", "from": "model", "predicate": "discriminates", "to": "right pixels", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_25", "file_type": "json", "from": "model", "predicate": "beats", "to": "state of the art results", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_25", "file_type": "json", "from": "model", "predicate": "performs", "to": "weakly supervised object segmentation task", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_25", "file_type": "json", "from": "model", "predicate": "compared_with", "to": "fully-supervised segmentation approaches", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_238", "file_type": "json", "from": "model", "predicate": "unifies", "to": "existing ZSL algorithms", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_238", "file_type": "json", "from": "model", "predicate": "demonstrates", "to": "performance gains", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_402", "file_type": "json", "from": "model", "predicate": "uses", "to": "patch representations", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_402", "file_type": "json", "from": "model", "predicate": "is_benchmarked_on", "to": "Intrinsic Images in the Wild dataset", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_415", "file_type": "json", "from": "model", "predicate": "is", "to": "low-rank", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_415", "file_type": "json", "from": "model", "predicate": "improves", "to": "generalization power", "type": "causal", "width": 0.89}, {"arrows": "to", "chunk_id": "doc_0_chunk_452", "file_type": "json", "from": "model", "predicate": "accounts_for", "to": "global salience effects", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_452", "file_type": "json", "from": "model", "predicate": "achieves", "to": "state-of-the-art accuracy", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_476", "file_type": "json", "from": "model", "predicate": "fits", "to": "noisy images", "type": "factual", "width": 0.91}, {"arrows": "to", "chunk_id": "doc_0_chunk_510", "file_type": "json", "from": "model", "predicate": "constrained_by", "to": "loss minimization", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_533", "file_type": "json", "from": "model", "predicate": "integrates", "to": "computer vision models", "type": "conceptual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_533", "file_type": "json", "from": "model", "predicate": "integrates", "to": "human input", "type": "conceptual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_533", "file_type": "json", "from": "model", "predicate": "uses", "to": "Markov Decision Process", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_547", "file_type": "json", "from": "model", "predicate": "is a model of", "to": "bottom-up segmentation", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_591", "file_type": "json", "from": "model", "predicate": "evaluated_on", "to": "We Are Family", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_592", "file_type": "json", "from": "model", "predicate": "requires", "to": "computations", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_593", "file_type": "json", "from": "model", "predicate": "evaluated_on", "to": "Stickmen dataset", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_593", "file_type": "json", "from": "model", "predicate": "achieves", "to": "performance improvements", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_631", "file_type": "json", "from": "model", "predicate": "addresses", "to": "light transport complexities", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_263", "file_type": "json", "from": "Convolutional Neural Network", "predicate": "is_architecture_for", "to": "pose estimation", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_24", "file_type": "json", "from": "training", "predicate": "prioritizes", "to": "pixels important for image classification", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_25", "file_type": "json", "from": "training", "predicate": "focuses_on", "to": "pixels", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_565", "file_type": "json", "from": "training", "predicate": "is_expensive_due_to", "to": "structured prediction subroutine", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_25", "file_type": "json", "from": "network-based model", "predicate": "constrained_during", "to": "training", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_25", "file_type": "json", "from": "network-based model", "predicate": "weights", "to": "important pixels", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_25", "file_type": "json", "from": "system", "predicate": "trained_using", "to": "Imaginet dataset", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_165", "file_type": "json", "from": "system", "predicate": "leverages", "to": "interest regions", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_164", "file_type": "json", "from": "system", "predicate": "requires", "to": "user\u0027s passive participation", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_165", "file_type": "json", "from": "system", "predicate": "calibrates itself", "to": "without user participation", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_165", "file_type": "json", "from": "system", "predicate": "requires", "to": "calibration", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_286", "file_type": "json", "from": "system", "predicate": "is", "to": "efficient", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_286", "file_type": "json", "from": "system", "predicate": "handles", "to": "noisy data", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_286", "file_type": "json", "from": "system", "predicate": "handles", "to": "missing data", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_286", "file_type": "json", "from": "system", "predicate": "handles", "to": "sudden camera motions", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_286", "file_type": "json", "from": "system", "predicate": "requires", "to": "no training data", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_286", "file_type": "json", "from": "system", "predicate": "performs", "to": "comparably to batch methods", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_286", "file_type": "json", "from": "system", "predicate": "improves_over", "to": "sequential methods", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_288", "file_type": "json", "from": "system", "predicate": "characterized by", "to": "elegant structure", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_288", "file_type": "json", "from": "system", "predicate": "characterized by", "to": "speed", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_288", "file_type": "json", "from": "system", "predicate": "performs", "to": "pixel-level salience computation", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_288", "file_type": "json", "from": "system", "predicate": "integrated into", "to": "object proposal generation framework", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_288", "file_type": "json", "from": "system", "predicate": "achieves", "to": "state-of-the-art performance", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_340", "file_type": "json", "from": "system", "predicate": "uses", "to": "supervised machine learning", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_340", "file_type": "json", "from": "system", "predicate": "learns_to", "to": "synthesize images", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_340", "file_type": "json", "from": "system", "predicate": "enables", "to": "redirection of gaze", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_340", "file_type": "json", "from": "system", "predicate": "is", "to": "computationally efficient", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_340", "file_type": "json", "from": "system", "predicate": "runs_on", "to": "laptop", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_340", "file_type": "json", "from": "system", "predicate": "avoids", "to": "uncanny valley effect", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_532", "file_type": "json", "from": "system", "predicate": "takes as input", "to": "image to annotate", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_532", "file_type": "json", "from": "system", "predicate": "takes as input", "to": "annotation constraints", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_532", "file_type": "json", "from": "system", "predicate": "produces", "to": "object annotations", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_614", "file_type": "json", "from": "system", "predicate": "has", "to": "relationships", "type": "conceptual", "width": 0.7}, {"arrows": "to", "chunk_id": "doc_0_chunk_614", "file_type": "json", "from": "system", "predicate": "has", "to": "dependencies", "type": "conceptual", "width": 0.68}, {"arrows": "to", "chunk_id": "doc_0_chunk_25", "file_type": "json", "from": "segmentation experiments", "predicate": "performed_on", "to": "Pascal VOC dataset", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_26", "file_type": "json", "from": "Model", "predicate": "achieves", "to": "state of the art results", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_26", "file_type": "json", "from": "Model", "predicate": "compared_with", "to": "fully-supervised segmentation approaches", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_68", "file_type": "json", "from": "Model", "predicate": "is", "to": "Quasi-parametric Model", "type": "conceptual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_68", "file_type": "json", "from": "Model", "predicate": "retrieves", "to": "KNN Images", "type": "factual", "width": 0.91}, {"arrows": "to", "chunk_id": "doc_0_chunk_68", "file_type": "json", "from": "Model", "predicate": "matches", "to": "Semantic Regions", "type": "factual", "width": 0.89}, {"arrows": "to", "chunk_id": "doc_0_chunk_68", "file_type": "json", "from": "Model", "predicate": "fuses", "to": "Matched Regions", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_68", "file_type": "json", "from": "Model", "predicate": "refines", "to": "Result", "type": "factual", "width": 0.87}, {"arrows": "to", "chunk_id": "doc_0_chunk_26", "file_type": "json", "from": "Object Segmentation", "predicate": "is a", "to": "task", "type": "conceptual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_290", "file_type": "json", "from": "Object Segmentation", "predicate": "is_related_to", "to": "Computer Vision", "type": "conceptual", "width": 0.7}, {"arrows": "to", "chunk_id": "doc_0_chunk_484", "file_type": "json", "from": "Object Segmentation", "predicate": "is_related_to", "to": "Salient Object Detection", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_26", "file_type": "json", "from": "Weakly Supervised Segmentation", "predicate": "is a", "to": "approach", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_64", "file_type": "json", "from": "approach", "predicate": "models", "to": "textured 3D non-rigid models", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_64", "file_type": "json", "from": "approach", "predicate": "includes", "to": "photometric information", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_64", "file_type": "json", "from": "approach", "predicate": "uses", "to": "shape manifold", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_64", "file_type": "json", "from": "approach", "predicate": "introduces", "to": "new discretization method", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_65", "file_type": "json", "from": "approach", "predicate": "demonstrates", "to": "high performance", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_75", "file_type": "json", "from": "approach", "predicate": "is_faster_than", "to": "existing learning-based methods", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_75", "file_type": "json", "from": "approach", "predicate": "provides", "to": "best results", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_87", "file_type": "json", "from": "approach", "predicate": "handles", "to": "complex non-uniform motion blur", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_88", "file_type": "json", "from": "approach", "predicate": "handles", "to": "non-uniform motion blur", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_88", "file_type": "json", "from": "approach", "predicate": "is", "to": "deblurring model", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_95", "file_type": "json", "from": "approach", "predicate": "is useful when", "to": "original enhancement algorithms are unknown", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_95", "file_type": "json", "from": "approach", "predicate": "is useful when", "to": "original enhancement algorithms are inaccessible", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_125", "file_type": "json", "from": "approach", "predicate": "validates_on", "to": "first-person point-of-view videos", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_125", "file_type": "json", "from": "approach", "predicate": "corresponds_to", "to": "Cutkosky grasp taxonomy", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_146", "file_type": "json", "from": "approach", "predicate": "demonstrates", "to": "consistent performance gain", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_152", "file_type": "json", "from": "approach", "predicate": "uses", "to": "datasets", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_182", "file_type": "json", "from": "approach", "predicate": "focuses_on", "to": "depth estimation", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_182", "file_type": "json", "from": "approach", "predicate": "involves", "to": "synthesizing intermediate views", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_182", "file_type": "json", "from": "approach", "predicate": "enhances", "to": "depth perception", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_182", "file_type": "json", "from": "approach", "predicate": "demonstrates", "to": "improvements", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_197", "file_type": "json", "from": "approach", "predicate": "has", "to": "lower computational complexity", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_197", "file_type": "json", "from": "approach", "predicate": "achieves", "to": "state-of-the-art performance", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_197", "file_type": "json", "from": "approach", "predicate": "performs_on", "to": "Stanford Background dataset", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_197", "file_type": "json", "from": "approach", "predicate": "performs_on", "to": "SIFT Flow datasets", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_205", "file_type": "json", "from": "approach", "predicate": "evaluated_on", "to": "public datasets", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_260", "file_type": "json", "from": "approach", "predicate": "uses", "to": "semiglobal matching", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_272", "file_type": "json", "from": "approach", "predicate": "demonstrated_on", "to": "non-flat manifolds", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_272", "file_type": "json", "from": "approach", "predicate": "applied_to", "to": "Euclidean spaces", "type": "factual", "width": 0.86}, {"arrows": "to", "chunk_id": "doc_0_chunk_285", "file_type": "json", "from": "approach", "predicate": "relies on", "to": "global representations", "type": "conceptual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_285", "file_type": "json", "from": "approach", "predicate": "ensures", "to": "temporal consistency", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_285", "file_type": "json", "from": "approach", "predicate": "ensures", "to": "spatial consistency", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_285", "file_type": "json", "from": "approach", "predicate": "robust to", "to": "noisy data", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_285", "file_type": "json", "from": "approach", "predicate": "robust to", "to": "missing data", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_285", "file_type": "json", "from": "approach", "predicate": "robust to", "to": "sudden camera motions", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_285", "file_type": "json", "from": "approach", "predicate": "requires", "to": "training data", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_299", "file_type": "json", "from": "approach", "predicate": "utilizes", "to": "encoding method", "type": "factual", "width": 0.91}, {"arrows": "to", "chunk_id": "doc_0_chunk_309", "file_type": "json", "from": "approach", "predicate": "uses", "to": "part-based region matching", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_310", "file_type": "json", "from": "approach", "predicate": "outperforms", "to": "state of the art", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_310", "file_type": "json", "from": "approach", "predicate": "performs", "to": "evaluations", "type": "factual", "width": 0.78}, {"arrows": "to", "chunk_id": "doc_0_chunk_335", "file_type": "json", "from": "approach", "predicate": "involves", "to": "generating motion part candidates", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_335", "file_type": "json", "from": "approach", "predicate": "formulates", "to": "objective function", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_355", "file_type": "json", "from": "approach", "predicate": "avoids", "to": "expensive annotations", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_355", "file_type": "json", "from": "approach", "predicate": "achieves", "to": "significant improvements", "type": "factual", "width": 0.91}, {"arrows": "to", "chunk_id": "doc_0_chunk_355", "file_type": "json", "from": "approach", "predicate": "demonstrates", "to": "competitive performance", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_381", "file_type": "json", "from": "approach", "predicate": "demonstrates", "to": "effectiveness", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_402", "file_type": "json", "from": "approach", "predicate": "learns", "to": "lightness differences between pixels", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_415", "file_type": "json", "from": "approach", "predicate": "uses", "to": "depth features", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_415", "file_type": "json", "from": "approach", "predicate": "uses", "to": "RGB visual features", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_436", "file_type": "json", "from": "approach", "predicate": "has_property", "to": "high efficiency", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_436", "file_type": "json", "from": "approach", "predicate": "has_property", "to": "robust performance", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_436", "file_type": "json", "from": "approach", "predicate": "supports", "to": "varying camera-to-scene arrangements", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_436", "file_type": "json", "from": "approach", "predicate": "minimizes", "to": "mutual failures", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_436", "file_type": "json", "from": "approach", "predicate": "achieves", "to": "50 fps", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_445", "file_type": "json", "from": "approach", "predicate": "demonstrates", "to": "generalizability", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_445", "file_type": "json", "from": "approach", "predicate": "tested_on", "to": "LFW", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_470", "file_type": "json", "from": "approach", "predicate": "is", "to": "data fusion approach", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_470", "file_type": "json", "from": "approach", "predicate": "evaluates", "to": "projections reliability", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_470", "file_type": "json", "from": "approach", "predicate": "fuses", "to": "super-pixel segmentations", "type": "factual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_470", "file_type": "json", "from": "approach", "predicate": "refines", "to": "alignment of projections", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_491", "file_type": "json", "from": "approach", "predicate": "illustrated_with", "to": "Robotics (Sarcos) dataset", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_523", "file_type": "json", "from": "approach", "predicate": "incorporates", "to": "spatial features", "type": "factual", "width": 0.89}, {"arrows": "to", "chunk_id": "doc_0_chunk_523", "file_type": "json", "from": "approach", "predicate": "enables", "to": "generalization", "type": "factual", "width": 0.91}, {"arrows": "to", "chunk_id": "doc_0_chunk_528", "file_type": "json", "from": "approach", "predicate": "allows_creation_of", "to": "new structures", "type": "conceptual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_528", "file_type": "json", "from": "approach", "predicate": "demonstrates_flexibility_with", "to": "furniture design", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_528", "file_type": "json", "from": "approach", "predicate": "demonstrates_flexibility_with", "to": "archaeology", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_533", "file_type": "json", "from": "approach", "predicate": "validates_on", "to": "ILSVRC2014 dataset", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_533", "file_type": "json", "from": "approach", "predicate": "is_a", "to": "human-in-the-loop labeling approach", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_545", "file_type": "json", "from": "approach", "predicate": "achieves", "to": "highly accurate bottom-up object segmentation", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_545", "file_type": "json", "from": "approach", "predicate": "generates", "to": "set of regions", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_545", "file_type": "json", "from": "approach", "predicate": "trains", "to": "ensemble of figure-ground segmentation models", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_547", "file_type": "json", "from": "approach", "predicate": "capable of learning", "to": "model", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_569", "file_type": "json", "from": "approach", "predicate": "exploits", "to": "statistical structure", "type": "conceptual", "width": 0.82}, {"arrows": "to", "chunk_id": "doc_0_chunk_591", "file_type": "json", "from": "approach", "predicate": "deals_with", "to": "parsing humans", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_600", "file_type": "json", "from": "approach", "predicate": "evaluated on", "to": "gaze-enabled egocentric video dataset", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_612", "file_type": "json", "from": "approach", "predicate": "incorporates", "to": "visual cues", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_612", "file_type": "json", "from": "approach", "predicate": "incorporates", "to": "textual cues", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_614", "file_type": "json", "from": "approach", "predicate": "is", "to": "quantitative", "type": "factual", "width": 0.82}, {"arrows": "to", "chunk_id": "doc_0_chunk_636", "file_type": "json", "from": "approach", "predicate": "exhibits", "to": "theoretical properties", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_636", "file_type": "json", "from": "approach", "predicate": "evaluated_on", "to": "synthetic data sets", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_636", "file_type": "json", "from": "approach", "predicate": "improves_performance", "to": "state-of-the-art methods", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_636", "file_type": "json", "from": "approach", "predicate": "activates", "to": "set of cliques", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_636", "file_type": "json", "from": "approach", "predicate": "evaluated_on", "to": "real-world data sets", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_674", "file_type": "json", "from": "approach", "predicate": "incorporates", "to": "salience as prior", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_674", "file_type": "json", "from": "approach", "predicate": "utilizes", "to": "spatial edges", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_674", "file_type": "json", "from": "approach", "predicate": "utilizes", "to": "temporal motion boundaries", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_685", "file_type": "json", "from": "approach", "predicate": "presents", "to": "uni\ufb01ed saliency detection framework", "type": "conceptual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_686", "file_type": "json", "from": "approach", "predicate": "outperforms", "to": "state-of-the-art solution", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_686", "file_type": "json", "from": "approach", "predicate": "operates_on", "to": "2D, 3D and 4D data", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_689", "file_type": "json", "from": "approach", "predicate": "results_in", "to": "image labels", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_694", "file_type": "json", "from": "approach", "predicate": "uses", "to": "light path triangulation", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_694", "file_type": "json", "from": "approach", "predicate": "handles", "to": "unknown refractive indices", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_694", "file_type": "json", "from": "approach", "predicate": "suitable_for", "to": "complex transparent objects", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_694", "file_type": "json", "from": "approach", "predicate": "demonstrates", "to": "feasibility", "type": "factual", "width": 0.78}, {"arrows": "to", "chunk_id": "doc_0_chunk_703", "file_type": "json", "from": "approach", "predicate": "focuses_on", "to": "improving object detection", "type": "conceptual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_703", "file_type": "json", "from": "approach", "predicate": "identifies", "to": "representative parts", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_714", "file_type": "json", "from": "approach", "predicate": "demonstrates", "to": "superior performance", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_714", "file_type": "json", "from": "approach", "predicate": "compares_to", "to": "existing NR-IQA algorithms", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_714", "file_type": "json", "from": "approach", "predicate": "achieves", "to": "comparable results", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_715", "file_type": "json", "from": "approach", "predicate": "exhibits", "to": "generalization ability", "type": "factual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_716", "file_type": "json", "from": "approach", "predicate": "demonstrates", "to": "applicability to object segmentation", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_718", "file_type": "json", "from": "approach", "predicate": "applicable_to", "to": "object and motion segmentation", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_717", "file_type": "json", "from": "approach", "predicate": "demonstrates applicability to", "to": "general object and motion segmentation", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_718", "file_type": "json", "from": "approach", "predicate": "improves_over", "to": "unsupervised segmentation", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_717", "file_type": "json", "from": "approach", "predicate": "achieves results comparable to", "to": "best task-specific approaches", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_718", "file_type": "json", "from": "approach", "predicate": "comparable_to", "to": "task-specific approaches", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_26", "file_type": "json", "from": "Convolutional Neural Networks", "predicate": "is a", "to": "network type", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_205", "file_type": "json", "from": "Convolutional Neural Networks", "predicate": "models", "to": "salience", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_216", "file_type": "json", "from": "Convolutional Neural Networks", "predicate": "improves", "to": "Real-time Performance", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_216", "file_type": "json", "from": "Convolutional Neural Networks", "predicate": "contributes_to", "to": "Bounding Box Calibration", "type": "conceptual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_226", "file_type": "json", "from": "Convolutional Neural Networks", "predicate": "is_a", "to": "Architecture", "type": "conceptual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_26", "file_type": "json", "from": "Multiple Instance Learning", "predicate": "is a", "to": "method", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_26", "file_type": "json", "from": "Image-level Training", "predicate": "is a", "to": "training method", "type": "conceptual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_417", "file_type": "json", "from": "training method", "predicate": "applicable_to", "to": "HMMs", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_418", "file_type": "json", "from": "training method", "predicate": "suitable_for", "to": "Hidden Markov Models", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_26", "file_type": "json", "from": "Arbel\u00e1ez et al. (2009)", "predicate": "authored", "to": "Multiscale combinatorial grouping", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_548", "file_type": "json", "from": "Multiscale combinatorial grouping", "predicate": "presented_in", "to": "CVPR", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_26", "file_type": "json", "from": "Boyd \u0026 Vandenberghe (2004)", "predicate": "authored", "to": "Convex optimization", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_653", "file_type": "json", "from": "Convex optimization", "predicate": "is_publication", "to": "Cambridge University Press", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_26", "file_type": "json", "from": "Bridle (1990)", "predicate": "authored", "to": "Probabilistic interpretation of feedforward classification network outputs", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_27", "file_type": "json", "from": "Probabilistic interpretation", "predicate": "related_to", "to": "Statistical pattern recognition", "type": "conceptual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_27", "file_type": "json", "from": "Feedforward classification network outputs", "predicate": "has_interpretation", "to": "Probabilistic interpretation", "type": "conceptual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_27", "file_type": "json", "from": "Efficient graph-based image segmentation", "predicate": "published_in", "to": "International Journal of Computer Vision (IJCV)", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_27", "file_type": "json", "from": "Semantic segmentation", "predicate": "requires", "to": "Rich feature hierarchies", "type": "conceptual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_27", "file_type": "json", "from": "Simultaneous detection and segmentation", "predicate": "published_in", "to": "European Conference on Computer Vision (ECCV)", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_326", "file_type": "json", "from": "Simultaneous detection and segmentation", "predicate": "is_task", "to": "Fine-grained Localization", "type": "conceptual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_27", "file_type": "json", "from": "Graph-based image segmentation", "predicate": "is_a", "to": "Image segmentation", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_718", "file_type": "json", "from": "Image segmentation", "predicate": "achieved_by", "to": "probabilistic bottom-up aggregation", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_28", "file_type": "json", "from": "Hariharan et al.", "predicate": "presented_at", "to": "ECCV", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_311", "file_type": "json", "from": "Hariharan et al.", "predicate": "published", "to": "Discriminative decorrelation for clustering and classification", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_571", "file_type": "json", "from": "Hariharan et al.", "predicate": "authored", "to": "Hypercolumns for object segmentation and fine-grained localization", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_28", "file_type": "json", "from": "Krizhevsky et al.", "predicate": "presented_at", "to": "NIPS", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_643", "file_type": "json", "from": "Krizhevsky et al.", "predicate": "developed", "to": "deep convolutional neural networks", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_132", "file_type": "json", "from": "NIPS", "predicate": "published", "to": "Optimal teaching for limited-capacity human learners", "type": "factual", "width": 0.7}, {"arrows": "to", "chunk_id": "doc_0_chunk_513", "file_type": "json", "from": "NIPS", "predicate": "is_publication_venue", "to": "research paper", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_627", "file_type": "json", "from": "NIPS", "predicate": "publishes", "to": "Angular quantization", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_643", "file_type": "json", "from": "NIPS", "predicate": "is_conference_for", "to": "machine learning research", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_672", "file_type": "json", "from": "NIPS", "predicate": "is_conference", "to": "conference", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_28", "file_type": "json", "from": "LeCun et al.", "predicate": "published_in", "to": "Proceedings of the IEEE", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_28", "file_type": "json", "from": "Maron \u0026 Lozano-P\u00e9rez", "predicate": "presented_at", "to": "NIPS", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_28", "file_type": "json", "from": "Pedro O. Pinheiro", "predicate": "affiliated_with", "to": "Idiap Research Institute", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_29", "file_type": "json", "from": "Yeqing Li", "predicate": "author_of", "to": "Deep Sparse Representation", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_34", "file_type": "json", "from": "Yeqing Li", "predicate": "affiliated_with", "to": "University of Texas at Arlington", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_29", "file_type": "json", "from": "Deep Sparse Representation", "predicate": "presented_at", "to": "CVPR", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_29", "file_type": "json", "from": "Deep Sparse Representation", "predicate": "is_a", "to": "image registration technique", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_29", "file_type": "json", "from": "Chen Chen", "predicate": "author_of", "to": "Deep Sparse Representation", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_34", "file_type": "json", "from": "Chen Chen", "predicate": "affiliated_with", "to": "University of Texas at Arlington", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_29", "file_type": "json", "from": "Fei Yang", "predicate": "author_of", "to": "Deep Sparse Representation", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_28", "file_type": "json", "from": "Junzhou Huang", "predicate": "author_of", "to": "Deep Sparse Representation", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_29", "file_type": "json", "from": "Junzhou Huang", "predicate": "author_of", "to": "Deep Sparse representation", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_34", "file_type": "json", "from": "Junzhou Huang", "predicate": "affiliated_with", "to": "University of Texas at Arlington", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_29", "file_type": "json", "from": "Menlo Park", "predicate": "located_in", "to": "USA", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_29", "file_type": "json", "from": "Li_Deep_Sparse_Representation_2015_CVPR_paper.pdf", "predicate": "contains", "to": "Deep Sparse Representation", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_30", "file_type": "json", "from": "paper", "predicate": "proposes", "to": "similarity measure", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_35", "file_type": "json", "from": "paper", "predicate": "addresses", "to": "limitation", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_39", "file_type": "json", "from": "paper", "predicate": "proposes", "to": "new deep architecture", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_51", "file_type": "json", "from": "paper", "predicate": "authored_by", "to": "Jianping Shi", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_51", "file_type": "json", "from": "paper", "predicate": "authored_by", "to": "Li Xu", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_56", "file_type": "json", "from": "paper", "predicate": "introduces", "to": "parametrization of the trifocal tensor", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_64", "file_type": "json", "from": "paper", "predicate": "proposes", "to": "Weighted Heat Kernel Signature (W-HKS)", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_71", "file_type": "json", "from": "paper", "predicate": "addresses", "to": "Human Detection", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_71", "file_type": "json", "from": "paper", "predicate": "proposes", "to": "combination models", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_71", "file_type": "json", "from": "paper", "predicate": "introduces", "to": "HOG-III features", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_71", "file_type": "json", "from": "paper", "predicate": "introduces", "to": "weighted-NMS fusion algorithm", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_81", "file_type": "json", "from": "paper", "predicate": "addresses", "to": "fundamental matrix estimation problem", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_87", "file_type": "json", "from": "paper", "predicate": "addresses", "to": "problem of estimating and removing non-uniform motion blur", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_93", "file_type": "json", "from": "paper", "predicate": "addresses", "to": "problem", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_99", "file_type": "json", "from": "paper", "predicate": "presents", "to": "framework", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_105", "file_type": "json", "from": "paper", "predicate": "proposes", "to": "sub-categorization model", "type": "conceptual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_116", "file_type": "json", "from": "paper", "predicate": "discusses", "to": "Edge Radiance Profiles", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_129", "file_type": "json", "from": "paper", "predicate": "proposes", "to": "Interactive Machine Teaching algorithm", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_145", "file_type": "json", "from": "paper", "predicate": "proposes", "to": "unified co-saliency detection framework", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_415", "file_type": "json", "from": "paper", "predicate": "proposes", "to": "approach", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_182", "file_type": "json", "from": "paper", "predicate": "addresses", "to": "disparity refinement", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_184", "file_type": "json", "from": "paper", "predicate": "investigates", "to": "impact of parameters", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_187", "file_type": "json", "from": "paper", "predicate": "describes", "to": "depth image enhancement method", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_197", "file_type": "json", "from": "paper", "predicate": "addresses", "to": "pixel-level segmentation", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_197", "file_type": "json", "from": "paper", "predicate": "uses", "to": "Long Short Term Memory (LSTM) recurrent neural networks", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_482", "file_type": "json", "from": "paper", "predicate": "presents", "to": "algorithm", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_205", "file_type": "json", "from": "paper", "predicate": "designs", "to": "pre-training scheme", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_211", "file_type": "json", "from": "paper", "predicate": "utilizes", "to": "power factorization", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_211", "file_type": "json", "from": "paper", "predicate": "utilizes", "to": "GPCA", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_211", "file_type": "json", "from": "paper", "predicate": "influenced_by", "to": "Sparse representation", "type": "factual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_225", "file_type": "json", "from": "paper", "predicate": "investigates", "to": "CNN architectures", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_231", "file_type": "json", "from": "paper", "predicate": "addresses", "to": "unknown sparsity", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_234", "file_type": "json", "from": "paper", "predicate": "discusses", "to": "visual attributes", "type": "conceptual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_614", "file_type": "json", "from": "paper", "predicate": "appears in", "to": "CVPR", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_238", "file_type": "json", "from": "paper", "predicate": "proposes", "to": "semantic class label graph", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_238", "file_type": "json", "from": "paper", "predicate": "introduces", "to": "absorbing Markov chain process", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_243", "file_type": "json", "from": "paper", "predicate": "introduces", "to": "two principled approaches", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_256", "file_type": "json", "from": "paper", "predicate": "presents", "to": "supplementary material", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_260", "file_type": "json", "from": "paper", "predicate": "aims_to_improve", "to": "accuracy", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_260", "file_type": "json", "from": "paper", "predicate": "compares_against", "to": "KITTI dataset", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_271", "file_type": "json", "from": "paper", "predicate": "addresses_problem", "to": "coding and dictionary learning", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_280", "file_type": "json", "from": "paper", "predicate": "introduces", "to": "geodesic-preserving method", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_285", "file_type": "json", "from": "paper", "predicate": "proposes", "to": "solution", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_288", "file_type": "json", "from": "paper", "predicate": "investigates", "to": "salience model", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_580", "file_type": "json", "from": "paper", "predicate": "presents", "to": "method", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_293", "file_type": "json", "from": "paper", "predicate": "evaluated_on", "to": "KITTI benchmark", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_293", "file_type": "json", "from": "paper", "predicate": "evaluated_on", "to": "MPI Sintel benchmark", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_293", "file_type": "json", "from": "paper", "predicate": "evaluated_on", "to": "Middlebury benchmark", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_309", "file_type": "json", "from": "paper", "predicate": "authored_by", "to": "Suha Kwak", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_309", "file_type": "json", "from": "paper", "predicate": "authored_by", "to": "Cordelia Schmid", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_309", "file_type": "json", "from": "paper", "predicate": "title", "to": "Unsupervised Object Discovery", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_309", "file_type": "json", "from": "paper", "predicate": "year", "to": "2015", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_320", "file_type": "json", "from": "paper", "predicate": "addresses", "to": "generic instance search problem", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_329", "file_type": "json", "from": "paper", "predicate": "proposes", "to": "shape feature learning scheme", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_340", "file_type": "json", "from": "paper", "predicate": "addresses", "to": "challenge", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_340", "file_type": "json", "from": "paper", "predicate": "demonstrates", "to": "system\u0027s performance", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_362", "file_type": "json", "from": "paper", "predicate": "addresses", "to": "image restoration", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_362", "file_type": "json", "from": "paper", "predicate": "proposes", "to": "\u21130TV-PADMM", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_369", "file_type": "json", "from": "paper", "predicate": "introduces", "to": "BOLD (Binary Online Learned Descriptor)", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_374", "file_type": "json", "from": "paper", "predicate": "uses", "to": "multiple instance learning framework", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_421", "file_type": "json", "from": "paper", "predicate": "studies_problem", "to": "absolute pose of a perspective camera", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_430", "file_type": "json", "from": "paper", "predicate": "addresses_problem", "to": "pose estimation", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_431", "file_type": "json", "from": "paper", "predicate": "addresses", "to": "structure and motion estimation", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_444", "file_type": "json", "from": "paper", "predicate": "addresses", "to": "age invariant face recognition", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_451", "file_type": "json", "from": "paper", "predicate": "proposes", "to": "measure of salience", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_470", "file_type": "json", "from": "paper", "predicate": "proposes", "to": "geo-semantic segmentation method", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_486", "file_type": "json", "from": "paper", "predicate": "introduces", "to": "Markov Chain approach", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_486", "file_type": "json", "from": "paper", "predicate": "applies", "to": "Hierarchical approaches", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_486", "file_type": "json", "from": "paper", "predicate": "focuses_on", "to": "efficient computation", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_487", "file_type": "json", "from": "paper", "predicate": "focuses_on", "to": "salient region detection", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_487", "file_type": "json", "from": "paper", "predicate": "applies", "to": "visual salience detection", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_491", "file_type": "json", "from": "paper", "predicate": "investigates", "to": "Sparse Kernel Multi-task Learning (SKMTL) models", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_493", "file_type": "json", "from": "paper", "predicate": "argues", "to": "multiple homographies estimation", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_506", "file_type": "json", "from": "paper", "predicate": "addresses", "to": "photometric stereo", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_506", "file_type": "json", "from": "paper", "predicate": "proposes", "to": "mesh deformation approach", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_510", "file_type": "json", "from": "paper", "predicate": "proposes", "to": "deep hashing (DH) approach", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_510", "file_type": "json", "from": "paper", "predicate": "develops", "to": "deep neural network", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_532", "file_type": "json", "from": "paper", "predicate": "brings together", "to": "object detection advancements", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_532", "file_type": "json", "from": "paper", "predicate": "brings together", "to": "crowd engineering", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_555", "file_type": "json", "from": "paper", "predicate": "addresses", "to": "problem of learning long binary codes", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_555", "file_type": "json", "from": "paper", "predicate": "overcomes", "to": "lack of effective regularizer", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_555", "file_type": "json", "from": "paper", "predicate": "overcomes", "to": "high computational cost", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_555", "file_type": "json", "from": "paper", "predicate": "introduces", "to": "sparsity encouraging regularizer", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_591", "file_type": "json", "from": "paper", "predicate": "presented_by", "to": "Chen Xianjie", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_591", "file_type": "json", "from": "paper", "predicate": "authored_by", "to": "Alan Yuilie", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_596", "file_type": "json", "from": "paper", "predicate": "proposes", "to": "unified approach", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_600", "file_type": "json", "from": "paper", "predicate": "addresses", "to": "egocentric video summarization", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_614", "file_type": "json", "from": "paper", "predicate": "discusses", "to": "image alignment", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_614", "file_type": "json", "from": "paper", "predicate": "focuses on", "to": "complex system", "type": "conceptual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_616", "file_type": "json", "from": "paper", "predicate": "addresses", "to": "problem of recovering a complete 3D model", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_616", "file_type": "json", "from": "paper", "predicate": "investigates", "to": "viewpoint-based shape matching", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_616", "file_type": "json", "from": "paper", "predicate": "investigates", "to": "3D deformation", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_616", "file_type": "json", "from": "paper", "predicate": "investigates", "to": "3D mesh analysis", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_616", "file_type": "json", "from": "paper", "predicate": "investigates", "to": "3D model synthesis", "type": "factual", "width": 0.91}, {"arrows": "to", "chunk_id": "doc_0_chunk_619", "file_type": "json", "from": "paper", "predicate": "addresses", "to": "limitations", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_650", "file_type": "json", "from": "paper", "predicate": "describes", "to": "method for selecting features", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_669", "file_type": "json", "from": "paper", "predicate": "makes_contribution", "to": "analysis of impact", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_674", "file_type": "json", "from": "paper", "predicate": "introduces", "to": "unsupervised method", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_689", "file_type": "json", "from": "paper", "predicate": "presents", "to": "robust regression method", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_689", "file_type": "json", "from": "paper", "predicate": "authored_by", "to": "Hui Wu", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_689", "file_type": "json", "from": "paper", "predicate": "authored_by", "to": "Richard Souvenir", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_693", "file_type": "json", "from": "paper", "predicate": "addresses", "to": "surface shape reconstruction problem", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_698", "file_type": "json", "from": "paper", "predicate": "proposes", "to": "volumetric deformation model", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_703", "file_type": "json", "from": "paper", "predicate": "introduces", "to": "Adaptive Region Pooling", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_703", "file_type": "json", "from": "paper", "predicate": "demonstrates", "to": "effectiveness of ARP", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_713", "file_type": "json", "from": "paper", "predicate": "proposes", "to": "new no-referece (NR) image quality assessment (IQA) framework", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_716", "file_type": "json", "from": "paper", "predicate": "addresses", "to": "video segmentation", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_30", "file_type": "json", "from": "similarity measure", "predicate": "based_on", "to": "deep sparse representation", "type": "conceptual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_30", "file_type": "json", "from": "images", "predicate": "sparsified_in", "to": "gradient domain", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_30", "file_type": "json", "from": "images", "predicate": "sparsified_in", "to": "frequency domain", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_99", "file_type": "json", "from": "images", "predicate": "collected_from", "to": "4,665 KM2", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_340", "file_type": "json", "from": "images", "predicate": "have", "to": "altered gaze direction", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_631", "file_type": "json", "from": "images", "predicate": "contain", "to": "artifacts", "type": "factual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_30", "file_type": "json", "from": "limitations", "predicate": "concern", "to": "spatially-varying intensity distortions", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_300", "file_type": "json", "from": "state-of-the-art methods", "predicate": "estimates", "to": "ground-truth eye-gaze", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_31", "file_type": "json", "from": "RASL", "predicate": "addresses", "to": "spatially-varying intensity distortions", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_31", "file_type": "json", "from": "RASL", "predicate": "achieves", "to": "robustness", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_31", "file_type": "json", "from": "RASL", "predicate": "achieves", "to": "accuracy", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_31", "file_type": "json", "from": "RASL", "predicate": "achieves", "to": "efficiency", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_31", "file_type": "json", "from": "RASL", "predicate": "utilizes", "to": "sparse decomposition", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_31", "file_type": "json", "from": "RASL", "predicate": "utilizes", "to": "low-rank decomposition", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_452", "file_type": "json", "from": "accuracy", "predicate": "is_characteristic_of", "to": "state-of-the-art", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_624", "file_type": "json", "from": "accuracy", "predicate": "is", "to": "comparable", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_31", "file_type": "json", "from": "Deformable medical image registration", "predicate": "is_method_of", "to": "IEEE Transactions on Medical Imaging", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_32", "file_type": "json", "from": "IEEE Transactions on Medical Imaging", "predicate": "is_a", "to": "journal", "type": "conceptual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_32", "file_type": "json", "from": "IEEE Transactions on Medical Imaging", "predicate": "has_volume", "to": "32", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_32", "file_type": "json", "from": "IEEE Transactions on Medical Imaging", "predicate": "has_issue", "to": "7", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_31", "file_type": "json", "from": "Robust principal component analysis", "predicate": "published_in", "to": "Journal of the ACM", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_32", "file_type": "json", "from": "Journal of the ACM", "predicate": "has_volume", "to": "58", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_32", "file_type": "json", "from": "Journal of the ACM", "predicate": "has_issue", "to": "3", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_31", "file_type": "json", "from": "Deep sparse representation", "predicate": "is_type_of", "to": "Deep Learning", "type": "conceptual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_36", "file_type": "json", "from": "Deep Learning", "predicate": "surpasses", "to": "Traditional Deep Features", "type": "factual", "width": 0.82}, {"arrows": "to", "chunk_id": "doc_0_chunk_193", "file_type": "json", "from": "Deep Learning", "predicate": "utilizes", "to": "CNNs", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_376", "file_type": "json", "from": "Deep Learning", "predicate": "relates to", "to": "Convolutional deep belief networks", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_31", "file_type": "json", "from": "Intensity Distortions", "predicate": "limits", "to": "existing approaches", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_243", "file_type": "json", "from": "existing approaches", "predicate": "rely_on", "to": "predefined weights", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_266", "file_type": "json", "from": "existing approaches", "predicate": "suffer_from", "to": "computational cost", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_266", "file_type": "json", "from": "existing approaches", "predicate": "depend_on", "to": "initialization", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_266", "file_type": "json", "from": "existing approaches", "predicate": "experience", "to": "error accumulation", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_495", "file_type": "json", "from": "existing approaches", "predicate": "relies on", "to": "incomplete constraint satisfaction", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_616", "file_type": "json", "from": "existing approaches", "predicate": "require", "to": "user interaction", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_698", "file_type": "json", "from": "existing approaches", "predicate": "rely_on", "to": "surface-based strategies", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_32", "file_type": "json", "from": "journal", "predicate": "is_a", "to": "Journal of the ACM", "type": "conceptual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_32", "file_type": "json", "from": "Pattern Recognition", "predicate": "is_a", "to": "journal", "type": "conceptual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_32", "file_type": "json", "from": "Pattern Recognition", "predicate": "has_volume", "to": "35", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_32", "file_type": "json", "from": "Pattern Recognition", "predicate": "has_issue", "to": "2", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_77", "file_type": "json", "from": "Pattern Recognition", "predicate": "published", "to": "Automatic color constancy algorithm selection and combination", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_32", "file_type": "json", "from": "IEEE Transactions on Geoscience and Remote Sensing", "predicate": "is_a", "to": "journal", "type": "conceptual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_32", "file_type": "json", "from": "IEEE Transactions on Geoscience and Remote Sensing", "predicate": "has_volume", "to": "46", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_102", "file_type": "json", "from": "IEEE Transactions on Geoscience and Remote Sensing", "predicate": "published", "to": "Automatic analysis of the difference image", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_102", "file_type": "json", "from": "IEEE Transactions on Geoscience and Remote Sensing", "predicate": "published", "to": "A latent analysis of earth surface dynamic evolution", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_32", "file_type": "json", "from": "IEEE Transactions on Geoscientific and Remote Sensing", "predicate": "has_issue", "to": "5", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_32", "file_type": "json", "from": "Medical image analysis", "predicate": "is_a", "to": "journal", "type": "conceptual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_32", "file_type": "json", "from": "Medical image analysis", "predicate": "has_volume", "to": "18", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_32", "file_type": "json", "from": "Medical image analysis", "predicate": "has_issue", "to": "6", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_33", "file_type": "json", "from": "H", "predicate": "authored", "to": "Landmark matching based retinal image alignment", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_33", "file_type": "json", "from": "Maguire", "predicate": "authored", "to": "Landmark matching based retinal image alignment", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_33", "file_type": "json", "from": "Brainard", "predicate": "authored", "to": "Landmark matching based retinal image alignment", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_33", "file_type": "json", "from": "Tzimiropouulos", "predicate": "authored", "to": "Robust FFT-based scale-invariant image registration", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_33", "file_type": "json", "from": "Argyriou", "predicate": "authored", "to": "Robust FFT-based scale-invariant image registration", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_33", "file_type": "json", "from": "Zafeiriou", "predicate": "authored", "to": "Robust FFT-based scale-invariant image registration", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_595", "file_type": "json", "from": "Zafeiriou", "predicate": "authored", "to": "Unifying Holistic and Parts-Based Deformable Model Fitting", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_33", "file_type": "json", "from": "Stathaki", "predicate": "authored", "to": "Robust FFT-based scale-invariant image registration", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_33", "file_type": "json", "from": "Viola", "predicate": "authored", "to": "Alignment by maximization of mutual information", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_217", "file_type": "json", "from": "Viola", "predicate": "authored", "to": "Rapid object detection", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_33", "file_type": "json", "from": "Wells III", "predicate": "authored", "to": "Alignment by maximization of mutual information", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_33", "file_type": "json", "from": "Gross", "predicate": "authored", "to": "Multi-pie", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_33", "file_type": "json", "from": "Matthews", "predicate": "authored", "to": "Multi-pie", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_33", "file_type": "json", "from": "Cohn", "predicate": "authored", "to": "Multi-pie", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_33", "file_type": "json", "from": "Kanade", "predicate": "authored", "to": "Multi-pie", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_33", "file_type": "json", "from": "Baker", "predicate": "authored", "to": "Multi-pie", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_33", "file_type": "json", "from": "Zitova", "predicate": "authored", "to": "Image registration methods", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_33", "file_type": "json", "from": "Flusser", "predicate": "authored", "to": "Image registration methods", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_34", "file_type": "json", "from": "Tsung-Yi Lin", "predicate": "author_of", "to": "Lin_Learning_Deep_Representations_2015_CVPR_paper", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_38", "file_type": "json", "from": "Tsung-Yi Lin", "predicate": "affiliated_with", "to": "Cornell Tech", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_34", "file_type": "json", "from": "Lin_Learning_Deep_Representations_2015_CVPR_paper", "predicate": "published_in", "to": "Image and vision computing", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_34", "file_type": "json", "from": "Lin_Learning_Deep_Representations_2015_CVPR_paper", "predicate": "volume", "to": "21", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_34", "file_type": "json", "from": "Lin_Learning_Deep_Representations_2015_CVPR_paper", "predicate": "page_range", "to": "977\u20131000", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_34", "file_type": "json", "from": "Yin Cui", "predicate": "author_of", "to": "Lin_Learning_Deep_Representations_2015_CVPR_paper", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_38", "file_type": "json", "from": "Yin Cui", "predicate": "affiliated_with", "to": "Cornell Tech", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_34", "file_type": "json", "from": "Serge Belongie", "predicate": "author_of", "to": "Lin_Learning_Deep_Representations_2015_CVPR_paper", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_37", "file_type": "json", "from": "Serge Belongie", "predicate": "affiliation", "to": "Cornell Tech", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_34", "file_type": "json", "from": "James Hays", "predicate": "author_of", "to": "Lin_Learning_Deep_Representations_2015_CVPR_paper", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_38", "file_type": "json", "from": "James Hays", "predicate": "affiliated_with", "to": "Brown University", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_35", "file_type": "json", "from": "geo-tagged images", "predicate": "spurs", "to": "image-based geolocalization algorithms", "type": "causal", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_35", "file_type": "json", "from": "image-based geolocalization algorithms", "predicate": "matches", "to": "ground-level images", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_35", "file_type": "json", "from": "Where-CNN", "predicate": "is", "to": "deep learning approach", "type": "conceptual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_35", "file_type": "json", "from": "Where-CNN", "predicate": "inspired_by", "to": "face verification", "type": "conceptual", "width": 0.89}, {"arrows": "to", "chunk_id": "doc_0_chunk_397", "file_type": "json", "from": "face verification", "predicate": "is", "to": "cross-dataset task", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_35", "file_type": "json", "from": "dataset", "predicate": "contains", "to": "78K aligned cross-view image pairs", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_151", "file_type": "json", "from": "dataset", "predicate": "includes", "to": "108 crowd scenes", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_151", "file_type": "json", "from": "dataset", "predicate": "contains", "to": "nearly 200,000 head", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_152", "file_type": "json", "from": "dataset", "predicate": "has_size", "to": "200,000 head annotations", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_152", "file_type": "json", "from": "dataset", "predicate": "evaluates", "to": "cross-scene crowd counting methods", "type": "conceptual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_152", "file_type": "json", "from": "dataset", "predicate": "is", "to": "new", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_314", "file_type": "json", "from": "dataset", "predicate": "forms", "to": "pose-dependent model of joint limits", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_314", "file_type": "json", "from": "dataset", "predicate": "is_available_for", "to": "research purposes", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_520", "file_type": "json", "from": "dataset", "predicate": "is", "to": "annotated through crowdsourcing", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_727", "file_type": "json", "from": "dataset", "predicate": "serves_as", "to": "ground truth", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_727", "file_type": "json", "from": "dataset", "predicate": "complements", "to": "existing annotations", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_727", "file_type": "json", "from": "dataset", "predicate": "offers", "to": "new possibilities", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_35", "file_type": "json", "from": "matching views", "predicate": "are", "to": "close", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_35", "file_type": "json", "from": "mismatched views", "predicate": "are", "to": "far apart", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_36", "file_type": "json", "from": "Geolocalization", "predicate": "relies_on", "to": "Aerial Imagery", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_36", "file_type": "json", "from": "Cross-View Matching", "predicate": "uses", "to": "Feature Representation", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_36", "file_type": "json", "from": "Cross-View Matching", "predicate": "generalizes_to", "to": "Novel Locations", "type": "factual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_36", "file_type": "json", "from": "Deep Convolutional Neural Networks", "predicate": "used_in", "to": "Image Classification", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_376", "file_type": "json", "from": "Image Classification", "predicate": "uses", "to": "Deep Learning", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_36", "file_type": "json", "from": "Google Street View", "predicate": "captures", "to": "World", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_36", "file_type": "json", "from": "Distinctive Image Features", "predicate": "derived_from", "to": "Scale-Invariant Keypoints", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_36", "file_type": "json", "from": "Deepface", "predicate": "achieves", "to": "Human-Level Performance", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_36", "file_type": "json", "from": "Learning a Similarity Metric", "predicate": "applied_to", "to": "Face Verification", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_37", "file_type": "json", "from": "Chopra et al. (2005)", "predicate": "published_in", "to": "CVPR", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_37", "file_type": "json", "from": "Lin et al. (2013)", "predicate": "published_in", "to": "CVPR", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_37", "file_type": "json", "from": "Bansal \u0026 Daniilidis (2014)", "predicate": "published_in", "to": "CVPR", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_37", "file_type": "json", "from": "van der Maaten \u0026 Hinton (2008)", "predicate": "published_in", "to": "JMLR", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_377", "file_type": "json", "from": "JMLR", "predicate": "published", "to": "Matching words and pictures", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_37", "file_type": "json", "from": "Xiao et al. (2010)", "predicate": "published_in", "to": "CVPR", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_37", "file_type": "json", "from": "Felzenszwalb et al. (2010)", "predicate": "published_in", "to": "PAMI", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_513", "file_type": "json", "from": "PAMI", "predicate": "is_publication_venue", "to": "research paper", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_530", "file_type": "json", "from": "PAMI", "predicate": "is_publication_platform_for", "to": "minimization in vision", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_38", "file_type": "json", "from": "Cornell Tech", "predicate": "affiliated_with", "to": "Serge Belongie", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_38", "file_type": "json", "from": "Junho Yim", "predicate": "author_of", "to": "Rotating Your Face Using Multi-task Deep Neural Network", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_38", "file_type": "json", "from": "Heechul Jung", "predicate": "author_of", "to": "Rotating Your face Using Multi-task Deep Neural Network", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_40", "file_type": "json", "from": "Heechul Jung", "predicate": "member_of", "to": "School of Electrical Engineerin, KAIST", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_38", "file_type": "json", "from": "ByungIn Yoo", "predicate": "author_of", "to": "Rotating Your Face Using Multi-task Deep Neural Network", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_40", "file_type": "json", "from": "ByungIn Yoo", "predicate": "member_of", "to": "Samsung Advanced Institute of Technology", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_38", "file_type": "json", "from": "Changkyu Choi", "predicate": "author_of", "to": "Rotating Your Face Using Multi-task Deep Neural Network", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_40", "file_type": "json", "from": "Changkyu Choi", "predicate": "member_of", "to": "Samsung Advanced Institute of Technology", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_38", "file_type": "json", "from": "Dusik Park", "predicate": "author_of", "to": "Rotating Your Face Using Multi-task Deep Neural Network", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_40", "file_type": "json", "from": "Dusik Park", "predicate": "member_of", "to": "Samsung Advanced Institute of Technology", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_38", "file_type": "json", "from": "Junmo Kim", "predicate": "author_of", "to": "Rotating Your Face Using Multi-task Deep Neural Network", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_40", "file_type": "json", "from": "Junmo Kim", "predicate": "member_of", "to": "School of Electrical Engineering, KAIST", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_519", "file_type": "json", "from": "Junmo Kim", "predicate": "affiliated_with", "to": "School of Electrical Engineering", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_41", "file_type": "json", "from": "Junmo Kim", "predicate": "email", "to": "junmo.kim@kaisten.ac.kr", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_41", "file_type": "json", "from": "Junmo Kim", "predicate": "works_at", "to": "KAIST", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_514", "file_type": "json", "from": "Junmo Kim", "predicate": "is_author_of", "to": "Unsupervised Simultaneous Orthogonal Basis Clustering Feature Selection", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_515", "file_type": "json", "from": "Junmo Kim", "predicate": "affiliated_with", "to": "Department of Automation", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_38", "file_type": "json", "from": "Yim_Rotating_Your_Face_2015_CVPR_paper.pdf", "predicate": "is_a", "to": "CVPR paper", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_573", "file_type": "json", "from": "CVPR paper", "predicate": "is_a", "to": "publication", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_39", "file_type": "json", "from": "Face recognition", "predicate": "is_problem_of", "to": "viewpoint and illumination changes", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_257", "file_type": "json", "from": "Face recognition", "predicate": "evaluated_by", "to": "Feret methodology", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_39", "file_type": "json", "from": "new deep architecture", "predicate": "based_on", "to": "novel type of multitask learning", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_39", "file_type": "json", "from": "target-pose face image", "predicate": "derived_from", "to": "arbitrary pose and illumination image", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_39", "file_type": "json", "from": "target pose", "predicate": "controlled_by", "to": "user\u2019s intention", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_39", "file_type": "json", "from": "multi-task model", "predicate": "improves", "to": "identity preservation", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_39", "file_type": "json", "from": "Controlled Pose Image (CPI)", "predicate": "used_for", "to": "pose-illumination- invariant feature", "type": "factual", "width": 0.91}, {"arrows": "to", "chunk_id": "doc_0_chunk_40", "file_type": "json", "from": "MultiPIE dataset", "predicate": "is_used_for", "to": "Face Recognition", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_40", "file_type": "json", "from": "Proposed method", "predicate": "outperforms", "to": "state-of-the-art algorithms", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_40", "file_type": "json", "from": "Proposed method", "predicate": "operates_on", "to": "MultiPIE dataset", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_215", "file_type": "json", "from": "Proposed method", "predicate": "achieves", "to": "state-of-the-art detection performance", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_215", "file_type": "json", "from": "Proposed method", "predicate": "runs_at", "to": "14 FPS", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_215", "file_type": "json", "from": "Proposed method", "predicate": "runs_at", "to": "100 FPS", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_41", "file_type": "json", "from": "Ramakrishna Vedantam", "predicate": "author_of", "to": "CIDEr", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_41", "file_type": "json", "from": "CIDEr", "predicate": "is_a", "to": "Image Description Evaluation", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_41", "file_type": "json", "from": "CIDEr", "predicate": "presented_at", "to": "CVPR", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_41", "file_type": "json", "from": "CIDEr", "predicate": "year", "to": "2015", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_43", "file_type": "json", "from": "CIDEr", "predicate": "is_named", "to": "CIDEr-D", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_44", "file_type": "json", "from": "CIDEr", "predicate": "is_a", "to": "Automated Metrics", "type": "conceptual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_41", "file_type": "json", "from": "C. Lawrence Zitnick", "predicate": "author_of", "to": "CIDEr", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_44", "file_type": "json", "from": "C. Lawrence Zitnick", "predicate": "affiliated_with", "to": "Microsoft Research", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_41", "file_type": "json", "from": "Devi Parikh", "predicate": "author_of", "to": "CIDEr", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_613", "file_type": "json", "from": "Devi Parikh", "predicate": "affiliated_with", "to": "Virginia Tech", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_41", "file_type": "json", "from": "KAIST", "predicate": "has_department", "to": "School of Electrical Engineering", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_42", "file_type": "json", "from": "Image Description", "predicate": "is_challenge_in", "to": "Computer Vision", "type": "conceptual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_42", "file_type": "json", "from": "Image Description", "predicate": "is_challenge_in", "to": "Natural Language Processing", "type": "conceptual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_709", "file_type": "json", "from": "Image Description", "predicate": "related_to", "to": "Sequence Learning", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_708", "file_type": "json", "from": "Image Description", "predicate": "utilizes", "to": "perceptual representations", "type": "factual", "width": 0.78}, {"arrows": "to", "chunk_id": "doc_0_chunk_125", "file_type": "json", "from": "Computer Vision", "predicate": "studies", "to": "Hand-Object Interaction", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_193", "file_type": "json", "from": "Computer Vision", "predicate": "encompasses", "to": "Material Recognition", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_305", "file_type": "json", "from": "Computer Vision", "predicate": "includes", "to": "Salient Object Detection", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_467", "file_type": "json", "from": "Computer Vision", "predicate": "is_conference_of", "to": "IEEE", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_44", "file_type": "json", "from": "Natural Language Processing", "predicate": "is_related_to", "to": "Computer Vision", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_42", "file_type": "json", "from": "Object Detection", "predicate": "contributes_to", "to": "Image Description", "type": "conceptual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_220", "file_type": "json", "from": "Object Detection", "predicate": "field_of_study", "to": "Computer Vision", "type": "conceptual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_265", "file_type": "json", "from": "Object Detection", "predicate": "influenced_by", "to": "Convolutional Neural Networks (CNNs)", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_276", "file_type": "json", "from": "Object Detection", "predicate": "evaluated_on", "to": "PASCAL VOC 2007", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_276", "file_type": "json", "from": "Object Detection", "predicate": "evaluated_on", "to": "PASCUAL VOC 2012", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_42", "file_type": "json", "from": "Attribute Classification", "predicate": "contributes_to", "to": "Image Description", "type": "conceptual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_42", "file_type": "json", "from": "Paradigm", "predicate": "uses", "to": "Human Consensus", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_42", "file_type": "json", "from": "Paradigm", "predicate": "consists_of", "to": "Triplet-based Method", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_42", "file_type": "json", "from": "Paradigm", "predicate": "consists_of", "to": "Automated Metric", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_42", "file_type": "json", "from": "Paradigm", "predicate": "consists_of", "to": "Datasets", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_42", "file_type": "json", "from": "Datasets", "predicate": "includes", "to": "PASCAL-50S", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_42", "file_type": "json", "from": "Datasets", "predicate": "includes", "to": "ABSTRACT-50S", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_43", "file_type": "json", "from": "PASCAL-50S", "predicate": "is_dataset", "to": "dataset", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_42", "file_type": "json", "from": "Metric", "predicate": "captures", "to": "Human Judgment", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_43", "file_type": "json", "from": "nsensus", "predicate": "is_dataset", "to": "dataset", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_43", "file_type": "json", "from": "metric", "predicate": "captures", "to": "human judgment", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_43", "file_type": "json", "from": "metric", "predicate": "better_than", "to": "existing metrics", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_43", "file_type": "json", "from": "sentences", "predicate": "generated_by", "to": "various sources", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_44", "file_type": "json", "from": "CIDEr-D", "predicate": "is_part_of", "to": "MS COCO evaluation server", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_43", "file_type": "json", "from": "MS COCO evaluation server", "predicate": "enables", "to": "systematic evaluation", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_43", "file_type": "json", "from": "image description approaches", "predicate": "evaluated_by", "to": "protocol", "type": "factual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_561", "file_type": "json", "from": "Microsoft Research", "predicate": "produces", "to": "CVPR paper", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_667", "file_type": "json", "from": "Microsoft Research", "predicate": "employs", "to": "Cem Keskin", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_667", "file_type": "json", "from": "Microsoft Research", "predicate": "employs", "to": "Pushmeet Kohli", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_667", "file_type": "json", "from": "Microsoft Research", "predicate": "employs", "to": "Shahram Izadi", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_44", "file_type": "json", "from": "Zhenzhong Lan", "predicate": "is_author_of", "to": "Beyond Gaussian Pyramid", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_45", "file_type": "json", "from": "Zhenzhong Lan", "predicate": "affiliated_with", "to": "School of Computer Science, Carnegie Mellon University", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_44", "file_type": "json", "from": "Beyond Gaussian Pyramid", "predicate": "presented_at", "to": "CVPR", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_44", "file_type": "json", "from": "Alexander G. Hauptmann", "predicate": "is_author_of", "to": "Beyond Gaussian Pyramid", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_46", "file_type": "json", "from": "Alexander G. Hauptmann", "predicate": "affiliated_with", "to": "School of Computer Science, Carnegie Mellon University", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_44", "file_type": "json", "from": "Bhiksha Raj", "predicate": "is_author_of", "to": "Beyond Gaussian Pyramid", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_46", "file_type": "json", "from": "Bhiksha Raj", "predicate": "affiliated_with", "to": "School of Computer Science, Carnegie Mellon University", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_46", "file_type": "json", "from": "Bhiksha Raj", "predicate": "has_email", "to": "bhiksha@cs.cmu.edu", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_45", "file_type": "json", "from": "Lan_Beyond_Gausian_Pyramid_2015_CVPR_supplemental", "predicate": "supports_paper", "to": "Beyond Gaussian Pyramid: Multi-skip Feature Stacking for Action Recognition", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_45", "file_type": "json", "from": "Lan_Beyond_Gausian_Pyramid_2015_CVPR_supplemental", "predicate": "provides", "to": "proof of theorem 1", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_45", "file_type": "json", "from": "Lan_Beyond_Gausian_Pyramid_2015_CVPR_supplemental", "predicate": "provides", "to": "proof of theorem 2", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_45", "file_type": "json", "from": "Beyond Gaussian Pyramid: Multi-skip Feature Stacking for Action Recognition", "predicate": "addresses", "to": "Action Recognition", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_45", "file_type": "json", "from": "Beyond Gaussian Pyramid: Multi-skip Feature Stacking for Action Recognition", "predicate": "uses", "to": "Matrix Bernstein\u0027s Inequality", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_45", "file_type": "json", "from": "Action Recognition", "predicate": "relies_on", "to": "Feature Stacking", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_334", "file_type": "json", "from": "Action Recognition", "predicate": "uses", "to": "Trajectory Group Selection", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_336", "file_type": "json", "from": "Action Recognition", "predicate": "uses", "to": "Fisher Vector Representation", "type": "factual", "width": 0.78}, {"arrows": "to", "chunk_id": "doc_0_chunk_336", "file_type": "json", "from": "Action Recognition", "predicate": "improves_with", "to": "Trajectories", "type": "factual", "width": 0.87}, {"arrows": "to", "chunk_id": "doc_0_chunk_45", "file_type": "json", "from": "Ming Lin", "predicate": "affiliated_with", "to": "School of Computer Science, Carnegie Mellon University", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_45", "file_type": "json", "from": "Feature Stacking", "predicate": "impacts", "to": "Condition Number", "type": "conceptual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_46", "file_type": "json", "from": "Alexander G. Hauptman", "predicate": "has_email", "to": "cli@cs.cmu.edu", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_46", "file_type": "json", "from": "Kwang In Kim", "predicate": "author_of", "to": "Kim_Local_High-Order_Regularization_2015_CVPR_paper.pdf", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_46", "file_type": "json", "from": "James Tompkin", "predicate": "author_of", "to": "Kim_Local_High-Order_Regularization_2015_CVPR_paper.pdf", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_46", "file_type": "json", "from": "Hanspeter Pfister", "predicate": "author_of", "to": "Kim_Local_High-Order_RegularIZATION_2015_CVPR_paper.pdf", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_46", "file_type": "json", "from": "Kim_Local_High-Order_RegularIZATION_2015_CVPR_paper.pdf", "predicate": "is_paper_about", "to": "Local High-order Regularization on Data Manifolds", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_46", "file_type": "json", "from": "Christian Theobalt", "predicate": "author_of", "to": "Kim_Local_High-Order_RegularIZATION_2015_CVPR_paper.pdf", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_50", "file_type": "json", "from": "Christian Theobalt", "predicate": "affiliated_with", "to": "MPI for Informatics", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_438", "file_type": "json", "from": "Christian Theobalt", "predicate": "affiliated_with", "to": "Max Planck Institute for Informatics", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_46", "file_type": "json", "from": "Carnegie Mellon University", "predicate": "has_school", "to": "School of Computer Science", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_349", "file_type": "json", "from": "Carnegie Mellon University", "predicate": "located_in", "to": "Pittsburgh", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_677", "file_type": "json", "from": "School of Computer Science", "predicate": "part of", "to": "Beijing Institute of Technology", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_47", "file_type": "json", "from": "Graph Laplacian Regularizer", "predicate": "suffers_from", "to": "degeneracy", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_47", "file_type": "json", "from": "Iterated Graph Laplacian", "predicate": "addresses", "to": "degeneracy", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_47", "file_type": "json", "from": "Iterated Graph Laplacian", "predicate": "incurs", "to": "computational complexity", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_47", "file_type": "json", "from": "Proposed Regularizer", "predicate": "maintains", "to": "sparsity", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_47", "file_type": "json", "from": "Proposed Regularizer", "predicate": "based_on", "to": "local derivative evaluations", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_47", "file_type": "json", "from": "Approach", "predicate": "builds", "to": "manifold approximation", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_66", "file_type": "json", "from": "Approach", "predicate": "performs", "to": "Textured 3D Shape Retrieval", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_206", "file_type": "json", "from": "Approach", "predicate": "evaluated_on", "to": "public datasets", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_304", "file_type": "json", "from": "Approach", "predicate": "evaluated on", "to": "SED dataset", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_304", "file_type": "json", "from": "Approach", "predicate": "evaluated on", "to": "HKU-IS dataset", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_304", "file_type": "json", "from": "Approach", "predicate": "demonstrates", "to": "Superior Performance", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_304", "file_type": "json", "from": "Approach", "predicate": "improves", "to": "Precision", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_304", "file_type": "json", "from": "Approach", "predicate": "improves", "to": "Recall", "type": "factual", "width": 0.87}, {"arrows": "to", "chunk_id": "doc_0_chunk_304", "file_type": "json", "from": "Approach", "predicate": "improves", "to": "F-measure", "type": "factual", "width": 0.86}, {"arrows": "to", "chunk_id": "doc_0_chunk_304", "file_type": "json", "from": "Approach", "predicate": "improves", "to": "Mean Absolute Error", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_534", "file_type": "json", "from": "Approach", "predicate": "validated_on", "to": "ILSVRC2014 object detection dataset", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_534", "file_type": "json", "from": "Approach", "predicate": "deals_with", "to": "human input", "type": "conceptual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_534", "file_type": "json", "from": "Approach", "predicate": "uses", "to": "Markov Decision Process", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_551", "file_type": "json", "from": "Approach", "predicate": "uses", "to": "image representation", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_635", "file_type": "json", "from": "Approach", "predicate": "considers", "to": "Input features", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_635", "file_type": "json", "from": "Approach", "predicate": "considers", "to": "Labels", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_635", "file_type": "json", "from": "Approach", "predicate": "exhibits", "to": "Strong theoretical properties", "type": "factual", "width": 0.87}, {"arrows": "to", "chunk_id": "doc_0_chunk_635", "file_type": "json", "from": "Approach", "predicate": "improves", "to": "Performance", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_637", "file_type": "json", "from": "Approach", "predicate": "exhibits", "to": "theoretical properties", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_637", "file_type": "json", "from": "Approach", "predicate": "exhibits", "to": "performance improvement", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_47", "file_type": "json", "from": "Experiments", "predicate": "demonstrate", "to": "effectiveness", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_47", "file_type": "json", "from": "Experiments", "predicate": "demonstrate", "to": "efficiency", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_374", "file_type": "json", "from": "Experiments", "predicate": "demonstrate", "to": "performance", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_376", "file_type": "json", "from": "Experiments", "predicate": "demonstrates", "to": "convincing performance", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_389", "file_type": "json", "from": "Experiments", "predicate": "demonstrates", "to": "effectiveness of approach", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_389", "file_type": "json", "from": "Experiments", "predicate": "uses", "to": "RMRC dataset", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_439", "file_type": "json", "from": "Experiments", "predicate": "conducted_on", "to": "four large image datasets", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_625", "file_type": "json", "from": "Experiments", "predicate": "demonstrates", "to": "superior performance", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_566", "file_type": "json", "from": "Experiments", "predicate": "demonstrates", "to": "faster convergence", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_625", "file_type": "json", "from": "Experiments", "predicate": "evaluates_performance_on", "to": "large-scale datasets", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_645", "file_type": "json", "from": "Experiments", "predicate": "demonstrate", "to": "improvement", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_662", "file_type": "json", "from": "Experiments", "predicate": "demonstrates", "to": "speed-up of up to a factor of 100", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_664", "file_type": "json", "from": "Experiments", "predicate": "evaluate_on", "to": "ImageNET", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_664", "file_type": "json", "from": "Experiments", "predicate": "evaluate_on", "to": "GIST1M", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_664", "file_type": "json", "from": "Experiments", "predicate": "evaluate_on", "to": "SUN-attribute", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_47", "file_type": "json", "from": "Manifold Approximation", "predicate": "is", "to": "surrogate geometry", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_48", "file_type": "json", "from": "Manifold Approximation", "predicate": "is_related_to", "to": "Laplacian eigenmaps", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_48", "file_type": "json", "from": "Graph Laplacian Regularization", "predicate": "is_method_for", "to": "Semi-Supervised Learning", "type": "conceptual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_48", "file_type": "json", "from": "Semi-Supervised Learning", "predicate": "uses", "to": "Graph Laplacian Regularization", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_440", "file_type": "json", "from": "Laplacian eigenmaps", "predicate": "is_technique_for", "to": "dimensionality reduction", "type": "conceptual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_48", "file_type": "json", "from": "High-Order Derivatives", "predicate": "used_in", "to": "Hessian eigenmaps", "type": "factual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_690", "file_type": "json", "from": "Hessian eigenmaps", "predicate": "is_a", "to": "locally linear embedding technique", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_690", "file_type": "json", "from": "Hessian eigenmaps", "predicate": "addresses", "to": "high-dimensional data", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_48", "file_type": "json", "from": "Hessian eigenMaps", "predicate": "is_technique_for", "to": "locally linear embedding", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_671", "file_type": "json", "from": "Normalized cuts", "predicate": "used_for", "to": "image segmentation", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_213", "file_type": "json", "from": "image segmentation", "predicate": "uses", "to": "lossy data compression", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_48", "file_type": "json", "from": "Reproducing Kernel Hilbert Space (RKHS)", "predicate": "is_framework_for", "to": "Semi-Supervised Learning", "type": "conceptual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_273", "file_type": "json", "from": "Reproducing Kernel Hilbert Space (RKHS)", "predicate": "is_topic_of", "to": "cvpr_papers", "type": "conceptual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_48", "file_type": "json", "from": "Spectral Clustering", "predicate": "uses", "to": "Graph Laplacian Regularization", "type": "conceptual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_49", "file_type": "json", "from": "Spectral Clustering", "predicate": "tutorial_on", "to": "Statistics and Computing", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_210", "file_type": "json", "from": "Spectral Clustering", "predicate": "is_technique", "to": "clustering", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_48", "file_type": "json", "from": "Plug-in classifiers", "predicate": "achieves", "to": "fast learning rates", "type": "factual", "width": 0.7}, {"arrows": "to", "chunk_id": "doc_0_chunk_49", "file_type": "json", "from": "Supervised Learning", "predicate": "published_by", "to": "MIT Press", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_49", "file_type": "json", "from": "Supervised Learning", "predicate": "year", "to": "2006", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_440", "file_type": "json", "from": "Supervised Learning", "predicate": "relates_to", "to": "learning binary codes", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_670", "file_type": "json", "from": "Normalized Cuts", "predicate": "applied_to", "to": "Image Segmentation", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_210", "file_type": "json", "from": "Normalized Cuts", "predicate": "is_work_in", "to": "graph-based image segmentation", "type": "conceptual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_193", "file_type": "json", "from": "Image Segmentation", "predicate": "achieves", "to": "73.1% mean class accuracy", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_49", "file_type": "json", "from": "Real Analysis and Probability", "predicate": "published_by", "to": "Cambridge University Press", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_49", "file_type": "json", "from": "Graphs", "predicate": "related_to", "to": "Manifolds", "type": "conceptual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_49", "file_type": "json", "from": "Graph Laplacians", "predicate": "exhibits", "to": "Pointwise Consistency", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_50", "file_type": "json", "from": "Jianping Shi", "predicate": "authored", "to": "Just Noticeable Defocus Blur Detection and Estimation", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_50", "file_type": "json", "from": "Jianping Shi", "predicate": "contributed_to", "to": "CVPR", "type": "factual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_52", "file_type": "json", "from": "Jianping Shi", "predicate": "affiliated_with", "to": "The Chinese University of Hong Kong", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_50", "file_type": "json", "from": "Just Noticeable Defocus Blur Detection and Estimation", "predicate": "is_paper_of", "to": "CVPR", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_50", "file_type": "json", "from": "Li Xu", "predicate": "authored", "to": "Just Noticeable Defocus Blur Detection and Estimation", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_50", "file_type": "json", "from": "Li Xu", "predicate": "contributed_to", "to": "CVPR", "type": "factual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_52", "file_type": "json", "from": "Li Xu", "predicate": "affiliated_with", "to": "Lenovo R\u0026T", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_50", "file_type": "json", "from": "Jiaya Jia", "predicate": "authored", "to": "Just Noticeable Defocus Blur Detectio and Estimation", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_50", "file_type": "json", "from": "Jiaya Jia", "predicate": "contributed_to", "to": "CVPR", "type": "factual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_52", "file_type": "json", "from": "Jiaya Jia", "predicate": "affiliated_with", "to": "The Chinese University of Hong Kong", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_729", "file_type": "json", "from": "Jiaya Jia", "predicate": "author of", "to": "Deep LAC", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_50", "file_type": "json", "from": "Shi_Just_Noticeable_Defocus_2015_CVPR_paper.pdf", "predicate": "represents", "to": "Just Noticeable Defocus Blur Detection and Estimation", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_51", "file_type": "json", "from": "just noticeable blur", "predicate": "caused_by", "to": "defocus", "type": "causal", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_51", "file_type": "json", "from": "just noticeable blur", "predicate": "spans", "to": "small number of pixels", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_51", "file_type": "json", "from": "slight edge blurriness", "predicate": "contains", "to": "informative clues", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_51", "file_type": "json", "from": "informative clues", "predicate": "related_to", "to": "depth", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_51", "file_type": "json", "from": "blur descriptors", "predicate": "based_on", "to": "local information", "type": "factual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_51", "file_type": "json", "from": "blur feature", "predicate": "uses", "to": "sparse representation", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_51", "file_type": "json", "from": "blur feature", "predicate": "uses", "to": "image decomposition", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_51", "file_type": "json", "from": "sparse edge representation", "predicate": "corresponds_to", "to": "blur strength estimation", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_52", "file_type": "json", "from": "age decomposition", "predicate": "establishes_correspondence", "to": "sparse edge representation", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_52", "file_type": "json", "from": "feature", "predicate": "manifests", "to": "generality", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_52", "file_type": "json", "from": "feature", "predicate": "manifests", "to": "robustness", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_52", "file_type": "json", "from": "Bernt Schiele", "predicate": "author_of", "to": "Filtered Channel Features for Pedestrian Detection", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_183", "file_type": "json", "from": "Bernt Schiele", "predicate": "is_author_of", "to": "Taking a Deeper Look at Pedestrians", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_186", "file_type": "json", "from": "Bernt Schiele", "predicate": "affiliated_with", "to": "Max Planck Institute for Informatics", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_52", "file_type": "json", "from": "Filtered Channel Features for Pedestrian Detection", "predicate": "is_supplemental_to", "to": "Zhang_Filtered_Feature_Channels_2015_CVPR_supplemental.pdf", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_52", "file_type": "json", "from": "Shanshan Zhang", "predicate": "author_of", "to": "Filtered Channel Features for Pedestrian Detection", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_53", "file_type": "json", "from": "Shanshan Zhang", "predicate": "authored", "to": "Zhang_Filtered_Feature_Channels_2015_CVPR_supplemental", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_52", "file_type": "json", "from": "Rodrigo Benenson", "predicate": "author_of", "to": "Filtered Channel Features for Pedestrian Detection", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_53", "file_type": "json", "from": "Rodrigo Benenson", "predicate": "authored", "to": "Zhang_Filtered_Feature_Channels_2015_CVPR_supplemental", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_183", "file_type": "json", "from": "Rodrigo Benenson", "predicate": "is_author_of", "to": "Taking a Deeper Look at Pedestrians", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_186", "file_type": "json", "from": "Rodrigo Benenson", "predicate": "affiliated_with", "to": "Max Planck Institute for Informatics", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_53", "file_type": "json", "from": "Checkerboards4x3 model", "predicate": "is_model", "to": "pedestrian detection model", "type": "conceptual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_53", "file_type": "json", "from": "Roerei model", "predicate": "is_model", "to": "weaker model", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_53", "file_type": "json", "from": "filtered channels", "predicate": "don\u0027t alter", "to": "areas of pedestrian deemed informative", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_53", "file_type": "json", "from": "filtered channels", "predicate": "enable", "to": "extraction of discriminative information", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_53", "file_type": "json", "from": "channel U", "predicate": "used for", "to": "face", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_53", "file_type": "json", "from": "channel L", "predicate": "used for", "to": "body", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_53", "file_type": "json", "from": "gradient magnitude channel", "predicate": "used for", "to": "body", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_54", "file_type": "json", "from": "filter usage distribution", "predicate": "is_similar_across", "to": "filter bank families", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_54", "file_type": "json", "from": "filter bank families", "predicate": "influences", "to": "filter usage distribution", "type": "conceptual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_53", "file_type": "json", "from": "filter", "predicate": "used as", "to": "feature for decision tree split nodes", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_54", "file_type": "json", "from": "filter", "predicate": "used_as", "to": "decision tree split node feature", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_54", "file_type": "json", "from": "text", "predicate": "referenced_in", "to": "Benenson, R., Mathias, M., Tuytelaars, T., \u0026 Van Gools, L. (2013)", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_54", "file_type": "json", "from": "Roerei, et al.", "predicate": "related_to", "to": "filter usage", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_54", "file_type": "json", "from": "ACF", "predicate": "related_to", "to": "filter usage", "type": "conceptual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_54", "file_type": "json", "from": "decision tree split nodes", "predicate": "utilizes", "to": "filter features", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_54", "file_type": "json", "from": "filter features", "predicate": "used_in", "to": "decision tree", "type": "factual", "width": 0.87}, {"arrows": "to", "chunk_id": "doc_0_chunk_54", "file_type": "json", "from": "spatial feature distribution", "predicate": "related_to", "to": "pedestrian detection", "type": "conceptual", "width": 0.7}, {"arrows": "to", "chunk_id": "doc_0_chunk_248", "file_type": "json", "from": "pedestrian detection", "predicate": "is_precursor_to", "to": "re-identification", "type": "conceptual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_577", "file_type": "json", "from": "pedestrian detection", "predicate": "is_subject_of", "to": "research", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_578", "file_type": "json", "from": "pedestrian detection", "predicate": "is_a", "to": "image analysis task", "type": "conceptual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_56", "file_type": "json", "from": "parametrization of the trifocal tensor", "predicate": "based on", "to": "quotient Riemannian manifold", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_56", "file_type": "json", "from": "parametrization of the trifocal tensor", "predicate": "is", "to": "almost symmetric", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_56", "file_type": "json", "from": "parametrization of the trifocal tensor", "predicate": "utilizes", "to": "preferred camera", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_56", "file_type": "json", "from": "parametrization of the trifocal tensor", "predicate": "incorporated into", "to": "optimization techniques on manifolds", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_56", "file_type": "json", "from": "Riemannian structure", "predicate": "provides", "to": "notion of distance", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_56", "file_type": "json", "from": "distance between trifocal tensors", "predicate": "produces", "to": "meaningful results", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_56", "file_type": "json", "from": "distance between trifocal tensors", "predicate": "related to", "to": "Structure from Motion problem", "type": "factual", "width": 0.82}, {"arrows": "to", "chunk_id": "doc_0_chunk_56", "file_type": "json", "from": "work", "predicate": "investigates", "to": "new formulation of the trifocal tensor", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_527", "file_type": "json", "from": "work", "predicate": "addresses", "to": "challenge", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_256", "file_type": "json", "from": "work", "predicate": "derives", "to": "angular support", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_288", "file_type": "json", "from": "work", "predicate": "highlights", "to": "foundational approaches", "type": "conceptual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_290", "file_type": "json", "from": "work", "predicate": "highlights", "to": "importance", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_380", "file_type": "json", "from": "work", "predicate": "builds", "to": "object appearance model", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_380", "file_type": "json", "from": "work", "predicate": "uses", "to": "familiar objects", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_56", "file_type": "json", "from": "trifocal tensor", "predicate": "is a", "to": "tensor", "type": "conceptual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_57", "file_type": "json", "from": "Structure from Motion", "predicate": "investigates", "to": "Trifocal Tensor", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_57", "file_type": "json", "from": "Structure from Motion", "predicate": "is_field_of", "to": "Geometric Computer Vision", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_57", "file_type": "json", "from": "Trifocal Tensor", "predicate": "measured between", "to": "distances", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_57", "file_type": "json", "from": "Camera Calibration", "predicate": "related to", "to": "Geometric Computer Vision", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_422", "file_type": "json", "from": "Camera Calibration", "predicate": "related_to", "to": "Absolute Pose Estimation", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_57", "file_type": "json", "from": "Optimization Algorithms", "predicate": "written by", "to": "Absil, Mahony, and Sepulchre", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_57", "file_type": "json", "from": "Nonlinear Programming", "predicate": "written by", "to": "Bertsekas", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_57", "file_type": "json", "from": "Manopt", "predicate": "developed by", "to": "Boumal, Mishra, Absil, and Sepulchre", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_57", "file_type": "json", "from": "Lines and points", "predicate": "described in", "to": "Hartley\u0027s work", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_57", "file_type": "json", "from": "Multiple View Geometry", "predicate": "written by", "to": "Hartley and Zisserman", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_58", "file_type": "json", "from": "Hartley, R. I.", "predicate": "authored", "to": "views and the trifocal tensor", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_58", "file_type": "json", "from": "Hartley, R. I.", "predicate": "authored", "to": "Projective reconstruction from line correspondences", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_58", "file_type": "json", "from": "Hartley, R. I.", "predicate": "coauthored", "to": "Multiple View Geometry in Computer Vision", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_58", "file_type": "json", "from": "views and the trifocal tensor", "predicate": "published in", "to": "Int. J. Comput. Vision", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_58", "file_type": "json", "from": "Projective reconstruction from line correspondences", "predicate": "presented at", "to": "IEEE Conf. on Computer Vision and Pattern Recognition", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_58", "file_type": "json", "from": "Papapdoulo, T.", "predicate": "authored", "to": "A new characterization of the trifocal tensor", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_58", "file_type": "json", "from": "A new characterization of the trifocal tensor", "predicate": "presented at", "to": "European Conference on Computer Vision", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_59", "file_type": "json", "from": "European Conference on Computer Vision", "predicate": "published_in", "to": "Image and Vision Computing", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_399", "file_type": "json", "from": "European Conference on Computer Vision", "predicate": "is_conference_of", "to": "Computer Vision", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_424", "file_type": "json", "from": "European Conference on Computer Vision", "predicate": "published", "to": "Numerically stable optimization", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_58", "file_type": "json", "from": "Kendall, D. G.", "predicate": "authored", "to": "Shape Manifolds, Procustean Metrics, and Complex Projective Spaces", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_58", "file_type": "json", "from": "Shape Manifolds, Procustean Metrics, and Complex Projective Spaces", "predicate": "published in", "to": "Bulletin of the London Mathematical Society", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_58", "file_type": "json", "from": "Torr, P.", "predicate": "authored", "to": "Robust parameterization and computation of the trifocal tensor", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_59", "file_type": "json", "from": "Torr, P.", "predicate": "authored", "to": "Robust parameterization", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_58", "file_type": "json", "from": "Robust parameterization and computation of the trifocal tensor", "predicate": "published in", "to": "Image and Vision Computing", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_58", "file_type": "json", "from": "Multiple View Geometry in Computer Vision", "predicate": "published by", "to": "Cambridge University Press", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_59", "file_type": "json", "from": "Weng, J.", "predicate": "authored", "to": "Motion and structure", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_59", "file_type": "json", "from": "Grasp Laboratory", "predicate": "located_in", "to": "University of Pennsylvania", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_59", "file_type": "json", "from": "tron@seas.upenn.edu", "predicate": "email_address_of", "to": "Roberto Tron", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_59", "file_type": "json", "from": "kostas@cis.upenn.edu", "predicate": "email_address_of", "to": "Kostas Daniilidis", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_59", "file_type": "json", "from": "Fast 2D Border Ownership Assignment", "predicate": "authored_by", "to": "Cornelia Ferm\u00fcller", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_59", "file_type": "json", "from": "Fast 2D Border Ownership Assignment", "predicate": "authored_by", "to": "Ching L. Teo", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_59", "file_type": "json", "from": "Fast 2D Border Ownership Assignment", "predicate": "authored_by", "to": "Yiannis Aloimonos", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_63", "file_type": "json", "from": "Cornelia Ferm\u00fcller", "predicate": "affiliated_with", "to": "Computer Vision Lab", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_63", "file_type": "json", "from": "Ching L. Teo", "predicate": "affiliated_with", "to": "Computer Vision\u003c0xC2\u003e\u003c0xA0\u003eLab", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_59", "file_type": "json", "from": "Teo_Fast_2D_Border_2015_CVPR_paper.pdf", "predicate": "is_a", "to": "PDF document", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_60", "file_type": "json", "from": "Structured Random Forests (SRF)", "predicate": "used_for", "to": "boundary detection", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_61", "file_type": "json", "from": "Structured Random Forests (SRF)", "predicate": "related_to", "to": "Image Segmentation", "type": "conceptual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_60", "file_type": "json", "from": "shape descriptors", "predicate": "are_type_of", "to": "HoG-like descriptors", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_64", "file_type": "json", "from": "shape descriptors", "predicate": "incorporates", "to": "photometric features", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_60", "file_type": "json", "from": "spectral properties", "predicate": "analyzed_with", "to": "PCA", "type": "factual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_60", "file_type": "json", "from": "semi-global grouping cues", "predicate": "indicate", "to": "perceived depth", "type": "factual", "width": 0.82}, {"arrows": "to", "chunk_id": "doc_0_chunk_61", "file_type": "json", "from": "Experimental results", "predicate": "evaluated_on", "to": "Berkeley Segmentation Dataset", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_61", "file_type": "json", "from": "Experimental results", "predicate": "evaluated_on", "to": "NYU Depth V2 dataset", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_82", "file_type": "json", "from": "Experimental results", "predicate": "show", "to": "Method\u0027s effectiveness", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_209", "file_type": "json", "from": "Experimental results", "predicate": "demonstrate", "to": "MoG Regression outperforms subspace clustering methods", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_232", "file_type": "json", "from": "Experimental results", "predicate": "validate", "to": "proposed approach", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_397", "file_type": "json", "from": "Experimental results", "predicate": "validate", "to": "effectiveness of proposed methods", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_61", "file_type": "json", "from": "Feature Extraction", "predicate": "employs", "to": "HoG", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_61", "file_type": "json", "from": "Feature Extraction", "predicate": "employs", "to": "PCA", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_61", "file_type": "json", "from": "Cheng et al. (2014)", "predicate": "developed", "to": "Bing", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_61", "file_type": "json", "from": "Dalal \u0026 Triggs (2005)", "predicate": "introduced", "to": "Histograms of oriented gradients", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_657", "file_type": "json", "from": "Dalal \u0026 Triggs (2005)", "predicate": "introduces", "to": "HOG features", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_548", "file_type": "json", "from": "Histograms of oriented gradients", "predicate": "used_for", "to": "human detection", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_62", "file_type": "json", "from": "Binarized normed gradients", "predicate": "used_for", "to": "objectness estimation", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_62", "file_type": "json", "from": "objectness estimation", "predicate": "processed_at", "to": "300fps", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_62", "file_type": "json", "from": "Fast edge detection", "predicate": "uses", "to": "structured forests", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_62", "file_type": "json", "from": "Fast feature pyramids", "predicate": "used_for", "to": "object detection", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_387", "file_type": "json", "from": "object detection", "predicate": "faces_challenge", "to": "3D scenes", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_594", "file_type": "json", "from": "object detection", "predicate": "uses", "to": "grammar models", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_62", "file_type": "json", "from": "Category-independent object proposals", "predicate": "has_characteristic", "to": "diverse ranking", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_62", "file_type": "json", "from": "Extremely randomized trees", "predicate": "is_a", "to": "machine learning algorithm", "type": "conceptual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_62", "file_type": "json", "from": "Perceptual organization", "predicate": "used_for", "to": "recognition of indoor scenes", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_62", "file_type": "json", "from": "Random decision forests", "predicate": "is_a", "to": "machine learning algorithm", "type": "conceptual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_63", "file_type": "json", "from": "Computer Vision Lab", "predicate": "located_at", "to": "University of Maryland", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_63", "file_type": "json", "from": "University of Maryland", "predicate": "located_in", "to": "College Park, MD", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_63", "file_type": "json", "from": "Mostafa Abdelrahman", "predicate": "author_of", "to": "Heat Diffusion Over Weighted Manifolds", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_66", "file_type": "json", "from": "Mostafa Abdelrahman", "predicate": "affiliated_with", "to": "Electrical Engineering Department", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_66", "file_type": "json", "from": "Mostafa Abdelrahman", "predicate": "located_at", "to": "Assiut University", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_63", "file_type": "json", "from": "Heat Diffusion Over Weighted Manifolds", "predicate": "is_a", "to": "descriptor", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_63", "file_type": "json", "from": "Aly Farag", "predicate": "author_of", "to": "Heat Diffusion Over Weighted Manifolds", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_66", "file_type": "json", "from": "Aly Farag", "predicate": "affiliated_with", "to": "CVIP Lab", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_66", "file_type": "json", "from": "Aly Farag", "predicate": "located_at", "to": "University of Louisville", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_63", "file_type": "json", "from": "David Swanson", "predicate": "author_of", "to": "Heat Diffusion Over Weighted Manifolds", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_66", "file_type": "json", "from": "David Swanson", "predicate": "affiliated_with", "to": "Department of Mathematics", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_67", "file_type": "json", "from": "David Swanson", "predicate": "affiliated_with", "to": "University of Louisville", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_63", "file_type": "json", "from": "Moumen T. El-Melegy", "predicate": "author_of", "to": "Heat Diffusion Over Weighted Manifolds", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_67", "file_type": "json", "from": "Moumen T. El-Melegy", "predicate": "affiliated_with", "to": "Electrical Engineering Department", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_66", "file_type": "json", "from": "Moumen T. El-Melegy", "predicate": "located_at", "to": "Assiut University", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_119", "file_type": "json", "from": "descriptor", "predicate": "incorporated in", "to": "Mumford-Shah energy", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_369", "file_type": "json", "from": "descriptor", "predicate": "consists_of", "to": "binary strings", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_369", "file_type": "json", "from": "descriptor", "predicate": "uses", "to": "masked Hamming distance calculation", "type": "factual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_63", "file_type": "json", "from": "Abdelrahman_Heat_Diffusion_Over_2015_CVPR_paper.pdf", "predicate": "contains", "to": "Heat Diffusion Over Weighted Manifolds", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_64", "file_type": "json", "from": "existing descriptors", "predicate": "focus on", "to": "geometric properties", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_64", "file_type": "json", "from": "existing descriptors", "predicate": "focus on", "to": "topological properties", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_64", "file_type": "json", "from": "discretization method", "predicate": "uses", "to": "finite element approximation", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_64", "file_type": "json", "from": "weighted heat kernel signature", "predicate": "encodes", "to": "photometric information", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_64", "file_type": "json", "from": "weighted heat kernel signature", "predicate": "encodes", "to": "geometric information", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_64", "file_type": "json", "from": "weighted heat kernel signature", "predicate": "incorporates", "to": "method for scale invariance", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_65", "file_type": "json", "from": "heat kernel signature", "predicate": "encodes", "to": "photometric information", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_65", "file_type": "json", "from": "heat kernel signature", "predicate": "encodes", "to": "geometric information", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_65", "file_type": "json", "from": "challenges", "predicate": "arise from", "to": "pure geometric methods", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_65", "file_type": "json", "from": "challenges", "predicate": "arise from", "to": "pure photometric methods", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_65", "file_type": "json", "from": "experimental results", "predicate": "confirm", "to": "approach\u0027s performance", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_94", "file_type": "json", "from": "experimental results", "predicate": "demonstrate", "to": "high fidelity", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_94", "file_type": "json", "from": "experimental results", "predicate": "demonstrate", "to": "temporal consistency", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_231", "file_type": "json", "from": "experimental results", "predicate": "validates", "to": "proposed approach", "type": "factual", "width": 0.91}, {"arrows": "to", "chunk_id": "doc_0_chunk_382", "file_type": "json", "from": "experimental results", "predicate": "demonstrates", "to": "effectiveness", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_381", "file_type": "json", "from": "experimental results", "predicate": "evaluates", "to": "approach", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_382", "file_type": "json", "from": "experimental results", "predicate": "on", "to": "image datasets", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_382", "file_type": "json", "from": "experimental results", "predicate": "on", "to": "video datasets", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_65", "file_type": "json", "from": "textured shape retrieval", "predicate": "requires", "to": "approach", "type": "conceptual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_66", "file_type": "json", "from": "Challenges", "predicate": "arise_from", "to": "Pure Geometric Methods", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_66", "file_type": "json", "from": "Challenges", "predicate": "arise_from", "to": "Photometric Shape Descriptors", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_66", "file_type": "json", "from": "Weighted Heat Kernel Signature", "predicate": "related_to", "to": "Heat Diffusion on Manifold", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_67", "file_type": "json", "from": "Electrical Engineering Department", "predicate": "part_of", "to": "Assiut University", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_67", "file_type": "json", "from": "Assiut University", "predicate": "located_in", "to": "Assiut", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_67", "file_type": "json", "from": "Si Liu", "predicate": "author_of", "to": "Matching-CNN Meets KNN", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_69", "file_type": "json", "from": "Si Liu", "predicate": "affiliated_with", "to": "SKLOIs", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_67", "file_type": "json", "from": "Xiaodan Liang", "predicate": "author_of", "to": "Matching-CNN Meets KNN", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_69", "file_type": "json", "from": "Xiaodan Liang", "predicate": "affiliated_with", "to": "National University of Singapore", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_67", "file_type": "json", "from": "Luoqi Liu", "predicate": "author_of", "to": "Matching-CNN Meets KNN", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_69", "file_type": "json", "from": "Luoqi Liu", "predicate": "affiliated_with", "to": "National University of Singapore", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_67", "file_type": "json", "from": "Xiaohui Shen", "predicate": "author_of", "to": "Matching-CNN Meets KNN", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_522", "file_type": "json", "from": "Xiaohui Shen", "predicate": "affiliated_with", "to": "Adobe Research", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_214", "file_type": "json", "from": "Xiaohui Shen", "predicate": "author_of", "to": "A Convolutional Neural Network Cascade", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_519", "file_type": "json", "from": "Xiaohui Shen", "predicate": "authored", "to": "Salient Object Subitizing", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_67", "file_type": "json", "from": "Jianchao Yang", "predicate": "author_of", "to": "Matching-CNN Meets KNN", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_69", "file_type": "json", "from": "Jianchao Yang", "predicate": "affiliated_with", "to": "Adobe Research", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_67", "file_type": "json", "from": "Changshen Xu", "predicate": "author_of", "to": "Matching-CNN Meets KNN", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_69", "file_type": "json", "from": "Changshen Xu", "predicate": "affiliated_with", "to": "IA", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_67", "file_type": "json", "from": "Liang Lin", "predicate": "author_of", "to": "Matching-CNN Meets KNN", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_69", "file_type": "json", "from": "Liang Lin", "predicate": "affiliated_with", "to": "Sun Yat-sen University", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_67", "file_type": "json", "from": "Xiaochun Cao", "predicate": "author_of", "to": "Matching-CNN Meets KNN", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_69", "file_type": "json", "from": "Xiaochun Cao", "predicate": "affiliated_with", "to": "SKLOIs", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_67", "file_type": "json", "from": "Shuicheng Yan", "predicate": "author_of", "to": "Matching-CNN Meets KNN", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_70", "file_type": "json", "from": "Shuicheng Yan", "predicate": "affiliated_with", "to": "National University of Singapore", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_334", "file_type": "json", "from": "Shuicheng Yan", "predicate": "is_author_of", "to": "Motion Part Regularization", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_68", "file_type": "json", "from": "Work", "predicate": "introduces", "to": "Solution", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_68", "file_type": "json", "from": "Solution", "predicate": "addresses", "to": "Human Parsing", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_68", "file_type": "json", "from": "Quasi-parametric Model", "predicate": "leverages", "to": "KNN Framework", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_68", "file_type": "json", "from": "Quasi-parametric Model", "predicate": "incorporates", "to": "M-CNN", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_68", "file_type": "json", "from": "M-CNN", "predicate": "predicts", "to": "Matching Confidence", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_68", "file_type": "json", "from": "M-CNN", "predicate": "predicts", "to": "Displacements", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_69", "file_type": "json", "from": "M-CNN", "predicate": "uses", "to": "superpixel smoothing", "type": "conceptual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_69", "file_type": "json", "from": "M-CNN", "predicate": "fuses", "to": "matched regions", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_69", "file_type": "json", "from": "M-CNN", "predicate": "improves", "to": "performance", "type": "causal", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_68", "file_type": "json", "from": "Evaluations", "predicate": "demonstrate", "to": "Performance Gains", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_264", "file_type": "json", "from": "performance", "predicate": "affected_by", "to": "object characteristics", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_300", "file_type": "json", "from": "performance", "predicate": "compared_to", "to": "state-of-the-art methods", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_375", "file_type": "json", "from": "performance", "predicate": "is", "to": "convincing", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_624", "file_type": "json", "from": "performance", "predicate": "compared_to", "to": "existing methods", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_69", "file_type": "json", "from": "SKLOIs", "predicate": "is_part_of", "to": "IIE", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_70", "file_type": "json", "from": "IIE", "predicate": "is_affiliated_with", "to": "Chinese Academy of Sciences", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_728", "file_type": "json", "from": "National University of Singapore", "predicate": "hosts", "to": "ineering", "type": "factual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_70", "file_type": "json", "from": "OIS", "predicate": "is_affiliated_with", "to": "Chinese Academy of Sciences", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_70", "file_type": "json", "from": "Yunsheng Jiang", "predicate": "contributed_to", "to": "Combination Features and Models for Human Detection", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_70", "file_type": "json", "from": "Combination Features and Models for Human Detection", "predicate": "is_published_in", "to": "CVPR", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_70", "file_type": "json", "from": "Combination Features and Models for Human Detection", "predicate": "concerns", "to": "Human Detection", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_70", "file_type": "json", "from": "Jinwen Ma", "predicate": "is_author_of", "to": "Combination Features and Models for Human Description", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_70", "file_type": "json", "from": "Jinwen Ma", "predicate": "contributed_to", "to": "Combination Features and Models for Human Detection", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_74", "file_type": "json", "from": "Jinwen Ma", "predicate": "affiliated_with", "to": "Peking University", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_74", "file_type": "json", "from": "Jinwen Ma", "predicate": "works_in", "to": "Department of Information Science", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_70", "file_type": "json", "from": "Jiang_Combination_Features_and_2015_CVPR_paper.pdf", "predicate": "contains", "to": "Combination Features and Models for Human Detection", "type": "factual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_71", "file_type": "json", "from": "combination models", "predicate": "has_feature", "to": "complementary features", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_71", "file_type": "json", "from": "existing features", "predicate": "exhibit", "to": "biases", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_71", "file_type": "json", "from": "biases", "predicate": "limit", "to": "performance", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_71", "file_type": "json", "from": "HOG-III features", "predicate": "combines", "to": "color features", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_71", "file_type": "json", "from": "approaches", "predicate": "improve", "to": "detection performance", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_71", "file_type": "json", "from": "approaches", "predicate": "maintain", "to": "computational efficiency", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_243", "file_type": "json", "from": "approaches", "predicate": "are", "to": "flexible", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_71", "file_type": "json", "from": "experiments", "predicate": "conducted_on", "to": "PASCAL VOC datasets", "type": "factual", "width": 0.91}, {"arrows": "to", "chunk_id": "doc_0_chunk_120", "file_type": "json", "from": "experiments", "predicate": "indicate", "to": "more accurate segmentation", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_388", "file_type": "json", "from": "experiments", "predicate": "demonstrate", "to": "effectiveness", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_152", "file_type": "json", "from": "experiments", "predicate": "demonstrates", "to": "reliability", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_180", "file_type": "json", "from": "experiments", "predicate": "demonstrate", "to": "superiority in speed", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_180", "file_type": "json", "from": "experiments", "predicate": "demonstrate", "to": "competitive surface quality", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_256", "file_type": "json", "from": "experiments", "predicate": "demonstrate", "to": "effect of blur", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_256", "file_type": "json", "from": "experiments", "predicate": "provide", "to": "geometric derivations", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_274", "file_type": "json", "from": "experiments", "predicate": "performed_on", "to": "PASPAL VOC 2", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_275", "file_type": "json", "from": "experiments", "predicate": "evaluates_on", "to": "PASPAL VOC 2007", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_294", "file_type": "json", "from": "experiments", "predicate": "evaluates_on", "to": "KITTI benchmark", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_294", "file_type": "json", "from": "experiments", "predicate": "evaluates_on", "to": "MPI Sintel benchmark", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_294", "file_type": "json", "from": "experiments", "predicate": "evaluates_on", "to": "Middlebury benchmark", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_375", "file_type": "json", "from": "experiments", "predicate": "demonstrate", "to": "classification", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_375", "file_type": "json", "from": "experiments", "predicate": "demonstrate", "to": "image annotation", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_388", "file_type": "json", "from": "experiments", "predicate": "on", "to": "RMRC dataset", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_623", "file_type": "json", "from": "experiments", "predicate": "use", "to": "large-scale datasets", "type": "factual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_72", "file_type": "json", "from": "Belongie et al. (2001)", "predicate": "presented_in", "to": "IEEE Int\u0027l Conf. on Computer Vision (IC CV)", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_72", "file_type": "json", "from": "Hubel (1995)", "predicate": "authored", "to": "Eye, brain, and vision", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_72", "file_type": "json", "from": "Eye, brain, and vision", "predicate": "related_to", "to": "human detection", "type": "conceptual", "width": 0.65}, {"arrows": "to", "chunk_id": "doc_0_chunk_72", "file_type": "json", "from": "Ioffe \u0026 Forsyth (2001)", "predicate": "presented_in", "to": "IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_72", "file_type": "json", "from": "Dalal (2006)", "predicate": "authored", "to": "Finding people in images and videos", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_72", "file_type": "json", "from": "Felzenszwalb et al. (2008)", "predicate": "presented_in", "to": "IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_657", "file_type": "json", "from": "Felzenszwalb et al. (2008)", "predicate": "presents", "to": "deformable part models", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_72", "file_type": "json", "from": "HOG-III Features", "predicate": "used_for", "to": "human detection", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_72", "file_type": "json", "from": "Model Fusion", "predicate": "improves", "to": "detection performance", "type": "causal", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_72", "file_type": "json", "from": "Weighted-NMS", "predicate": "improves", "to": "detection performance", "type": "causal", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_72", "file_type": "json", "from": "Matching Shapes", "predicate": "contributes_to", "to": "human detection", "type": "conceptual", "width": 0.7}, {"arrows": "to", "chunk_id": "doc_0_chunk_73", "file_type": "json", "from": "elzenszwalb, P. F.", "predicate": "authored", "to": "A discriminatively trained, multiscale, deformable part model", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_73", "file_type": "json", "from": "Girshick, R. B.", "predicate": "authored", "to": "Rich feature hierarchies", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_368", "file_type": "json", "from": "Girshick, R. B.", "predicate": "authored", "to": "Object detection with discriminatively trained part-based models", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_73", "file_type": "json", "from": "Viola, P.", "predicate": "authored", "to": "Robust real-time face detection", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_73", "file_type": "json", "from": "Robust real-time face detection", "predicate": "published_in", "to": "International Journal of Computer Vision", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_73", "file_type": "json", "from": "Object detection grammar", "predicate": "presented_at", "to": "IEEE Int\u2019l Conf. on Computer Vision (ICCV) Workshops", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_73", "file_type": "json", "from": "Object detection with grammar models", "predicate": "presented_at", "to": "Advances in Neural Information Processing Systems", "type": "factual", "width": 0.89}, {"arrows": "to", "chunk_id": "doc_0_chunk_73", "file_type": "json", "from": "A discriminatively trained, mult scale, deformable part model", "predicate": "presented_at", "to": "IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_270", "file_type": "json", "from": "International Journal of Computer Vision", "predicate": "publishes", "to": "appearance models", "type": "conceptual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_423", "file_type": "json", "from": "International Journal of Computer Vision", "predicate": "published", "to": "polynomial equation solving", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_692", "file_type": "json", "from": "International Journal of Computer Vision", "predicate": "is_publication", "to": "journal", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_671", "file_type": "json", "from": "International Journal of Computer Vision", "predicate": "is_a", "to": "publication", "type": "conceptual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_658", "file_type": "json", "from": "Advances in Neural Information Processing Systems", "predicate": "publishes", "to": "Learning to count objects in images", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_74", "file_type": "json", "from": "Gkioxari, G.", "predicate": "writes_paper", "to": "Using k-poselets for detecting people and localizing their keypoints", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_74", "file_type": "json", "from": "Gkioxari, G.", "predicate": "co-authors_with", "to": "Hariharan, B.", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_74", "file_type": "json", "from": "Gkioxari, G.", "predicate": "co-authors_with", "to": "Malik, J.", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_326", "file_type": "json", "from": "Hariharan, B.", "predicate": "authored", "to": "Simultaneous detection and segmentation", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_74", "file_type": "json", "from": "GkioxARI, G.", "predicate": "co-authors_with", "to": "Girshick, R.", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_218", "file_type": "json", "from": "Girshick, R.", "predicate": "authored", "to": "Rich feature hierarchies for accurate object detection and semantic segmentation", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_593", "file_type": "json", "from": "Girshick, R.", "predicate": "authored", "to": "Rich feature hierarchies", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_644", "file_type": "json", "from": "Girshick, R.", "predicate": "affiliated_with", "to": "CVPR", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_74", "file_type": "json", "from": "YunshEng Jiang", "predicate": "affiliated_with", "to": "Peking University", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_74", "file_type": "json", "from": "YunshEng Jiang", "predicate": "works_in", "to": "Department of Information Science", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_74", "file_type": "json", "from": "Cheng", "predicate": "authors", "to": "Effective Learning-Based Illuminant Estimation Using Simple Features", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_74", "file_type": "json", "from": "Cheng", "predicate": "co-authors_with", "to": "Price", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_74", "file_type": "json", "from": "Cheng", "predicate": "co-authors_with", "to": "Cohen", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_74", "file_type": "json", "from": "Cheng", "predicate": "co-authors_with", "to": "Michael S. Brown", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_377", "file_type": "json", "from": "Cheng", "predicate": "authored", "to": "Bing: Binarized normed gradients", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_378", "file_type": "json", "from": "Cheng", "predicate": "authored", "to": "Binarized normed gradients", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_78", "file_type": "json", "from": "Michael S. Brown", "predicate": "affiliated_with", "to": "National University of Singapore", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_75", "file_type": "json", "from": "Illumination estimation", "predicate": "is_process_of", "to": "determining chromaticity", "type": "conceptual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_75", "file_type": "json", "from": "Illumination estimation", "predicate": "relates_to", "to": "white-balancing", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_75", "file_type": "json", "from": "computational color constancy", "predicate": "is_topic_in", "to": "computer vision", "type": "conceptual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_75", "file_type": "json", "from": "problem", "predicate": "is_nature", "to": "ill-posed", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_81", "file_type": "json", "from": "problem", "predicate": "is", "to": "nonconvex", "type": "conceptual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_81", "file_type": "json", "from": "problem", "predicate": "solved by", "to": "sequence of convex semi-de\ufb01nite programs", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_93", "file_type": "json", "from": "problem", "predicate": "is_about", "to": "temporally consistent video post-processing", "type": "factual", "width": 1.0}, {"arrows": "to", "chunk_id": "doc_0_chunk_309", "file_type": "json", "from": "problem", "predicate": "is_type_of", "to": "discovery problem", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_309", "file_type": "json", "from": "problem", "predicate": "requires", "to": "localization", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_314", "file_type": "json", "from": "problem", "predicate": "is", "to": "inherently ill-posed", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_698", "file_type": "json", "from": "problem", "predicate": "has_application", "to": "3D shape motion recovery", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_76", "file_type": "json", "from": "best results", "predicate": "reported on", "to": "modern color constancy data sets", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_75", "file_type": "json", "from": "results", "predicate": "reported_on", "to": "color constancy data sets", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_315", "file_type": "json", "from": "results", "predicate": "evaluated on", "to": "CMU mocap dataset", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_315", "file_type": "json", "from": "results", "predicate": "evaluated using", "to": "manual annotations", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_76", "file_type": "json", "from": "our approach", "predicate": "is faster than", "to": "existing learning-based methods", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_76", "file_type": "json", "from": "our approach", "predicate": "gives", "to": "best results", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_76", "file_type": "json", "from": "Forsyth, D. A.", "predicate": "authored", "to": "novel algorithm", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_582", "file_type": "json", "from": "Forsyth, D. A.", "predicate": "authored", "to": "Variable-source shading analysis", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_76", "file_type": "json", "from": "Bani\u0107, N.", "predicate": "authored", "to": "Color dog", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_76", "file_type": "json", "from": "Color dog", "predicate": "guides", "to": "global illumination estimation", "type": "conceptual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_76", "file_type": "json", "from": "Color dog", "predicate": "improves", "to": "accuracy", "type": "causal", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_76", "file_type": "json", "from": "Funt, B.", "predicate": "authored", "to": "support vector regression", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_76", "file_type": "json", "from": "support vector regression", "predicate": "estimates", "to": "illumination chromaticity", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_76", "file_type": "json", "from": "Gao, S.", "predicate": "authored", "to": "color constancy", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_77", "file_type": "json", "from": "Gao, S.", "predicate": "authored", "to": "Ef\ufb01cient color constancy with local surface re\ufb02ectance statistics", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_76", "file_type": "json", "from": "color constancy", "predicate": "uses", "to": "local surface reflectance statistics", "type": "factual", "width": 0.89}, {"arrows": "to", "chunk_id": "doc_0_chunk_504", "file_type": "json", "from": "color constancy", "predicate": "related to", "to": "specular reflection", "type": "conceptual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_76", "file_type": "json", "from": "Learning-Based Methods", "predicate": "is", "to": "slower than", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_77", "file_type": "json", "from": "Color and Imaging Conference", "predicate": "published", "to": "via support vector regression", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_77", "file_type": "json", "from": "Barnard, K.", "predicate": "authored", "to": "A comparison of computational color constancy algorithms", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_77", "file_type": "json", "from": "Barnard, K.", "predicate": "authored", "to": "A data set for color research", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_77", "file_type": "json", "from": "TIP", "predicate": "published", "to": "A comparison of computational color constancy algorithms", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_77", "file_type": "json", "from": "TIP", "predicate": "published", "to": "Improving color constancy using indoor - outdoor image classification", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_78", "file_type": "json", "from": "TIP", "predicate": "is_journal", "to": "IEEE Transactions on Image Processing", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_77", "file_type": "json", "from": "Color Research \u0026 Application", "predicate": "published", "to": "A data set for color research", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_77", "file_type": "json", "from": "Gehler, P. V.", "predicate": "authored", "to": "Bayesian color constancy revisited", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_77", "file_type": "json", "from": "Bianco, S.", "predicate": "authored", "to": "Improving color constancy using indoor - outdoor image classification", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_78", "file_type": "json", "from": "Bianco, S.", "predicate": "is_author_of", "to": "Automatic color constancy algorithm selection and combination", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_96", "file_type": "json", "from": "IEEE Transactions on Image Processing", "predicate": "has_volume", "to": "16", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_102", "file_type": "json", "from": "IEEE Transactions on Image Processing", "predicate": "published", "to": "The regularized iteratively reweighted mad method", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_587", "file_type": "json", "from": "IEEE Transactions on Image Processing", "predicate": "publishes", "to": "Locally linear regression for pose-invariant face recognition", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_78", "file_type": "json", "from": "Botev, Z.", "predicate": "is_author_of", "to": "Kernel density estimation via diffusion", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_78", "file_type": "json", "from": "The Annals of Statistics", "predicate": "is_journal", "to": "The Annals of Statistics", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_78", "file_type": "json", "from": "Dongliang Cheng", "predicate": "affiliated_with", "to": "National University of Singapore", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_522", "file_type": "json", "from": "Brian Price", "predicate": "affiliated_with", "to": "Adobe Research", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_519", "file_type": "json", "from": "Brian Price", "predicate": "authored", "to": "Salient Object Subitizing", "type": "factual", "width": 0.91}, {"arrows": "to", "chunk_id": "doc_0_chunk_78", "file_type": "json", "from": "Scott Cohen", "predicate": "affiliated_with", "to": "Adobe Research", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_78", "file_type": "json", "from": "Hui Wu", "predicate": "is_author_of", "to": "Robust Regression on Image Manifolds for Ordered Label Denoising", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_79", "file_type": "json", "from": "Hui Wu", "predicate": "affiliated_with", "to": "University of North Carolina at Charlotte", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_80", "file_type": "json", "from": "Hui Wu", "predicate": "affiliated_with", "to": "University of North Carolin", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_78", "file_type": "json", "from": "Richard Souvenir", "predicate": "is_author_of", "to": "Robust Regression on Image Manifold for Ordered Label Denoising", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_692", "file_type": "json", "from": "Richard Souvenir", "predicate": "affiliated_with", "to": "University of North Carolina at Charlotte", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_80", "file_type": "json", "from": "Richard Souvenir", "predicate": "affiliated_with", "to": "University of North Carolin", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_78", "file_type": "json", "from": "Wu_Robust_Regression_on_2015_CVPR_supplemental.pdf", "predicate": "is_file", "to": "supplemental material", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_79", "file_type": "json", "from": "Wu_Robust_Regression_on_2015_CVPR_supplemental", "predicate": "focuses_on", "to": "Robust Regression", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_690", "file_type": "json", "from": "Robust Regression", "predicate": "addresses", "to": "image labels", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_79", "file_type": "json", "from": "Wu_Robust_Reservation_on_2015_CVPR_supplemental", "predicate": "addresses", "to": "Ordered Label Denoising", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_79", "file_type": "json", "from": "Figures 1-4", "predicate": "illustrate", "to": "performance", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_79", "file_type": "json", "from": "RANSC", "predicate": "is_method", "to": "Robust Regression", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_79", "file_type": "json", "from": "K-NN", "predicate": "is_method", "to": "Robust Regression", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_79", "file_type": "json", "from": "RBFN", "predicate": "is_method", "to": "Robust Regression", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_79", "file_type": "json", "from": "SVR", "predicate": "is_method", "to": "Robust Regression", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_79", "file_type": "json", "from": "KSPCA", "predicate": "is_method", "to": "Robust Regression", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_79", "file_type": "json", "from": "H3R", "predicate": "is_method", "to": "Robust Regression", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_79", "file_type": "json", "from": "Statue Data Set", "predicate": "is_example", "to": "Image Manifolds", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_79", "file_type": "json", "from": "Face Pose Estimation", "predicate": "is_example", "to": "Image Manifolds", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_80", "file_type": "json", "from": "Y. Cheng", "predicate": "affiliated_with", "to": "University of North Carolin", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_80", "file_type": "json", "from": "Y. Cheng", "predicate": "author_of", "to": "A Convex Optimization Approach", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_85", "file_type": "json", "from": "Y. Cheng", "predicate": "affiliation", "to": "Northeastern University", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_80", "file_type": "json", "from": "J. A. Lopez", "predicate": "affiliated_with", "to": "University of North Carolin", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_85", "file_type": "json", "from": "J. A. Lopez", "predicate": "affiliation", "to": "Northeastern University", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_80", "file_type": "json", "from": "O. Camps", "predicate": "affiliated_with", "to": "University of North Carolin", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_85", "file_type": "json", "from": "O. Camps", "predicate": "affiliation", "to": "Northeastern University", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_80", "file_type": "json", "from": "M. Sznaier", "predicate": "affiliated_with", "to": "University of North Carolin", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_85", "file_type": "json", "from": "M. Sznaier", "predicate": "affiliation", "to": "Northeastern University", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_80", "file_type": "json", "from": "Cheng_A_Convex_Optimization_2015_CVPR_paper", "predicate": "is_a", "to": "CVPR paper", "type": "conceptual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_80", "file_type": "json", "from": "hwu13@uncc.edu", "predicate": "email_address_of", "to": "Hui Wu", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_80", "file_type": "json", "from": "souvenir@uncc.edu", "predicate": "email_address_of", "to": "Richard Souvenir", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_81", "file_type": "json", "from": "framework", "predicate": "is", "to": "general nonconvex", "type": "conceptual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_81", "file_type": "json", "from": "framework", "predicate": "accounts for", "to": "rank-2 constraint", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_81", "file_type": "json", "from": "framework", "predicate": "accounts for", "to": "noise", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_99", "file_type": "json", "from": "framework", "predicate": "is_for", "to": "damage detection", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_99", "file_type": "json", "from": "framework", "predicate": "is_type_of", "to": "semi-supervised learning", "type": "conceptual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_99", "file_type": "json", "from": "framework", "predicate": "uses", "to": "hierarchical shape features", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_145", "file_type": "json", "from": "framework", "predicate": "introduces", "to": "novel insights", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_145", "file_type": "json", "from": "framework", "predicate": "uses", "to": "convolutional neural network", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_145", "file_type": "json", "from": "framework", "predicate": "exploits", "to": "visually similar neighbors", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_146", "file_type": "json", "from": "framework", "predicate": "explores", "to": "deep information", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_146", "file_type": "json", "from": "framework", "predicate": "explores", "to": "wide information", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_243", "file_type": "json", "from": "framework", "predicate": "is_built_on", "to": "multiple visual features", "type": "factual", "width": 0.91}, {"arrows": "to", "chunk_id": "doc_0_chunk_243", "file_type": "json", "from": "framework", "predicate": "outperforms", "to": "state-of-the-art methods", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_374", "file_type": "json", "from": "framework", "predicate": "learns", "to": "relationship", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_375", "file_type": "json", "from": "framework", "predicate": "extracts", "to": "region-keyword pairs", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_375", "file_type": "json", "from": "framework", "predicate": "demonstrates", "to": "performance", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_388", "file_type": "json", "from": "framework", "predicate": "utilizes", "to": "labeled 2D samples", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_387", "file_type": "json", "from": "framework", "predicate": "uses", "to": "3D CAD models", "type": "factual", "width": 0.89}, {"arrows": "to", "chunk_id": "doc_0_chunk_388", "file_type": "json", "from": "framework", "predicate": "overcomes", "to": "lack of 3D training data", "type": "causal", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_435", "file_type": "json", "from": "framework", "predicate": "combines", "to": "model-based generative tracking", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_435", "file_type": "json", "from": "framework", "predicate": "combines", "to": "discriminative hand pose detection", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_445", "file_type": "json", "from": "framework", "predicate": "confirmed_by", "to": "experimentation", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_445", "file_type": "json", "from": "framework", "predicate": "has_property", "to": "effectiveness", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_476", "file_type": "json", "from": "framework", "predicate": "believed to be", "to": "extensible to other range-weighted filters", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_477", "file_type": "json", "from": "framework", "predicate": "believed_to_be_extensible_to", "to": "range-weighted algorithms", "type": "conceptual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_477", "file_type": "json", "from": "framework", "predicate": "addresses", "to": "noisy images", "type": "factual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_491", "file_type": "json", "from": "framework", "predicate": "can_recover", "to": "clustered structures", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_614", "file_type": "json", "from": "framework", "predicate": "defines", "to": "solutions", "type": "conceptual", "width": 0.7}, {"arrows": "to", "chunk_id": "doc_0_chunk_655", "file_type": "json", "from": "framework", "predicate": "addresses", "to": "partially-occluded small instances", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_685", "file_type": "json", "from": "framework", "predicate": "handles", "to": "heterogenous types of input data", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_713", "file_type": "json", "from": "framework", "predicate": "combines", "to": "features", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_713", "file_type": "json", "from": "framework", "predicate": "evaluates_on", "to": "LIVE dataset", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_713", "file_type": "json", "from": "framework", "predicate": "demonstrates", "to": "superior performance", "type": "factual", "width": 0.89}, {"arrows": "to", "chunk_id": "doc_0_chunk_713", "file_type": "json", "from": "framework", "predicate": "achieves", "to": "comparable results", "type": "factual", "width": 0.87}, {"arrows": "to", "chunk_id": "doc_0_chunk_81", "file_type": "json", "from": "algorithm", "predicate": "is", "to": "extensible", "type": "conceptual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_81", "file_type": "json", "from": "algorithm", "predicate": "handles", "to": "partially labeled correspondences", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_81", "file_type": "json", "from": "algorithm", "predicate": "leverages", "to": "co-occurrence information", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_129", "file_type": "json", "from": "algorithm", "predicate": "enables", "to": "computer", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_129", "file_type": "json", "from": "algorithm", "predicate": "chooses", "to": "labeled images", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_129", "file_type": "json", "from": "algorithm", "predicate": "is_type_of", "to": "Adaptive Algorithms", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_134", "file_type": "json", "from": "algorithm", "predicate": "proposes_solution_for", "to": "global maximization of consensus", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_134", "file_type": "json", "from": "algorithm", "predicate": "frames_problem_as", "to": "tree search problem", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_134", "file_type": "json", "from": "algorithm", "predicate": "utilizes", "to": "A* search", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_134", "file_type": "json", "from": "algorithm", "predicate": "achieves", "to": "orders of magnitude faster performance", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_134", "file_type": "json", "from": "algorithm", "predicate": "improves_performance_compared_to", "to": "previous exact methods", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_199", "file_type": "json", "from": "algorithm", "predicate": "approximates", "to": "energy", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_199", "file_type": "json", "from": "algorithm", "predicate": "leverages", "to": "multi-label graph cut algorithm", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_199", "file_type": "json", "from": "algorithm", "predicate": "inspired_by", "to": "Iterively Reweighted Least Squares (IRLS) algorithm", "type": "conceptual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_199", "file_type": "json", "from": "algorithm", "predicate": "demonstrates_effectiveness_on", "to": "stereo correspondence estimation", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_199", "file_type": "json", "from": "algorithm", "predicate": "demonstrates_effectiveness_on", "to": "image inpainting problems", "type": "factual", "width": 0.91}, {"arrows": "to", "chunk_id": "doc_0_chunk_199", "file_type": "json", "from": "algorithm", "predicate": "outperforms", "to": "graph-cut-based algorithms", "type": "factual", "width": 0.89}, {"arrows": "to", "chunk_id": "doc_0_chunk_199", "file_type": "json", "from": "algorithm", "predicate": "yields", "to": "lower energy values", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_221", "file_type": "json", "from": "algorithm", "predicate": "is_algorithm_for", "to": "salient object detection", "type": "conceptual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_435", "file_type": "json", "from": "algorithm", "predicate": "uses", "to": "detection-guided optimization strategy", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_435", "file_type": "json", "from": "algorithm", "predicate": "increases", "to": "robustness", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_435", "file_type": "json", "from": "algorithm", "predicate": "increases", "to": "speed", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_453", "file_type": "json", "from": "algorithm", "predicate": "finds", "to": "region-of-interest", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_465", "file_type": "json", "from": "algorithm", "predicate": "extracts", "to": "objects", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_465", "file_type": "json", "from": "algorithm", "predicate": "leverages", "to": "eye tracking data", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_465", "file_type": "json", "from": "algorithm", "predicate": "uses", "to": "spatio-temporal mixed graph", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_465", "file_type": "json", "from": "algorithm", "predicate": "uses", "to": "binary linear integer programming", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_482", "file_type": "json", "from": "algorithm", "predicate": "integrates", "to": "local estimation", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_482", "file_type": "json", "from": "algorithm", "predicate": "integrates", "to": "global search", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_483", "file_type": "json", "from": "algorithm", "predicate": "performs", "to": "state-of-the-art methods", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_527", "file_type": "json", "from": "algorithm", "predicate": "combines", "to": "strengths", "type": "conceptual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_527", "file_type": "json", "from": "algorithm", "predicate": "leverages", "to": "dominant orientations", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_527", "file_type": "json", "from": "algorithm", "predicate": "guides", "to": "interpretation", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_527", "file_type": "json", "from": "algorithm", "predicate": "assigns", "to": "polygon", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_527", "file_type": "json", "from": "algorithm", "predicate": "allows for", "to": "creation", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_565", "file_type": "json", "from": "algorithm", "predicate": "uses", "to": "geometrically motivated criterion", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_565", "file_type": "json", "from": "algorithm", "predicate": "demonstrates", "to": "faster convergence", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_565", "file_type": "json", "from": "algorithm", "predicate": "reduces", "to": "total runtime", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_565", "file_type": "json", "from": "algorithm", "predicate": "uses", "to": "block-coordinate Frank-Wolfe (BCFW) algorithm", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_574", "file_type": "json", "from": "algorithm", "predicate": "uses", "to": "Gaussian Mixture Model", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_602", "file_type": "json", "from": "algorithm", "predicate": "addresses", "to": "submodular maximization", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_689", "file_type": "json", "from": "algorithm", "predicate": "addresses", "to": "mis-labeled examples", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_81", "file_type": "json", "from": "optimization", "predicate": "is", "to": "robust", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_82", "file_type": "json", "from": "Multiple view geometry", "predicate": "is_foundational_to", "to": "topic", "type": "conceptual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_82", "file_type": "json", "from": "Multiple view geometry", "predicate": "relates_to", "to": "computer vision", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_201", "file_type": "json", "from": "Multiple view geometry", "predicate": "is_reference_for", "to": "geometric relationships", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_82", "file_type": "json", "from": "Hartley, R. \u0026 Zisserman, A.", "predicate": "authored", "to": "Multiple view geometry", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_82", "file_type": "json", "from": "Mohan, K. \u0026 Fazel, M.", "predicate": "authored", "to": "Iterative reweighted algorithms", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_82", "file_type": "json", "from": "Lasserre, J. B.", "predicate": "authored", "to": "Global optimization with polynomials", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_82", "file_type": "json", "from": "Global optimization with polynomials", "predicate": "important_for", "to": "polynomial optimization methods", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_679", "file_type": "json", "from": "outliers", "predicate": "arise_from", "to": "background clutter", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_82", "file_type": "json", "from": "Fundamental Matrix Estimation", "predicate": "is_a", "to": "topic", "type": "conceptual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_82", "file_type": "json", "from": "Robust Optimization", "predicate": "is_a", "to": "topic", "type": "conceptual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_82", "file_type": "json", "from": "Rank-Constrained Optimization", "predicate": "is_a", "to": "topic", "type": "conceptual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_83", "file_type": "json", "from": "Global optimization", "predicate": "uses", "to": "polynomial methods", "type": "conceptual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_83", "file_type": "json", "from": "polynomial methods", "predicate": "relevant to", "to": "optimization methods", "type": "conceptual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_83", "file_type": "json", "from": "Lasserre", "predicate": "authored", "to": "Global optimization with polynomials and the problem of moments", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_83", "file_type": "json", "from": "Bugarin et al.", "predicate": "compared", "to": "polynomial global optimization", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_83", "file_type": "json", "from": "polynomial global optimization", "predicate": "is an alternative to", "to": "eight-point algorithm", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_83", "file_type": "json", "from": "fundamental matrix estimation", "predicate": "uses", "to": "polynomial global optimization", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_83", "file_type": "json", "from": "fundamental matrix estimation", "predicate": "uses", "to": "eight-point algorithm", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_83", "file_type": "json", "from": "fundamental matrix estimation", "predicate": "is a type of", "to": "computer vision technique", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_84", "file_type": "json", "from": "fundamental matrix estimation", "predicate": "is_topic_of", "to": "International journal of computer vision", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_84", "file_type": "json", "from": "fundamental matrix estimation", "predicate": "is_topic_of", "to": "Computer Vision and Pattern Recognition (CVPR)", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_84", "file_type": "json", "from": "fundamental matrix estimation", "predicate": "uses", "to": "Mlesac", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_84", "file_type": "json", "from": "fundamental matrix estimation", "predicate": "is_studied_in", "to": "Computer Vision and Image Understanding", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_84", "file_type": "json", "from": "fundamental matrix estimation", "predicate": "requires", "to": "algorithm", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_84", "file_type": "json", "from": "eight-point algorithm", "predicate": "is_a", "to": "fundamental matrix estimation method", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_83", "file_type": "json", "from": "Torr \u0026 Murray", "predicate": "studied", "to": "fundamental matrix estimation methods", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_203", "file_type": "json", "from": "International journal of computer vision", "predicate": "publishes", "to": "taxonomies", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_203", "file_type": "json", "from": "Computer Vision and Pattern Recognition (CVPR)", "predicate": "presents", "to": "comparative study", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_595", "file_type": "json", "from": "Computer Vision and Pattern Recognition (CVPR)", "predicate": "is_publication_venue_for", "to": "Histograms of oriented gradients", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_84", "file_type": "json", "from": "Mlesac", "predicate": "is_a", "to": "robust estimator", "type": "conceptual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_84", "file_type": "json", "from": "SDP relaxations", "predicate": "is_topic_of", "to": "SIAM Journal on Optimization", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_85", "file_type": "json", "from": "SDP relaxations", "predicate": "used in", "to": "polynomial optimization", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_84", "file_type": "json", "from": "Zheng, Y., Sugimoto, S., \u0026 Okutomi, M.", "predicate": "developed", "to": "eight-point algorithm", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_85", "file_type": "json", "from": "Lasserre (2006)", "predicate": "discusses", "to": "SDP relaxations", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_85", "file_type": "json", "from": "Sugaya \u0026 Kanatani (2007)", "predicate": "focuses on", "to": "high-accuracy computation", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_86", "file_type": "json", "from": "Northeastern University", "predicate": "located_in", "to": "Boston", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_85", "file_type": "json", "from": "RANSA", "predicate": "applied to", "to": "image analysis", "type": "factual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_429", "file_type": "json", "from": "RANSA", "predicate": "is_technique_for", "to": "model fitting", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_429", "file_type": "json", "from": "RANSA", "predicate": "is_relevant_to", "to": "computer vision problems", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_429", "file_type": "json", "from": "RANSA", "predicate": "improves", "to": "number of inliers", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_86", "file_type": "json", "from": "University", "predicate": "located_in", "to": "Boston", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_86", "file_type": "json", "from": "Boston", "predicate": "is_located_in", "to": "Massachusetts", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_86", "file_type": "json", "from": "Jian Sun", "predicate": "is_author_of", "to": "Learning a Convolutional Neural Network", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_224", "file_type": "json", "from": "Jian Sun", "predicate": "co-authored", "to": "Convolutional Neural Networks at Constrained Time Cost", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_225", "file_type": "json", "from": "Jian Sun", "predicate": "is author of", "to": "paper", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_560", "file_type": "json", "from": "Jian Sun", "predicate": "affiliated_with", "to": "Microsoft Research", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_279", "file_type": "json", "from": "Jian Sun", "predicate": "is_author_of", "to": "A Geodesic-Preserving Method for Image Warping", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_553", "file_type": "json", "from": "Jian Sun", "predicate": "author_of", "to": "Sparse Projections for High-Dimensional Binary Codes", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_86", "file_type": "json", "from": "Learning a Convolutional Neural Network", "predicate": "is_published_in", "to": "CVPR", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_86", "file_type": "json", "from": "Learning a Convolutional Neural Network", "predicate": "addresses", "to": "Non-uniform Motion Blur Removal", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_86", "file_type": "json", "from": "Wenfei Cao", "predicate": "is_author_of", "to": "Learning a Convolutional Neural Network", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_86", "file_type": "json", "from": "Jean Ponce", "predicate": "is_author_of", "to": "Learning a Convolutional Neural Network", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_307", "file_type": "json", "from": "Jean Ponce", "predicate": "is_author_of", "to": "Unsupervised Object Detection and Localization", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_308", "file_type": "json", "from": "Jean Ponce", "predicate": "is_author_of", "to": "Unsupervised Object Discovery and Localization in the Wild", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_313", "file_type": "json", "from": "Jean Ponce", "predicate": "affiliated_with", "to": "\u00c9cole Normale Sup\u00e9rieure", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_313", "file_type": "json", "from": "Jean Ponce", "predicate": "affiliated_with", "to": "PSL Research University", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_86", "file_type": "json", "from": "lopez.jo@husky.neu.edu", "predicate": "associated_with", "to": "Northeastern University", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_86", "file_type": "json", "from": "camps@coe.neu.edu", "predicate": "associated_with", "to": "Northeastern University", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_86", "file_type": "json", "from": "msznaier@coe.neu.edu", "predicate": "associated_with", "to": "Northeastern University", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_87", "file_type": "json", "from": "authors", "predicate": "propose", "to": "deep learning approach", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_271", "file_type": "json", "from": "authors", "predicate": "propose_to_utilize", "to": "kernels", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_271", "file_type": "json", "from": "authors", "predicate": "introduce", "to": "general Riemannian coding framework", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_280", "file_type": "json", "from": "authors", "predicate": "solve", "to": "energy function", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_280", "file_type": "json", "from": "authors", "predicate": "uses", "to": "Gauss-Newton method", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_619", "file_type": "json", "from": "authors", "predicate": "propose", "to": "novel approach", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_87", "file_type": "json", "from": "CNN", "predicate": "uses", "to": "convolutional neural network", "type": "conceptual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_274", "file_type": "json", "from": "CNN", "predicate": "trained_with", "to": "structured loss", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_87", "file_type": "json", "from": "motion kernels", "predicate": "extended_by", "to": "image rotations", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_87", "file_type": "json", "from": "Markov random field model", "predicate": "used_for", "to": "inferring dense non-uniform motion blur field", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_87", "file_type": "json", "from": "Markov random field model", "predicate": "enforces", "to": "motion smoothness", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_451", "file_type": "json", "from": "Markov random field model", "predicate": "accounts_for", "to": "global salience effects", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_451", "file_type": "json", "from": "Markov random field model", "predicate": "achieves", "to": "state-of-the-art accuracy", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_87", "file_type": "json", "from": "deblurring model", "predicate": "removes", "to": "motion blur", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_88", "file_type": "json", "from": "deblurring model", "predicate": "uses", "to": "patch-level image prior", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_88", "file_type": "json", "from": "deblurring model", "predicate": "is_type_of", "to": "non-uniform model", "type": "conceptual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_88", "file_type": "json", "from": "deblurring model", "predicate": "improves", "to": "image quality", "type": "factual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_88", "file_type": "json", "from": "motion blur", "predicate": "is_type_of", "to": "image artifact", "type": "conceptual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_88", "file_type": "json", "from": "patch-level image prior", "predicate": "is_related_to", "to": "patch-based image processing", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_88", "file_type": "json", "from": "ion blur", "predicate": "removed_by", "to": "deblurring model", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_89", "file_type": "json", "from": "Object detection systems", "predicate": "requires", "to": "large number of classes", "type": "conceptual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_274", "file_type": "json", "from": "Object detection systems", "predicate": "based_on", "to": "deep convolutional neural network (CNN)", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_89", "file_type": "json", "from": "extensive convolution operations", "predicate": "causes", "to": "long detection times", "type": "causal", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_89", "file_type": "json", "from": "sparse coding methods", "predicate": "aims_to_reduce", "to": "computational complexity", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_89", "file_type": "json", "from": "sparse coding methods", "predicate": "compromises", "to": "accuracy", "type": "causal", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_89", "file_type": "json", "from": "Regularized Sparse Coding", "predicate": "uses", "to": "filter functionality reconstruction", "type": "conceptual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_89", "file_type": "json", "from": "Regularized Sparse Coding", "predicate": "minimizes", "to": "score map error", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_89", "file_type": "json", "from": "Regularized Sparse Coding", "predicate": "achieves", "to": "16x speedup", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_89", "file_type": "json", "from": "Regularized Sparse Coding", "predicate": "results_in", "to": "0.04 mAP drop", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_89", "file_type": "json", "from": "Regularized Sparse Coding", "predicate": "demonstrates_applicability_for", "to": "parallel computing", "type": "factual", "width": 0.91}, {"arrows": "to", "chunk_id": "doc_0_chunk_89", "file_type": "json", "from": "ILSVIRC 2013", "predicate": "evaluates", "to": "Regularized Sparse Coding", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_89", "file_type": "json", "from": "Deformable Part Model", "predicate": "compared_to", "to": "Regularized Sparse Coding", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_90", "file_type": "json", "from": "parallel computing", "predicate": "occurs_on", "to": "GPUs", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_90", "file_type": "json", "from": "Ting-Hsuan Chao", "predicate": "affiliated_with", "to": "National Taiwan University", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_90", "file_type": "json", "from": "Yen-Liang Lin", "predicate": "affiliated_with", "to": "National Taiwan University", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_90", "file_type": "json", "from": "Yin-Hsi Kuo", "predicate": "affiliated_with", "to": "National Taiwan University", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_90", "file_type": "json", "from": "Winston H. Hsu", "predicate": "affiliated_with", "to": "National Taiwan University", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_90", "file_type": "json", "from": "Xiao-Ming Wu", "predicate": "affiliated_with", "to": "author", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_667", "file_type": "json", "from": "Xiao-Ming Wu", "predicate": "authored", "to": "New Insights into Laplacian Similarity Search", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_673", "file_type": "json", "from": "Xiao-Ming Wu", "predicate": "affiliation", "to": "Department of Electrical Engineering, Columbia University", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_673", "file_type": "json", "from": "Xiao-Ming Wu", "predicate": "has_email", "to": "xmwu@ee.columbia.edu", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_90", "file_type": "json", "from": "Zhenguo Li", "predicate": "affiliated_with", "to": "author", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_673", "file_type": "json", "from": "Zhenguo Li", "predicate": "affiliation", "to": "Huawei Noah\u2019s Ark Lab, Hong Kong", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_673", "file_type": "json", "from": "Zhenguo Li", "predicate": "has_email", "to": "li.zhenguo@huawei.com", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_667", "file_type": "json", "from": "Zhenguo Li", "predicate": "authored", "to": "New Insights into Laplacian Similarity Research", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_90", "file_type": "json", "from": "Shih-Fu Chang", "predicate": "affiliated_with", "to": "author", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_673", "file_type": "json", "from": "Shih-Fu Chang", "predicate": "has_email", "to": "sfchang@ee.columbia.edu", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_319", "file_type": "json", "from": "Shih-Fu Chang", "predicate": "author_of", "to": "Attributes and Categories for Generic Instance Search from One Example", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_320", "file_type": "json", "from": "Shih-Fu Chang", "predicate": "is_author_of", "to": "paper", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_667", "file_type": "json", "from": "Shih-Fu Chang", "predicate": "authored", "to": "New Insights into Laplacian Similarity Search", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_91", "file_type": "json", "from": "X.-M. Wu", "predicate": "affiliated_with", "to": "Department of Electrical Engineering, Columbia University", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_91", "file_type": "json", "from": "X.-M. Wu", "predicate": "email", "to": "xmwu@ee.columbia.edu", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_91", "file_type": "json", "from": "X.-M. Wu", "predicate": "co-authored", "to": "Analyzing the harmonic structure in graph-based learning", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_91", "file_type": "json", "from": "X.-M. Wu", "predicate": "co-authored", "to": "New insights into laplacian similarity search", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_91", "file_type": "json", "from": "X.-M. Wu", "predicate": "co-authored_with", "to": "Zhenguo Li", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_91", "file_type": "json", "from": "X.-M. Wu", "predicate": "co-authored_with", "to": "Shih-Fu Chang", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_92", "file_type": "json", "from": "sfchang@ee.columbia.edu", "predicate": "affiliated_with", "to": "Columbia University", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_672", "file_type": "json", "from": "Analyzing the harmonic structure in graph-based learning", "predicate": "published_in", "to": "NIPS", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_672", "file_type": "json", "from": "Analyzing the harmonic structure in graph-based learning", "predicate": "deals_with", "to": "harmonic structure", "type": "conceptual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_92", "file_type": "json", "from": "Xuan Dong", "predicate": "author_of", "to": "Region-based Temporally Consistent Video Post-processing", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_98", "file_type": "json", "from": "Xuan Dong", "predicate": "affiliated_with", "to": "Tsinghua University", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_91", "file_type": "json", "from": "Region-based Temporally Consistent Video Post-processing", "predicate": "authored_by", "to": "Boyan Bonev", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_91", "file_type": "json", "from": "Region-based Temporally Consistent Video Post-processing", "predicate": "authored_by", "to": "Alan L. Yuille", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_92", "file_type": "json", "from": "Region-based Temporally Consistent Video Post-processing", "predicate": "presented_at", "to": "CVPR", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_92", "file_type": "json", "from": "Region-based Temporally Consistent Video Post-processing", "predicate": "year", "to": "2015", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_92", "file_type": "json", "from": "Region-based Temporally Consistent Video Post-processing", "predicate": "file_name", "to": "Dong_Region-Based_Temporally_Consistent_2015_CVPR_paper.pdf", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_92", "file_type": "json", "from": "Boyan Bonev", "predicate": "author_of", "to": "Region-based Temporically Consistent Video Post-processing", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_98", "file_type": "json", "from": "Boyan Bonev", "predicate": "affiliated_with", "to": "UC Los Angeles", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_91", "file_type": "json", "from": "Region-based Temporately Consistent Video Post-processing", "predicate": "authored_by", "to": "Yu Zhu", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_92", "file_type": "json", "from": "Yu Zhu", "predicate": "author_of", "to": "Region-based Temporally Consistent Video Post-processing", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_98", "file_type": "json", "from": "Yu Zhu", "predicate": "affiliated_with", "to": "Northwestern Polytechnical University", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_92", "file_type": "json", "from": "Alan L. Yuille", "predicate": "author_of", "to": "Region-based Temporally Consistent Video Post-processing", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_98", "file_type": "json", "from": "Alan L. Yuille", "predicate": "affiliated_with", "to": "UC Los Angeles", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_92", "file_type": "json", "from": "Columbia University", "predicate": "has_department", "to": "Department of Electrical Engineering", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_672", "file_type": "json", "from": "Department of Electrical Engineering", "predicate": "is_department_of", "to": "Columbia University", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_93", "file_type": "json", "from": "goal", "predicate": "aims_for", "to": "fidelity", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_471", "file_type": "json", "from": "goal", "predicate": "is_to_create", "to": "semantically segmented images", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_532", "file_type": "json", "from": "goal", "predicate": "is", "to": "localizing every object in an image", "type": "conceptual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_616", "file_type": "json", "from": "goal", "predicate": "is to", "to": "reconstruct a 3D model automatically", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_93", "file_type": "json", "from": "enhancement algorithms", "predicate": "enforces", "to": "spatially consistent prior", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_93", "file_type": "json", "from": "spatially consistent prior", "predicate": "relates", "to": "pixels with same RGB values", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_93", "file_type": "json", "from": "enhancement of regions", "predicate": "considers", "to": "fidelity", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_93", "file_type": "json", "from": "enhancement of regions", "predicate": "considers", "to": "temporal consistency", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_93", "file_type": "json", "from": "enhancement of regions", "predicate": "considers", "to": "spatial consistency", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_94", "file_type": "json", "from": "frames", "predicate": "consider", "to": "fidelity", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_95", "file_type": "json", "from": "Slic superpixels", "predicate": "compared to", "to": "superpixel methods", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_95", "file_type": "json", "from": "Slic superpixels", "predicate": "is described in", "to": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_223", "file_type": "json", "from": "Slic superpixels", "predicate": "is_a", "to": "superpixels", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_675", "file_type": "json", "from": "Slic superpixels", "predicate": "compared_to", "to": "state-of-the-art superpixel methods", "type": "factual", "width": 0.82}, {"arrows": "to", "chunk_id": "doc_0_chunk_675", "file_type": "json", "from": "Slic superpixels", "predicate": "presented_in", "to": "IEEE TPAM, 2012", "type": "factual", "width": 0.78}, {"arrows": "to", "chunk_id": "doc_0_chunk_177", "file_type": "json", "from": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "predicate": "is_publication_venue_for", "to": "reference [4]", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_692", "file_type": "json", "from": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "predicate": "is_publication", "to": "journal", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_448", "file_type": "json", "from": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "predicate": "publishes", "to": "Nonparametric Discriminant Analysis for Face Recognition", "type": "factual", "width": 2.0}, {"arrows": "to", "chunk_id": "doc_0_chunk_587", "file_type": "json", "from": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "predicate": "publishes", "to": "Multisculse local phase quantization for robust component-based face recognition", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_588", "file_type": "json", "from": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "predicate": "publishes", "to": "local phase quantization", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_671", "file_type": "json", "from": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "predicate": "is_a", "to": "publication", "type": "conceptual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_719", "file_type": "json", "from": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "predicate": "published", "to": "textures", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_719", "file_type": "json", "from": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "predicate": "published", "to": "Spacetime texture representation and recognition", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_95", "file_type": "json", "from": "tone management", "predicate": "is described in", "to": "ACM Trans. on Graph.", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_97", "file_type": "json", "from": "ACM Trans. on Graph.", "predicate": "published", "to": "Tonal stabilization of video", "type": "factual", "width": 2.0}, {"arrows": "to", "chunk_id": "doc_0_chunk_97", "file_type": "json", "from": "ACM Trans. on Graph.", "predicate": "published", "to": "Patch-based high dynamic range video", "type": "factual", "width": 2.0}, {"arrows": "to", "chunk_id": "doc_0_chunk_95", "file_type": "json", "from": "video color grading", "predicate": "is described in", "to": "ACM Trans. on Graph.", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_95", "file_type": "json", "from": "color transformation", "predicate": "is described in", "to": "IEEE Transactions on Image Processing", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_96", "file_type": "json", "from": "color transformation", "predicate": "applied_to", "to": "image", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_95", "file_type": "json", "from": "Image Enhancement Algorithms", "predicate": "are", "to": "enhancement algorithms", "type": "conceptual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_96", "file_type": "json", "from": "Y. Chang", "predicate": "authored", "to": "Example-based color transformation of image and video using basic color categories", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_97", "file_type": "json", "from": "Y. Chang", "predicate": "authored", "to": "Example-based color transformation", "type": "factual", "width": 2.0}, {"arrows": "to", "chunk_id": "doc_0_chunk_96", "file_type": "json", "from": "Example-based color transformation of image and video using basic color categories", "predicate": "published_in", "to": "IEEE Transactions on Image Processing", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_96", "file_type": "json", "from": "Example-based color transformation of image and video using basic color categories", "predicate": "deals_with", "to": "color transformation", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_96", "file_type": "json", "from": "S. Saito", "predicate": "authored", "to": "Example-based color transformation of image and video using basic color categories", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_97", "file_type": "json", "from": "S. Saito", "predicate": "co-authored", "to": "Example-based color transformation", "type": "factual", "width": 2.0}, {"arrows": "to", "chunk_id": "doc_0_chunk_96", "file_type": "json", "from": "M. Nakajima", "predicate": "authored", "to": "Example-based color transformation of image and video using basic color categories", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_97", "file_type": "json", "from": "M. Nakajima", "predicate": "co-authored", "to": "Example-based color transform", "type": "factual", "width": 2.0}, {"arrows": "to", "chunk_id": "doc_0_chunk_96", "file_type": "json", "from": "article", "predicate": "has_page_range", "to": "1\u201311", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_96", "file_type": "json", "from": "article", "predicate": "has_year", "to": "2013", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_97", "file_type": "json", "from": "Z. Farbman", "predicate": "authored", "to": "Tonal stabilization of video", "type": "factual", "width": 2.0}, {"arrows": "to", "chunk_id": "doc_0_chunk_97", "file_type": "json", "from": "D. Lischinski", "predicate": "co-authored", "to": "Tonal stabilization of video", "type": "factual", "width": 2.0}, {"arrows": "to", "chunk_id": "doc_0_chunk_97", "file_type": "json", "from": "P. F. Felzenszwalb", "predicate": "authored", "to": "Efficient belief propagation", "type": "factual", "width": 2.0}, {"arrows": "to", "chunk_id": "doc_0_chunk_97", "file_type": "json", "from": "D. P. Huttenlocher", "predicate": "co-authored", "to": "Efficient belief propagation", "type": "factual", "width": 2.0}, {"arrows": "to", "chunk_id": "doc_0_chunk_97", "file_type": "json", "from": "M. Grundmann", "predicate": "authored", "to": "Post-processing approach", "type": "factual", "width": 2.0}, {"arrows": "to", "chunk_id": "doc_0_chunk_97", "file_type": "json", "from": "Y. Hacohen", "predicate": "authored", "to": "Non-rigid dense correspondence", "type": "factual", "width": 2.0}, {"arrows": "to", "chunk_id": "doc_0_chunk_97", "file_type": "json", "from": "N. K. Kalantari", "predicate": "authored", "to": "Patch-based high dynamic range video", "type": "factual", "width": 2.0}, {"arrows": "to", "chunk_id": "doc_0_chunk_98", "file_type": "json", "from": "age enhancement", "predicate": "published_in", "to": "ACM Trans. Graph.", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_98", "file_type": "json", "from": "N. K. Kalantria", "predicate": "authored", "to": "Patch-based high dynamic range video", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_98", "file_type": "json", "from": "S. B. Kang", "predicate": "authored", "to": "High dynamic range video", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_98", "file_type": "json", "from": "High dynamic range video", "predicate": "published_in", "to": "ACM Trans. on Graph.", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_515", "file_type": "json", "from": "Tsinghua University", "predicate": "located_in", "to": "Beijing", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_98", "file_type": "json", "from": "Lionel Gueguen", "predicate": "authored", "to": "Large-Scale Damage Detection Using Satellite Imagery", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_98", "file_type": "json", "from": "Large-Scale Damage Detection Using Satellite Imagery", "predicate": "published_in", "to": "CVPR", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_98", "file_type": "json", "from": "Raffay Hamid", "predicate": "authored", "to": "Large-Scale Damage Detection Using Satellite Imagery", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_104", "file_type": "json", "from": "Raffay Hamid", "predicate": "affiliated_with", "to": "DigitalGlobe Inc.", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_104", "file_type": "json", "from": "Raffay Hamid", "predicate": "has_email", "to": "mhamid@digitalGlobe.com", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_99", "file_type": "json", "from": "Satellite imagery", "predicate": "is_used_for", "to": "assessing damages", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_99", "file_type": "json", "from": "Manual inspection", "predicate": "is_limited_by", "to": "vast amount of data", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_155", "file_type": "json", "from": "semi-supervised learning", "predicate": "improves", "to": "crowd counting", "type": "causal", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_99", "file_type": "json", "from": "study", "predicate": "uses", "to": "88 million images", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_99", "file_type": "json", "from": "study", "predicate": "analyzes", "to": "sun angle", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_99", "file_type": "json", "from": "study", "predicate": "analyzes", "to": "sensor resolution", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_99", "file_type": "json", "from": "study", "predicate": "analyzes", "to": "registration differences", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_225", "file_type": "json", "from": "study", "predicate": "considers", "to": "time constraints during offline training", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_99", "file_type": "json", "from": "4,665 KM2", "predicate": "is_across", "to": "12 locations", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_100", "file_type": "json", "from": "user study", "predicate": "demonstrates", "to": "ten-fold reduction", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_100", "file_type": "json", "from": "user study", "predicate": "evaluates", "to": "representation", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_99", "file_type": "json", "from": "ten-fold reduction", "predicate": "is_in", "to": "human annotation time", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_100", "file_type": "json", "from": "ten-fold reduction", "predicate": "impacts", "to": "efficiency", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_99", "file_type": "json", "from": "hierarchical shape features", "predicate": "is_within", "to": "bag-of-visual words setting", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_100", "file_type": "json", "from": "representation", "predicate": "compared_to", "to": "five alternatives", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_100", "file_type": "json", "from": "representation", "predicate": "offers", "to": "time efficiency", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_100", "file_type": "json", "from": "representation", "predicate": "improves", "to": "annotation process", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_321", "file_type": "json", "from": "representation", "predicate": "is", "to": "core challenge", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_321", "file_type": "json", "from": "representation", "predicate": "maintains", "to": "rich representation", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_100", "file_type": "json", "from": "detection accuracy", "predicate": "compared_to", "to": "manual inspection", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_100", "file_type": "json", "from": "detection accuracy", "predicate": "experienced", "to": "minimal loss", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_101", "file_type": "json", "from": "User study", "predicate": "demonstrates", "to": "ten-fold reduction in annotation time", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_101", "file_type": "json", "from": "User study", "predicate": "results in", "to": "minimal loss in detection accuracy", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_101", "file_type": "json", "from": "Damage detection", "predicate": "utilizes", "to": "Hierarchical shape features", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_101", "file_type": "json", "from": "Semi-supervised learning", "predicate": "addresses", "to": "Novelty detection", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_690", "file_type": "json", "from": "Semi-supervised learning", "predicate": "used_in", "to": "gigantic image collections", "type": "factual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_101", "file_type": "json", "from": "Xia et al. (2010)", "predicate": "publishes", "to": "Shape-based invariant texture indexing", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_101", "file_type": "json", "from": "Markou \u0026 Singh (2003)", "predicate": "reviews", "to": "Novelty detection", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_101", "file_type": "json", "from": "Blanchard et al. (2010)", "predicate": "investigates", "to": "Semi-supervised novelty detection", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_101", "file_type": "json", "from": "Bruzone \u0026 Prieto (2000)", "predicate": "analyzes", "to": "Difference image", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_101", "file_type": "json", "from": "Difference image", "predicate": "supports", "to": "Unsupervised change detection", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_101", "file_type": "json", "from": "Satellite imagery analysis", "predicate": "benefits from", "to": "Damage detection", "type": "conceptual", "width": 0.87}, {"arrows": "to", "chunk_id": "doc_0_chunk_102", "file_type": "json", "from": "Bruzzone", "predicate": "authored", "to": "Automatic analysis of the difference image", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_102", "file_type": "json", "from": "Prieto", "predicate": "authored", "to": "Automatic analysis of the difference image", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_102", "file_type": "json", "from": "Gerard", "predicate": "authored", "to": "A quasi-linear algorithm", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_102", "file_type": "json", "from": "International Symposium on Mathematical Morphology", "predicate": "published", "to": "A quasi-linear algorithm", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_102", "file_type": "json", "from": "Monasse", "predicate": "authored", "to": "Fast computation of a contrast-invariant image representation", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_102", "file_type": "json", "from": "Guichard", "predicate": "authored", "to": "Fast computation of a contrast-invariant image representation", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_102", "file_type": "json", "from": "Nielsen", "predicate": "authored", "to": "The regularized iteratively reweighted mad method", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_102", "file_type": "json", "from": "Vaduva", "predicate": "authored", "to": "A latent analysis of earth surface dynamic evolution", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_103", "file_type": "json", "from": "at", "predicate": "author_of", "to": "A latent analysis of earth surface dynamic evolution using change map time series", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_103", "file_type": "json", "from": "Lazarescu", "predicate": "author_of", "to": "A latent analysis of earth surface dynamic evolution using change map time series", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_103", "file_type": "json", "from": "Datcu", "predicate": "author_of", "to": "A latent anisotropy of earth surface dynamic evolution using change map time series", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_103", "file_type": "json", "from": "Gomez-Chova", "predicate": "author_of", "to": "Kernel-based framework for multitemporal and multisource remote sensing data classification and change detection", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_103", "file_type": "json", "from": "Wang", "predicate": "author_of", "to": "Locality-constrained linear coding for image classification", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_161", "file_type": "json", "from": "Wang", "predicate": "authored", "to": "Semi-supervised hashing", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_378", "file_type": "json", "from": "Wang", "predicate": "authored", "to": "Weakly Supervised Localization", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_538", "file_type": "json", "from": "Wang", "predicate": "authored", "to": "LIBLINEAR", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_103", "file_type": "json", "from": "Yang", "predicate": "author_of", "to": "Locality-constrained linear coding for image classification", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_621", "file_type": "json", "from": "Yang", "predicate": "co-authored", "to": "A survey on transfer learning", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_103", "file_type": "json", "from": "Song", "predicate": "author_of", "to": "Fusing Subcategory Probabilities for Texture Classi\ufb01cation", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_103", "file_type": "json", "from": "Cai", "predicate": "author_of", "to": "Fusing Subcategory Probabilities for Texture Classi\ufb01cation", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_103", "file_type": "json", "from": "Gueguen", "predicate": "affiliated_with", "to": "DigitalGlobe Inc.", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_103", "file_type": "json", "from": "Hamid", "predicate": "affiliated_with", "to": "DigitalGlobe Inc.", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_104", "file_type": "json", "from": "Yang Song", "predicate": "is_author_of", "to": "Fusing Subcategory Probabilities for Texture Classification", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_106", "file_type": "json", "from": "Yang Song", "predicate": "affiliated_with", "to": "BMIT Research Group", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_104", "file_type": "json", "from": "Fusing Subcategory Probabilities for Texture Classification", "predicate": "presented_at", "to": "CVPR", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_104", "file_type": "json", "from": "Fusing Subcategory Probabilities for Texture Classification", "predicate": "is_located_at", "to": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_104", "file_type": "json", "from": "Fusing Subcategory Probabilities for Texture Classification", "predicate": "has_file_name", "to": "Song_Fusing_Subcategory_Probabilities_2015_CVPR_paper.pdf", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_104", "file_type": "json", "from": "Qing Li", "predicate": "is_author_of", "to": "Fusing SubCategory Probabilities for Texture Classification", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_104", "file_type": "json", "from": "Fan Zhang", "predicate": "is_author_of", "to": "Fusing Subcategory Probabilities for Texture Classification", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_104", "file_type": "json", "from": "David Dagan Feng", "predicate": "is_author_of", "to": "Fusing Subcategory Probabilities for Texture Classification", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_107", "file_type": "json", "from": "David Dagan Feng", "predicate": "affiliated_with", "to": "BMIT Research Group", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_104", "file_type": "json", "from": "Heng Huang", "predicate": "is_author_of", "to": "Fusing Subcategory Probabilities for Texture Classification", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_107", "file_type": "json", "from": "Heng Huang", "predicate": "affiliated_with", "to": "Department of Computer Science and Engineering", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_105", "file_type": "json", "from": "Texture classification", "predicate": "remains", "to": "challenging", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_105", "file_type": "json", "from": "Texture classification", "predicate": "faces", "to": "high intra-class variation", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_105", "file_type": "json", "from": "sub-categorization model", "predicate": "applied to", "to": "texture classification", "type": "conceptual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_105", "file_type": "json", "from": "class", "predicate": "is divided into", "to": "subcategories", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_105", "file_type": "json", "from": "subcategories", "predicate": "have", "to": "distinctiveness", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_105", "file_type": "json", "from": "subcategories", "predicate": "have", "to": "representativeness", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_105", "file_type": "json", "from": "subcategory probabilities", "predicate": "are fused based on", "to": "contribution levels", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_105", "file_type": "json", "from": "subcategory probabilities", "predicate": "are fused based on", "to": "cluster qualities", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_105", "file_type": "json", "from": "fused probability", "predicate": "added to", "to": "multiclass classification probability", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_106", "file_type": "json", "from": "Texture Classification", "predicate": "uses", "to": "Method", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_106", "file_type": "json", "from": "BMIT Research Group", "predicate": "located_in", "to": "University of Sydney", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_107", "file_type": "json", "from": "BMIT Research Group", "predicate": "is_part_of", "to": "School of IT", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_107", "file_type": "json", "from": "University of Sydney", "predicate": "located_in", "to": "Australia", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_107", "file_type": "json", "from": "School of IT", "predicate": "located_in", "to": "University of Sydney", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_107", "file_type": "json", "from": "Department of Computer Science and Engineering", "predicate": "located_in", "to": "University of Texas, Arlington", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_107", "file_type": "json", "from": "Manohar Paluri", "predicate": "author_of", "to": "Beyond Frontal Faces", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_108", "file_type": "json", "from": "Manohar Paluri", "predicate": "is_author_of", "to": "Zhang_Beyond_Frontal_Faces_2015_CVPR_paper", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_110", "file_type": "json", "from": "Manohar Paluri", "predicate": "works_at", "to": "Facebook AI Research", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_107", "file_type": "json", "from": "Beyond Frontal Faces", "predicate": "published_in", "to": "CVPR", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_107", "file_type": "json", "from": "Yaniv Taigman", "predicate": "author_of", "to": "Beyond Frontal Faces", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_108", "file_type": "json", "from": "Yaniv Taigman", "predicate": "is_author_of", "to": "Zhang_Beyond_Frontal_Faces_2015_CVPR_paper", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_110", "file_type": "json", "from": "Yaniv Taigman", "predicate": "works_at", "to": "Facebook AI Research", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_107", "file_type": "json", "from": "Rob Fergus", "predicate": "author_of", "to": "Beyond Frontal Faces", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_108", "file_type": "json", "from": "Rob Fergus", "predicate": "is_author_of", "to": "Zhang_Beyond_Frontal_Faces_2015_CVPR_paper", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_110", "file_type": "json", "from": "Rob Fergus", "predicate": "works_at", "to": "Facebook AI Research", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_107", "file_type": "json", "from": "Lubomir Bourdev", "predicate": "author_of", "to": "Beyond Frontal Faces", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_108", "file_type": "json", "from": "Lubomir Bourdev", "predicate": "is_author_of", "to": "Zhang_Beyond_Frontal_Faces_2015_CVPR_paper", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_110", "file_type": "json", "from": "Lubomir Bourdev", "predicate": "works_at", "to": "Facebook AI Research", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_108", "file_type": "json", "from": "Zhang_Beyond_Frontal_Faces_2015_CVPR_paper", "predicate": "explores_task", "to": "recognizing peoples\u2019 identities", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_108", "file_type": "json", "from": "PIPA dataset", "predicate": "facilitates", "to": "recognizing peoples\u2019 identities", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_108", "file_type": "json", "from": "PIPA dataset", "predicate": "consists_of", "to": "60000 instances", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_108", "file_type": "json", "from": "PIPA dataset", "predicate": "contains", "to": "2000 individuals", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_108", "file_type": "json", "from": "PIPER method", "predicate": "combines", "to": "face recognizer", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_108", "file_type": "json", "from": "PIPER method", "predicate": "combines", "to": "global recognizer", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_109", "file_type": "json", "from": "face recognizer", "predicate": "combined_with", "to": "global recognizer", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_108", "file_type": "json", "from": "person images", "predicate": "contains", "to": "frontal face", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_109", "file_type": "json", "from": "person recognizers", "predicate": "trained_by", "to": "deep convolutional networks", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_109", "file_type": "json", "from": "deep convolutional networks", "predicate": "aims_to", "to": "discount pose variations", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_110", "file_type": "json", "from": "PIPER", "predicate": "improves_performance_on", "to": "DeepFace", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_109", "file_type": "json", "from": "PIPER", "predicate": "operates_in", "to": "unconstrained setup", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_109", "file_type": "json", "from": "DeepFace", "predicate": "is_regarded_as", "to": "best face recognizers", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_110", "file_type": "json", "from": "DeepFace", "predicate": "measured_on", "to": "LFW dataset", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_110", "file_type": "json", "from": "DeepFace", "predicate": "is_a", "to": "face recognizer", "type": "conceptual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_110", "file_type": "json", "from": "Pose Invariant Recognition", "predicate": "is_setting_for", "to": "PIPER", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_110", "file_type": "json", "from": "Paul Wohlhart", "predicate": "works_at", "to": "Institute for Computer Vision and Graphics, Graz University of Technology, Austria", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_110", "file_type": "json", "from": "Paul Wohlhart", "predicate": "contributed_to", "to": "Learning Descriptors for Object Recognition and 3D Pose Estimation", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_112", "file_type": "json", "from": "Paul Wohlhart", "predicate": "affiliated_with", "to": "Institute for Computer Vision and Graphics", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_111", "file_type": "json", "from": "Paul Wohlhart", "predicate": "email", "to": "{wohlhart}@icg.tugraz.at", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_111", "file_type": "json", "from": "Paul Wohlhart", "predicate": "contributed_to", "to": "2015 CVPR paper", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_111", "file_type": "json", "from": "Paul Wohlhart", "predicate": "works_in", "to": "Computer Graphics", "type": "factual", "width": 0.7}, {"arrows": "to", "chunk_id": "doc_0_chunk_112", "file_type": "json", "from": "Paul Wohlhart", "predicate": "located_in", "to": "Austria", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_110", "file_type": "json", "from": "Learning Descriptors for Object Recognition and 3D Pose Estimation", "predicate": "is_paper", "to": "cvpr_papers", "type": "factual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_549", "file_type": "json", "from": "cvpr_papers", "predicate": "located_in", "to": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_584", "file_type": "json", "from": "cvpr_papers", "predicate": "located_in", "to": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/", "type": "factual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_111", "file_type": "json", "from": "Institute for Computer Vision and Graphics", "predicate": "part_of", "to": "Graz University of Technology", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_111", "file_type": "json", "from": "Institute for Computer Vision and Graphics", "predicate": "research_area", "to": "Computer Vision", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_111", "file_type": "json", "from": "Graz University of Technology", "predicate": "located_in", "to": "Austria", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_112", "file_type": "json", "from": "Graz University of Technology", "predicate": "hosts", "to": "Institute for Computer Vision and Graphics", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_303", "file_type": "json", "from": "Graz University of Technology", "predicate": "has_institute", "to": "Institute for Computer Graphics and Vision", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_111", "file_type": "json", "from": "2015 CVPR paper", "predicate": "focuses_on", "to": "Learning Descriptors", "type": "conceptual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_111", "file_type": "json", "from": "2015 CVPR paper", "predicate": "published_by", "to": "IEEE", "type": "factual", "width": 0.65}, {"arrows": "to", "chunk_id": "doc_0_chunk_111", "file_type": "json", "from": "Learning Descriptors", "predicate": "related_to", "to": "Computer Vision", "type": "conceptual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_281", "file_type": "json", "from": "IEEE", "predicate": "published", "to": "A shape-preserving approach to image resizing", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_112", "file_type": "json", "from": "Vincent LePetit", "predicate": "affiliated_with", "to": "Institute for Computer Vision and Graphics", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_112", "file_type": "json", "from": "Vincent LePetit", "predicate": "located_in", "to": "Austria", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_112", "file_type": "json", "from": "Joe Yue-Hei Ng", "predicate": "author_of", "to": "Beyond Short Snippets", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_115", "file_type": "json", "from": "Joe Yue-Hei Ng", "predicate": "affiliation", "to": "University of Maryland, College Park", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_112", "file_type": "json", "from": "Beyond Short Snippets", "predicate": "presented_at", "to": "CVPR", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_112", "file_type": "json", "from": "Matthew Hausknecht", "predicate": "author_of", "to": "Beyond Short Snippets", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_115", "file_type": "json", "from": "Matthew Hausknecht", "predicate": "affiliation", "to": "University of Texas at Austin", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_112", "file_type": "json", "from": "Sudheendra Vijayanarasimhan", "predicate": "author_of", "to": "Beyond Short Snippets", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_115", "file_type": "json", "from": "Sudheendra Vijayanarasimhan", "predicate": "affiliation", "to": "Google, Inc.", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_112", "file_type": "json", "from": "Oriol Vinyals", "predicate": "author_of", "to": "Beyond Short Snippets", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_115", "file_type": "json", "from": "Oriol Vinyals", "predicate": "affiliation", "to": "Google, Inc.", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_112", "file_type": "json", "from": "Rajat Monga", "predicate": "author_of", "to": "Beyond Short Snppets", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_115", "file_type": "json", "from": "Rajat Monga", "predicate": "affiliation", "to": "Google, Inc.", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_112", "file_type": "json", "from": "George Toderici", "predicate": "author_of", "to": "Beyond Short Snippets", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_115", "file_type": "json", "from": "George Toderici", "predicate": "affiliation", "to": "Google, Inc.", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_113", "file_type": "json", "from": "Convolutional neural networks (CNNs)", "predicate": "applied_for", "to": "image recognition problems", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_113", "file_type": "json", "from": "Convolutional neural networks (CNNs)", "predicate": "yields", "to": "state-of-the-art results", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_113", "file_type": "json", "from": "deep neural network architectures", "predicate": "combines", "to": "image information", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_113", "file_type": "json", "from": "image information", "predicate": "spans", "to": "longer time periods", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_114", "file_type": "json", "from": "video", "predicate": "models_as", "to": "ordered sequence of frames", "type": "conceptual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_379", "file_type": "json", "from": "video", "predicate": "contains", "to": "novel object", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_380", "file_type": "json", "from": "video", "predicate": "contains", "to": "frames", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_114", "file_type": "json", "from": "recurrent neural network", "predicate": "uses", "to": "Long Short-Term Memory (LSTM) cells", "type": "conceptual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_114", "file_type": "json", "from": "LSTM cells", "predicate": "connected_to", "to": "output of CNN", "type": "conceptual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_114", "file_type": "json", "from": "networks", "predicate": "exhibits", "to": "performance improvements", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_114", "file_type": "json", "from": "networks", "predicate": "performs_on", "to": "UCF-101 datasets", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_197", "file_type": "json", "from": "networks", "predicate": "captures", "to": "local contextual information", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_197", "file_type": "json", "from": "networks", "predicate": "captures", "to": "global contextual information", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_197", "file_type": "json", "from": "networks", "predicate": "operates_on", "to": "RGB values", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_593", "file_type": "json", "from": "performance improvements", "predicate": "over", "to": "alternative algorithms", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_114", "file_type": "json", "from": "UCF-101 datasets", "predicate": "has_performance", "to": "88.6%", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_115", "file_type": "json", "from": "UCF-101 datasets", "predicate": "compared_to", "to": "88.0%", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_114", "file_type": "json", "from": "UCF-101 datasets", "predicate": "has_performance_without", "to": "optical flow information", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_115", "file_type": "json", "from": "UCF-101 datasets", "predicate": "compared_to", "to": "73.0%", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_115", "file_type": "json", "from": "UCF-101 datasets", "predicate": "has_accuracy", "to": "82.6%", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_115", "file_type": "json", "from": "Sports 1 million dataset", "predicate": "has_accuracy", "to": "73.1%", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_115", "file_type": "json", "from": "Sports 1 million dataset", "predicate": "compared_to", "to": "60.9%", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_115", "file_type": "json", "from": "Ioannis Gkiouslekas", "predicate": "author_of", "to": "On the Appearance of Translucnt Edges", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_633", "file_type": "json", "from": "Ioannis Gkiouslekas", "predicate": "affiliated with", "to": "Harvard SEAS", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_116", "file_type": "json", "from": "Gkiouslekas", "predicate": "affiliated_with", "to": "Harvard SEAS", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_117", "file_type": "json", "from": "Harvard SEAS", "predicate": "is_an_institution_of", "to": "Higher Education", "type": "conceptual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_633", "file_type": "json", "from": "Bruce Walter", "predicate": "affiliated with", "to": "Cornell University", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_117", "file_type": "json", "from": "Bruce Walter", "predicate": "has_email", "to": "bruce.walter@cornell.edu", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_628", "file_type": "json", "from": "Bruce Walter", "predicate": "author_of", "to": "On the Appearance of Translueceny Edges", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_117", "file_type": "json", "from": "Cornell University", "predicate": "is_an_institution_of", "to": "Higher Education", "type": "conceptual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_196", "file_type": "json", "from": "Cornell University", "predicate": "has_department", "to": "Department of Computer Science", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_634", "file_type": "json", "from": "Edward H. Adelson", "predicate": "affiliation", "to": "Massachusetts Institute of Technology", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_117", "file_type": "json", "from": "Edward H. Adelson", "predicate": "has_email", "to": "adelson@cail.mit.edu", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_628", "file_type": "json", "from": "Edward H. Adelson", "predicate": "author_of", "to": "On the Appearance of Translueceny Edges", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_117", "file_type": "json", "from": "Massachusetts Institute of Technology", "predicate": "is_an_institution_of", "to": "Higher Education", "type": "conceptual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_144", "file_type": "json", "from": "Massachusetts Institute of Technology", "predicate": "hosts", "to": "CSAIL", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_144", "file_type": "json", "from": "Massachusetts Institute of Technology", "predicate": "hosts", "to": "LIDS", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_116", "file_type": "json", "from": "Walter", "predicate": "authored", "to": "Microfacet models", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_116", "file_type": "json", "from": "Microfacet models", "predicate": "addresses", "to": "refraction through rough surfaces", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_116", "file_type": "json", "from": "Microfacet models", "predicate": "presented_at", "to": "EGSR", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_117", "file_type": "json", "from": "Ioannis Gkioleakas", "predicate": "affiliated_with", "to": "Harvard SEAS", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_117", "file_type": "json", "from": "Ioannis Gkioleakas", "predicate": "has_email", "to": "igkio@seas.harvard.edu", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_117", "file_type": "json", "from": "Higher Education", "predicate": "provides", "to": "Academic Degrees", "type": "conceptual", "width": 0.7}, {"arrows": "to", "chunk_id": "doc_0_chunk_118", "file_type": "json", "from": "Ioannis Gkiousleas", "predicate": "is_affiliated_with", "to": "Harvard SEAS", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_634", "file_type": "json", "from": "Todd Zickler", "predicate": "affiliation", "to": "Harvard SEAS", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_628", "file_type": "json", "from": "Todd Zickler", "predicate": "author_of", "to": "On the Appearence of Translueceny Edges", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_634", "file_type": "json", "from": "Kavita Bala", "predicate": "affiliation", "to": "Cornell University", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_191", "file_type": "json", "from": "Kavita Bala", "predicate": "is_author_of", "to": "Material recognition in 2015 CVPR paper", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_192", "file_type": "json", "from": "Kavita Bala", "predicate": "contributes_to", "to": "Bell_Material_Recognition_in_2015_CVPR_paper", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_196", "file_type": "json", "from": "Kavita Bala", "predicate": "affiliated_with", "to": "Department of Computer Science, Cornell University", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_195", "file_type": "json", "from": "Kavita Bala", "predicate": "has_email", "to": "kb@cs.cornell.edu", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_628", "file_type": "json", "from": "Kavita Bala", "predicate": "author_of", "to": "On the Appearance of Translueceny Edges", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_118", "file_type": "json", "from": "Naeemullah Khan", "predicate": "is_author_of", "to": "Shape-Tailored Local Descriptors", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_118", "file_type": "json", "from": "Shape-Tailored Local Descriptors", "predicate": "is_a", "to": "CVPR paper", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_118", "file_type": "json", "from": "Shape-Tailored Local Descriptors", "predicate": "has_application_in", "to": "Segmentation", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_118", "file_type": "json", "from": "Shape-Tailored Local Descriptors", "predicate": "has_application_in", "to": "Tracking", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_118", "file_type": "json", "from": "Marei Algarni", "predicate": "is_author_of", "to": "Shape-Tailored Local Descriptors", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_121", "file_type": "json", "from": "Marei Algarni", "predicate": "affiliated_with", "to": "King Abdullah University of Science \u0026 Technology (KAUST)", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_118", "file_type": "json", "from": "Anthony Yezzi", "predicate": "is_author_of", "to": "Shape-Tailed Local Descriptors", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_121", "file_type": "json", "from": "Anthony Yezzi", "predicate": "affiliated_with", "to": "Georgia Institute of Technology", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_118", "file_type": "json", "from": "Ganesh Sundaramoorthi", "predicate": "is_author_of", "to": "Shape-Tailored Local Descriptors", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_121", "file_type": "json", "from": "Ganesh Sundaramoorthi", "predicate": "affiliated_with", "to": "King Abdullah University of Science \u0026 Technology (KAUST)", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_119", "file_type": "json", "from": "descriptors", "predicate": "are", "to": "dense descriptors", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_119", "file_type": "json", "from": "descriptors", "predicate": "used for", "to": "texture segmentation", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_119", "file_type": "json", "from": "descriptors", "predicate": "formed from", "to": "shape-dependent scale spaces", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_119", "file_type": "json", "from": "descriptors", "predicate": "do not", "to": "aggregate image data across boundary", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_120", "file_type": "json", "from": "descriptors", "predicate": "lead_to", "to": "accurate segmentation", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_119", "file_type": "json", "from": "descriptors", "predicate": "are", "to": "shape-dependent", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_120", "file_type": "json", "from": "descriptors", "predicate": "outperforms", "to": "state-of-the-art", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_120", "file_type": "json", "from": "descriptors", "predicate": "are", "to": "non-shape dependent", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_330", "file_type": "json", "from": "datasets", "predicate": "contain", "to": "3D models", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_624", "file_type": "json", "from": "datasets", "predicate": "are", "to": "SIFTs", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_120", "file_type": "json", "from": "accurate segmentation", "predicate": "improves", "to": "texture segmentation", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_119", "file_type": "json", "from": "oriented gradients", "predicate": "used in", "to": "existing descriptors", "type": "factual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_120", "file_type": "json", "from": "textured object tracking", "predicate": "relies_on", "to": "texture segmentation", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_121", "file_type": "json", "from": "Shape-Tailored Descriptors (STLD)", "predicate": "improves", "to": "texture segmentation", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_121", "file_type": "json", "from": "Shape-Tailori Descriptors (STLD)", "predicate": "outperforms", "to": "non-shape dependent descriptors", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_121", "file_type": "json", "from": "Local Descriptors", "predicate": "used in", "to": "texture segmentation", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_122", "file_type": "json", "from": "De-An Huang", "predicate": "is_author_of", "to": "How Do We Use Our Hands?", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_123", "file_type": "json", "from": "De-An Huang", "predicate": "is_author_of", "to": "Common Grasps", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_126", "file_type": "json", "from": "De-An Huang", "predicate": "affiliated_with", "to": "Cnegie Mellon University", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_122", "file_type": "json", "from": "How Do We Use Our Hands?", "predicate": "discusses", "to": "common grasps", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_121", "file_type": "json", "from": "Partial Differential Equations (PDE)", "predicate": "used in", "to": "texture segmentation", "type": "factual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_122", "file_type": "json", "from": "Minghuang Ma", "predicate": "is_author_of", "to": "How DoWe Use Our Hands?", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_123", "file_type": "json", "from": "Minghuang Ma", "predicate": "is_author_of", "to": "Common Grasps", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_126", "file_type": "json", "from": "Minghuang Ma", "predicate": "affiliated_with", "to": "Cnegie Mellon University", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_127", "file_type": "json", "from": "Minghuang Ma", "predicate": "affiliated_with", "to": "Canezie Mellon University", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_128", "file_type": "json", "from": "Minghuang Ma", "predicate": "affiliated_with", "to": "Carnegie Mellon University", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_122", "file_type": "json", "from": "Wei-Chiu Ma", "predicate": "is_author_of", "to": "How Do We Use Our Hands?", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_123", "file_type": "json", "from": "Wei-Chiu Ma", "predicate": "is_author_of", "to": "Common Grasps", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_126", "file_type": "json", "from": "Wei-Chiu Ma", "predicate": "affiliated_with", "to": "Cnegie Mellon University", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_127", "file_type": "json", "from": "Wei-Chiu Ma", "predicate": "affiliated_with", "to": "Canezie Mellon University", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_128", "file_type": "json", "from": "Wei-Chiu Ma", "predicate": "affiliated_with", "to": "Carnegie Mellon University", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_122", "file_type": "json", "from": "Kris M. Kitani", "predicate": "is_author_of", "to": "How Do We Use Our Hands?", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_123", "file_type": "json", "from": "Kris M. Kitani", "predicate": "is_author_of", "to": "Common Graspt", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_127", "file_type": "json", "from": "Kris M. Kitani", "predicate": "affiliated_with", "to": "Canezie Mellon University", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_122", "file_type": "json", "from": "KAUST", "predicate": "located_in", "to": "Saudi Arabia", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_122", "file_type": "json", "from": "ganesh.sundaramoorthi@kust.edu.sa", "predicate": "is_affiliated_with", "to": "KAUST", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_122", "file_type": "json", "from": "Huang_How_Do_We2015_CVPR_paper.pdf", "predicate": "is_paper", "to": "How Do We Use Our Hands?", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_123", "file_type": "json", "from": "computer vision techniques", "predicate": "can_be_used_to", "to": "advance prehensile analysis", "type": "conceptual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_123", "file_type": "json", "from": "prehensile analysis", "predicate": "is_a", "to": "multi-disciplinary field", "type": "conceptual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_123", "file_type": "json", "from": "researchers", "predicate": "analyze", "to": "hand-object interaction videos", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_123", "file_type": "json", "from": "wearable cameras", "predicate": "can_be_used_to", "to": "automatically discover common modes of human hand use", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_600", "file_type": "json", "from": "wearable cameras", "predicate": "led to", "to": "increase in egocentric videos", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_123", "file_type": "json", "from": "unsupervised clustering techniques", "predicate": "can_be_used_to", "to": "automatically discover common modes of human hand use", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_123", "file_type": "json", "from": "first-person point-of-view camera", "predicate": "is_used_for", "to": "observing human hand use", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_123", "file_type": "json", "from": "common modes of human hand use", "predicate": "are_discovered_by", "to": "wearable cameras", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_125", "file_type": "json", "from": "first-person point-of-view videos", "predicate": "includes", "to": "choreographed scenarios", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_125", "file_type": "json", "from": "Grasp Taxonomy", "predicate": "is_taxonomy_of", "to": "hand-object interaction", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_125", "file_type": "json", "from": "Determinantal Point Process (DPP)", "predicate": "is_used_in", "to": "approach", "type": "factual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_125", "file_type": "json", "from": "N. Ailon", "predicate": "authors", "to": "Streaming k-means approximation", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_125", "file_type": "json", "from": "W. Barbakh", "predicate": "authors", "to": "Online clustering algorithms", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_125", "file_type": "json", "from": "A. Fathi", "predicate": "authors", "to": "Social interactions: A first-person perspective", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_125", "file_type": "json", "from": "R. Filipovych", "predicate": "authors", "to": "Recognizing primitive interactions", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_125", "file_type": "json", "from": "J. Case-Smith", "predicate": "authored", "to": "Development of hand skills in children", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_126", "file_type": "json", "from": "Development of hand skills in children", "predicate": "addresses", "to": "hand skills", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_126", "file_type": "json", "from": "L. Cheng", "predicate": "coauthored", "to": "Pixel-level hand detection in ego-centric videos", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_126", "file_type": "json", "from": "American Occupational Therapy Association", "predicate": "published", "to": "Development of hand skills in children", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_126", "file_type": "json", "from": "M. Cutkosky", "predicate": "authored", "to": "On grasp choice, grasp models", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_126", "file_type": "json", "from": "H. N. Djidjev", "predicate": "coauthored", "to": "Computing shortest paths", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_126", "file_type": "json", "from": "C. Desai", "predicate": "coauthored", "to": "Discriminaitive models for static human-object interactions", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_126", "file_type": "json", "from": "Kris M. Kitan", "predicate": "affiliated_with", "to": "Cnegie Mellon University", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_127", "file_type": "json", "from": "Canezie Mellon University", "predicate": "has_affiliation", "to": "Kris M. Kitani", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_127", "file_type": "json", "from": "Canezie Mellon University", "predicate": "has_affiliation", "to": "Wei-Chiu Ma", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_127", "file_type": "json", "from": "Canezie Mellon University", "predicate": "has_affiliation", "to": "Minghuang Ma", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_127", "file_type": "json", "from": "deanh@andrew.cmu.edu", "predicate": "associated_with", "to": "Canezie Mellon University", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_128", "file_type": "json", "from": "deanh@andrew.cmu.edu", "predicate": "associated_with", "to": "Carnegie Mellon University", "type": "factual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_127", "file_type": "json", "from": "minghuam@andrew.cmu.edu", "predicate": "associated_with", "to": "Canezie Mellon University", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_127", "file_type": "json", "from": "weichium@andrew.cmu.edu", "predicate": "associated_with", "to": "Canezie Mellon University", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_128", "file_type": "json", "from": "Edward Johns", "predicate": "is_author_of", "to": "Becoming the Expert", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_128", "file_type": "json", "from": "Becoming the Expert", "predicate": "is_paper_about", "to": "Machine Teaching", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_128", "file_type": "json", "from": "Becoming the Expert", "predicate": "published_in", "to": "CVPR", "type": "factual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_128", "file_type": "json", "from": "Oisin Mac Aodha", "predicate": "is_author_of", "to": "Becoming the Efficient", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_128", "file_type": "json", "from": "Gabriel J. Brostow", "predicate": "is_author_of", "to": "Becoming the Expert", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_128", "file_type": "json", "from": "Johns_Becoming_the_Expert_2015_CVPR_paper.pdf", "predicate": "represents", "to": "Becoming the Expert", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_129", "file_type": "json", "from": "computer", "predicate": "teaches", "to": "challenging visual concepts", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_129", "file_type": "json", "from": "teaching strategy", "predicate": "produces", "to": "experts", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_129", "file_type": "json", "from": "challenge", "predicate": "concerns", "to": "annotators", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_260", "file_type": "json", "from": "challenge", "predicate": "related_to", "to": "stereo matching", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_260", "file_type": "json", "from": "challenge", "predicate": "involves", "to": "ground control points", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_527", "file_type": "json", "from": "challenge", "predicate": "of", "to": "interpreting line drawings", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_129", "file_type": "json", "from": "annotators", "predicate": "have", "to": "expertise", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_130", "file_type": "json", "from": "Interactive Machine Teaching", "predicate": "relates_to", "to": "Human Learning", "type": "conceptual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_130", "file_type": "json", "from": "Interactive Machine Teaching", "predicate": "utilizes", "to": "Adaptive Algorithms", "type": "conceptual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_130", "file_type": "json", "from": "Interactive Machine Teaching", "predicate": "involves", "to": "Visual Classification", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_130", "file_type": "json", "from": "Bruner", "predicate": "authored", "to": "The Process of Education", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_130", "file_type": "json", "from": "The Process of Education", "predicate": "provides", "to": "foundational concepts", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_130", "file_type": "json", "from": "foundational concepts", "predicate": "relates_to", "to": "teaching and learning", "type": "conceptual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_130", "file_type": "json", "from": "Curriculum learning", "predicate": "aligns_with", "to": "teaching strategies", "type": "conceptual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_130", "file_type": "json", "from": "Love", "predicate": "authored", "to": "Categorization", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_131", "file_type": "json", "from": "Categorization", "predicate": "is_element_of", "to": "cognitive neuroscience", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_130", "file_type": "json", "from": "Categorization", "predicate": "is_element_of", "to": "core tasks", "type": "conceptual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_131", "file_type": "json", "from": "cognitive neuroscience", "predicate": "provides_perspective_on", "to": "Categorization", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_131", "file_type": "json", "from": "active learning", "predicate": "is_explored_in", "to": "paper", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_131", "file_type": "json", "from": "active learning", "predicate": "is_technique", "to": "overview", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_131", "file_type": "json", "from": "active learning", "predicate": "is_key", "to": "technique", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_131", "file_type": "json", "from": "Machine teaching", "predicate": "is_problem_of", "to": "machine learning", "type": "conceptual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_131", "file_type": "json", "from": "Machine teaching", "predicate": "is_approach_to", "to": "education", "type": "conceptual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_695", "file_type": "json", "from": "overview", "predicate": "is_about", "to": "transparent object reconstruction", "type": "conceptual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_465", "file_type": "json", "from": "technique", "predicate": "outperforms", "to": "video segmentation", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_520", "file_type": "json", "from": "technique", "predicate": "is_based_on", "to": "CNN", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_520", "file_type": "json", "from": "technique", "predicate": "achieves", "to": "high accuracy", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_520", "file_type": "json", "from": "technique", "predicate": "detects", "to": "salient objects", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_520", "file_type": "json", "from": "technique", "predicate": "predicts", "to": "number of objects", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_131", "file_type": "json", "from": "sampling estimation", "predicate": "reduces", "to": "error", "type": "causal", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_132", "file_type": "json", "from": "machine teaching", "predicate": "is_relevant_to", "to": "approach toward optimal education", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_132", "file_type": "json", "from": "approach toward optimal education", "predicate": "incorporates", "to": "machine teaching", "type": "conceptual", "width": 0.87}, {"arrows": "to", "chunk_id": "doc_0_chunk_132", "file_type": "json", "from": "paper\u0027s design", "predicate": "addresses", "to": "learners with limited cognitive capacity", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_132", "file_type": "json", "from": "algorithmic teaching", "predicate": "provides", "to": "background", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_132", "file_type": "json", "from": "Love \u0026 Patil", "predicate": "addresses", "to": "teaching learners", "type": "factual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_132", "file_type": "json", "from": "Balbach \u0026 Zeugmann", "predicate": "contributes_to", "to": "algorithmic teaching methods", "type": "factual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_132", "file_type": "json", "from": "Basu \u0026 Christensen", "predicate": "focuses_on", "to": "teaching classification boundaries", "type": "factual", "width": 0.82}, {"arrows": "to", "chunk_id": "doc_0_chunk_133", "file_type": "json", "from": "Basu \u0026 Christensen", "predicate": "related_to", "to": "teaching classification tasks", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_132", "file_type": "json", "from": "Language and Automata Theory and Applications", "predicate": "published", "to": "Recent developments in algorithmic teaching", "type": "factual", "width": 0.68}, {"arrows": "to", "chunk_id": "doc_0_chunk_133", "file_type": "json", "from": "teaching classification tasks", "predicate": "requires", "to": "algorithmic teaching methods", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_133", "file_type": "json", "from": "Gigu`ere \u0026 Love", "predicate": "explores", "to": "cognitive limitations", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_133", "file_type": "json", "from": "cognitive limitations", "predicate": "influences", "to": "teaching strategies", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_133", "file_type": "json", "from": "Chin", "predicate": "affiliated_with", "to": "University College London", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_133", "file_type": "json", "from": "Eriksson", "predicate": "affiliated_with", "to": "University College London", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_133", "file_type": "json", "from": "Suter", "predicate": "affiliated_with", "to": "University College London", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_133", "file_type": "json", "from": "Chin_Efficient_Globally_Optimal_2015_CVPR_paper.pdf", "predicate": "is_a", "to": "CVPR paper", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_133", "file_type": "json", "from": "Efficient Globally Optimal Consensus Maximisation", "predicate": "described_in", "to": "Chin_Efficient_Globally_Optimal_2015_CVPR_paper.pdf", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_134", "file_type": "json", "from": "Maximum Consensus", "predicate": "is_criterion_for", "to": "Robust Estimation", "type": "definitional", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_135", "file_type": "json", "from": "Maximum Consensus", "predicate": "is_related_to", "to": "Optimization Problems", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_134", "file_type": "json", "from": "randomized sample-and-test techniques", "predicate": "does_not_guarantee", "to": "optimality", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_134", "file_type": "json", "from": "globally optimal algorithms", "predicate": "is_too_slow_compared_to", "to": "randomized methods", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_134", "file_type": "json", "from": "tree search problem", "predicate": "uses", "to": "LP-type methods", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_135", "file_type": "json", "from": "A* Search", "predicate": "is_a", "to": "Search Algorithm", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_135", "file_type": "json", "from": "A* Search", "predicate": "achieves", "to": "globally optimal results", "type": "factual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_135", "file_type": "json", "from": "Tree Search", "predicate": "is_a", "to": "Search Algorithm", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_135", "file_type": "json", "from": "LP-type Methods", "predicate": "addresses", "to": "Optimization Problems", "type": "conceptual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_135", "file_type": "json", "from": "N. Amenta", "predicate": "authored", "to": "Optimal Point Placement for Mesh Smoothing", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_135", "file_type": "json", "from": "M. Bern", "predicate": "authored", "to": "Optimal Point Placement for Mesh Smoothing", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_135", "file_type": "json", "from": "D. Eppstein", "predicate": "authored", "to": "Optimal Point Placement for Mesh Smoothing", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_135", "file_type": "json", "from": "B. Chazelle", "predicate": "authored", "to": "On Linear-Time Deterministic Algorithms", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_135", "file_type": "json", "from": "J. Matou\u02c7sek", "predicate": "authored", "to": "On Linear-Time Deterministic Algorithms", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_135", "file_type": "json", "from": "Quasiconvex Programming", "predicate": "is_a", "to": "Optimization Technique", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_136", "file_type": "json", "from": "optimization algorithms", "predicate": "related_to", "to": "core theme", "type": "conceptual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_136", "file_type": "json", "from": "optimization algorithms", "predicate": "used_in", "to": "computer vision", "type": "conceptual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_136", "file_type": "json", "from": "RANSA algorithm", "predicate": "introduces", "to": "outlier rejection", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_136", "file_type": "json", "from": "RANSA algorithm", "predicate": "applies_to", "to": "image analysis", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_136", "file_type": "json", "from": "RANSA algorithm", "predicate": "applies_to", "to": "automated cartography", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_137", "file_type": "json", "from": "multiple view geometry", "predicate": "is_topic", "to": "central topic", "type": "conceptual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_136", "file_type": "json", "from": "multiple view geometry", "predicate": "is_reference_for", "to": "computer vision", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_136", "file_type": "json", "from": "l\u221e triangulation", "predicate": "is_method_for", "to": "outlier handling", "type": "factual", "width": 0.89}, {"arrows": "to", "chunk_id": "doc_0_chunk_137", "file_type": "json", "from": "H. Li", "predicate": "authored", "to": "algorithm for l\u221e triangulation", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_137", "file_type": "json", "from": "algorithm for l\u221e triangulation", "predicate": "handles", "to": "outliers", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_137", "file_type": "json", "from": "geometric optimization", "predicate": "related_to", "to": "constraints", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_137", "file_type": "json", "from": "C. Olsson, O. Enqvist, and F. Kahl", "predicate": "focused_on", "to": "outlier handling in matching", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_137", "file_type": "json", "from": "matching", "predicate": "part_of", "to": "registration", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_137", "file_type": "json", "from": "C. Olsson, A. Eriksson, and F. Kahl", "predicate": "addressed", "to": "optimization techniques", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_137", "file_type": "json", "from": "optimization techniques", "predicate": "for", "to": "l\u221e-norm problems", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_433", "file_type": "json", "from": "optimization techniques", "predicate": "important_for", "to": "efficient computation", "type": "conceptual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_137", "file_type": "json", "from": "Tat-Jun Chin", "predicate": "affiliated_with", "to": "School of Computer Science", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_138", "file_type": "json", "from": "Tat-Jun Chin", "predicate": "is_affiliated_with", "to": "School of Computer Science, The University of Adelaide", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_137", "file_type": "json", "from": "University of Adelaide", "predicate": "contains", "to": "School of Computer Science", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_640", "file_type": "json", "from": "University of Adelaide", "predicate": "has_author", "to": "Bolei Zhou", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_640", "file_type": "json", "from": "University of Adelaide", "predicate": "has_author", "to": "Fuyuan Hu", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_640", "file_type": "json", "from": "University of Adelaide", "predicate": "has_author", "to": "Zhen Zhang", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_640", "file_type": "json", "from": "University of Adelaide", "predicate": "has_author", "to": "Anton van den Hengel", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_640", "file_type": "json", "from": "University of Adelaide", "predicate": "has_author", "to": "Chunhua Shen", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_137", "file_type": "json", "from": "Anders Eriksson", "predicate": "affiliated_with", "to": "School of Computer Science", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_138", "file_type": "json", "from": "Anders Eriksson", "predicate": "is_affiliated_with", "to": "School of Electrical Engineering and Computer Science, Queensland University of Technology", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_138", "file_type": "json", "from": "Pulak Purkait", "predicate": "is_affiliated_with", "to": "School of Computer Science, The University of Adelaide", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_138", "file_type": "json", "from": "Julian Straub", "predicate": "is_author_of", "to": "Small-Variance Nonparametric Clustering on the Hypsphere", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_143", "file_type": "json", "from": "Julian Straub", "predicate": "affiliated_with", "to": "CSAIL", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_143", "file_type": "json", "from": "Julian Straub", "predicate": "affiliated_with", "to": "LIDS", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_138", "file_type": "json", "from": "Small-Variance Nonparametric Clustering on the Hypsphere", "predicate": "is_located_at", "to": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Straub_Small-Variance_Nonparametric_Clustering_2015_CVPR_paper.pdf", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_138", "file_type": "json", "from": "Trevor Campbell", "predicate": "is_author_of", "to": "Small-Variance Nonparametric Clustering on the Hypsphere", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_144", "file_type": "json", "from": "Trevor Campbell", "predicate": "affiliated_with", "to": "CSAIL", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_144", "file_type": "json", "from": "Trevor Campbell", "predicate": "affiliated_with", "to": "LIDS", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_138", "file_type": "json", "from": "Jonathan P. How", "predicate": "is_author_of", "to": "Small-Variance Nonparametric Clustering on the Hypsphere", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_144", "file_type": "json", "from": "Jonathan P. How", "predicate": "affiliated_with", "to": "CSAIL", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_143", "file_type": "json", "from": "Jonathan P. How", "predicate": "affiliated_with", "to": "LIDS", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_138", "file_type": "json", "from": "John W. Fisher III", "predicate": "is_author_of", "to": "Small-Variance Nonparametric Clustering on the Hypsphere", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_144", "file_type": "json", "from": "John W. Fisher III", "predicate": "affiliated_with", "to": "CSAIL", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_143", "file_type": "json", "from": "John W. Fisher III", "predicate": "affiliated_with", "to": "LIDS", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_144", "file_type": "json", "from": "John W. Fisher III", "predicate": "email", "to": "fisher@csaill.mit.edu", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_138", "file_type": "json", "from": "Straub_Small-Variance_Nonparametric_Clustering_2015_CVPR_paper.pdf", "predicate": "describes", "to": "Small-Variance Nonparametric Clustering on the Hypshere", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_139", "file_type": "json", "from": "surface normals", "predicate": "reflect", "to": "distribution of structural regularities", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_139", "file_type": "json", "from": "algorithms", "predicate": "derived from", "to": "Bayesian nonparametric von-Mises-Fisher (vMF) mixture distributions", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_139", "file_type": "json", "from": "algorithms", "predicate": "respect", "to": "geometry of directional data", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_140", "file_type": "json", "from": "algorithms", "predicate": "demonstrates_performance_on", "to": "synthetic directional data", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_139", "file_type": "json", "from": "algorithms", "predicate": "demonstrated on", "to": "real 3D surface normals", "type": "factual", "width": 0.82}, {"arrows": "to", "chunk_id": "doc_0_chunk_140", "file_type": "json", "from": "algorithms", "predicate": "respects", "to": "geometry", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_140", "file_type": "json", "from": "algorithms", "predicate": "demonstrates_performance_on", "to": "3D surface normals", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_140", "file_type": "json", "from": "algorithms", "predicate": "generalizes_to", "to": "high dimensional directional data", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_139", "file_type": "json", "from": "DDP-vMF-means", "predicate": "infers", "to": "temporally evolving cluster structure", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_139", "file_type": "json", "from": "DDP-vMF-means", "predicate": "handles", "to": "streaming data", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_140", "file_type": "json", "from": "directional data", "predicate": "lies_on", "to": "unit sphere", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_139", "file_type": "json", "from": "DP-vMF-means", "predicate": "is a", "to": "Dirichlet process (DP) vMF mixture", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_140", "file_type": "json", "from": "high dimensional directional data", "predicate": "includes", "to": "protein backbone configurations", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_140", "file_type": "json", "from": "high dimensional directional data", "predicate": "includes", "to": "semantic word vectors", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_141", "file_type": "json", "from": "Algorithms", "predicate": "utilize", "to": "RGB-D sensors", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_141", "file_type": "json", "from": "Algorithms", "predicate": "generalize_to", "to": "protein backbone configurations", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_141", "file_type": "json", "from": "Algorithms", "predicate": "generalize_to", "to": "semantic word vectors", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_567", "file_type": "json", "from": "Algorithms", "predicate": "measured_by", "to": "Runtime", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_567", "file_type": "json", "from": "Algorithms", "predicate": "measured_by", "to": "Solution Quality", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_141", "file_type": "json", "from": "Surface Normals", "predicate": "derived_from", "to": "RGB-D sensors", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_141", "file_type": "json", "from": "Bayesian Nonparametric Clustering", "predicate": "relates_to", "to": "von-Mises-Fisher Distributions", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_141", "file_type": "json", "from": "von-Mises-Fisher Distributions", "predicate": "used_in", "to": "Bayesian Nonparametric Clustering", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_141", "file_type": "json", "from": "Streaming Data Analysis", "predicate": "related_to", "to": "Bayesian Nonparametric Clustering", "type": "conceptual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_141", "file_type": "json", "from": "Abramowitz \u0026 Stegun", "predicate": "provides", "to": "mathematical background", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_141", "file_type": "json", "from": "Neal", "predicate": "discusses", "to": "Markov chain sampling", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_141", "file_type": "json", "from": "Markov chain sampling", "predicate": "applied_to", "to": "DPMMs", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_141", "file_type": "json", "from": "Jiang, Kulis, \u0026 Jordan", "predicate": "provides", "to": "theoretical analysis", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_141", "file_type": "json", "from": "DPMMs", "predicate": "analyzed_by", "to": "Jiang, Kulis, \u0026 Jordan", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_142", "file_type": "json", "from": "DPMMs", "predicate": "analyzed by", "to": "Jiang, K., Kulis, B., and Jordan, M.", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_142", "file_type": "json", "from": "DPMMs", "predicate": "related_to", "to": "Bayesian nonparametrics", "type": "conceptual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_143", "file_type": "json", "from": "DPMMs", "predicate": "focuses_on", "to": "spherical data", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_142", "file_type": "json", "from": "Latent Dirichlet Allocation", "predicate": "is_relevant_to", "to": "spherical topic models", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_142", "file_type": "json", "from": "Latent Dirichlet Allocation", "predicate": "presented in", "to": "JMLR", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_142", "file_type": "json", "from": "k-means", "predicate": "connected_to", "to": "Bayesian nonparametrics", "type": "conceptual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_142", "file_type": "json", "from": "Directional statistics", "predicate": "treated in", "to": "Mardia, K. V. and Jupp, P. E.", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_142", "file_type": "json", "from": "Directional statistics", "predicate": "is_described_in", "to": "John Wiley \u0026 Sons", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_142", "file_type": "json", "from": "Bayesian nonparametric methods", "predicate": "analyzed in", "to": "Ferguson, T.", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_142", "file_type": "json", "from": "Bayesian nonparametric methods", "predicate": "is_foundational_to", "to": "DPMMs", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_143", "file_type": "json", "from": "Ferguson distributions", "predicate": "introduced_through", "to": "p\u00b4olya urn schemes", "type": "conceptual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_143", "file_type": "json", "from": "Dirichlet processes", "predicate": "provides_overview_in", "to": "Encyclopedia of Machine Learning", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_143", "file_type": "json", "from": "Bayesian analysis", "predicate": "analyzes", "to": "nonparametric problems", "type": "factual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_144", "file_type": "json", "from": "Dingwen Zhang", "predicate": "author_of", "to": "Co-Saliency Detection via Looking Deep and Wide", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_149", "file_type": "json", "from": "Dingwen Zhang", "predicate": "affiliated_with", "to": "Northwestern Polytechnical University", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_144", "file_type": "json", "from": "Co-Saliency Detection via Looking Deep and Wide", "predicate": "published_as", "to": "cvpr_papers/Zhang_Co-Saliency_Detection_via_2015_CVPR_paper.pdf", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_144", "file_type": "json", "from": "Junwei Han", "predicate": "author_of", "to": "Co-Saliency Detection via Looking Deep and Wide", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_149", "file_type": "json", "from": "Junwei Han", "predicate": "affiliated_with", "to": "Northwestern Polytechnical University", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_144", "file_type": "json", "from": "Chao Li", "predicate": "author_of", "to": "Co-Saliency Detection via Looking Deep and Wide", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_149", "file_type": "json", "from": "Chao Li", "predicate": "affiliated_with", "to": "Northwestern Polytechnical University", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_144", "file_type": "json", "from": "Jingdong Wang", "predicate": "author_of", "to": "Co-Saliency Detection via Looking Deep and Wide", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_150", "file_type": "json", "from": "Jingdong Wang", "predicate": "affiliation", "to": "Microsoft Research", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_150", "file_type": "json", "from": "Jingdong Wang", "predicate": "email", "to": "jingdw@microsoft.com", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_145", "file_type": "json", "from": "co-saliency detection", "predicate": "is_essential_for", "to": "video foreground extraction", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_145", "file_type": "json", "from": "co-saliency detection", "predicate": "is_essential_for", "to": "surveillance", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_145", "file_type": "json", "from": "co-saliency detection", "predicate": "is_essential_for", "to": "image retrieval", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_145", "file_type": "json", "from": "co-saliency detection", "predicate": "is_essential_for", "to": "image annotation", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_145", "file_type": "json", "from": "network", "predicate": "improves", "to": "representation of co-salient objects", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_364", "file_type": "json", "from": "network", "predicate": "outputs", "to": "similarity value", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_366", "file_type": "json", "from": "network", "predicate": "achieves_results", "to": "state of the art", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_366", "file_type": "json", "from": "network", "predicate": "fine-tuned_on", "to": "small target data set", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_366", "file_type": "json", "from": "network", "predicate": "trained_on", "to": "large data set", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_145", "file_type": "json", "from": "neighbors", "predicate": "suppresses", "to": "common background regions", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_146", "file_type": "json", "from": "co-salience scores", "predicate": "calculated_by", "to": "intra-image contrast", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_146", "file_type": "json", "from": "co-salience scores", "predicate": "calculated_by", "to": "intra-group consistency", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_146", "file_type": "json", "from": "window-level co-salience scores", "predicate": "converted_to", "to": "superpixel-level co-salience maps", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_146", "file_type": "json", "from": "superpixel-level co-salience maps", "predicate": "generated_by", "to": "foreground region agreement strategy", "type": "conceptual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_146", "file_type": "json", "from": "Bayesian formulation", "predicate": "integrates", "to": "intra-image contrast", "type": "conceptual", "width": 0.83}, {"arrows": "to", "chunk_id": "doc_0_chunk_146", "file_type": "json", "from": "Bayesian formulation", "predicate": "integrates", "to": "intra-group consistency", "type": "conceptual", "width": 0.83}, {"arrows": "to", "chunk_id": "doc_0_chunk_147", "file_type": "json", "from": "l-level co-saliency maps", "predicate": "achieves", "to": "foreground region agreement", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_147", "file_type": "json", "from": "proposed approach", "predicate": "demonstrates", "to": "consistent performance gain", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_159", "file_type": "json", "from": "proposed approach", "predicate": "demonstrates_superiority_over", "to": "state-of-the-art hashing methods", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_311", "file_type": "json", "from": "proposed approach", "predicate": "outperforms", "to": "current state of the art", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_311", "file_type": "json", "from": "proposed approach", "predicate": "achieves", "to": "robust object discovery", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_511", "file_type": "json", "from": "proposed approach", "predicate": "shows superiority over", "to": "state-of-the-arts", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_657", "file_type": "json", "from": "proposed approach", "predicate": "achieves", "to": "state-of-the-art performance", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_657", "file_type": "json", "from": "proposed approach", "predicate": "operates on", "to": "fluorescence microscopy cell images", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_657", "file_type": "json", "from": "proposed approach", "predicate": "operates on", "to": "UCSD pedestrians", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_657", "file_type": "json", "from": "proposed approach", "predicate": "operates on", "to": "small animals", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_657", "file_type": "json", "from": "proposed approach", "predicate": "operates on", "to": "insects", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_723", "file_type": "json", "from": "proposed approach", "predicate": "is_effective", "to": "Visual Object Tracking", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_723", "file_type": "json", "from": "proposed approach", "predicate": "compared_to", "to": "state-of-the-art trackers", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_147", "file_type": "json", "from": "Co-salient object detection", "predicate": "addresses", "to": "multiple images", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_147", "file_type": "json", "from": "Unified approach", "predicate": "employs", "to": "low rank matrix recovery", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_147", "file_type": "json", "from": "iCoseg", "predicate": "provides", "to": "intelligent scribble guidance", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_147", "file_type": "json", "from": "Co-salience detection", "predicate": "relates to", "to": "Convolutional Neural Networks (CNNs)", "type": "conceptual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_147", "file_type": "json", "from": "Co-salience detection", "predicate": "relates to", "to": "Image Group Consistency", "type": "conceptual", "width": 0.7}, {"arrows": "to", "chunk_id": "doc_0_chunk_217", "file_type": "json", "from": "Convolutional Neural Networks (CNNs)", "predicate": "applied_to", "to": "images", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_225", "file_type": "json", "from": "Convolutional Neural Networks (CNNs)", "predicate": "improves", "to": "image recognition accuracy", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_225", "file_type": "json", "from": "Convolutional Neural Networks (CNNs)", "predicate": "increasingly", "to": "complex", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_225", "file_type": "json", "from": "Convolutional Neural Networks (CNNs)", "predicate": "increasingly", "to": "time-consuming", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_265", "file_type": "json", "from": "Convolutional Neural Networks (CNNs)", "predicate": "used_in", "to": "Pose Estimation", "type": "conceptual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_265", "file_type": "json", "from": "Convolutional Neural Networks (CNNs)", "predicate": "used_in", "to": "Object Detection", "type": "conceptual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_265", "file_type": "json", "from": "Convolutional Neural Networks (CNNs)", "predicate": "analyzed_in", "to": "Song_Joint_Multi-Feature_Spatial_2015_CVPR_paper", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_326", "file_type": "json", "from": "Convolutional Neural Networks (CNNs)", "predicate": "used_in", "to": "Deep Convolutional Networks", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_551", "file_type": "json", "from": "Convolutional Neural Networks (CNNs)", "predicate": "used in", "to": "ImageNet classification", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_551", "file_type": "json", "from": "Convolutional Neural Networks (CNNs)", "predicate": "uses", "to": "deep learning", "type": "conceptual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_147", "file_type": "json", "from": "Bayesian Formulation", "predicate": "is used in", "to": "salient object detection", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_485", "file_type": "json", "from": "salient object detection", "predicate": "uses", "to": "discriminative regional feature integration", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_147", "file_type": "json", "from": "Visual Attention", "predicate": "improves", "to": "co-salience detection", "type": "conceptual", "width": 0.78}, {"arrows": "to", "chunk_id": "doc_0_chunk_466", "file_type": "json", "from": "Visual Attention", "predicate": "related_to", "to": "Binary Linear Integer Programming", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_148", "file_type": "json", "from": "alient object detection", "predicate": "presented_at", "to": "CVPR (Conference)", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_148", "file_type": "json", "from": "Xie, Y.", "predicate": "authored", "to": "Bayesian salience", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_148", "file_type": "json", "from": "Bayesian salience", "predicate": "utilizes", "to": "low and mid level cues", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_148", "file_type": "json", "from": "Han, J.", "predicate": "authored", "to": "object-oriented visual salieny detection framework", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_148", "file_type": "json", "from": "object-oriented visual salieny detection framework", "predicate": "based_on", "to": "sparse coding representations", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_148", "file_type": "json", "from": "Rubinstein, M.", "predicate": "authored", "to": "joint object discovery", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_148", "file_type": "json", "from": "joint object discovery", "predicate": "utilizes", "to": "internet images", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_148", "file_type": "json", "from": "Jiang, H.", "predicate": "authored", "to": "salient object segmentation", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_148", "file_type": "json", "from": "salient object segmentation", "predicate": "incorporates", "to": "context prior", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_149", "file_type": "json", "from": "IEEE Trans. Image Process.", "predicate": "is_journal_of", "to": "Image Processing", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_200", "file_type": "json", "from": "Image Processing", "predicate": "includes_applications", "to": "Stereo/Inpainting", "type": "factual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_149", "file_type": "json", "from": "is_author_of", "predicate": "writes", "to": "Cong Zhang", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_149", "file_type": "json", "from": "is_author_of", "predicate": "writes", "to": "Hongsheng Li", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_149", "file_type": "json", "from": "is_author_of", "predicate": "writes", "to": "Xiaogang Wang", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_149", "file_type": "json", "from": "is_author_of", "predicate": "writes", "to": "Xiaokang Yang", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_150", "file_type": "json", "from": "Cong Zhang", "predicate": "author_of", "to": "Cross-Scene Crowd Counting", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_156", "file_type": "json", "from": "Cong Zhang", "predicate": "affiliated_with", "to": "Institute of Image Communication and Network Engineering, Shanghai Jiao Tong University", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_156", "file_type": "json", "from": "Hongsheng Li", "predicate": "affiliated_with", "to": "Department of Electronic Engineering, The Chinese University of Hong Kong", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_204", "file_type": "json", "from": "Hongsheng Li", "predicate": "authored", "to": "Saliency Detection by Multi-Context Deep Learning", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_208", "file_type": "json", "from": "Hongsheng Li", "predicate": "affiliated_with", "to": "Department of Electronic Engineering", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_208", "file_type": "json", "from": "Hongsheng Li", "predicate": "email", "to": "hsli@ee.cuhk.edu.hk", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_150", "file_type": "json", "from": "Xiaogang Wang", "predicate": "author_of", "to": "Cross-Scene Crowd Counting", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_156", "file_type": "json", "from": "Xiaogang Wang", "predicate": "affiliated_with", "to": "Department of Electronic Engineering, The Chinese University of Hong Kong", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_204", "file_type": "json", "from": "Xiaogang Wang", "predicate": "authored", "to": "Saliency Detection by Multi-Content Deep Learning", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_208", "file_type": "json", "from": "Xiaogang Wang", "predicate": "affiliated_with", "to": "Shenzhen Institutes of Advanced Technology", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_459", "file_type": "json", "from": "Xiaogang Wang", "predicate": "author_of", "to": "Deeply Learned Attributes", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_150", "file_type": "json", "from": "Xiaokang Yang", "predicate": "author_of", "to": "Cross-Scene Crowd Counting", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_156", "file_type": "json", "from": "Xiaokang Yang", "predicate": "affiliated_with", "to": "Institute of Image Communication and Network Engineering, Shanghai Jiao Tong University", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_157", "file_type": "json", "from": "Xiaokang Yang", "predicate": "affiliated_with", "to": "Chinese University of Hong Kong", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_157", "file_type": "json", "from": "Xiaokang Yang", "predicate": "email", "to": "xgwang@ee.cuhk.edu.hk", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_157", "file_type": "json", "from": "Xiaokang Yang", "predicate": "email", "to": "xk yang@sjtu.edu.cn", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_334", "file_type": "json", "from": "Xiaokang Yang", "predicate": "is_author_of", "to": "Motion Part Regularization", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_149", "file_type": "json", "from": "Automatic salient object segmentation", "predicate": "presented_at", "to": "BMVC", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_149", "file_type": "json", "from": "Self-Adaptively Weighted Co-Saliency Detection", "predicate": "published_in", "to": "IEEE Trans. Image Process.", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_150", "file_type": "json", "from": "Cross-Scene Crowd Counting", "predicate": "publication_venue", "to": "CVPR", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_150", "file_type": "json", "from": "Cross-Scene Crowd Counting", "predicate": "year", "to": "2015", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_150", "file_type": "json", "from": "Cross-Scene Crowd Counting", "predicate": "method", "to": "Deep Convolutional Neural Networks", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_150", "file_type": "json", "from": "Zhang_Cross-Scene_Crowd_Counting_2015_CVPR_paper.pdf", "predicate": "document_of", "to": "Cross-Scene Crowd Counting", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_151", "file_type": "json", "from": "Cross-scene crowd counting", "predicate": "is_task", "to": "crowd counting", "type": "conceptual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_151", "file_type": "json", "from": "Cross-scene crowd counting", "predicate": "requires", "to": "no laborious data annotation", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_151", "file_type": "json", "from": "existing crowd counting methods", "predicate": "experiences", "to": "significant performance drop", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_151", "file_type": "json", "from": "deep convolutional neural network (CNN)", "predicate": "is_used_for", "to": "crowd counting", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_151", "file_type": "json", "from": "deep convolutional neural network (CNN)", "predicate": "is_trained_with", "to": "crowd density", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_274", "file_type": "json", "from": "deep convolutional neural network (CNN)", "predicate": "improves", "to": "object detection benchmarks", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_151", "file_type": "json", "from": "switchable learning approach", "predicate": "aims_to_achieve", "to": "better local optimum", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_151", "file_type": "json", "from": "data-driven method", "predicate": "is_used_to", "to": "fine-tune CNN model", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_152", "file_type": "json", "from": "CNN model", "predicate": "is", "to": "trained", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_153", "file_type": "json", "from": "Crowd Counting", "predicate": "requires", "to": "Deep Convolutional Neural Networks (CNNs)", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_153", "file_type": "json", "from": "Crowd Counting", "predicate": "addresses", "to": "object counting", "type": "conceptual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_153", "file_type": "json", "from": "Crowd Counting", "predicate": "addressed_by", "to": "Chen et al. (2013)", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_657", "file_type": "json", "from": "Crowd Counting", "predicate": "is_topic", "to": "Computer Vision", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_206", "file_type": "json", "from": "Deep Convolutional Neural Networks (CNNs)", "predicate": "used_in", "to": "Salient Object Detection", "type": "conceptual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_276", "file_type": "json", "from": "Deep Convolutional Neural Networks (CNNs)", "predicate": "is_used_in", "to": "Object Detection", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_276", "file_type": "json", "from": "Deep Convolutional Neural Networks (CNNs)", "predicate": "related_to", "to": "Dimensionality Reduction", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_153", "file_type": "json", "from": "Chen et al. (2013)", "predicate": "introduces", "to": "cumulative attribute space", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_153", "file_type": "json", "from": "Chen et al. (2013)", "predicate": "evaluates", "to": "cross-scene crowd counting methods", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_153", "file_type": "json", "from": "cumulative attribute space", "predicate": "relevant_to", "to": "crowd density estimation", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_153", "file_type": "json", "from": "Lempitsky \u0026 Zisserman (2010)", "predicate": "presents", "to": "object counting", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_153", "file_type": "json", "from": "Chen et al. (2012)", "predicate": "focuses_on", "to": "feature mining", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_153", "file_type": "json", "from": "feature mining", "predicate": "applied_to", "to": "localized crowd counting", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_154", "file_type": "json", "from": "localized crowd counting", "predicate": "is_aspect_of", "to": "research", "type": "conceptual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_154", "file_type": "json", "from": "Loy et al. (2012) research", "predicate": "focuses_on", "to": "localized crowd counting", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_154", "file_type": "json", "from": "An et al. (2007) research", "predicate": "demonstrates", "to": "kernel ridge regression", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_154", "file_type": "json", "from": "kernel ridge regression", "predicate": "is_technique", "to": "vision tasks", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_154", "file_type": "json", "from": "Kai et al. (2014) research", "predicate": "introduces", "to": "fully convolutional network", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_154", "file_type": "json", "from": "fully convolutional network", "predicate": "is_used_for", "to": "crowd segmentation", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_154", "file_type": "json", "from": "fully convolutional network", "predicate": "is_a", "to": "neural network", "type": "conceptual", "width": 0.91}, {"arrows": "to", "chunk_id": "doc_0_chunk_154", "file_type": "json", "from": "crowd segmentation", "predicate": "is_field_of", "to": "computer vision", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_154", "file_type": "json", "from": "Kong et al. (2006) research", "predicate": "addresses", "to": "viewpoint invariance", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_155", "file_type": "json", "from": "Kong et al. (2006)", "predicate": "addresses", "to": "viewpoint invariance in crowd counting", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_155", "file_type": "json", "from": "viewpoint invariance in crowd counting", "predicate": "is_a", "to": "practical consideration", "type": "conceptual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_155", "file_type": "json", "from": "Jing et al. (2015)", "predicate": "explores", "to": "deep learning for attribute extraction", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_155", "file_type": "json", "from": "attribute extraction", "predicate": "contributes_to", "to": "crowd scene understanding", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_155", "file_type": "json", "from": "Fiaschi et al. (2012)", "predicate": "investigates", "to": "regression forest for counting", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_155", "file_type": "json", "from": "regression forest for counting", "predicate": "is_alternative_to", "to": "neural networks", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_553", "file_type": "json", "from": "neural networks", "predicate": "used_for", "to": "dimensionality reduction", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_155", "file_type": "json", "from": "Loy et al. (2013)", "predicate": "explores", "to": "semi-supervised learning for crowd counting", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_155", "file_type": "json", "from": "Loy et al. (2013)", "predicate": "explores", "to": "transfer learning for crowd counting", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_155", "file_type": "json", "from": "ICPR", "predicate": "is_a", "to": "conference", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_155", "file_type": "json", "from": "ICCV", "predicate": "is_a", "to": "conference", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_194", "file_type": "json", "from": "ICCV", "predicate": "published", "to": "Class-specific material categorisation", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_318", "file_type": "json", "from": "ICCV", "predicate": "is_a", "to": "Computer Vision Conference", "type": "conceptual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_342", "file_type": "json", "from": "ICCV", "predicate": "published", "to": "1841\u20131848", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_526", "file_type": "json", "from": "ICCV", "predicate": "publication_venue", "to": "Ground truth dataset", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_156", "file_type": "json", "from": "Gong, S.", "predicate": "authored", "to": "From semi-supervised to transfer counting of crowds", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_156", "file_type": "json", "from": "Xiang, T.", "predicate": "authored", "to": "From semi-supervised to transfer counting of crowds", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_269", "file_type": "json", "from": "Lowe, D. G.", "predicate": "authored", "to": "Distinctive image features from scale-invariant keypoints", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_269", "file_type": "json", "from": "Distinctive image features from scale-invariant keypoints", "predicate": "used_for", "to": "feature extraction", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_156", "file_type": "json", "from": "SIFT features", "predicate": "introduced_in", "to": "Distinctive image features from scale-invariant keypoints", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_156", "file_type": "json", "from": "SIFT features", "predicate": "is_used_in", "to": "crowd analysis", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_249", "file_type": "json", "from": "SIFT features", "predicate": "is_building_block_for", "to": "image representation techniques", "type": "conceptual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_249", "file_type": "json", "from": "SIFT features", "predicate": "is_a", "to": "keypoints", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_370", "file_type": "json", "from": "SIFT features", "predicate": "described_in", "to": "IJCV, 60:91\u2013110, 2004", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_552", "file_type": "json", "from": "SIFT features", "predicate": "is_component_of", "to": "image processing tasks", "type": "conceptual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_551", "file_type": "json", "from": "SIFT features", "predicate": "is a", "to": "key component", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_552", "file_type": "json", "from": "SIFT features", "predicate": "is_a", "to": "distinctive image features", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_659", "file_type": "json", "from": "SIFT features", "predicate": "used for", "to": "feature detection", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_659", "file_type": "json", "from": "SIFT features", "predicate": "used for", "to": "feature matching", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_157", "file_type": "json", "from": "Yongzhen Huang", "predicate": "author_of", "to": "Deep SemanticRanking Based Hashing", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_163", "file_type": "json", "from": "Yongzhen Huang", "predicate": "affiliated_with", "to": "Center for Research on Intelligent Perception and Computing", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_157", "file_type": "json", "from": "Liang Wang", "predicate": "author_of", "to": "Deep Semantic Ranking Based Hashing", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_163", "file_type": "json", "from": "Liang Wang", "predicate": "affiliated_with", "to": "Center for Research on Intelligent Perception and Computing", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_157", "file_type": "json", "from": "Deep Semantic Ranking Based Hashing", "predicate": "is_paper_in", "to": "CVPR", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_157", "file_type": "json", "from": "Deep Semantic Ranking Based Hashing", "predicate": "addresses", "to": "Multi-Label Image Retrieval", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_157", "file_type": "json", "from": "Tieniu Tan", "predicate": "author_of", "to": "Deep Semantic Ranking Based Hashing", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_163", "file_type": "json", "from": "Tieniu Tan", "predicate": "affiliated_with", "to": "Center for Research on Intelligent Perception and Computing", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_157", "file_type": "json", "from": "Institute of Image Communication and Network Engineering", "predicate": "affiliated_with", "to": "Shanghai Jiao Tong University", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_159", "file_type": "json", "from": "deep hash functions", "predicate": "overcomes_limitation_of", "to": "hand-crafted features", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_159", "file_type": "json", "from": "deep hash functions", "predicate": "learns_from", "to": "ranking list", "type": "causal", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_159", "file_type": "json", "from": "ranking list", "predicate": "encodes", "to": "multilevel similarity information", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_159", "file_type": "json", "from": "multilevel similarity information", "predicate": "is_encoded_by", "to": "ranking list", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_159", "file_type": "json", "from": "semantic representation", "predicate": "is_limited_by", "to": "hand-crafted features", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_159", "file_type": "json", "from": "hash codes", "predicate": "derived_from", "to": "semantic representation", "type": "factual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_160", "file_type": "json", "from": "Deep Hash Functions", "predicate": "guided_by", "to": "Similarity Information", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_160", "file_type": "json", "from": "Proposed Approach", "predicate": "compared_to", "to": "Hashing Methods", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_570", "file_type": "json", "from": "ImageNet Classification", "predicate": "uses", "to": "Deep Convolutional Neural Networks", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_160", "file_type": "json", "from": "Deep Convolutional Ranking", "predicate": "addresses", "to": "Multi-label Image Annotation", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_160", "file_type": "json", "from": "Iterative Quantization", "predicate": "aims_to_learn", "to": "Binary Codes", "type": "conceptual", "width": 0.91}, {"arrows": "to", "chunk_id": "doc_0_chunk_160", "file_type": "json", "from": "Binary Codes", "predicate": "used_for", "to": "Image Retrieval", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_440", "file_type": "json", "from": "Image Retrieval", "predicate": "uses", "to": "Hashing", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_570", "file_type": "json", "from": "Krizhevsky", "predicate": "authors", "to": "ImageNet Classification", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_328", "file_type": "json", "from": "Krizhevsky", "predicate": "co_authors_with", "to": "Hinton", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_551", "file_type": "json", "from": "Krizhevsky", "predicate": "authored", "to": "Imaginet classification with deep convolutional neural networks", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_160", "file_type": "json", "from": "Gong", "predicate": "authored", "to": "Deep Convolutional Ranking", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_160", "file_type": "json", "from": "Gong", "predicate": "authored", "to": "Iterative Quantization", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_161", "file_type": "json", "from": "Gong", "predicate": "authored", "to": "Deep convolutional ranking", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_443", "file_type": "json", "from": "Gong", "predicate": "authored", "to": "A Maximum Entropy Feature Descriptor", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_602", "file_type": "json", "from": "Gong", "predicate": "authored", "to": "sequential subset selection", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_161", "file_type": "json", "from": "Perronnin", "predicate": "authored", "to": "Iterative quantization", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_161", "file_type": "json", "from": "Iterative quantization", "predicate": "applies", "to": "binary codes", "type": "conceptual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_511", "file_type": "json", "from": "binary codes", "predicate": "distribute evenly", "to": "each bit", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_161", "file_type": "json", "from": "Norouzi", "predicate": "authored", "to": "Hamming distance metric learning", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_443", "file_type": "json", "from": "Norouzi", "predicate": "authored", "to": "Minimal loss hashing", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_161", "file_type": "json", "from": "Hamming distance metric learning", "predicate": "presented_in", "to": "Advances in Neural Information Processing Systems", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_161", "file_type": "json", "from": "Semi-supervised hashing", "predicate": "aims_for", "to": "scalable image retrieval", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_514", "file_type": "json", "from": "Semi-supervised hashing", "predicate": "is_presented_in", "to": "*CVPR*", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_627", "file_type": "json", "from": "Semi-supervised hashing", "predicate": "explores", "to": "techniques", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_161", "file_type": "json", "from": "Torralba", "predicate": "authored", "to": "Small codes", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_302", "file_type": "json", "from": "Torralba", "predicate": "authored", "to": "Learning to Predict Where Humans Look", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_161", "file_type": "json", "from": "Small codes", "predicate": "used_for", "to": "image databases", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_161", "file_type": "json", "from": "Deep convolutional ranking", "predicate": "addresses", "to": "multilabel image annotation", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_162", "file_type": "json", "from": "Deep convolutional ranking", "predicate": "is_application_of", "to": "multilable image annotation", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_162", "file_type": "json", "from": "Gong, Y.", "predicate": "authored", "to": "Deep convolutional ranking", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_162", "file_type": "json", "from": "Krizhevsky, A.", "predicate": "authored", "to": "One weird trick", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_218", "file_type": "json", "from": "Krizhevsky, A.", "predicate": "authored", "to": "ImageNet classification with deep convolutional neural networks", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_277", "file_type": "json", "from": "Krizhevsky, A.", "predicate": "authors", "to": "ImageNet Classification", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_593", "file_type": "json", "from": "Krizhevsky, A.", "predicate": "authored", "to": "ImageNet classification", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_594", "file_type": "json", "from": "Krizhevsky, A.", "predicate": "co_author_of", "to": "Sutskever, I.", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_594", "file_type": "json", "from": "Krizhevsky, A.", "predicate": "co_author_of", "to": "Hinton, G. E.", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_162", "file_type": "json", "from": "One weird trick", "predicate": "describes", "to": "parallelizing convolutional neural networks", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_443", "file_type": "json", "from": "Minimal loss hashing", "predicate": "introduced", "to": "compact binary codes", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_442", "file_type": "json", "from": "compact binary codes", "predicate": "minimizes", "to": "loss", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_162", "file_type": "json", "from": "Lin, G.", "predicate": "authored", "to": "Optimizing ranking measures", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_162", "file_type": "json", "from": "Optimizing ranking measures", "predicate": "focuses_on", "to": "compact binary code learning", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_162", "file_type": "json", "from": "Fang Zhao", "predicate": "is_affiliated_with", "to": "Center for Research on Intelligent Perception and Computing", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_163", "file_type": "json", "from": "Center for Research on Intelligent Perception and Computing", "predicate": "is_part_of", "to": "Institute of Automation", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_163", "file_type": "json", "from": "Institute of Automation", "predicate": "is_part_of", "to": "Chinese Academy of Sciences", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_589", "file_type": "json", "from": "Institute of Automation", "predicate": "houses", "to": "Center for Biometrics and Security Research", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_589", "file_type": "json", "from": "Institute of Automation", "predicate": "houses", "to": "National Laboratory of Pattern Recognition", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_163", "file_type": "json", "from": "David Perra", "predicate": "is_author_of", "to": "Adaptive Eye-Camera Calibration", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_168", "file_type": "json", "from": "David Perra", "predicate": "affiliated_with", "to": "Google Inc.", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_163", "file_type": "json", "from": "Adaptive Eye-Camera Calibration", "predicate": "is_paper", "to": "cvpr_papers", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_163", "file_type": "json", "from": "Rohit Kumar Gupta", "predicate": "is_author_of", "to": "Adaptive Eye-Camera Calibration", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_169", "file_type": "json", "from": "Rohit Kumar Gupta", "predicate": "affiliated_with", "to": "The University of North Carolina at Chapel Hill", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_169", "file_type": "json", "from": "Rohit Kumar Gupta", "predicate": "email", "to": "rkgupta@cs.unc.edu", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_163", "file_type": "json", "from": "fang.zhao@nlpr.ia.ac.cn", "predicate": "affiliated_with", "to": "Center for Research on Intelligent Perception and Computing", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_163", "file_type": "json", "from": "Perra_Adaptive_Eye-Camera_Calibration_2015_CVPR_paper.pdf", "predicate": "is_file", "to": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_164", "file_type": "json", "from": "calibration scheme", "predicate": "solves_for", "to": "globally optimal model", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_164", "file_type": "json", "from": "calibration scheme", "predicate": "addresses", "to": "changes in calibration", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_164", "file_type": "json", "from": "calibration scheme", "predicate": "calculates", "to": "locally optimal eye-device transformation", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_164", "file_type": "json", "from": "calibration scheme", "predicate": "computes_from", "to": "local window of previous frames", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_164", "file_type": "json", "from": "calibration scheme", "predicate": "is", "to": "continuous", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_164", "file_type": "json", "from": "calibration scheme", "predicate": "is", "to": "locally optimal", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_166", "file_type": "json", "from": "calibration scheme", "predicate": "outperforms", "to": "state of the art systems", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_166", "file_type": "json", "from": "calibration scheme", "predicate": "is less restrictive to", "to": "environment", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_164", "file_type": "json", "from": "calibration schemes", "predicate": "performed_on", "to": "per-user basis", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_165", "file_type": "json", "from": "interest regions", "predicate": "located within", "to": "user\u2019s environment", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_164", "file_type": "json", "from": "eye-device transformation", "predicate": "is", "to": "locally optimal", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_165", "file_type": "json", "from": "proposed calibration scheme", "predicate": "outperforms", "to": "existing state of the art systems", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_165", "file_type": "json", "from": "proposed calibration scheme", "predicate": "is", "to": "less restrictive", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_165", "file_type": "json", "from": "proposed calibration scheme", "predicate": "is less restrictive to", "to": "environment", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_166", "file_type": "json", "from": "Alnajar et al.", "predicate": "published", "to": "Calibration-free gaze estimation", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_166", "file_type": "json", "from": "Calibration-free gaze estimation", "predicate": "uses", "to": "human gaze patterns", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_166", "file_type": "json", "from": "Chen and Ji", "predicate": "published", "to": "Probabilistic gaze estimation", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_166", "file_type": "json", "from": "Probabilistic gaze estimation", "predicate": "avoids", "to": "active personal calibration", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_166", "file_type": "json", "from": "Corno et al.", "predicate": "published", "to": "cost-effective solution", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_166", "file_type": "json", "from": "cost-effective solution", "predicate": "is for", "to": "eye-gaze assistive technology", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_166", "file_type": "json", "from": "eye-camera calibration", "predicate": "is related to", "to": "gaze tracking", "type": "conceptual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_167", "file_type": "json", "from": "F. Corno", "predicate": "presented_at", "to": "IEEE International Conference on Multimedia and Expo", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_167", "file_type": "json", "from": "IEEE International Conference on Multimedia and Expo", "predicate": "hosts", "to": "research on assistive technology", "type": "factual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_167", "file_type": "json", "from": "E. Guestrin", "predicate": "researched", "to": "remote gaze estimation", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_167", "file_type": "json", "from": "remote gaze estimation", "predicate": "uses", "to": "pupil center", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_167", "file_type": "json", "from": "D. Hansen", "predicate": "surveyed", "to": "models for eyes and gaze", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_167", "file_type": "json", "from": "J. Harel", "predicate": "developed", "to": "graph-based visual saliency", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_207", "file_type": "json", "from": "J. Harel", "predicate": "authored", "to": "Graph-based visual salience", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_167", "file_type": "json", "from": "graph-based visual saliency", "predicate": "is_a", "to": "neural processing method", "type": "conceptual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_167", "file_type": "json", "from": "X. Hou", "predicate": "researched", "to": "image signature", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_168", "file_type": "json", "from": "X. Hou", "predicate": "published_in", "to": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_167", "file_type": "json", "from": "image signature", "predicate": "highlights", "to": "sparse salient regions", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_167", "file_type": "json", "from": "IEEE Transactions on Biomedical Engineering", "predicate": "publishes", "to": "research on remote gaze estimation", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_168", "file_type": "json", "from": "U. Lahiri", "predicate": "published_in", "to": "Neural Systems and Rehabilitation Engineering, IEEE Transactions on", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_168", "file_type": "json", "from": "U. Lahiri", "predicate": "published_in", "to": "Virtual Rehabilitation (ICVR)", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_168", "file_type": "json", "from": "U. Lahiri", "predicate": "developed_system_for", "to": "children with autism", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_168", "file_type": "json", "from": "U. Lahiri", "predicate": "developed_system_for", "to": "social communication", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_168", "file_type": "json", "from": "R. Kumar", "predicate": "published_in", "to": "Computer Vision and Pattern Recognition", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_168", "file_type": "json", "from": "Jan-Micheal Frahm", "predicate": "affiliated_with", "to": "The University of North Carolian at Chapel Hill", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_169", "file_type": "json", "from": "Jan-Micheal Frahm", "predicate": "affiliated_with", "to": "The University of North Carolina at Chapel Hill", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_169", "file_type": "json", "from": "Jan-Micheal Frahm", "predicate": "email", "to": "jmf@cs.unc.edu", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_169", "file_type": "json", "from": "Nianuan Jiang", "predicate": "author_of", "to": "Direct Structure Estimation for 3D Reconstruction", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_172", "file_type": "json", "from": "Nianuan Jiang", "predicate": "affiliated_with", "to": "Advanced Digital Sciences Center", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_173", "file_type": "json", "from": "Nianuan Jiang", "predicate": "affiliated_with", "to": "Advanced Digital Sciences Center, Singapore", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_169", "file_type": "json", "from": "Direct Structure Estimation for 3D Reconstruction", "predicate": "published_in", "to": "CVPR", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_169", "file_type": "json", "from": "Wen-Yan Lin", "predicate": "author_of", "to": "Direct Structure Estimated for 3D Reconstruction", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_172", "file_type": "json", "from": "Wen-Yan Lin", "predicate": "affiliated_with", "to": "Advanced Digital Sciences Center", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_173", "file_type": "json", "from": "Wen-Yan Lin", "predicate": "affiliated_with", "to": "Advanced Digital Sciences Center, Singapore", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_169", "file_type": "json", "from": "Minh N. Do", "predicate": "author_of", "to": "Direct Structure Estimation for 3D Reconstruction", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_172", "file_type": "json", "from": "Minh N. Do", "predicate": "affiliated_with", "to": "Advanced Digital Sciences Center", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_173", "file_type": "json", "from": "Minh N. Do", "predicate": "affiliated_with", "to": "University of Illinois at Urbana-Champaign", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_169", "file_type": "json", "from": "Jiangbo Lu", "predicate": "author_of", "to": "Direct Structure Estimation for 3D Reconstruction", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_173", "file_type": "json", "from": "Jiangbo Lu", "predicate": "affiliated_with", "to": "Advanced Digital Sciences Center, Singapore", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_169", "file_type": "json", "from": "Jiang_Direct_Structure_Estimation_2015_CVPR_paper.pdf", "predicate": "documents", "to": "Direct Structure Estimation for 3D Reconstruction", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_170", "file_type": "json", "from": "Structure from Motion (SFM)", "predicate": "requires", "to": "camera pose estimation", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_171", "file_type": "json", "from": "Structure from Motion (SFM)", "predicate": "incorporates", "to": "Euclidean Rigidity", "type": "conceptual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_171", "file_type": "json", "from": "Structure from Motion (SFM)", "predicate": "solves", "to": "scene reconstruction", "type": "conceptual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_171", "file_type": "json", "from": "Structure from Motion (SFM)", "predicate": "relies on", "to": "Homography Estimation", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_432", "file_type": "json", "from": "camera pose estimation", "predicate": "is_part_of", "to": "visual SLAM", "type": "conceptual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_170", "file_type": "json", "from": "Euclidean Rigidity", "predicate": "enables", "to": "scene structure recovery", "type": "conceptual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_170", "file_type": "json", "from": "Direct StructureEstimation (DSE)", "predicate": "combines with", "to": "homography estimation", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_170", "file_type": "json", "from": "Direct Structure Estimation (DSE)", "predicate": "provides", "to": "formulation for scene structure recovery", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_170", "file_type": "json", "from": "Direct Structure Estimation (DSE)", "predicate": "works well for", "to": "recovering scene structure", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_170", "file_type": "json", "from": "Direct Structure Estimation (DSE)", "predicate": "works well for", "to": "recovering camera poses", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_171", "file_type": "json", "from": "Direct Structure Estimation (DSE)", "predicate": "is a method in", "to": "Structure from Motion (SFM)", "type": "conceptual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_170", "file_type": "json", "from": "scene structure", "predicate": "is recovered from", "to": "sideway motion", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_170", "file_type": "json", "from": "scene structure", "predicate": "occurs in", "to": "planar or general man-made scenes", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_171", "file_type": "json", "from": "Camera Pose Estimation", "predicate": "is part of", "to": "Structure from Motion (SFM)", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_287", "file_type": "json", "from": "Camera Pose Estimation", "predicate": "is_related_to", "to": "Non-Rigid Structure from Motion", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_171", "file_type": "json", "from": "D. Nist\u00e9r", "predicate": "authored", "to": "solution to five-point relative pose problem", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_171", "file_type": "json", "from": "D. G. Aliaga", "predicate": "authored", "to": "simplifying reconstruction of 3d models", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_171", "file_type": "json", "from": "K. S. Arun", "predicate": "authored", "to": "least-squares fitting of two 3-d point sets", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_171", "file_type": "json", "from": "D. Crandall", "predicate": "authored", "to": "discrete-continuous optimization", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_171", "file_type": "json", "from": "D. W. Eggert", "predicate": "authored", "to": "comparison of four major algorithms", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_172", "file_type": "json", "from": "D. W. Eggert", "predicate": "published", "to": "Estimating 3-d rigid body transformations", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_171", "file_type": "json", "from": "least-squares fitting", "predicate": "is a technique in", "to": "3d point set alignment", "type": "conceptual", "width": 0.7}, {"arrows": "to", "chunk_id": "doc_0_chunk_691", "file_type": "json", "from": "M. A. Fischler", "predicate": "author of", "to": "Random sample consensus", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_691", "file_type": "json", "from": "Random sample consensus", "predicate": "published in", "to": "Communications of the ACM", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_429", "file_type": "json", "from": "Random sample consensus", "predicate": "is_paradigm_for", "to": "model fitting", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_690", "file_type": "json", "from": "Random sample consensus", "predicate": "applies_to", "to": "image analysis", "type": "factual", "width": 0.87}, {"arrows": "to", "chunk_id": "doc_0_chunk_401", "file_type": "json", "from": "Advanced Digital Sciences Center", "predicate": "located_in", "to": "Singapore", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_172", "file_type": "json", "from": "Structure from motion", "predicate": "presented_at", "to": "CVPR", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_172", "file_type": "json", "from": "R. Hartley", "predicate": "published", "to": "In defense of the eight-point algorithm", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_172", "file_type": "json", "from": "H. Isack", "predicate": "co_authored", "to": "Energy-based geometric multi-model fitting", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_172", "file_type": "json", "from": "N. Jiang", "predicate": "co_authored", "to": "A global linear method for camera pose registration", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_173", "file_type": "json", "from": "Stefan Roth", "predicate": "author_of", "to": "Richter_Discriminative_Shape_From_2015_CVPR_supplemental", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_177", "file_type": "json", "from": "Stefan Roth", "predicate": "affiliated_with", "to": "Department of Computer Science, TU Darmstadt", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_178", "file_type": "json", "from": "Stefan Roth", "predicate": "works_at", "to": "TU Darmstadt", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_522", "file_type": "json", "from": "Stefan Roth", "predicate": "affiliated_with", "to": "Adobe Research", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_522", "file_type": "json", "from": "Stefan Roth", "predicate": "authored", "to": "Discriminaitve Shape from Shading", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_526", "file_type": "json", "from": "Stefan Roth", "predicate": "affiliated_with", "to": "Department of Computer Science", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_173", "file_type": "json", "from": "Richter_Discriminative_Shape_From_2015_CVPR_supplemental", "predicate": "publication_venue", "to": "CVPR", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_173", "file_type": "json", "from": "Richter_Discriminative_Shape_From_2015_CVPR_supplemental", "predicate": "publication_year", "to": "2015", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_173", "file_type": "json", "from": "Richter_Discriminative_Shape_From_2015_CVPR_supplemental", "predicate": "describes", "to": "Discriminative Shape from Shading", "type": "conceptual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_174", "file_type": "json", "from": "Shape from shading method", "predicate": "is_compared_to", "to": "other approaches", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_174", "file_type": "json", "from": "Shape from shading method", "predicate": "combines", "to": "local context", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_174", "file_type": "json", "from": "Shape from shading method", "predicate": "uses", "to": "learning framework", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_174", "file_type": "json", "from": "Shape from shading method", "predicate": "achieves", "to": "improved reconstructions", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_174", "file_type": "json", "from": "Shape from shading method", "predicate": "relies_on", "to": "smooth local context", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_174", "file_type": "json", "from": "Results", "predicate": "presented_on", "to": "real images", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_174", "file_type": "json", "from": "Dataset", "predicate": "is", "to": "ground truth dataset", "type": "conceptual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_174", "file_type": "json", "from": "Dataset", "predicate": "contains", "to": "real objects", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_175", "file_type": "json", "from": "Dataset", "predicate": "consists_of", "to": "Real Objects", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_175", "file_type": "json", "from": "Shape from Shading", "predicate": "related_to", "to": "Surface Reconstruction", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_175", "file_type": "json", "from": "Shape from Shading", "predicate": "related_to", "to": "Illumination Estimation", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_175", "file_type": "json", "from": "Shape from Shading", "predicate": "incorporates", "to": "Local and Global Context", "type": "conceptual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_175", "file_type": "json", "from": "Shape from Shading", "predicate": "utilizes", "to": "Machine Learning", "type": "conceptual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_524", "file_type": "json", "from": "Shape from Shading", "predicate": "is_topic", "to": "research area", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_181", "file_type": "json", "from": "Surface Reconstruction", "predicate": "is_topic_of", "to": "Zhang_Light_Field_From_2015_CVPR_paper", "type": "conceptual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_188", "file_type": "json", "from": "Surface Reconstruction", "predicate": "requires", "to": "RGB-D Cameras", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_175", "file_type": "json", "from": "Barron, J. T., \u0026 Malik, J.", "predicate": "contributes_to", "to": "Color Constancy", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_175", "file_type": "json", "from": "Johnson, M. K., \u0026 Adelson, E. H.", "predicate": "addresses", "to": "Shape Estimation", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_175", "file_type": "json", "from": "Johnson, M. K., \u0026 Adelson, E. H.", "predicate": "addresses", "to": "Natural Illumination", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_176", "file_type": "json", "from": "Shape Estimation", "predicate": "addressed in", "to": "Johnson \u0026 Adelson (2011) paper", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_177", "file_type": "json", "from": "Jacobs, D. W.", "predicate": "appears_in_author_list_for", "to": "reference [4]", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_177", "file_type": "json", "from": "Basri, R.", "predicate": "appears_in_author_list_for", "to": "reference [4]", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_525", "file_type": "json", "from": "Stephan R. Richter", "predicate": "affiliated_with", "to": "Department of Computer Science, TU Darmstadt", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_178", "file_type": "json", "from": "Stephan R. Richter", "predicate": "works_at", "to": "TU Darmstadt", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_522", "file_type": "json", "from": "Stephan R. Richter", "predicate": "affiliated_with", "to": "Adobe Research", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_522", "file_type": "json", "from": "Stephan R. Richter", "predicate": "authored", "to": "Discriminaitve Shape from Shading", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_526", "file_type": "json", "from": "Stephan R. Richter", "predicate": "affiliated_with", "to": "Department of Computer Science", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_177", "file_type": "json", "from": "Department of Computer Science, TU Darmstadt", "predicate": "is_affiliation_of", "to": "Stephan R. Richter", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_177", "file_type": "json", "from": "Department of Computer Science, TU Darmstadt", "predicate": "is_affiliation_of", "to": "Stefan Roth", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_526", "file_type": "json", "from": "TU Darmstadt", "predicate": "hosts_department", "to": "Department of Computer Science", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_178", "file_type": "json", "from": "Andr\u00e1s B\u00f3dis-Sz\u0151m\u0151ru", "predicate": "authors", "to": "Superpixel Meshes", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_178", "file_type": "json", "from": "Superpixel Meshes", "predicate": "publication_venue", "to": "CVPR", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_178", "file_type": "json", "from": "Superpixel Meshes", "predicate": "year", "to": "2015", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_178", "file_type": "json", "from": "Superpixel Meshes", "predicate": "describes", "to": "surface reconstruction", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_178", "file_type": "json", "from": "Superpixel Meshes", "predicate": "preserves", "to": "edges", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_178", "file_type": "json", "from": "Hayko Riemenschneider", "predicate": "authors", "to": "Superpixel Meses", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_181", "file_type": "json", "from": "Hayko Riemenschneider", "predicate": "affiliated_with", "to": "ETH Zurich, Computer Vision Lab", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_178", "file_type": "json", "from": "Luc Van Gool", "predicate": "authors", "to": "Superpixel Meshes", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_181", "file_type": "json", "from": "Luc Van Gool", "predicate": "affiliated_with", "to": "PSI-VISICS, KU Leuven", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_250", "file_type": "json", "from": "Luc Van Gool", "predicate": "author_of", "to": "Metric imitation by manifold transfer", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_255", "file_type": "json", "from": "Luc Van Gool", "predicate": "affiliated_with", "to": "VISICS, ESAT/PSI, KU Leuven", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_255", "file_type": "json", "from": "Luc Van Gool", "predicate": "affiliated_with", "to": "Computer Vision Lab, ETH Zurich", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_255", "file_type": "json", "from": "Luc Van Gool", "predicate": "authored", "to": "Privacy Preserving Optics for Miniature Vision Sensors", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_178", "file_type": "json", "from": "Bodis-Szomoru_Superpixel_Meshes_for_2015_CVPR_supplemental.pdf", "predicate": "contains", "to": "Superpixel Meshes", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_179", "file_type": "json", "from": "Multi-View-Stereo methods", "predicate": "aim_for", "to": "highest detail", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_179", "file_type": "json", "from": "surface reconstruction method", "predicate": "is_based_on", "to": "image edges", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_179", "file_type": "json", "from": "surface reconstruction method", "predicate": "is_constrained_by", "to": "second-order smoothness constraints", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_179", "file_type": "json", "from": "meshes", "predicate": "have_quality", "to": "classic MVS surfaces", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_180", "file_type": "json", "from": "meshes", "predicate": "are", "to": "edge-aligned", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_180", "file_type": "json", "from": "meshes", "predicate": "are", "to": "compact", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_180", "file_type": "json", "from": "meshes", "predicate": "aligned_with", "to": "image gradients", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_179", "file_type": "json", "from": "dense depth optimization", "predicate": "occurs_over", "to": "Ground Control Points", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_179", "file_type": "json", "from": "SfM points", "predicate": "used_as", "to": "GCPs", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_180", "file_type": "json", "from": "LiDAR", "predicate": "used_as", "to": "GCPs", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_179", "file_type": "json", "from": "RGB-D", "predicate": "can_be_used_as", "to": "GCPs", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_180", "file_type": "json", "from": "Structure-from-Motion (SfM) points", "predicate": "used_as", "to": "GCPs", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_180", "file_type": "json", "from": "renderings", "predicate": "are", "to": "lightweight", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_180", "file_type": "json", "from": "renderings", "predicate": "are", "to": "per-face flat", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_181", "file_type": "json", "from": "Zhang_Light_Field_From_2015_CVPR_paper", "predicate": "authored_by", "to": "Didyk et.al", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_181", "file_type": "json", "from": "Zhang_Light_Field_From_2015_CVPR_paper", "predicate": "improves", "to": "surface quality", "type": "causal", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_181", "file_type": "json", "from": "Superpixels", "predicate": "is_topic_of", "to": "Zhang_Light_Light_Field_From_2015_CVPR_paper", "type": "conceptual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_181", "file_type": "json", "from": "Structure-from-Motion (SfM)", "predicate": "is_topic_of", "to": "Zhang_Light_Field_From_2015_CVPR_paper", "type": "conceptual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_181", "file_type": "json", "from": "Mesh Generation", "predicate": "is_topic_of", "to": "Zhang_Light_Field_From_2015_CVPR_paper", "type": "conceptual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_181", "file_type": "json", "from": "Edge-Preerving Methods", "predicate": "is_topic_of", "to": "Zhang_Light_Field_From_2015_CVPR_paper", "type": "conceptual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_181", "file_type": "json", "from": "Andr\u00b4as B\u00b4odis-Szomor\u00b4u", "predicate": "affiliated_with", "to": "ETH Zurich, Computer Vision Lab", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_182", "file_type": "json", "from": "intermediate views", "predicate": "uses", "to": "light field synthesis", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_183", "file_type": "json", "from": "Depth Estimation", "predicate": "improves", "to": "existing methods", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_187", "file_type": "json", "from": "existing methods", "predicate": "use", "to": "pixel-coordinates", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_493", "file_type": "json", "from": "existing methods", "predicate": "has_issue", "to": "failure to enforce consistency constraints", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_183", "file_type": "json", "from": "View Synthesis", "predicate": "is_a", "to": "technique", "type": "conceptual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_183", "file_type": "json", "from": "Light Field Reconstruction", "predicate": "is_a", "to": "technique", "type": "conceptual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_183", "file_type": "json", "from": "Disparity Refinement", "predicate": "is_a", "to": "technique", "type": "conceptual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_183", "file_type": "json", "from": "Iterative View Generation", "predicate": "is_a", "to": "technique", "type": "conceptual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_183", "file_type": "json", "from": "Jan Hosang", "predicate": "is_author_of", "to": "Taking a Deeper Look at Pedestrians", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_183", "file_type": "json", "from": "Taking a Deeper Look at Pedestrians", "predicate": "is_paper", "to": "CVPR", "type": "factual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_183", "file_type": "json", "from": "Mohamed Omran", "predicate": "is_author_of", "to": "Taking a Deeper Look at Pedestrians", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_186", "file_type": "json", "from": "Mohamed Omran", "predicate": "affiliated_with", "to": "Max Planck Institute for Informatics", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_184", "file_type": "json", "from": "CifarNet", "predicate": "used_in", "to": "pedestrian detection", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_184", "file_type": "json", "from": "AlexNet", "predicate": "used_in", "to": "pedestrian detection", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_246", "file_type": "json", "from": "AlexNet", "predicate": "introduced_in", "to": "Krizhevsky, A., Sutskever, I., and Hinton, G. E.", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_247", "file_type": "json", "from": "AlexNet", "predicate": "precedes", "to": "re-identification models", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_247", "file_type": "json", "from": "AlexNet", "predicate": "is_example_of", "to": "deep learning", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_408", "file_type": "json", "from": "AlexNet", "predicate": "is_a", "to": "deep convolutional neural network", "type": "conceptual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_408", "file_type": "json", "from": "AlexNet", "predicate": "achieved", "to": "breakthrough performance", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_407", "file_type": "json", "from": "AlexNet", "predicate": "influences", "to": "research in intrinsic image decomposition", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_408", "file_type": "json", "from": "AlexNet", "predicate": "influences", "to": "subsequent research", "type": "causal", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_184", "file_type": "json", "from": "filter size", "predicate": "affects", "to": "performance", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_184", "file_type": "json", "from": "layer width", "predicate": "affects", "to": "performance", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_184", "file_type": "json", "from": "learning rate policies", "predicate": "affects", "to": "performance", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_184", "file_type": "json", "from": "pedestrian heights", "predicate": "distributed_in", "to": "datasets", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_184", "file_type": "json", "from": "Calttech dataset", "predicate": "contains", "to": "pedestrian heights", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_184", "file_type": "json", "from": "KITTI dataset", "predicate": "contains", "to": "pedestrian heights", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_260", "file_type": "json", "from": "KITTI dataset", "predicate": "is", "to": "benchmark", "type": "conceptual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_260", "file_type": "json", "from": "KITTI dataset", "predicate": "used_in", "to": "vision", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_260", "file_type": "json", "from": "KITTI dataset", "predicate": "used_in", "to": "robotics", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_261", "file_type": "json", "from": "KITTI dataset", "predicate": "is_benchmark_for", "to": "vision and robotics", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_184", "file_type": "json", "from": "transferability", "predicate": "varies_between", "to": "datasets", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_184", "file_type": "json", "from": "neural network training", "predicate": "is_sensitive_to", "to": "parameter choices", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_184", "file_type": "json", "from": "parameter optimization", "predicate": "is_important_for", "to": "optimal results", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_185", "file_type": "json", "from": "Pedestrian Detection", "predicate": "is_field_of", "to": "Neural Network Training", "type": "conceptual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_185", "file_type": "json", "from": "Pedestrian Detection", "predicate": "requires", "to": "Parameter Optimization", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_185", "file_type": "json", "from": "Parameter Optimization", "predicate": "influences", "to": "optimal results", "type": "causal", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_185", "file_type": "json", "from": "Dataset Analysis", "predicate": "supports", "to": "Neural Network Training", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_185", "file_type": "json", "from": "Transfer Learning", "predicate": "improves", "to": "Neural Network Training", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_185", "file_type": "json", "from": "Benenson, R.", "predicate": "affiliated_with", "to": "Max Planck Institute for Informatics", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_185", "file_type": "json", "from": "Benenson, R.", "predicate": "email", "to": "rodrigo.benenson@mpi-inf.mpg.de", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_186", "file_type": "json", "from": "Max Planck Institute for Informatics", "predicate": "research_area", "to": "informatics", "type": "conceptual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_437", "file_type": "json", "from": "Max Planck Institute for Informatics", "predicate": "research_area", "to": "computer vision", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_185", "file_type": "json", "from": "Omran, M.", "predicate": "affiliated_with", "to": "Max Planck Institute for Informatics", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_185", "file_type": "json", "from": "Omran, M.", "predicate": "email", "to": "mohamed.omran@mpi-inf.mpg.de", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_185", "file_type": "json", "from": "Schiele, B.", "predicate": "affiliated_with", "to": "Max Planck Institute for Informatics", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_185", "file_type": "json", "from": "Schiele, B.", "predicate": "email", "to": "unspecified", "type": "factual", "width": 0.7}, {"arrows": "to", "chunk_id": "doc_0_chunk_185", "file_type": "json", "from": "Hosang, J.", "predicate": "email", "to": "jan.hosang@mpi-inf.mpg.de", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_186", "file_type": "json", "from": "Kiyoshi Matsuo", "predicate": "contributed_to", "to": "Depth Image Enhancement Using Local Tangent Plane Approximations", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_190", "file_type": "json", "from": "Kiyoshi Matsuo", "predicate": "affiliated_with", "to": "Hokuyo Automatic Co., LTD.", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_186", "file_type": "json", "from": "Depth Image Enhancement Using Local Tangent Plane Approximations", "predicate": "presented_at", "to": "CVPR", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_186", "file_type": "json", "from": "Depth Image Enhancement Using Local Tangent Plane Approximations", "predicate": "publication_date", "to": "2015", "type": "factual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_186", "file_type": "json", "from": "Yoshimitsu Aoki", "predicate": "contributed_to", "to": "Depth Image Enhancement Using Local Tangent Plane Approximations", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_190", "file_type": "json", "from": "Yoshimitsu Aoki", "predicate": "affiliated_with", "to": "Keio University", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_187", "file_type": "json", "from": "depth image enhancement method", "predicate": "aims_for", "to": "consumer RGB-D cameras", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_187", "file_type": "json", "from": "pixel-coordinates", "predicate": "is_unsuitable_for", "to": "handling local geometries", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_187", "file_type": "json", "from": "two steps", "predicate": "include", "to": "calculation of local tangents", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_187", "file_type": "json", "from": "two steps", "predicate": "include", "to": "surface reconstruction", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_187", "file_type": "json", "from": "depth image enhancement", "predicate": "achieved_by", "to": "local geometries", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_188", "file_type": "json", "from": "Local Tangent Planes", "predicate": "related_to", "to": "Depth Image Enhancement", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_188", "file_type": "json", "from": "RGB-D Cameras", "predicate": "used_for", "to": "Surface Reconstruction", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_188", "file_type": "json", "from": "Noise Reduction", "predicate": "improves", "to": "Completion Rate", "type": "causal", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_188", "file_type": "json", "from": "Asus Xtion Pro Live", "predicate": "is_example_of", "to": "RGB-D Cameras", "type": "factual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_188", "file_type": "json", "from": "Kim et al. (2013)", "predicate": "published", "to": "joint intensity and depth analysis model", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_188", "file_type": "json", "from": "Kim et al. (2014)", "predicate": "addresses", "to": "depth map upsampling", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_189", "file_type": "json", "from": "Lee et al.", "predicate": "publishes", "to": "Journal of Signal Processing Systems", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_189", "file_type": "json", "from": "depth map upsampling method", "predicate": "is_robust_to", "to": "misalignment of depth and color boundaries", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_189", "file_type": "json", "from": "Kopf et al.", "predicate": "develops", "to": "Joint bilateral upsampling", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_189", "file_type": "json", "from": "Joint bilateral upsampling", "predicate": "is_a", "to": "upsampling method", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_189", "file_type": "json", "from": "Li et al.", "predicate": "proposes", "to": "Joint example-based depth map super-resolution", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_189", "file_type": "json", "from": "Joint example-based depth map super-resolution", "predicate": "aims_to_improve", "to": "depth map resolution", "type": "conceptual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_189", "file_type": "json", "from": "Lu et al.", "predicate": "addresses", "to": "Depth enhancement", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_189", "file_type": "json", "from": "Depth enhancement", "predicate": "uses", "to": "low-rank matrix completion", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_189", "file_type": "json", "from": "Joint geodesic up-sampling", "predicate": "targets", "to": "depth images", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_190", "file_type": "json", "from": "S. Lu", "predicate": "authored", "to": "Depth enhancement via low-rank matrix completion", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_190", "file_type": "json", "from": "S. Lu", "predicate": "presented_at", "to": "CVPR 2014", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_190", "file_type": "json", "from": "D. Scharstein", "predicate": "authored", "to": "Learning conditional random fields for stereo", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_190", "file_type": "json", "from": "J. Papon", "predicate": "authored", "to": "Point cloud video object segmentation", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_190", "file_type": "json", "from": "J. Papon", "predicate": "presented_at", "to": "IROS 2013", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_190", "file_type": "json", "from": "Sean Bell", "predicate": "author_of", "to": "Material Recognition in the Wild", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_191", "file_type": "json", "from": "Sean Bell", "predicate": "is_author_of", "to": "Material Recognition in 2015 CVPR paper", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_191", "file_type": "json", "from": "Sean Bell", "predicate": "works_in_field", "to": "Material Recognition", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_191", "file_type": "json", "from": "Sean Bell", "predicate": "contributed_to", "to": "Materials in Context Database", "type": "factual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_192", "file_type": "json", "from": "Sean Bell", "predicate": "contributes_to", "to": "Bell_Material_Recognition_in_2015_CVPR_paper", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_195", "file_type": "json", "from": "Sean Bell", "predicate": "affiliated_with", "to": "Department of Computer Science, Cornell University", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_195", "file_type": "json", "from": "Sean Bell", "predicate": "has_email", "to": "sbell@cs.cornell.edu", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_190", "file_type": "json", "from": "Material Recognition in the Wild", "predicate": "uses", "to": "Materials in Context Database", "type": "conceptual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_190", "file_type": "json", "from": "Paul Upchurch", "predicate": "author_of", "to": "Material Recognition in the Wild", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_191", "file_type": "json", "from": "Paul Upchurch", "predicate": "is_author_of", "to": "Material Recognition in 2015 CVPR paper", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_192", "file_type": "json", "from": "Paul Upchurch", "predicate": "contributes_to", "to": "Bell_Material_Detection_in_2015_CVPR_paper", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_195", "file_type": "json", "from": "Paul Upchurch", "predicate": "has_email", "to": "paulu@cs.cornell.edu", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_191", "file_type": "json", "from": "Material Recognition in 2015 CVPR paper", "predicate": "published_in", "to": "CVPR", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_191", "file_type": "json", "from": "Material Recognition in 2015 CVPR paper", "predicate": "located_in", "to": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_191", "file_type": "json", "from": "Material Recognition in 2015 CVPR paper", "predicate": "addresses_topic", "to": "Material Recognition", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_191", "file_type": "json", "from": "Noah Snavely", "predicate": "is_author_of", "to": "Material Recognition in 2015 CVPR paper", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_192", "file_type": "json", "from": "Noah Snavely", "predicate": "contributes_to", "to": "Bell_Material_Recognition_in_2015_CVPR_paper", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_196", "file_type": "json", "from": "Noah Snavely", "predicate": "affiliated_with", "to": "Department of Computer Science, Cornell University", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_195", "file_type": "json", "from": "Noah Snavely", "predicate": "has_email", "to": "snavely@cs.cornell.edu", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_193", "file_type": "json", "from": "Material Recognition", "predicate": "requires", "to": "large, well-sampled datasets", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_192", "file_type": "json", "from": "material recognition", "predicate": "is_challenging_due_to", "to": "rich surface texture", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_192", "file_type": "json", "from": "material recognition", "predicate": "is_challenging_due_to", "to": "geometry", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_192", "file_type": "json", "from": "material recognition", "predicate": "is_challenging_due_to", "to": "lighting conditions", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_192", "file_type": "json", "from": "MINC", "predicate": "is_a", "to": "dataset", "type": "conceptual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_192", "file_type": "json", "from": "MINC", "predicate": "is", "to": "large-scale", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_192", "file_type": "json", "from": "MINC", "predicate": "is", "to": "open", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_192", "file_type": "json", "from": "CNNs", "predicate": "are_used_for", "to": "material classification", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_192", "file_type": "json", "from": "CNNs", "predicate": "are_used_for", "to": "material segmentation", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_192", "file_type": "json", "from": "patch-based classification", "predicate": "achieves", "to": "85.2% mean class accuracy", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_192", "file_type": "json", "from": "full image segmentation", "predicate": "achieves", "to": "73.1% mean class accuracy", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_193", "file_type": "json", "from": "Dataset Creation", "predicate": "uses", "to": "MINC", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_193", "file_type": "json", "from": "G. Patterson et al.", "predicate": "authored", "to": "The SUN Attribute Database", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_193", "file_type": "json", "from": "The SUN Attribute Database", "predicate": "facilitates", "to": "Deeper Scene Understanding", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_193", "file_type": "json", "from": "S. Bell et al.", "predicate": "authored", "to": "OpenSurposes", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_193", "file_type": "json", "from": "OpenSurfaces", "predicate": "is", "to": "richly annotated catalog", "type": "factual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_193", "file_type": "json", "from": "X. Qi et al.", "predicate": "authored", "to": "Pairwise rotation invariant co-occurrence local binary pattern", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_193", "file_type": "json", "from": "B. Caputo et al.", "predicate": "authored", "to": "Class-speci\ufb01c material categorisation", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_194", "file_type": "json", "from": "variant co-occurrence local binary pattern", "predicate": "is_method", "to": "image processing", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_194", "file_type": "json", "from": "ImageNet Large Scale Visual Recognition Challenge", "predicate": "is_event", "to": "visual recognition", "type": "conceptual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_226", "file_type": "json", "from": "ImageNet Large Scale Visual Recognition Challenge", "predicate": "is_benchmark_for", "to": "Image Classification", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_194", "file_type": "json", "from": "ACM Transactions on Graphics (TOG)", "predicate": "published", "to": "Re\ufb02ectence and texture of real-world surfaces", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_195", "file_type": "json", "from": "LabelMe", "predicate": "is_database_and_tool_for", "to": "image annotation", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_194", "file_type": "json", "from": "Re\ufb02ectance", "predicate": "is_property_of", "to": "real-world surfaces", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_194", "file_type": "json", "from": "texture", "predicate": "is_property_of", "to": "real-world surfaces", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_194", "file_type": "json", "from": "Describing textures in the wild", "predicate": "addresses", "to": "texture", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_195", "file_type": "json", "from": "TOG", "predicate": "is_publication_of", "to": "Graphics", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_195", "file_type": "json", "from": "Pascal VOC Challenge", "predicate": "is_challenge_in", "to": "Visual Object Classes", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_385", "file_type": "json", "from": "Department of Computer Science", "predicate": "located_in", "to": "Canada", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_196", "file_type": "json", "from": "Wonmin Byeon", "predicate": "is_author_of", "to": "Scene Labeling with LSTM Recurrent Neural Networks", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_198", "file_type": "json", "from": "Wonmin Byeon", "predicate": "affiliated_with", "to": "German Research Center for Arti\ufb01cial Intelligence (DFKI)", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_196", "file_type": "json", "from": "Scene Labeling with LSTM Recurrent Neural Networks", "predicate": "is_paper_type", "to": "CVPR paper", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_196", "file_type": "json", "from": "Thomas M. Breuel", "predicate": "is_author_of", "to": "Scene Labeling with LSTM Recurrent Neural Networks", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_198", "file_type": "json", "from": "Thomas M. Breuel", "predicate": "affiliated_with", "to": "University of Kaiserslautern", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_196", "file_type": "json", "from": "Federico Raue", "predicate": "is_author_of", "to": "Scene Labeling with LSTM Recurrent Neural Networks", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_198", "file_type": "json", "from": "Federico Raue", "predicate": "affiliated_with", "to": "German Research Center for Arti\ufb01cial Intelligence (DFKI)", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_196", "file_type": "json", "from": "Byeon_Scene_Labeling_With_2015_CVPR_paper.pdf", "predicate": "contains", "to": "Scene Labeling with LSTM Recurrent Neural Networks", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_197", "file_type": "json", "from": "Accurate scene labeling", "predicate": "is_step_towards", "to": "image understanding", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_197", "file_type": "json", "from": "LSTM recurrent neural networks", "predicate": "is_a", "to": "neural network", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_289", "file_type": "json", "from": "state-of-the-art performance", "predicate": "measured_on", "to": "benchmark datasets", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_198", "file_type": "json", "from": "Networks", "predicate": "captures", "to": "local contextual information", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_198", "file_type": "json", "from": "Networks", "predicate": "captures", "to": "global contextual information", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_198", "file_type": "json", "from": "Networks", "predicate": "operates_on", "to": "raw RGB values", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_198", "file_type": "json", "from": "Networks", "predicate": "adapts_well_for", "to": "complex scene images", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_198", "file_type": "json", "from": "Thalaiyasingam Ajanthan", "predicate": "author_of", "to": "Iteratively Reweighted Graph Cut", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_204", "file_type": "json", "from": "Thalaiyasingam Ajanthan", "predicate": "affiliated with", "to": "Australian National University", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_198", "file_type": "json", "from": "Iteratively Reweighted Graph Cut", "predicate": "used_for", "to": "Multi-label MRFs", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_199", "file_type": "json", "from": "energy", "predicate": "of", "to": "Multi-label Markov Random Fields (MRFs)", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_293", "file_type": "json", "from": "energy", "predicate": "takes_into_account", "to": "piecewise constant model assumption", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_293", "file_type": "json", "from": "energy", "predicate": "takes_into_account", "to": "flow field continuity constraint", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_199", "file_type": "json", "from": "MRFs", "predicate": "has_property", "to": "Non-convex Priors", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_200", "file_type": "json", "from": "Multi-label Markov Random Fields", "predicate": "outperforms", "to": "graph-cut-based algorithms", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_200", "file_type": "json", "from": "Multi-label Markov Random Fields", "predicate": "yields", "to": "lower energy values", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_200", "file_type": "json", "from": "Multi-label Markov Random Fields", "predicate": "has_optimization_method", "to": "Graph Cut Optimization", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_200", "file_type": "json", "from": "Ishikawa, H.", "predicate": "authored", "to": "Exact optimization for Markov random fields with convex priors", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_200", "file_type": "json", "from": "Ishikawa, H.", "predicate": "is_foundational_work_on", "to": "MRF optimization", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_200", "file_type": "json", "from": "Boykov, Y.", "predicate": "authored", "to": "Fast approximate energy minimization via graph cuts", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_200", "file_type": "json", "from": "Boykov, Y.", "predicate": "method_uses", "to": "graph cut methods", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_297", "file_type": "json", "from": "Boykov, Y.", "predicate": "co-authored", "to": "Fast approximate energy minimization", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_548", "file_type": "json", "from": "Fast approximate energy minimization via graph cuts", "predicate": "presented_in", "to": "PAMI", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_548", "file_type": "json", "from": "Fast approximate energy minimization via graph cuts", "predicate": "uses", "to": "graph cuts", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_200", "file_type": "json", "from": "Kolmogorov, V.", "predicate": "authored", "to": "Convergent tree-reweighted message passing", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_200", "file_type": "json", "from": "Kolmogorov, V.", "predicate": "focuses_on", "to": "reweighted message passing", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_201", "file_type": "json", "from": "reweighted message passing", "predicate": "contributes_to", "to": "energy minimization", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_200", "file_type": "json", "from": "Iteratively Reweighted Algorithms", "predicate": "related_to", "to": "reweighted message passing", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_201", "file_type": "json", "from": "energy minimization", "predicate": "related_to", "to": "pattern analysis", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_201", "file_type": "json", "from": "geometric relationships", "predicate": "studied_in", "to": "Multiple view geometry", "type": "factual", "width": 0.91}, {"arrows": "to", "chunk_id": "doc_0_chunk_201", "file_type": "json", "from": "energy minimization techniques", "predicate": "is_analyzed_in", "to": "comparative study", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_202", "file_type": "json", "from": "energy minimization techniques", "predicate": "related to", "to": "smoothness-based priors", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_201", "file_type": "json", "from": "comparative study", "predicate": "published_in", "to": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_203", "file_type": "json", "from": "comparative study", "predicate": "examines", "to": "inference techniques", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_201", "file_type": "json", "from": "Markov random fields", "predicate": "uses", "to": "energy minimization methods", "type": "conceptual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_201", "file_type": "json", "from": "smoothness-based priors", "predicate": "associated_with", "to": "Markov random fields", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_202", "file_type": "json", "from": "Pattern Analysis and Machine Intelligence", "predicate": "published", "to": "ds with smoothness-based priors", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_606", "file_type": "json", "from": "Pattern Analysis and Machine Intelligence", "predicate": "is_journal_of", "to": "Large Displacement Optical Flow", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_202", "file_type": "json", "from": "ds with smoothness-based priors", "predicate": "analyzes", "to": "energy minimization techniques", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_202", "file_type": "json", "from": "Boykov et al.", "predicate": "published", "to": "An experimental comparison", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_298", "file_type": "json", "from": "Boykov et al.", "predicate": "published", "to": "IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI)", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_298", "file_type": "json", "from": "Boykov et al.", "predicate": "authored", "to": "Fast approximate energy minimization via graph cuts", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_202", "file_type": "json", "from": "An experimental comparison", "predicate": "compares", "to": "min-cut/max-flow algorithms", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_202", "file_type": "json", "from": "min-cut/max-flow algorithms", "predicate": "used for", "to": "energy minimization", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_202", "file_type": "json", "from": "Pock et al.", "predicate": "introduced", "to": "convex formulation", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_202", "file_type": "json", "from": "Pock et al.", "predicate": "presented in", "to": "Computer Vision\u2013ECCV 2008", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_202", "file_type": "json", "from": "convex formulation", "predicate": "for", "to": "multi-label problems", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_203", "file_type": "json", "from": "multi-label problems", "predicate": "addressed_by", "to": "convex formulation", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_203", "file_type": "json", "from": "Computer Vision\u2013ECCV 2008", "predicate": "presents", "to": "convex formulation", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_203", "file_type": "json", "from": "H. f.", "predicate": "introduces", "to": "convex formulation", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_203", "file_type": "json", "from": "Kappes et al.", "predicate": "conducts", "to": "comparative study", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_203", "file_type": "json", "from": "Scharstein \u0026 Szeliski", "predicate": "addresses", "to": "dense two-frame stereo correspondence algorithms", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_203", "file_type": "json", "from": "dense two-frame stereo correspondence algorithms", "predicate": "is_important_for", "to": "understanding", "type": "conceptual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_204", "file_type": "json", "from": "stereo correspondence algorithms", "predicate": "addressed by", "to": "International journal of computer vision", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_204", "file_type": "json", "from": "Vekler", "predicate": "authored", "to": "Multi-label moves for mrfs", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_204", "file_type": "json", "from": "Multi-label moves for mrfs", "predicate": "uses", "to": "truncated convex priors", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_273", "file_type": "json", "from": "Mathieu Salzmann", "predicate": "affiliated_with", "to": "Australian National University", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_270", "file_type": "json", "from": "Mathieu Salzmann", "predicate": "author of", "to": "Riemannian Coding and Dictionary Learning", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_273", "file_type": "json", "from": "Mathieu Salzmann", "predicate": "affiliated_with", "to": "NICITA", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_204", "file_type": "json", "from": "Hongdong Li", "predicate": "affiliated with", "to": "Australian National University", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_292", "file_type": "json", "from": "Hongdong Li", "predicate": "author_of", "to": "Dense, Accurate Optical Flow Estimation", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_298", "file_type": "json", "from": "Hongdong Li", "predicate": "affiliated_with", "to": "Research School of Engineering", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_298", "file_type": "json", "from": "Hongdong Li", "predicate": "affiliated_with", "to": "The Australian National University (ANU)", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_298", "file_type": "json", "from": "Hongdong Li", "predicate": "affiliated_with", "to": "NICTA", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_204", "file_type": "json", "from": "Rui Zhao", "predicate": "authored", "to": "Saliency Detection by Multi-Context Deep Learning", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_207", "file_type": "json", "from": "Rui Zhao", "predicate": "affiliated_with", "to": "Shenzhen Institutes of Advanced Technology", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_204", "file_type": "json", "from": "Saliency Detection by Multi-Context Deep Learning", "predicate": "uses", "to": "Multi-Context Deep Learning", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_204", "file_type": "json", "from": "Wanli Ouyang", "predicate": "authored", "to": "Saliency Detection by Multi-Context Deep Learning", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_208", "file_type": "json", "from": "Wanli Ouyang", "predicate": "affiliated_with", "to": "Department of Electronic Engineering", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_208", "file_type": "json", "from": "Wanli Ouyang", "predicate": "email", "to": "wlouyang@ee.cuhk.edu.hk", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_205", "file_type": "json", "from": "Image salience detection", "predicate": "aims_to", "to": "highlight visually salient regions", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_205", "file_type": "json", "from": "Conventional approaches", "predicate": "struggle_with", "to": "salient objects in low-contrast backgrounds", "type": "factual", "width": 0.5}, {"arrows": "to", "chunk_id": "doc_0_chunk_205", "file_type": "json", "from": "multi-context deep learning framework", "predicate": "employs", "to": "Convolutional Neural Networks", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_205", "file_type": "json", "from": "multi-context deep learning framework", "predicate": "considers", "to": "global context", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_205", "file_type": "json", "from": "multi-context deep learning framework", "predicate": "considers", "to": "local context", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_452", "file_type": "json", "from": "salience", "predicate": "connects_to", "to": "probabilistic inference", "type": "conceptual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_674", "file_type": "json", "from": "salience", "predicate": "is_aware_of", "to": "unsupervised method", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_205", "file_type": "json", "from": "pre-training scheme", "predicate": "improves", "to": "performance", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_206", "file_type": "json", "from": "Task-specific pre-training scheme", "predicate": "improves", "to": "performance", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_305", "file_type": "json", "from": "Frequency-tuned salient region detection", "predicate": "published_in", "to": "CVPR", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_305", "file_type": "json", "from": "Frequency-tuned salient region detection", "predicate": "is_foundational_work_in", "to": "Salient Object Detection", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_206", "file_type": "json", "from": "Training products of experts", "predicate": "published_in", "to": "Neural computation", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_206", "file_type": "json", "from": "Saliency detection", "predicate": "presented_at", "to": "CVPR", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_206", "file_type": "json", "from": "Category-independent object-level saliency detection", "predicate": "presented_at", "to": "ICCV", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_206", "file_type": "json", "from": "Graph-based visual saliency", "predicate": "presented_at", "to": "NIPS", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_206", "file_type": "json", "from": "Salient Object Detection", "predicate": "related_to", "to": "Computer Vision", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_206", "file_type": "json", "from": "Salient Object Detection", "predicate": "related_to", "to": "Multi-Context Modeling", "type": "conceptual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_219", "file_type": "json", "from": "Salient Object Detection", "predicate": "uses", "to": "bootstrap learning", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_220", "file_type": "json", "from": "Salient Object Detection", "predicate": "presented_at", "to": "CVPR", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_220", "file_type": "json", "from": "Salient Object Detection", "predicate": "year", "to": "2015", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_220", "file_type": "json", "from": "Salient Object Detection", "predicate": "uses", "to": "Bootstrap Learning", "type": "conceptual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_301", "file_type": "json", "from": "Salient Object Detection", "predicate": "evaluated_on", "to": "challenging datasets", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_305", "file_type": "json", "from": "Salient Object Detection", "predicate": "utilizes", "to": "Deep Learning Features", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_305", "file_type": "json", "from": "Salient Object Detection", "predicate": "incorporates", "to": "Multiscale Analysis", "type": "conceptual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_207", "file_type": "json", "from": "ik", "predicate": "published_in", "to": "arXiv", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_207", "file_type": "json", "from": "ik", "predicate": "focuses_on", "to": "object detection", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_207", "file_type": "json", "from": "ik", "predicate": "focuses_on", "to": "semantic segmentation", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_207", "file_type": "json", "from": "A. Borji", "predicate": "authored", "to": "Boosting bottom-up and top-down visual features", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_291", "file_type": "json", "from": "A. Borji", "predicate": "authored", "to": "visual attention modeling", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_291", "file_type": "json", "from": "M.-M. Cheng", "predicate": "developed", "to": "Global contrast based salient region detection", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_305", "file_type": "json", "from": "Global contrast based salient region detection", "predicate": "is_foundational_work_in", "to": "Salient Object Detection", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_305", "file_type": "json", "from": "Global contrast based salient region detection", "predicate": "published_in", "to": "TRAMI", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_207", "file_type": "json", "from": "R. Mairon", "predicate": "authored", "to": "A closer look at context", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_449", "file_type": "json", "from": "Shenzhen Institutes of Advanced Technology", "predicate": "part_of", "to": "Chinese Academy of Sciences", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_208", "file_type": "json", "from": "Advanced Technology", "predicate": "part_of", "to": "Chinese Academy of Sciences", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_208", "file_type": "json", "from": "Baohua Li", "predicate": "author_of", "to": "Subspace Clustering by Mixture of Gaussian Regression", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_214", "file_type": "json", "from": "Baohua Li", "predicate": "affiliation", "to": "Dalian University of Technology", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_208", "file_type": "json", "from": "Subspace Clustering by Mixture of Gaussian Regression", "predicate": "published_in", "to": "CVPR", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_208", "file_type": "json", "from": "Ying Zhang", "predicate": "author_of", "to": "Subspace Clustering by Mixture of Gaussian Regression", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_213", "file_type": "json", "from": "Ying Zhang", "predicate": "is_affiliated_with", "to": "Dalian\u003c0xC2\u003e\u003c0xA0\u003eUniversity of Technology", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_214", "file_type": "json", "from": "Ying Zhang", "predicate": "affiliation", "to": "Dalian University of Technology", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_208", "file_type": "json", "from": "Zhouchen Lin", "predicate": "author_of", "to": "Subclone Clustering by Mixture of Gaussian Regression", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_213", "file_type": "json", "from": "Zhouchen Lin", "predicate": "is_affiliated_with", "to": "Dalian University of Technology", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_214", "file_type": "json", "from": "Zhouchen Lin", "predicate": "is_part_of", "to": "School of EECS", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_208", "file_type": "json", "from": "Huchuan Lu", "predicate": "author_of", "to": "Subspace Clustering by Mixture of Gaussian Regression", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_224", "file_type": "json", "from": "Huchuan Lu", "predicate": "affiliated_with", "to": "Dalian University of Technology", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_220", "file_type": "json", "from": "Huchuan Lu", "predicate": "authored", "to": "Salient Object Detectio", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_480", "file_type": "json", "from": "Huchuan Lu", "predicate": "is_author_of", "to": "Deep Networks for Saliency Detection", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_481", "file_type": "json", "from": "Huchuan Lu", "predicate": "is_author_of", "to": "Deep Networks for Salience Detected", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_209", "file_type": "json", "from": "Subspace clustering", "predicate": "aims_to", "to": "multi-subspace representation", "type": "conceptual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_209", "file_type": "json", "from": "Subspace clustering", "predicate": "operates_in", "to": "high-dimensional space", "type": "conceptual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_209", "file_type": "json", "from": "Existing methods", "predicate": "relies_on", "to": "norms", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_209", "file_type": "json", "from": "MoG Regression", "predicate": "approaches", "to": "subspace clustering", "type": "conceptual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_209", "file_type": "json", "from": "MoG Regression", "predicate": "provides", "to": "affinity matrix", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_209", "file_type": "json", "from": "MoG Regression", "predicate": "improves", "to": "clustering performance", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_209", "file_type": "json", "from": "MoG Regression", "predicate": "models", "to": "noise distributions", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_209", "file_type": "json", "from": "Mixture of Gausians (MoG)", "predicate": "models", "to": "noise", "type": "conceptual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_209", "file_type": "json", "from": "Noise Modeling", "predicate": "is_approach_in", "to": "Subspace clustering", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_210", "file_type": "json", "from": "Noise Modeling", "predicate": "is_technique", "to": "clustering", "type": "conceptual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_210", "file_type": "json", "from": "Subspace Clustering", "predicate": "outperforms", "to": "state-of-the-art subspace clustering methods", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_210", "file_type": "json", "from": "K-plane clustering", "predicate": "is_technique_for", "to": "segmentation", "type": "conceptual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_210", "file_type": "json", "from": "K-plane clustering", "predicate": "introduced_in", "to": "Journal of Global Optimization", "type": "factual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_471", "file_type": "json", "from": "segmentation", "predicate": "results_in", "to": "geo-referenced images", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_716", "file_type": "json", "from": "segmentation", "predicate": "is", "to": "hierarchical", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_716", "file_type": "json", "from": "segmentation", "predicate": "is", "to": "graph-based", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_717", "file_type": "json", "from": "segmentation", "predicate": "requires", "to": "object identification", "type": "conceptual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_717", "file_type": "json", "from": "segmentation", "predicate": "requires", "to": "motion analysis", "type": "conceptual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_210", "file_type": "json", "from": "power factorization", "predicate": "utilized_for", "to": "motion segmentation", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_213", "file_type": "json", "from": "motion segmentation", "predicate": "covers", "to": "various types of motion", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_213", "file_type": "json", "from": "motion segmentation", "predicate": "provides", "to": "broad framework", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_717", "file_type": "json", "from": "motion segmentation", "predicate": "is_part_of", "to": "object and motion segmentation", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_210", "file_type": "json", "from": "GPCA", "predicate": "utilized_for", "to": "motion segmentation", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_211", "file_type": "json", "from": "GPCA", "predicate": "is", "to": "technique", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_211", "file_type": "json", "from": "GPCA", "predicate": "used_in", "to": "referenced papers", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_210", "file_type": "json", "from": "Mixture of Gaussian Regression", "predicate": "is_technique", "to": "clustering", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_210", "file_type": "json", "from": "Affinity Matrix Construction", "predicate": "is_part_of", "to": "clustering", "type": "conceptual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_211", "file_type": "json", "from": "Spectral clustering", "predicate": "is", "to": "technique", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_211", "file_type": "json", "from": "tutorial", "predicate": "provides", "to": "overview", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_211", "file_type": "json", "from": "tutorial", "predicate": "focuses_on", "to": "Spectral clustering", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_211", "file_type": "json", "from": "Sparse representation", "predicate": "is", "to": "technique", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_212", "file_type": "json", "from": "Sparse representation", "predicate": "informs", "to": "Approaches", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_212", "file_type": "json", "from": "Sparse representation", "predicate": "is_technique", "to": "Pattern Analysis", "type": "conceptual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_211", "file_type": "json", "from": "Wright et al.", "predicate": "develops", "to": "face recognition approach", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_211", "file_type": "json", "from": "face recognition approach", "predicate": "uses", "to": "Sparse representation", "type": "factual", "width": 0.82}, {"arrows": "to", "chunk_id": "doc_0_chunk_212", "file_type": "json", "from": "EM algorithm", "predicate": "facilitates", "to": "Segmentation", "type": "causation", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_212", "file_type": "json", "from": "EM algorithm", "predicate": "addresses", "to": "Convergence properties", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_212", "file_type": "json", "from": "EM algorithm", "predicate": "is_algorithm", "to": "Gaussian mixtures", "type": "conceptual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_212", "file_type": "json", "from": "Motion segmentation", "predicate": "covered_by", "to": "Framework", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_212", "file_type": "json", "from": "Motion segmentation", "predicate": "is_process", "to": "Computer Vision", "type": "conceptual", "width": 0.7}, {"arrows": "to", "chunk_id": "doc_0_chunk_295", "file_type": "json", "from": "Motion segmentation", "predicate": "related_to", "to": "Optical flow estimation", "type": "conceptual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_212", "file_type": "json", "from": "Framework", "predicate": "covers", "to": "Types of motion", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_376", "file_type": "json", "from": "Framework", "predicate": "extracts", "to": "region-keyword pairs", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_478", "file_type": "json", "from": "Framework", "predicate": "improves", "to": "Image Quality", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_213", "file_type": "json", "from": "lossy data compression", "predicate": "is_methodology", "to": "image segmentation", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_213", "file_type": "json", "from": "robust PCA", "predicate": "is_consideration_for", "to": "segmentation", "type": "conceptual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_214", "file_type": "json", "from": "A Convolutional Neural Network Cascade", "predicate": "is_a", "to": "Face Detection", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_214", "file_type": "json", "from": "A Convolutional Neural Network Cascade", "predicate": "publication_venue", "to": "CVPR paper", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_216", "file_type": "json", "from": "Face Detection", "predicate": "uses", "to": "Convolutional Neural Networks", "type": "conceptual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_216", "file_type": "json", "from": "Face Detection", "predicate": "runs_at", "to": "14 FPS", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_216", "file_type": "json", "from": "Face Detection", "predicate": "runs_at", "to": "100 FPS", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_217", "file_type": "json", "from": "Face Detection", "predicate": "uses", "to": "Convolutional Neural Networks (CNNs)", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_214", "file_type": "json", "from": "Haoxiang Li", "predicate": "author_of", "to": "A Convolutional Neural Network Cascade", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_219", "file_type": "json", "from": "Haoxiang Li", "predicate": "is_affiliated_with", "to": "Stevens Institute of Technology", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_408", "file_type": "json", "from": "Haoxiang Li", "predicate": "is_author_of", "to": "Hierarchical-PEP Model", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_214", "file_type": "json", "from": "Zhe Lin", "predicate": "author_of", "to": "A Convolutional Neural Network Cascade", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_521", "file_type": "json", "from": "Zhe Lin", "predicate": "affiliated_with", "to": "Adobe Research", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_519", "file_type": "json", "from": "Zhe Lin", "predicate": "authored", "to": "Salient Object Subitizing", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_214", "file_type": "json", "from": "Jonathan Brandt", "predicate": "author_of", "to": "A Convolutional Neural Network Cascade", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_214", "file_type": "json", "from": "Gang Hua", "predicate": "author_of", "to": "A Convolutional Neural Network Cascade", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_414", "file_type": "json", "from": "Gang Hua", "predicate": "affiliated_with", "to": "Stevens Institute of Technology", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_408", "file_type": "json", "from": "Gang Hua", "predicate": "is_author_of", "to": "Hierarchical-PEP Model", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_414", "file_type": "json", "from": "Gang Hua", "predicate": "has_email", "to": "{ghua}@steverns.edu", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_215", "file_type": "json", "from": "Face detection", "predicate": "faces", "to": "large visual variations", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_215", "file_type": "json", "from": "Face detection", "predicate": "faces", "to": "large search space", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_215", "file_type": "json", "from": "Advanced models", "predicate": "addresses", "to": "visual variations", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_215", "file_type": "json", "from": "Advanced models", "predicate": "is", "to": "computationally expensive", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_215", "file_type": "json", "from": "Paper", "predicate": "proposes", "to": "CNN cascade architecture", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_304", "file_type": "json", "from": "Paper", "predicate": "introduces", "to": "Approach", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_645", "file_type": "json", "from": "Paper", "predicate": "proposes", "to": "subgraph matching formulation", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_215", "file_type": "json", "from": "CNN cascade architecture", "predicate": "balances", "to": "conflicting demands", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_215", "file_type": "json", "from": "Cascade", "predicate": "rejects", "to": "background regions", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_215", "file_type": "json", "from": "CNN-based calibration stage", "predicate": "improves", "to": "localization effectiveness", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_216", "file_type": "json", "from": "100 FPS", "predicate": "requires", "to": "GPU", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_216", "file_type": "json", "from": "Cascade Architecture", "predicate": "optimizes", "to": "Real-time Performance", "type": "factual", "width": 0.87}, {"arrows": "to", "chunk_id": "doc_0_chunk_217", "file_type": "json", "from": "Cascade Architecture", "predicate": "enhances", "to": "Face Detection", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_217", "file_type": "json", "from": "Bounding Box Calibration", "predicate": "improves", "to": "Face Detection", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_217", "file_type": "json", "from": "LeCun", "predicate": "authored", "to": "Convolutional networks", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_217", "file_type": "json", "from": "Convolutional networks", "predicate": "processes", "to": "speech", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_217", "file_type": "json", "from": "Rowley", "predicate": "researched", "to": "Neural network-based face detection", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_217", "file_type": "json", "from": "Felzenszwalb", "predicate": "developed", "to": "part-based models", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_217", "file_type": "json", "from": "part-based models", "predicate": "used_for", "to": "Object Detection", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_218", "file_type": "json", "from": "P. F.", "predicate": "authored", "to": "Object detection with discriminatatively trained part-based models", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_368", "file_type": "json", "from": "P. F.", "predicate": "authored", "to": "Object detection with discriminatively trained part-based models", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_368", "file_type": "json", "from": "P. F.", "predicate": "affiliated_with", "to": "IEEE Trans. Pattern Anal. Mach. Intell", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_218", "file_type": "json", "from": "Object detection with discriminatively trained part-based models", "predicate": "published_in", "to": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_218", "file_type": "json", "from": "Jain, V.", "predicate": "authored", "to": "Fddb: A benchmark for face detection in unconstrained settings", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_218", "file_type": "json", "from": "Fddb: A benchmark for face detection in unconstrained settings", "predicate": "is_report_from", "to": "University of Massachusetts, Amherst", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_218", "file_type": "json", "from": "Rich feature hierarchies for accurate object detection and semantic segmentation", "predicate": "published_as", "to": "arXiv preprint arXiv:1311.2524", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_218", "file_type": "json", "from": "ImageNet classification with deep convolutional neural networks", "predicate": "published_in", "to": "Advances in neural information processing systems", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_218", "file_type": "json", "from": "Jia, Y.", "predicate": "authored", "to": "Caffe: Convolutional architecture for fast feature embedding", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_692", "file_type": "json", "from": "Advances in neural information processing systems", "predicate": "is_publication", "to": "conference", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_219", "file_type": "json", "from": "helhamer, E.", "predicate": "is_author_of", "to": "Caffe", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_219", "file_type": "json", "from": "Vaillant, R.", "predicate": "is_author_of", "to": "object localization approach", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_219", "file_type": "json", "from": "Yang, B.", "predicate": "is_author_of", "to": "multi-view face detection method", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_413", "file_type": "json", "from": "Stevens Institute of Technology", "predicate": "has_author", "to": "Haoxiang Li", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_220", "file_type": "json", "from": "Na Tong", "predicate": "authored", "to": "Salient Object Detection", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_224", "file_type": "json", "from": "Na Tong", "predicate": "affiliated_with", "to": "Dalian University of Technology", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_220", "file_type": "json", "from": "Xiang Ruan", "predicate": "authored", "to": "Salient Object Detection", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_224", "file_type": "json", "from": "Xiang Ruan", "predicate": "affiliated_with", "to": "OMRON Corporation", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_480", "file_type": "json", "from": "Xiang Ruan", "predicate": "is_author_of", "to": "Deep Networks for Saliency Detection", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_481", "file_type": "json", "from": "Xiang Ruan", "predicate": "is_author_of", "to": "Deep Networks for Salience Detection", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_220", "file_type": "json", "from": "Ming-Hsuan Yang", "predicate": "authored", "to": "Salient Object Detection", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_544", "file_type": "json", "from": "Ming-Hsuan Yang", "predicate": "affiliated_with", "to": "University of California at Merced", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_480", "file_type": "json", "from": "Ming-Hsuan Yang", "predicate": "is_author_of", "to": "Deep Networks for Saliency Detection", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_481", "file_type": "json", "from": "Ming-Hsuan Yang", "predicate": "is_author_of", "to": "Deep Networks for Salience Detection", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_539", "file_type": "json", "from": "Ming-Hsuan Yang", "predicate": "is_author_of", "to": "JOTS", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_702", "file_type": "json", "from": "Ming-Hsuan Yang", "predicate": "author_of", "to": "Adaptive Region Pooling for Object Detection", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_704", "file_type": "json", "from": "Ming-Hsuan Yang", "predicate": "affiliated_with", "to": "UC Merced", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_220", "file_type": "json", "from": "Bootstrap Learning", "predicate": "is_method_of", "to": "Object Detection", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_220", "file_type": "json", "from": "Tong_Salient_Object_Detection_2015_CVPR_paper.pdf", "predicate": "contains", "to": "Salient Object Detection", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_221", "file_type": "json", "from": "bootstrap learning algorithm", "predicate": "exploits", "to": "weak models", "type": "conceptual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_221", "file_type": "json", "from": "bootstrap learning algorithm", "predicate": "exploits", "to": "strong models", "type": "conceptual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_221", "file_type": "json", "from": "bootstrap learning algorithm", "predicate": "performs_favorably_against", "to": "state-of-the-art salience detection methods", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_221", "file_type": "json", "from": "weak salience map", "predicate": "generated_from", "to": "image priors", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_221", "file_type": "json", "from": "weak salience map", "predicate": "generates", "to": "training samples", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_221", "file_type": "json", "from": "strong classi\ufb01er", "predicate": "detects", "to": "salient pixels", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_221", "file_type": "json", "from": "strong classi\ufb01er", "predicate": "based_on", "to": "input image", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_221", "file_type": "json", "from": "multiscale salience maps", "predicate": "improves", "to": "detection performance", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_221", "file_type": "json", "from": "bootstrap learning approach", "predicate": "results_in", "to": "signi\ufb01cant improvement", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_222", "file_type": "json", "from": "Salieny Detection Methods", "predicate": "is_state_of_the_art", "to": "Computer Vision", "type": "conceptual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_222", "file_type": "json", "from": "Bootstrap Learning Approach", "predicate": "is_applicable_to", "to": "Bottom-up Salieny Models", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_222", "file_type": "json", "from": "Li et al. (2014)", "predicate": "researches", "to": "Salient Object Segmentation", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_222", "file_type": "json", "from": "Salient Object Segmentation", "predicate": "is_field_of", "to": "Computer Vision", "type": "conceptual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_222", "file_type": "json", "from": "Achanta et al. (2009)", "predicate": "researches", "to": "Frequency-tuned Salient Region Detection", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_222", "file_type": "json", "from": "Movahedi \u0026 Elder (2010)", "predicate": "designs", "to": "Performance Measures", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_222", "file_type": "json", "from": "Performance Measures", "predicate": "validates", "to": "Salient Object Segmentation", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_222", "file_type": "json", "from": "Achanta et al. (2010)", "predicate": "researches", "to": "Slic Superpixels", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_223", "file_type": "json", "from": "Achanta, R.", "predicate": "authored", "to": "Slic superpixels", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_223", "file_type": "json", "from": "Ojala, T.", "predicate": "authored", "to": "Multiresolution gray-scale texture classification", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_223", "file_type": "json", "from": "Bach, F. R.", "predicate": "authored", "to": "Multiple kernel learning", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_223", "file_type": "json", "from": "Multiple kernel learning", "predicate": "uses", "to": "SMO algorithm", "type": "factual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_223", "file_type": "json", "from": "Perazzi, F.", "predicate": "authored", "to": "Saliency filters", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_223", "file_type": "json", "from": "Borji, A.", "predicate": "authored", "to": "Salient object detection benchmark", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_301", "file_type": "json", "from": "Borji, A.", "predicate": "authored", "to": "Salient Object Detection: A Benchmark", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_223", "file_type": "json", "from": "authored", "predicate": "Segmenting salient objects", "to": "Rahtu, E.", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_223", "file_type": "json", "from": "Salient object segmentation", "predicate": "uses", "to": "performance measures", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_569", "file_type": "json", "from": "superpixels", "predicate": "classified_by", "to": "feedforward multilayer network", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_685", "file_type": "json", "from": "superpixels", "predicate": "builds", "to": "primitive saliency dictionary", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_223", "file_type": "json", "from": "Local binary patterns", "predicate": "used_in", "to": "texture classification", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_224", "file_type": "json", "from": "J. Heikkil\u00e4", "predicate": "authored", "to": "Segmenting salient objects from images and videos", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_224", "file_type": "json", "from": "Segmenting salient objects from images and videos", "predicate": "presented_at", "to": "ECCV", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_224", "file_type": "json", "from": "Kaiming He", "predicate": "co-authored", "to": "Convolutional Neural Networks at Constrained Time Cost", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_225", "file_type": "json", "from": "Kaiming He", "predicate": "is author of", "to": "paper", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_560", "file_type": "json", "from": "Kaiming He", "predicate": "affiliated_with", "to": "Microsoft Research", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_278", "file_type": "json", "from": "Kaiming He", "predicate": "author", "to": "A Geodesic-Prepreserving Method for Image Warping", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_279", "file_type": "json", "from": "Kaiming He", "predicate": "is_author_of", "to": "A Geolesic-Preerving Method for Image Warping", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_553", "file_type": "json", "from": "Kaiming He", "predicate": "author_of", "to": "Sparse Projections for High-Dimensional Binary Codes", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_224", "file_type": "json", "from": "Convolutional Neural Networks at Constained Time Cost", "predicate": "presented_at", "to": "CVPR", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_544", "file_type": "json", "from": "University of California at Merced", "predicate": "located_in", "to": "USA", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_225", "file_type": "json", "from": "architecture", "predicate": "achieves", "to": "competitive accuracy", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_225", "file_type": "json", "from": "architecture", "predicate": "is", "to": "20% faster than AlexNet", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_364", "file_type": "json", "from": "architecture", "predicate": "designed_for", "to": "person re-identification", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_528", "file_type": "json", "from": "architecture", "predicate": "is_example_of", "to": "application domain", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_569", "file_type": "json", "from": "architecture", "predicate": "achieves", "to": "69.6% average accuracy", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_569", "file_type": "json", "from": "architecture", "predicate": "tested_on", "to": "PAS-CAL VOC 2012 test set", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_225", "file_type": "json", "from": "accuracy improvements", "predicate": "influenced_by", "to": "factors", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_225", "file_type": "json", "from": "ImageNet dataset", "predicate": "has", "to": "11.8% top-5 error", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_226", "file_type": "json", "from": "Offline Training", "predicate": "has_constraint", "to": "Time Constraints", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_226", "file_type": "json", "from": "Accuracy Improvements", "predicate": "influenced_by", "to": "Factors", "type": "conceptual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_226", "file_type": "json", "from": "Accuracy Improvements", "predicate": "requires", "to": "Limited Time Budget", "type": "causal", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_529", "file_type": "json", "from": "Architecture", "predicate": "is_example_of", "to": "Computer-Aided Design application", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_570", "file_type": "json", "from": "Architecture", "predicate": "achieves_accuracy", "to": "PAS-CAL VOC 2012 test set", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_706", "file_type": "json", "from": "Architecture", "predicate": "is", "to": "recurrent convolutional", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_706", "file_type": "json", "from": "Architecture", "predicate": "is", "to": "large-scale visual learning", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_226", "file_type": "json", "from": "ImageNet Dataset", "predicate": "is_foundational_for", "to": "Computer Vision Tasks", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_226", "file_type": "json", "from": "ImageNet Dataset", "predicate": "is_hierarchical", "to": "Image Database", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_226", "file_type": "json", "from": "Deng et al. (2009)", "predicate": "introduces", "to": "ImageNet Dataset", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_664", "file_type": "json", "from": "Deng et al. (2009)", "predicate": "creates", "to": "ImageNet", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_226", "file_type": "json", "from": "Layer Replacement", "predicate": "is_optimization_strategy", "to": "Architecture Optimization", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_227", "file_type": "json", "from": "ImageNet", "predicate": "is_used_for", "to": "computer vision tasks", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_710", "file_type": "json", "from": "ImageNet", "predicate": "is_a", "to": "hierarchical image database", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_312", "file_type": "json", "from": "ImageNet", "predicate": "is_database_for", "to": "Hierarchical Image Database", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_389", "file_type": "json", "from": "ImageNet", "predicate": "is_competition", "to": "ILSVRC2012", "type": "factual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_407", "file_type": "json", "from": "ImageNet", "predicate": "is", "to": "large dataset", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_407", "file_type": "json", "from": "ImageNet", "predicate": "used for", "to": "training computer vision models", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_408", "file_type": "json", "from": "ImageNet", "predicate": "is_used_by", "to": "AlexNet", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_594", "file_type": "json", "from": "ImageNet", "predicate": "is_classified_with", "to": "deep convolutional neural networks", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_710", "file_type": "json", "from": "ImageNet", "predicate": "presented_at", "to": "CVPR", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_711", "file_type": "json", "from": "ImageNet", "predicate": "is_database_of", "to": "hierarchical image", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_227", "file_type": "json", "from": "very deep convolutional networks", "predicate": "is_a", "to": "development", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_227", "file_type": "json", "from": "very deep convolutional networks", "predicate": "is_a", "to": "network architecture", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_227", "file_type": "json", "from": "multi-column deep neural networks", "predicate": "demonstrates", "to": "power of deep learning", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_227", "file_type": "json", "from": "multi-column deep neural networks", "predicate": "is_used_for", "to": "image classification", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_228", "file_type": "json", "from": "rich feature hierarchies", "predicate": "used_for", "to": "object detection", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_228", "file_type": "json", "from": "rich feature hierarchies", "predicate": "used_for", "to": "semantic segmentation", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_227", "file_type": "json", "from": "deep learning", "predicate": "uses", "to": "multi-column deep neural networks", "type": "factual", "width": 0.86}, {"arrows": "to", "chunk_id": "doc_0_chunk_247", "file_type": "json", "from": "deep learning", "predicate": "encompasses", "to": "AlexNet", "type": "conceptual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_404", "file_type": "json", "from": "deep learning", "predicate": "is_related_to", "to": "intrinsic image decomposition", "type": "conceptual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_228", "file_type": "json", "from": "Malik et al. (2014)", "predicate": "introduces", "to": "rich feature hierarchies", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_228", "file_type": "json", "from": "Zeiler et al. (2014)", "predicate": "focuses_on", "to": "convolutional neural networks", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_228", "file_type": "json", "from": "convolutional neural networks", "predicate": "important_for", "to": "interpretability", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_228", "file_type": "json", "from": "Eigen et al. (2013)", "predicate": "explores", "to": "deep architectures", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_228", "file_type": "json", "from": "deep architectures", "predicate": "understood_using", "to": "recursive convolutional networks", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_228", "file_type": "json", "from": "recursive convolutional networks", "predicate": "used_to_understand", "to": "deep architectures", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_228", "file_type": "json", "from": "Chatfield et al. (2014)", "predicate": "investigates", "to": "convolutional networks", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_228", "file_type": "json", "from": "Chatfield et al. (2014)", "predicate": "focuses_on", "to": "details", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_234", "file_type": "json", "from": "convolutional networks", "predicate": "used_in", "to": "image processing", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_325", "file_type": "json", "from": "convolutional networks", "predicate": "uses", "to": "output of last layer", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_229", "file_type": "json", "from": "Return of the devil in the details", "predicate": "investigates", "to": "convolutional networks", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_229", "file_type": "json", "from": "Overfeat", "predicate": "is", "to": "integrated recognition, localization, and detection system", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_229", "file_type": "json", "from": "Going deeper with convolutions", "predicate": "explores", "to": "convolutions", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_230", "file_type": "json", "from": "Lei Zhang", "predicate": "is_author_of", "to": "Reweighted Laplace Prior Based Hyperspectral Compressive Sensing", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_230", "file_type": "json", "from": "Yanning Zhang", "predicate": "is_author_of", "to": "Reweighted Laplace Prior Based Hyperspectral Compressive Sensing", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_231", "file_type": "json", "from": "Yanning Zhang", "predicate": "author_of", "to": "Zhang_Reweighted_Laplace_Prior_2015_CVPR_supplemental.pdf", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_229", "file_type": "json", "from": "Chunna Tian", "predicate": "is_author_of", "to": "Reweighted Laplace Prior Based Hyperspectral Compressive Sensing", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_230", "file_type": "json", "from": "Chunna Tian", "predicate": "is_author_of", "to": "Reweighted Laplace Prior Based Hyperspectra Compressives Sensing", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_231", "file_type": "json", "from": "Chunna Tian", "predicate": "author_of", "to": "Zhang_Reweighted_Lapless_Prior_2015_CVPR_supplemental.pdf", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_229", "file_type": "json", "from": "Fei Li", "predicate": "is_author_of", "to": "Reweighted Laplace Prior Based Hyperspectral Compressive Sensing", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_230", "file_type": "json", "from": "Fei Li", "predicate": "is_author_of", "to": "Reweighted Laplace Prior Based Hyperspectral Compressives Sensing", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_231", "file_type": "json", "from": "Fei Li", "predicate": "author_of", "to": "Zhang_Reweighted_Laplace_Prior_2015_CVPR_supplemental.pdf", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_230", "file_type": "json", "from": "Wei Wei", "predicate": "is_author_of", "to": "Reweighted Laplace Prior Based Hyperspectral Compressive Sensing", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_230", "file_type": "json", "from": "Reweighted Laplace Prior Based Hyperspectral Compressives Sensing", "predicate": "is_published_in", "to": "CVPR", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_230", "file_type": "json", "from": "Reweighted Laplace Prior Based Hyperspectral Compressives Sensing", "predicate": "deals_with", "to": "Hyperspectral Compressives Sensing", "type": "conceptual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_230", "file_type": "json", "from": "Reweighted Laplace Prior Based Hyperspectral Compressives Sensing", "predicate": "uses", "to": "Laplace Prior", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_230", "file_type": "json", "from": "Reweighted Laplace Prior Based Hyperspectral Compressives Sensing", "predicate": "is_supplemental_material_for", "to": "Zhang_Reweighted_Laplace_Prior_2015_CVPR_supplemental.pdf", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_230", "file_type": "json", "from": "Zhang_Reweighted_Laplace_Prior_2015_CVPR_supplemental.pdf", "predicate": "is_located_at", "to": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers", "type": "factual", "width": 0.7}, {"arrows": "to", "chunk_id": "doc_0_chunk_231", "file_type": "json", "from": "Zhang_Reweighted_Laplace_Prior_2015_CVPR_supplemental.pdf", "predicate": "details", "to": "hyperspectral compressive sensing method", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_231", "file_type": "json", "from": "hyperspectral compressive sensing method", "predicate": "utilizes", "to": "reweighted Laplace prior", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_231", "file_type": "json", "from": "optimization procedure", "predicate": "involves", "to": "matrix algebra manipulations", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_231", "file_type": "json", "from": "optimization procedure", "predicate": "involves", "to": "conjugate functions", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_231", "file_type": "json", "from": "optimization procedure", "predicate": "includes", "to": "sparsity learning over \u03b3", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_231", "file_type": "json", "from": "optimization procedure", "predicate": "includes", "to": "noise estimation over \u03bb", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_231", "file_type": "json", "from": "conjugate functions", "predicate": "transforms", "to": "non-convex optimization problems", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_232", "file_type": "json", "from": "Hyberspectral Compressive Sensing", "predicate": "is_a", "to": "approach", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_232", "file_type": "json", "from": "Reweighted Laplace Prior", "predicate": "is_a", "to": "optimization technique", "type": "conceptual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_232", "file_type": "json", "from": "Sparsity Learning", "predicate": "is_a", "to": "optimization technique", "type": "conceptual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_232", "file_type": "json", "from": "Noise Estimation", "predicate": "is_a", "to": "optimization technique", "type": "conceptual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_232", "file_type": "json", "from": "Fang_Collaborative_Feature_Learning_2015_CVPR_paper", "predicate": "is_a", "to": "research paper", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_232", "file_type": "json", "from": "Fang_Collaborative_Feature_Learning_2015_CVPR_paper", "predicate": "published_in", "to": "CVPR", "type": "factual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_233", "file_type": "json", "from": "Paper Abstract", "predicate": "requires", "to": "readable text", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_233", "file_type": "json", "from": "Research", "predicate": "potentially covers", "to": "Data Encoding/Decoding", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_233", "file_type": "json", "from": "Research", "predicate": "potentially covers", "to": "Text Corruption/Error Correction", "type": "conceptual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_233", "file_type": "json", "from": "Research", "predicate": "potentially covers", "to": "Pattern Recognition", "type": "conceptual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_233", "file_type": "json", "from": "Research", "predicate": "potentially covers", "to": "Information Retrieval", "type": "conceptual", "width": 0.7}, {"arrows": "to", "chunk_id": "doc_0_chunk_453", "file_type": "json", "from": "Research", "predicate": "connects", "to": "Salience", "type": "conceptual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_629", "file_type": "json", "from": "Research", "predicate": "discusses", "to": "Material Metamers", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_629", "file_type": "json", "from": "Research", "predicate": "has implications for", "to": "Visual Inference tasks", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_233", "file_type": "json", "from": "Victor Escorcia", "predicate": "is_author_of", "to": "On the Relationship between Visual Attributes and Convolutional Networks", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_234", "file_type": "json", "from": "Victor Escorcia", "predicate": "authored", "to": "paper", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_235", "file_type": "json", "from": "Victor Escorcia", "predicate": "author_of", "to": "Escorcia_On_the_Relationship_2015_CVPR_paper.pdf", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_237", "file_type": "json", "from": "Victor Escorcia", "predicate": "affiliated with", "to": "King Abdullah University of Science and Technology", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_233", "file_type": "json", "from": "Juan Carlos Niebles", "predicate": "is_author_of", "to": "On the Relationship between Visual Attributes and Convolutional Networks", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_234", "file_type": "json", "from": "Juan Carlos Niebles", "predicate": "authored", "to": "paper", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_235", "file_type": "json", "from": "Juan Carlos Niebles", "predicate": "author_of", "to": "Escorcia_On_the_Relationship_2015_CVPR_paper.pdf", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_237", "file_type": "json", "from": "Juan Carlos Niebles", "predicate": "affiliated with", "to": "Universidad del Norte", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_233", "file_type": "json", "from": "Bernard Ghanem", "predicate": "is_author_of", "to": "On the Relationship between Visual Attributes and Convolutional Networks", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_234", "file_type": "json", "from": "Bernard Ghanem", "predicate": "authored", "to": "paper", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_235", "file_type": "json", "from": "Bernard Ghanem", "predicate": "author_of", "to": "Escorcia_On_the_Relationship_2015_CVPR_paper.pdf", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_237", "file_type": "json", "from": "Bernard Ghanem", "predicate": "affiliated with", "to": "King Abdullah University of Science and Technology", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_361", "file_type": "json", "from": "Bernard Ghanem", "predicate": "is_author_of", "to": "\u21130TV: A New Method", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_363", "file_type": "json", "from": "Bernard Ghanem", "predicate": "affiliated_with", "to": "King Abdullah University of Science and Technology (KAUST)", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_363", "file_type": "json", "from": "Bernard Ghanem", "predicate": "email", "to": "bernard.ghanem@kust.edu.sa", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_233", "file_type": "json", "from": "Convolutional Networks", "predicate": "related_to", "to": "Visual Attributes", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_237", "file_type": "json", "from": "Visual Attributes", "predicate": "influence", "to": "conv-net based object recognition", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_234", "file_type": "json", "from": "visual attributes", "predicate": "impacts", "to": "convolutional networks", "type": "causal", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_234", "file_type": "json", "from": "Escorcia_On_the_Relationship_2015_CVPR_paper.pdf", "predicate": "is_a", "to": "CVPR paper", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_235", "file_type": "json", "from": "Escorcia_On_the_Relationship_2015_CVPR_paper.pdf", "predicate": "located_in", "to": "/mnt/DATA/Glucomaa/LLM/Ollama_pdf_handle/cvpr_papers", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_235", "file_type": "json", "from": "conv-nets", "predicate": "learns", "to": "abstract concepts", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_235", "file_type": "json", "from": "conv-nets", "predicate": "use", "to": "semantic visual attributes", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_235", "file_type": "json", "from": "abstract concepts", "predicate": "are_examples_of", "to": "objects in images", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_235", "file_type": "json", "from": "semantic visual attributes", "predicate": "impact", "to": "object description", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_235", "file_type": "json", "from": "Attribute Centric Nodes (ACNs)", "predicate": "exist_in", "to": "conv-net", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_235", "file_type": "json", "from": "Attribute Centric Nodes (ACNs)", "predicate": "encode", "to": "visual attribute representation", "type": "factual", "width": 0.91}, {"arrows": "to", "chunk_id": "doc_0_chunk_235", "file_type": "json", "from": "Attribute Centric Nodes (ACNs)", "predicate": "contribute_to", "to": "discrimination", "type": "factual", "width": 0.87}, {"arrows": "to", "chunk_id": "doc_0_chunk_235", "file_type": "json", "from": "conv-net", "predicate": "is_trained_to_recognize", "to": "objects", "type": "factual", "width": 0.89}, {"arrows": "to", "chunk_id": "doc_0_chunk_237", "file_type": "json", "from": "conv-net", "predicate": "performs", "to": "object recognition", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_236", "file_type": "json", "from": "visual attribute representation", "predicate": "encoded by", "to": "conv-net nodes", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_236", "file_type": "json", "from": "conv-net nodes", "predicate": "encode", "to": "information", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_236", "file_type": "json", "from": "conv-net nodes", "predicate": "distributed across", "to": "layers", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_236", "file_type": "json", "from": "conv-net nodes", "predicate": "are", "to": "sparsely distributed", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_236", "file_type": "json", "from": "conv-net nodes", "predicate": "play a role in", "to": "object recognition", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_236", "file_type": "json", "from": "conv-net nodes", "predicate": "are", "to": "unevenly distributed", "type": "factual", "width": 0.87}, {"arrows": "to", "chunk_id": "doc_0_chunk_236", "file_type": "json", "from": "information", "predicate": "pertinent to", "to": "visual attribute representation and discrimination", "type": "conceptual", "width": 0.91}, {"arrows": "to", "chunk_id": "doc_0_chunk_237", "file_type": "json", "from": "Zero-Shot Object Recognition", "predicate": "uses", "to": "Semantic Manifold Distance", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_237", "file_type": "json", "from": "Zhenyong Fu", "predicate": "authored", "to": "Zero-Shot Object Recognition", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_242", "file_type": "json", "from": "Zhenyong Fu", "predicate": "affiliated_with", "to": "Queen Mary, University of London", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_237", "file_type": "json", "from": "Tao Xiang", "predicate": "authored", "to": "Zero-Shot Object Recognition", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_241", "file_type": "json", "from": "Tao Xiang", "predicate": "affiliated_with", "to": "Queen Mary, University of London", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_242", "file_type": "json", "from": "Tao Xiang", "predicate": "affiliated_with", "to": "QueenMary, University of London", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_237", "file_type": "json", "from": "Elyor Kodirov", "predicate": "authored", "to": "Zero-Shot Object Research", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_242", "file_type": "json", "from": "Elyor Kodirov", "predicate": "affiliated_with", "to": "Queen Mary, University of London", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_237", "file_type": "json", "from": "Shaogang Gong", "predicate": "authored", "to": "Zero-Shot Object Research", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_238", "file_type": "json", "from": "Zero-shot learning", "predicate": "aims_to", "to": "recognise objects", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_238", "file_type": "json", "from": "Zero-shot learning", "predicate": "learns", "to": "knowledge transfer", "type": "conceptual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_238", "file_type": "json", "from": "existing works", "predicate": "measures", "to": "similarity", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_238", "file_type": "json", "from": "similarity", "predicate": "located_in", "to": "semantic embedding space", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_238", "file_type": "json", "from": "semantic embedding space", "predicate": "uses", "to": "distance metrics", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_238", "file_type": "json", "from": "distance metrics", "predicate": "do_not_consider", "to": "intrinsic structure", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_238", "file_type": "json", "from": "semantic categories", "predicate": "has", "to": "intrinsic structure", "type": "conceptual", "width": 0.82}, {"arrows": "to", "chunk_id": "doc_0_chunk_238", "file_type": "json", "from": "absorbing Markov chain process", "predicate": "computes", "to": "semantic manifold distance", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_239", "file_type": "json", "from": "performance gains", "predicate": "observed_on", "to": "ImageNet", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_239", "file_type": "json", "from": "performance gains", "predicate": "observed_on", "to": "AwA datasets", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_239", "file_type": "json", "from": "proposed model", "predicate": "unifies", "to": "ZSL algorithms", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_239", "file_type": "json", "from": "proposed model", "predicate": "demonstrates", "to": "performance gains", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_239", "file_type": "json", "from": "semantic manifold", "predicate": "used_by", "to": "AMP", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_239", "file_type": "json", "from": "AMP", "predicate": "computes", "to": "semantic manifold distance", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_239", "file_type": "json", "from": "Label-embedding", "predicate": "presented_in", "to": "Akata et al. (2013)", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_322", "file_type": "json", "from": "Label-embedding", "predicate": "supports", "to": "attribute-based classification", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_239", "file_type": "json", "from": "Single-example learning", "predicate": "presented_in", "to": "Bart \u0026 Ullman (2005)", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_239", "file_type": "json", "from": "Cluster kernels", "predicate": "presented_in", "to": "Chapelle et al. (2002)", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_239", "file_type": "json", "from": "Zero-shot learning (ZSL)", "predicate": "relies_on", "to": "knowledge transfer", "type": "conceptual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_240", "file_type": "json", "from": "Chapelle", "predicate": "authored", "to": "Cluster kernels", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_240", "file_type": "json", "from": "Weston", "predicate": "authored", "to": "Cluster kernels", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_240", "file_type": "json", "from": "Sch\u00f6lkopf", "predicate": "authored", "to": "Cluster kernels", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_240", "file_type": "json", "from": "Deng", "predicate": "authored", "to": "Large-scale object classification", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_240", "file_type": "json", "from": "Deng", "predicate": "authored", "to": "ImageNet", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_240", "file_type": "json", "from": "Ding", "predicate": "authored", "to": "Large-scale object classification", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_639", "file_type": "json", "from": "Ding", "predicate": "authored", "to": "Learning higher-order graph structure", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_639", "file_type": "json", "from": "Ding", "predicate": "published_in", "to": "Advances in Neural Information Processing Systems", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_240", "file_type": "json", "from": "Label relation graphs", "predicate": "used_in", "to": "Large-scale object classification", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_240", "file_type": "json", "from": "Dong", "predicate": "authored", "to": "ImageNet", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_240", "file_type": "json", "from": "Frome", "predicate": "authored", "to": "Devise", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_240", "file_type": "json", "from": "Devise", "predicate": "is_a", "to": "embedding model", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_241", "file_type": "json", "from": "Devise", "predicate": "presented_at", "to": "Advances in Neural Information Processing Systems", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_241", "file_type": "json", "from": "Devise", "predicate": "implements", "to": "visual-semantic embedding", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_241", "file_type": "json", "from": "Transductive multi-view embedding", "predicate": "presented_at", "to": "ECCV", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_241", "file_type": "json", "from": "Transductive multi-view embedding", "predicate": "aims_for", "to": "zero-shot recognition", "type": "conceptual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_241", "file_type": "json", "from": "Zero shot recognition with unreliable attributes", "predicate": "published_as", "to": "arXiv preprint", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_710", "file_type": "json", "from": "ImageNet classification", "predicate": "presented_at", "to": "NIPS", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_593", "file_type": "json", "from": "ImageNet classification", "predicate": "used", "to": "deep convolutional neural networks", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_552", "file_type": "json", "from": "ImageNet classification", "predicate": "uses", "to": "SIFT features", "type": "conceptual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_241", "file_type": "json", "from": "Efficient estimation of word representations", "predicate": "published_as", "to": "arXiv preprint", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_242", "file_type": "json", "from": "Sakrapee Paisitkriangkrai", "predicate": "author_of", "to": "Learning to rank in person re-identi\ufb01cation with metric ensembles", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_249", "file_type": "json", "from": "Sakrapee Paisitkriangkrai", "predicate": "affiliated_with", "to": "The University of Adelaide", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_249", "file_type": "json", "from": "Sakrapee Paisitkriangkrai", "predicate": "affiliated_with", "to": "Australian Centre for Robotic Vision", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_242", "file_type": "json", "from": "Learning to rank in person re-identi\ufb01cation with metric ensembles", "predicate": "is_paper_of", "to": "CVPR", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_242", "file_type": "json", "from": "Learning to rank in person re-identi\ufb01cation with metric ensembles", "predicate": "published_in", "to": "2015", "type": "factual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_242", "file_type": "json", "from": "Learning to rank in person re-identi\ufb01cation with metric ensembles", "predicate": "file_name", "to": "Paisitkriangrai_Learning_to_Rank_2015_CVPR_paper.pdf", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_242", "file_type": "json", "from": "Chunhua Shen", "predicate": "author_of", "to": "Learning to rank in person re-identi\ufb01cation with metric ensembles", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_249", "file_type": "json", "from": "Chunhua Shen", "predicate": "affiliated_with", "to": "The University of Adelaide", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_249", "file_type": "json", "from": "Chunhua Shen", "predicate": "affiliated_with", "to": "Australian Centre for Robotic Vision", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_250", "file_type": "json", "from": "Chunhua Shen", "predicate": "affiliated_with", "to": "The University of Adelaide, Australia", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_438", "file_type": "json", "from": "Chunhua Shen", "predicate": "author_of", "to": "Supervised Discrete Hashing", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_639", "file_type": "json", "from": "Chunhua Shen", "predicate": "affiliated_with", "to": "University of Adelaide", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_634", "file_type": "json", "from": "Chunhua Shen", "predicate": "author_of", "to": "Learning graph structure...", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_242", "file_type": "json", "from": "Anton van den Hengel", "predicate": "author_of", "to": "Learning to rank in person re-identi\ufb01cation with metric ensembles", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_250", "file_type": "json", "from": "Anton van den Hengel", "predicate": "affiliated_with", "to": "The University of Adelaide, Australia", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_639", "file_type": "json", "from": "Anton van den Hengel", "predicate": "affiliated_with", "to": "Australian Centre for Robotic Vision", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_492", "file_type": "json", "from": "Anton van den Hengel", "predicate": "is_author_of", "to": "Robust Multiple Homography Estimation", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_634", "file_type": "json", "from": "Anton van den Hengel", "predicate": "author_of", "to": "Learning graph structure...", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_639", "file_type": "json", "from": "Anton van den Hengel", "predicate": "affiliated_with", "to": "University of Adelaide", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_243", "file_type": "json", "from": "predefined weights", "predicate": "are", "to": "not adaptable", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_243", "file_type": "json", "from": "two principled approaches", "predicate": "optimize", "to": "relative distance", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_243", "file_type": "json", "from": "two principled approaches", "predicate": "maximize", "to": "average rank-k recognition rate", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_245", "file_type": "json", "from": "RMS methods", "predicate": "improves", "to": "rank-1 recognition rates", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_245", "file_type": "json", "from": "Ensemble-based approaches", "predicate": "are", "to": "flexible", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_245", "file_type": "json", "from": "Ensemble-based approaches", "predicate": "can be combined with", "to": "linear metrics", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_245", "file_type": "json", "from": "Ensemble-based approaches", "predicate": "can be combined with", "to": "non-linear metrics", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_245", "file_type": "json", "from": "Similarity metric", "predicate": "introduced in", "to": "Chopra et al. (2005)", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_245", "file_type": "json", "from": "Person Re-Identification", "predicate": "is covered in", "to": "Gong et al. (2014)", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_245", "file_type": "json", "from": "Person Re-Identification", "predicate": "relies on", "to": "Similarity metric", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_246", "file_type": "json", "from": "Person Re-Identification", "predicate": "described_in", "to": "Gong, S., Crisitan, M., Yan, S., and Loy, C. C.", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_245", "file_type": "json", "from": "Gong et al. (2014)", "predicate": "provides", "to": "techniques", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_685", "file_type": "json", "from": "techniques", "predicate": "adopt", "to": "solution frameworks", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_246", "file_type": "json", "from": "Gong, S., Crisitan, M., Yan, S., and Loy, C. C.", "predicate": "provides", "to": "techniques", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_246", "file_type": "json", "from": "support vector method", "predicate": "relevant_to", "to": "performance measures", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_246", "file_type": "json", "from": "support vector method", "predicate": "authored", "to": "Joachims, T.", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_247", "file_type": "json", "from": "re-identification models", "predicate": "builds_upon", "to": "AlexNet", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_246", "file_type": "json", "from": "re-identification systems", "predicate": "evaluated_using", "to": "performance measures", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_246", "file_type": "json", "from": "Imaginet classification", "predicate": "utilized", "to": "deep convolutional neural networks", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_247", "file_type": "json", "from": "Mahalanobis distance", "predicate": "is_technique", "to": "metric learning", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_247", "file_type": "json", "from": "Mahalanobis distance", "predicate": "is_used_in", "to": "person re-identification", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_248", "file_type": "json", "from": "metric learning", "predicate": "is_aspect_of", "to": "scalability", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_248", "file_type": "json", "from": "distance metric learning", "predicate": "addresses", "to": "computational challenges", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_247", "file_type": "json", "from": "distance metric learning", "predicate": "improves", "to": "scalability", "type": "causal", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_248", "file_type": "json", "from": "computational challenges", "predicate": "impacts", "to": "scalability", "type": "causal", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_248", "file_type": "json", "from": "paper (Felzenszwalb et al., 2010)", "predicate": "introduces", "to": "part-based model", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_248", "file_type": "json", "from": "paper (Felzenszwalb et al., 2010)", "predicate": "addresses", "to": "object detection", "type": "factual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_248", "file_type": "json", "from": "part-based model", "predicate": "relevant_to", "to": "pedestrian detection", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_249", "file_type": "json", "from": "image representation techniques", "predicate": "used_in", "to": "re-identification", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_249", "file_type": "json", "from": "shape and appearance information", "predicate": "is_strategy_for", "to": "robust re-identification", "type": "conceptual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_249", "file_type": "json", "from": "kernel methods", "predicate": "used_in", "to": "metric learning", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_357", "file_type": "json", "from": "kernel methods", "predicate": "operates_on", "to": "geodesic metric spaces", "type": "conceptual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_359", "file_type": "json", "from": "kernel methods", "predicate": "include", "to": "geodesic Laplacian kernels", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_250", "file_type": "json", "from": "Australian Centre for Robotic Vision", "predicate": "located_in", "to": "Australia", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_640", "file_type": "json", "from": "Australian Centre for Robotic Vision", "predicate": "part_of", "to": "University of Adelaide", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_250", "file_type": "json", "from": "Adelaide, Australia", "predicate": "is_located_in", "to": "Australia", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_250", "file_type": "json", "from": "The University of Adelaide, Australia", "predicate": "located_in", "to": "Australia", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_250", "file_type": "json", "from": "Dengxin Dai", "predicate": "author_of", "to": "Metric imitation by manifold transfer", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_254", "file_type": "json", "from": "Dengxin Dai", "predicate": "affiliated_with", "to": "Computer Vision Lab, ETH Zurich", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_250", "file_type": "json", "from": "Metric imitation by manifold transfer", "predicate": "application_area", "to": "efficient vision applications", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_250", "file_type": "json", "from": "Till Kroeger", "predicate": "author_of", "to": "Metric imitation by manifold transfer", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_250", "file_type": "json", "from": "Radu Timofte", "predicate": "author_of", "to": "Metric imitation by manifold transfer", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_255", "file_type": "json", "from": "Radu Timofte", "predicate": "affiliated_with", "to": "Computer Vision Lab, ETH Zurich", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_251", "file_type": "json", "from": "Metric Imitation", "predicate": "aims_to", "to": "improve performance", "type": "conceptual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_251", "file_type": "json", "from": "Metric Imitation", "predicate": "transfers", "to": "manifold structure", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_251", "file_type": "json", "from": "Metric Imitation", "predicate": "demonstrated_on", "to": "instance-based object retrieval", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_251", "file_type": "json", "from": "Metric Imitation", "predicate": "demonstrated_on", "to": "image clustering", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_251", "file_type": "json", "from": "Metric Imitation", "predicate": "demonstrated_on", "to": "category-based image retrieval", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_251", "file_type": "json", "from": "Metric Imitation", "predicate": "yields", "to": "better performance", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_251", "file_type": "json", "from": "vision applications", "predicate": "benefits_from", "to": "Metric Imitation", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_251", "file_type": "json", "from": "GIST features", "predicate": "used_as", "to": "target features", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_251", "file_type": "json", "from": "SIFT-llc", "predicate": "used_as", "to": "source features", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_251", "file_type": "json", "from": "object-bank", "predicate": "used_as", "to": "source features", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_251", "file_type": "json", "from": "CNN features", "predicate": "used_as", "to": "source features", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_252", "file_type": "json", "from": "CNN features", "predicate": "is_source_feature", "to": "performance", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_252", "file_type": "json", "from": "Metric Imitation (MI)", "predicate": "yields", "to": "better performance", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_252", "file_type": "json", "from": "Metric Imitation (MI)", "predicate": "compared_to", "to": "original target features", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_252", "file_type": "json", "from": "Bosch, A.", "predicate": "authored", "to": "Image classification using random forests and ferns", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_252", "file_type": "json", "from": "Chatfield, K.", "predicate": "authored", "to": "Return of the devil in the details", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_252", "file_type": "json", "from": "Dai, D.", "predicate": "authored", "to": "Ensemble partitioning", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_252", "file_type": "json", "from": "llc", "predicate": "is_source_feature", "to": "performance", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_252", "file_type": "json", "from": "Object Retrieval", "predicate": "relates_to", "to": "Object-bank (OB)", "type": "conceptual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_252", "file_type": "json", "from": "Image Clustering", "predicate": "relates_to", "to": "Ensemble partitioning", "type": "conceptual", "width": 0.7}, {"arrows": "to", "chunk_id": "doc_0_chunk_253", "file_type": "json", "from": "Dana et al.", "predicate": "authored", "to": "Reflectance and texture of real-world surfaces", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_253", "file_type": "json", "from": "Reflectance and texture of real-world surfaces", "predicate": "published in", "to": "*ACM Trans. Graph.*", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_253", "file_type": "json", "from": "Fei-Fei et al.", "predicate": "authored", "to": "Learning generative visual models", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_253", "file_type": "json", "from": "Learning generative visual models", "predicate": "presented at", "to": "Workshop on Generative-Model Based Vision", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_253", "file_type": "json", "from": "Lazebnik et al.", "predicate": "authored", "to": "Spatial pyramid matching", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_253", "file_type": "json", "from": "Spatial pyramid matching", "predicate": "used for", "to": "recognizing natural scene categories", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_253", "file_type": "json", "from": "Li \u0026 Fei-Fei", "predicate": "authored", "to": "What, where and who?", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_377", "file_type": "json", "from": "What, where and who?", "predicate": "authored", "to": "Li, L.-J.", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_377", "file_type": "json", "from": "What, where and who?", "predicate": "published", "to": "ICCV", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_253", "file_type": "json", "from": "Li et al. (2010)", "predicate": "authored", "to": "Object bank", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_254", "file_type": "json", "from": "Object bank", "predicate": "aims_to", "to": "scene classification", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_377", "file_type": "json", "from": "Object bank", "predicate": "published", "to": "NIPS", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_254", "file_type": "json", "from": "Li, L.-J. et al. (2010)", "predicate": "published_in", "to": "*NIPS*", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_254", "file_type": "json", "from": "Nist\u00b4er, D. \u0026 Stew\u00b4enius, H. (2006)", "predicate": "published_in", "to": "*CVPR*", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_254", "file_type": "json", "from": "Vocabulary tree", "predicate": "enables", "to": "scalable recognition", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_254", "file_type": "json", "from": "Oliva, A. \u0026 Torralba, A. (2001)", "predicate": "published_in", "to": "*IJCV*", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_254", "file_type": "json", "from": "Spatial envelope", "predicate": "represents", "to": "shape of the scene", "type": "conceptual", "width": 0.89}, {"arrows": "to", "chunk_id": "doc_0_chunk_254", "file_type": "json", "from": "Tuytelaars, T. et al. (2009)", "predicate": "published_in", "to": "*IJCLP*", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_254", "file_type": "json", "from": "Object discovery", "predicate": "is", "to": "unsupervised", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_254", "file_type": "json", "from": "Wang, J. et al. (2010)", "predicate": "published_in", "to": "*CVPR*", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_254", "file_type": "json", "from": "Locality-constrained linear coding", "predicate": "applied_to", "to": "image classification", "type": "conceptual", "width": 0.87}, {"arrows": "to", "chunk_id": "doc_0_chunk_255", "file_type": "json", "from": "Computer Vision Lab, ETH Zurich", "predicate": "located_in", "to": "ETH Zurich", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_255", "file_type": "json", "from": "Francesco Pittaluca", "predicate": "affiliated_with", "to": "Computer Vision Lab, ETH Zurich", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_255", "file_type": "json", "from": "Francesco Pittaluca", "predicate": "authored", "to": "Privacy Preserving Optics for Miniature Vision Sensors", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_255", "file_type": "json", "from": "Sanjeev J. Koppal", "predicate": "affiliated_with", "to": "Computer Vision Lab, ETH Zurich", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_255", "file_type": "json", "from": "Sanjeev J. Koppal", "predicate": "authored", "to": "Privacy Preserving Optics for Miniature Vision Sensors", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_255", "file_type": "json", "from": "Pittaluga_Privacy_Preserving_Optics_2015_CVPR_supplemental.pdf", "predicate": "is_supplement_to", "to": "Privacy Preserving Optics for Miniature Vision Sensors", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_255", "file_type": "json", "from": "Privacy Preseving Optics for Miniature Vision Sensors", "predicate": "presented_at", "to": "CVPR", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_256", "file_type": "json", "from": "supplementary material", "predicate": "for", "to": "privacy-preserving optics", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_256", "file_type": "json", "from": "privacy-preserving optics", "predicate": "designed for", "to": "minature vision sensors", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_256", "file_type": "json", "from": "impact of defocusing optics", "predicate": "on", "to": "performance of face recognition algorithms", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_256", "file_type": "json", "from": "angular support", "predicate": "for", "to": "FLIR One thermal sensor", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_256", "file_type": "json", "from": "angular support", "predicate": "for", "to": "Kinect time-of-flight sensor", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_257", "file_type": "json", "from": "FLIR One thermal sensor", "predicate": "related_to", "to": "Angular support derivation", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_257", "file_type": "json", "from": "Kinect time-of-flight sensor", "predicate": "related_to", "to": "Angular support derivation", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_256", "file_type": "json", "from": "effect of blur", "predicate": "on", "to": "face recognition rates", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_256", "file_type": "json", "from": "geometric derivations", "predicate": "for", "to": "determining angular support", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_257", "file_type": "json", "from": "Privacy-preserving optics", "predicate": "concerns", "to": "Facial images", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_258", "file_type": "json", "from": "Feret methodology", "predicate": "is_standard_for", "to": "face recognition evaluation", "type": "conceptual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_257", "file_type": "json", "from": "CSU face identification evaluation system", "predicate": "describes", "to": "Face identification", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_257", "file_type": "json", "from": "Angular support derivation", "predicate": "relevant_to", "to": "Sensor positioning", "type": "conceptual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_257", "file_type": "json", "from": "Bolme, D. S. et al.", "predicate": "authored", "to": "CSU face identification evaluation system", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_257", "file_type": "json", "from": "Newton, E. et al.", "predicate": "authored", "to": "Privacy-preserving techniques", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_257", "file_type": "json", "from": "Phillips, P. J. et al.", "predicate": "authored", "to": "Feret evaluation methodology", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_257", "file_type": "json", "from": "Feret evaluation methodology", "predicate": "standard_for", "to": "Face recognition evaluation", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_258", "file_type": "json", "from": "Park, Min-Gyu", "predicate": "is_author_of", "to": "Leveraging Stereo Matching with Learning-based Con\ufb01dence Measures", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_258", "file_type": "json", "from": "Park, Min-Gyu", "predicate": "affiliated_with", "to": "University of Florida", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_258", "file_type": "json", "from": "Yoon, Kuk-Jin", "predicate": "is_author_of", "to": "Leverging Stereo Matching with Learning-based Con\ufb01dence Measures", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_258", "file_type": "json", "from": "Pittaluga, Francesco", "predicate": "is_author_of", "to": "P. J. The furet evaluation methodology", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_258", "file_type": "json", "from": "Pittaluga, Francesco", "predicate": "affiliated_with", "to": "University of Florida", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_258", "file_type": "json", "from": "Random Forests", "predicate": "is_machine_learning_algorithm", "to": "algorithm", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_259", "file_type": "json", "from": "Random Forests", "predicate": "is_a", "to": "machine learning algorithm", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_259", "file_type": "json", "from": "Random Forests", "predicate": "is_used_in", "to": "machine learning", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_341", "file_type": "json", "from": "Random Forests", "predicate": "is_published_in", "to": "Machine Learning", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_258", "file_type": "json", "from": "Breiman, L.", "predicate": "authored", "to": "Random forests", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_342", "file_type": "json", "from": "Random forests", "predicate": "used for", "to": "real time 3d face analysis", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_258", "file_type": "json", "from": "Koppal, Sanjeev J.", "predicate": "affiliated_with", "to": "University of Florida", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_341", "file_type": "json", "from": "Breiman", "predicate": "authored", "to": "Random Forests", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_259", "file_type": "json", "from": "stereo confidence metric", "predicate": "is_a", "to": "key aspect of stereo vision", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_259", "file_type": "json", "from": "stereo confidence metric", "predicate": "is_related_to", "to": "stereo vision", "type": "conceptual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_259", "file_type": "json", "from": "Egnal", "predicate": "authored", "to": "stereo confidence metric", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_259", "file_type": "json", "from": "Hirschm\u00fcller", "predicate": "presented", "to": "semiglobal matching", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_259", "file_type": "json", "from": "semiglobal matching", "predicate": "uses", "to": "mutual information", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_259", "file_type": "json", "from": "semiglobal matching", "predicate": "is_technique_for", "to": "stereo processing", "type": "factual", "width": 0.91}, {"arrows": "to", "chunk_id": "doc_0_chunk_261", "file_type": "json", "from": "stereo vision", "predicate": "requires", "to": "confidence measures", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_261", "file_type": "json", "from": "Hu, X.", "predicate": "authored", "to": "quantitative evaluation", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_261", "file_type": "json", "from": "quantitative evaluation", "predicate": "evaluates", "to": "confidence measures", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_261", "file_type": "json", "from": "confidence measures", "predicate": "is_aspect_of", "to": "stereo vision", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_261", "file_type": "json", "from": "dense matching", "predicate": "occurs_in", "to": "complex scenes", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_261", "file_type": "json", "from": "Manduchi, R.", "predicate": "proposed", "to": "distinctiveness maps", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_261", "file_type": "json", "from": "distinctiveness maps", "predicate": "is_technique_for", "to": "image matching", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_261", "file_type": "json", "from": "image matching", "predicate": "utilizes", "to": "distinctiveness maps", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_263", "file_type": "json", "from": "pose estimation", "predicate": "is_problem_of", "to": "determining viewpoint", "type": "conceptual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_430", "file_type": "json", "from": "pose estimation", "predicate": "is_problem_in", "to": "computer vision", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_263", "file_type": "json", "from": "viewpoint", "predicate": "explains", "to": "coarse pose", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_263", "file_type": "json", "from": "keypoint prediction", "predicate": "captures", "to": "finer details", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_263", "file_type": "json", "from": "constrained setting", "predicate": "has_input", "to": "bounding boxes", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_263", "file_type": "json", "from": "detection setting", "predicate": "is_more_challenging_than", "to": "constrained setting", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_263", "file_type": "json", "from": "viewpoint estimates", "predicate": "improves", "to": "keypoint predictions", "type": "causal", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_263", "file_type": "json", "from": "object characteristics", "predicate": "affects", "to": "performance", "type": "causal", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_263", "file_type": "json", "from": "Tulsiani_Viewpoints_and_Keypoints_2015_CVPR_paper", "predicate": "authored_by", "to": "Shubham Tulsiani", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_263", "file_type": "json", "from": "Tulsiani_Viewpoints_and_Keypoints_2015_CVPR_paper", "predicate": "authored_by", "to": "Jitendra Malik", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_488", "file_type": "json", "from": "Shubham Tulsiani", "predicate": "affiliated_with", "to": "University of California, Berkeley", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_488", "file_type": "json", "from": "Shubham Tulsiani", "predicate": "email", "to": "shubhtuls@eecs.berkeley.edu", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_489", "file_type": "json", "from": "Jitendra Malik", "predicate": "affiliates_with", "to": "University of California, Berkeley", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_488", "file_type": "json", "from": "Jitendra Malik", "predicate": "email", "to": "malik@eecs.berkeley.edu", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_488", "file_type": "json", "from": "Jitendra Malik", "predicate": "contributed_to", "to": "Category-Speci\ufb01c Object Reconstruction from a Single Image", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_264", "file_type": "json", "from": "effort", "predicate": "aims_to", "to": "goal", "type": "conceptual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_264", "file_type": "json", "from": "future efforts", "predicate": "guided_by", "to": "analysis", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_264", "file_type": "json", "from": "analysis", "predicate": "focuses_on", "to": "error modes", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_264", "file_type": "json", "from": "analysis", "predicate": "examines", "to": "effect", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_265", "file_type": "json", "from": "Pose Estimation", "predicate": "improves_via", "to": "Convolutional Neural Networks (CNNs)", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_265", "file_type": "json", "from": "Pose Estimation", "predicate": "requires", "to": "Keypoint Prediction", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_265", "file_type": "json", "from": "Pose Estimation", "predicate": "requires", "to": "Viewpoint Prediction", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_422", "file_type": "json", "from": "Pose Estimation", "predicate": "affected_by", "to": "Refraction", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_265", "file_type": "json", "from": "Abed Malti", "predicate": "author_of", "to": "Malti_A_Linear_Least-Squares_2015_CVPR_paper", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_269", "file_type": "json", "from": "Abed Malti", "predicate": "affiliated_with", "to": "Fluminance/INRIA", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_270", "file_type": "json", "from": "Abed Malti", "predicate": "affiliation", "to": "Fuminance/INRIA", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_265", "file_type": "json", "from": "Adrien Bartoli", "predicate": "author_of", "to": "Maiti_A_Linear_Least-Squares_2015_CVPR_paper", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_270", "file_type": "json", "from": "Adrien Bartoli", "predicate": "affiliation", "to": "ALCoV/ISIT", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_265", "file_type": "json", "from": "Richard Hartley", "predicate": "author_of", "to": "Malti_A_Linear_Least-Squares_2015_CVPR_paper", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_270", "file_type": "json", "from": "Richard Hartley", "predicate": "affiliation", "to": "Australian National University", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_270", "file_type": "json", "from": "Richard Hartley", "predicate": "affiliation", "to": "NICTA", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_266", "file_type": "json", "from": "Shape-from-Template methods", "predicate": "struggles_with", "to": "balancing accuracy, speed, and robustness", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_266", "file_type": "json", "from": "non-linear optimization", "predicate": "is_a", "to": "existing approach", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_266", "file_type": "json", "from": "Kalman filtering", "predicate": "is_a", "to": "existing approach", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_266", "file_type": "json", "from": "proposed solution", "predicate": "utilizes", "to": "mechanical constraints", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_266", "file_type": "json", "from": "proposed solution", "predicate": "employs", "to": "finite element methods", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_266", "file_type": "json", "from": "proposed solution", "predicate": "allows_for", "to": "accurate reconstruction", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_266", "file_type": "json", "from": "proposed solution", "predicate": "offers", "to": "efficient reconstruction", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_266", "file_type": "json", "from": "proposed solution", "predicate": "is_a", "to": "linear least-squares SfT method", "type": "conceptual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_266", "file_type": "json", "from": "finite element methods", "predicate": "represents", "to": "surface", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_266", "file_type": "json", "from": "finite element methods", "predicate": "represents", "to": "deformation", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_267", "file_type": "json", "from": "Shape-from-Template (SfT)", "predicate": "models", "to": "elastic deformations", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_267", "file_type": "json", "from": "Shape-from-Template (SfT)", "predicate": "uses", "to": "linear least-squares estimation", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_267", "file_type": "json", "from": "elastic deformations", "predicate": "requires", "to": "accurate reconstruction", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_267", "file_type": "json", "from": "elastic deformations", "predicate": "is_modeled_by", "to": "fully linear least-squares SfT method", "type": "factual", "width": 0.87}, {"arrows": "to", "chunk_id": "doc_0_chunk_267", "file_type": "json", "from": "Finite Element Methods (FEM)", "predicate": "codes", "to": "non-rigid EKF monocular SLAM", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_267", "file_type": "json", "from": "Finite Element Methods (FEM)", "predicate": "is_used_in", "to": "sequential bayesian non-rigid structure from motion", "type": "factual", "width": 0.89}, {"arrows": "to", "chunk_id": "doc_0_chunk_267", "file_type": "json", "from": "Agudo, A.", "predicate": "authored", "to": "FEM models to code non-rigid EKF monocular SLAM", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_267", "file_type": "json", "from": "Agudo, A.", "predicate": "authored", "to": "Finite element based sequential bayesian non-rigid structure from motion", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_267", "file_type": "json", "from": "Finite element based sequential bayesian non-rigid structure from motion", "predicate": "is_cited", "to": "frequently", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_267", "file_type": "json", "from": "FEM models", "predicate": "is_cited", "to": "frequently", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_268", "file_type": "json", "from": "Agudo et al.", "predicate": "cited_by", "to": "related work", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_268", "file_type": "json", "from": "Agudo et al.", "predicate": "addresses", "to": "structure from motion", "type": "conceptual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_268", "file_type": "json", "from": "Bartoli et al.", "predicate": "cited_by", "to": "methodology", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_268", "file_type": "json", "from": "Bartoli et al.", "predicate": "addresses", "to": "surface reconstruction", "type": "conceptual", "width": 0.78}, {"arrows": "to", "chunk_id": "doc_0_chunk_268", "file_type": "json", "from": "Salzmann and Urtasun", "predicate": "related_to", "to": "approach", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_268", "file_type": "json", "from": "Moreno-Noguer and Porta", "predicate": "related_to", "to": "approach", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_268", "file_type": "json", "from": "Moreno-Noguer and Porta", "predicate": "addresses", "to": "shape recovery", "type": "conceptual", "width": 0.72}, {"arrows": "to", "chunk_id": "doc_0_chunk_268", "file_type": "json", "from": "Finite Element Methods", "predicate": "described_in", "to": "Chaskalovic", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_268", "file_type": "json", "from": "Salzmann and Urutasun", "predicate": "addresses", "to": "3D reconstruction", "type": "conceptual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_269", "file_type": "json", "from": "ape recovery", "predicate": "related_to", "to": "CVPR", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_269", "file_type": "json", "from": "Chaskaloric", "predicate": "authored", "to": "Finite Elements Methods for Engineering Sciences", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_269", "file_type": "json", "from": "Finite Elements Methods for Engineering Sciences", "predicate": "provides", "to": "background knowledge", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_269", "file_type": "json", "from": "Reconstructing sharply folding surfaces", "predicate": "presented_at", "to": "CVPR", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_269", "file_type": "json", "from": "Salzmann, M., and Fua, P.", "predicate": "authored", "to": "Linear local models for monocular reconstruction", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_269", "file_type": "json", "from": "Linear local models for monocular reconstruction", "predicate": "published_in", "to": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_269", "file_type": "json", "from": "Matthews, I., and Baker, S.", "predicate": "authored", "to": "Active appearance models revisited", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_269", "file_type": "json", "from": "Active appearance models revisited", "predicate": "related_to", "to": "modeling techniques", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_270", "file_type": "json", "from": "appearance models", "predicate": "revisited in", "to": "International Journal of Computer Vision", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_382", "file_type": "json", "from": "appearance models", "predicate": "of", "to": "familiar objects", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_270", "file_type": "json", "from": "Riemannian Coding", "predicate": "uses", "to": "Kernels", "type": "conceptual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_270", "file_type": "json", "from": "Mehrtash Harandi", "predicate": "author of", "to": "Riemannian Coding and Dictionary Learning", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_271", "file_type": "json", "from": "covariance descriptors", "predicate": "lies_on", "to": "Riemannian manifolds", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_359", "file_type": "json", "from": "Riemannian manifolds", "predicate": "relevant to", "to": "geodesic Laplacian kernels", "type": "conceptual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_271", "file_type": "json", "from": "normalized histograms", "predicate": "lies_on", "to": "Riemannian manifolds", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_271", "file_type": "json", "from": "linear subspaces", "predicate": "lies_on", "to": "Riemannianmanifolds", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_271", "file_type": "json", "from": "2D shape outlines", "predicate": "lies_on", "to": "Riemannian manifolds", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_271", "file_type": "json", "from": "existing solutions", "predicate": "are", "to": "dedicated to specific manifolds", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_271", "file_type": "json", "from": "existing solutions", "predicate": "rely_on", "to": "optimization problems", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_271", "file_type": "json", "from": "optimization problems", "predicate": "are", "to": "difficult to solve", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_271", "file_type": "json", "from": "general Riemannian coding framework", "predicate": "has_counterpart", "to": "kernel-based counterpart", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_271", "file_type": "json", "from": "kernel-based counterpart", "predicate": "allows_for", "to": "generalization beyond sparse coding", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_272", "file_type": "json", "from": "kernel-based counterpart", "predicate": "related_to", "to": "kernel parameters", "type": "conceptual", "width": 0.84}, {"arrows": "to", "chunk_id": "doc_0_chunk_272", "file_type": "json", "from": "Riemannian coding framework", "predicate": "has_counterpart", "to": "kernel-based counterpart", "type": "conceptual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_272", "file_type": "json", "from": "Riemannian coding framework", "predicate": "allows", "to": "generalization beyond sparse coding", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_272", "file_type": "json", "from": "Riemannian coding framework", "predicate": "enables", "to": "learning of kernel parameters", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_272", "file_type": "json", "from": "Riemannian coding framework", "predicate": "simplifies", "to": "dictionary learning", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_272", "file_type": "json", "from": "sparse coding", "predicate": "limited_by", "to": "flat data", "type": "conceptual", "width": 0.82}, {"arrows": "to", "chunk_id": "doc_0_chunk_272", "file_type": "json", "from": "coding schemes", "predicate": "require", "to": "efficient solutions", "type": "conceptual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_273", "file_type": "json", "from": "Riemannian Manifolds", "predicate": "is_topic_of", "to": "cvpr_papers", "type": "conceptual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_273", "file_type": "json", "from": "Dictionary Learning", "predicate": "is_topic_of", "to": "cvpr_papers", "type": "conceptual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_273", "file_type": "json", "from": "Coding Theory", "predicate": "is_topic_of", "to": "cvpr_papers", "type": "conceptual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_273", "file_type": "json", "from": "Mehrtas Harandi", "predicate": "affiliated_with", "to": "Australian National University", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_273", "file_type": "json", "from": "Mehrtas Harandi", "predicate": "affiliated_with", "to": "NICITA", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_273", "file_type": "json", "from": "Yuting Zhang", "predicate": "author_of", "to": "Improving Object Detection", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_278", "file_type": "json", "from": "Yuting Zhang", "predicate": "affiliated_with", "to": "Department of Computer Science, Zhejiang University", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_273", "file_type": "json", "from": "Improving Object Detection", "predicate": "presented_at", "to": "CVPR", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_273", "file_type": "json", "from": "Kihyuk Sohn", "predicate": "author_of", "to": "Improving ObjectDetection", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_278", "file_type": "json", "from": "Kihyuk Sohn", "predicate": "affiliated_with", "to": "Department of Electrical Engineering and Computer Science, University of Michigan", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_273", "file_type": "json", "from": "Ruben Villegas", "predicate": "author_of", "to": "Improving Object Detection", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_278", "file_type": "json", "from": "Ruben Villegas", "predicate": "affiliated_with", "to": "Department of Electrical Engineering and Computer Science, University of Michigan", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_273", "file_type": "json", "from": "Gang Pan", "predicate": "author_of", "to": "Improving Object Detection", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_278", "file_type": "json", "from": "Gang Pan", "predicate": "affiliated_with", "to": "Department of Computer Science, Zhejiang University", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_273", "file_type": "json", "from": "Honglak Lee", "predicate": "author_of", "to": "Improving Object Detection", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_278", "file_type": "json", "from": "Honglak Lee", "predicate": "affiliated_with", "to": "Department of Electrical Engineering and Computer Science, University of Michigan", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_274", "file_type": "json", "from": "inaccurate localization", "predicate": "is", "to": "major source of error", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_274", "file_type": "json", "from": "search algorithm", "predicate": "based_on", "to": "Bayesian optimization", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_274", "file_type": "json", "from": "search algorithm", "predicate": "proposes", "to": "candidate regions", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_274", "file_type": "json", "from": "structured loss", "predicate": "penalizes", "to": "localization inaccuracy", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_274", "file_type": "json", "from": "methods", "predicate": "improves", "to": "detection performance", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_274", "file_type": "json", "from": "methods", "predicate": "improves_over", "to": "baseline method", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_314", "file_type": "json", "from": "methods", "predicate": "impose", "to": "prior over human poses", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_320", "file_type": "json", "from": "methods", "predicate": "extend to", "to": "object search", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_637", "file_type": "json", "from": "methods", "predicate": "are", "to": "state-of-the-art", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_275", "file_type": "json", "from": "loss", "predicate": "penalizes", "to": "localization inaccuracy", "type": "conceptual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_275", "file_type": "json", "from": "proposed methods", "predicate": "improves", "to": "detection performance", "type": "causal", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_275", "file_type": "json", "from": "proposed methods", "predicate": "performs_better_than", "to": "baseline method", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_398", "file_type": "json", "from": "proposed methods", "predicate": "validates", "to": "effectiveness", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_275", "file_type": "json", "from": "two methods", "predicate": "are", "to": "complementary", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_275", "file_type": "json", "from": "combined methods", "predicate": "outperforms", "to": "state-of-the-art", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_276", "file_type": "json", "from": "Bayesian Optimization", "predicate": "used_in", "to": "Object Detection", "type": "factual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_276", "file_type": "json", "from": "Structured Prediction (Structured SVM)", "predicate": "used_in", "to": "Object Detection", "type": "factual", "width": 0.82}, {"arrows": "to", "chunk_id": "doc_0_chunk_276", "file_type": "json", "from": "Localization Accuracy", "predicate": "metric_for", "to": "Object Detection", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_276", "file_type": "json", "from": "Deep Networks", "predicate": "trained_using", "to": "Greedy Layer-Wise Training", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_481", "file_type": "json", "from": "Deep Networks", "predicate": "performs", "to": "Local Estimation", "type": "conceptual", "width": 0.82}, {"arrows": "to", "chunk_id": "doc_0_chunk_481", "file_type": "json", "from": "Deep Networks", "predicate": "performs", "to": "Global Search", "type": "conceptual", "width": 0.82}, {"arrows": "to", "chunk_id": "doc_0_chunk_398", "file_type": "json", "from": "Local Binary Patterns", "predicate": "applied_to", "to": "Face Recognition", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_277", "file_type": "json", "from": "Representation Learning", "predicate": "is_publication_of", "to": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_277", "file_type": "json", "from": "Bengio, Y.", "predicate": "authors", "to": "Representation Learning", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_399", "file_type": "json", "from": "Bengio, Y.", "predicate": "authored", "to": "Learning deep architectures for AI", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_277", "file_type": "json", "from": "Girschick, R.", "predicate": "authors", "to": "Rich Feature Hierarchies", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_570", "file_type": "json", "from": "Rich Feature Hierarchies", "predicate": "improves", "to": "Object Detection", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_570", "file_type": "json", "from": "Rich Feature Hierarchies", "predicate": "improves", "to": "Semantic Segmentation", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_277", "file_type": "json", "from": "Everingham, M.", "predicate": "organizes", "to": "VOC2007", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_660", "file_type": "json", "from": "Everingham, M.", "predicate": "authored", "to": "PASCAL VOC challenge description", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_323", "file_type": "json", "from": "VOC2007", "predicate": "is_a", "to": "Visual Object Classes Challenge", "type": "conceptual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_324", "file_type": "json", "from": "VOC2007", "predicate": "is_challenge_of", "to": "Pascal Network", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_277", "file_type": "json", "from": "Deng, J.", "predicate": "authors", "to": "ImageNet", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_277", "file_type": "json", "from": "Donahue, J.", "predicate": "authors", "to": "DeCAF", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_277", "file_type": "json", "from": "DeCAF", "predicate": "is_for", "to": "Visual Recognition", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_278", "file_type": "json", "from": "DeCAF", "predicate": "published_in", "to": "CoRR", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_278", "file_type": "json", "from": "DeCAF", "predicate": "author", "to": "Donahue, J.", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_278", "file_type": "json", "from": "Erhan", "predicate": "author", "to": "Erhan, D.", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_278", "file_type": "json", "from": "Erhan", "predicate": "presented_at", "to": "CVPR", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_278", "file_type": "json", "from": "Dongping Li", "predicate": "author", "to": "A Geodesic-Prepreserving Method for Image Warping", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_279", "file_type": "json", "from": "Dongping Li", "predicate": "is_author_of", "to": "A Geodesic-Preserving Method for Image Warping", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_283", "file_type": "json", "from": "Dongping Li", "predicate": "affiliated_with", "to": "Zhejiang University", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_279", "file_type": "json", "from": "Department of Electrical Engineering and Computer Science", "predicate": "is_affiliation_of", "to": "University of Michigan", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_279", "file_type": "json", "from": "A Geodesic-Preserving Method for Image Warping", "predicate": "is_publication_of", "to": "CVPR", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_279", "file_type": "json", "from": "A Geodesic-Preserving Method for Image Warping", "predicate": "addresses", "to": "Image Warping", "type": "conceptual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_279", "file_type": "json", "from": "Kun Zhou", "predicate": "is_author_of", "to": "A Geodesic-Preserving Method for Image Warping", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_283", "file_type": "json", "from": "Kun Zhou", "predicate": "affiliated_with", "to": "Zhejiang University", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_279", "file_type": "json", "from": "honglak@umich.edu", "predicate": "is_email_of", "to": "University of Michigan", "type": "factual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_279", "file_type": "json", "from": "Li_A_Geodesic-Preserving_Method_2015_CVPR_supplemental.pdf", "predicate": "is_related_to", "to": "A Geodesic-Preserving Method for Image Warping", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_281", "file_type": "json", "from": "Image Warping", "predicate": "achieves", "to": "high-quality warped images", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_280", "file_type": "json", "from": "geodesic-preserving method", "predicate": "aims to", "to": "maintain shape", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_280", "file_type": "json", "from": "geodesic-preserving method", "predicate": "minimizes", "to": "distortions", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_280", "file_type": "json", "from": "core of method", "predicate": "lies in", "to": "preserving geodesic distances", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_280", "file_type": "json", "from": "preserving geodesic distances", "predicate": "ensures", "to": "local smoothness", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_280", "file_type": "json", "from": "preserving geodesic distances", "predicate": "prevents", "to": "unwanted artifacts", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_280", "file_type": "json", "from": "energy function", "predicate": "incorporates", "to": "shape preservation terms", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_280", "file_type": "json", "from": "energy function", "predicate": "incorporates", "to": "boundary preservation terms", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_280", "file_type": "json", "from": "energy function", "predicate": "incorporates", "to": "geodesic preservation terms", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_282", "file_type": "json", "from": "Gauss-Newton method", "predicate": "is_a", "to": "optimization technique", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_281", "file_type": "json", "from": "Shape and Boundary Preservation", "predicate": "is_method_of", "to": "Image Warping", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_281", "file_type": "json", "from": "Energy Minimization", "predicate": "is_technique_for", "to": "Image Warping", "type": "conceptual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_281", "file_type": "json", "from": "Gausss-Newton Method", "predicate": "is_method_for", "to": "Energy Minimization", "type": "conceptual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_281", "file_type": "json", "from": "Zhang, G. et al.", "predicate": "authored", "to": "A shape-preserving approach to image resizing", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_281", "file_type": "json", "from": "A shape-preserivng approach to image resizing", "predicate": "addresses", "to": "Shape and Boundary Preservation", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_281", "file_type": "json", "from": "Rotation matrix R\u03b8,\u03c6", "predicate": "is_component_of", "to": "Eqn. (1)", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_281", "file_type": "json", "from": "Shape-preserving term ES(V)", "predicate": "is_component_of", "to": "Eqn. (7)", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_282", "file_type": "json", "from": "shape-preserving approach", "predicate": "uses", "to": "Eqn. (7) - Shape-preserving term ES(V)", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_282", "file_type": "json", "from": "Eqn. (7) - Shape-preserving term ES(V)", "predicate": "is_part_of", "to": "shape-preserving approach", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_282", "file_type": "json", "from": "local smoothness preservation", "predicate": "described_by", "to": "Eqn. (4) - Local smoothness preservation EC(V)", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_282", "file_type": "json", "from": "Eqn. (4) - Local smoothness preservation EC(V)", "predicate": "contributes_to", "to": "local smoothness preservation", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_282", "file_type": "json", "from": "Eqn. (5) - Combined energy function E(V)", "predicate": "represents", "to": "overall energy function", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_283", "file_type": "json", "from": "Q", "predicate": "relies_on", "to": "orthogonal matrices", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_282", "file_type": "json", "from": "orthogonal matrices", "predicate": "is_a", "to": "mathematical concept", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_283", "file_type": "json", "from": "orthogonal matrices", "predicate": "is_concept_in", "to": "mathematics", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_282", "file_type": "json", "from": "E(V)", "predicate": "combines", "to": "various terms", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_283", "file_type": "json", "from": "Antonio Agudo", "predicate": "affiliated_with", "to": "Microsoft Research", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_284", "file_type": "json", "from": "Antonio Agudo", "predicate": "is_author_of", "to": "Simultaneous Pose and Non-Rigid Shape with Particle Dynamics", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_287", "file_type": "json", "from": "Antonio Agudo", "predicate": "affiliated_with", "to": "Instituto de Invesigaci\u00b4on en Ingenier\u00b4\u0131a de Arag\u00b4on (I3A)", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_283", "file_type": "json", "from": "Francesc Moreno-Noguer", "predicate": "affiliated_with", "to": "Microsoft Research", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_284", "file_type": "json", "from": "Francesc Moreno-Noguer", "predicate": "is_author_of", "to": "Simultaneous Pose and Non-Rigid Shape with Particle Dynamics", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_287", "file_type": "json", "from": "Francesc Moreno-Noguer", "predicate": "affiliated_with", "to": "Institut de Rob`otica i Inform`atica Industrial (CSI-UPC)", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_283", "file_type": "json", "from": "Simultaneous Pose and Non-Rigid Shape", "predicate": "uses", "to": "particle dynamics", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_284", "file_type": "json", "from": "particle dynamics", "predicate": "used_in", "to": "Simultaneous Pose and Non-Rigid Shape", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_283", "file_type": "json", "from": "geodesic preservation", "predicate": "is_concept_in", "to": "geometry", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_284", "file_type": "json", "from": "Simultaneous Pose and Non-Rigid Shape with Particle Dynamics", "predicate": "presented_at", "to": "CVPR", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_284", "file_type": "json", "from": "Simultaneous Pose and Non-Rigid Shape with Particle Dynamics", "predicate": "deals_with", "to": "particle dynamics", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_284", "file_type": "json", "from": "Simultaneous Pose and Non-Rigid Shape with Particle Dynamics", "predicate": "addresses", "to": "non-rigid shape", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_284", "file_type": "json", "from": "Simultaneous Pose and Non-Rigid Shape with Particle Dynamics", "predicate": "concerns", "to": "simultaneous pose", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_284", "file_type": "json", "from": "Simultaneous Pose and Non-Rigid Shape with Particle Dynamics", "predicate": "is_a", "to": "CVPR paper", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_284", "file_type": "json", "from": "non-rigid shape", "predicate": "requires", "to": "particle dynamics", "type": "conceptual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_285", "file_type": "json", "from": "solution", "predicate": "estimates", "to": "camera pose", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_427", "file_type": "json", "from": "solution", "predicate": "solves", "to": "absolute pose problem", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_428", "file_type": "json", "from": "solution", "predicate": "computes", "to": "camera position", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_428", "file_type": "json", "from": "solution", "predicate": "computes", "to": "camera orientation", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_428", "file_type": "json", "from": "solution", "predicate": "computes", "to": "translational velocity", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_428", "file_type": "json", "from": "solution", "predicate": "computes", "to": "angular velocity", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_428", "file_type": "json", "from": "solution", "predicate": "improves", "to": "number of inliers", "type": "factual", "width": 0.87}, {"arrows": "to", "chunk_id": "doc_0_chunk_428", "file_type": "json", "from": "solution", "predicate": "uses", "to": "RANSA", "type": "factual", "width": 0.86}, {"arrows": "to", "chunk_id": "doc_0_chunk_716", "file_type": "json", "from": "solution", "predicate": "uses", "to": "spatiotemporal filters", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_716", "file_type": "json", "from": "solution", "predicate": "uses", "to": "metric-learning framework", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_285", "file_type": "json", "from": "object", "predicate": "modeled as", "to": "ensemble of particles", "type": "conceptual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_375", "file_type": "json", "from": "object", "predicate": "has_relationship", "to": "annotation proposals", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_381", "file_type": "json", "from": "object", "predicate": "is_unseen", "to": "object", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_285", "file_type": "json", "from": "particle", "predicate": "ruled by", "to": "Newton\u2019s second law of motion", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_285", "file_type": "json", "from": "dynamic model", "predicate": "incorporated into", "to": "bundle adjustment framework", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_286", "file_type": "json", "from": "validation", "predicate": "occurs_in", "to": "real video sequences", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_286", "file_type": "json", "from": "motion", "predicate": "is", "to": "articulated", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_286", "file_type": "json", "from": "motion", "predicate": "is", "to": "non-rigid", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_286", "file_type": "json", "from": "shapes", "predicate": "are", "to": "continuous", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_286", "file_type": "json", "from": "shapes", "predicate": "are", "to": "discontinuous", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_286", "file_type": "json", "from": "batch methods", "predicate": "are", "to": "computationally expensive", "type": "factual", "width": 0.91}, {"arrows": "to", "chunk_id": "doc_0_chunk_287", "file_type": "json", "from": "System", "predicate": "performs_comparable_to", "to": "Competing Batch Methods", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_341", "file_type": "json", "from": "System", "predicate": "avoids", "to": "Uncanny Valley Effect", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_341", "file_type": "json", "from": "System", "predicate": "evaluated_with", "to": "Qualitative Evaluations", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_287", "file_type": "json", "from": "Non-Rigid Structure from Motion", "predicate": "is_topic_of", "to": "Paper", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_287", "file_type": "json", "from": "Particle Dynamics", "predicate": "is_related_to", "to": "Non-Rigid Structure from Motion", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_287", "file_type": "json", "from": "Bundle Adjustment", "predicate": "is_related_to", "to": "Non-Rigid Structure from Motion", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_287", "file_type": "json", "from": "Monocular Video Analysis", "predicate": "is_related_to", "to": "Non-Rigid Structure from Motion", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_287", "file_type": "json", "from": "Instituto de Invesigaci\u00b4on en Ingenier\u00b4\u0131a de Arag\u00b4on (I3a)", "predicate": "located_in", "to": "Universidad de Zaragoza", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_287", "file_type": "json", "from": "Simone Frintrop", "predicate": "is_author_of", "to": "Paper", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_291", "file_type": "json", "from": "Simone Frintrop", "predicate": "affiliated_with", "to": "Rheinische Friedrich-Wilhelms-Universit\u00a8at Bonn", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_292", "file_type": "json", "from": "Simone Frintrop", "predicate": "affiliated_with", "to": "Rheinische Friedrich-Wilhelms-Universit\u00e4t Bonn", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_292", "file_type": "json", "from": "Simone Frintrop", "predicate": "email", "to": "frintrop@iai.uni-bonn.de", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_287", "file_type": "json", "from": "Thomas Werner", "predicate": "is_author_of", "to": "Paper", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_291", "file_type": "json", "from": "Thomas Werner", "predicate": "affiliated_with", "to": "Rheinische Friedrich-Wilhelms-Universit\u00a8at Bonn", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_292", "file_type": "json", "from": "Thomas Werner", "predicate": "affiliated_with", "to": "Rheinische Friedrich-Wilhelms-Universit\u00e4t Bonn", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_287", "file_type": "json", "from": "Germ\u00e1n M. Garc\u00eda", "predicate": "is_author_of", "to": "Paper", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_292", "file_type": "json", "from": "Germ\u00e1n M. Garc\u00eda", "predicate": "affiliated_with", "to": "Rheinische Friedrich-Wilhelms-Universit\u00e4t Bonn", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_288", "file_type": "json", "from": "salience model", "predicate": "proposed by", "to": "Itti et al.", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_288", "file_type": "json", "from": "adaptations", "predicate": "concerns", "to": "scale-space structure", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_288", "file_type": "json", "from": "scale-space structure", "predicate": "introduces", "to": "twin pyramid", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_290", "file_type": "json", "from": "foundational approaches", "predicate": "requires", "to": "adaptation", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_288", "file_type": "json", "from": "VOCUS2", "predicate": "is a", "to": "system", "type": "conceptual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_289", "file_type": "json", "from": "ration framework", "predicate": "produces", "to": "segment-based salience maps", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_289", "file_type": "json", "from": "ration framework", "predicate": "aims_for", "to": "high performance", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_289", "file_type": "json", "from": "segment-based salience maps", "predicate": "achieves", "to": "state-of-the-art performance", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_289", "file_type": "json", "from": "segment-based salience maps", "predicate": "used_in", "to": "modern applications", "type": "factual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_289", "file_type": "json", "from": "importance", "predicate": "related_to", "to": "revisiting foundational approaches", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_289", "file_type": "json", "from": "adaptation", "predicate": "for", "to": "modern applications", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_290", "file_type": "json", "from": "t-based salience maps", "predicate": "achieves", "to": "state-of-the-art performance", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_290", "file_type": "json", "from": "Saliency Models", "predicate": "includes", "to": "Itti Model", "type": "conceptual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_290", "file_type": "json", "from": "Itti Model", "predicate": "is_related_to", "to": "Computer Vision", "type": "conceptual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_290", "file_type": "json", "from": "A cognitive approach for object discovery", "predicate": "presented_in", "to": "ICPR", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_290", "file_type": "json", "from": "Discriminant salieny", "predicate": "presented_in", "to": "TPAMI", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_290", "file_type": "json", "from": "State-of-the-art in visual attention modeling", "predicate": "authored_by", "to": "A. Borji", "type": "factual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_290", "file_type": "json", "from": "Scale-space representation", "predicate": "is_used_in", "to": "Computer Vision", "type": "conceptual", "width": 0.7}, {"arrows": "to", "chunk_id": "doc_0_chunk_291", "file_type": "json", "from": "asri", "predicate": "authored", "to": "Image segmentation", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_291", "file_type": "json", "from": "L. Itti", "predicate": "developed", "to": "salienicy-based visual attention model", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_291", "file_type": "json", "from": "N. D. B. Bruce", "predicate": "proposed", "to": "information theoretic approach", "type": "conceptual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_291", "file_type": "json", "from": "L. Hurvich", "predicate": "proposed", "to": "opponent-process theory", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_292", "file_type": "json", "from": "Rheinische Friedrich-Wilhelms-Universit\u00e4t Bonn", "predicate": "location", "to": "Bonn", "type": "factual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_292", "file_type": "json", "from": "Jiaolong Yang", "predicate": "author_of", "to": "Dense, Accurate Optical Flow Estimation", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_298", "file_type": "json", "from": "Jiaolong Yang", "predicate": "affiliated_with", "to": "Beijing Lab of Intelligent Information Technology", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_298", "file_type": "json", "from": "Jiaolong Yang", "predicate": "affiliated_with", "to": "Beijing Institute of Technology", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_292", "file_type": "json", "from": "Dense, Accurate Optical Flow Estimation", "predicate": "publication_venue", "to": "CVPR", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_292", "file_type": "json", "from": "Dense, Accurate Optical Flow Estimation", "predicate": "year", "to": "2015", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_292", "file_type": "json", "from": "Yang_Dense_Accurate_Optical_2015_CVPR_paper", "predicate": "is_a", "to": "PDF document", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_293", "file_type": "json", "from": "multi-model fitting scheme", "predicate": "achieved_by", "to": "energy minimization", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_295", "file_type": "json", "from": "Optical flow benchmarks", "predicate": "include", "to": "KITTI", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_295", "file_type": "json", "from": "Optical flow benchmarks", "predicate": "include", "to": "MPI Sintel", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_606", "file_type": "json", "from": "KITTI", "predicate": "is_benchmark_for", "to": "Optical Flow", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_606", "file_type": "json", "from": "KITTI", "predicate": "is_benchmark_for", "to": "Scene Flow", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_295", "file_type": "json", "from": "Optical flow estimation", "predicate": "uses", "to": "Piecewise parametric models", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_295", "file_type": "json", "from": "Optical flow estimation", "predicate": "uses", "to": "Energy minimization", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_295", "file_type": "json", "from": "Homography transformation", "predicate": "used_in", "to": "Optical flow estimation", "type": "conceptual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_295", "file_type": "json", "from": "Baker et al. (2011)", "predicate": "published", "to": "database and evaluation methodology", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_295", "file_type": "json", "from": "Bao et al. (2014)", "predicate": "published", "to": "Fast edge-preserving patchmatch", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_295", "file_type": "json", "from": "Barnes et al. (2009)", "predicate": "published", "to": "PatchMatch", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_581", "file_type": "json", "from": "PatchMatch", "predicate": "addresses", "to": "structural image editing", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_296", "file_type": "json", "from": "PatchMatch", "predicate": "presented_in", "to": "ACM Transactions on Graphics (TOG)", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_296", "file_type": "json", "from": "Piecewise image registration", "predicate": "presented_in", "to": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_296", "file_type": "json", "from": "Multiway cut", "predicate": "used_for", "to": "stereo and motion", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_296", "file_type": "json", "from": "Multiway cut", "predicate": "handles", "to": "slanted surfaces", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_296", "file_type": "json", "from": "robust estimation", "predicate": "addresses", "to": "multiple motions", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_296", "file_type": "json", "from": "robust estimation", "predicate": "produces", "to": "parametric flow fields", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_296", "file_type": "json", "from": "robust estimation", "predicate": "produces", "to": "piecewise-smooth flow fields", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_296", "file_type": "json", "from": "Black", "predicate": "coauthored_with", "to": "Anandan", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_297", "file_type": "json", "from": "Black, M. J.", "predicate": "co-authored", "to": "The robust estimation of multiple motions", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_297", "file_type": "json", "from": "Black, M. J.", "predicate": "co-authored", "to": "Estimating optical flow in segmented images", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_297", "file_type": "json", "from": "Anandan, P.", "predicate": "co-authored", "to": "The robust estimation of multiple motions", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_297", "file_type": "json", "from": "Jepson, A. D.", "predicate": "co-authored", "to": "Estimating optical flow in segmented images", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_297", "file_type": "json", "from": "Bleyer, M.", "predicate": "co-authored", "to": "PatchMatch Stereo", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_297", "file_type": "json", "from": "Rhemann, C.", "predicate": "co-authored", "to": "PatchMatch Stereo", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_297", "file_type": "json", "from": "Rother, C.", "predicate": "co-authored", "to": "PatchMatch Stere", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_298", "file_type": "json", "from": "Braux-Zin et al.", "predicate": "authored", "to": "A general dense image matching framework", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_677", "file_type": "json", "from": "Beijing Lab of Intelligent Information Technology", "predicate": "is part of", "to": "School of Computer Science", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_298", "file_type": "json", "from": "Thomas Mauthner", "predicate": "authored", "to": "Encoding Based Saliency Detection for Videos and Images", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_298", "file_type": "json", "from": "Horst Possegger", "predicate": "authored", "to": "Encoding Based Saliency Detection for Videos and Images", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_299", "file_type": "json", "from": "Horst Possegger", "predicate": "contributed_to", "to": "Mauthner_Encoding_Based_Saliency_2015_CVPR_paper", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_298", "file_type": "json", "from": "Georg Waltner", "predicate": "authored", "to": "Encoding Based Saliency Detection for Videos and Images", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_299", "file_type": "json", "from": "Georg Waltner", "predicate": "contributed_to", "to": "Mauthner_Encoding_Based_Salieney_2015_CVPR_paper", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_303", "file_type": "json", "from": "Georg Waltner", "predicate": "is_affiliated_with", "to": "Institute for Computer Graphics and Vision", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_303", "file_type": "json", "from": "Georg Waltner", "predicate": "has_email", "to": "waltner@icg.tugraz.at", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_298", "file_type": "json", "from": "Horst Bischof", "predicate": "authored", "to": "Encoding Based Saliency Detection for Videos and Images", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_299", "file_type": "json", "from": "Horst Bischof", "predicate": "contributed_to", "to": "Mauthne_Encoding_Based_Salieney_2015_CVPR_paper", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_303", "file_type": "json", "from": "Horst Bischof", "predicate": "has_email", "to": "bischof@icg.tugraz.at", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_298", "file_type": "json", "from": "Mauthner_Encoding_Based_Saliency_2015_CVPR_paper", "predicate": "is_a", "to": "CVPR paper", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_299", "file_type": "json", "from": "eye-traking data", "predicate": "used_in", "to": "predicting human gaze", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_299", "file_type": "json", "from": "reliance on human gaze", "predicate": "introduces", "to": "bias", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_299", "file_type": "json", "from": "encoding method", "predicate": "approximates", "to": "joint feature distributions", "type": "factual", "width": 0.87}, {"arrows": "to", "chunk_id": "doc_0_chunk_299", "file_type": "json", "from": "salience computation", "predicate": "is", "to": "efficient", "type": "factual", "width": 0.86}, {"arrows": "to", "chunk_id": "doc_0_chunk_299", "file_type": "json", "from": "salience computation", "predicate": "enforces", "to": "Gestalt principle of figure-ground segregation", "type": "conceptual", "width": 0.84}, {"arrows": "to", "chunk_id": "doc_0_chunk_300", "file_type": "json", "from": "re-ground segregation", "predicate": "is", "to": "method", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_301", "file_type": "json", "from": "Video Salience Detection", "predicate": "uses", "to": "Gestalt Principles", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_301", "file_type": "json", "from": "Video Salience Detection", "predicate": "employs", "to": "Encoding Methods", "type": "factual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_301", "file_type": "json", "from": "Video Salience Detection", "predicate": "related_to", "to": "Human Activity Recognition", "type": "conceptual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_301", "file_type": "json", "from": "Itti, L.", "predicate": "authored", "to": "model of salience-based visual attention", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_301", "file_type": "json", "from": "Alexe, B.", "predicate": "authored", "to": "What is an object?", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_301", "file_type": "json", "from": "Liu, T.", "predicate": "authored", "to": "Learning to Detect A Salient Object", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_301", "file_type": "json", "from": "Johansson, G.", "predicate": "authored", "to": "model for analysis of biological motion", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_301", "file_type": "json", "from": "Gorelick, L.", "predicate": "authored", "to": "Actions as Space-Time Shapes", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_302", "file_type": "json", "from": "Gorelick", "predicate": "authored", "to": "Actions as Space-Time Shapes", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_302", "file_type": "json", "from": "Blank", "predicate": "authored", "to": "Actions as Space-Time Shapes", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_302", "file_type": "json", "from": "Shechtman", "predicate": "authored", "to": "Actions as Space-Time Shapes", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_302", "file_type": "json", "from": "Irani", "predicate": "authored", "to": "Actions as Space-Time Shapes", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_302", "file_type": "json", "from": "Basri", "predicate": "authored", "to": "Actions as Space-Time Shapes", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_302", "file_type": "json", "from": "Borji", "predicate": "authored", "to": "Salient Object Detection: A Benchmark", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_302", "file_type": "json", "from": "Sihte", "predicate": "authored", "to": "Salient Object Determination: A Benchmark", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_302", "file_type": "json", "from": "Itti", "predicate": "authored", "to": "Salient Object Detection: A Benchmark", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_302", "file_type": "json", "from": "Guo", "predicate": "authored", "to": "Spatio-temporal Saliency detection", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_302", "file_type": "json", "from": "Zhang", "predicate": "authored", "to": "Spatio-temporal Saliency detection", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_302", "file_type": "json", "from": "Judd", "predicate": "authored", "to": "Learning to Predict Where Humans Look", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_302", "file_type": "json", "from": "Ehinger", "predicate": "authored", "to": "Learning to Predict Where Humans Look", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_302", "file_type": "json", "from": "Durand", "predicate": "authored", "to": "Learning to Predict Where Humans Look", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_302", "file_type": "json", "from": "Harel", "predicate": "authored", "to": "Graph-based Visual Saliency", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_302", "file_type": "json", "from": "Koch", "predicate": "authored", "to": "Graph-based Visual Saliency", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_302", "file_type": "json", "from": "Perona", "predicate": "authored", "to": "Graph-based Visual Saliency", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_621", "file_type": "json", "from": "Perona", "predicate": "co-developed", "to": "Caltech-256 object category dataset", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_302", "file_type": "json", "from": "Rahtu", "predicate": "authored", "to": "Segmenting Salient Objects from Images and Videos", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_302", "file_type": "json", "from": "Kannala", "predicate": "authored", "to": "Segmenting Salient Objects from Images and Videos", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_302", "file_type": "json", "from": "Salo", "predicate": "authored", "to": "Segmenting Salient Objects from Images and Videos", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_302", "file_type": "json", "from": "Heikkil\u00a8a", "predicate": "authored", "to": "Segmenting Salient Objects from Images and Videos", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_302", "file_type": "json", "from": "Mauthner", "predicate": "affiliated_with", "to": "Institute for Computer Graphics and Vision, Graz University of Technology", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_302", "file_type": "json", "from": "Possegger", "predicate": "affiliated_with", "to": "Institute for Computer Graphics and Vision, Graz University of Technology", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_302", "file_type": "json", "from": "Waltner", "predicate": "affiliated_with", "to": "Institute for Computer Graphics and Vision, Graz University of Technology", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_303", "file_type": "json", "from": "Institute for Computer Graphics and Vision", "predicate": "is_part_of", "to": "Graz University of Technology", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_303", "file_type": "json", "from": "Institute for Computer Graphics and Vision", "predicate": "has_email", "to": "possegger@icg.tugraz.at", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_303", "file_type": "json", "from": "Guanbin Li", "predicate": "is_author_of", "to": "Visual Saliency Based on Multiscale Deep Features", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_303", "file_type": "json", "from": "Visual Saliency Based on Multiscale Deep Features", "predicate": "is_publication_of", "to": "CVPR", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_303", "file_type": "json", "from": "Yizhou Yu", "predicate": "is_author_of", "to": "Visual Saliency Based on Multiscale Deep Features", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_304", "file_type": "json", "from": "Deep Features", "predicate": "extracted at", "to": "Multiple Scales", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_304", "file_type": "json", "from": "MDF approach", "predicate": "generates", "to": "Salience maps", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_305", "file_type": "json", "from": "MDF approach", "predicate": "generates", "to": "accurate salience maps", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_305", "file_type": "json", "from": "Spectral residual approach", "predicate": "is_method_for", "to": "Salience detection", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_305", "file_type": "json", "from": "Salience detection", "predicate": "is_field_within", "to": "Image Processing", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_305", "file_type": "json", "from": "Salient object detection", "predicate": "employs", "to": "discriminative regional feature integration", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_306", "file_type": "json", "from": "Salient object detection", "predicate": "is_field_of", "to": "computer vision", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_687", "file_type": "json", "from": "Salient object detection", "predicate": "is_benchmark", "to": "ECCV", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_306", "file_type": "json", "from": "Yuan et al. (2013)", "predicate": "is_foundational_work_in", "to": "salient object detection", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_306", "file_type": "json", "from": "Perazzi et al. (2012)", "predicate": "is_foundational_work_in", "to": "salient region detection", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_487", "file_type": "json", "from": "salient region detection", "predicate": "is_task_in", "to": "computer vision", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_306", "file_type": "json", "from": "Wei et al. (2012)", "predicate": "is_foundational_work_in", "to": "salient region detection", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_306", "file_type": "json", "from": "Yan et al. (2013)", "predicate": "is_foundational_work_in", "to": "salient object detection", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_307", "file_type": "json", "from": "Yang et al. (2013)", "predicate": "is_foundational_work", "to": "salient object detection", "type": "conceptual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_307", "file_type": "json", "from": "Yang et al. (2013)", "predicate": "presented_in", "to": "CVPR", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_306", "file_type": "json", "from": "Salient region detection", "predicate": "is_field_of", "to": "computer vision", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_306", "file_type": "json", "from": "Salience filters", "predicate": "method_for", "to": "salient region detection", "type": "factual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_307", "file_type": "json", "from": "Zhu et al. (2014)", "predicate": "is_foundational_work", "to": "salient object detection", "type": "conceptual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_307", "file_type": "json", "from": "Sun et al. (2014)", "predicate": "focuses_on", "to": "salient object detection", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_307", "file_type": "json", "from": "Sun et al. (2014)", "predicate": "uses", "to": "discriminative manifold-based approach", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_307", "file_type": "json", "from": "Sun et al. (2014)", "predicate": "presented_in", "to": "CVPR", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_307", "file_type": "json", "from": "Minsu Cho", "predicate": "is_author_of", "to": "Unsupervised Object Discovery and Localization", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_308", "file_type": "json", "from": "Minsu Cho", "predicate": "is_author_of", "to": "Unsupervised Object Discovery and Localization in the Wild", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_312", "file_type": "json", "from": "Minsu Cho", "predicate": "affiliated_with", "to": "Inria", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_307", "file_type": "json", "from": "Unsupervised Object Discovery and Localization", "predicate": "uses", "to": "bottom-up region proposals", "type": "factual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_307", "file_type": "json", "from": "Suha Kwak", "predicate": "is_author_of", "to": "Unsupervised Object Discovery and Localization", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_308", "file_type": "json", "from": "Suha Kwak", "predicate": "is_author_of", "to": "Unsupervised Object Discovery and Localization in the Wild", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_313", "file_type": "json", "from": "Suha Kwak", "predicate": "affiliated_with", "to": "Inria", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_307", "file_type": "json", "from": "Cordelia Schmid", "predicate": "is_author_of", "to": "Unsupervised Object Detection and Localization", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_313", "file_type": "json", "from": "Cordelia Schmid", "predicate": "affiliated_with", "to": "Inria", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_308", "file_type": "json", "from": "Unsupervised Object Discovery and Localization in the Wild", "predicate": "is_publication_of", "to": "CVPR", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_308", "file_type": "json", "from": "Unsupervised Object Discovery and Localization in the Wild", "predicate": "has_topic", "to": "object discovery", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_308", "file_type": "json", "from": "Unsupervised Object Discovery and Localization in the Wild", "predicate": "has_topic", "to": "object localization", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_308", "file_type": "json", "from": "Unsupervised Object Discovery and Localization in the Wild", "predicate": "uses_method", "to": "part-based matching", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_311", "file_type": "json", "from": "object discovery", "predicate": "occurs in", "to": "mixed-class datasets", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_308", "file_type": "json", "from": "Unsupervised Object Localization", "predicate": "uses_approach", "to": "bottom-up region proposals", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_309", "file_type": "json", "from": "setting", "predicate": "is", "to": "fully unsupervised", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_309", "file_type": "json", "from": "region proposals", "predicate": "form", "to": "candidate bounding boxes", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_309", "file_type": "json", "from": "correspondence", "predicate": "evaluated_using", "to": "probabilistic Hough transform", "type": "factual", "width": 0.91}, {"arrows": "to", "chunk_id": "doc_0_chunk_309", "file_type": "json", "from": "Hough transform", "predicate": "considers", "to": "appearance", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_309", "file_type": "json", "from": "Hough transform", "predicate": "considers", "to": "spatial consistency", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_310", "file_type": "json", "from": "candidate correspondence", "predicate": "considers", "to": "appearance", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_310", "file_type": "json", "from": "candidate correspondence", "predicate": "considers", "to": "spatial consistency", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_310", "file_type": "json", "from": "dominant objects", "predicate": "are discovered by", "to": "comparing scores", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_310", "file_type": "json", "from": "dominant objects", "predicate": "are localized by", "to": "selecting regions", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_310", "file_type": "json", "from": "regions", "predicate": "contain", "to": "dominant objects", "type": "factual", "width": 0.82}, {"arrows": "to", "chunk_id": "doc_0_chunk_545", "file_type": "json", "from": "regions", "predicate": "delineate", "to": "candidate objects", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_310", "file_type": "json", "from": "evaluations", "predicate": "occur on", "to": "standard benchmarks", "type": "factual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_311", "file_type": "json", "from": "rd benchmarks", "predicate": "demonstrates", "to": "proposed approach", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_311", "file_type": "json", "from": "Probabiltistic Hough transform", "predicate": "used for", "to": "multiple object detection", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_547", "file_type": "json", "from": "Alexe et al.", "predicate": "authored", "to": "Measuring the object-ness of image windows", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_311", "file_type": "json", "from": "Ballard", "predicate": "published", "to": "Generalizing the Hough transform", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_311", "file_type": "json", "from": "Joulin et al.", "predicate": "published", "to": "Discriminative clustering for image co-segmentation", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_311", "file_type": "json", "from": "Cho et al.", "predicate": "published", "to": "Learning graphs to match", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_647", "file_type": "json", "from": "Learning graphs to match", "predicate": "presented_in", "to": "Proceedings of the IEEE International Conference on Computer Vision", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_311", "file_type": "json", "from": "Unsupervised object localization", "predicate": "related to", "to": "object discovery", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_312", "file_type": "json", "from": "Image Co-segmentation", "predicate": "presented_at", "to": "CVPR", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_312", "file_type": "json", "from": "Learning Graphs", "predicate": "presented_at", "to": "ICCV", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_312", "file_type": "json", "from": "Efficient Image Localization", "predicate": "presented_at", "to": "ECCV", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_312", "file_type": "json", "from": "Pictoiral Structures", "predicate": "published_in", "to": "IJCV", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_530", "file_type": "json", "from": "IJCV", "predicate": "is_publication_platform_for", "to": "Architectural modeling", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_719", "file_type": "json", "from": "IJCV", "predicate": "published", "to": "Dynamic textures", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_719", "file_type": "json", "from": "IJCV", "predicate": "published", "to": "Dynamic texture detection based on motion analysis", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_313", "file_type": "json", "from": "Ijaz Akhter", "predicate": "contributor_to", "to": "Pose-Conditioned Joint Angle Limits", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_319", "file_type": "json", "from": "Ijaz Akhter", "predicate": "affiliation", "to": "Max Planck Institute for Intelligent Systems", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_319", "file_type": "json", "from": "Ijaz Akhter", "predicate": "email", "to": "ijaz.akhter@tuebingen.mpg.de", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_313", "file_type": "json", "from": "Pose-Conditioned Joint Angle Limits", "predicate": "presented_at", "to": "CVPR", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_313", "file_type": "json", "from": "Pose-Conditioned Joint Angle Limits", "predicate": "file_name", "to": "Akhter_Pose-Conditioned_Joint_Angle_2015_CVPR_paper.pdf", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_313", "file_type": "json", "from": "Michael J. Black", "predicate": "author_of", "to": "Pose-Conditioning Joint Angle Limits", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_313", "file_type": "json", "from": "Michael J. Black", "predicate": "contributor_to", "to": "Pose-Conditioned Joint Angle Limits", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_319", "file_type": "json", "from": "Michael J. Black", "predicate": "affiliation", "to": "Max Planck Institute for Intelligent Systems", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_314", "file_type": "json", "from": "3D human pose estimation", "predicate": "is_central_to", "to": "analysis of people in images and video", "type": "conceptual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_314", "file_type": "json", "from": "joint limits", "predicate": "vary_with", "to": "pose", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_314", "file_type": "json", "from": "motion capture dataset", "predicate": "explores", "to": "range of human poses", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_315", "file_type": "json", "from": "3D pose", "predicate": "derived from", "to": "2D joint locations", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_315", "file_type": "json", "from": "detections", "predicate": "performed on", "to": "Leeds sports pose dataset", "type": "factual", "width": 0.82}, {"arrows": "to", "chunk_id": "doc_0_chunk_316", "file_type": "json", "from": "he-art results", "predicate": "evaluates", "to": "2D to 3D pose estimation", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_316", "file_type": "json", "from": "he-art results", "predicate": "uses", "to": "CMU mocap dataset", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_316", "file_type": "json", "from": "superior results", "predicate": "based_on", "to": "manual annotations", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_316", "file_type": "json", "from": "superior results", "predicate": "based_on", "to": "automatic detections", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_316", "file_type": "json", "from": "automatic detections", "predicate": "uses", "to": "Leeds sports pose dataset", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_316", "file_type": "json", "from": "Andriluka, M. et al. (2010)", "predicate": "addresses", "to": "monocular 3D pose estimation and tracking", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_316", "file_type": "json", "from": "Barr`on, C. \u0026 Kakadiaris, I. (2001)", "predicate": "addresses", "to": "estimating anthropometry and pose", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_316", "file_type": "json", "from": "BenAbdelkader, C. \u0026 Yacoob, Y. (2008)", "predicate": "addresses", "to": "statistical estimation of human anthropometry", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_316", "file_type": "json", "from": "Human Pose Reconstruction", "predicate": "relies_on", "to": "Motion Capture Data", "type": "conceptual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_316", "file_type": "json", "from": "Prior Models", "predicate": "inform", "to": "Human Pose Reconstruction", "type": "conceptual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_317", "file_type": "json", "from": "Bourdev \u0026 Malik", "predicate": "published_in", "to": "International Conference on Computer Vision", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_317", "file_type": "json", "from": "Poselets", "predicate": "is_a", "to": "body part detector", "type": "conceptual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_317", "file_type": "json", "from": "Poselets", "predicate": "trained_using", "to": "3D human pose annotations", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_317", "file_type": "json", "from": "Chen, Nie, \u0026 Ji", "predicate": "published_in", "to": "IEEE Transactions on Image Processing", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_317", "file_type": "json", "from": "Guan et al.", "predicate": "published_in", "to": "Int. Conf. on Computer Vision (ICCV)", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_317", "file_type": "json", "from": "Estimating human shape", "predicate": "requires", "to": "single image", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_317", "file_type": "json", "from": "Grochow et al.", "predicate": "published_in", "to": "ACM Transactions on Graphics (TOG)", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_318", "file_type": "json", "from": "Grochow et al.", "predicate": "published", "to": "Style-based inverse kinematics", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_317", "file_type": "json", "from": "Style-based inverse kinematics", "predicate": "is_a", "to": "method", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_318", "file_type": "json", "from": "Style-based inverse kinematics", "predicate": "appeared_in", "to": "ACM Transactions on Graphics (TOG)", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_317", "file_type": "json", "from": "human anthropometry", "predicate": "estimated_from", "to": "single uncalibrated image", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_318", "file_type": "json", "from": "three-dimensional multivariate model", "predicate": "appeared_in", "to": "Clinical Biomechanics", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_318", "file_type": "json", "from": "Herda et al.", "predicate": "published", "to": "Hierarchical implicit surface joint limits", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_318", "file_type": "json", "from": "Hierarchical implicit surface joint limits", "predicate": "appeared_in", "to": "Computer Vision and Image Understanding", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_318", "file_type": "json", "from": "Lin et al.", "predicate": "published", "to": "sketching interface", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_547", "file_type": "json", "from": "Lin et al.", "predicate": "authored", "to": "Microsoft COCO: Common objects in context", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_318", "file_type": "json", "from": "sketching interface", "predicate": "appeared_in", "to": "IEEE Transactions on Visualization and Computer Graphics", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_319", "file_type": "json", "from": "Max Planck Institute for Intelligent Systems", "predicate": "location", "to": "Tuebingen", "type": "factual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_319", "file_type": "json", "from": "Ran Tao", "predicate": "author_of", "to": "Attributes and Categories for Generic Instance Search from One Example", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_320", "file_type": "json", "from": "Ran Tao", "predicate": "is_author_of", "to": "paper", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_319", "file_type": "json", "from": "Attributes and Categories for Generic Instance Search from One Example", "predicate": "published_in", "to": "CVPR paper", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_319", "file_type": "json", "from": "Arnold W.M. Smeulders", "predicate": "author_of", "to": "Attributes and Categories for Generic Instance Search from One Example", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_320", "file_type": "json", "from": "Arnold W.M. Smeulders", "predicate": "is_author_of", "to": "paper", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_319", "file_type": "json", "from": "Tao_Attributes_and_Categories_2015_CVPR_paper", "predicate": "represents", "to": "Attributes and Categories for Generic Instance Search from One Example", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_320", "file_type": "json", "from": "instance search methods", "predicate": "struggle with", "to": "arbitrary 3D objects", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_320", "file_type": "json", "from": "arbitrary 3D objects", "predicate": "include", "to": "shoes", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_320", "file_type": "json", "from": "category-specific attributes", "predicate": "propose using", "to": "authors", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_320", "file_type": "json", "from": "category-specific attributes", "predicate": "handles", "to": "appearance variations", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_321", "file_type": "json", "from": "category-level information", "predicate": "combines_with", "to": "category-specific attributes", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_321", "file_type": "json", "from": "combination", "predicate": "outperforms", "to": "approaches relying on low-level features", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_320", "file_type": "json", "from": "core challenge", "predicate": "is", "to": "representing query image", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_321", "file_type": "json", "from": "query image", "predicate": "requires", "to": "robust representation", "type": "conceptual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_321", "file_type": "json", "from": "robust representation", "predicate": "is_resistant_to", "to": "appearance variations", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_322", "file_type": "json", "from": "robust representation", "predicate": "handles", "to": "Appearance Variation", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_321", "file_type": "json", "from": "rich representation", "predicate": "enables", "to": "distinction from similar instances", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_321", "file_type": "json", "from": "distinction", "predicate": "is_between", "to": "similar instances", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_322", "file_type": "json", "from": "Generic Instance Search", "predicate": "relies_on", "to": "low-level features", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_322", "file_type": "json", "from": "Core Challenge", "predicate": "involves", "to": "robust representation", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_322", "file_type": "json", "from": "Appearance Variation", "predicate": "impacts", "to": "robust representation", "type": "causal", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_322", "file_type": "json", "from": "Attribute Representation", "predicate": "facilitates", "to": "distinction", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_322", "file_type": "json", "from": "Large Scale Visual Recognition Challenge", "predicate": "provides", "to": "dataset", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_322", "file_type": "json", "from": "Attribute Transfer", "predicate": "enables", "to": "detecting unseen object classes", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_322", "file_type": "json", "from": "Multiple queries", "predicate": "supports", "to": "large scale specific object retrieval", "type": "factual", "width": 0.87}, {"arrows": "to", "chunk_id": "doc_0_chunk_323", "file_type": "json", "from": "Aranandjelovic", "predicate": "authored", "to": "Multiple queries", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_323", "file_type": "json", "from": "Zisserman", "predicate": "co-authored", "to": "Multiple queries", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_323", "file_type": "json", "from": "Naphade", "predicate": "authored", "to": "concept ontology", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_323", "file_type": "json", "from": "concept ontology", "predicate": "is_published_in", "to": "IEEE MultiMedia", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_323", "file_type": "json", "from": "Perdoch", "predicate": "authored", "to": "efficient representation", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_323", "file_type": "json", "from": "Farhad", "predicate": "authored", "to": "describing objects", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_324", "file_type": "json", "from": "Farhad", "predicate": "authors", "to": "CVPR paper", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_324", "file_type": "json", "from": "Farhad", "predicate": "affiliated_with", "to": "ISLA", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_323", "file_type": "json", "from": "describing objects", "predicate": "focuses_on", "to": "object attributes", "type": "conceptual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_325", "file_type": "json", "from": "Recognition algorithms", "predicate": "based on", "to": "convolutional networks", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_325", "file_type": "json", "from": "last layer output", "predicate": "acts as", "to": "feature representation", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_325", "file_type": "json", "from": "last layer output", "predicate": "is", "to": "spatially coarse", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_325", "file_type": "json", "from": "earlier layers", "predicate": "are", "to": "precise in localization", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_325", "file_type": "json", "from": "earlier layers", "predicate": "lacks", "to": "semantics", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_325", "file_type": "json", "from": "hypercolumn", "predicate": "is", "to": "vector of activations", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_325", "file_type": "json", "from": "pixel", "predicate": "above", "to": "CNN units", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_325", "file_type": "json", "from": "hypercolumns", "predicate": "acts as", "to": "pixel descriptors", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_325", "file_type": "json", "from": "hypercolumns", "predicate": "demonstrates", "to": "improvements", "type": "factual", "width": 0.89}, {"arrows": "to", "chunk_id": "doc_0_chunk_325", "file_type": "json", "from": "hypercolumns", "predicate": "improves", "to": "state-of-the-art results", "type": "factual", "width": 0.91}, {"arrows": "to", "chunk_id": "doc_0_chunk_325", "file_type": "json", "from": "simultaneous detection", "predicate": "is a", "to": "localization task", "type": "conceptual", "width": 0.86}, {"arrows": "to", "chunk_id": "doc_0_chunk_325", "file_type": "json", "from": "keypoint localization", "predicate": "is a", "to": "localization task", "type": "conceptual", "width": 0.86}, {"arrows": "to", "chunk_id": "doc_0_chunk_325", "file_type": "json", "from": "part labeling", "predicate": "is a", "to": "localization task", "type": "conceptual", "width": 0.86}, {"arrows": "to", "chunk_id": "doc_0_chunk_326", "file_type": "json", "from": "Keypoint Localization", "predicate": "is_task", "to": "Fine-grained Localization", "type": "conceptual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_326", "file_type": "json", "from": "Spatial Pyramid Pooling", "predicate": "improves", "to": "Visual Recognition", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_326", "file_type": "json", "from": "Multi-scale Feature Integration", "predicate": "used_in", "to": "Object Segmentation", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_326", "file_type": "json", "from": "Arbel\u00e1ez, P.", "predicate": "authored", "to": "Multiscale Combinatorial Grouping", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_326", "file_type": "json", "from": "He, K.", "predicate": "authored", "to": "Spatial Pyramid Pooling", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_326", "file_type": "json", "from": "Barron, J. T.", "predicate": "authored", "to": "Volumetric Semantic Segmentation", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_326", "file_type": "json", "from": "Hypercolumn Representation", "predicate": "enhances", "to": "Object Segmentation", "type": "conceptual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_327", "file_type": "json", "from": "Barron et al.", "predicate": "published_in", "to": "ICCV", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_327", "file_type": "json", "from": "Hubel \u0026 Wiesel", "predicate": "published_in", "to": "The Journal of Physiology", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_327", "file_type": "json", "from": "Bo \u0026 Fowlakes", "predicate": "published_in", "to": "CVPR", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_327", "file_type": "json", "from": "Ionescu et al.", "predicate": "published_in", "to": "CVPR", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_327", "file_type": "json", "from": "Jones \u0026 Malik", "predicate": "published_in", "to": "ECCV", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_327", "file_type": "json", "from": "Koenderink \u0026 van Doorn", "predicate": "published_in", "to": "Biological Cybernetics", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_327", "file_type": "json", "from": "Volumetric semantic segmentation", "predicate": "uses", "to": "pyramid context features", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_327", "file_type": "json", "from": "Visual cortex", "predicate": "studied_in", "to": "Hubel \u0026 Wiesel\u0027s work", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_327", "file_type": "json", "from": "Pedestrian parsing", "predicate": "uses", "to": "shape-based methods", "type": "conceptual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_328", "file_type": "json", "from": "Biological cybernetics", "predicate": "is_journal", "to": "visual system", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_328", "file_type": "json", "from": "Malik", "predicate": "is_affiliated_with", "to": "University of California, Berkeley", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_328", "file_type": "json", "from": "Xie", "predicate": "is_author_of", "to": "DeepShape", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_328", "file_type": "json", "from": "DeepShape", "predicate": "is_a", "to": "shape descriptor", "type": "conceptual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_328", "file_type": "json", "from": "DeepShape", "predicate": "supports", "to": "3D shape matching", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_328", "file_type": "json", "from": "DeepShape", "predicate": "supports", "to": "3D shape retrieval", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_329", "file_type": "json", "from": "shape descriptor", "predicate": "used for", "to": "3D shape matching and retrieval", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_330", "file_type": "json", "from": "shape descriptor", "predicate": "is_formed_by", "to": "multiple discriminative auto-encoders", "type": "structural", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_330", "file_type": "json", "from": "shape descriptor", "predicate": "used_for", "to": "3D shape matching", "type": "functional", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_330", "file_type": "json", "from": "shape descriptor", "predicate": "used_for", "to": "3D shape retrieval", "type": "functional", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_329", "file_type": "json", "from": "3D model", "predicate": "poses challenge to", "to": "3D shape matching and retrieval", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_329", "file_type": "json", "from": "shape feature learning scheme", "predicate": "uses", "to": "multiscale shape distribution", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_329", "file_type": "json", "from": "auto-encoder", "predicate": "is", "to": "discriminative deep auto-encoder", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_329", "file_type": "json", "from": "shape distribution", "predicate": "is used as input to", "to": "auto-encoder", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_329", "file_type": "json", "from": "Fisher discrimination criterion", "predicate": "is imposed on", "to": "neurons", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_329", "file_type": "json", "from": "Fisher discrimination criterion", "predicate": "applied to", "to": "hidden layer neurons", "type": "factual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_329", "file_type": "json", "from": "neurons", "predicate": "are concatenated to form", "to": "shape descriptor", "type": "factual", "width": 0.91}, {"arrows": "to", "chunk_id": "doc_0_chunk_330", "file_type": "json", "from": "3D models", "predicate": "have", "to": "geometric variations", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_331", "file_type": "json", "from": "Proposed Method", "predicate": "used_for", "to": "3D shape matching and retrieval", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_331", "file_type": "json", "from": "Proposed Method", "predicate": "demonstrated_by", "to": "Experimental results", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_331", "file_type": "json", "from": "Agathos et al. (2009)", "predicate": "addresses", "to": "Retrieval of 3D articulated objects", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_331", "file_type": "json", "from": "Assfalg et al. (2007)", "predicate": "addresses", "to": "Content-based retrieval of 3D objects", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_331", "file_type": "json", "from": "Belongie et al. (2000)", "predicate": "introduces", "to": "Shape context", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_332", "file_type": "json", "from": "Belongie et al. (2000)", "predicate": "developed", "to": "Shape Context", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_331", "file_type": "json", "from": "Shape context", "predicate": "used_for", "to": "shape matching and object recognition", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_331", "file_type": "json", "from": "Geometric Feature Learning", "predicate": "related_to", "to": "3D shape matching", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_331", "file_type": "json", "from": "Deep Auto-encoders", "predicate": "applied_in", "to": "3D shape matching", "type": "factual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_331", "file_type": "json", "from": "Ric variations", "predicate": "is_example_of", "to": "Mcgill dataset", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_331", "file_type": "json", "from": "Ric variations", "predicate": "is_example_of", "to": "SHREC\u002710 Shape dataset", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_332", "file_type": "json", "from": "Shape Context", "predicate": "used for", "to": "shape matching", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_332", "file_type": "json", "from": "Shape Context", "predicate": "used for", "to": "object recognition", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_332", "file_type": "json", "from": "Deep Architectures", "predicate": "relevant to", "to": "AI", "type": "conceptual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_332", "file_type": "json", "from": "Shape Google", "predicate": "uses", "to": "geometric words", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_332", "file_type": "json", "from": "Isometry-invariant distances", "predicate": "computed by", "to": "Bronstein et al. (2006)", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_332", "file_type": "json", "from": "Gromov-Hausdorff framework", "predicate": "supports", "to": "non-rigid shape matching", "type": "conceptual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_332", "file_type": "json", "from": "Diffusion geometry", "predicate": "contributes to", "to": "topologically-robust matching", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_332", "file_type": "json", "from": "Bronstein et al. (2011)", "predicate": "introduced", "to": "Shape Google", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_333", "file_type": "json", "from": "Mahmoudi, M.", "predicate": "authored", "to": "A Gromov-Hausdorff framework", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_333", "file_type": "json", "from": "A Gromov-Hausdorff framework", "predicate": "is_used_in", "to": "shape matching", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_361", "file_type": "json", "from": "A Gromov-Hausdorff framework", "predicate": "is_published_in", "to": "International Journal of Computer Vision", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_333", "file_type": "json", "from": "Chen, D.-Y.", "predicate": "authored", "to": "On visual similarity based 3D model retrieval", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_333", "file_type": "json", "from": "On visual similarity based 3D model retrieval", "predicate": "addresses", "to": "3D model retrieval", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_333", "file_type": "json", "from": "Chen, X.", "predicate": "authored", "to": "A benchmark for 3D mesh segmentation", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_587", "file_type": "json", "from": "Chen, X.", "predicate": "authored", "to": "Locally linear regression for pose-invariant face recognition", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_333", "file_type": "json", "from": "A benchmark", "predicate": "is_for", "to": "3D mesh segmentation", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_333", "file_type": "json", "from": "De Goes, F.", "predicate": "authored", "to": "A hierarchical segmentation", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_333", "file_type": "json", "from": "A hierarchical segmentation", "predicate": "concerns", "to": "articulated bodies", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_333", "file_type": "json", "from": "Jin Xie", "predicate": "works_at", "to": "New York University Abu Dhabi", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_333", "file_type": "json", "from": "Jin Xie", "predicate": "has_email", "to": "jin.xie@nyu.edu", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_333", "file_type": "json", "from": "Yi Fang", "predicate": "works_at", "to": "New York University Abu Dhabi", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_333", "file_type": "json", "from": "Yi Fang", "predicate": "has_email", "to": "yfang@nyu.edu", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_334", "file_type": "json", "from": "Fan Zhu", "predicate": "is_affiliated_with", "to": "Department of Electrical and Computer Engineering", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_334", "file_type": "json", "from": "Department of Electrical and ComputerEngineering", "predicate": "is_located_at", "to": "New York University Abu Dhabi", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_334", "file_type": "json", "from": "Edward Wong", "predicate": "is_affiliated_with", "to": "Polytechnic School of Engineering", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_334", "file_type": "json", "from": "Polytechnic School of Engineering", "predicate": "is_part_of", "to": "New York University", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_334", "file_type": "json", "from": "Bingbing Ni", "predicate": "is_author_of", "to": "Motion Part Regularization", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_337", "file_type": "json", "from": "Bingbing Ni", "predicate": "affiliated_with", "to": "ADSC Singapore", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_334", "file_type": "json", "from": "Motion Part Regularization", "predicate": "improves", "to": "Action Recognition", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_334", "file_type": "json", "from": "Pierre Moulin", "predicate": "is_author_of", "to": "Motion Part Regularization", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_514", "file_type": "json", "from": "Pierre Moulin", "predicate": "affiliated_with", "to": "Department of ECE", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_334", "file_type": "json", "from": "golf", "predicate": "is_an_action_label", "to": "Action", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_334", "file_type": "json", "from": "punch", "predicate": "is_an_action_label", "to": "Action", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_334", "file_type": "json", "from": "Fisher vector", "predicate": "is_used_in", "to": "Action Recognition", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_335", "file_type": "json", "from": "Ni_Motion_Part_Regularization_2015_CVPR_paper", "predicate": "proposes", "to": "Motion Part Regularization framework", "type": "conceptual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_335", "file_type": "json", "from": "Motion Part Regularization framework", "predicate": "aims_to_improve", "to": "action recognition", "type": "conceptual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_335", "file_type": "json", "from": "Motion Part Regularization framework", "predicate": "mines", "to": "dense trajectories", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_418", "file_type": "json", "from": "action recognition", "predicate": "implemented_in", "to": "3D natural scenes", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_418", "file_type": "json", "from": "action recognition", "predicate": "uses", "to": "depth cameras", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_335", "file_type": "json", "from": "discriminativeness weighted Fisher vector representation", "predicate": "is_more_discriminative_than", "to": "traditional Fisher vector", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_335", "file_type": "json", "from": "objective function", "predicate": "encourages", "to": "sparse selection of trajectory groups", "type": "factual", "width": 0.91}, {"arrows": "to", "chunk_id": "doc_0_chunk_335", "file_type": "json", "from": "objective function", "predicate": "includes", "to": "action class discriminative term", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_511", "file_type": "json", "from": "objective function", "predicate": "includes", "to": "discriminative term", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_335", "file_type": "json", "from": "optimization algorithm", "predicate": "uses", "to": "auxiliary variables", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_336", "file_type": "json", "from": "Optimization Algorithm", "predicate": "used_for", "to": "Action Recognition", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_336", "file_type": "json", "from": "Motion Part", "predicate": "has", "to": "Discriminative Weights", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_336", "file_type": "json", "from": "Dense Trajectories", "predicate": "described_in", "to": "Wang et al. (2011)", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_336", "file_type": "json", "from": "Behavior Recognition", "predicate": "uses", "to": "Spatio-Temporal Grouping", "type": "factual", "width": 0.82}, {"arrows": "to", "chunk_id": "doc_0_chunk_336", "file_type": "json", "from": "LIBSVM", "predicate": "is_a", "to": "library", "type": "conceptual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_336", "file_type": "json", "from": "Automatic Annotation", "predicate": "uses", "to": "Human Actions", "type": "factual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_337", "file_type": "json", "from": "O. Duchenne", "predicate": "authors", "to": "Automatic annotation of human actions in video", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_337", "file_type": "json", "from": "I. Laptev", "predicate": "authors", "to": "Automatic annotation of human actions in video", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_337", "file_type": "json", "from": "J. Sivic", "predicate": "authors", "to": "Automatic annotation of human actions in video", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_337", "file_type": "json", "from": "J. Ponce", "predicate": "authors", "to": "Automatic annotation of human actions in video", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_337", "file_type": "json", "from": "H. Wang", "predicate": "authors", "to": "Action recognition with improved trajectories", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_337", "file_type": "json", "from": "C. Schmid", "predicate": "authors", "to": "Action recognition with improved trajectories", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_383", "file_type": "json", "from": "C. Schmid", "predicate": "co-authored", "to": "Multi-fold mil training", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_337", "file_type": "json", "from": "P. Felzenszwalb", "predicate": "authors", "to": "Object detection with discriminatively trained part based models", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_337", "file_type": "json", "from": "R. Girshick", "predicate": "authors", "to": "Object detection with discriminatively trained part based models", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_437", "file_type": "json", "from": "R. Girshick", "predicate": "co_authored", "to": "Efficient regression of general-activity human poses", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_337", "file_type": "json", "from": "D. McAlleser", "predicate": "authors", "to": "Object detection with discriminatively trained part based models", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_337", "file_type": "json", "from": "D. Ramanan", "predicate": "authors", "to": "Object description with discriminatively trained part based models", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_337", "file_type": "json", "from": "J. Wang", "predicate": "authors", "to": "Mining actionlet ensemble for action recognition with depth cameras", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_337", "file_type": "json", "from": "Z. Liu", "predicate": "authors", "to": "Mining actionlet ensemble for action recognition with depth cameras", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_337", "file_type": "json", "from": "Y. Wu", "predicate": "authors", "to": "Mining actionlet ensemble for action recognition with depth cameras", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_337", "file_type": "json", "from": "J. Yuan", "predicate": "authors", "to": "Mining actionlet ensemble for action recognition with depth cameras", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_337", "file_type": "json", "from": "M. Jain", "predicate": "authors", "to": "Better exploiting motion for better action recognition", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_337", "file_type": "json", "from": "H. Jegou", "predicate": "authors", "to": "Better exploiting motion for better action recognition", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_337", "file_type": "json", "from": "P. Bouthemy", "predicate": "authors", "to": "Better exploiting motion for better action recognition", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_337", "file_type": "json", "from": "Y.-G. Jiang", "predicate": "authors", "to": "Trajectory-based modeling of human actions with motion reference points", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_337", "file_type": "json", "from": "Q. Dai", "predicate": "authors", "to": "Trajectory-based modeling of human actions with motion reference points", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_337", "file_type": "json", "from": "X. Xue", "predicate": "authors", "to": "Trajectory-based modeling of human actions with motion reference points", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_337", "file_type": "json", "from": "W. Liu", "predicate": "authors", "to": "Trajectory-based modeling of human actions with motion reference points", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_337", "file_type": "json", "from": "C.-W. Ngo", "predicate": "authors", "to": "Trajectory-based modeling of human actions with motion reference points", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_337", "file_type": "json", "from": "Pierre Bounameaux", "predicate": "affiliated_with", "to": "ADSC Singapore", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_340", "file_type": "json", "from": "gaze correction solutions", "predicate": "relies_on", "to": "additional hardware", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_340", "file_type": "json", "from": "pixel replacement operations", "predicate": "localized_around", "to": "eyes", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_341", "file_type": "json", "from": "Monocular Gaze Correction", "predicate": "is_field_of", "to": "Computer Vision", "type": "conceptual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_341", "file_type": "json", "from": "Monocular Gaze Correction", "predicate": "uses", "to": "Machine Learning", "type": "conceptual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_341", "file_type": "json", "from": "Uncanny Valley Effect", "predicate": "is_effect_of", "to": "Localized Pixel Replacement", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_341", "file_type": "json", "from": "Amit", "predicate": "authored", "to": "Shape Quantization", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_341", "file_type": "json", "from": "Shape Quantization", "predicate": "is_published_in", "to": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_341", "file_type": "json", "from": "Doll\u00e1r", "predicate": "authored", "to": "Structured Forests", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_341", "file_type": "json", "from": "Structured Forests", "predicate": "facilitates", "to": "Fast Edge Detection", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_341", "file_type": "json", "from": "Fanelli", "predicate": "authored", "to": "Random Forests for 3D Face Analysis", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_341", "file_type": "json", "from": "Random Forests for 3D Face Analysis", "predicate": "is_published_in", "to": "International Journal of Computer Vision", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_342", "file_type": "json", "from": "Fanelli, G.", "predicate": "authored", "to": "Random forests for real time 3d face analysis", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_342", "file_type": "json", "from": "Gaze correction", "predicate": "achieved with", "to": "single webcam", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_342", "file_type": "json", "from": "Gall, J.", "predicate": "authored", "to": "Class-specific hough forests for object detection", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_342", "file_type": "json", "from": "Hough forests", "predicate": "used for", "to": "object detection", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_342", "file_type": "json", "from": "Jones, A.", "predicate": "authored", "to": "Achieving eye contact in a one-to-many 3D video teleconferecing system", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_342", "file_type": "json", "from": "eye contact", "predicate": "achieved in", "to": "3D video teleconferencing system", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_342", "file_type": "json", "from": "Kazemi, V.", "predicate": "authored", "to": "One milliseccond face alignment with an ensemble of regression trees", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_343", "file_type": "json", "from": "Kazemi, V.", "predicate": "authored", "to": "One milliseccond face alignment", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_343", "file_type": "json", "from": "Kazemi, V.", "predicate": "collaborated_with", "to": "Sullivan, J.", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_342", "file_type": "json", "from": "face alignment", "predicate": "achieved with", "to": "regression trees", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_343", "file_type": "json", "from": "Kuster, C.", "predicate": "authored", "to": "Gaze correction", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_343", "file_type": "json", "from": "Ren, S.", "predicate": "authored", "to": "Face alignment", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_343", "file_type": "json", "from": "Ren, S.", "predicate": "collaborated_with", "to": "Cao, X.", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_413", "file_type": "json", "from": "Cao, X.", "predicate": "authored", "to": "practical transfer learning algorithm", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_343", "file_type": "json", "from": "Kononenko, Daniil", "predicate": "affiliated_with", "to": "Skolkovo Institute of Science and Technology (Skoltech)", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_343", "file_type": "json", "from": "Lempitsky, Victor", "predicate": "affiliated_with", "to": "Skolkovo Institute of Science and Technology (Skeltech)", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_343", "file_type": "json", "from": "Zhao, Kaili", "predicate": "authored", "to": "Joint Patch and Multi-label Learning", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_343", "file_type": "json", "from": "Joint Patch and Multi-label Learning", "predicate": "focuses_on", "to": "Facial Action Unit Detection", "type": "conceptual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_343", "file_type": "json", "from": "Zhao_Joint_Patch_and_2015_CVPR_paper.pdf", "predicate": "is_publication_of", "to": "Joint Patch and Multi-label Learning", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_344", "file_type": "json", "from": "Facial Action Coding System", "predicate": "is_system_for", "to": "describing facial movements", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_346", "file_type": "json", "from": "Facial Action Coding System", "predicate": "is_reference_for", "to": "human face", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_346", "file_type": "json", "from": "Facial Action Coding System", "predicate": "defines", "to": "facial action units", "type": "conceptual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_347", "file_type": "json", "from": "Facial Action Coding System", "predicate": "is_reference_for", "to": "facial action unit detection", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_344", "file_type": "json", "from": "Action Units", "predicate": "part_of", "to": "Facial Action Coding System", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_345", "file_type": "json", "from": "JPML", "predicate": "is_instance_of", "to": "Multi-label Learning", "type": "conceptual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_345", "file_type": "json", "from": "JPML", "predicate": "achieves", "to": "highest average F1 scores", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_345", "file_type": "json", "from": "JPML", "predicate": "compared_to", "to": "state-of-the-art methods", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_345", "file_type": "json", "from": "CK+", "predicate": "is_a", "to": "dataset", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_345", "file_type": "json", "from": "BP4D", "predicate": "is_a", "to": "dataset", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_345", "file_type": "json", "from": "Machine learning", "predicate": "addresses", "to": "facial expression recognition", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_595", "file_type": "json", "from": "Machine learning", "predicate": "is_publication_venue_for", "to": "Support-vector networks", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_345", "file_type": "json", "from": "FACIAL Action Coding System (FACS)", "predicate": "related_to", "to": "facial expression recognition", "type": "conceptual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_345", "file_type": "json", "from": "Action Unit (AU) Detection", "predicate": "part_of", "to": "FACIAL Action Coding System (FACS)", "type": "conceptual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_345", "file_type": "json", "from": "Patch Learning", "predicate": "related_to", "to": "JPML", "type": "conceptual", "width": 0.7}, {"arrows": "to", "chunk_id": "doc_0_chunk_345", "file_type": "json", "from": "Alternating direction method of multipliers", "predicate": "utilized_in", "to": "optimization techniques", "type": "factual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_346", "file_type": "json", "from": "Alternating Direction Method of Multipliers", "predicate": "is_optimization_technique", "to": "optimization techniques", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_346", "file_type": "json", "from": "Affective Computing", "predicate": "is_field_of", "to": "Machine Learning", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_346", "file_type": "json", "from": "Facial Action Unit Event Detection", "predicate": "is_topic_of", "to": "ICCV", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_346", "file_type": "json", "from": "Facial Action Unit Event Detection", "predicate": "uses", "to": "cascade of tasks", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_346", "file_type": "json", "from": "X. Ding", "predicate": "is_author_of", "to": "Facial Action Unit Event Detection", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_347", "file_type": "json", "from": "P. Ekman", "predicate": "authored", "to": "Facial Action Coding System", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_346", "file_type": "json", "from": "F. De la Torre", "predicate": "is_author_of", "to": "Intraface", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_347", "file_type": "json", "from": "F. De la Torre", "predicate": "authored", "to": "Selective transfer machine", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_347", "file_type": "json", "from": "J. C. Hager", "predicate": "authored", "to": "Facial Action Coding System", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_347", "file_type": "json", "from": "Selective transfer machine", "predicate": "addresses", "to": "personalization in facial action unit detection", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_347", "file_type": "json", "from": "W.-S. Chu", "predicate": "authored", "to": "Selective transfer machine", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_347", "file_type": "json", "from": "J. F. Cohn", "predicate": "authored", "to": "Selective transfer machine", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_347", "file_type": "json", "from": "Facing imbalanced data", "predicate": "deals_with", "to": "imbalanced datasets", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_347", "file_type": "json", "from": "L. A. Jeni", "predicate": "authored", "to": "Facing imbalanced data", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_347", "file_type": "json", "from": "Data-free prior model", "predicate": "uses", "to": "data-free approach", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_348", "file_type": "json", "from": "Data-free prior model", "predicate": "for", "to": "facial action unit recognition", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_347", "file_type": "json", "from": "Y. Li", "predicate": "authored", "to": "Data-free prior model", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_348", "file_type": "json", "from": "Y. Zhao", "predicate": "affiliated_with", "to": "School of Comm. and Info. Engineering", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_348", "file_type": "json", "from": "School of Comm. and Info. Engineering", "predicate": "part_of", "to": "Beijing University of Posts and Telecom.", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_348", "file_type": "json", "from": "Beijing University of Posts and Telecom.", "predicate": "located_in", "to": "Beijing", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_348", "file_type": "json", "from": "G. Littlewort", "predicate": "authored", "to": "Dynamics of facial expression", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_348", "file_type": "json", "from": "Dynamics of facial expression", "predicate": "uses", "to": "AU-cascades", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_348", "file_type": "json", "from": "AU-cascades", "predicate": "for", "to": "action unit detection", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_348", "file_type": "json", "from": "Wen-Sheng Chu", "predicate": "affiliated_with", "to": "Robotics Institute", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_349", "file_type": "json", "from": "Robotics Institute", "predicate": "part_of", "to": "Carnegie Mellon University", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_349", "file_type": "json", "from": "Fernando De la Torre", "predicate": "affiliated_with", "to": "Robotics Institute", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_348", "file_type": "json", "from": "Jeffrey F. Cohn", "predicate": "affiliated_with", "to": "Robotic Institute", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_349", "file_type": "json", "from": "Jeffrey F. Cohn", "predicate": "affiliated_with", "to": "Robotics Institute", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_349", "file_type": "json", "from": "Honggang Zhang", "predicate": "affiliated_with", "to": "School of Comm. and Info. Engineering", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_349", "file_type": "json", "from": "Beijing University of Posts and Telecom", "predicate": "located_in", "to": "China", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_349", "file_type": "json", "from": "TVSum", "predicate": "summarizes", "to": "Web Videos", "type": "conceptual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_349", "file_type": "json", "from": "TVSum", "predicate": "uses", "to": "Titles", "type": "conceptual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_350", "file_type": "json", "from": "TVSum", "predicate": "is_a", "to": "video summarization framework", "type": "conceptual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_350", "file_type": "json", "from": "TVSum", "predicate": "guides", "to": "title-based image search results", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_350", "file_type": "json", "from": "TVSum", "predicate": "produces", "to": "superior quality summaries", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_351", "file_type": "json", "from": "TVSum", "predicate": "compared_to", "to": "existing approaches", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_351", "file_type": "json", "from": "TVSum", "predicate": "produces", "to": "summaries", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_351", "file_type": "json", "from": "TVSum", "predicate": "has_quality", "to": "superior", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_349", "file_type": "json", "from": "Yale Song", "predicate": "author_of", "to": "TVSum", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_354", "file_type": "json", "from": "Yale Song", "predicate": "works_at", "to": "Yahoo Labs", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_353", "file_type": "json", "from": "Yale Song", "predicate": "has_contact", "to": "yalessong@yahoo-inc.com", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_349", "file_type": "json", "from": "Jordi Vallmitjana", "predicate": "author_of", "to": "TVSum", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_354", "file_type": "json", "from": "Jordi Vallmitjana", "predicate": "works_at", "to": "Yahoo Labs", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_349", "file_type": "json", "from": "Amanda Stent", "predicate": "author_of", "to": "TVSum", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_354", "file_type": "json", "from": "Amanda Stent", "predicate": "works_at", "to": "Yahoo Labs", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_349", "file_type": "json", "from": "Alejandro Jaimes", "predicate": "author_of", "to": "TVSum", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_354", "file_type": "json", "from": "Alejandro Jaimes", "predicate": "works_at", "to": "Yahoo Labs", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_349", "file_type": "json", "from": "Song_TVSum_Summarizing_Web_2015_CVPR_paper.pdf", "predicate": "is_a", "to": "CVPR paper", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_350", "file_type": "json", "from": "Video summarization", "predicate": "is_challenging_problem_due_to", "to": "need for prior knowledge", "type": "conceptual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_350", "file_type": "json", "from": "video titles", "predicate": "are", "to": "descriptive", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_350", "file_type": "json", "from": "co-archetypal analysis", "predicate": "is", "to": "novel technique", "type": "conceptual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_350", "file_type": "json", "from": "co-archetypal analysis", "predicate": "learns", "to": "visual concepts", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_350", "file_type": "json", "from": "visual concepts", "predicate": "are_shared_between", "to": "video and images", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_350", "file_type": "json", "from": "TVSum50", "predicate": "is_a", "to": "benchmark dataset", "type": "conceptual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_351", "file_type": "json", "from": "TVSum50", "predicate": "is_a", "to": "dataset", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_351", "file_type": "json", "from": "TVSum50", "predicate": "introduced_in", "to": "study", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_350", "file_type": "json", "from": "image search results", "predicate": "contains", "to": "noise and variance", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_351", "file_type": "json", "from": "Co-Archetypal Analysis", "predicate": "is_a", "to": "analysis method", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_351", "file_type": "json", "from": "Canonical Visual Concepts", "predicate": "is_a", "to": "concept", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_351", "file_type": "json", "from": "M. Basseville", "predicate": "authored", "to": "Detection of abrupt changes", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_351", "file_type": "json", "from": "A. Beck", "predicate": "authored", "to": "shrinkage-thresholding algorithm", "type": "factual", "width": 0.91}, {"arrows": "to", "chunk_id": "doc_0_chunk_351", "file_type": "json", "from": "K. Bleakley", "predicate": "authored", "to": "group fused lasso", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_351", "file_type": "json", "from": "Y. Chen", "predicate": "authored", "to": "archetypal analysis", "type": "factual", "width": 0.89}, {"arrows": "to", "chunk_id": "doc_0_chunk_351", "file_type": "json", "from": "S. Fidler", "predicate": "authored", "to": "sentence is worth a thousand pixels", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_352", "file_type": "json", "from": "S. Fidler", "predicate": "co-authored", "to": "A sentence is worth a thousand pixels", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_352", "file_type": "json", "from": "Airal", "predicate": "co-authored", "to": "Fast and robust archetypal analysis", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_352", "file_type": "json", "from": "Fast and robust archetypal analysis", "predicate": "published_in", "to": "CVPR", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_352", "file_type": "json", "from": "M. Gygli", "predicate": "co-authored", "to": "Creating summaries from user videos", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_352", "file_type": "json", "from": "Creating summaries from user videos", "predicate": "published_in", "to": "ECCV", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_352", "file_type": "json", "from": "Y. Jia", "predicate": "co-authored", "to": "Visual concept learning", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_352", "file_type": "json", "from": "Visual concept learning", "predicate": "published_in", "to": "NIPS", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_352", "file_type": "json", "from": "Y. J. Lee", "predicate": "co-authored", "to": "Discovering important people and objects", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_383", "file_type": "json", "from": "Y. J. Lee", "predicate": "authored", "to": "Object-graphs for context-aware category discovery", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_352", "file_type": "json", "from": "Discovering important people and objects", "predicate": "published_in", "to": "CVPR", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_352", "file_type": "json", "from": "L. Li", "predicate": "co-authored", "to": "Video summarization via transferrable structured learning", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_352", "file_type": "json", "from": "Video summarization via transferrable structured learning", "predicate": "published_in", "to": "WWW", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_352", "file_type": "json", "from": "D. Lin", "predicate": "co-authored", "to": "Visual semantic search", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_352", "file_type": "json", "from": "Visual semantic search", "predicate": "published_in", "to": "CVPR", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_353", "file_type": "json", "from": "Visual semantic search", "predicate": "is_method_for", "to": "Retrieving videos", "type": "conceptual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_353", "file_type": "json", "from": "Visual semantic search", "predicate": "uses", "to": "complex textual queries", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_353", "file_type": "json", "from": "Yahoo Labs", "predicate": "is_organization", "to": "research institution", "type": "conceptual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_354", "file_type": "json", "from": "isual semantic search", "predicate": "is_topic_of", "to": "video retrieval", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_354", "file_type": "json", "from": "isual semantic search", "predicate": "presented_at", "to": "CVPR", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_354", "file_type": "json", "from": "Tianjun Xiao", "predicate": "is_author_of", "to": "The Application of Two-level Attention Models", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_356", "file_type": "json", "from": "Tianjun Xiao", "predicate": "affiliated_with", "to": "Institute of Computer Science and Technologies", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_354", "file_type": "json", "from": "The Application of Two-level Attention Models", "predicate": "uses", "to": "Deep Convolutional Neural Network", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_354", "file_type": "json", "from": "Yichong Xu", "predicate": "is_author_of", "to": "The Application of Two-level Attention Models", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_356", "file_type": "json", "from": "Yichong Xu", "predicate": "affiliated_with", "to": "Microsoft Research", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_354", "file_type": "json", "from": "Kuiyuan Yang", "predicate": "is_author_of", "to": "The Application of Two-level Attention Models", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_356", "file_type": "json", "from": "Kuiyuan Yang", "predicate": "affiliated_with", "to": "Microsoft Research", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_354", "file_type": "json", "from": "Jiaxing Zhang", "predicate": "is_author_of", "to": "The Application of Two-level Attention Models", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_356", "file_type": "json", "from": "Jiaxing Zhang", "predicate": "affiliated_with", "to": "Microsoft Research", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_354", "file_type": "json", "from": "Yuxin Peng", "predicate": "is_author_of", "to": "The Application of Two-level Attack Models", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_356", "file_type": "json", "from": "Yuxin Peng", "predicate": "affiliated_with", "to": "Institute of Computer Science and Technology", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_354", "file_type": "json", "from": "Zheng Zhang", "predicate": "is_author_of", "to": "The Application of Two-level Attention Models", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_356", "file_type": "json", "from": "Zheng Zhang", "predicate": "affiliated_with", "to": "New York University Shanghai", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_355", "file_type": "json", "from": "Fine-grained classification", "predicate": "is_challenging_due_to", "to": "subtle differences between categories", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_355", "file_type": "json", "from": "pipeline", "predicate": "integrates", "to": "bottom-up attention", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_355", "file_type": "json", "from": "pipeline", "predicate": "integrates", "to": "object-level top-down attention", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_355", "file_type": "json", "from": "pipeline", "predicate": "integrates", "to": "part-level top-down attention", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_356", "file_type": "json", "from": "Fine-grained image classification", "predicate": "relies_on", "to": "Visual attention models", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_356", "file_type": "json", "from": "Fine-grained image classification", "predicate": "uses", "to": "Deep convolutional neural networks", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_710", "file_type": "json", "from": "Deep convolutional neural networks", "predicate": "used_for", "to": "ImageNet classification", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_356", "file_type": "json", "from": "Weak supervision", "predicate": "addresses", "to": "additional annotations", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_356", "file_type": "json", "from": "Institute of Computer Science and Technology", "predicate": "located_at", "to": "Peking University", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_356", "file_type": "json", "from": "Geodesic Exponential Kernel", "predicate": "addresses", "to": "curvature and linearity conflict", "type": "factual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_356", "file_type": "json", "from": "Aasa Feragen", "predicate": "author_of", "to": "Geodesic Exponential Kernel", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_361", "file_type": "json", "from": "Aasa Feragen", "predicate": "is_affiliated_with", "to": "DIKU, University of Copenhagen", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_356", "file_type": "json", "from": "Fran\u00e7ois Lauze", "predicate": "author_of", "to": "Geodesic Exponential Kernel", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_357", "file_type": "json", "from": "Fran\u00e7ois Lauze", "predicate": "is_associated_with", "to": "Feragen", "type": "factual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_361", "file_type": "json", "from": "Fran\u00e7ois Lauze", "predicate": "is_affiliated_with", "to": "DIKU, University of Copenhagen", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_356", "file_type": "json", "from": "S\u00f8ren Hauberg", "predicate": "author_of", "to": "Geodesic Exponential Kernel", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_361", "file_type": "json", "from": "S\u00f8ren Hauberg", "predicate": "is_affiliated_with", "to": "DTU Compute", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_359", "file_type": "json", "from": "geodesic metric spaces", "predicate": "related to", "to": "geodesic Laplacian kernels", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_357", "file_type": "json", "from": "Gaussian kernel", "predicate": "is_generalized_to", "to": "positive definite kernel", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_357", "file_type": "json", "from": "positive definite kernel", "predicate": "requires", "to": "flat space", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_357", "file_type": "json", "from": "geodesic Gaussian kernel", "predicate": "is_positive_definite_if_and_only_if", "to": "Riemannian manifold is Euclidean", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_358", "file_type": "json", "from": "geodesic Laplacian kernel", "predicate": "retains", "to": "positive de\ufb01niteness", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_358", "file_type": "json", "from": "geodesic Laplacian kernel", "predicate": "is applicable to", "to": "curved spaces", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_358", "file_type": "json", "from": "geodesic Laplacian kernel", "predicate": "generalized to", "to": "spheres", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_358", "file_type": "json", "from": "geodesic Laplacian kernel", "predicate": "generalized to", "to": "hyperbolic spaces", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_358", "file_type": "json", "from": "geodesic Laplacian kernel", "predicate": "is a type of", "to": "kernel", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_359", "file_type": "json", "from": "curved spaces", "predicate": "include", "to": "spheres", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_359", "file_type": "json", "from": "curved spaces", "predicate": "include", "to": "hyperbolic spaces", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_357", "file_type": "json", "from": "spaces", "predicate": "has_property", "to": "conditionally negative de\ufb01nite distances", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_358", "file_type": "json", "from": "theoretical results", "predicate": "are verified", "to": "empirically", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_359", "file_type": "json", "from": "geodesic Laplacian kernels", "predicate": "can be generalized to", "to": "curved spaces", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_359", "file_type": "json", "from": "Gaussian kernels", "predicate": "related to", "to": "kernel methods", "type": "conceptual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_359", "file_type": "json", "from": "Laplacian kernels", "predicate": "related to", "to": "kernel methods", "type": "conceptual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_359", "file_type": "json", "from": "M. Alamgir and U. von Luxburg", "predicate": "authored", "to": "Shortest path distance in random k-nearest neighbor graphs", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_359", "file_type": "json", "from": "N. Dalal and B. Triggs", "predicate": "authored", "to": "Histograms of oriented gradients for human detection", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_359", "file_type": "json", "from": "S. Amar\u00ed and H. Nagaoka", "predicate": "authored", "to": "Methods of information geometry", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_360", "file_type": "json", "from": "Arsigny et al.", "predicate": "publishes", "to": "Fast and simple calculus on tensors", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_360", "file_type": "json", "from": "Fast and simple calculus on tensors", "predicate": "appears_in", "to": "MICCAI", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_360", "file_type": "json", "from": "Feragen et al.", "predicate": "publishes", "to": "Means in spaces of tree-like shapes", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_360", "file_type": "json", "from": "Feragen et al.", "predicate": "publishes", "to": "Scalable kernels for graphs", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_360", "file_type": "json", "from": "Means in spaces of tree-like shapes", "predicate": "appears_in", "to": "ICCV", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_360", "file_type": "json", "from": "Scalable kernels for graphs", "predicate": "appears_in", "to": "NIPS", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_360", "file_type": "json", "from": "Bekka and de la Harple", "predicate": "publishes", "to": "Kazhdan\u2019s Property (T)", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_360", "file_type": "json", "from": "Kazhdan\u2019s Property (T)", "predicate": "is_a", "to": "mathematical_property", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_361", "file_type": "json", "from": "Kazhdan\u2019s Property (T)", "predicate": "is_presented_in", "to": "New Mathematical Monographs", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_360", "file_type": "json", "from": "Bridson and Hae\ufb02iger", "predicate": "publishes", "to": "Metric spaces of non-positive curvature", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_361", "file_type": "json", "from": "Ganzhao Yuan", "predicate": "is_author_of", "to": "\u21130TV: A New Method", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_362", "file_type": "json", "from": "Ganzhao Yuan", "predicate": "is_author_of", "to": "paper", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_363", "file_type": "json", "from": "Ganzhao Yuan", "predicate": "affiliated_with", "to": "South China University of Technology (SCUT)", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_363", "file_type": "json", "from": "Ganzhao Yuan", "predicate": "email", "to": "yuan Ganzhao@gmail.com", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_361", "file_type": "json", "from": "\u21130TV: A New Method", "predicate": "addresses", "to": "Image Restoration", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_363", "file_type": "json", "from": "Image Restoration", "predicate": "uses", "to": "Total Variation", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_363", "file_type": "json", "from": "Image Restoration", "predicate": "uses", "to": "PADMM", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_361", "file_type": "json", "from": "Yuan_L0TV_A_New_2015_CVPR_paper", "predicate": "is_a", "to": "CVPR paper", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_362", "file_type": "json", "from": "image restoration", "predicate": "affected_by", "to": "Impulse Noise", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_362", "file_type": "json", "from": "\u21130TV-PADMM", "predicate": "solves", "to": "TV-based restoration problem", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_362", "file_type": "json", "from": "TV-based restoration problem", "predicate": "uses", "to": "\u21130-norm data fidelity", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_362", "file_type": "json", "from": "MPEC", "predicate": "solved_with", "to": "PADMM", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_362", "file_type": "json", "from": "PADMM", "predicate": "is_method_of", "to": "Alternating Direction Method of Multipliers", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_363", "file_type": "json", "from": "Ejaz Ahmed", "predicate": "author_of", "to": "An Improved Deep Learning Architecture", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_368", "file_type": "json", "from": "Ejaz Ahmed", "predicate": "affiliated_with", "to": "University of Maryland", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_363", "file_type": "json", "from": "An Improved Deep Learning Architecture", "predicate": "presented_at", "to": "CVPR", "type": "factual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_363", "file_type": "json", "from": "Michael Jones", "predicate": "author_of", "to": "An Improved Deep Learning Architecture", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_368", "file_type": "json", "from": "Michael Jones", "predicate": "affiliated_with", "to": "Mitsubishi Electric Research Labs", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_363", "file_type": "json", "from": "Tim K. Marks", "predicate": "author_of", "to": "An Improved Deep Learning Architecture", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_368", "file_type": "json", "from": "Tim K. Marks", "predicate": "affiliated_with", "to": "Mitsubishi Electric Research Labs", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_364", "file_type": "json", "from": "similarity value", "predicate": "indicates", "to": "same person", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_364", "file_type": "json", "from": "layer", "predicate": "computes", "to": "cross-input neighborhood differences", "type": "operational", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_364", "file_type": "json", "from": "cross-input neighborhood differences", "predicate": "captures", "to": "local relationships", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_364", "file_type": "json", "from": "local relationships", "predicate": "based_on", "to": "mid-level features", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_364", "file_type": "json", "from": "patch summary features", "predicate": "computed_by", "to": "layer", "type": "operational", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_365", "file_type": "json", "from": "layer of patch summary features", "predicate": "computes", "to": "high-level summary", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_365", "file_type": "json", "from": "CUHK03", "predicate": "is", "to": "large data set", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_365", "file_type": "json", "from": "CUHK01", "predicate": "is", "to": "medium-sized data set", "type": "factual", "width": 0.91}, {"arrows": "to", "chunk_id": "doc_0_chunk_366", "file_type": "json", "from": "VIPeR", "predicate": "is", "to": "small data set", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_365", "file_type": "json", "from": "initial training", "predicate": "improves results", "to": "fine-tuning", "type": "causal", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_366", "file_type": "json", "from": "Deep Convolutional Architecture", "predicate": "used_for", "to": "person re-identification", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_366", "file_type": "json", "from": "Similarity Metric Learning", "predicate": "used_for", "to": "person re-identification", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_366", "file_type": "json", "from": "Neighborhood Difference Layer", "predicate": "used_for", "to": "person re-identification", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_366", "file_type": "json", "from": "Metric Learning", "predicate": "used_for", "to": "person re-identification", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_398", "file_type": "json", "from": "Metric Learning", "predicate": "utilizes", "to": "Distribution Divergence", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_366", "file_type": "json", "from": "[Li, W., \u0026 Wang, X. (2013)]", "predicate": "published_in", "to": "CVPR", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_367", "file_type": "json", "from": "Li, Z.", "predicate": "authored", "to": "Learning locally-adaptive decision functions", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_672", "file_type": "json", "from": "Li, Z.", "predicate": "authored", "to": "Analyzing the harmonic structure in graph-based learning", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_367", "file_type": "json", "from": "Bazzani, L.", "predicate": "authored", "to": "Multiple-shot person re-identi\ufb01cation", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_367", "file_type": "json", "from": "Multiple-shot person re-identi\ufb01cation", "predicate": "uses", "to": "chromatic analyses", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_367", "file_type": "json", "from": "Bottou, L.", "predicate": "authored", "to": "Stochastic gradient tricks", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_367", "file_type": "json", "from": "Davis, J. V.", "predicate": "authored", "to": "Information-theoretic metric learning", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_367", "file_type": "json", "from": "Farenzena, M.", "predicate": "authored", "to": "Person re-identi\ufb01cation by symmetry-driven accumulation", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_367", "file_type": "json", "from": "Person re-identi\ufb01cation", "predicate": "relies_on", "to": "local features", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_483", "file_type": "json", "from": "local features", "predicate": "capture", "to": "local contrast", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_483", "file_type": "json", "from": "local features", "predicate": "capture", "to": "shape information", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_368", "file_type": "json", "from": "McAllester, D.", "predicate": "authored", "to": "Object detection with discriminatively trained part-based models", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_368", "file_type": "json", "from": "Vassileios Balntas", "predicate": "authored", "to": "BOLD", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_372", "file_type": "json", "from": "Vassileios Balntas", "predicate": "affiliated_with", "to": "University of Surrey, UK", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_368", "file_type": "json", "from": "BOLD", "predicate": "is_a", "to": "Binary Online Learned Descriptor", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_368", "file_type": "json", "from": "Lilian Tang", "predicate": "authored", "to": "BOLD", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_372", "file_type": "json", "from": "Lilian Tang", "predicate": "affiliated_with", "to": "University of Surrey, UK", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_368", "file_type": "json", "from": "Krystian Mikolajczyk", "predicate": "authored", "to": "BOLD", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_372", "file_type": "json", "from": "Krystian Mikolajczyk", "predicate": "affiliated_with", "to": "University of Surrey, UK", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_369", "file_type": "json", "from": "BOLD (Binary Online Learned Descriptor)", "predicate": "is_a", "to": "approach", "type": "conceptual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_369", "file_type": "json", "from": "BOLD (Binary Online Learned Descriptor)", "predicate": "optimizes", "to": "image patch", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_369", "file_type": "json", "from": "binary strings", "predicate": "represents", "to": "test results", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_369", "file_type": "json", "from": "binary strings", "predicate": "indicates", "to": "subset of robust tests", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_369", "file_type": "json", "from": "per-patch optimization", "predicate": "performs_better_than", "to": "global optimization", "type": "factual", "width": 0.87}, {"arrows": "to", "chunk_id": "doc_0_chunk_370", "file_type": "json", "from": "Masked Hamming distance", "predicate": "requires", "to": "tests", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_370", "file_type": "json", "from": "Per-patch optimization", "predicate": "benefits_over", "to": "Global optimization", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_370", "file_type": "json", "from": "D. G. Lowe", "predicate": "developed", "to": "SIFT features", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_370", "file_type": "json", "from": "Local descriptors", "predicate": "compared_in", "to": "IEEE TPAMI, 27(10):1615\u20131630, 2005", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_370", "file_type": "json", "from": "SURF descriptor", "predicate": "introduced_in", "to": "ECCV, 2006", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_371", "file_type": "json", "from": "SURF descriptor", "predicate": "introduced_by", "to": "G. H. M. Brown and S. Winder", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_370", "file_type": "json", "from": "Local image descriptors", "predicate": "explores_learning", "to": "Discriminative learning", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_370", "file_type": "json", "from": "Discriminative learning", "predicate": "described_in", "to": "IEEE TPAMI, 33(1):43\u201357, 2010", "type": "factual", "width": 0.91}, {"arrows": "to", "chunk_id": "doc_0_chunk_370", "file_type": "json", "from": "Binary descriptors", "predicate": "related_to", "to": "Image matching", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_370", "file_type": "json", "from": "Online descriptor optimization", "predicate": "improves", "to": "Image matching", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_371", "file_type": "json", "from": "G. H. M. Brown and S. Winder", "predicate": "explores", "to": "discriminative learning", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_371", "file_type": "json", "from": "discriminative learning", "predicate": "used_in", "to": "SURF descriptor", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_371", "file_type": "json", "from": "interest point detection", "predicate": "addressed_by", "to": "K. Mikolajczyk and C. Schmid", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_371", "file_type": "json", "from": "tracking applications", "predicate": "relevant_to", "to": "Struck", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_371", "file_type": "json", "from": "Tracking-learning-detection", "predicate": "discusses", "to": "integration", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_371", "file_type": "json", "from": "Tracking-learning-detection", "predicate": "integrates", "to": "tracking", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_371", "file_type": "json", "from": "keypoint recognition", "predicate": "method", "to": "random ferns", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_371", "file_type": "json", "from": "random ferns", "predicate": "enhances", "to": "keypoint recognition", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_371", "file_type": "json", "from": "M. Ozuysal", "predicate": "authored_by", "to": "random ferns", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_372", "file_type": "json", "from": "M. Ozuysal", "predicate": "authored", "to": "Fast keypoint recognition method", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_542", "file_type": "json", "from": "tracking", "predicate": "incorporates", "to": "topological constraints", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_542", "file_type": "json", "from": "tracking", "predicate": "addresses", "to": "deformable objects", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_542", "file_type": "json", "from": "tracking", "predicate": "addresses", "to": "occluded objects", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_542", "file_type": "json", "from": "tracking", "predicate": "uses", "to": "dynamic graph", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_372", "file_type": "json", "from": "Fast keypoint recognition method", "predicate": "published_in", "to": "IEEE TPAMI", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_377", "file_type": "json", "from": "IEEE TPAMI", "predicate": "published", "to": "annotation of pictures", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_372", "file_type": "json", "from": "ORB", "predicate": "is_alternative_to", "to": "SIFT", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_372", "file_type": "json", "from": "ORB", "predicate": "is_alternative_to", "to": "SURF", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_537", "file_type": "json", "from": "SURF", "predicate": "developed by", "to": "Bay, H.", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_537", "file_type": "json", "from": "SURF", "predicate": "published in", "to": "Computer Vision and Image Understanding", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_372", "file_type": "json", "from": "V. L. T. Trzcinski", "predicate": "authored", "to": "Boosting Binary Keypoint Descriptors", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_372", "file_type": "json", "from": "Jiajun Wu", "predicate": "authored", "to": "Deep Multiple Instance Learning for Image Classi\ufb01cation and Auto-Annotation", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_373", "file_type": "json", "from": "Jiajun Wu", "predicate": "is_author_of", "to": "Deep Multiple Instance Learning", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_372", "file_type": "json", "from": "Deep Multiple Instance Learning for Image Classi\ufb01cation and Auto-Annotation", "predicate": "presented_at", "to": "CVPR", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_373", "file_type": "json", "from": "Deep Multiple Instance Learning", "predicate": "is_presented_at", "to": "CVPR", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_373", "file_type": "json", "from": "Deep Multiple Instance Learning", "predicate": "is_a", "to": "Learning Algorithm", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_373", "file_type": "json", "from": "Deep Multiple Instance Learning", "predicate": "addresses", "to": "Glaucoma", "type": "factual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_373", "file_type": "json", "from": "Yinan Yu", "predicate": "is_author_of", "to": "Deep Multiple Instance Learning", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_378", "file_type": "json", "from": "Yinan Yu", "predicate": "affiliated_with", "to": "Institute of Deep Learning", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_373", "file_type": "json", "from": "Kai Yu", "predicate": "is_author_of", "to": "Deep Multiple instance Learning", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_373", "file_type": "json", "from": "Wu_Deep_Multiple_Instance_2015_CVPR_paper.pdf", "predicate": "contains", "to": "Deep Multiple Instance Learning", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_374", "file_type": "json", "from": "Deep learning", "predicate": "achieved", "to": "tremendous improvements", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_374", "file_type": "json", "from": "object proposals", "predicate": "regarded as", "to": "instance sets", "type": "conceptual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_374", "file_type": "json", "from": "text annotations", "predicate": "regarded as", "to": "instance sets", "type": "conceptual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_374", "file_type": "json", "from": "systems", "predicate": "exploits", "to": "MIL property", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_374", "file_type": "json", "from": "systems", "predicate": "uses", "to": "deep learning strategies", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_375", "file_type": "json", "from": "region-keyword pairs", "predicate": "are", "to": "reasonable", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_375", "file_type": "json", "from": "extraction", "predicate": "requires", "to": "little supervision", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_376", "file_type": "json", "from": "Region-keyword pairs", "predicate": "requires", "to": "little supervision", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_376", "file_type": "json", "from": "Multiple Instance Learning (MIL)", "predicate": "is a field of", "to": "Machine Learning", "type": "conceptual", "width": 0.78}, {"arrows": "to", "chunk_id": "doc_0_chunk_376", "file_type": "json", "from": "Andrews et al.", "predicate": "studies", "to": "Support vector machines", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_376", "file_type": "json", "from": "Li \u0026 Wang", "predicate": "studies", "to": "computerized annotation", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_642", "file_type": "json", "from": "computerized annotation", "predicate": "applied_to", "to": "pictures", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_376", "file_type": "json", "from": "Barnard et al.", "predicate": "studies", "to": "Matching words and pictures", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_376", "file_type": "json", "from": "Image Annotation", "predicate": "uses", "to": "Deep Learning", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_377", "file_type": "json", "from": "Barnard", "predicate": "authored", "to": "Matching words and pictures", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_377", "file_type": "json", "from": "Li, L.-J.", "predicate": "authored", "to": "Object bank", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_377", "file_type": "json", "from": "Chen", "predicate": "authored", "to": "Hierarchical matching", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_595", "file_type": "json", "from": "Chen", "predicate": "affiliated_with", "to": "University of California, Los Angeles", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_377", "file_type": "json", "from": "Hierarchical matching", "predicate": "published", "to": "CVPR", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_377", "file_type": "json", "from": "Li, Q.", "predicate": "authored", "to": "Harvesting mid-level visual concepts", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_377", "file_type": "json", "from": "Harvesting mid-level visual concepts", "predicate": "published", "to": "CVPR", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_377", "file_type": "json", "from": "Bing: Binarized normed gradients", "predicate": "published", "to": "CVPR", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_378", "file_type": "json", "from": "Imaginet", "predicate": "is_database_of", "to": "hierarchical image", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_643", "file_type": "json", "from": "Imaginet", "predicate": "is_database_of", "to": "hierarchical images", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_665", "file_type": "json", "from": "Imaginet", "predicate": "is_a", "to": "hierarchical image database", "type": "conceptual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_665", "file_type": "json", "from": "Imaginet", "predicate": "presented_at", "to": "Computer Vision and Pattern Recognition, 2009", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_378", "file_type": "json", "from": "Rochan", "predicate": "authored", "to": "Weakly Supervised Localization", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_378", "file_type": "json", "from": "Weakly Supervised Localization", "predicate": "presented_at", "to": "CVPR", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_378", "file_type": "json", "from": "Mrigank Rochan", "predicate": "affiliated_with", "to": "Massachusetts Institute of Technology", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_385", "file_type": "json", "from": "Mrigank Rochan", "predicate": "affiliated_with", "to": "University of Manitoba", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_385", "file_type": "json", "from": "Mrigank Rochan", "predicate": "works_in", "to": "Department of Computer Science", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_378", "file_type": "json", "from": "Chang Huang", "predicate": "affiliated_with", "to": "Institute of Deep Learning", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_379", "file_type": "json", "from": "training images", "predicate": "requires", "to": "object bounding boxes", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_379", "file_type": "json", "from": "weakly labeled data", "predicate": "is_easier_to_collect_than", "to": "training images", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_379", "file_type": "json", "from": "YouTube videos", "predicate": "provides", "to": "user-generated tags", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_379", "file_type": "json", "from": "image search", "predicate": "enables_collection_of", "to": "weakly labeled images", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_379", "file_type": "json", "from": "collection of images", "predicate": "labeled_with", "to": "object category", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_656", "file_type": "json", "from": "bounding box", "predicate": "estimated_using", "to": "local density map", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_381", "file_type": "json", "from": "image datasets", "predicate": "used_for", "to": "experimental results", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_381", "file_type": "json", "from": "video datasets", "predicate": "used_for", "to": "experimental results", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_382", "file_type": "json", "from": "Lampert et al. (2009)", "predicate": "addresses", "to": "unseen object classes", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_383", "file_type": "json", "from": "K. Grauman", "predicate": "co-authored", "to": "Object-graphs for context-aware category discovery", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_383", "file_type": "json", "from": "R. G. Cinbis", "predicate": "authored", "to": "Multi-fold mil training", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_383", "file_type": "json", "from": "J. Verbeek", "predicate": "co-authored", "to": "Multi-fold mil training", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_383", "file_type": "json", "from": "T. Mikolov", "predicate": "authored", "to": "Distributed representations of words and phrases", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_383", "file_type": "json", "from": "I. Sutskever", "predicate": "co-authored", "to": "Distributed representations of words and phrases", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_383", "file_type": "json", "from": "M. H. Nguyen", "predicate": "authored", "to": "Weakly supervised discrimiative localization", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_384", "file_type": "json", "from": "A. Papazoglou", "predicate": "authored", "to": "Fast object segmentation", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_384", "file_type": "json", "from": "Fast object segmentation", "predicate": "presented_at", "to": "IEEE International Conference on Computer Vision", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_383", "file_type": "json", "from": "V. Ferrari", "predicate": "co-authored", "to": "Fast object segmentation", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_383", "file_type": "json", "from": "Object-graphs", "predicate": "addresses", "to": "context-aware category discovery", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_383", "file_type": "json", "from": "Distributed representations", "predicate": "enables", "to": "compositionality", "type": "conceptual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_638", "file_type": "json", "from": "IEEE International Conference on Computer Vision", "predicate": "presented_at", "to": "Graph structured sparsity model", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_384", "file_type": "json", "from": "A. Prest", "predicate": "authored", "to": "Learning object class detectors", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_384", "file_type": "json", "from": "M. Rohrbach", "predicate": "authored", "to": "Evaluating knowledge transfer", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_384", "file_type": "json", "from": "M. Rohrbach", "predicate": "authored", "to": "What helps where", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_384", "file_type": "json", "from": "Evaluating knowledge transfer", "predicate": "presented_at", "to": "IEEE Conference on Computer Vision and Pattern Recognition", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_384", "file_type": "json", "from": "What helps where", "predicate": "presented_at", "to": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_385", "file_type": "json", "from": "University of Manitoba", "predicate": "has_department", "to": "Department of Computer Science", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_386", "file_type": "json", "from": "Yang Wang", "predicate": "affiliated_with", "to": "University of Manitoba", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_385", "file_type": "json", "from": "Yang Wang", "predicate": "works_in", "to": "Department of Computer Science", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_385", "file_type": "json", "from": "Mriganka Rochan", "predicate": "has_email", "to": "mrochan@cs.umanitoba.ca", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_386", "file_type": "json", "from": "rigank Rochan", "predicate": "affiliated_with", "to": "University of Manitoba", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_386", "file_type": "json", "from": "Wei Liu", "predicate": "contributor_to", "to": "Towards 3D Object Detection", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_391", "file_type": "json", "from": "Wei Liu", "predicate": "affiliated_with", "to": "Dep. of Cognitive Science", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_391", "file_type": "json", "from": "Wei Liu", "predicate": "located_in", "to": "Xiamen University", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_438", "file_type": "json", "from": "Wei Liu", "predicate": "author_of", "to": "Supervised Discrete Hashing", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_443", "file_type": "json", "from": "Wei Liu", "predicate": "affiliated_with", "to": "IBM Research", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_386", "file_type": "json", "from": "Towards 3D Object Detection", "predicate": "presented_at", "to": "CVPR", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_386", "file_type": "json", "from": "Rongrong Ji", "predicate": "contributor_to", "to": "Towards 3D Object Detection", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_391", "file_type": "json", "from": "Rongrong Ji", "predicate": "affiliated_with", "to": "Dep. of Cognitive Space", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_391", "file_type": "json", "from": "Rongrong Ji", "predicate": "located_in", "to": "Xiamen University", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_386", "file_type": "json", "from": "Shaozi Li", "predicate": "author_of", "to": "Towards 3D Object Detection", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_386", "file_type": "json", "from": "Shaozi Li", "predicate": "contributor_to", "to": "Towards 3D ObjectDetection", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_391", "file_type": "json", "from": "Shaozi Li", "predicate": "affiliated_with", "to": "Dep. of Cognitive Science", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_386", "file_type": "json", "from": "ywang@cs.umanitoba.ca", "predicate": "contact_email", "to": "Yang Wang", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_387", "file_type": "json", "from": "accurate detection algorithm", "predicate": "requires", "to": "RGB and depth modalities", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_387", "file_type": "json", "from": "RGB and depth modalities", "predicate": "are", "to": "correlated", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_387", "file_type": "json", "from": "cross-modality deep learning framework", "predicate": "utilizes", "to": "deep Boltzmann Machines", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_387", "file_type": "json", "from": "cross-modality deep learning framework", "predicate": "addresses", "to": "lack of 3D training data", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_388", "file_type": "json", "from": "labeled 2D samples", "predicate": "from", "to": "existing datasets", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_388", "file_type": "json", "from": "3D CAD models", "predicate": "are", "to": "models", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_387", "file_type": "json", "from": "RMRC dataset", "predicate": "demonstrates", "to": "effectiveness", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_388", "file_type": "json", "from": "RMRC dataset", "predicate": "is", "to": "dataset", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_387", "file_type": "json", "from": "cross-modality features", "predicate": "captured from", "to": "RGBD data", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_706", "file_type": "json", "from": "models", "predicate": "can be", "to": "compositional", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_707", "file_type": "json", "from": "models", "predicate": "optimized_with", "to": "backpropagation", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_707", "file_type": "json", "from": "models", "predicate": "learn", "to": "temporal dynamics", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_707", "file_type": "json", "from": "models", "predicate": "learn", "to": "convolutional perceptual representations", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_707", "file_type": "json", "from": "models", "predicate": "have", "to": "distinct advantages", "type": "factual", "width": 0.87}, {"arrows": "to", "chunk_id": "doc_0_chunk_389", "file_type": "json", "from": "k", "predicate": "utilizes", "to": "labeled 2D samples", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_389", "file_type": "json", "from": "k", "predicate": "utilizes", "to": "3D CAD models", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_389", "file_type": "json", "from": "k", "predicate": "addresses", "to": "lack of 3D training data", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_389", "file_type": "json", "from": "Semantic labeling", "predicate": "is_method_for", "to": "3d point clouds", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_389", "file_type": "json", "from": "Semantic labeling", "predicate": "is_described_in", "to": "Advances in Neural Information Processing Systems", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_389", "file_type": "json", "from": "Learning rich features", "predicate": "uses", "to": "RGBD images", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_389", "file_type": "json", "from": "Learning rich features", "predicate": "is_presented_in", "to": "European Conference on Computer Vision", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_390", "file_type": "json", "from": "ILSVRC2012", "predicate": "held_in", "to": "2012", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_389", "file_type": "json", "from": "Efficient 3d scene labeling", "predicate": "uses", "to": "field-s of trees", "type": "factual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_390", "file_type": "json", "from": "Efficient 3d scene labeling", "predicate": "presented_at", "to": "International Conference on Computer Vision", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_390", "file_type": "json", "from": "geNet", "predicate": "is_competition", "to": "ILSVRC2012", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_390", "file_type": "json", "from": "O. Kahler", "predicate": "authored", "to": "Efficient 3d scene labeling", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_390", "file_type": "json", "from": "N. Srivastava", "predicate": "authored", "to": "Multimodal learning", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_390", "file_type": "json", "from": "K. Lai", "predicate": "authored", "to": "Detection-based object labeling", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_390", "file_type": "json", "from": "Detection-based object labeling", "predicate": "presented_at", "to": "IEEE International Conference on Robotics and Automation", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_390", "file_type": "json", "from": "L. Bo", "predicate": "authored", "to": "Unsupervised feature learning", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_390", "file_type": "json", "from": "Unsupervised feature learning", "predicate": "focuses_on", "to": "rgb-d based object recognition", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_390", "file_type": "json", "from": "X. Xiong", "predicate": "authored", "to": "3-d scene analysis", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_390", "file_type": "json", "from": "3-d scene analysis", "predicate": "uses", "to": "sequenced predictions", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_390", "file_type": "json", "from": "A. Wang", "predicate": "authored", "to": "Multi-modal unsupervised feature learning", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_390", "file_type": "json", "from": "Multi-modal unsupervised feature learning", "predicate": "focuses_on", "to": "rgb-d scene labeling", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_391", "file_type": "json", "from": "Multi-modal unsupervised feature learning", "predicate": "presented_at", "to": "European Conference on Computer Vision", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_391", "file_type": "json", "from": "Abhishek Sharma", "predicate": "author_of", "to": "Deep Hierarchical Parsing", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_392", "file_type": "json", "from": "Abhishek Sharma", "predicate": "authored", "to": "Sharma_Deep_Hierarial_Parsing_2015_CVPR_paper", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_394", "file_type": "json", "from": "Abhishek Sharma", "predicate": "affiliated_with", "to": "Computer Science Department", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_394", "file_type": "json", "from": "Abhishek Sharma", "predicate": "email", "to": "bhokaal@cs.umd.edu", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_395", "file_type": "json", "from": "Abhishek Sharma", "predicate": "works_at", "to": "University of Maryland", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_391", "file_type": "json", "from": "Deep Hierarchical Parsing", "predicate": "presented_at", "to": "CVPR", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_391", "file_type": "json", "from": "Oncel Tuzel", "predicate": "author_of", "to": "Deep Hierarchical Parsing", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_392", "file_type": "json", "from": "Oncel Tuzel", "predicate": "authored", "to": "Sharma_Deep_Hierarial_Parsing_2015_CVPR_paper", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_395", "file_type": "json", "from": "Oncel Tuzel", "predicate": "works_at", "to": "MERL", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_391", "file_type": "json", "from": "David W. Jacobs", "predicate": "author_of", "to": "Deep Hierarchical Parsing", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_392", "file_type": "json", "from": "David W. Jacobs", "predicate": "authored", "to": "Sharma_Deep_Hierarial_Parsing_2015_CVPR_paper", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_395", "file_type": "json", "from": "David W. Jacobs", "predicate": "works_at", "to": "University of Maryland", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_395", "file_type": "json", "from": "David W. Jacobs", "predicate": "affiliated_with", "to": "Computer Science Department", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_391", "file_type": "json", "from": "Sliding shapes", "predicate": "presented_at", "to": "European Conference on Computer Vision", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_392", "file_type": "json", "from": "Sharma_Deep_Hierarial_Parsing_2015_CVPR_paper", "predicate": "proposes", "to": "improvements to RCPN", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_392", "file_type": "json", "from": "RCPN", "predicate": "is_a", "to": "deep feed-forward neural network", "type": "conceptual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_392", "file_type": "json", "from": "RCPN", "predicate": "used_for", "to": "semantic segmentation", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_392", "file_type": "json", "from": "bypass error paths", "predicate": "hinders", "to": "contextual propagation", "type": "causal", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_392", "file_type": "json", "from": "bypass error paths", "predicate": "reduces", "to": "performance", "type": "caushal", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_392", "file_type": "json", "from": "modifications", "predicate": "incorporates", "to": "classification loss", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_392", "file_type": "json", "from": "modifications", "predicate": "utilizes", "to": "tree-style MRF", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_392", "file_type": "json", "from": "modifications", "predicate": "achieves", "to": "state-of-the-art results", "type": "factual", "width": 0.91}, {"arrows": "to", "chunk_id": "doc_0_chunk_392", "file_type": "json", "from": "classification loss", "predicate": "is_part_of", "to": "random parse trees", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_392", "file_type": "json", "from": "tree-style MRF", "predicate": "models", "to": "hierarchical dependencies", "type": "factual", "width": 0.89}, {"arrows": "to", "chunk_id": "doc_0_chunk_393", "file_type": "json", "from": "Tree-Style MRF", "predicate": "models", "to": "hierarchical dependencies", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_393", "file_type": "json", "from": "Modifications", "predicate": "enhance", "to": "performance", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_393", "file_type": "json", "from": "Modifications", "predicate": "achieves", "to": "state-of-the-art results", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_393", "file_type": "json", "from": "Semantic Segmentation", "predicate": "uses", "to": "Deep Neural Networks", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_472", "file_type": "json", "from": "Semantic Segmentation", "predicate": "is_related_to", "to": "Image Alignment", "type": "conceptual", "width": 0.7}, {"arrows": "to", "chunk_id": "doc_0_chunk_570", "file_type": "json", "from": "Semantic Segmentation", "predicate": "uses", "to": "Regions", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_570", "file_type": "json", "from": "Semantic Segmentation", "predicate": "uses", "to": "Parts", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_393", "file_type": "json", "from": "Recursive Context Propagation Network (RCPN)", "predicate": "related_to", "to": "Contextual Propagation", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_393", "file_type": "json", "from": "Socher et al. (2011)", "predicate": "researches", "to": "Recursive Neural Networks", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_393", "file_type": "json", "from": "Farabet et al. (2013)", "predicate": "researches", "to": "scene labeling", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_393", "file_type": "json", "from": "Fergus and Eigen (2012)", "predicate": "researches", "to": "image parsing", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_393", "file_type": "json", "from": "Markov Random Fields (MRF)", "predicate": "used_in", "to": "Semantic Segmentation", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_394", "file_type": "json", "from": "Najman", "predicate": "authored", "to": "Learning hierarchical features for scene labeling", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_394", "file_type": "json", "from": "Learning hierarchical features for scene labeling", "predicate": "presented_at", "to": "IEEE TPAM", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_394", "file_type": "json", "from": "R. Fergus", "predicate": "authored", "to": "Nonparametric image parsing", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_394", "file_type": "json", "from": "Nonparametric image parsing", "predicate": "presented_at", "to": "IEEE CVPR", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_394", "file_type": "json", "from": "A. Torralba", "predicate": "authored", "to": "Context-based vision system", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_394", "file_type": "json", "from": "Context-based vision system", "predicate": "presented_at", "to": "IEEE CVPR", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_394", "file_type": "json", "from": "P. H. O. Pinheiro", "predicate": "authored", "to": "Recurrent convolutional neural networks", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_394", "file_type": "json", "from": "Recurrent convolutional neural networks", "predicate": "presented_at", "to": "ICML", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_394", "file_type": "json", "from": "A. Sharma", "predicate": "authored", "to": "Recursive context propagation network", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_394", "file_type": "json", "from": "Recursive context propagation network", "predicate": "presented_at", "to": "NIPS", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_394", "file_type": "json", "from": "J. Tighe", "predicate": "authored", "to": "Finding things", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_394", "file_type": "json", "from": "J. Tighe", "predicate": "authored", "to": "Superparsing", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_394", "file_type": "json", "from": "Finding things", "predicate": "presented_at", "to": "IEEE CVPR", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_394", "file_type": "json", "from": "R. Mottaghi", "predicate": "authored", "to": "Analyzing semantic segmentation", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_394", "file_type": "json", "from": "Analyzing semantic segmentation", "predicate": "presented_at", "to": "IEEE CVPR", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_394", "file_type": "json", "from": "Superparsing", "predicate": "published_in", "to": "Int. J. Comput. Vision", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_395", "file_type": "json", "from": "IEEE CVPR", "predicate": "is_conference_of", "to": "Computer Vision", "type": "conceptual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_633", "file_type": "json", "from": "IEEE CVPR", "predicate": "publishes", "to": "Bell et al.", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_513", "file_type": "json", "from": "ICML", "predicate": "is_publication_venue", "to": "research paper", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_395", "file_type": "json", "from": "Junlin Hu", "predicate": "is_author_of", "to": "Deep Transfer Metric Learning", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_395", "file_type": "json", "from": "Junlin Hu", "predicate": "affiliated_with", "to": "Computer Science Department", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_400", "file_type": "json", "from": "Junlin Hu", "predicate": "affiliated_with", "to": "School of Electrical and Electronic Engineering", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_400", "file_type": "json", "from": "Junlin Hu", "predicate": "email", "to": "jhu007@e.ntu.edu.sg", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_395", "file_type": "json", "from": "Deep Transfer Metric Learning", "predicate": "is_paper_in", "to": "IEEE CVPR", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_395", "file_type": "json", "from": "Jiwen Lu", "predicate": "is_author_of", "to": "Deep Transfer Metric Learning", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_514", "file_type": "json", "from": "Jiwen Lu", "predicate": "affiliated_with", "to": "Advanced Digital Sciences Center", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_401", "file_type": "json", "from": "Jiwen Lu", "predicate": "affiliated_with", "to": "Nanyang Technological University", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_401", "file_type": "json", "from": "Jiwen Lu", "predicate": "email", "to": "jiwen.lu@adsc.com.sg", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_396", "file_type": "json", "from": "metric learning methods", "predicate": "typically_assume", "to": "similar scenarios", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_396", "file_type": "json", "from": "real-world visual recognition applications", "predicate": "often_unmet", "to": "similar scenarios", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_396", "file_type": "json", "from": "DTML method", "predicate": "addresses", "to": "challenge", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_396", "file_type": "json", "from": "DTML method", "predicate": "transfers", "to": "discriminative knowledge", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_396", "file_type": "json", "from": "DTML method", "predicate": "maximizes", "to": "inter-class variations", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_396", "file_type": "json", "from": "DTML method", "predicate": "minimizes", "to": "intra-class variations", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_396", "file_type": "json", "from": "DTML method", "predicate": "minimizes", "to": "distribution divergence", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_397", "file_type": "json", "from": "DSTML method", "predicate": "optimizes", "to": "outputs of hidden and top layers", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_396", "file_type": "json", "from": "DSTML method", "predicate": "is_a", "to": "deeply supervised transfer metric learning", "type": "conceptual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_397", "file_type": "json", "from": "DSTML method", "predicate": "is", "to": "transfer metric learning method", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_397", "file_type": "json", "from": "DSTML method", "predicate": "developed_for", "to": "cross-dataset tasks", "type": "factual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_397", "file_type": "json", "from": "vergence", "predicate": "exists_between", "to": "source and target domains", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_397", "file_type": "json", "from": "hidden layers", "predicate": "part_of", "to": "neural network", "type": "conceptual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_397", "file_type": "json", "from": "top layers", "predicate": "part_of", "to": "neural network", "type": "conceptual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_398", "file_type": "json", "from": "Deep Transfer Metric Learning (DTML)", "predicate": "is_subfield_of", "to": "Metric Learning", "type": "conceptual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_398", "file_type": "json", "from": "Deep Transfer Metric Learning (DTML)", "predicate": "addresses", "to": "Cross-Domain Visual Recognition", "type": "conceptual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_398", "file_type": "json", "from": "Deep Transfer Metric Learning (DTML)", "predicate": "employs", "to": "Deep Neural Networks", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_398", "file_type": "json", "from": "Cross-Domain Visual Recognition", "predicate": "benefits_from", "to": "Deep Neural Networks", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_398", "file_type": "json", "from": "Face Description", "predicate": "uses", "to": "Local Binary Patterns", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_398", "file_type": "json", "from": "Learning Deep Architectures", "predicate": "contributes_to", "to": "AI", "type": "conceptual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_398", "file_type": "json", "from": "Predictive Structures", "predicate": "learned_from", "to": "Multiple Tasks", "type": "factual", "width": 0.86}, {"arrows": "to", "chunk_id": "doc_0_chunk_399", "file_type": "json", "from": "Journal of Machine Learning Research", "predicate": "is_publication_of", "to": "Machine Learning", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_399", "file_type": "json", "from": "Foundations and Trends in Machine Learning", "predicate": "is_publication_of", "to": "Machine Learning", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_399", "file_type": "json", "from": "ACM Multimedia", "predicate": "is_publication_of", "to": "ACM", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_627", "file_type": "json", "from": "ACM Multimedia", "predicate": "publishes", "to": "Graph-based search methods", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_399", "file_type": "json", "from": "Chen, D.", "predicate": "authored", "to": "Bayesian face revisited", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_588", "file_type": "json", "from": "Bayesian face revisited", "predicate": "provides", "to": "joint formulation", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_399", "file_type": "json", "from": "Duan, L.", "predicate": "authored", "to": "Domain transfer SVM", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_399", "file_type": "json", "from": "Conference on Computer Vision and Pattern Recognition", "predicate": "is_conference_of", "to": "Computer Vision", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_400", "file_type": "json", "from": "Gray \u0026 Tao (2008)", "predicate": "presented_at", "to": "European Conference on Computer Vision", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_400", "file_type": "json", "from": "Gretton et al. (2006)", "predicate": "presented_at", "to": "Neural Information Processing Systems", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_407", "file_type": "json", "from": "Neural Information Processing Systems", "predicate": "hosts", "to": "Imaginet classification with deep convolutional neural networks", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_400", "file_type": "json", "from": "Hinton et al. (2006)", "predicate": "presented_at", "to": "Neural Computation", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_400", "file_type": "json", "from": "Huang et al. (2012)", "predicate": "presented_at", "to": "Conference on Computer Vision and Pattern Recognition", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_400", "file_type": "json", "from": "Nanyang Technological University", "predicate": "located_in", "to": "Singapore", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_401", "file_type": "json", "from": "Yap-Peng Tan", "predicate": "email", "to": "eyptan@ntu.edu.sg", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_401", "file_type": "json", "from": "Takuya Narihira", "predicate": "author_of", "to": "Learning Lightness from Human Judgement", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_401", "file_type": "json", "from": "Learning Lightness from Human Judgement", "predicate": "presented_at", "to": "CVPR", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_401", "file_type": "json", "from": "Michael Maire", "predicate": "author_of", "to": "Learning Lightness from Human Judgement", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_408", "file_type": "json", "from": "Michael Maire", "predicate": "affiliated_with", "to": "TTI Chicago", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_401", "file_type": "json", "from": "Stella X. Yu", "predicate": "author_of", "to": "Learning Lightness from Human Judgement", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_408", "file_type": "json", "from": "Stella X. Yu", "predicate": "affiliated_with", "to": "UC Berkeley", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_408", "file_type": "json", "from": "Stella X. Yu", "predicate": "affiliated_with", "to": "ICSI", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_401", "file_type": "json", "from": "Narihira_Learning_Lightness_From_2015_CVPR_paper.pdf", "predicate": "contains", "to": "Learning Lightness from Human Judgement", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_402", "file_type": "json", "from": "Narihira_Learning_Lightness_From_2015_CVPR_paper", "predicate": "addresses", "to": "inferring lightness", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_402", "file_type": "json", "from": "inferring lightness", "predicate": "is_problem_of", "to": "perceived re\ufb02ectance", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_402", "file_type": "json", "from": "classic methods", "predicate": "view problem_as", "to": "intrinsic image decomposition", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_402", "file_type": "json", "from": "intrinsic image decomposition", "predicate": "separates", "to": "re\ufb02ectance and shading components", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_404", "file_type": "json", "from": "intrinsic image decomposition", "predicate": "relies_on", "to": "lightness perception", "type": "conceptual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_402", "file_type": "json", "from": "patch representations", "predicate": "are_built_using", "to": "deep networks", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_483", "file_type": "json", "from": "deep networks", "predicate": "exploits", "to": "global saliency cues", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_402", "file_type": "json", "from": "local lightness model", "predicate": "achieves_performance", "to": "on-par with global lightness model", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_403", "file_type": "json", "from": "local lightness model", "predicate": "achieves_performance", "to": "on-par performance", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_403", "file_type": "json", "from": "local lightness model", "predicate": "competes_with", "to": "state-of-the-art global lightness model", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_403", "file_type": "json", "from": "local lightness model", "predicate": "is_dataset", "to": "ld dataset", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_404", "file_type": "json", "from": "global lightness model", "predicate": "incorporates", "to": "shading/re\ufb02ectance priors", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_404", "file_type": "json", "from": "global lightness model", "predicate": "uses", "to": "dense conditional random field formulation", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_403", "file_type": "json", "from": "state-of-the-art global lightness model", "predicate": "incorporates", "to": "shading/reflectance priors", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_403", "file_type": "json", "from": "state-of-the-art global lightness model", "predicate": "uses", "to": "dense conditional random field formulation", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_403", "file_type": "json", "from": "state-of-the-art global lightness model", "predicate": "incorporates", "to": "multiple priors", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_403", "file_type": "json", "from": "dense conditional random field formulation", "predicate": "enables", "to": "simultaneous reasoning", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_403", "file_type": "json", "from": "simultaneous reasoning", "predicate": "occurs_between", "to": "pairs of pixels", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_404", "file_type": "json", "from": "lightness perception", "predicate": "is_concept_in", "to": "cognitive neurosciences", "type": "conceptual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_404", "file_type": "json", "from": "lightness perception", "predicate": "is_foundation_for", "to": "intrinsic image decomposition", "type": "conceptual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_404", "file_type": "json", "from": "Lightness perception and lightness illusions", "predicate": "provides", "to": "foundational work", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_417", "file_type": "json", "from": "foundational work", "predicate": "related_to", "to": "feature learning", "type": "conceptual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_404", "file_type": "json", "from": "H. G. Barrow", "predicate": "authored", "to": "Recovering intrinsic scene characteristics from images", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_404", "file_type": "json", "from": "Recovering intrinsic scene characteristics from images", "predicate": "lays_groundwork_for", "to": "intrinsic image algorithms", "type": "conceptual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_525", "file_type": "json", "from": "intrinsic image algorithms", "predicate": "evaluated_using", "to": "baseline evaluations", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_404", "file_type": "json", "from": "relative reflectance", "predicate": "is_concept_in", "to": "intrinsic image decomposition", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_404", "file_type": "json", "from": "human judgment data", "predicate": "is_related_to", "to": "lightness perception", "type": "conceptual", "width": 0.7}, {"arrows": "to", "chunk_id": "doc_0_chunk_405", "file_type": "json", "from": "Computer Vision Systems", "predicate": "publishes", "to": "scene characteristics from images", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_405", "file_type": "json", "from": "scene characteristics from images", "predicate": "lays groundwork for", "to": "intrinsic image algorithms", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_405", "file_type": "json", "from": "Retinex theory", "predicate": "introduced by", "to": "E. H. Land and J. J. McCann", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_405", "file_type": "json", "from": "Retinex theory", "predicate": "explains", "to": "brightness perception", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_405", "file_type": "json", "from": "Retinex theory", "predicate": "relevant to", "to": "intrinsic image recovery", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_405", "file_type": "json", "from": "determining lightness from an image", "predicate": "is part of", "to": "intrinsic image decomposition", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_405", "file_type": "json", "from": "M. Tappen, W. Freeman, and E. Adelson", "predicate": "addresses", "to": "intrinsic image recovery", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_405", "file_type": "json", "from": "intrinsic image recovery", "predicate": "uses", "to": "single image", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_406", "file_type": "json", "from": "intrinsic image recovery", "predicate": "addressed_in", "to": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 2005", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_406", "file_type": "json", "from": "R. Grosse", "predicate": "authored", "to": "Ground truth dataset", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_526", "file_type": "json", "from": "Ground truth dataset", "predicate": "evaluated_for", "to": "intrinsic image algorithms", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_406", "file_type": "json", "from": "International Conference on Computer Vision, 2009", "predicate": "hosts", "to": "Ground truth dataset", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_406", "file_type": "json", "from": "Y. Tang", "predicate": "authored", "to": "Deep Lambertian networks", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_406", "file_type": "json", "from": "Deep Lambertian networks", "predicate": "employs", "to": "Lambertian reflectance models", "type": "conceptual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_406", "file_type": "json", "from": "Deep Lambertian networks", "predicate": "aims_to", "to": "intrinsic image decomposition", "type": "conceptual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_406", "file_type": "json", "from": "Lambertian reflectance models", "predicate": "used_in", "to": "Deep Lambertian networks", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_407", "file_type": "json", "from": "Lambertian reflectance models", "predicate": "used in", "to": "deep learning approaches", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_406", "file_type": "json", "from": "International Conference on Machine Learning, 2012", "predicate": "hosts", "to": "Deep Lambertian networks", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_407", "file_type": "json", "from": "Lambertian networks", "predicate": "explores", "to": "deep learning approaches", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_407", "file_type": "json", "from": "deep learning approaches", "predicate": "used for", "to": "intrinsic image decomposition", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_408", "file_type": "json", "from": "Takaya Narihira", "predicate": "affiliated_with", "to": "UC Berkeley", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_408", "file_type": "json", "from": "Takaya Narihira", "predicate": "affiliated_with", "to": "ICSI", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_408", "file_type": "json", "from": "Takaya Narihira", "predicate": "affiliated_with", "to": "Sony Corp.", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_712", "file_type": "json", "from": "ICSI", "predicate": "is_part_of", "to": "UC Berkeley", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_408", "file_type": "json", "from": "Hierarchical-PEP Model", "predicate": "addresses", "to": "face recognition", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_447", "file_type": "json", "from": "face recognition", "predicate": "requires", "to": "Labelled faces in the wild", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_409", "file_type": "json", "from": "Hierarchical-PEP model", "predicate": "addresses", "to": "Pose variation", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_409", "file_type": "json", "from": "Hierarchical-PEP model", "predicate": "is_inspired_by", "to": "Probabilistic Elastic Part (PEP) model", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_409", "file_type": "json", "from": "Hierarchical-PEP model", "predicate": "is_inspired_by", "to": "Deep Hierarchical Architectures", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_409", "file_type": "json", "from": "Hierarchical-PEP model", "predicate": "exploits", "to": "Fine-grained Structures", "type": "factual", "width": 0.91}, {"arrows": "to", "chunk_id": "doc_0_chunk_409", "file_type": "json", "from": "Hierarchical-PEP model", "predicate": "is_guided_by", "to": "Supervised Information", "type": "factual", "width": 0.89}, {"arrows": "to", "chunk_id": "doc_0_chunk_409", "file_type": "json", "from": "Hierarchical-PEP model", "predicate": "is_verified_on", "to": "LFW", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_409", "file_type": "json", "from": "Hierarchical-PEP model", "predicate": "is_verified_on", "to": "YouTube Faces", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_409", "file_type": "json", "from": "Hierarchical-PEP model", "predicate": "is_verified_on", "to": "PaSC", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_409", "file_type": "json", "from": "Face Image", "predicate": "is_decomposed_into", "to": "Face Parts", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_409", "file_type": "json", "from": "Face Parts", "predicate": "has_level", "to": "Detail Level", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_409", "file_type": "json", "from": "Face Part Representations", "predicate": "is_stacked_at", "to": "Layer", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_409", "file_type": "json", "from": "Layer", "predicate": "reduces", "to": "Dimensionality", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_409", "file_type": "json", "from": "Face Representation", "predicate": "is", "to": "Invariant", "type": "conceptual", "width": 0.87}, {"arrows": "to", "chunk_id": "doc_0_chunk_586", "file_type": "json", "from": "LFW", "predicate": "is_a", "to": "dataset", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_585", "file_type": "json", "from": "LFW", "predicate": "is_used_in", "to": "experiments", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_410", "file_type": "json", "from": "face parts", "predicate": "is_analyzed_by", "to": "supervised information", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_410", "file_type": "json", "from": "face recognition challenge", "predicate": "is", "to": "PaSC", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_411", "file_type": "json", "from": "PEP (Probabilistic Elastic Part) Model", "predicate": "is_guided_by", "to": "supervised information", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_411", "file_type": "json", "from": "Ahonen, T.", "predicate": "authored", "to": "Face recognition with local binary patterns", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_411", "file_type": "json", "from": "Face recognition with local binary patterns", "predicate": "presented_in", "to": "European Conference on Computer Vision", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_411", "file_type": "json", "from": "Grauman, K.", "predicate": "authored", "to": "The pyramid match kernel", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_411", "file_type": "json", "from": "The pyramid match kernel", "predicate": "presented_in", "to": "IEEE International Conference on Computer Vision", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_411", "file_type": "json", "from": "Hu, J.", "predicate": "authored", "to": "Discriminative deep metric learning", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_413", "file_type": "json", "from": "Hu, J.", "predicate": "authored", "to": "Large margin multi-metric learning", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_413", "file_type": "json", "from": "Hu, J.", "predicate": "affiliated_with", "to": "Stevens Institute of Technology", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_412", "file_type": "json", "from": "Discriminative deep metric learning", "predicate": "used_for", "to": "face verification", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_447", "file_type": "json", "from": "Eigenfaces", "predicate": "compared_with", "to": "Fisherfaces", "type": "conceptual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_412", "file_type": "json", "from": "Fisherfaces", "predicate": "is_a", "to": "linear projection", "type": "conceptual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_412", "file_type": "json", "from": "Unsupervised joint alignment", "predicate": "applied_to", "to": "complex images", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_412", "file_type": "json", "from": "Labeled faces in the wild", "predicate": "provides", "to": "reporting procedures", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_412", "file_type": "json", "from": "Large margin multi-metric learning", "predicate": "used_for", "to": "face verification", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_412", "file_type": "json", "from": "Large margin multi-metric learning", "predicate": "used_for", "to": "kinship verification", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_412", "file_type": "json", "from": "Asian Conference on Computer Vision (ACCV)", "predicate": "is_a", "to": "conference", "type": "conceptual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_412", "file_type": "json", "from": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition", "predicate": "is_a", "to": "publication", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_413", "file_type": "json", "from": "Lei, Z.", "predicate": "authored", "to": "discriminant face descriptor", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_413", "file_type": "json", "from": "Simonyan, K.", "predicate": "authored", "to": "Deep fisher networks", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_572", "file_type": "json", "from": "Simonyan, K.", "predicate": "authored", "to": "Very deep convolutional networks", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_414", "file_type": "json", "from": "Yu Kong", "predicate": "contributed_to", "to": "Bilinear Heterogeneous Information Machine", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_419", "file_type": "json", "from": "Yu Kong", "predicate": "is_author_of", "to": "low-rank bilinear classification", "type": "factual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_414", "file_type": "json", "from": "Bilinear Heterogeneous Information Machine", "predicate": "is_paper_title", "to": "Kong_Bilinear_Heterogeneous_Information_2015_CVPR_paper.pdf", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_414", "file_type": "json", "from": "Bilinear Heterogeneous Information Machine", "predicate": "addresses_problem", "to": "RGB-D Action Recognition", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_414", "file_type": "json", "from": "Yun Fu", "predicate": "contributed_to", "to": "Bilinear Heterogeneous Information Machine", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_420", "file_type": "json", "from": "Yun Fu", "predicate": "affiliated_with", "to": "Northeastern University", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_420", "file_type": "json", "from": "Yun Fu", "predicate": "email", "to": "yunfu@ece.neu.edu", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_414", "file_type": "json", "from": "Kong_Bilinear_Heterogeneous_Information_2015_CVPR_paper.pdf", "predicate": "is_published_in", "to": "CVPR", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_415", "file_type": "json", "from": "space", "predicate": "is", "to": "learned", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_415", "file_type": "json", "from": "knowledge", "predicate": "is", "to": "shared", "type": "factual", "width": 0.87}, {"arrows": "to", "chunk_id": "doc_0_chunk_416", "file_type": "json", "from": "RGB-D action datasets", "predicate": "are", "to": "public datasets", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_416", "file_type": "json", "from": "low-rank classifier", "predicate": "improves", "to": "generalization power", "type": "causual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_416", "file_type": "json", "from": "low-rank classifier", "predicate": "minimizes", "to": "eter", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_416", "file_type": "json", "from": "promising results", "predicate": "when", "to": "RGB data are missing", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_416", "file_type": "json", "from": "promising results", "predicate": "when", "to": "depth data are missing", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_417", "file_type": "json", "from": "Argyriou et al. (2008)", "predicate": "provides", "to": "foundational work", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_417", "file_type": "json", "from": "feature learning", "predicate": "relevant_to", "to": "Action Recognition", "type": "conceptual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_417", "file_type": "json", "from": "Bo et al. (2011)", "predicate": "deals_with", "to": "object recognition", "type": "factual", "width": 0.91}, {"arrows": "to", "chunk_id": "doc_0_chunk_417", "file_type": "json", "from": "Bo et al. (2011)", "predicate": "uses", "to": "kernel descriptors", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_417", "file_type": "json", "from": "Do and Artieres (2009)", "predicate": "introduces", "to": "training method", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_417", "file_type": "json", "from": "HMMs", "predicate": "often_used_in", "to": "Action Recognition", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_418", "file_type": "json", "from": "Artieres et al.", "predicate": "presents", "to": "training method", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_418", "file_type": "json", "from": "Hidden Markov Models", "predicate": "used_in", "to": "action recognition", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_418", "file_type": "json", "from": "Had\ufb01eld and Bowden", "predicate": "addresses", "to": "action recognition", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_418", "file_type": "json", "from": "Ji et al.", "predicate": "presents", "to": "3D Convolutional Neural Networks", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_418", "file_type": "json", "from": "3D Convolutional Neural Networks", "predicate": "dominant_in", "to": "action recognition", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_418", "file_type": "json", "from": "Kobayashi", "predicate": "introduces", "to": "low-rank bilinear classification", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_419", "file_type": "json", "from": "low-rank bilinear classification", "predicate": "is_method_for", "to": "modeling complex interactions", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_419", "file_type": "json", "from": "spatio-temporal depth cuboid similarity feature", "predicate": "is_feature_representation_for", "to": "activity recognition", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_419", "file_type": "json", "from": "activity recognition", "predicate": "requires", "to": "depth data", "type": "factual", "width": 0.82}, {"arrows": "to", "chunk_id": "doc_0_chunk_419", "file_type": "json", "from": "information bottleneck method", "predicate": "is_applicable_to", "to": "feature selection", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_419", "file_type": "json", "from": "information bottleneck method", "predicate": "is_applicable_to", "to": "representation learning", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_419", "file_type": "json", "from": "depth camera", "predicate": "used_for", "to": "activity recognition", "type": "factual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_419", "file_type": "json", "from": "depth sequences", "predicate": "used_for", "to": "action recognition", "type": "factual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_419", "file_type": "json", "from": "HON4D", "predicate": "is_based_on", "to": "oriented 4D normals", "type": "conceptual", "width": 0.7}, {"arrows": "to", "chunk_id": "doc_0_chunk_420", "file_type": "json", "from": "information bottlenecks", "predicate": "is_concept", "to": "feature selection", "type": "conceptual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_420", "file_type": "json", "from": "information bottlenecks", "predicate": "is_concept", "to": "representation learning", "type": "conceptual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_420", "file_type": "json", "from": "Sebastian Haner", "predicate": "authored", "to": "Absolute Pose for Cameras", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_425", "file_type": "json", "from": "Sebastian Haner", "predicate": "affiliated_with", "to": "Centre for Mathematical Sciences", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_425", "file_type": "json", "from": "Sebastian Haner", "predicate": "affiliated_with", "to": "Lund University", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_426", "file_type": "json", "from": "Sebastian Haner", "predicate": "affiliated_with", "to": "Centre for Mathematical Sciences, Lund University", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_426", "file_type": "json", "from": "Sebastian Haner", "predicate": "has_email", "to": "haner@maths.lth.se", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_420", "file_type": "json", "from": "Absolute Pose for Cameras", "predicate": "presented_at", "to": "CVPR", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_420", "file_type": "json", "from": "Absolute Pose for Cameras", "predicate": "addresses", "to": "camera pose estimation", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_420", "file_type": "json", "from": "Absolute Pose for Cameras", "predicate": "involves", "to": "refractive interfaces", "type": "conceptual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_420", "file_type": "json", "from": "Kalle \u02daAstr\u00a8om", "predicate": "authored", "to": "Absolute Pose for Cameras", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_425", "file_type": "json", "from": "Kalle \u02daAstr\u00a8om", "predicate": "affiliated_with", "to": "Workshop on Omnidirectional Vision", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_426", "file_type": "json", "from": "Kalle \u02daAstr\u00a8om", "predicate": "affiliated_with", "to": "Centre for Mathematical Sciences, Lund University", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_426", "file_type": "json", "from": "Kalle \u02daAstr\u00a8om", "predicate": "has_email", "to": "kalle@maths.lth.se", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_420", "file_type": "json", "from": "Yukong", "predicate": "email", "to": "yukong@ece.neu.edu", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_421", "file_type": "json", "from": "perspective camera", "predicate": "observes", "to": "scene", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_493", "file_type": "json", "from": "scene", "predicate": "has_property", "to": "rigidity", "type": "conceptual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_421", "file_type": "json", "from": "refractive plane", "predicate": "is_boundary_between", "to": "transparent media", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_421", "file_type": "json", "from": "solvers", "predicate": "developed_for", "to": "2D cases", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_421", "file_type": "json", "from": "solvers", "predicate": "are", "to": "minimal", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_421", "file_type": "json", "from": "solvers", "predicate": "evaluated_on", "to": "synthetic data", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_421", "file_type": "json", "from": "solvers", "predicate": "evaluated_on", "to": "real data", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_421", "file_type": "json", "from": "Snell\u2019s law", "predicate": "gives_rise_to", "to": "false solutions", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_421", "file_type": "json", "from": "false solutions", "predicate": "increases", "to": "complexity of problem", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_421", "file_type": "json", "from": "pose estimates", "predicate": "requires", "to": "explicitly modelling refraction", "type": "causal", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_422", "file_type": "json", "from": "Absolute Pose Estimation", "predicate": "requires", "to": "Refractive Interfaces Modelling", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_422", "file_type": "json", "from": "Refractive Interfaces", "predicate": "governed_by", "to": "Snell\u0027s Law", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_422", "file_type": "json", "from": "Structure-and-Motion", "predicate": "uses", "to": "Camera Calibration", "type": "conceptual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_422", "file_type": "json", "from": "Structure-and-Motion", "predicate": "benefits_from", "to": "Polynomial Equation Solving", "type": "factual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_422", "file_type": "json", "from": "Agrawal et al. (2012)", "predicate": "introduces", "to": "Multi-layer Flat Refractive Geometry Theory", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_422", "file_type": "json", "from": "Byr\u00a8od et al. (2009)", "predicate": "focuses_on", "to": "Polynomial Equation Solving", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_422", "file_type": "json", "from": "Polynomial Equation Solving", "predicate": "applied_to", "to": "Computer Vision", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_422", "file_type": "json", "from": "Multi-layer Flat Refractive Geometry", "predicate": "improves", "to": "Pose Estimation Accuracy", "type": "causal", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_423", "file_type": "json", "from": "polynomial equation solving", "predicate": "applied_to", "to": "computer vision", "type": "conceptual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_423", "file_type": "json", "from": "Ideals, Varieties, and Algorithms", "predicate": "focuses_on", "to": "computational algebraic geometry", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_423", "file_type": "json", "from": "Ideals, Varieties, and Algorithms", "predicate": "focuses_on", "to": "commutative algebra", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_423", "file_type": "json", "from": "\u02daAstr\u00a8om, Kuang, \u0026 Ask", "predicate": "researched", "to": "polynomial equation solving optimization", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_423", "file_type": "json", "from": "polynomial equation solving optimization", "predicate": "related_to", "to": "p-fold symmetries", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_423", "file_type": "json", "from": "Chari \u0026 Sturm", "predicate": "researched", "to": "multi-view geometry", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_423", "file_type": "json", "from": "multi-view geometry", "predicate": "concerns", "to": "refractive plane", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_424", "file_type": "json", "from": "Chari, V.", "predicate": "authored", "to": "Multi-view geometry of the refractive plane", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_424", "file_type": "json", "from": "Multi-view geometry of the refractive plane", "predicate": "cited_by", "to": "British Machine Vision Conference", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_424", "file_type": "json", "from": "Sturm, P. F.", "predicate": "co-authored", "to": "Multi-view geometry of the refractive plane", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_424", "file_type": "json", "from": "Fitzgibbon, A. W.", "predicate": "authored", "to": "Simultaneous linear estimation", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_424", "file_type": "json", "from": "Fitzgibbon, A. W.", "predicate": "presented_at", "to": "Conference on Computer Vision and Pattern Recognition", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_424", "file_type": "json", "from": "Simultaneous linear estimation", "predicate": "relevant_to", "to": "geometric estimation", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_424", "file_type": "json", "from": "Simultaneous linear estimation", "predicate": "addresses", "to": "lens distortion", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_424", "file_type": "json", "from": "Kuang, Y.", "predicate": "co-authored", "to": "Numerically stable optimization", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_424", "file_type": "json", "from": "Numerically stable optimization", "predicate": "focuses_on", "to": "polynomial solvers", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_425", "file_type": "json", "from": "chmid", "predicate": "edited", "to": "European Conference on ComputerVision", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_425", "file_type": "json", "from": "European Conference on ComputerVision", "predicate": "is_volume_of", "to": "Lecture Notes in Computer Science", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_425", "file_type": "json", "from": "European Conference on ComputerVision", "predicate": "focuses_on", "to": "polynomial solver optimization", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_433", "file_type": "json", "from": "Lecture Notes in Computer Science", "predicate": "publishes", "to": "Computer Vision - ECCV 2008", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_434", "file_type": "json", "from": "Lecture Notes in Computer Science", "predicate": "has_volume", "to": "5304", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_425", "file_type": "json", "from": "Nist\u00e9r", "predicate": "addresses", "to": "generalized 3-point pose problem", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_425", "file_type": "json", "from": "Stew\u00e9nius", "predicate": "addresses", "to": "generalized relative pose problems", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_425", "file_type": "json", "from": "Stew\u00e9nius", "predicate": "related_to", "to": "pose estimation", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_425", "file_type": "json", "from": "Kukelova", "predicate": "focuses_on", "to": "polynomial eigenvalue solutions", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_425", "file_type": "json", "from": "Kukelova", "predicate": "focuses_on", "to": "minimal problems", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_426", "file_type": "json", "from": "R6P", "predicate": "is_topic_of", "to": "Albl_R6P_-_Rolling_2015_CVPR_paper", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_426", "file_type": "json", "from": "Albl_R6P_-_Rolling_2015_CVPR_paper", "predicate": "published_in", "to": "CVPR", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_426", "file_type": "json", "from": "Albl_R6P_-_Rolling_2015_CVPR_paper", "predicate": "focuses_on", "to": "polynomial solutions", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_426", "file_type": "json", "from": "Cenek Albl", "predicate": "is_author_of", "to": "Albl_R6P_-_Rolling_2015_CVPR_paper", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_434", "file_type": "json", "from": "Cenek Albl", "predicate": "affiliated_with", "to": "Czech Technical University in Prague", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_426", "file_type": "json", "from": "Zuana Kukelova", "predicate": "is_author_of", "to": "Albi_R6P_-_Rolling_2015_CVPR_paper", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_434", "file_type": "json", "from": "Zuana Kukelova", "predicate": "affiliated_with", "to": "Microsoft Research Ltd", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_426", "file_type": "json", "from": "Tomas Pajdla", "predicate": "is_author_of", "to": "Albl_R6P_-_Rolling_2015_CVPR_paper", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_434", "file_type": "json", "from": "Tomas Pajdla", "predicate": "affiliated_with", "to": "Czech Technical University in Prague", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_427", "file_type": "json", "from": "absolute pose problem", "predicate": "is_problem_in", "to": "computer vision", "type": "conceptual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_427", "file_type": "json", "from": "rolling shutter", "predicate": "present_in", "to": "digital cameras", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_427", "file_type": "json", "from": "camera model", "predicate": "is_type_of", "to": "model", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_427", "file_type": "json", "from": "camera model", "predicate": "is_verified_for", "to": "polynomial solver", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_427", "file_type": "json", "from": "camera orientation", "predicate": "is_approximated_by", "to": "linear approximation", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_428", "file_type": "json", "from": "camera orientation", "predicate": "has_error", "to": "6 degrees", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_427", "file_type": "json", "from": "linear approximation", "predicate": "is_valid_around", "to": "identity rotation", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_428", "file_type": "json", "from": "P3P algorithm", "predicate": "estimates", "to": "camera orientation", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_427", "file_type": "json", "from": "P3P algorithm", "predicate": "can_be_used_to", "to": "estimate camera orientation", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_428", "file_type": "json", "from": "P3P algorithm", "predicate": "brings", "to": "camera rotation matrix", "type": "factual", "width": 0.91}, {"arrows": "to", "chunk_id": "doc_0_chunk_428", "file_type": "json", "from": "camera rotation velocity", "predicate": "reaches", "to": "30deg/frame", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_428", "file_type": "json", "from": "Ithm", "predicate": "estimates", "to": "camera orientation", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_428", "file_type": "json", "from": "camera rotation matrix", "predicate": "approaches", "to": "identity", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_428", "file_type": "json", "from": "camera position", "predicate": "has_error", "to": "2%", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_428", "file_type": "json", "from": "orientation error", "predicate": "is_less_than", "to": "0.5 degrees", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_429", "file_type": "json", "from": "orientation error", "predicate": "is_less_than", "to": "half a degree", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_429", "file_type": "json", "from": "Rolling Shutter Cameras", "predicate": "deals_with", "to": "Absolute Pose Problem", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_429", "file_type": "json", "from": "Absolute Pose Problem", "predicate": "requires", "to": "Polynomial Solvers", "type": "conceptual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_429", "file_type": "json", "from": "Linearized Camera Models", "predicate": "used_in", "to": "Absolute Pose Problem", "type": "conceptual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_429", "file_type": "json", "from": "relative position error", "predicate": "is_less_than", "to": "2%", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_430", "file_type": "json", "from": "RANSAC", "predicate": "is_technique_for", "to": "robust model fitting", "type": "conceptual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_695", "file_type": "json", "from": "RANSAC", "predicate": "is_algorithm_for", "to": "model fitting", "type": "conceptual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_430", "file_type": "json", "from": "robust model fitting", "predicate": "is_relevant_to", "to": "computer vision problems", "type": "conceptual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_430", "file_type": "json", "from": "structure and motion estimation", "predicate": "is_focus_of", "to": "paper", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_430", "file_type": "json", "from": "structure and motion estimation", "predicate": "is_problem_in", "to": "computer vision", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_430", "file_type": "json", "from": "Haralick", "predicate": "authored_paper", "to": "pose estimation", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_430", "file_type": "json", "from": "Hedborg", "predicate": "authored_paper", "to": "structure and motion estimation", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_430", "file_type": "json", "from": "RANAC", "predicate": "is_core_technique", "to": "automated cartography", "type": "factual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_432", "file_type": "json", "from": "RANAC", "predicate": "is_technique_for", "to": "robust model fitting", "type": "conceptual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_432", "file_type": "json", "from": "RANAC", "predicate": "is_relevant_to", "to": "computer vision problems", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_431", "file_type": "json", "from": "rolling shutter video", "predicate": "has_property", "to": "structure and motion estimation", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_431", "file_type": "json", "from": "rolling shutter video", "predicate": "benefits_from", "to": "inertial measurements", "type": "causal", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_431", "file_type": "json", "from": "rolling shutter video", "predicate": "requires", "to": "motion estimation", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_432", "file_type": "json", "from": "rolling shutter video", "predicate": "improves", "to": "accuracy", "type": "causal", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_431", "file_type": "json", "from": "rolling shutter data", "predicate": "uses", "to": "bundle adjustment", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_431", "file_type": "json", "from": "bundle adjustment", "predicate": "applies_to", "to": "rolling shutter data", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_431", "file_type": "json", "from": "C. Jia and B. L. Evans", "predicate": "combines", "to": "rolling shutter video", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_432", "file_type": "json", "from": "inertial measurements", "predicate": "improves", "to": "accuracy", "type": "causal", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_542", "file_type": "json", "from": "motion estimation", "predicate": "relevant_to", "to": "video analysis", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_432", "file_type": "json", "from": "ter video recti\ufb01cation", "predicate": "combines", "to": "rolling shutter video", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_432", "file_type": "json", "from": "ter video recti\ufb01cation", "predicate": "combines", "to": "inertial measurements", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_433", "file_type": "json", "from": "visual SLAM", "predicate": "relevant_to", "to": "camera pose estimation", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_432", "file_type": "json", "from": "parallel tracking and mapping", "predicate": "introduced_in", "to": "ISMAR \u201909", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_433", "file_type": "json", "from": "EEE International Symposium on Mixed and augmented Reality", "predicate": "introduces", "to": "visual SLAM", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_433", "file_type": "json", "from": "Z. Kukelova", "predicate": "authors", "to": "Singly-bordered block-diagonal form", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_433", "file_type": "json", "from": "Z. Kukelova", "predicate": "authors", "to": "Automatic generator of minimal problem solvers", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_433", "file_type": "json", "from": "Singly-bordered block-diagonal form", "predicate": "addresses", "to": "optimization techniques", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_486", "file_type": "json", "from": "efficient computation", "predicate": "relates_to", "to": "salient regions", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_433", "file_type": "json", "from": "Automatic generator of minimal problem solvers", "predicate": "related_to", "to": "efficient solvers", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_433", "file_type": "json", "from": "efficient solvers", "predicate": "contributes_to", "to": "efficient computation", "type": "conceptual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_433", "file_type": "json", "from": "ECCV 2008", "predicate": "publishes", "to": "Automatic generator of minimal problem solvers", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_434", "file_type": "json", "from": "Proceedings", "predicate": "part_of", "to": "Lecture Notes in Computer Science", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_434", "file_type": "json", "from": "Cox", "predicate": "authored", "to": "Using Algebraic Geometry", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_434", "file_type": "json", "from": "Using Algebraic Geometry", "predicate": "published_by", "to": "Springer", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_530", "file_type": "json", "from": "Springer", "predicate": "published", "to": "Line Drawing Interpretation", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_434", "file_type": "json", "from": "Sridhar", "predicate": "authored", "to": "Fast and Robust Hand Tracking", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_434", "file_type": "json", "from": "Sridhar", "predicate": "collaborated_with", "to": "Theobalt", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_434", "file_type": "json", "from": "Fast and Robust Hand Tracking", "predicate": "uses", "to": "Detection-Guided Optimization", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_436", "file_type": "json", "from": "Baak et al.", "predicate": "authored", "to": "full body pose reconstruction", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_436", "file_type": "json", "from": "full body pose reconstruction", "predicate": "uses", "to": "depth camera", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_436", "file_type": "json", "from": "Ballan et al.", "predicate": "authored", "to": "motion capture of hands", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_436", "file_type": "json", "from": "Bhattacharyya", "predicate": "authored", "to": "measure of divergence", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_436", "file_type": "json", "from": "Criminisi and Shotton", "predicate": "authored", "to": "Decision forests", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_437", "file_type": "json", "from": "Srinath Srilhar", "predicate": "affiliated_with", "to": "Max Planck Institute for Informatics", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_437", "file_type": "json", "from": "A. Criminisi", "predicate": "co_authored", "to": "Decision forests for computer vision", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_437", "file_type": "json", "from": "A. Criminisi", "predicate": "co_authored", "to": "Learning to be a depth camera", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_437", "file_type": "json", "from": "A. Criminisi", "predicate": "co_authored", "to": "Efficient regression of general-activity human poses", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_677", "file_type": "json", "from": "A. Criminisi", "predicate": "authored", "to": "Geodesic image and video editing", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_437", "file_type": "json", "from": "J. Shotton", "predicate": "co_authored", "to": "Decision forests for computer vision", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_437", "file_type": "json", "from": "J. Shotton", "predicate": "co_authored", "to": "Learning to be a depth camera", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_437", "file_type": "json", "from": "J. Shotton", "predicate": "co_authored", "to": "Efficient regression of general-activity human poses", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_437", "file_type": "json", "from": "S. R. Fanello", "predicate": "co_authored", "to": "Learning to be a depth camera", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_437", "file_type": "json", "from": "H. Hamer", "predicate": "co_authored", "to": "Tracking a hand manipulating an object", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_437", "file_type": "json", "from": "C. Keskin", "predicate": "co_authored", "to": "Real time hand pose estimation", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_438", "file_type": "json", "from": "C. Keskin", "predicate": "affiliated_with", "to": "ICCV Workshops", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_438", "file_type": "json", "from": "F. Kirac", "predicate": "affiliated_with", "to": "ICCV Workshops", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_438", "file_type": "json", "from": "Y. Kara", "predicate": "affiliated_with", "to": "ICCV", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_438", "file_type": "json", "from": "L. Akarun", "predicate": "affiliated_with", "to": "ICCV Workshops", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_438", "file_type": "json", "from": "Srinath Sridhar", "predicate": "affiliated_with", "to": "Max Planck Institute for Informatics", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_438", "file_type": "json", "from": "Antti Oulasvirta", "predicate": "affiliated_with", "to": "Aalto University", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_438", "file_type": "json", "from": "Fumin Shen", "predicate": "author_of", "to": "Supervised Discrete Hashing", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_443", "file_type": "json", "from": "Fumin Shen", "predicate": "affiliated_with", "to": "University of Electronic Science and Technology of China", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_438", "file_type": "json", "from": "Supervised Discrete Hashing", "predicate": "presented_at", "to": "CVPR", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_438", "file_type": "json", "from": "Heng Tao Shen", "predicate": "author_of", "to": "Supervised Discrete Hashing", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_443", "file_type": "json", "from": "Heng Tao Shen", "predicate": "affiliated_with", "to": "The University of Queensland", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_439", "file_type": "json", "from": "Supervised Discrete Hanning (SDH)", "predicate": "is_a", "to": "hashing framework", "type": "conceptual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_439", "file_type": "json", "from": "Supervised Discrete Hanning (SDH)", "predicate": "designed_for", "to": "linear classification", "type": "conceptual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_439", "file_type": "json", "from": "handling discrete constraints", "predicate": "leads_to", "to": "NP-hard optimization problems", "type": "causal", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_439", "file_type": "json", "from": "objective", "predicate": "reformulated_by", "to": "introducing an auxiliary variable and regularization algorithm", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_439", "file_type": "json", "from": "cyclic coordinate descent", "predicate": "solves", "to": "regularization sub-problem", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_439", "file_type": "json", "from": "SDH", "predicate": "achieves", "to": "high-quality discrete solutions", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_439", "file_type": "json", "from": "SDH", "predicate": "enables", "to": "handling of massive datasets", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_440", "file_type": "json", "from": "SDH", "predicate": "superior_to", "to": "state-of-the-art hashing methods", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_511", "file_type": "json", "from": "SDH", "predicate": "extends", "to": "DH", "type": "conceptual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_511", "file_type": "json", "from": "SDH", "predicate": "includes", "to": "discriminative term", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_440", "file_type": "json", "from": "Hashing", "predicate": "enables", "to": "handling of massive datasets", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_440", "file_type": "json", "from": "Hashing", "predicate": "applied_to", "to": "image datasets", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_440", "file_type": "json", "from": "p-stable distributions", "predicate": "basis_for", "to": "hashing technique", "type": "conceptual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_440", "file_type": "json", "from": "bilinear projections", "predicate": "used_for", "to": "learning binary codes", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_559", "file_type": "json", "from": "bilinear projections", "predicate": "relate_to", "to": "binary code learning", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_441", "file_type": "json", "from": "learning binary codes", "predicate": "uses", "to": "bilinear projections", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_440", "file_type": "json", "from": "Belkin, M., \u0026 Niyogi, P.", "predicate": "authored", "to": "foundational paper", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_440", "file_type": "json", "from": "Datar, N., et al.", "predicate": "introduced", "to": "hashing technique", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_440", "file_type": "json", "from": "Gong, Y., et al.", "predicate": "explored", "to": "bilinear projections", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_441", "file_type": "json", "from": "Rowley et al. (2013) paper", "predicate": "explores", "to": "learning binary codes", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_441", "file_type": "json", "from": "Weiss et al. (2008) paper", "predicate": "introduces", "to": "spectral hashing", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_441", "file_type": "json", "from": "spectral hashing", "predicate": "is_a", "to": "contribution to field", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_441", "file_type": "json", "from": "Gong et al. (2013) paper", "predicate": "presents", "to": "iterative quantization approach", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_441", "file_type": "json", "from": "iterative quantization approach", "predicate": "aims at", "to": "learning binary codes", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_441", "file_type": "json", "from": "iterative quantization approach", "predicate": "uses", "to": "procustean approach", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_441", "file_type": "json", "from": "Kulis \u0026 Darrell (2009) paper", "predicate": "introduces", "to": "method for learning to hash", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_441", "file_type": "json", "from": "method for learning to hash", "predicate": "uses", "to": "binary reconstructive embeddings", "type": "factual", "width": 0.91}, {"arrows": "to", "chunk_id": "doc_0_chunk_441", "file_type": "json", "from": "binary reconstructive embeddings", "predicate": "is_a", "to": "method", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_442", "file_type": "json", "from": "Kulis \u0026 Darrell (2009)", "predicate": "introduces", "to": "binary reconstructive embeddings hashing method", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_442", "file_type": "json", "from": "binary reconstructive embeddings hashing method", "predicate": "is_a", "to": "hashing technique", "type": "conceptual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_442", "file_type": "json", "from": "Liu, Wang, Kumar, \u0026 Chang (2011)", "predicate": "explores", "to": "hashing techniques", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_442", "file_type": "json", "from": "hashing techniques", "predicate": "utilizes", "to": "graph structures", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_559", "file_type": "json", "from": "hashing techniques", "predicate": "are_used_for", "to": "efficient similarity search", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_442", "file_type": "json", "from": "Wang, Kumar, \u0026 Chang (2012)", "predicate": "addresses", "to": "hashing", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_442", "file_type": "json", "from": "hashing", "predicate": "operates_in", "to": "semi-supervised setting", "type": "conceptual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_442", "file_type": "json", "from": "Shen \u0026 Hao (2011)", "predicate": "relates_to", "to": "learning and classification", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_442", "file_type": "json", "from": "Norouzi \u0026 Blei (2011)", "predicate": "introduces", "to": "minimal loss hashing", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_442", "file_type": "json", "from": "minimal loss hashing", "predicate": "creates", "to": "compact binary codes", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_443", "file_type": "json", "from": "Blei", "predicate": "authored", "to": "Minimal loss hashing", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_443", "file_type": "json", "from": "A Maximum Entropy Feature Descriptor", "predicate": "used_for", "to": "Age Invariant Face Recognition", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_443", "file_type": "json", "from": "Li, Zhifeng", "predicate": "authored", "to": "A Maximum Entropy Feature Descriptor", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_446", "file_type": "json", "from": "Li, Zhifeng", "predicate": "authored", "to": "Nonparametric Discriminent Analysis for Face Recognition", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_443", "file_type": "json", "from": "Tao, Dacheng", "predicate": "authored", "to": "A Maximum Entropy Feature Descriptor", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_443", "file_type": "json", "from": "Liu, Jianzhang", "predicate": "authored", "to": "A Maximum Entropy Feature Descriptor", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_443", "file_type": "json", "from": "Li, Xuelong", "predicate": "authored", "to": "A Maximum Entropy Feature Descriptor", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_444", "file_type": "json", "from": "maximum entropy feature descriptor", "predicate": "encodes", "to": "microstructure", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_444", "file_type": "json", "from": "maximum entropy feature descriptor", "predicate": "transforms into", "to": "discrete codes", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_444", "file_type": "json", "from": "maximum entropy feature descriptor", "predicate": "is", "to": "feature descriptor", "type": "conceptual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_444", "file_type": "json", "from": "sampling", "predicate": "extracts", "to": "discriminatory information", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_444", "file_type": "json", "from": "identity factor analysis", "predicate": "estimates", "to": "probability of same identity", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_444", "file_type": "json", "from": "experimentation", "predicate": "uses", "to": "MORPH dataset", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_444", "file_type": "json", "from": "experimentation", "predicate": "uses", "to": "FGNET dataset", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_444", "file_type": "json", "from": "experimentation", "predicate": "uses", "to": "LFW dataset", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_445", "file_type": "json", "from": "experimentation", "predicate": "performed_on", "to": "MORPH", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_445", "file_type": "json", "from": "MORPH", "predicate": "is_a", "to": "face aging dataset", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_445", "file_type": "json", "from": "FGNET", "predicate": "is_a", "to": "face aging dataset", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_446", "file_type": "json", "from": "Age Invariant Face Recognition (AIFR)", "predicate": "evaluated_on", "to": "MORPH", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_446", "file_type": "json", "from": "Age Invariant Face Recognition (AIFR)", "predicate": "evaluated_on", "to": "LFW dataset", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_446", "file_type": "json", "from": "Age Incompliant Face Recognition (AIFR)", "predicate": "evaluated_on", "to": "FGNET", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_446", "file_type": "json", "from": "Maximum Entropy Feature Descriptor (MEFD)", "predicate": "used_in", "to": "Age Invariant Face Recognition (AIFR)", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_446", "file_type": "json", "from": "Identity Factor Analysis (IFA)", "predicate": "used_in", "to": "Age Invariant Face Recognition (AIFR)", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_446", "file_type": "json", "from": "Nonparametric Discriminent Analysis for Face Recognition", "predicate": "published_in", "to": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_446", "file_type": "json", "from": "Wang, Xiaogang", "predicate": "authored", "to": "A unified framework for subspace face recognition", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_446", "file_type": "json", "from": "A unified framework for subspace face recognition", "predicate": "published_in", "to": "IEEE Trans. Pattern Anal. Mach. Intell.", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_446", "file_type": "json", "from": "Li, Unsang", "predicate": "authored", "to": "A discriminative model for age invariant face recognition", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_447", "file_type": "json", "from": "Li, Unsang", "predicate": "published", "to": "IEEE Transactions on Information Forensics and Security", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_447", "file_type": "json", "from": "Li, Unsang", "predicate": "collaborated_with", "to": "Zhifeng Li", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_446", "file_type": "json", "from": "A discriminative model for age invariant face recognition", "predicate": "published_in", "to": "IEEE Transactions on Information Forensics and Security", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_627", "file_type": "json", "from": "IEEE Trans. Pattern Anal. Mach. Intell.", "predicate": "publishes", "to": "Semi-supervised hashing", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_447", "file_type": "json", "from": "Park, Unsav", "predicate": "published", "to": "IEEE Trans. Pattern Anal. Mach. Intell.", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_447", "file_type": "json", "from": "Park, Unsav", "predicate": "collaborated_with", "to": "Yiying Tong", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_447", "file_type": "json", "from": "Belhumeur, Peter N.", "predicate": "published", "to": "IEEE Trans. Pattern Anal. Mach. Intell.", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_447", "file_type": "json", "from": "Gong, D.", "predicate": "presented", "to": "ICCV 2013", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_447", "file_type": "json", "from": "Huang, G.B.", "predicate": "created", "to": "Labelled faces in the wild", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_448", "file_type": "json", "from": "Labelled faces in the wild", "predicate": "is_database_for", "to": "face recognition", "type": "factual", "width": 2.0}, {"arrows": "to", "chunk_id": "doc_0_chunk_448", "file_type": "json", "from": "Labelled faces in the wild", "predicate": "studies", "to": "face recognition in unconstrained environments", "type": "factual", "width": 2.0}, {"arrows": "to", "chunk_id": "doc_0_chunk_448", "file_type": "json", "from": "Zhifeng Li", "predicate": "is_affiliated_with", "to": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "type": "factual", "width": 2.0}, {"arrows": "to", "chunk_id": "doc_0_chunk_449", "file_type": "json", "from": "Zhifeng Li", "predicate": "affiliated_with", "to": "Shenzhen Key Lab of Computer Vision and Pattern Recogniton", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_448", "file_type": "json", "from": "Technical Report 07-49", "predicate": "published_by", "to": "University of Massachusetts, Amherst", "type": "factual", "width": 2.0}, {"arrows": "to", "chunk_id": "doc_0_chunk_448", "file_type": "json", "from": "Nonparametric Discriminant Analysis for Face Recognition", "predicate": "addresses", "to": "face recognition", "type": "factual", "width": 2.0}, {"arrows": "to", "chunk_id": "doc_0_chunk_448", "file_type": "json", "from": "Random sampling LDA", "predicate": "used_for", "to": "face recognition", "type": "factual", "width": 2.0}, {"arrows": "to", "chunk_id": "doc_0_chunk_448", "file_type": "json", "from": "DiHong Gong", "predicate": "is_affiliated_with", "to": "Shenzhen Key Lab of Computer Vision and Pattern Recognition", "type": "factual", "width": 2.0}, {"arrows": "to", "chunk_id": "doc_0_chunk_449", "file_type": "json", "from": "Shenzhen Key Lab of Computer Vision and Pattern Recognition", "predicate": "part_of", "to": "Shenzhen Institutes of Advanced Technology", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_449", "file_type": "json", "from": "Dihong Gong", "predicate": "affiliated_with", "to": "Shenzhen Key Lab of Computer Vision and Pattern Recognition", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_449", "file_type": "json", "from": "Dacheng Tao", "predicate": "affiliated_with", "to": "University of Technology, Sydney", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_449", "file_type": "json", "from": "Jianzhuang Liu", "predicate": "affiliated_with", "to": "Dept. of Information Engineering", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_449", "file_type": "json", "from": "Jianzhuang Liu", "predicate": "affiliated_with", "to": "Huawei Technologies Co. Ltd.", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_449", "file_type": "json", "from": "Xuelong Li", "predicate": "affiliated_with", "to": "Xi\u0027an Institute of Optics and Precision Mechanics", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_449", "file_type": "json", "from": "Xi\u0027an Institute of Optics and Precision Mechanics", "predicate": "part_of", "to": "Chinese Academy of Sciences", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_449", "file_type": "json", "from": "Sayed Hossein Khatoonabadi", "predicate": "author_of", "to": "research paper", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_455", "file_type": "json", "from": "Sayed Hossein Khatoonabadi", "predicate": "affiliated_with", "to": "Simon Fraser University", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_450", "file_type": "json", "from": "Sayed Hosheen Khatoonabadi", "predicate": "is_author_of", "to": "How Many Bits Does It Take for a Stimulus to Be Salient?", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_450", "file_type": "json", "from": "How Many Bits Does It Take for a Stimulus to Be Salient?", "predicate": "is_paper", "to": "CVPR paper", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_450", "file_type": "json", "from": "Nuno Vasconcelos", "predicate": "is_author_of", "to": "How Many Bits Does It Take for a Stimulus to Be Salient?", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_455", "file_type": "json", "from": "Nuno Vasconcelos", "predicate": "affiliated_with", "to": "University of California, San Diego", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_450", "file_type": "json", "from": "Yuifeng Shan", "predicate": "is_author_of", "to": "How Many Bits Does It Take for a Stimulus to Be Salient?", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_450", "file_type": "json", "from": "Khatoonabadi_How_Many_Bits_2015_CVPR_paper.pdf", "predicate": "contains", "to": "How Many Bits Does It Take for a Stimulus to Be Salient?", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_450", "file_type": "json", "from": "xuelong_li@opt.ac.cn", "predicate": "is_email_contact", "to": "hanics", "type": "factual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_450", "file_type": "json", "from": "Stimulus", "predicate": "has_property", "to": "salience", "type": "conceptual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_451", "file_type": "json", "from": "computational models", "predicate": "focus_of_research", "to": "salience", "type": "conceptual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_451", "file_type": "json", "from": "early approaches", "predicate": "model_salience_as", "to": "center-surround filters", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_451", "file_type": "json", "from": "recent works", "predicate": "seek", "to": "general computational principles", "type": "conceptual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_451", "file_type": "json", "from": "measure of salience", "predicate": "based_on", "to": "bits required by video compressor", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_451", "file_type": "json", "from": "measure of salience", "predicate": "demonstrates", "to": "predictive power", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_451", "file_type": "json", "from": "measure of salience", "predicate": "is_embedded_in", "to": "Markov random field model", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_452", "file_type": "json", "from": "probabilistic inference", "predicate": "relates_to", "to": "salience", "type": "conceptual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_452", "file_type": "json", "from": "brain", "predicate": "viewed_as", "to": "universal compression device", "type": "conceptual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_452", "file_type": "json", "from": "universal compression device", "predicate": "represents", "to": "brain", "type": "conceptual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_453", "file_type": "json", "from": "Fixation Prediction", "predicate": "achieves", "to": "state-of-the-art accuracy", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_453", "file_type": "json", "from": "view of the brain", "predicate": "is_a", "to": "universal compression device", "type": "conceptual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_453", "file_type": "json", "from": "Agarwal et al. (2003)", "predicate": "develops", "to": "algorithm", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_453", "file_type": "json", "from": "region-of-interest", "predicate": "is_in", "to": "compressed MPEG domain", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_453", "file_type": "json", "from": "Hou and Zhang (2007)", "predicate": "proposes", "to": "spectral residual approach", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_453", "file_type": "json", "from": "spectral residual approach", "predicate": "is_a", "to": "salience detection method", "type": "conceptual", "width": 0.91}, {"arrows": "to", "chunk_id": "doc_0_chunk_453", "file_type": "json", "from": "Helbing and Molnar (1995)", "predicate": "models", "to": "pedestrian dynamics", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_454", "file_type": "json", "from": "EE CVPR\u201907", "predicate": "published_in", "to": "IEEE", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_454", "file_type": "json", "from": "Agarwal, G.", "predicate": "authored", "to": "algorithm", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_454", "file_type": "json", "from": "Anstis, S. M.", "predicate": "authored", "to": "perception of apparent movement", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_454", "file_type": "json", "from": "Attneave, F.", "predicate": "authored", "to": "Informational aspects of visual perception", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_454", "file_type": "json", "from": "Barlow, H.", "predicate": "authored", "to": "Cerebral cortex as a model builder", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_454", "file_type": "json", "from": "Barlow, H.", "predicate": "authored", "to": "Redundancy reduction revisited", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_454", "file_type": "json", "from": "Besag, J.", "predicate": "authored", "to": "Spatial interaction", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_455", "file_type": "json", "from": "Royal Statistical Society", "predicate": "publishes", "to": "Series B", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_455", "file_type": "json", "from": "Series B", "predicate": "volume", "to": "36", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_455", "file_type": "json", "from": "Series B", "predicate": "issue", "to": "192\u2013236", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_455", "file_type": "json", "from": "Ivan V. Bajic", "predicate": "affiliated_with", "to": "Simon Fraser University", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_455", "file_type": "json", "from": "Limin Wang", "predicate": "author_of", "to": "Action Recognition with Trajectory-Pooled Deep-Convolutional Descriptors", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_459", "file_type": "json", "from": "Limin Wang", "predicate": "affiliated_with", "to": "Department of Information Engineering", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_458", "file_type": "json", "from": "Limin Wang", "predicate": "contact_email", "to": "07wanglimin@gmail.com", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_455", "file_type": "json", "from": "Action Recognition with Trajectory-Pooled Deep-Convolutional Descriptors", "predicate": "published_in", "to": "CVPR paper", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_455", "file_type": "json", "from": "Yu Qiao", "predicate": "author_of", "to": "Action Recognition with Trajectory-Pooled Deep-Convolutional Descriptors", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_458", "file_type": "json", "from": "Yu Qiao", "predicate": "is", "to": "researcher", "type": "conceptual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_459", "file_type": "json", "from": "Yu Qiao", "predicate": "affiliated_with", "to": "Shenzhen Institutes of Advanced Technology", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_459", "file_type": "json", "from": "Yu Qiao", "predicate": "works_at", "to": "CAS", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_455", "file_type": "json", "from": "Xiaoou Tang", "predicate": "author_of", "to": "Action Recognized with Trajectory-Pooled Deep-Convolutional Descriptors", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_459", "file_type": "json", "from": "Xiaoou Tang", "predicate": "affiliated_with", "to": "Department of Information Engineering", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_456", "file_type": "json", "from": "TDD", "predicate": "is_a", "to": "video representation", "type": "conceptual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_456", "file_type": "json", "from": "TDD", "predicate": "combines", "to": "hand-crafted features", "type": "conceptual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_456", "file_type": "json", "from": "TDD", "predicate": "utilizes", "to": "deep architectures", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_456", "file_type": "json", "from": "TDD", "predicate": "employs", "to": "trajectory-constrained pooling", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_456", "file_type": "json", "from": "trajectory-constrained pooling", "predicate": "aggregates", "to": "feature maps", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_456", "file_type": "json", "from": "feature maps", "predicate": "are_generated_by", "to": "deep architectures", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_456", "file_type": "json", "from": "normalization methods", "predicate": "enhance", "to": "robustness", "type": "causal", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_456", "file_type": "json", "from": "TDDs", "predicate": "outperform", "to": "hand-crafted features", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_456", "file_type": "json", "from": "TDDs", "predicate": "outperform", "to": "deep-learned features", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_456", "file_type": "json", "from": "HMD-B51", "predicate": "is_a", "to": "dataset", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_456", "file_type": "json", "from": "UCF101", "predicate": "is_a", "to": "dataset", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_457", "file_type": "json", "from": "Human Action Recognition", "predicate": "achieves", "to": "state-of-the-art performance", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_457", "file_type": "json", "from": "Deep Convolutional Descriptors", "predicate": "used in", "to": "Human Action Recognition", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_457", "file_type": "json", "from": "Trajectory-Constrained Pooling", "predicate": "used in", "to": "Human Action Recognition", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_457", "file_type": "json", "from": "Multi-view super vector", "predicate": "used for", "to": "action recognition", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_457", "file_type": "json", "from": "Convolutional Nets", "predicate": "delves into", "to": "details", "type": "factual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_457", "file_type": "json", "from": "Aggarwal, J. K., \u0026 Ryoo, M. S.", "predicate": "authored", "to": "Human activity analysis review", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_457", "file_type": "json", "from": "Bay, H., Tuytelaars, T., \u0026 Van Gool, L. J.", "predicate": "authored", "to": "SURF description", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_458", "file_type": "json", "from": "Karpathy et al.", "predicate": "used", "to": "convolutional neural networks", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_458", "file_type": "json", "from": "HMDB", "predicate": "is", "to": "video database", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_459", "file_type": "json", "from": "HMDB", "predicate": "is_database_for", "to": "human motion recognition", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_459", "file_type": "json", "from": "T", "predicate": "authored", "to": "HMDB", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_459", "file_type": "json", "from": "T", "predicate": "published_in", "to": "ICCV", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_459", "file_type": "json", "from": "Jing Shao", "predicate": "author_of", "to": "Deeply Learned Attributes", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_463", "file_type": "json", "from": "Jing Shao", "predicate": "affiliated_with", "to": "Department of Electronic Engineering", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_459", "file_type": "json", "from": "Deeply Learned Attributes", "predicate": "presented_at", "to": "CVPR", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_459", "file_type": "json", "from": "Kai Kang", "predicate": "author_of", "to": "Deeply Learned Attributes", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_463", "file_type": "json", "from": "Kai Kang", "predicate": "affiliated_with", "to": "Department of Electronic Engineering", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_459", "file_type": "json", "from": "Chen Change Loy", "predicate": "author_of", "to": "Deeply Learned Attributes", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_460", "file_type": "json", "from": "Crowded scene understanding", "predicate": "is_problem_in", "to": "computer vision", "type": "conceptual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_460", "file_type": "json", "from": "Deep model", "predicate": "learns", "to": "appearance features", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_460", "file_type": "json", "from": "Deep model", "predicate": "learns", "to": "motion features", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_460", "file_type": "json", "from": "Crowd motion channels", "predicate": "is_input_of", "to": "deep model", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_460", "file_type": "json", "from": "Crowd motion channels", "predicate": "inspired_by", "to": "generic properties of crowd systems", "type": "conceptual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_460", "file_type": "json", "from": "WWW Crowd dataset", "predicate": "contains", "to": "10,000 videos", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_460", "file_type": "json", "from": "WWW Crowd dataset", "predicate": "contains", "to": "8,257 crowded scenes", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_462", "file_type": "json", "from": "WWW Crowd dataset", "predicate": "used for", "to": "cross-scene attribute recognition", "type": "factual", "width": 0.7}, {"arrows": "to", "chunk_id": "doc_0_chunk_460", "file_type": "json", "from": "Attribute set", "predicate": "has_quantity", "to": "94 attributes", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_460", "file_type": "json", "from": "Deep models", "predicate": "displays", "to": "significant performance improvements", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_460", "file_type": "json", "from": "significant performance improvements", "predicate": "occurs_in", "to": "cross-scene attribute recognition", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_461", "file_type": "json", "from": "deep models", "predicate": "displays", "to": "significant performance improvements", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_461", "file_type": "json", "from": "deep models", "predicate": "compared_to", "to": "feature-based baselines", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_461", "file_type": "json", "from": "deep models", "predicate": "improves", "to": "performance", "type": "causal", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_461", "file_type": "json", "from": "deep models", "predicate": "uses", "to": "deeply learned features", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_462", "file_type": "json", "from": "deeply learned features", "predicate": "behaves with", "to": "superior performance", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_461", "file_type": "json", "from": "deeply learned features", "predicate": "utilized_in", "to": "multi-task learning", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_461", "file_type": "json", "from": "attribute recognition", "predicate": "occurs_in", "to": "cross-scene", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_462", "file_type": "json", "from": "attribute recognition", "predicate": "relies on", "to": "crowd-related features", "type": "conceptual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_461", "file_type": "json", "from": "baselines", "predicate": "related_to", "to": "feature-based", "type": "conceptual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_462", "file_type": "json", "from": "Deep learning models", "predicate": "improves", "to": "cross-scene attribute recognition", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_462", "file_type": "json", "from": "Deep learning models", "predicate": "utilizes", "to": "crowd motion channels", "type": "factual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_462", "file_type": "json", "from": "Ali and Shah (2007)", "predicate": "addresses", "to": "crowd flow segmentation", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_462", "file_type": "json", "from": "Ali and Shah (2008)", "predicate": "addresses", "to": "tracking in high density crowd scenes", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_462", "file_type": "json", "from": "Andrade, Blunsden, and Fisher (2006)", "predicate": "addresses", "to": "event detection in crowd scenes", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_462", "file_type": "json", "from": "Chan and Vasconcelos (2008)", "predicate": "addresses", "to": "video segmentation", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_465", "file_type": "json", "from": "video segmentation", "predicate": "uses", "to": "eye tracking prior", "type": "factual", "width": 0.87}, {"arrows": "to", "chunk_id": "doc_0_chunk_716", "file_type": "json", "from": "video segmentation", "predicate": "involves", "to": "dynamic textures", "type": "conceptual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_463", "file_type": "json", "from": "oring", "predicate": "published_in", "to": "CVPR 2008", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_463", "file_type": "json", "from": "Vasconcelos, N.", "predicate": "authored", "to": "Modeling, clustering, and segmenting video", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_463", "file_type": "json", "from": "Dalal, N.", "predicate": "authored", "to": "Histograms of oriented gradients", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_463", "file_type": "json", "from": "Triggers, B.", "predicate": "authored", "to": "Histograms of oriented gradients", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_463", "file_type": "json", "from": "Farhad, A.", "predicate": "authored", "to": "Describing objects by their attributes", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_463", "file_type": "json", "from": "Hospedales, T.", "predicate": "authored", "to": "A markov clustering topic model", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_463", "file_type": "json", "from": "Kang, K.", "predicate": "authored", "to": "Fully convolutional neural networks", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_463", "file_type": "json", "from": "Wang, X.", "predicate": "authored", "to": "Fully convolutional neural networks", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_465", "file_type": "json", "from": "eye tracking data", "predicate": "identifies", "to": "dominant visual tracks", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_466", "file_type": "json", "from": "eye tracking data", "predicate": "enhances", "to": "Object Extraction", "type": "causal", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_465", "file_type": "json", "from": "dominant visual tracks", "predicate": "guides", "to": "object search algorithm", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_465", "file_type": "json", "from": "object boundaries", "predicate": "refined_by", "to": "grabcut segmentation", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_466", "file_type": "json", "from": "Intriligator \u0026 Cavanagh", "predicate": "published", "to": "Cognitive psychology article", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_466", "file_type": "json", "from": "Cognitive psychology article", "predicate": "studies", "to": "visual attention", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_466", "file_type": "json", "from": "Itti, Koch, \u0026 Niebur", "predicate": "developed", "to": "salience-based visual attention model", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_466", "file_type": "json", "from": "salience-based visual attention model", "predicate": "enables", "to": "rapid scene analysis", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_466", "file_type": "json", "from": "salience-based visual attention model", "predicate": "improves", "to": "scene analysis", "type": "causal", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_466", "file_type": "json", "from": "Judd, Ehinger, Durand, \u0026 Torralba", "predicate": "researched", "to": "human gaze prediction", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_466", "file_type": "json", "from": "Object Extraction", "predicate": "benefits_from", "to": "eye tracking data", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_466", "file_type": "json", "from": "Video Segmentation", "predicate": "utilizes", "to": "Visual Attention", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_540", "file_type": "json", "from": "Video Segmentation", "predicate": "requires", "to": "Energy Function Optimization", "type": "conceptual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_467", "file_type": "json", "from": "Judd et al.", "predicate": "authored", "to": "Learning to predict where humans look", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_467", "file_type": "json", "from": "Judd et al.", "predicate": "published_in", "to": "Computer Vision, 2009 IEEE 12th international conference on", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_467", "file_type": "json", "from": "Karthikeyan et al. (2012)", "predicate": "authored", "to": "Uni\ufb01ed probabilistic framework", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_467", "file_type": "json", "from": "Borji \u0026 Itti", "predicate": "authored", "to": "State-of-the-art in visual attention modeling", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_467", "file_type": "json", "from": "Borji \u0026 Itti", "predicate": "published_in", "to": "Pattern Analysis and Machine Intelligence, IEEE Transactions on", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_467", "file_type": "json", "from": "Borji, Sihite, \u0026 Itti", "predicate": "authored", "to": "Salient object detection: A benchmark", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_467", "file_type": "json", "from": "Borji, Sihite, \u0026 Itti", "predicate": "published_in", "to": "Computer Vision\u2013ECCV 2012", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_468", "file_type": "json", "from": "Computer Vision\u2013ECCV 2012", "predicate": "appears_in", "to": "pages 414\u2013429", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_588", "file_type": "json", "from": "Computer Vision\u2013ECCV 2012", "predicate": "hosts", "to": "Bayesian face revisited", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_467", "file_type": "json", "from": "Karthikeyan et al. (2013)", "predicate": "authored", "to": "Learning top-down scene context", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_467", "file_type": "json", "from": "Karthikeyan et al. (2013)", "predicate": "published_in", "to": "ICIP, IEEE", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_468", "file_type": "json", "from": "Karthikeyan, S.", "predicate": "affiliated_with", "to": "University of California Santa Barbara", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_468", "file_type": "json", "from": "Karthikeyan, S.", "predicate": "email", "to": "{karthikeyan, thuyen, manj}@ece.ucsb.edu", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_469", "file_type": "json", "from": "{karthikeyan, thuyen, manj}@ece.ucsb.edu", "predicate": "contact_email", "to": "B.S. Manjunath", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_468", "file_type": "json", "from": "Thuyen Ngo", "predicate": "email", "to": "{karthikeyan, thuyen, manj}@ece.ucsb.edu", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_469", "file_type": "json", "from": "Thuyen Ngo", "predicate": "affiliated_with", "to": "University of California Santa Barbara", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_469", "file_type": "json", "from": "Miguel Eckstein", "predicate": "affiliated_with", "to": "University of California Santa Barbara", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_468", "file_type": "json", "from": "Miguel Eckstein", "predicate": "email", "to": "eckstein@psych.ucsb.edu", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_469", "file_type": "json", "from": "eckstein@psych.ucsb.edu", "predicate": "contact_email", "to": "Miguel Eckstein", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_468", "file_type": "json", "from": "B.S. Manjunath", "predicate": "affiliated_with", "to": "University of California Santa Barbara", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_468", "file_type": "json", "from": "Felzenszwalb, P.", "predicate": "authored", "to": "A discriminatively trained, multiscale, deformable part model", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_468", "file_type": "json", "from": "Eckstein, M. P.", "predicate": "authored", "to": "Visual search: A retrospective", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_469", "file_type": "json", "from": "Shervin Ardeshir", "predicate": "author_of", "to": "Geo-Semantic Segmentation", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_469", "file_type": "json", "from": "Geo-Semantic Segmentation", "predicate": "published_in", "to": "CVPR", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_469", "file_type": "json", "from": "Ko\ufb01 Malcolm Collins-Sibley", "predicate": "author_of", "to": "Geo-Semantic Segmentation", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_469", "file_type": "json", "from": "Mubarak Shah", "predicate": "author_of", "to": "Geo-Semantic Segmentation", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_475", "file_type": "json", "from": "Mubarak Shah", "predicate": "affiliated_with", "to": "University of  Central Florida", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_475", "file_type": "json", "from": "Mubarak Shah", "predicate": "email", "to": "shah@crcv.ucf.edu", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_470", "file_type": "json", "from": "geo-semantic segmentation method", "predicate": "leverages", "to": "GIS databases", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_470", "file_type": "json", "from": "geo-semantic segmentation method", "predicate": "refines", "to": "GIS projections alignment", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_470", "file_type": "json", "from": "GIS data", "predicate": "includes", "to": "building locations", "type": "factual", "width": 0.82}, {"arrows": "to", "chunk_id": "doc_0_chunk_470", "file_type": "json", "from": "GIS data", "predicate": "includes", "to": "street locations", "type": "factual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_470", "file_type": "json", "from": "projections", "predicate": "affected_by", "to": "GPS errors", "type": "factual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_470", "file_type": "json", "from": "projections", "predicate": "affected_by", "to": "camera parameter inaccuracies", "type": "factual", "width": 0.72}, {"arrows": "to", "chunk_id": "doc_0_chunk_471", "file_type": "json", "from": "projections", "predicate": "refined_by", "to": "random walks", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_471", "file_type": "json", "from": "projections", "predicate": "refined_by", "to": "global transformations", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_663", "file_type": "json", "from": "projections", "predicate": "results in", "to": "fast and efficient projections", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_470", "file_type": "json", "from": "alignment", "predicate": "uses", "to": "random walks", "type": "factual", "width": 0.78}, {"arrows": "to", "chunk_id": "doc_0_chunk_470", "file_type": "json", "from": "alignment", "predicate": "uses", "to": "global transformations", "type": "factual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_471", "file_type": "json", "from": "random walks", "predicate": "is_a", "to": "optimization technique", "type": "conceptual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_471", "file_type": "json", "from": "global transformations", "predicate": "is_a", "to": "image processing technique", "type": "conceptual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_471", "file_type": "json", "from": "segmentations", "predicate": "improves", "to": "alignment of projections", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_471", "file_type": "json", "from": "geo-references", "predicate": "includes", "to": "addresses", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_471", "file_type": "json", "from": "geo-references", "predicate": "includes", "to": "geo-locations", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_472", "file_type": "json", "from": "Geo-semantic Segmentation", "predicate": "uses", "to": "Random Walks", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_472", "file_type": "json", "from": "Geo-semantic Segmentation", "predicate": "uses", "to": "Global Transformations", "type": "factual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_472", "file_type": "json", "from": "Geo-semantic Segmentation", "predicate": "results_in", "to": "Semantically Segmented Images", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_472", "file_type": "json", "from": "Geo-semantic Segmentation", "predicate": "relates_to", "to": "GIS Data Integration", "type": "conceptual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_472", "file_type": "json", "from": "Geo-semantic Segmentation", "predicate": "involves", "to": "Iterative Data Fusion", "type": "conceptual", "width": 0.7}, {"arrows": "to", "chunk_id": "doc_0_chunk_472", "file_type": "json", "from": "Semantically Segmented Images", "predicate": "has", "to": "Geo-references", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_472", "file_type": "json", "from": "Geo-references", "predicate": "include", "to": "Addresses", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_472", "file_type": "json", "from": "P. Zhao et al. [17]", "predicate": "discusses", "to": "Rectilinear parsing", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_472", "file_type": "json", "from": "O. Teboul et al. [10]", "predicate": "discusses", "to": "Segmentation of building facades", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_472", "file_type": "json", "from": "G. J. Brostow et al. [2]", "predicate": "discusses", "to": "Segmentation and recognition", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_473", "file_type": "json", "from": "EE", "predicate": "published", "to": "2010", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_473", "file_type": "json", "from": "Brostow", "predicate": "authored", "to": "Segmentation and recognition using structure from motion point clouds", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_473", "file_type": "json", "from": "He", "predicate": "authored", "to": "Multiscale conditional random fields for image labeling", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_473", "file_type": "json", "from": "CVPR 2004", "predicate": "hosts", "to": "Multiscale conditional random fields for image labeling", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_473", "file_type": "json", "from": "Liu", "predicate": "authored", "to": "Entropy rate superpixel segmentation", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_497", "file_type": "json", "from": "Liu", "predicate": "authored", "to": "Saturation-Preerving Specular Reflection Separation", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_473", "file_type": "json", "from": "M\u00fcller", "predicate": "authored", "to": "Procedural modeling of buildings", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_474", "file_type": "json", "from": "Musialski", "predicate": "authored", "to": "Interactive coherence-based fac\u00b8ade modeling", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_473", "file_type": "json", "from": "Computer Graphics Forum", "predicate": "publishes", "to": "Interactive coherence-based fac\u00b8ade modeling", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_474", "file_type": "json", "from": "dings", "predicate": "published_by", "to": "ACM", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_474", "file_type": "json", "from": "Ardeshir", "predicate": "affiliated_with", "to": "University of Central Florida", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_474", "file_type": "json", "from": "Ardeshir", "predicate": "authored", "to": "Gis-assisted object detection", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_474", "file_type": "json", "from": "Lerma", "predicate": "authored", "to": "Semantic segmentation", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_474", "file_type": "json", "from": "Hoiem", "predicate": "authored", "to": "Automatic photo popup", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_474", "file_type": "json", "from": "Collins-Sibley", "predicate": "affiliated_with", "to": "Northeaster University", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_474", "file_type": "json", "from": "Shah", "predicate": "affiliated_with", "to": "University of Central Florida", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_474", "file_type": "json", "from": "Huang", "predicate": "authored", "to": "Bayesian Inference", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_475", "file_type": "json", "from": "Chao-Tsung Huang", "predicate": "authored", "to": "Bayesian Inference for Neighborhood Filters", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_475", "file_type": "json", "from": "Chao-Tsung Huang", "predicate": "email", "to": "collins-sibley.k@husky.neu.edu", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_475", "file_type": "json", "from": "Bayesian Inference for Neighborhood Filters", "predicate": "application_in", "to": "Denoising", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_475", "file_type": "json", "from": "Bayesian Inference for Neighborhood Filters", "predicate": "field_of_study", "to": "Image Processing", "type": "conceptual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_475", "file_type": "json", "from": "Denoising", "predicate": "is_application_of", "to": "Image Processing", "type": "conceptual", "width": 0.7}, {"arrows": "to", "chunk_id": "doc_0_chunk_475", "file_type": "json", "from": "Huang_Bayesian_Infrenence_for_2015_CVPR_paper.pdf", "predicate": "publication_date", "to": "2015", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_475", "file_type": "json", "from": "Huang_Bayesian_Infrenence_for_2015_CVPR_paper.pdf", "predicate": "conference", "to": "CVPR", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_476", "file_type": "json", "from": "Range-weighted neighborhood filters", "predicate": "useful for", "to": "edge-preserving denoising", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_476", "file_type": "json", "from": "Range-weighted neighborhood filters", "predicate": "have", "to": "limited theoretical understanding", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_476", "file_type": "json", "from": "unified empirical Bayesian framework", "predicate": "directly infers", "to": "filters", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_476", "file_type": "json", "from": "unified empirical Bayesian framework", "predicate": "estimates", "to": "range variance", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_477", "file_type": "json", "from": "range variance", "predicate": "requires", "to": "accurate estimation", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_476", "file_type": "json", "from": "neighborhood noise model", "predicate": "reasons", "to": "Yaroslavsky, bilateral, and modified non-local means filters", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_476", "file_type": "json", "from": "EM+ algorithm", "predicate": "estimates", "to": "range variance", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_476", "file_type": "json", "from": "EM+ algorithm", "predicate": "uses", "to": "model fitting", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_477", "file_type": "json", "from": "noisy images", "predicate": "affects", "to": "image quality", "type": "causal", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_476", "file_type": "json", "from": "color-image denoising", "predicate": "demonstrates", "to": "model\u0027s effectiveness", "type": "factual", "width": 0.89}, {"arrows": "to", "chunk_id": "doc_0_chunk_477", "file_type": "json", "from": "recursive fitting", "predicate": "improves", "to": "image quality", "type": "causal", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_479", "file_type": "json", "from": "Paris, S.", "predicate": "authored", "to": "Bilateral filtering", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_479", "file_type": "json", "from": "Bilateral filtering", "predicate": "is_a", "to": "image processing technique", "type": "conceptual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_480", "file_type": "json", "from": "Bilateral filtering", "predicate": "is_introduced_in", "to": "International Conference on Computer Vision", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_479", "file_type": "json", "from": "Buades, A.", "predicate": "authored", "to": "image denoising algorithms review", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_479", "file_type": "json", "from": "image denoising algorithms review", "predicate": "published_in", "to": "SIAM Journal on Multi-scale Modeling and Simulation", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_479", "file_type": "json", "from": "Chatterjee, P.", "predicate": "authored", "to": "Patch-based near-optimal image denoising", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_479", "file_type": "json", "from": "Patch-based near-optimal image denoising", "predicate": "published_in", "to": "IEEE Transactions on Image Processing", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_479", "file_type": "json", "from": "Peng, H.", "predicate": "authored", "to": "Bilateral kernel parameter optimization", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_479", "file_type": "json", "from": "Peng, H.", "predicate": "authored", "to": "Multispectral image denoising", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_479", "file_type": "json", "from": "Bilateral kernel parameter optimization", "predicate": "presented_at", "to": "International Conference on Image Processing", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_479", "file_type": "json", "from": "Multispectral image denoising", "predicate": "uses", "to": "vector bilateral filter", "type": "conceptual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_480", "file_type": "json", "from": "Multispectral image denoising", "predicate": "is_addressed_in", "to": "IEEE Transactions on Image Processing", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_479", "file_type": "json", "from": "vector bilateral filter", "predicate": "is_a", "to": "image filtering technique", "type": "conceptual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_480", "file_type": "json", "from": "Vector bilateral filter", "predicate": "optimizes", "to": "Multispectral image denoising", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_480", "file_type": "json", "from": "Image denoising", "predicate": "uses", "to": "scale mixtures of gausians", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_480", "file_type": "json", "from": "Bilateral filter", "predicate": "is_improved_in", "to": "IEEE Transactions on Image Processing", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_480", "file_type": "json", "from": "Local Estimation", "predicate": "is_part_of", "to": "Deep Networks for Saliency Detection", "type": "conceptual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_484", "file_type": "json", "from": "Local Estimation", "predicate": "is_technique_in", "to": "Deep Neural Networks", "type": "conceptual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_480", "file_type": "json", "from": "Global Search", "predicate": "is_part_of", "to": "Deep Networks for Saliency Detection", "type": "conceptual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_484", "file_type": "json", "from": "Global Search", "predicate": "is_technique_in", "to": "Deep Neural Networks", "type": "conceptual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_480", "file_type": "json", "from": "Chao-Tsun Huang", "predicate": "is_affiliated_with", "to": "National Tsing Hua University", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_480", "file_type": "json", "from": "Lijun Wang", "predicate": "is_author_of", "to": "Deep Networks for Saliency Detection", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_481", "file_type": "json", "from": "Lijun Wang", "predicate": "is_author_of", "to": "Deep Networks for Salience Detection", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_481", "file_type": "json", "from": "Deep Networks for Salience Detection", "predicate": "presented_at", "to": "CVPR", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_481", "file_type": "json", "from": "Deep Networks for Salience Detection", "predicate": "is_paper", "to": "Wang_Deep_Networks_for_2015_CVPR_paper", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_481", "file_type": "json", "from": "Wang_Deep_Networks_for_2015_CVPR_paper", "predicate": "is_file", "to": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Wang_Deep_Networks_for_2015_CVPR_paper.pdf", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_481", "file_type": "json", "from": "Salience Detection", "predicate": "uses", "to": "Deep Networks", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_686", "file_type": "json", "from": "Salience Detection", "predicate": "related_to", "to": "Sparse Coding", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_686", "file_type": "json", "from": "Salience Detection", "predicate": "addresses", "to": "Visual Attention", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_482", "file_type": "json", "from": "local estimation", "predicate": "uses", "to": "DNN-L", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_482", "file_type": "json", "from": "local estimation", "predicate": "refines", "to": "object concepts", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_482", "file_type": "json", "from": "global search", "predicate": "uses", "to": "geometric information", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_482", "file_type": "json", "from": "global search", "predicate": "uses", "to": "global contrast", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_482", "file_type": "json", "from": "DNN-L", "predicate": "learns", "to": "local patch features", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_482", "file_type": "json", "from": "DNN-L", "predicate": "determines", "to": "salience value", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_482", "file_type": "json", "from": "DNN-G", "predicate": "trained_to_predict", "to": "salient score", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_482", "file_type": "json", "from": "salient object regions", "predicate": "generated_by", "to": "weighted sum", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_483", "file_type": "json", "from": "saliency map", "predicate": "generated_by", "to": "weighted sum", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_483", "file_type": "json", "from": "object region", "predicate": "predicted_by", "to": "saliency score", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_483", "file_type": "json", "from": "saliency score", "predicate": "based_on", "to": "global features", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_484", "file_type": "json", "from": "Algorithm", "predicate": "performs_favorably_against", "to": "state-of-the-art methods", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_576", "file_type": "json", "from": "Algorithm", "predicate": "uses", "to": "Gaussian Mixture Model", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_484", "file_type": "json", "from": "Salient Region Detection", "predicate": "is_concept_in", "to": "field", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_675", "file_type": "json", "from": "Salient Region Detection", "predicate": "precursor_to", "to": "Video Object Segmentation", "type": "conceptual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_675", "file_type": "json", "from": "Salient Region Detection", "predicate": "described_in", "to": "CVPR, 2012", "type": "factual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_484", "file_type": "json", "from": "R. Achanta et al.", "predicate": "provides", "to": "foundational work", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_484", "file_type": "json", "from": "J. Carreira and C. Sminchisescu", "predicate": "presents", "to": "method", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_484", "file_type": "json", "from": "Constrained parametric min-cuts", "predicate": "is_work_by", "to": "J. Carreira and C. Sminchisescu", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_676", "file_type": "json", "from": "Constrained parametric min-cuts", "predicate": "is_method_of", "to": "automatic object segmentation", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_676", "file_type": "json", "from": "Constrained parametric min-cuts", "predicate": "uses", "to": "min-cut approach", "type": "factual", "width": 0.87}, {"arrows": "to", "chunk_id": "doc_0_chunk_677", "file_type": "json", "from": "Constrained parametric min-cuts", "predicate": "addresses", "to": "object segmentation", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_484", "file_type": "json", "from": "Object Candidate Regions", "predicate": "is_part_of", "to": "Deep Neural Networks", "type": "conceptual", "width": 0.65}, {"arrows": "to", "chunk_id": "doc_0_chunk_485", "file_type": "json", "from": "tric min-cuts", "predicate": "used_for", "to": "object segmentation", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_485", "file_type": "json", "from": "visual salience", "predicate": "established_by", "to": "Itti, Koch, and Niebur (1998)", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_485", "file_type": "json", "from": "Itti, Koch, and Niebur (1998)", "predicate": "presented", "to": "computational model", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_485", "file_type": "json", "from": "Itti, Koch, and Niebur (1998)", "predicate": "published_in", "to": "PAMI", "type": "factual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_485", "file_type": "json", "from": "saliency detection", "predicate": "uses", "to": "absorbing Markov chain", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_485", "file_type": "json", "from": "absorbing Markov chain", "predicate": "is_approach_for", "to": "saliency detection", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_486", "file_type": "json", "from": "Markov Chain approach", "predicate": "is_used_for", "to": "saliency detection", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_486", "file_type": "json", "from": "Hierarchical approaches", "predicate": "are_common_in", "to": "computer vision", "type": "conceptual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_486", "file_type": "json", "from": "Selective search", "predicate": "used_for", "to": "generating object proposals", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_486", "file_type": "json", "from": "Selective search", "predicate": "used_as", "to": "preprocessing step", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_486", "file_type": "json", "from": "salient object detection pipelines", "predicate": "uses", "to": "Selective search", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_487", "file_type": "json", "from": "simultaneous detection and segmentation", "predicate": "is_a", "to": "related task", "type": "conceptual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_487", "file_type": "json", "from": "simultaneous detection and segmentation", "predicate": "is_related_to", "to": "salient region detection", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_487", "file_type": "json", "from": "Bayesian models", "predicate": "are_used_in", "to": "computer vision", "type": "conceptual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_487", "file_type": "json", "from": "ICC paper", "predicate": "focuses_on", "to": "efficient computation", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_487", "file_type": "json", "from": "ECCV paper", "predicate": "addresses", "to": "simultaneous detection and segmentation", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_487", "file_type": "json", "from": "ICIP paper", "predicate": "uses", "to": "Bayesian model", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_488", "file_type": "json", "from": "Abhishek Kar", "predicate": "affiliated_with", "to": "University of California, Berkeley", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_488", "file_type": "json", "from": "Abhishek Kar", "predicate": "email", "to": "akar@eecs.berkeley.edu", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_488", "file_type": "json", "from": "Abhishek Kar", "predicate": "contributed_to", "to": "Category-Speci\ufb01c Object Reconstruction from a Single Image", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_488", "file_type": "json", "from": "Jo\u02dcao Carreira", "predicate": "affiliated_with", "to": "University of California, Berkeley", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_488", "file_type": "json", "from": "Jo\u02dcao Carreira", "predicate": "email", "to": "carreira@eecs.berkeley.edu", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_488", "file_type": "json", "from": "Jo\u02dcao Carreira", "predicate": "contributed_to", "to": "Category-Speci\ufb01c Object Reconstruction from a Single Image", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_488", "file_type": "json", "from": "Shubham Tulisiani", "predicate": "contributed_to", "to": "Category-Speci\ufb01c Object Reconstruction from a Single Image", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_566", "file_type": "json", "from": "Thorsten Beier", "predicate": "contributed_to", "to": "Fusion Moves for Correlation Clustering", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_489", "file_type": "json", "from": "Thorsten Beier", "predicate": "affiliates_with", "to": "University of Heidelberg", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_489", "file_type": "json", "from": "Thorsten Beier", "predicate": "has_email", "to": "thorsten.beier@iwr.uni-heidelberg.de", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_567", "file_type": "json", "from": "Thorsten Beier", "predicate": "associated_with", "to": "Beier_Fusion_Moves_for_2015_CVPR_supplemental", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_568", "file_type": "json", "from": "Thorsten Beier", "predicate": "works_at", "to": "University of Heidelberg (Iwr)", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_489", "file_type": "json", "from": "Fusion Moves for Correlation Clustering", "predicate": "is_paper_of", "to": "CVPR", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_566", "file_type": "json", "from": "Fusion Moves for Correlation Clustering", "predicate": "is_supplement_for", "to": "Beier_Fusion_Moves_for_2015_CVPR_supplemental.pdf", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_566", "file_type": "json", "from": "Fred A. Hamprecht", "predicate": "contributed_to", "to": "Fusion Moves for Correlation Clustering", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_489", "file_type": "json", "from": "Fred A. Hamprecht", "predicate": "affiliates_with", "to": "University of Heidelberg", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_489", "file_type": "json", "from": "Fred A. Hamprecht", "predicate": "has_email", "to": "fred.hamprecht@iwr.uni-heidelberg.de", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_568", "file_type": "json", "from": "Fred A. Hamprecht", "predicate": "works_at", "to": "University of Heidelberg (Iwr)", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_489", "file_type": "json", "from": "J\u00f6rg H. Kappes", "predicate": "affiliates_with", "to": "University of Heidelberg", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_489", "file_type": "json", "from": "J\u00f6rg H. Kappes", "predicate": "affiliates_with", "to": "Math", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_489", "file_type": "json", "from": "J\u00f6rg H. Kappes", "predicate": "has_email", "to": "kappes@math.uni-heidelberg.de", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_566", "file_type": "json", "from": "J\u00f6rg H. Kappes", "predicate": "contributed_to", "to": "Fusion Moves for Correlation Clustering", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_490", "file_type": "json", "from": "Hossein Rahmani", "predicate": "affiliated_with", "to": "The University of Western Australia", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_490", "file_type": "json", "from": "Hossein Rahmani", "predicate": "contributed_to", "to": "Learning a Non-linear Knowledge Transfer Model for Cross-View Action Recognition", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_490", "file_type": "json", "from": "Hossein Rahmani", "predicate": "is_author", "to": "Rahmani_Learning_a_Non-Linear_2015_CVPR_paper.pdf", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_490", "file_type": "json", "from": "Hossein Rahman\u0131", "predicate": "email", "to": "hossein@csse.uwa.edu.au", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_490", "file_type": "json", "from": "Ajmal Mian", "predicate": "affiliated_with", "to": "The University of Western Canada", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_490", "file_type": "json", "from": "Ajmal Mian", "predicate": "email", "to": "ajmal.mian@uwa.edu.au", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_490", "file_type": "json", "from": "Ajmal Mian", "predicate": "contributed_to", "to": "Learning a Non-linear Knowledge Transfer Model for Cross-View Action Recognition", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_490", "file_type": "json", "from": "Ajmal Mian", "predicate": "is_author", "to": "Rahmani_Learning_a_Non-Linear_2015_CVPR_paper.pdf", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_490", "file_type": "json", "from": "Learning a Non-linear Knowledge Transfer Model for Cross-View Action Recognition", "predicate": "presented_at", "to": "CVPR", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_491", "file_type": "json", "from": "Sparse Kernel Multi-task Learning (SKMTL) models", "predicate": "relates_to", "to": "Convex Multi-task Cluster Learning", "type": "conceptual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_491", "file_type": "json", "from": "Sparse Kernel Multi-task Learning (SKMTL) models", "predicate": "uses", "to": "Laplacian Eigenmaps", "type": "factual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_491", "file_type": "json", "from": "SKMTL problem", "predicate": "is", "to": "jointly convex", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_491", "file_type": "json", "from": "clustered structures", "predicate": "of", "to": "tasks", "type": "conceptual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_612", "file_type": "json", "from": "tasks", "predicate": "serve as", "to": "benchmarks", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_612", "file_type": "json", "from": "tasks", "predicate": "evaluating", "to": "progress", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_491", "file_type": "json", "from": "Robotics (Sarcos) dataset", "predicate": "is_example_of", "to": "dataset", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_491", "file_type": "json", "from": "sparse structure", "predicate": "has_implications_in", "to": "settings beyond computer vision", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_492", "file_type": "json", "from": "Laplacian Eigenmaps", "predicate": "related_to", "to": "Sparse Kernel Multi-task Learning", "type": "factual", "width": 0.7}, {"arrows": "to", "chunk_id": "doc_0_chunk_492", "file_type": "json", "from": "Sparse Kernel Multi-task Learning", "predicate": "related_to", "to": "computer vision", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_492", "file_type": "json", "from": "Joint Convexity", "predicate": "related_to", "to": "Sparse Kernel Multi-task Learning", "type": "factual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_492", "file_type": "json", "from": "Cluster Multi-task Learning", "predicate": "related_to", "to": "Sparse Kernel Multi-task Learning", "type": "factual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_492", "file_type": "json", "from": "Robotics", "predicate": "uses", "to": "Sarcos dataset", "type": "factual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_492", "file_type": "json", "from": "Robust Multiple Homography Estimation", "predicate": "is_problem", "to": "ill-solved problem", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_492", "file_type": "json", "from": "Zygmunt L. Szpak", "predicate": "is_author_of", "to": "Robust Multiple Homography Estimation", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_492", "file_type": "json", "from": "Wojciech Chojnacki", "predicate": "is_author_of", "to": "Robust Multiple Homography Estimation", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_492", "file_type": "json", "from": "Szpak_Robust_Multiple_Homography_2015_CVPR_paper.pdf", "predicate": "is_publication", "to": "Robust Multiple Homography Estimation", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_493", "file_type": "json", "from": "multiple homographies estimation", "predicate": "is", "to": "ill-solved problem", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_493", "file_type": "json", "from": "rigidity", "predicate": "implies", "to": "consistency constraints", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_493", "file_type": "json", "from": "homographies", "predicate": "must_satisfy", "to": "new constraints", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_493", "file_type": "json", "from": "homographies", "predicate": "are_related_to", "to": "views", "type": "conceptual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_493", "file_type": "json", "from": "epipolar geometries", "predicate": "are", "to": "inconsistent", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_493", "file_type": "json", "from": "robust multi-structure estimation methods", "predicate": "requires", "to": "enforcing constraints on homography matrices", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_494", "file_type": "json", "from": "multi-structure estimation methods", "predicate": "is_capable_of", "to": "enforcing constraints", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_494", "file_type": "json", "from": "multi-structure estimation methods", "predicate": "is", "to": "robust", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_494", "file_type": "json", "from": "homography matrices", "predicate": "has_constraint", "to": "constraints", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_494", "file_type": "json", "from": "new generation", "predicate": "includes", "to": "robust multi-structure estimation methods", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_494", "file_type": "json", "from": "critiques", "predicate": "targets", "to": "existing approaches", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_495", "file_type": "json", "from": "Robust Multi-Structure Estimation", "predicate": "enforces", "to": "constraints on homography matrices", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_495", "file_type": "json", "from": "Robust Multi-Structure Estimation", "predicate": "critiques", "to": "existing approaches", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_495", "file_type": "json", "from": "Homography Matrices", "predicate": "related to", "to": "Projective Geometry", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_495", "file_type": "json", "from": "Homography Matrices", "predicate": "related to", "to": "Epiopolar Geometry", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_495", "file_type": "json", "from": "Baker, S., Datta, A., and Kanade, T.", "predicate": "authored", "to": "Parameterizing homographies", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_495", "file_type": "json", "from": "Parameterizing homographies", "predicate": "is a", "to": "tech. rep. CMU-RI-TR-06-11", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_495", "file_type": "json", "from": "Bernstein, D. S.", "predicate": "authored", "to": "Matrix Mathematics", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_495", "file_type": "json", "from": "Chen, P., and Suter, D.", "predicate": "authored", "to": "Rank constraints for homographies", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_496", "file_type": "json", "from": "Chen, P.", "predicate": "authors", "to": "Rank constraints", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_496", "file_type": "json", "from": "Rank constraints", "predicate": "concerns", "to": "homographies", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_496", "file_type": "json", "from": "Suter, D.", "predicate": "authors", "to": "Rank constraints", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_496", "file_type": "json", "from": "Chojnacki, W.", "predicate": "authors", "to": "Multiple homography estimation", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_496", "file_type": "json", "from": "Chojnacki, W.", "predicate": "authors", "to": "Dimensionality result", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_496", "file_type": "json", "from": "Multiple homography estimation", "predicate": "uses", "to": "consistency constraints", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_496", "file_type": "json", "from": "Szpak, Z.", "predicate": "authors", "to": "Multiple homography estimation", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_496", "file_type": "json", "from": "van den Hengel, A.", "predicate": "authors", "to": "Multiple homography estimation", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_496", "file_type": "json", "from": "van den Hengel, A.", "predicate": "authors", "to": "Dimensionality result", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_496", "file_type": "json", "from": "Dimensionality result", "predicate": "concerns", "to": "homography matrices", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_496", "file_type": "json", "from": "Fouhey, D. F.", "predicate": "authors", "to": "Multiple plane detection", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_496", "file_type": "json", "from": "Multiple plane detection", "predicate": "uses", "to": "J-linkage", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_496", "file_type": "json", "from": "Scharstein, D.", "predicate": "authors", "to": "Multiple plane detection", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_496", "file_type": "json", "from": "Briggs, A. J.", "predicate": "authors", "to": "Multiple plane detection", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_496", "file_type": "json", "from": "Goldberger, J.", "predicate": "authors", "to": "Camera projection matrices", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_497", "file_type": "json", "from": "Goldberger", "predicate": "published", "to": "Reconstructing camera projection matrices", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_497", "file_type": "json", "from": "Irving", "predicate": "authored", "to": "Integers, Polynomials, and Rings", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_497", "file_type": "json", "from": "Szpak", "predicate": "affiliated_with", "to": "School of Computer Science", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_497", "file_type": "json", "from": "Chojnicki", "predicate": "affiliated_with", "to": "School of Computer Science", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_497", "file_type": "json", "from": "van den Hengel", "predicate": "affiliated_with", "to": "School of Computer Science", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_497", "file_type": "json", "from": "Saturation-Preerving Specular Reflection Separation", "predicate": "published_in", "to": "CVPR", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_497", "file_type": "json", "from": "Yuan", "predicate": "authored", "to": "Saturation-Preerving Specular Reflection Separation", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_497", "file_type": "json", "from": "Zheng", "predicate": "authored", "to": "Saturation-Preerving Specular Reflection Separation", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_497", "file_type": "json", "from": "Wu", "predicate": "authored", "to": "Saturation-Preerving Specular Reflection Separation", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_688", "file_type": "json", "from": "Wu", "predicate": "affiliated with", "to": "University of Delaware", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_688", "file_type": "json", "from": "Wu", "predicate": "authored", "to": "Robust Regression on Image Manifolds", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_688", "file_type": "json", "from": "Wu", "predicate": "email", "to": "nianyi@eecis.udel.edu", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_498", "file_type": "json", "from": "Yuanliu Liu", "predicate": "authored", "to": "Saturation-Preerving Specular Reflection Paper", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_504", "file_type": "json", "from": "Yuanliu Liu", "predicate": "affiliated with", "to": "Institute of Artificial AI and Robotics", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_505", "file_type": "json", "from": "Yuanliu Liu", "predicate": "affiliated_with", "to": "Institute of Artificial Intelligence and Robotics", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_498", "file_type": "json", "from": "Saturation-Preerving Specular Reflection Paper", "predicate": "presented_at", "to": "CVPR", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_498", "file_type": "json", "from": "Saturation-Preerving Specular Reflection Paper", "predicate": "publication_year", "to": "2015", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_498", "file_type": "json", "from": "Zejian Yuan", "predicate": "authored", "to": "Saturation-Preerving Specular Reflection Paper", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_504", "file_type": "json", "from": "Zejian Yuan", "predicate": "associated with", "to": "Yuanliu Liu", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_505", "file_type": "json", "from": "Zejian Yuan", "predicate": "affiliated_with", "to": "Institute of Artificial AI and Robotics", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_498", "file_type": "json", "from": "Nanning Zheng", "predicate": "authored", "to": "Saturation-Preerving Specular Reflection Paper", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_505", "file_type": "json", "from": "Nanning Zheng", "predicate": "affiliated_with", "to": "Institute of Artificial AI and Robotics", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_498", "file_type": "json", "from": "Yang Wu", "predicate": "authored", "to": "Saturation-Preerving Specular Reflection Paper", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_505", "file_type": "json", "from": "Yang Wu", "predicate": "affiliated_with", "to": "Nara Institute of Science and Technology", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_498", "file_type": "json", "from": "Reflection", "predicate": "is_topic_of", "to": "Saturation-Preerving Specular Reflection Paper", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_498", "file_type": "json", "from": "Specular Reflection", "predicate": "is_topic_of", "to": "Saturation-Preserving Specular Reflection Paper", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_501", "file_type": "json", "from": "Specular Reflection", "predicate": "is_separated_by", "to": "Linear Programming", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_501", "file_type": "json", "from": "Specular Reflection", "predicate": "is_related_to", "to": "Diffuse Reflection", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_498", "file_type": "json", "from": "Liu_Saturation-Preerving Specular Reflection Paper", "predicate": "file_path", "to": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Liu_Saturation-Preerving_Specular_Reflection_2015_CVPR_paper.pdf", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_499", "file_type": "json", "from": "Specular reflection", "predicate": "decreases", "to": "saturation of surface colors", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_499", "file_type": "json", "from": "decreased saturation", "predicate": "leads to", "to": "confusion with other colors", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_499", "file_type": "json", "from": "Traditional methods", "predicate": "suffer from", "to": "hue-saturation ambiguity", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_499", "file_type": "json", "from": "Specular-free images", "predicate": "are", "to": "oversaturated", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_499", "file_type": "json", "from": "This paper", "predicate": "proposes", "to": "two-step approach", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_499", "file_type": "json", "from": "two-step approach", "predicate": "produces", "to": "over-saturated specular-free image", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_499", "file_type": "json", "from": "over-saturated specular-free image", "predicate": "is produced through", "to": "global chromaticity propagation", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_499", "file_type": "json", "from": "Saturation", "predicate": "is recovered based on", "to": "piecewise constancy of diffuse chromaticity", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_499", "file_type": "json", "from": "Saturation", "predicate": "is recovered based on", "to": "spatial sparsity/smoothness of specular reflection", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_499", "file_type": "json", "from": "achieved by increasing", "predicate": "uses", "to": "linear programming", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_500", "file_type": "json", "from": "linear programming", "predicate": "is_used_to", "to": "increase achromatic component", "type": "factual", "width": 0.89}, {"arrows": "to", "chunk_id": "doc_0_chunk_500", "file_type": "json", "from": "diffuse chromaticity", "predicate": "has_property", "to": "chromaticity", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_500", "file_type": "json", "from": "specular reflection", "predicate": "has_property", "to": "spatial sparsity", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_500", "file_type": "json", "from": "specular reflection", "predicate": "is_separated_from", "to": "surface colors", "type": "factual", "width": 0.87}, {"arrows": "to", "chunk_id": "doc_0_chunk_500", "file_type": "json", "from": "spatial sparsity", "predicate": "is_property_of", "to": "specular reflection", "type": "conceptual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_500", "file_type": "json", "from": "achromatic component", "predicate": "used_with", "to": "linear programming", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_500", "file_type": "json", "from": "surface colors", "predicate": "has_property", "to": "saturation", "type": "conceptual", "width": 0.91}, {"arrows": "to", "chunk_id": "doc_0_chunk_501", "file_type": "json", "from": "Diffuse Chromaticity", "predicate": "is_component_of", "to": "Linear Programming", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_501", "file_type": "json", "from": "Linear Programming", "predicate": "preserves", "to": "surface color saturation", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_501", "file_type": "json", "from": "Linear Programming", "predicate": "used_in", "to": "reflection component separation", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_501", "file_type": "json", "from": "Shafer, S.", "predicate": "authored", "to": "foundational work", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_501", "file_type": "json", "from": "Artusi, A. et al.", "predicate": "authored", "to": "survey of specular removal methods", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_501", "file_type": "json", "from": "Hue-Saturation Ambiguity", "predicate": "is_addressed_by", "to": "Linear Programming", "type": "factual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_501", "file_type": "json", "from": "Chromaticity Propagation", "predicate": "is_related_to", "to": "Diffuse Chromaticity", "type": "conceptual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_502", "file_type": "json", "from": "Diffuse and specular interface reflections", "predicate": "studied_in", "to": "International Journal of Computer Vision", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_502", "file_type": "json", "from": "Gonzalez \u0026 Woods", "predicate": "authored", "to": "Digital Image Processing", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_502", "file_type": "json", "from": "Land \u0026 McCann", "predicate": "introduced", "to": "retinex theory", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_502", "file_type": "json", "from": "retinex theory", "predicate": "relevant_to", "to": "color constancy", "type": "conceptual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_502", "file_type": "json", "from": "Kim et al.", "predicate": "utilized", "to": "dark channel prior", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_502", "file_type": "json", "from": "dark channel prior", "predicate": "technique_for", "to": "specular reflection separation", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_502", "file_type": "json", "from": "Mallick et al.", "predicate": "explored", "to": "specular surfaces", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_502", "file_type": "json", "from": "Mallick et al.", "predicate": "used", "to": "color information", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_504", "file_type": "json", "from": "specular surfaces", "predicate": "reconstructed using", "to": "color information", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_503", "file_type": "json", "from": "S. P.", "predicate": "authored", "to": "Beyond lambert", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_503", "file_type": "json", "from": "Beyond lambert", "predicate": "explores", "to": "reconstructing specular surfaces", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_503", "file_type": "json", "from": "Zickler", "predicate": "authored", "to": "Beyond lambert", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_503", "file_type": "json", "from": "T.", "predicate": "authored", "to": "Beyond lambert", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_503", "file_type": "json", "from": "P. N. Belhumeur", "predicate": "authored", "to": "Beyond lambert", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_503", "file_type": "json", "from": "D. J. Kriegman", "predicate": "authored", "to": "Beyond lambert", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_504", "file_type": "json", "from": "P., Zickler, T., Belhumeur, P. N., \u0026 Kriegman, D. J.", "predicate": "explores", "to": "specular surfaces", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_504", "file_type": "json", "from": "Lin, S., \u0026 Shum, H.-Y.", "predicate": "addresses", "to": "separation of diffuse and specular reflection", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_504", "file_type": "json", "from": "Tan, R. T., Nishino, K., \u0026 Ikeuchi, K.", "predicate": "addresses", "to": "color constancy", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_504", "file_type": "json", "from": "Mallick, S. P., Zickler, T., Kriegman, D. J., \u0026 Belhumeur, P. N.", "predicate": "presents", "to": "PDE approach", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_504", "file_type": "json", "from": "PDE approach", "predicate": "used for", "to": "specular removal", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_504", "file_type": "json", "from": "diffuse reflection", "predicate": "related to", "to": "specular reflection", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_505", "file_type": "json", "from": "Wuyuan Xie", "predicate": "author_of", "to": "Photometric Stereo with Near Point Lighting", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_505", "file_type": "json", "from": "Photometric Stereo with Near Point Lighting", "predicate": "presents", "to": "PDE approach", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_505", "file_type": "json", "from": "Photometric Stereo with Near Point Lighting", "predicate": "published_in", "to": "CVPR paper", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_505", "file_type": "json", "from": "Chengkai Dai", "predicate": "author_of", "to": "Photometric Stereo with Near Point Lighting", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_505", "file_type": "json", "from": "Charlie C. L. Wang", "predicate": "author_of", "to": "Photometric Stereo with Near Point Lighting", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_505", "file_type": "json", "from": "Xie_Photometric_Stereo_With_2015_CVPR_paper.pdf", "predicate": "is_a", "to": "CVPR paper", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_506", "file_type": "json", "from": "photometric stereo", "predicate": "occurs under", "to": "near point lighting", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_506", "file_type": "json", "from": "near point lighting", "predicate": "introduces", "to": "nonlinear relationship", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_506", "file_type": "json", "from": "nonlinear relationship", "predicate": "links", "to": "local surface normals", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_506", "file_type": "json", "from": "nonlinear relationship", "predicate": "links", "to": "light source positions", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_506", "file_type": "json", "from": "mesh deformation approach", "predicate": "determines", "to": "facet position", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_506", "file_type": "json", "from": "mesh deformation approach", "predicate": "determines", "to": "facet orientation", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_506", "file_type": "json", "from": "mesh deformation approach", "predicate": "decomposes into", "to": "local projection", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_506", "file_type": "json", "from": "mesh deformation approach", "predicate": "decomposes into", "to": "global blending", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_507", "file_type": "json", "from": "Photometric Stereo", "predicate": "requires", "to": "Nonlinear Optimization", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_507", "file_type": "json", "from": "S. Barsky", "predicate": "authored", "to": "4-source photometric stereo technique", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_507", "file_type": "json", "from": "4-source photometric stereo technique", "predicate": "addresses", "to": "highlights and shadows", "type": "factual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_507", "file_type": "json", "from": "D. Nehab", "predicate": "authored", "to": "Efficiently combining positions and normals", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_507", "file_type": "json", "from": "Efficiently combining positions and normals", "predicate": "aims_for", "to": "precise 3d geometry", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_507", "file_type": "json", "from": "A. Hertzmann", "predicate": "authored", "to": "Example-based photometric stereo", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_507", "file_type": "json", "from": "Example-based photometric stereo", "predicate": "reconstructs", "to": "shape", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_507", "file_type": "json", "from": "Shape and spatially-ranging brdfs", "predicate": "derived_from", "to": "photometric stereo", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_507", "file_type": "json", "from": "Mesh Deformation", "predicate": "related_to", "to": "Surface Reconstruction", "type": "conceptual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_507", "file_type": "json", "from": "Near Point Lighting", "predicate": "influences", "to": "Photometric Stereo", "type": "conceptual", "width": 0.7}, {"arrows": "to", "chunk_id": "doc_0_chunk_508", "file_type": "json", "from": "Photometric stereo", "predicate": "is_studied_in", "to": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_508", "file_type": "json", "from": "Photometric stereo", "predicate": "is_studied_in", "to": "Computer Vision Workshops (ICCV Workshops)", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_508", "file_type": "json", "from": "Photometric stereo", "predicate": "uses", "to": "multiple images", "type": "factual", "width": 0.82}, {"arrows": "to", "chunk_id": "doc_0_chunk_508", "file_type": "json", "from": "Photometric stereo", "predicate": "uses", "to": "point light sources", "type": "factual", "width": 0.89}, {"arrows": "to", "chunk_id": "doc_0_chunk_508", "file_type": "json", "from": "Shape", "predicate": "is_studied_in", "to": "Proceedings of the Fifth Eurographics Symposium on Geometry Processing", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_508", "file_type": "json", "from": "Surface orientation", "predicate": "is_determined_by", "to": "photometric method", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_508", "file_type": "json", "from": "Surface modeling", "predicate": "is_approached_by", "to": "as-rigid-as-possible surface modeling", "type": "conceptual", "width": 0.87}, {"arrows": "to", "chunk_id": "doc_0_chunk_508", "file_type": "json", "from": "Discrete geometry", "predicate": "is_shaped_by", "to": "projections", "type": "conceptual", "width": 0.86}, {"arrows": "to", "chunk_id": "doc_0_chunk_508", "file_type": "json", "from": "BRDF", "predicate": "is_studied_in", "to": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "type": "factual", "width": 0.91}, {"arrows": "to", "chunk_id": "doc_0_chunk_509", "file_type": "json", "from": "urographics Symposium on Geometry Processing", "predicate": "held in", "to": "2007", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_509", "file_type": "json", "from": "S. Bouaziz", "predicate": "affiliated with", "to": "urographics Symposium on Geometry Processing", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_510", "file_type": "json", "from": "deep hashing (DH) approach", "predicate": "used_for", "to": "compact binary codes", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_510", "file_type": "json", "from": "existing binary codes learning methods", "predicate": "uses", "to": "single linear projection", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_510", "file_type": "json", "from": "deep neural network", "predicate": "performs", "to": "hierarchical non-linear transformations", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_510", "file_type": "json", "from": "deep neural network", "predicate": "exploits", "to": "nonlinear relationship of samples", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_510", "file_type": "json", "from": "loss minimization", "predicate": "relates_to", "to": "real-valued feature descriptor", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_510", "file_type": "json", "from": "different bits", "predicate": "are", "to": "independent", "type": "factual", "width": 0.82}, {"arrows": "to", "chunk_id": "doc_0_chunk_511", "file_type": "json", "from": "different bits", "predicate": "are independent", "to": "each other", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_511", "file_type": "json", "from": "nary vector", "predicate": "is_minimized", "to": "binary codes", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_511", "file_type": "json", "from": "discriminative term", "predicate": "maximizes", "to": "inter-class variations", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_511", "file_type": "json", "from": "discriminative term", "predicate": "minimizes", "to": "intra-class variations", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_511", "file_type": "json", "from": "learned binary codes", "predicate": "have", "to": "discriminative power", "type": "conceptual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_512", "file_type": "json", "from": "Deep Hashing", "predicate": "is_a", "to": "Binary Codes Learning", "type": "conceptual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_512", "file_type": "json", "from": "Deep Hashing", "predicate": "supports", "to": "Large-Scale Visual Search", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_512", "file_type": "json", "from": "Binary Codes Learning", "predicate": "aims_to", "to": "maximize inter-class variations", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_512", "file_type": "json", "from": "Binary Codes Learning", "predicate": "aims_to", "to": "minimize intra-class variations", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_512", "file_type": "json", "from": "Hashing Functions", "predicate": "used_in", "to": "Deep HHashing", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_664", "file_type": "json", "from": "Andoni \u0026 Indyk (2006)", "predicate": "proposes", "to": "Near-optimal Hashing Algorithms", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_512", "file_type": "json", "from": "Near-optimal Hashing Algorithms", "predicate": "enables", "to": "Approximate Nearest Neighbor Search", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_512", "file_type": "json", "from": "Gong et al. (2012)", "predicate": "proposes", "to": "Angular Quantization-based Binary Codes", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_512", "file_type": "json", "from": "Angular Quantization-based Binary Codes", "predicate": "facilitates", "to": "Fast Similarity Search", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_512", "file_type": "json", "from": "Hinton \u0026 Salakhutdinov (2006)", "predicate": "investigates", "to": "Reducing Data Dimensionality", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_513", "file_type": "json", "from": "Neural Networks", "predicate": "reduces", "to": "data dimensionality", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_551", "file_type": "json", "from": "Neural Networks", "predicate": "provides", "to": "background knowledge", "type": "conceptual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_557", "file_type": "json", "from": "Neural Networks", "predicate": "used_for", "to": "Object Recognition", "type": "factual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_513", "file_type": "json", "from": "Tiny Images", "predicate": "used_for", "to": "nonparametric object recognition", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_513", "file_type": "json", "from": "Hash Bit Selection", "predicate": "provides", "to": "unified solution", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_513", "file_type": "json", "from": "Shift-invariant kernels", "predicate": "generates", "to": "locality-sensitive binary codes", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_513", "file_type": "json", "from": "Minimal Loss Hashing", "predicate": "creates", "to": "compact binary codes", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_513", "file_type": "json", "from": "Science", "predicate": "is_publication_venue", "to": "research paper", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_514", "file_type": "json", "from": "Torralba, A.", "predicate": "authored", "to": "80 million tiny images", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_514", "file_type": "json", "from": "80 million tiny images", "predicate": "is_published_in", "to": "*PAM*I", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_514", "file_type": "json", "from": "Wang, J.", "predicate": "authored", "to": "Semi-supervised hashing", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_514", "file_type": "json", "from": "Venice Erin Liong", "predicate": "affiliated_with", "to": "Advanced Digital Sciences Center", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_514", "file_type": "json", "from": "Gang Wang", "predicate": "affiliated_with", "to": "School of Electrical and Electronic Engineering", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_514", "file_type": "json", "from": "Jie Zhou", "predicate": "affiliated_with", "to": "Department of Automation", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_515", "file_type": "json", "from": "Department of Automation", "predicate": "is_part_of", "to": "Tsinghua University", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_514", "file_type": "json", "from": "Dongyoon Han", "predicate": "is_author_of", "to": "Unsupervised Simultaneous Orthogonal Basis Clustering Feature Selection", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_515", "file_type": "json", "from": "Dongyoon Han", "predicate": "affiliated_with", "to": "Department of Automation", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_515", "file_type": "json", "from": "Dongyoon Han", "predicate": "email", "to": "jzhou@tsinghua.edu.cn", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_515", "file_type": "json", "from": "Unsupervised Simultaneous Orthogonal Basis Clustering Feature Selection", "predicate": "is_topic_of", "to": "Han_Unpublished_Simultaneous_Orthogonal_2015_CVPR_paper", "type": "conceptual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_515", "file_type": "json", "from": "Han_Unsupervised_Simultaneous_Orthogonal_2015_CVPR_paper", "predicate": "publication_venue", "to": "CVPR", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_515", "file_type": "json", "from": "Han_Unsupervised_Simultaneous_Orthogonal_2015_CVPR_paper", "predicate": "year", "to": "2015", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_516", "file_type": "json", "from": "feature selection methods", "predicate": "is_characterized_as", "to": "supervised and unsupervised feature selection methods", "type": "conceptual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_516", "file_type": "json", "from": "SOCFS", "predicate": "is_a", "to": "unsupervised feature selection method", "type": "conceptual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_516", "file_type": "json", "from": "SOCFS", "predicate": "designed_for", "to": "feature selection on unlabeled data", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_516", "file_type": "json", "from": "SOCFS", "predicate": "uses", "to": "regularized regression-based formulation", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_516", "file_type": "json", "from": "SOCFS", "predicate": "achieves", "to": "state-of-the-art results", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_516", "file_type": "json", "from": "SOCFS", "predicate": "performs_on", "to": "real world datasets", "type": "factual", "width": 0.91}, {"arrows": "to", "chunk_id": "doc_0_chunk_516", "file_type": "json", "from": "regularized regression-based formulation", "predicate": "uses", "to": "target matrix", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_516", "file_type": "json", "from": "target matrix", "predicate": "captures", "to": "latent cluster centers", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_516", "file_type": "json", "from": "target matrix", "predicate": "guides", "to": "projection matrix", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_516", "file_type": "json", "from": "projection matrix", "predicate": "selects", "to": "discriminative features", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_555", "file_type": "json", "from": "projection matrix", "predicate": "has_property", "to": "sparse nature", "type": "conceptual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_518", "file_type": "json", "from": "Nie et al.", "predicate": "authored", "to": "feature selection via joint l2,1-norms minimization", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_518", "file_type": "json", "from": "feature selection via joint l2,1-norms minimization", "predicate": "published in", "to": "NIPS", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_518", "file_type": "json", "from": "Nene et al.", "predicate": "authored", "to": "Columbia object image library (coil-20)", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_518", "file_type": "json", "from": "Columbia object image library (coil-20)", "predicate": "is_technical_report", "to": "CCUCS-005-96", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_518", "file_type": "json", "from": "Yang et al.", "predicate": "authored", "to": "l2,1-norm regularized discriminative feature selection", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_518", "file_type": "json", "from": "Qian and Zhai", "predicate": "authored", "to": "Robust unsupervised feature selection", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_518", "file_type": "json", "from": "Robust unsupervised feature selection", "predicate": "published in", "to": "IJCAI", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_518", "file_type": "json", "from": "Sch\u00a8onemann", "predicate": "authored", "to": "generalized solution of the orthogonal Procustes problem", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_518", "file_type": "json", "from": "Zhao and Liu", "predicate": "authored", "to": "Spectral feature selection", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_518", "file_type": "json", "from": "Spectral feature selection", "predicate": "published in", "to": "ICML", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_519", "file_type": "json", "from": "Procrustes problem", "predicate": "published_in", "to": "Psychometrika", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_519", "file_type": "json", "from": "Zhao, Z.", "predicate": "authored", "to": "Spectral feature selection", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_519", "file_type": "json", "from": "Samaria, F. S.", "predicate": "authored", "to": "stochastic model", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_519", "file_type": "json", "from": "Jianming Zhang", "predicate": "authored", "to": "Salient Object Subitizing", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_521", "file_type": "json", "from": "Jianming Zhang", "predicate": "affiliated_with", "to": "Boston University", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_519", "file_type": "json", "from": "Shugao Ma", "predicate": "authored", "to": "Salient Object Subitizing", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_521", "file_type": "json", "from": "Shugao Ma", "predicate": "affiliated_with", "to": "Boston University", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_519", "file_type": "json", "from": "Mehrnooush Sameki", "predicate": "authored", "to": "Salient Object Subitizing", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_519", "file_type": "json", "from": "Stan Sclaroff", "predicate": "authored", "to": "Salient Object Subitizing", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_521", "file_type": "json", "from": "Stan Sclaroff", "predicate": "affiliated_with", "to": "Boston University", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_519", "file_type": "json", "from": "Margrit Betke", "predicate": "authored", "to": "Salient Object Subitizing", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_521", "file_type": "json", "from": "Margrit Betke", "predicate": "affiliated_with", "to": "Boston University", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_519", "file_type": "json", "from": "Radom\u00edr M\u011bch", "predicate": "authored", "to": "Salient Object Subitizing", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_520", "file_type": "json", "from": "People", "predicate": "can", "to": "subitizing", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_520", "file_type": "json", "from": "Salient Object Subitizing (SOS)", "predicate": "aims_to", "to": "predict existence and number", "type": "conceptual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_521", "file_type": "json", "from": "Salient Object Subitizing (SOS)", "predicate": "has_application_in", "to": "salient object detection", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_521", "file_type": "json", "from": "Salient Object Subitizing (SOS)", "predicate": "utilizes", "to": "Convolutional Neural Networks (CNNs)", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_521", "file_type": "json", "from": "Crowd Sourcing", "predicate": "used_for", "to": "Dataset Creation", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_521", "file_type": "json", "from": "Holistic Image Analysis", "predicate": "encompasses", "to": "Salient Object Subitizing (SOS)", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_521", "file_type": "json", "from": "Computer Vision Applications", "predicate": "includes", "to": "object detection", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_521", "file_type": "json", "from": "Computer Vision Applications", "predicate": "includes", "to": "robot vision", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_521", "file_type": "json", "from": "Mehrnoosh Sameki", "predicate": "affiliated_with", "to": "Boston University", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_522", "file_type": "json", "from": "Radom\u00b4\u0131r M\u02d8ech", "predicate": "affiliated_with", "to": "Adobe Research", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_522", "file_type": "json", "from": "Discriminaitve Shape from Shading", "predicate": "is_paper_title", "to": "Richter_Discriminaitve_Shape_From_2015_CVPR_paper.pdf", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_522", "file_type": "json", "from": "Discriminaitve Shape from Shading", "predicate": "addresses_topic", "to": "Shape from Shading", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_522", "file_type": "json", "from": "Richter_Discriminaitve_Shape_From_2015_CVPR_paper.pdf", "predicate": "is_publication", "to": "CVPR", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_523", "file_type": "json", "from": "Estimating surface normals", "predicate": "is_a", "to": "challenging problem", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_523", "file_type": "json", "from": "Estimating surface normals", "predicate": "is", "to": "under-constrained problem", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_523", "file_type": "json", "from": "Simplifying assumptions", "predicate": "example_includes", "to": "directional lighting", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_523", "file_type": "json", "from": "Simplifying assumptions", "predicate": "example_includes", "to": "known reflectance maps", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_523", "file_type": "json", "from": "regression forests", "predicate": "incorporates", "to": "Von Mises-Fisher distributions", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_523", "file_type": "json", "from": "spatial features", "predicate": "example_includes", "to": "textons", "type": "factual", "width": 0.86}, {"arrows": "to", "chunk_id": "doc_0_chunk_523", "file_type": "json", "from": "spatial features", "predicate": "example_includes", "to": "novel silhouette features", "type": "factual", "width": 0.84}, {"arrows": "to", "chunk_id": "doc_0_chunk_523", "file_type": "json", "from": "generalization", "predicate": "relates_to", "to": "uncalibrated illumination", "type": "factual", "width": 0.87}, {"arrows": "to", "chunk_id": "doc_0_chunk_524", "file_type": "json", "from": "pixel-independent prediction", "predicate": "achieves", "to": "efficient estimation", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_524", "file_type": "json", "from": "Discrimiative Learning", "predicate": "is_topic", "to": "research area", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_524", "file_type": "json", "from": "J. T. Barron", "predicate": "authored", "to": "Color constancy, intrinsic images, and shape estimation", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_524", "file_type": "json", "from": "J. T. Barron", "predicate": "authored", "to": "Shape, albedo, and illumination from a single image", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_524", "file_type": "json", "from": "J. Ben-Arie", "predicate": "authored", "to": "A neural network approach", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_524", "file_type": "json", "from": "L. Breiman", "predicate": "authored", "to": "Random forests", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_524", "file_type": "json", "from": "ShapeCollage", "predicate": "interprets", "to": "shape", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_524", "file_type": "json", "from": "ShapeCollage", "predicate": "uses", "to": "example-based methods", "type": "factual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_525", "file_type": "json", "from": "ShapeCollage", "predicate": "presented_at", "to": "ECCV", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_525", "file_type": "json", "from": "ShapeCollage", "predicate": "addresses", "to": "occlusion-aware shape interpretation", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_525", "file_type": "json", "from": "Image-to-geometry registration", "predicate": "method_uses", "to": "mutual information", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_525", "file_type": "json", "from": "Image-to-geometry registration", "predicate": "exploits", "to": "geometric properties", "type": "factual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_525", "file_type": "json", "from": "Modeling data", "predicate": "uses", "to": "directional distributions", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_525", "file_type": "json", "from": "Dispersion", "predicate": "published_in", "to": "P. Roy. Soc. Lond. B", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_525", "file_type": "json", "from": "Floating scale reconstruction", "predicate": "presented_at", "to": "SIGGRAPH", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_530", "file_type": "json", "from": "SIGGRAPH", "predicate": "is_conference_for", "to": "Debevec et al.", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_526", "file_type": "json", "from": "Adelson", "predicate": "authored", "to": "Ground truth dataset", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_526", "file_type": "json", "from": "W. T. Freeman", "predicate": "authored", "to": "Ground truth dataset", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_526", "file_type": "json", "from": "Jean-Dominique FAVREAU", "predicate": "authored", "to": "Line Drawing Interpretation", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_530", "file_type": "json", "from": "Jean-Dominique FAVREAU", "predicate": "is_affiliated_with", "to": "INRIA Sophia-Antippolis", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_526", "file_type": "json", "from": "Line Drawing Interpretation", "predicate": "presented_at", "to": "CVPR", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_529", "file_type": "json", "from": "Line Drawing Interpretation", "predicate": "related_to", "to": "3D Scene Understanding", "type": "conceptual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_526", "file_type": "json", "from": "Adrien Bousseau", "predicate": "authored", "to": "Line Drawing Interpretation", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_530", "file_type": "json", "from": "Adrien Bousseau", "predicate": "is_affiliated_with", "to": "INRIA Sophia-Antipollis", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_531", "file_type": "json", "from": "Adrien Bousseau", "predicate": "works_at", "to": "INRIAL Sophia-Antipolis", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_527", "file_type": "json", "from": "line drawings", "predicate": "of", "to": "imaginary objects", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_527", "file_type": "json", "from": "line drawings", "predicate": "drawn over", "to": "photographs", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_574", "file_type": "json", "from": "photographs", "predicate": "contains", "to": "desired scene", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_574", "file_type": "json", "from": "photographs", "predicate": "contains", "to": "undesired reflections", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_527", "file_type": "json", "from": "computer vision algorithms", "predicate": "offer", "to": "limited support", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_527", "file_type": "json", "from": "multi-view stereo algorithms", "predicate": "reconstruct", "to": "real-world scenes", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_527", "file_type": "json", "from": "line-drawing interpretation algorithms", "predicate": "lack", "to": "contextual awareness", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_527", "file_type": "json", "from": "polygon", "predicate": "belongs to", "to": "orientation", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_528", "file_type": "json", "from": "new structures", "predicate": "present_in", "to": "real world", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_528", "file_type": "json", "from": "furniture design", "predicate": "is_example_of", "to": "application domain", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_528", "file_type": "json", "from": "archaeology", "predicate": "is_example_of", "to": "application domain", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_528", "file_type": "json", "from": "new orientation", "predicate": "allows", "to": "creation of new structures", "type": "conceptual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_528", "file_type": "json", "from": "new orientation", "predicate": "is", "to": "unknown orientation", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_529", "file_type": "json", "from": "Computer-Aided Design", "predicate": "facilitates", "to": "creation of new structures", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_529", "file_type": "json", "from": "Furniture Design", "predicate": "is_example_of", "to": "Computer-Aided Design application", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_529", "file_type": "json", "from": "O-snap", "predicate": "is_a", "to": "optimization-based snapping method", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_529", "file_type": "json", "from": "M. Arikan", "predicate": "author_of", "to": "O-snap", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_529", "file_type": "json", "from": "H. Barrow", "predicate": "author_of", "to": "Line Drawing Interpretation", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_529", "file_type": "json", "from": "Y. Boykov", "predicate": "author_of", "to": "energy minimization algorithms", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_529", "file_type": "json", "from": "A.-L. Chauve", "predicate": "author_of", "to": "3D reconstruction methods", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_529", "file_type": "json", "from": "Multi-View Stereo Reconstruction", "predicate": "uses", "to": "energy minimization algorithms", "type": "conceptual", "width": 0.7}, {"arrows": "to", "chunk_id": "doc_0_chunk_530", "file_type": "json", "from": "Furukawa et al.", "predicate": "presented_at", "to": "Manhattan-world stereo", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_530", "file_type": "json", "from": "Wiley-ISTE", "predicate": "published", "to": "Stochastic geometry for image analysis", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_530", "file_type": "json", "from": "Florent LAFARGE", "predicate": "is_affiliated_with", "to": "INRIA Sophia-Antipollis", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_531", "file_type": "json", "from": "Florent LAFARGE", "predicate": "works_at", "to": "INRIA Sophia-Antipolis", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_530", "file_type": "json", "from": "Bis Publishers", "predicate": "published", "to": " Sketching: The Basics", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_531", "file_type": "json", "from": "jean-dominuque.favreau@inria.fr", "predicate": "works_at", "to": "INRIA Sophia-Antipolis", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_531", "file_type": "json", "from": "Olga Russakovsky", "predicate": "is_author_of", "to": "Best of both worlds", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_534", "file_type": "json", "from": "Olga Russakovsky", "predicate": "affiliated_with", "to": "Stanford University", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_531", "file_type": "json", "from": "Best of both worlds", "predicate": "is_a", "to": "CVPR paper", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_531", "file_type": "json", "from": "Li-Jia Li", "predicate": "is_author_of", "to": "Best of both worlds", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_534", "file_type": "json", "from": "Li-Jia Li", "predicate": "works_at", "to": "Snapchat", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_531", "file_type": "json", "from": "Li Fei-Fei", "predicate": "is_author_of", "to": "Best of both worlds", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_534", "file_type": "json", "from": "Li Fei-Fei", "predicate": "affiliated_with", "to": "Stanford University", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_531", "file_type": "json", "from": "Russakovsky_Best_of_Both_2015_CVPR_paper.pdf", "predicate": "is_an_example_of", "to": "CVPR paper", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_531", "file_type": "json", "from": "jean-dominique.favreau@inria.fr", "predicate": "is_email_of", "to": "jean-dominique.favreau", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_531", "file_type": "json", "from": "florent.lafarge@inria.fr", "predicate": "is_email_of", "to": "Florent LAFARGE", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_531", "file_type": "json", "from": "adrien.bousseau@inria.fr", "predicate": "is_email_of", "to": "Adrien Bousseau", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_532", "file_type": "json", "from": "manual annotation", "predicate": "is", "to": "quite expensive", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_532", "file_type": "json", "from": "manual annotation", "predicate": "influenced by", "to": "crowd engineering innovations", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_532", "file_type": "json", "from": "automatic object detectors", "predicate": "detect", "to": "at most a few objects per image", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_533", "file_type": "json", "from": "object annotations", "predicate": "informed_by", "to": "human feedback", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_533", "file_type": "json", "from": "object annotations", "predicate": "informed_by", "to": "computer vision", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_533", "file_type": "json", "from": "computer vision models", "predicate": "provides", "to": "object annotations", "type": "conceptual", "width": 0.89}, {"arrows": "to", "chunk_id": "doc_0_chunk_533", "file_type": "json", "from": "human input", "predicate": "source_of", "to": "feedback", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_534", "file_type": "json", "from": "Visesh Chari", "predicate": "authored", "to": "On Pairwise Costs for Network Flow Multi-Object Tracking", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_536", "file_type": "json", "from": "Visesh Chari", "predicate": "is_affiliated_with", "to": "INRIA", "type": "factual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_535", "file_type": "json", "from": "Visesh Chari", "predicate": "affiliated_with", "to": "Ecole Normale Sup\u00b4erieure", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_534", "file_type": "json", "from": "Ivan Laptev", "predicate": "authored", "to": "On Pairwise Costs for Network Flow Multi-Object Tracking", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_536", "file_type": "json", "from": "Ivan Laptev", "predicate": "is_affiliated_with", "to": "INRIA", "type": "factual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_534", "file_type": "json", "from": "Josef Sivic", "predicate": "authored", "to": "On Pairwise Costs for Network Flow Multi-Object Tracking", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_536", "file_type": "json", "from": "Josef Sivic", "predicate": "is_affiliated_with", "to": "INRIA", "type": "factual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_534", "file_type": "json", "from": "Chari_On_Pairwise_Costs_2015_CVPR_supplemental", "predicate": "is_supplement_to", "to": "On Pairwise Costs for Network Flow Multi-Object Tracking", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_535", "file_type": "json", "from": "Multi-object Tracking", "predicate": "uses", "to": "Network Flow Optimization", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_536", "file_type": "json", "from": "Multi-object Tracking", "predicate": "is_field_of", "to": "Computer Vision", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_535", "file_type": "json", "from": "Network Flow Optimization", "predicate": "models", "to": "dependencies among tracks", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_535", "file_type": "json", "from": "Pairwise Costs", "predicate": "addresses", "to": "object detector failures", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_535", "file_type": "json", "from": "Pairwise Costs", "predicate": "introduced_to", "to": "min-cost network flow framework", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_535", "file_type": "json", "from": "Convex Relaxation", "predicate": "includes", "to": "efficient rounding heuristic", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_535", "file_type": "json", "from": "pairwise costs", "predicate": "evaluated_in", "to": "real-world video sequences", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_535", "file_type": "json", "from": "pairwise costs", "predicate": "improves", "to": "recent tracking methods", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_535", "file_type": "json", "from": "Tracking-by-Detection", "predicate": "uses", "to": "pairwise costs", "type": "conceptual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_536", "file_type": "json", "from": "Tracking-by-Detection", "predicate": "is_approach_in", "to": "Multi-object Tracking", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_536", "file_type": "json", "from": "TILDE", "predicate": "is_method", "to": "Temporally Invariant Learned Detector", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_536", "file_type": "json", "from": "Yannick Verdie", "predicate": "is_author_of", "to": "TILDE", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_539", "file_type": "json", "from": "Yannick Verdie", "predicate": "affiliated_with", "to": "Computer Vision Laboratory, EPFL", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_536", "file_type": "json", "from": "Kwang Moo Yi", "predicate": "is_author_of", "to": "TILDE", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_539", "file_type": "json", "from": "Kwang Moo Yi", "predicate": "affiliated_with", "to": "Computer Vision Laboratory, EPFL", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_536", "file_type": "json", "from": "Pascal Fua", "predicate": "is_author_of", "to": "TILDE", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_536", "file_type": "json", "from": "Vincent Lepetit", "predicate": "is_author_of", "to": "TILDE", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_536", "file_type": "json", "from": "Simon Lacoste-Julien", "predicate": "is_affiliated_with", "to": "INRIO", "type": "factual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_536", "file_type": "json", "from": "Cortes, C.", "predicate": "contributed_to", "to": "Support-Vector Networks", "type": "factual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_594", "file_type": "json", "from": "Cortes, C.", "predicate": "co_author_of", "to": "Vapnik, V.", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_537", "file_type": "json", "from": "Support-Vector Networks", "predicate": "published in", "to": "Machine Learning", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_536", "file_type": "json", "from": "Vapnik, V.", "predicate": "contributed_to", "to": "Support-Vector Networks", "type": "factual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_536", "file_type": "json", "from": "Harris, C.", "predicate": "contributed_to", "to": "Combined Corner and Edge Detector", "type": "factual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_536", "file_type": "json", "from": "Stephens, M.", "predicate": "contributed_to", "to": "Combined Corner and Edge Detector", "type": "factual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_537", "file_type": "json", "from": "C.", "predicate": "authored", "to": "Support-Vector Networks", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_537", "file_type": "json", "from": "Corner Detector", "predicate": "developed by", "to": "Harris, C.", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_537", "file_type": "json", "from": "Corner Detector", "predicate": "developed by", "to": "Stephens, M.", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_537", "file_type": "json", "from": "Hinging Hyperplanes", "predicate": "developed by", "to": "Breiman, L.", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_537", "file_type": "json", "from": "Hinging Hyperplanes", "predicate": "published in", "to": "IEEE Transactions on Information Theory", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_537", "file_type": "json", "from": "Gradient-Based Learning", "predicate": "applied to", "to": "Document Recognition", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_537", "file_type": "json", "from": "Af\ufb01ne Region Detectors", "predicate": "compared in", "to": "International Journal of Computer Vision", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_537", "file_type": "json", "from": "Mikolajczyk, K.", "predicate": "authored", "to": "A Comparison of Af\ufb01ne Region Detectors", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_538", "file_type": "json", "from": "rs", "predicate": "authored", "to": "A Comparison of Af\ufb01ne Region Detectors", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_538", "file_type": "json", "from": "Zisserma", "predicate": "authored", "to": "A Comparison of Af\ufb01ne Region Detectors", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_538", "file_type": "json", "from": "Mata", "predicate": "authored", "to": "A Comparison of Af\ufb01ne Region Detectors", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_538", "file_type": "json", "from": "Schaffalitzky", "predicate": "authored", "to": "A Comparison of Af\ufb01ne Region Detectors", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_538", "file_type": "json", "from": "Kadir", "predicate": "authored", "to": "A Comparison of Af\ufb01ne Region Detectors", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_538", "file_type": "json", "from": "Van Gool", "predicate": "authored", "to": "A Review of Af\ufb01ne Region Detectors", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_538", "file_type": "json", "from": "Dollar", "predicate": "authored", "to": "Supervised Learning of Edges and Object Boundaries", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_538", "file_type": "json", "from": "Tu", "predicate": "authored", "to": "Supervised Learning of Edges and Object Boundaries", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_538", "file_type": "json", "from": "Belongie", "predicate": "authored", "to": "Supervised Learning of Edges and Object Boundaries", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_538", "file_type": "json", "from": "Rosten", "predicate": "authored", "to": "Machine Learning for High-Speed Corner Detection", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_538", "file_type": "json", "from": "Drummond", "predicate": "authored", "to": "Machine Learning for High-Speed Corner Detection", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_538", "file_type": "json", "from": "Lowe", "predicate": "authored", "to": "Distinctive Image Features from Scale-Invariant Keypoints", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_639", "file_type": "json", "from": "Fan", "predicate": "developed", "to": "LIBLINEAR", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_639", "file_type": "json", "from": "Fan", "predicate": "published_in", "to": "Journal of Machine Learning Research", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_538", "file_type": "json", "from": "Chang", "predicate": "authored", "to": "LIBLINEAR", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_543", "file_type": "json", "from": "Chang", "predicate": "deals_with", "to": "tracking", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_538", "file_type": "json", "from": "Hsieh", "predicate": "authored", "to": "LIBLINEAR", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_538", "file_type": "json", "from": "Lin", "predicate": "authored", "to": "LIBLINEAR", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_538", "file_type": "json", "from": "Verdie", "predicate": "affiliated_with", "to": "EPFL", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_538", "file_type": "json", "from": "Yi", "predicate": "affiliated_with", "to": "EPFL", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_539", "file_type": "json", "from": "Vincent Le Petit", "predicate": "affiliated_with", "to": "Institute for Computer Graphics and Vision, Graz University of Technology", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_539", "file_type": "json", "from": "JOTS", "predicate": "is_a", "to": "Joint Online Tracking and Segmentation", "type": "conceptual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_540", "file_type": "json", "from": "Joint Online Tracking and Segmentation", "predicate": "addresses", "to": "Video Segmentation task", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_540", "file_type": "json", "from": "Joint Online Tracking and Segmentation", "predicate": "integrates", "to": "Multi-part tracking", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_540", "file_type": "json", "from": "Joint Online Tracking and Segmentation", "predicate": "integrates", "to": "Segmentation", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_540", "file_type": "json", "from": "Joint Online Tracking and Segmentation", "predicate": "uses", "to": "Energy Function Optimization", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_540", "file_type": "json", "from": "Joint Online Tracking and Segmentation", "predicate": "demonstrated_effectiveness_on", "to": "SegTrack database", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_540", "file_type": "json", "from": "Joint Online Tracking and Segmentation", "predicate": "demonstrated_effectiveness_on", "to": "SegTrack v2 database", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_539", "file_type": "json", "from": "Longyin Wen", "predicate": "is_author_of", "to": "JOTS", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_543", "file_type": "json", "from": "Longyin Wen", "predicate": "affiliated_with", "to": "NLPR, Institute of Automation, Chinese Academy of Sciences", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_539", "file_type": "json", "from": "Dawei Du", "predicate": "is_author_of", "to": "JETS", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_543", "file_type": "json", "from": "Dawei Du", "predicate": "affiliated_with", "to": "SCCE, University of Chinese Academy of Sciences", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_539", "file_type": "json", "from": "Zhen Lei", "predicate": "is_author_of", "to": "JOTS", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_543", "file_type": "json", "from": "Zhen Lei", "predicate": "affiliated_with", "to": "NLPR, Institute of Automation, Chinese Academy of Sciences", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_583", "file_type": "json", "from": "Zhen Lei", "predicate": "is_author_of", "to": "High-Fidelity Pose and Expression Normalization for Face Recognition in the Wild", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_584", "file_type": "json", "from": "Zhen Lei", "predicate": "is_author_of", "to": "High-Fidelity Pose and Expression Normalization for FaceRecognition in the Wild", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_589", "file_type": "json", "from": "Zhen Lei", "predicate": "affiliated_with", "to": "Center for Biometrics and Security Research", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_589", "file_type": "json", "from": "Zhen Lei", "predicate": "affiliated_with", "to": "National Laboratory of Pattern Recognition", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_539", "file_type": "json", "from": "Stan Z. Li", "predicate": "is_author_of", "to": "JOTS", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_544", "file_type": "json", "from": "Stan Z. Li", "predicate": "affiliated_with", "to": "NLPR", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_590", "file_type": "json", "from": "Stan Z. Li", "predicate": "affiliated_with", "to": "Center for Biometrics and Security Research", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_590", "file_type": "json", "from": "Stan Z. Li", "predicate": "affiliated_with", "to": "National Laboratory of Pattern Recognition", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_590", "file_type": "json", "from": "Stan Z. Li", "predicate": "email", "to": "szli@nlpr.ia.ac.cn", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_539", "file_type": "json", "from": "Wen_JOTS_Joint_Online_2015_CVPR_paper.pdf", "predicate": "is_publication_of", "to": "CVPR", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_540", "file_type": "json", "from": "Tracking and Segmentation stages", "predicate": "optimized_using", "to": "RANSA-style approach", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_540", "file_type": "json", "from": "Multi-part Models", "predicate": "related_to", "to": "Tracking and Segmentation", "type": "conceptual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_542", "file_type": "json", "from": "multi-target tracking", "predicate": "is_task_of", "to": "video analysis", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_542", "file_type": "json", "from": "topological constraints", "predicate": "enhances", "to": "tracking", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_543", "file_type": "json", "from": "Vasconcelos", "predicate": "affiliated_with", "to": "NLPR, Institute of Automation, Chinese Academy of Sciences", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_543", "file_type": "json", "from": "Vasconcelos", "predicate": "addresses", "to": "tracking deformable and occluded objects", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_543", "file_type": "json", "from": "S. Z. Li", "predicate": "affiliated_with", "to": "NLPR, Institute of Automation, Chinese Academy of Sciences", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_543", "file_type": "json", "from": "Delong", "predicate": "provides", "to": "optimization method", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_544", "file_type": "json", "from": "NLPR", "predicate": "located_in", "to": "Chinese Academy of Sciences", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_544", "file_type": "json", "from": "Philipp Kr\u00e4henbuhl", "predicate": "authored", "to": "Learning to Propose Objects", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_544", "file_type": "json", "from": "Learning to Propose Objects", "predicate": "authored", "to": "Vladlen Koltun", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_544", "file_type": "json", "from": "Learning to Propose Objects", "predicate": "is_publication_of", "to": "CVPR", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_544", "file_type": "json", "from": "Krahenbuhl_Learning_to_Propos_2015_CVPR_paper.pdf", "predicate": "represents", "to": "Learning to Propose Objects", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_544", "file_type": "json", "from": "zlei", "predicate": "affiliated_with", "to": "NLPR", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_545", "file_type": "json", "from": "ensemble", "predicate": "trained", "to": "jointly", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_546", "file_type": "json", "from": "ensemble", "predicate": "operates_on", "to": "elementary image features", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_546", "file_type": "json", "from": "ensemble", "predicate": "enables", "to": "rapid image analysis", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_546", "file_type": "json", "from": "ensemble", "predicate": "has", "to": "composition", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_546", "file_type": "json", "from": "ensemble", "predicate": "has", "to": "parameters", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_545", "file_type": "json", "from": "ensemble training", "predicate": "reduced_to", "to": "sequence of uncapacitated facility location problems", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_545", "file_type": "json", "from": "procedure", "predicate": "optimizes", "to": "size of the ensemble", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_545", "file_type": "json", "from": "procedure", "predicate": "optimizes", "to": "composition", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_636", "file_type": "json", "from": "procedure", "predicate": "is_scalable", "to": "set of cliques", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_636", "file_type": "json", "from": "procedure", "predicate": "activates", "to": "cliques", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_545", "file_type": "json", "from": "ensembles", "predicate": "operate_on", "to": "elementary image features", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_546", "file_type": "json", "from": "presented approach", "predicate": "outperforms", "to": "prior object proposal algorithms", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_546", "file_type": "json", "from": "presented approach", "predicate": "has", "to": "lowest running time", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_546", "file_type": "json", "from": "presented approach", "predicate": "capable_of_learning", "to": "bottom-up segmentation model", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_546", "file_type": "json", "from": "presented approach", "predicate": "learns", "to": "generally applicable model", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_547", "file_type": "json", "from": "trained ensembles", "predicate": "generalize across", "to": "datasets", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_546", "file_type": "json", "from": "bottom-up segmentation", "predicate": "is_a", "to": "model", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_547", "file_type": "json", "from": "Ensemble Methods", "predicate": "improves", "to": "running time", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_547", "file_type": "json", "from": "Arbel\u00e1ez et al. (2012)", "predicate": "authored", "to": "Semantic segmentation using regions and parts", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_547", "file_type": "json", "from": "Carreira et al.", "predicate": "authored", "to": "Free-form region description with second-order pooling", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_548", "file_type": "json", "from": "Free-form region description with second-order pooling", "predicate": "published_in", "to": "PAMI", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_547", "file_type": "json", "from": "Microsoft COCO", "predicate": "contains", "to": "common objects", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_547", "file_type": "json", "from": "Objectness", "predicate": "is measured by", "to": "Alexe et al.", "type": "factual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_548", "file_type": "json", "from": "Structured forests", "predicate": "presented_in", "to": "ICCV", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_548", "file_type": "json", "from": "BING", "predicate": "presented_in", "to": "CVPR", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_548", "file_type": "json", "from": "Geometry of cuts and metrics", "predicate": "published_by", "to": "Springer", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_548", "file_type": "json", "from": "Aravindh Mahendran", "predicate": "is_author_of", "to": "Understanding Deep Image Representations", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_549", "file_type": "json", "from": "Aravindh Mahendran", "predicate": "authored", "to": "Understanding Deep Image Representations by Inverting Them", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_549", "file_type": "json", "from": "Aravindh Mahendran", "predicate": "affiliated_with", "to": "Intel Labs", "type": "factual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_548", "file_type": "json", "from": "Andrea Vedaldi", "predicate": "is_author_of", "to": "Understanding Deep Image Representations", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_549", "file_type": "json", "from": "Andrea Vedaldi", "predicate": "authored", "to": "Understanding Deep Image Representations by Inverting Them", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_549", "file_type": "json", "from": "Understanding Deep Image Representations by Inverting Them", "predicate": "is_publication_of", "to": "CVPR", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_549", "file_type": "json", "from": "Understanding Deep Image Representations by Inverting Them", "predicate": "concerns", "to": "Deep Image Representations", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_549", "file_type": "json", "from": "Understanding Deep Image Presentations by Inverting Them", "predicate": "year", "to": "2015", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_549", "file_type": "json", "from": "Mahendran_Understanding_Deep_Image_2015_CVPR_paper.pdf", "predicate": "is_file_of", "to": "cvpr_papers", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_551", "file_type": "json", "from": "Image Representations", "predicate": "exhibits", "to": "geometric and photometric invariance", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_551", "file_type": "json", "from": "Bishop", "predicate": "authored", "to": "Neural Networks for Pattern Recognition", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_552", "file_type": "json", "from": "deformable part models", "predicate": "is_used_in", "to": "object detection", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_552", "file_type": "json", "from": "3D textons", "predicate": "used_in", "to": "material recognition", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_553", "file_type": "json", "from": "SIFT detector", "predicate": "is_implementation", "to": "open-source implementation", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_552", "file_type": "json", "from": "SIFT detector", "predicate": "is_part_of", "to": "image processing tasks", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_553", "file_type": "json", "from": "Zeiler \u0026 Fergus", "predicate": "focuses_on", "to": "visualizing convolutional networks", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_553", "file_type": "json", "from": "Hinton \u0026 Salakhutdinov", "predicate": "discusses", "to": "dimensionality reduction", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_553", "file_type": "json", "from": "Wang et al.", "predicate": "introduces", "to": "locality-constrained linear coding", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_553", "file_type": "json", "from": "Arvindh Mahendran", "predicate": "affiliated_with", "to": "University of Oxford", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_553", "file_type": "json", "from": "Yan Xia", "predicate": "author_of", "to": "Sparse Projections for High-Dimensional Binary Codes", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_554", "file_type": "json", "from": "Yan Xia", "predicate": "is_author_of", "to": "Sparse Projections", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_560", "file_type": "json", "from": "Yan Xia", "predicate": "affiliated_with", "to": "University of Science and Technology of China", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_553", "file_type": "json", "from": "Pushmeet Kohli", "predicate": "author_of", "to": "Sparse Projections for High-Dimensional Binary Codes", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_667", "file_type": "json", "from": "Pushmeet Kohli", "predicate": "affiliated_with", "to": "Microsoft Research", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_661", "file_type": "json", "from": "Pushmeet Kohli", "predicate": "author_of", "to": "Computationally Bounded Retrieval", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_554", "file_type": "json", "from": "Sparse Projections", "predicate": "is_topic_of", "to": "CVPR paper", "type": "conceptual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_555", "file_type": "json", "from": "problem of learning long binary codes", "predicate": "has_challenge", "to": "lack of effective regularizer", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_555", "file_type": "json", "from": "sparsity encouraging regularizer", "predicate": "reduces", "to": "number of parameters", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_555", "file_type": "json", "from": "sparsity encouraging regularizer", "predicate": "reduces", "to": "overfitting", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_555", "file_type": "json", "from": "sparse projection matrix", "predicate": "leads_to", "to": "reduction in computational cost", "type": "causal", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_556", "file_type": "json", "from": "dense projections", "predicate": "includes", "to": "ITQ", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_556", "file_type": "json", "from": "rix", "predicate": "leads_to", "to": "reduction in computational cost", "type": "causal", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_556", "file_type": "json", "from": "other methods", "predicate": "speeds_up", "to": "high-dimensional binary encoding", "type": "causal", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_557", "file_type": "json", "from": "Agrawal et al. (2014)", "predicate": "provides", "to": "Foundational context", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_557", "file_type": "json", "from": "Agrawal et al. (2014)", "predicate": "focuses_on", "to": "Neural Networks", "type": "factual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_597", "file_type": "json", "from": "Object Recognition", "predicate": "uses", "to": "Local Scale-Invariant Features", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_557", "file_type": "json", "from": "Fan et al. (2008)", "predicate": "introduces", "to": "Liblinear", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_665", "file_type": "json", "from": "Liblinear", "predicate": "is_a", "to": "library", "type": "conceptual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_559", "file_type": "json", "from": "binary code learning", "predicate": "is_technique_for", "to": "efficient similarity search", "type": "conceptual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_559", "file_type": "json", "from": "approximate nearest neighbor search", "predicate": "is_task_in", "to": "many applications", "type": "conceptual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_559", "file_type": "json", "from": "sparse approximation techniques", "predicate": "can_be_used_for", "to": "feature selection", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_559", "file_type": "json", "from": "sparse approximation techniques", "predicate": "can_be_used_for", "to": "dimensionality reduction", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_559", "file_type": "json", "from": "product quantization", "predicate": "is_used_for", "to": "approximate nearest neighbor search", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_559", "file_type": "json", "from": "atomic decomposition", "predicate": "uses", "to": "basis pursuit", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_559", "file_type": "json", "from": "basis pursuit", "predicate": "is_method_for", "to": "atomic decomposition", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_559", "file_type": "json", "from": "hashing algorithms", "predicate": "aim_for", "to": "approximate nearest neighbor search", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_560", "file_type": "json", "from": "ear-optimal hashing algorithms", "predicate": "used_for", "to": "approximate nearest neighbor search", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_560", "file_type": "json", "from": "ear-optimal hashing algorithms", "predicate": "presented_in", "to": "FOCS", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_560", "file_type": "json", "from": "locality-sensitive hashing", "predicate": "is_technique_for", "to": "approximate nearest neighbor search", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_560", "file_type": "json", "from": "locality-sensitive hashing", "predicate": "presented_in", "to": "Symposium on Computational Geometry", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_560", "file_type": "json", "from": "Procrustes analysis", "predicate": "is_method_for", "to": "finding optimal transformation", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_560", "file_type": "json", "from": "Procrustes problems", "predicate": "published_by", "to": "Oxford University Press", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_561", "file_type": "json", "from": "Huazhu Fu", "predicate": "affiliated_with", "to": "Microsoft Research", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_561", "file_type": "json", "from": "Dong Xu", "predicate": "affiliated_with", "to": "Microsoft Research", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_561", "file_type": "json", "from": "Stephen Lin", "predicate": "affiliated_with", "to": "Microsoft Research", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_561", "file_type": "json", "from": "Jiang Liu", "predicate": "affiliated_with", "to": "Microsoft Research", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_561", "file_type": "json", "from": "Object-based RGBD Image Co-segmentation with Mutex Constraint", "predicate": "is_a", "to": "CVPR paper", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_561", "file_type": "json", "from": "Object-based RGBD Image Co-segmentation with Mutex Constraint", "predicate": "authored_by", "to": "Huazhu Fu", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_561", "file_type": "json", "from": "Object-based RGBD Image Co-segmentation with Mutux Constraint", "predicate": "authored_by", "to": "Dong Xu", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_561", "file_type": "json", "from": "Object-based RGBD Image Co-segmentation with Mutux Constraint", "predicate": "authored_by", "to": "Stephen Lin", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_562", "file_type": "json", "from": "depth channel", "predicate": "enhances", "to": "identification of similar foreground objects", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_562", "file_type": "json", "from": "co-segmentation", "predicate": "formulated_in", "to": "fully-connected graph structure", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_562", "file_type": "json", "from": "graph structure", "predicate": "includes", "to": "mutex constraints", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_563", "file_type": "json", "from": "mutex constraints", "predicate": "prevents", "to": "improper solutions", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_562", "file_type": "json", "from": "object-based RGBD co-segmentation", "predicate": "outperforms", "to": "related methods", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_563", "file_type": "json", "from": "object-based RGBD co-segmentation", "predicate": "incorporates", "to": "mutex constraints", "type": "conceptual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_563", "file_type": "json", "from": "object-based RGBD co-segmentation", "predicate": "improves", "to": "segmentation accuracy", "type": "factual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_563", "file_type": "json", "from": "RGBD co-segmentation", "predicate": "outperforms", "to": "related techniques", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_563", "file_type": "json", "from": "comparable performance", "predicate": "relates_to", "to": "RGB co-segmentation techniques", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_564", "file_type": "json", "from": "RGB co-segmentation techniques", "predicate": "utilizes", "to": "depth maps", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_563", "file_type": "json", "from": "depth maps", "predicate": "estimated_from", "to": "RGB images", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_564", "file_type": "json", "from": "Depth maps", "predicate": "estimated from", "to": "RGB images", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_564", "file_type": "json", "from": "Object-Based Methods", "predicate": "is_a", "to": "Method", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_564", "file_type": "json", "from": "Mutual Exclusion Constraints", "predicate": "is_a", "to": "Method", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_564", "file_type": "json", "from": "Co-Saliency Maps", "predicate": "is_a", "to": "Method", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_564", "file_type": "json", "from": "Graph Formulation", "predicate": "is_a", "to": "Method", "type": "conceptual", "width": 0.87}, {"arrows": "to", "chunk_id": "doc_0_chunk_564", "file_type": "json", "from": "Multi-Plane Block-Coordinate Frank-Wilfe Algorithm", "predicate": "trains", "to": "Structural SVMs", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_564", "file_type": "json", "from": "Multi-Plane Block-Coordinate Frank-Wilfe Algorithm", "predicate": "uses", "to": "max-Oracle", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_564", "file_type": "json", "from": "Neel Shah", "predicate": "is_author_of", "to": "paper", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_566", "file_type": "json", "from": "Neel Shah", "predicate": "affiliated_with", "to": "IST Austria", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_564", "file_type": "json", "from": "Vladimir Kolmogorov", "predicate": "is_author_of", "to": "paper", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_566", "file_type": "json", "from": "Vladimir Kolmogorov", "predicate": "affiliated_with", "to": "IST Austria", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_564", "file_type": "json", "from": "Chris H. Lampert", "predicate": "is_author_of", "to": "paper", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_566", "file_type": "json", "from": "Chris H. Lampert", "predicate": "affiliated_with", "to": "IST Australia", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_565", "file_type": "json", "from": "Structural Support Vector Machines", "predicate": "is_effective_for", "to": "structured computer vision tasks", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_566", "file_type": "json", "from": "Structural Support Vector Machines", "predicate": "uses", "to": "Max-Oracle", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_566", "file_type": "json", "from": "Structural Support Vector Machines", "predicate": "uses", "to": "Frank-Wolfe Algorithm", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_566", "file_type": "json", "from": "Structural Support Vector Machines", "predicate": "uses", "to": "Block-Coordinate Methods", "type": "factual", "width": 0.82}, {"arrows": "to", "chunk_id": "doc_0_chunk_565", "file_type": "json", "from": "Frank-Wolfe algorithm", "predicate": "is_designed_for", "to": "training SSVMs", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_565", "file_type": "json", "from": "Frank-Wolfe algorithm", "predicate": "combines_with", "to": "caching mechanism", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_565", "file_type": "json", "from": "criterion", "predicate": "decides_whether_to", "to": "call max-oracle", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_565", "file_type": "json", "from": "max-oracle", "predicate": "is", "to": "bottleneck", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_566", "file_type": "json", "from": "Max-Oracle", "predicate": "is", "to": "optimization technique", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_567", "file_type": "json", "from": "Beier_Fusion_Moves_for_2015_CVPR_supplemental", "predicate": "is_supplemental_material_for", "to": "Fusion Moves for Correlation Clustering", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_567", "file_type": "json", "from": "PIVOT-BOEM", "predicate": "is_algorithm", "to": "Correlation Clustering", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_567", "file_type": "json", "from": "HC", "predicate": "is_algorithm", "to": "Correlation Clustering", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_567", "file_type": "json", "from": "CGC", "predicate": "is_algorithm", "to": "Correlation Clustering", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_567", "file_type": "json", "from": "Fusion Moves", "predicate": "is_variant_of", "to": "Correlation Clustering", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_567", "file_type": "json", "from": "Experimental Analysis", "predicate": "evaluates", "to": "Algorithms", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_567", "file_type": "json", "from": "Anytime Algorithms", "predicate": "exhibits", "to": "Anytime Behavior", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_567", "file_type": "json", "from": "Anytime Behavior", "predicate": "describes", "to": "Progressive Improvement", "type": "conceptual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_567", "file_type": "json", "from": "Dataset Performance", "predicate": "evaluated_on", "to": "Instances", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_568", "file_type": "json", "from": "University of Heidelberg (Iwr)", "predicate": "affiliation_of", "to": "Thorsten Beier", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_568", "file_type": "json", "from": "University of Heidelberg (Iwr)", "predicate": "affiliation_of", "to": "Fred A. Hamprecht", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_568", "file_type": "json", "from": "Mohammadreza Mostajabi", "predicate": "is_author_of", "to": "Feedforward Semantic Segmentation", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_572", "file_type": "json", "from": "Mohammadreza Mostajabi", "predicate": "affiliated_with", "to": "Toyota Technological Institute at Chicago", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_568", "file_type": "json", "from": "Feedforward Semantic Segmentation", "predicate": "published_in", "to": "CVPR", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_568", "file_type": "json", "from": "Payman Yadollahpour", "predicate": "is_author_of", "to": "Feedforward Semantic Segmentation", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_572", "file_type": "json", "from": "Payman Yadollahpour", "predicate": "affiliated_with", "to": "Toyota Technological Institute at Chicago", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_568", "file_type": "json", "from": "Gregory Shakhnarovich", "predicate": "is_author_of", "to": "Feedforward Semantic Segmentation", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_569", "file_type": "json", "from": "Gregory Shakhnarovich", "predicate": "authors", "to": "Mostajabi_Feedforward_Semantic_Segmentation_2015_CVPR_paper", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_572", "file_type": "json", "from": "Gregory Shakhnarovich", "predicate": "affiliated_with", "to": "Toyota Technological Institute at Chicago", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_568", "file_type": "json", "from": "University of Heidelberg (Department of Mathematics)", "predicate": "affiliation_of", "to": "J\u00f6rg H. Kappes", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_569", "file_type": "json", "from": "Mostajabi_Feedforwad_Semantic_Segmentation_2015_CVPR_paper", "predicate": "introduces", "to": "feed-forward architecture", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_569", "file_type": "json", "from": "feed-forward architecture", "predicate": "aims_to", "to": "semantic segmentation", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_569", "file_type": "json", "from": "image elements", "predicate": "mapped_to", "to": "rich feature representations", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_569", "file_type": "json", "from": "nested regions", "predicate": "obtained_by", "to": "zoom-out", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_569", "file_type": "json", "from": "zoom-out", "predicate": "starts_from", "to": "superpixel", "type": "factual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_571", "file_type": "json", "from": "Hypercolumns for object segmentation and fine-grained localization", "predicate": "is_publication_type", "to": "arXiv preprint", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_571", "file_type": "json", "from": "Hypercolumns for object segmentation and fine-grained localization", "predicate": "addresses", "to": "object segmentation", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_571", "file_type": "json", "from": "Chen et al.", "predicate": "authored", "to": "Semantic image segmentation with deep convolutional nets and fully connected crfs", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_571", "file_type": "json", "from": "Semantic image segmentation with deep convolutional nets and fully connected crfs", "predicate": "is_publication_type", "to": "arXiv preprint", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_571", "file_type": "json", "from": "Long et al.", "predicate": "authored", "to": "Fully convolutional networks for semantic segmentation", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_571", "file_type": "json", "from": "Fully convolutional networks for semantic segmentation", "predicate": "is_publication_type", "to": "arXiv preprint", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_571", "file_type": "json", "from": "Fully convolutional networks for semantic segmentation", "predicate": "addresses", "to": "semantic segmentation", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_571", "file_type": "json", "from": "Associative hierarchical CRFs for object class image segmentation", "predicate": "presented_at", "to": "ICCV", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_571", "file_type": "json", "from": "Carreira and Sminchisescu", "predicate": "authored", "to": "CPMC: Automatic object segmentation using constrained parametric min-cuts", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_571", "file_type": "json", "from": "CPMC: Automatic object segmentation using constrained parametric min-cuts", "predicate": "published_in", "to": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_572", "file_type": "json", "from": "CPMC", "predicate": "published_in", "to": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_572", "file_type": "json", "from": "Carreira, J.", "predicate": "authored", "to": "CPMC", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_572", "file_type": "json", "from": "Sminchisescu, C.", "predicate": "authored", "to": "CPMC", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_572", "file_type": "json", "from": "Zisserma, A.", "predicate": "authored", "to": "Very deep convolutional networks", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_572", "file_type": "json", "from": "Fr\u00b4edo Durand", "predicate": "authored", "to": "Reflection Removal using Ghosting Cues", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_573", "file_type": "json", "from": "Fr\u00b4edo Durand", "predicate": "is_author_of", "to": "Ghosting Cues", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_576", "file_type": "json", "from": "Fr\u00b4edo Durand", "predicate": "affiliated_with", "to": "MIT CSAIL", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_572", "file_type": "json", "from": "YiChang Shih", "predicate": "authored", "to": "Reflection Removal using Ghosting Cues", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_573", "file_type": "json", "from": "YiChang Shih", "predicate": "is_author_of", "to": "Ghosting Cues", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_576", "file_type": "json", "from": "YiChang Shih", "predicate": "affiliated_with", "to": "MIT CSAIL", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_572", "file_type": "json", "from": "Dilip Krishnan", "predicate": "authored", "to": "Reflection Removal using Ghosting Cues", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_573", "file_type": "json", "from": "Dilip Krishnan", "predicate": "is_author_of", "to": "Ghosting Cues", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_576", "file_type": "json", "from": "Dilip Krishnan", "predicate": "affiliated_with", "to": "Google Research", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_572", "file_type": "json", "from": "William T. Freeman", "predicate": "authored", "to": "Reflection Removal using Ghosting Cues", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_573", "file_type": "json", "from": "William T. Freeman", "predicate": "is_author_of", "to": "Ghosting Cunes", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_684", "file_type": "json", "from": "William T. Freeman", "predicate": "affilates_with", "to": "MIT CSAIL", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_683", "file_type": "json", "from": "William T. Freeman", "predicate": "email", "to": "billf@mit.edu", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_573", "file_type": "json", "from": "Ghosting Cues", "predicate": "is_discussed_in", "to": "Shih_Reflection_Removal_Using_2015_CVPR_paper", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_573", "file_type": "json", "from": "Shih_Reflection_Removal_Using_2015_CVPR_paper", "predicate": "is_publication", "to": "CVPR paper", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_573", "file_type": "json", "from": "Shih_Reflection_Removal_Using_2015_CVPR_paper", "predicate": "is_authored_by", "to": "YiChang Shih", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_573", "file_type": "json", "from": "Shih_Reflection_Removal_Using_2015_CVPR_paper", "predicate": "addresses", "to": "reflection removal", "type": "factual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_575", "file_type": "json", "from": "reflection removal", "predicate": "occurs_on", "to": "synthetic inputs", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_575", "file_type": "json", "from": "reflection removal", "predicate": "occurs_on", "to": "real-world inputs", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_574", "file_type": "json", "from": "layer separation", "predicate": "is", "to": "ill-posed problem", "type": "conceptual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_574", "file_type": "json", "from": "ghosting cues", "predicate": "exploits", "to": "asymmetry", "type": "conceptual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_574", "file_type": "json", "from": "ghosting cues", "predicate": "is", "to": "barely perceptible", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_574", "file_type": "json", "from": "ghosting cues", "predicate": "arises from", "to": "shifted double reflections", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_574", "file_type": "json", "from": "ghosted reflection", "predicate": "modeled using", "to": "double-impulse convolution kernel", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_575", "file_type": "json", "from": "ghosted reflection components", "predicate": "exhibits", "to": "relative attenuation", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_576", "file_type": "json", "from": "Reflection Removal", "predicate": "applied_to", "to": "synthetic inputs", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_576", "file_type": "json", "from": "Reflection Removal", "predicate": "applied_to", "to": "real-world inputs", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_576", "file_type": "json", "from": "Soonmin Hwang", "predicate": "author_of", "to": "Multispectral Pedestrian Detection", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_579", "file_type": "json", "from": "Soonmin Hwang", "predicate": "is", "to": "author", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_576", "file_type": "json", "from": "Multispectral Pedestrian Detection", "predicate": "is", "to": "Benchmark Dataset", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_577", "file_type": "json", "from": "pedestrian datasets", "predicate": "focus_on", "to": "color channel", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_577", "file_type": "json", "from": "thermal channel", "predicate": "is_helpful_for", "to": "detection", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_577", "file_type": "json", "from": "multispectral pedestrian dataset", "predicate": "addresses", "to": "limitation", "type": "causal", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_577", "file_type": "json", "from": "multispectral pedestrian dataset", "predicate": "provides", "to": "color-thermal image pairs", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_577", "file_type": "json", "from": "multispectral pedestrian dataset", "predicate": "is_as_large_as", "to": "previous color-based datasets", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_577", "file_type": "json", "from": "multispectral pedestrian dataset", "predicate": "provides", "to": "dense annotations", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_578", "file_type": "json", "from": "color-thermal image pairs", "predicate": "is_a", "to": "image type", "type": "conceptual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_577", "file_type": "json", "from": "multispectral ACF", "predicate": "is_extension_of", "to": "aggregated channel features (ACF)", "type": "conceptual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_577", "file_type": "json", "from": "multispectral ACF", "predicate": "handles", "to": "color-thermal image pairs", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_577", "file_type": "json", "from": "multispectral ACF", "predicate": "reduces", "to": "average miss rate of ACF", "type": "causal", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_578", "file_type": "json", "from": "spectral ACF", "predicate": "is_extension_of", "to": "aggregated channel features", "type": "conceptual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_578", "file_type": "json", "from": "spectral ACF", "predicate": "handles", "to": "color-thermal image pairs", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_578", "file_type": "json", "from": "aggregated channel features", "predicate": "is_a", "to": "image feature", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_578", "file_type": "json", "from": "Multispectral ACF", "predicate": "reduces", "to": "average miss rate", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_578", "file_type": "json", "from": "Multispectral ACF", "predicate": "reduces_by", "to": "15%", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_578", "file_type": "json", "from": "Multispectral ACF", "predicate": "improves", "to": "pedestrian detection", "type": "causal", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_579", "file_type": "json", "from": "Multispectral ACF", "predicate": "reduces", "to": "miss rate", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_579", "file_type": "json", "from": "Multispectral ACF", "predicate": "handles", "to": "color-thermal image pairs", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_579", "file_type": "json", "from": "Multispectral ACF", "predicate": "achieves", "to": "breakthrough in pedestrian detection", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_579", "file_type": "json", "from": "RGBD-Fusion", "predicate": "is", "to": "real-time", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_579", "file_type": "json", "from": "RGBD-Fusion", "predicate": "provides", "to": "high precision", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_579", "file_type": "json", "from": "Jaesik Park", "predicate": "is", "to": "author", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_579", "file_type": "json", "from": "Namil Kim", "predicate": "is", "to": "author", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_579", "file_type": "json", "from": "Roy Or", "predicate": "is", "to": "author", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_583", "file_type": "json", "from": "Roy Or", "predicate": "affiliated with", "to": "Technion, Israel Institute of Technology", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_580", "file_type": "json", "from": "RGB-D scanners", "predicate": "has_limitation", "to": "subtle details", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_581", "file_type": "json", "from": "RGB-D scanners", "predicate": "utilizes", "to": "Depth map enhancement", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_580", "file_type": "json", "from": "lighting model", "predicate": "handles", "to": "natural scene illumination", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_580", "file_type": "json", "from": "lighting model", "predicate": "integrated_into", "to": "shape from shading-like technique", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_580", "file_type": "json", "from": "detailed geometry", "predicate": "calculated_via", "to": "method", "type": "factual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_580", "file_type": "json", "from": "evidence", "predicate": "supports", "to": "improvement in depth", "type": "factual", "width": 0.87}, {"arrows": "to", "chunk_id": "doc_0_chunk_581", "file_type": "json", "from": "Depth map enhancement", "predicate": "relies_on", "to": "Shape from shading", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_581", "file_type": "json", "from": "Depth map enhancement", "predicate": "improves", "to": "depth", "type": "factual", "width": 0.82}, {"arrows": "to", "chunk_id": "doc_0_chunk_581", "file_type": "json", "from": "Shape from shading", "predicate": "influenced_by", "to": "Lighting models", "type": "conceptual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_581", "file_type": "json", "from": "Shape from shading", "predicate": "studied_in", "to": "Computer Vision, Graphics, and Image Processing", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_582", "file_type": "json", "from": "Shape from shading", "predicate": "is_a", "to": "computer vision technique", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_581", "file_type": "json", "from": "Real-time processing", "predicate": "requires", "to": "Depth map enhancement", "type": "conceptual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_581", "file_type": "json", "from": "Lambertian reflectance", "predicate": "described_in", "to": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_581", "file_type": "json", "from": "Bayesian nonparametric intrinsic image decomposition", "predicate": "presented_in", "to": "European Conference on Computer Vision", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_581", "file_type": "json", "from": "Variable-source shading analysis", "predicate": "published_in", "to": "International Journal of Computer Vision", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_582", "file_type": "json", "from": "Variable-source shading analysis", "predicate": "is_a", "to": "computer vision technique", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_582", "file_type": "json", "from": "Grosse, R.", "predicate": "authored", "to": "Ground-truth dataset", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_582", "file_type": "json", "from": "Ground-truth dataset", "predicate": "used_for", "to": "intrinsic image algorithms", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_582", "file_type": "json", "from": "Han, Y.", "predicate": "authored", "to": "High quality shape", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_582", "file_type": "json", "from": "High quality shape", "predicate": "derived_from", "to": "RGB-D image", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_582", "file_type": "json", "from": "Horn, B. K.", "predicate": "authored", "to": "PhD thesis", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_582", "file_type": "json", "from": "Horn, B. K.", "predicate": "coauthored", "to": "The variational approach", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_583", "file_type": "json", "from": "Horn \u0026 Brooks", "predicate": "authored", "to": "The variational approach to shape from shading", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_583", "file_type": "json", "from": "Horn \u0026 Brooks", "predicate": "published in", "to": "Computer Vision, Graphics, and Image Processing", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_583", "file_type": "json", "from": "Johnson \u0026 Adelison", "predicate": "presented at", "to": "IEEE Conference on Computer Vision and Pattern Recognition", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_583", "file_type": "json", "from": "Guy Rosman", "predicate": "affiliated with", "to": "Computer Science and Artificial Intelligence Lab, MIT", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_583", "file_type": "json", "from": "Aaron Wetzler", "predicate": "affiliated with", "to": "Technion, Israel Institute of Technology", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_583", "file_type": "json", "from": "Ron Kimmel", "predicate": "affiliated with", "to": "Technion, Israel Institute of Technology", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_583", "file_type": "json", "from": "Alfred M. Bruckstein", "predicate": "affiliated with", "to": "Technion, Israel Institute of Technology", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_584", "file_type": "json", "from": "Xiangyu Zhu", "predicate": "is_author_of", "to": "High-Fidelity Pose and Expression Normalization for Face Recognition in the Wild", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_589", "file_type": "json", "from": "Xiangyu Zhu", "predicate": "affiliated_with", "to": "Center for Biometrics and Security Research", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_589", "file_type": "json", "from": "Xiangyu Zhu", "predicate": "affiliated_with", "to": "National Laboratory of Pattern Recognition", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_584", "file_type": "json", "from": "High-Fidelity Pose and Expression Normalization for Face Recognition in the Wild", "predicate": "is_author_of", "to": "Stan Z. Li", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_584", "file_type": "json", "from": "High-Fidelity Pose and Expression Normalization for Face Recognition in the Wild", "predicate": "is_paper", "to": "cvpr_papers", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_583", "file_type": "json", "from": "Junnie Yan", "predicate": "is_author_of", "to": "High-Fidelity Pose and Expression Normalization for Face Recognition in the Wild", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_584", "file_type": "json", "from": "Junjie Yan", "predicate": "is_author_of", "to": "High-Fidelity Pose and Expression Normalization for Face Recognition in the Wild", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_589", "file_type": "json", "from": "Junjie Yan", "predicate": "affiliated_with", "to": "Center for Biomatrics and Security Research", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_590", "file_type": "json", "from": "Junjie Yan", "predicate": "affiliated_with", "to": "National Laboratory of Pattern Recognition", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_590", "file_type": "json", "from": "Junjie Yan", "predicate": "affiliated_with", "to": "Center for Biometrics and Security Research", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_590", "file_type": "json", "from": "Junjie Yan", "predicate": "email", "to": "jjyan@nlpr.ia.ac.cn", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_584", "file_type": "json", "from": "Dong Yi", "predicate": "is_author_of", "to": "High-Fidelity Pose and Expression Normalization for Face Recognition in the Wild", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_590", "file_type": "json", "from": "Dong Yi", "predicate": "affiliated_with", "to": "Center for Biometrics and Security Research", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_590", "file_type": "json", "from": "Dong Yi", "predicate": "affiliated_with", "to": "National Laboratory of Pattern Recognition", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_584", "file_type": "json", "from": "freddy@cs.technion.ac.il", "predicate": "affiliated_with", "to": "Technion - Israel Institute of Technology", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_584", "file_type": "json", "from": "Zhu_High-Fidelity_Pose_and_2015_CVPR_paper.pdf", "predicate": "is_file_of", "to": "cvpr_papers", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_585", "file_type": "json", "from": "face recognition performance", "predicate": "impacted_by", "to": "pose variations", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_585", "file_type": "json", "from": "HPEN method", "predicate": "addresses", "to": "challenge", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_585", "file_type": "json", "from": "HPEN method", "predicate": "utilizes", "to": "3D Morphable Model", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_585", "file_type": "json", "from": "HPEN method", "predicate": "generates", "to": "face images", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_585", "file_type": "json", "from": "HPEN method", "predicate": "involves", "to": "landmark marching", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_585", "file_type": "json", "from": "HPEN method", "predicate": "involves", "to": "3DMM fitting", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_585", "file_type": "json", "from": "HPEN method", "predicate": "involves", "to": "3D meshing", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_585", "file_type": "json", "from": "HPEN method", "predicate": "involves", "to": "Poisson Editing", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_585", "file_type": "json", "from": "face images", "predicate": "has_pose", "to": "frontal pose", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_585", "file_type": "json", "from": "face images", "predicate": "has_expression", "to": "neutral expression", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_586", "file_type": "json", "from": "Poisson Editing", "predicate": "used_for", "to": "inpainting", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_585", "file_type": "json", "from": "Multi-PIE", "predicate": "is_used_in", "to": "experiments", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_586", "file_type": "json", "from": "Multi-PIE", "predicate": "is_a", "to": "dataset", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_587", "file_type": "json", "from": "Amberg, B.", "predicate": "authored", "to": "Optimal step non-rigid icp algorithms for surface registration", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_587", "file_type": "json", "from": "Romdhani, S.", "predicate": "authored", "to": "Optimal step non-rigid icp algorithms for surface registration", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_587", "file_type": "json", "from": "Vetter, T.", "predicate": "authored", "to": "Optimal step non-rigid icp algorithms for surface registration", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_587", "file_type": "json", "from": "Chai, X.", "predicate": "authored", "to": "Locally linear regression for pose-invariant face recognition", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_587", "file_type": "json", "from": "Shan, S.", "predicate": "authored", "to": "Locally linear regression for pose-invariant face recognition", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_587", "file_type": "json", "from": "Gao, W.", "predicate": "authored", "to": "Locably linear regression for pose-invariant face recognition", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_587", "file_type": "json", "from": "Arashloo, S. R.", "predicate": "authored", "to": "Pose-invariant face matching using MRF energy minimization framework", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_587", "file_type": "json", "from": "Kittler, J.", "predicate": "authored", "to": "Pose-invariant face matching using MRF energy minimization framework", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_587", "file_type": "json", "from": "Chan, C. H.", "predicate": "authored", "to": "Multisculse local phase quantization for robust component-based face recognition", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_587", "file_type": "json", "from": "Tahir, M. A.", "predicate": "authored", "to": "Multisculse local phase quantization for robust component-based face recognition", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_588", "file_type": "json", "from": "local phase quantization", "predicate": "is_used_for", "to": "robust component-based face recognition", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_588", "file_type": "json", "from": "component-based face recognition", "predicate": "uses", "to": "kernel fusion", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_588", "file_type": "json", "from": "kernel fusion", "predicate": "combines", "to": "multiple descriptors", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_588", "file_type": "json", "from": "Chen, D. (2012)", "predicate": "authored", "to": "Bayesian face revisited", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_588", "file_type": "json", "from": "high-dimensional feature", "predicate": "benefits", "to": "efficient compression", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_588", "file_type": "json", "from": "Asthana, A. (2013)", "predicate": "authored", "to": "discriminative response map fitting", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_588", "file_type": "json", "from": "discriminative response map fitting", "predicate": "uses", "to": "constrained local models", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_588", "file_type": "json", "from": "2013 IEEE Conference on Computer Vision and Pattern Recognition", "predicate": "hosts", "to": "high-dimensional feature compression", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_589", "file_type": "json", "from": "Cheng, S.", "predicate": "authored", "to": "Robust discriminative response map fitting", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_598", "file_type": "json", "from": "Robust discriminative response map fitting", "predicate": "utilizes", "to": "constrained local models", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_589", "file_type": "json", "from": "Barkan, O.", "predicate": "authored", "to": "Fast high dimensional vector multiplication face recognition", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_589", "file_type": "json", "from": "Weill, J.", "predicate": "authored", "to": "Fast high dimensional vector multiplication face recognition", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_589", "file_type": "json", "from": "Wolf, L.", "predicate": "authored", "to": "Fast high dimensional vector multiplication face recognition", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_589", "file_type": "json", "from": "Aronowitz, H.", "predicate": "authored", "to": "Fast high dimensional vector multiplication face recognition", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_590", "file_type": "json", "from": "Parsing Occluded People", "predicate": "is_paper", "to": "CVPR paper", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_590", "file_type": "json", "from": "Parsing Occluded People", "predicate": "located_in", "to": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_590", "file_type": "json", "from": "Xianjie Chen", "predicate": "is_author_of", "to": "Parsing Occluded People", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_590", "file_type": "json", "from": "Alan Yuille", "predicate": "is_author_of", "to": "Parsing Occluded People", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_591", "file_type": "json", "from": "graphical model", "predicate": "has", "to": "tree structure", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_591", "file_type": "json", "from": "connected subtree", "predicate": "is", "to": "flexible composition", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_591", "file_type": "json", "from": "inference", "predicate": "requires", "to": "search over models", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_591", "file_type": "json", "from": "inference", "predicate": "exploits", "to": "part sharing", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_592", "file_type": "json", "from": "computations", "predicate": "is", "to": "twice as many", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_592", "file_type": "json", "from": "searching", "predicate": "searches_for", "to": "entire object", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_592", "file_type": "json", "from": "Stickmen dataset", "predicate": "is", "to": "standard benchmarked dataset", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_592", "file_type": "json", "from": "alternative algorithms", "predicate": "are", "to": "best", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_592", "file_type": "json", "from": "modeling", "predicate": "avoids", "to": "occlusion", "type": "conceptual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_593", "file_type": "json", "from": "Graphical models", "predicate": "related_to", "to": "object detection", "type": "conceptual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_594", "file_type": "json", "from": "Yuilie, A.", "predicate": "co_author_of", "to": "Chen, X.", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_594", "file_type": "json", "from": "Ferrari, V.", "predicate": "co_author_of", "to": "Marin-Jimenez, M.", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_594", "file_type": "json", "from": "human pose estimation", "predicate": "uses", "to": "progressive search space reduction", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_594", "file_type": "json", "from": "support-vector networks", "predicate": "is_method_in", "to": "machine learning", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_595", "file_type": "json", "from": "Cortes", "predicate": "authored", "to": "Support-vector networks", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_595", "file_type": "json", "from": "Dalal", "predicate": "authored", "to": "Histograms of oriented gradients", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_595", "file_type": "json", "from": "Triggs", "predicate": "authored", "to": "Histograms of oriented gradients", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_595", "file_type": "json", "from": "Sapp", "predicate": "authored", "to": "Adaptive pose priors", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_595", "file_type": "json", "from": "Jordan", "predicate": "authored", "to": "Adaptive pose priors", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_595", "file_type": "json", "from": "Taskar", "predicate": "authored", "to": "Adaptive pose priors", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_595", "file_type": "json", "from": "Yuille", "predicate": "affiliated_with", "to": "University of California, Los Angeles", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_595", "file_type": "json", "from": "Alabort-i-Medina", "predicate": "authored", "to": "Unifying Holistic and Parts-Based Deformable Model Fitting", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_596", "file_type": "json", "from": "deformable models", "predicate": "captures", "to": "degrees of freedom", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_597", "file_type": "json", "from": "Face Alignment", "predicate": "utilizes", "to": "Holistic Deformable Models", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_597", "file_type": "json", "from": "Face Alignment", "predicate": "utilizes", "to": "Parts-Based Deformable Models", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_597", "file_type": "json", "from": "Holistic Deformable Models", "predicate": "is_type_of", "to": "Deformable Models", "type": "conceptual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_597", "file_type": "json", "from": "Parts-Based Deformable Models", "predicate": "is_type_of", "to": "Deformable Models", "type": "conceptual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_597", "file_type": "json", "from": "Active Appearance Models", "predicate": "related_to", "to": "Face Alignment", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_597", "file_type": "json", "from": "Active Appearance Models", "predicate": "described_in", "to": "T. F. Cootes, G. J. Edwards, and C. J. Taylor (2001)", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_597", "file_type": "json", "from": "Active Shape Models", "predicate": "related_to", "to": "Face Alignment", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_597", "file_type": "json", "from": "Active Shape Models", "predicate": "described_in", "to": "T. F. Cootes, C. J. Taylor, D. H. Cooper, and J. Graham (1995)", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_597", "file_type": "json", "from": "Lucas-Kanade Method", "predicate": "related_to", "to": "Face Alignment", "type": "conceptual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_597", "file_type": "json", "from": "Bayesian Active Appearance Models", "predicate": "described_in", "to": "J. Alabort-i-Medina and S. Zafeiriou (2014)", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_598", "file_type": "json", "from": "Conference on Computer Vision and Pattern Recognition (CVPR)", "predicate": "held_in", "to": "2014", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_598", "file_type": "json", "from": "Lucas-Kanade", "predicate": "published_in", "to": "International Journal of Computer Vision (IJCR)", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_598", "file_type": "json", "from": "Lucas-Kanade", "predicate": "is_a", "to": "unifying framework", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_598", "file_type": "json", "from": "X. Cao", "predicate": "authored", "to": "Face alignment by explicit shape regression", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_598", "file_type": "json", "from": "Face alignment by explicit shape regression", "predicate": "presented_at", "to": "Conference on Computer Vision and Pattern Recognition (CVPR)", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_598", "file_type": "json", "from": "G. Papandreou", "predicate": "authored", "to": "Adaptive and constrained algorithms", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_598", "file_type": "json", "from": "Adaptive and constrained algorithms", "predicate": "used_for", "to": "inverse compositional active appearance model fitting", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_598", "file_type": "json", "from": "A. Asthana", "predicate": "authored", "to": "Robust discriminative response map fitting", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_599", "file_type": "json", "from": "A. Asthana", "predicate": "affiliated_with", "to": "Conference on Computer Vision and Pattern Recognition (CVPR)", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_599", "file_type": "json", "from": "S. Zafeiriou", "predicate": "affiliated_with", "to": "Conference on Computer Vision and Pattern Reduction (CVPR)", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_599", "file_type": "json", "from": "J. Sragih", "predicate": "affiliated_with", "to": "Conference on Computer Vision and Pattern Recognition (CVPR)", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_599", "file_type": "json", "from": "Joan Alabort-i-Medina", "predicate": "located_in", "to": "Imperial College London", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_599", "file_type": "json", "from": "Joan Alabort-i-Medina", "predicate": "email", "to": "ja310@imperial.ac.uk", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_599", "file_type": "json", "from": "Stefanos Zafeiriou", "predicate": "affiliated_with", "to": "Department of Computing", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_599", "file_type": "json", "from": "Stefanos Zafeiriou", "predicate": "affiliated_with", "to": "Imperial College London", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_599", "file_type": "json", "from": "Stefanos Zafeiriou", "predicate": "email", "to": "s.zafeiriou@imperial.ac.uk", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_599", "file_type": "json", "from": "Jia Xu", "predicate": "author_of", "to": "Gaze-Enabled Egocentric Video Summarization", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_603", "file_type": "json", "from": "Jia Xu", "predicate": "affiliated_with", "to": "University of Wisconsin-Madison", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_599", "file_type": "json", "from": "Lopamudra Mukherjee", "predicate": "author_of", "to": "Gaze-Enabled Egocentric Video Summarization", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_603", "file_type": "json", "from": "Lopamudra Mukherjee", "predicate": "affiliated_with", "to": "University of Wisconsin-Whitewater", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_599", "file_type": "json", "from": "Yin Li", "predicate": "author_of", "to": "Gaze-Enabled Egocentric Video Summarization", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_603", "file_type": "json", "from": "Yin Li", "predicate": "affiliated_with", "to": "Georgia Institute of Technology", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_599", "file_type": "json", "from": "Jamieson Warner", "predicate": "author_of", "to": "Gaze-Enabled Egocentric Video Summarization", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_599", "file_type": "json", "from": "James M. Rehg", "predicate": "author_of", "to": "Gaze-Enabled Egocentric Video Summarization", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_603", "file_type": "json", "from": "James M. Rehg", "predicate": "affiliated_with", "to": "Georgia Institute of Technology", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_599", "file_type": "json", "from": "Vikas Singh", "predicate": "author_of", "to": "Gaze-Enabled Egocentric Video Summarization", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_600", "file_type": "json", "from": "egocentric videos", "predicate": "necessitate", "to": "compact representation", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_600", "file_type": "json", "from": "egocentric video summarization", "predicate": "presents", "to": "unique challenges", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_600", "file_type": "json", "from": "gaze tracking information", "predicate": "improves", "to": "summarization", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_600", "file_type": "json", "from": "gaze tracking information", "predicate": "enables", "to": "frame comparison", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_600", "file_type": "json", "from": "summarization", "predicate": "requires", "to": "personalized summaries", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_600", "file_type": "json", "from": "summarization model", "predicate": "is based on", "to": "submodular function maximization", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_601", "file_type": "json", "from": "Egocentric Video Summarization", "predicate": "uses", "to": "Submodular Function Maximization", "type": "conceptual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_601", "file_type": "json", "from": "Submodular Function Maximization", "predicate": "solved_via", "to": "Multilinear Relaxation", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_601", "file_type": "json", "from": "Personalized Summarization", "predicate": "is_a_type_of", "to": "Egocentric Video Summarization", "type": "conceptual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_601", "file_type": "json", "from": "Almeida et al.", "predicate": "developed", "to": "VISON", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_601", "file_type": "json", "from": "VISON", "predicate": "is_for", "to": "Online Applications", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_601", "file_type": "json", "from": "Submodular Maximization", "predicate": "constrained_by", "to": "Partition Matroid", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_601", "file_type": "json", "from": "Filmus \u0026 Ward", "predicate": "developed", "to": "Combinatorial Algorithm", "type": "factual", "width": 0.89}, {"arrows": "to", "chunk_id": "doc_0_chunk_601", "file_type": "json", "from": "Fujishige", "predicate": "authored", "to": "Submodular Functions and Optimization", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_602", "file_type": "json", "from": "Fujishige", "predicate": "authored", "to": "Submodular functions and optimization", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_601", "file_type": "json", "from": "Gaze Tracking", "predicate": "enables", "to": "Personalized Summarization", "type": "conceptual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_601", "file_type": "json", "from": "Wearable Cameras", "predicate": "used_in", "to": "Egocentric Video Summarization", "type": "conceptual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_602", "file_type": "json", "from": "Ward", "predicate": "authored", "to": "algorithm", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_602", "file_type": "json", "from": "Submodular functions and optimization", "predicate": "discusses", "to": "submodular maximization", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_602", "file_type": "json", "from": "Iyer", "predicate": "authored", "to": "optimization", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_602", "file_type": "json", "from": "Krause", "predicate": "authored", "to": "information gathering", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_602", "file_type": "json", "from": "Xu", "predicate": "affiliated_with", "to": "University of Wisconsin-Madison", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_603", "file_type": "json", "from": "Andreas Geiger", "predicate": "author_of", "to": "Object Scene Flow for Autonomous Vehicles", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_609", "file_type": "json", "from": "Andreas Geiger", "predicate": "affiliated with", "to": "MPI Tubingen", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_603", "file_type": "json", "from": "Object Scene Flow for Autonomous Vehicles", "predicate": "presented_at", "to": "CVPR", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_603", "file_type": "json", "from": "Object Scene Flow for Autonomous Vehicles", "predicate": "supplemental_material_location", "to": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Menze_Object_Scene_Flow_2015_CVPR_supplemental.pdf", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_603", "file_type": "json", "from": "Moritz Menze", "predicate": "author_of", "to": "Object Scene Flow for Autonomous Vehicles", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_603", "file_type": "json", "from": "Menze_Object_Scene_Flow_2015_CVPR_supplemental", "predicate": "describes", "to": "Object Scene Flow for Autonomous Vehicles", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_604", "file_type": "json", "from": "supplementary document", "predicate": "provides", "to": "additional descriptions", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_604", "file_type": "json", "from": "supplementary document", "predicate": "provides", "to": "visualizations", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_604", "file_type": "json", "from": "supplementary document", "predicate": "provides", "to": "experiments", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_604", "file_type": "json", "from": "scene flow ground truth", "predicate": "generated_from", "to": "3D CAD models", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_604", "file_type": "json", "from": "model parameters", "predicate": "contributes_to", "to": "model sensitivity", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_605", "file_type": "json", "from": "KITTI stereo", "predicate": "is_benchmark_for", "to": "optical flow", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_605", "file_type": "json", "from": "KITTI stereo", "predicate": "is_benchmark_for", "to": "stereo", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_605", "file_type": "json", "from": "scene flow dataset", "predicate": "is", "to": "novel", "type": "conceptual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_605", "file_type": "json", "from": "sphere sequence", "predicate": "provides", "to": "qualitative results", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_606", "file_type": "json", "from": "Optical Flow", "predicate": "is_used_in", "to": "Scene Flow", "type": "conceptual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_606", "file_type": "json", "from": "Brox, T. \u0026 Malik, J.", "predicate": "authored", "to": "Large Displacement Optical Flow", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_606", "file_type": "json", "from": "Large Displacement Optical Flow", "predicate": "is_method_of", "to": "Variational Motion Estimation", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_606", "file_type": "json", "from": "Hirschmueller, H.", "predicate": "authored", "to": "Stereo Processing", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_606", "file_type": "json", "from": "Scene Flow Datasets", "predicate": "provides", "to": "Quantitative Results", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_606", "file_type": "json", "from": "Scene Flow Datasets", "predicate": "provides", "to": "Qualitative Results", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_607", "file_type": "json", "from": "Stereo processing", "predicate": "uses", "to": "semiglobal matching", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_607", "file_type": "json", "from": "Stereo processing", "predicate": "uses", "to": "mutual information", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_607", "file_type": "json", "from": "Scene flow estimation", "predicate": "method", "to": "growing correspondence seeds", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_607", "file_type": "json", "from": "Scene flow estimation", "predicate": "approach", "to": "piecewise rigid scene flow", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_607", "file_type": "json", "from": "Scene flow estimation", "predicate": "method", "to": "variational method", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_607", "file_type": "json", "from": "Cech et al. (2011)", "predicate": "addresses", "to": "Scene flow estimation", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_607", "file_type": "json", "from": "Vogel et al. (2013)", "predicate": "addresses", "to": "Scene flow estimation", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_607", "file_type": "json", "from": "Huguet \u0026 Devernay (2007)", "predicate": "addresses", "to": "Scene flow estimation", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_608", "file_type": "json", "from": "Huguet \u0026 Devernay (2007)", "predicate": "is_methodology_reference", "to": "scene flow estimation", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_608", "file_type": "json", "from": "Huguet \u0026 Devernay (2007)", "predicate": "presented_at", "to": "ICVV", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_607", "file_type": "json", "from": "Semiglobal matching", "predicate": "is_technique_for", "to": "Stereo processing", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_607", "file_type": "json", "from": "Mutual information", "predicate": "is_technique_for", "to": "Stereo processing", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_608", "file_type": "json", "from": "scene flow estimation", "predicate": "uses", "to": "stereo sequences", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_608", "file_type": "json", "from": "Sun, Roth, \u0026 Black (2013)", "predicate": "provides_analysis", "to": "optical flow estimation", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_608", "file_type": "json", "from": "optical flow estimation", "predicate": "relates_to", "to": "scene flow estimation", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_608", "file_type": "json", "from": "Hornacek, Fitzgibbon, \u0026 Rother (2014)", "predicate": "introduces", "to": "SphereFlow", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_608", "file_type": "json", "from": "Hornacek, Fitzgibbon, \u0026 Rother (2014)", "predicate": "presented_at", "to": "CVPR", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_608", "file_type": "json", "from": "SphereFlow", "predicate": "estimates", "to": "scene flow", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_608", "file_type": "json", "from": "Valgaerts et al. (2010)", "predicate": "addresses", "to": "motion and geometry estimation", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_608", "file_type": "json", "from": "stereo sequences", "predicate": "provides_data_for", "to": "motion estimation", "type": "factual", "width": 0.82}, {"arrows": "to", "chunk_id": "doc_0_chunk_609", "file_type": "json", "from": "Kert et al. (2010)", "predicate": "references", "to": "motion and geometry estimation", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_609", "file_type": "json", "from": "Wedel et al. (2008)", "predicate": "addresses", "to": "scene flow computation", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_609", "file_type": "json", "from": "Geiger et al. (2011)", "predicate": "focuses on", "to": "3D reconstruction techniques", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_609", "file_type": "json", "from": "Menz", "predicate": "affiliated with", "to": "Leibniz Universit\u00a8at Hannover", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_609", "file_type": "json", "from": "Tal Hassner", "predicate": "author of", "to": "Effective Face Frontalization", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_609", "file_type": "json", "from": "Shai Harel", "predicate": "author of", "to": "Effective Face Frontalization", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_609", "file_type": "json", "from": "Eran Paz", "predicate": "author of", "to": "Effective Face Frontalization", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_609", "file_type": "json", "from": "Roee Enbar", "predicate": "author of", "to": "Effective Face Frontalization", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_609", "file_type": "json", "from": "Hassner_Effective_Face_Frontalization_2015_CVPR_paper.pdf", "predicate": "contains", "to": "Effective Face Frontalization", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_610", "file_type": "json", "from": "face recognition systems", "predicate": "operates_in", "to": "unconstrained images", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_610", "file_type": "json", "from": "unconstrained images", "predicate": "exhibits", "to": "varying poses", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_610", "file_type": "json", "from": "unconstrained images", "predicate": "exhibits", "to": "varying expressions", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_610", "file_type": "json", "from": "unconstrained images", "predicate": "exhibits", "to": "varying lighting", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_610", "file_type": "json", "from": "Fronalization", "predicate": "solves", "to": "problem of varying poses", "type": "causal", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_610", "file_type": "json", "from": "Fronalization", "predicate": "solves", "to": "problem of varying lighting", "type": "causal", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_610", "file_type": "json", "from": "previous methods", "predicate": "relies_on", "to": "estimating 3D facial shapes", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_610", "file_type": "json", "from": "this paper", "predicate": "explores", "to": "simpler approach", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_612", "file_type": "json", "from": "this paper", "predicate": "proposes leveraging", "to": "visual common sense", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_662", "file_type": "json", "from": "this paper", "predicate": "proposes", "to": "novel method", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_610", "file_type": "json", "from": "simpler approach", "predicate": "uses", "to": "single 3D surface", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_610", "file_type": "json", "from": "frontal views", "predicate": "is", "to": "aesthetically pleasing", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_610", "file_type": "json", "from": "frontal views", "predicate": "improves", "to": "face recognition", "type": "causal", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_610", "file_type": "json", "from": "frontal views", "predicate": "improves", "to": "gender estimation", "type": "causal", "width": 0.89}, {"arrows": "to", "chunk_id": "doc_0_chunk_611", "file_type": "json", "from": "Face frontalization", "predicate": "produces", "to": "aesthetically pleasing frontal views", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_612", "file_type": "json", "from": "AI systems", "predicate": "excels at", "to": "factual question answering", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_612", "file_type": "json", "from": "AI systems", "predicate": "struggles with", "to": "common sense reasoning", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_612", "file_type": "json", "from": "visual common sense", "predicate": "is", "to": "semantic knowledge", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_613", "file_type": "json", "from": "Xiao Lin", "predicate": "affiliated_with", "to": "Virginia Tech", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_613", "file_type": "json", "from": "Xiao Lin", "predicate": "has_email", "to": "linxiao@vt.edu", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_613", "file_type": "json", "from": "Visual Common Sense", "predicate": "is_a", "to": "AI Reasoning", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_613", "file_type": "json", "from": "Visual Paraphasing", "predicate": "is_a", "to": "AI Reasoning", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_613", "file_type": "json", "from": "Hsu_Robust_Image_Alignment_2015_CVPR_paper.pdf", "predicate": "is_a", "to": "CVPR paper", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_613", "file_type": "json", "from": "Hsu_Robust_Image_Alignment_2015_CVPR_paper.pdf", "predicate": "located_in", "to": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_614", "file_type": "json", "from": "image alignment", "predicate": "is a", "to": "process", "type": "conceptual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_614", "file_type": "json", "from": "process", "predicate": "involves", "to": "data analysis", "type": "factual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_614", "file_type": "json", "from": "process", "predicate": "involves", "to": "optimization", "type": "factual", "width": 0.78}, {"arrows": "to", "chunk_id": "doc_0_chunk_615", "file_type": "json", "from": "Data Analysis", "predicate": "is_topic_of", "to": "Rock_Completing_3D_Object_2015_CVPR_paper.pdf", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_615", "file_type": "json", "from": "Rock_Completing_3D_Object_2015_CVPR_paper.pdf", "predicate": "requires", "to": "Parameter Estimation", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_615", "file_type": "json", "from": "Optimization", "predicate": "is_topic_of", "to": "Rock_Completing_3D_Object_2015_CVPR_paper.pdf", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_615", "file_type": "json", "from": "System Dynamics", "predicate": "is_topic_of", "to": "Rock_Completing_3D_Object_2015_CVPR_paper.pdf", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_615", "file_type": "json", "from": "Parameter Estimation", "predicate": "is_topic_of", "to": "Rock_Completing_3D_Object_2015_CVPR_paper.pdf", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_615", "file_type": "json", "from": "Relationship Modeling", "predicate": "is_topic_of", "to": "Rock_Compleeting_3D_Object_2015_CVPR_paper.pdf", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_615", "file_type": "json", "from": "Jason Rock", "predicate": "is_author_of", "to": "Rock_Completing_3D_Object_2015_CVPR_paper.pdf", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_617", "file_type": "json", "from": "Jason Rock", "predicate": "affiliation", "to": "University of Illinois at Urbana-Champaign", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_615", "file_type": "json", "from": "Justin Thorsten", "predicate": "is_author_of", "to": "Rock_Completing_3D_Object_2015_CVPR_paper.pdf", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_615", "file_type": "json", "from": "JunYoung Gwak", "predicate": "is_author_of", "to": "Rock_Completing_3D_Object_2015_CVPR_paper.pdf", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_617", "file_type": "json", "from": "JunYoung Gwak", "predicate": "affiliation", "to": "University of Illinois at Urbana-Champaign", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_615", "file_type": "json", "from": "Daeyun Shin", "predicate": "is_author_of", "to": "Rock_Completing_3D_Object_2015_CVPR_paper.pdf", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_617", "file_type": "json", "from": "Daeyun Shin", "predicate": "affiliation", "to": "University of Illinois at Urbana-Champaign", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_616", "file_type": "json", "from": "3D Shape Reconstruction", "predicate": "is a", "to": "topic", "type": "conceptual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_617", "file_type": "json", "from": "3D Shape Reconstruction", "predicate": "includes", "to": "View-Based Matching", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_617", "file_type": "json", "from": "3D Shape Reconstruction", "predicate": "includes", "to": "Shape Completion", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_617", "file_type": "json", "from": "3D Shape Reconstruction", "predicate": "includes", "to": "3D Model Synthesis", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_617", "file_type": "json", "from": "3D Shape Reconstruction", "predicate": "includes", "to": "Symmetry Transfer", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_617", "file_type": "json", "from": "Tanmay Gupta", "predicate": "affiliation", "to": "University of Illinois at Urbana-Champaign", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_617", "file_type": "json", "from": "Liu_Data-Driven_Sparsity-Based_Restoration_2015_CVPR_paper.pdf", "predicate": "is_a", "to": "CVPR paper", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_618", "file_type": "json", "from": "Rui Caseiro", "predicate": "authored", "to": "Beyond the Shortest Path", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_621", "file_type": "json", "from": "Rui Caseiro", "predicate": "affiliated_with", "to": "Institute of Systems and Robotics - University of Coimbra", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_618", "file_type": "json", "from": "Beyond the Shortest Path", "predicate": "presented_at", "to": "CVPR", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_618", "file_type": "json", "from": "Beyond the Shortest Path", "predicate": "addresses", "to": "domain adaptation", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_618", "file_type": "json", "from": "Beyond the Shortest Path", "predicate": "utilizes", "to": "spline flow", "type": "factual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_618", "file_type": "json", "from": "Pedro Martins", "predicate": "authored", "to": "Beyond the Shortest Path", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_622", "file_type": "json", "from": "Pedro Martins", "predicate": "is_affiliated_with", "to": "Institute of Systems and Robotics - University of Coimbra", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_618", "file_type": "json", "from": "Jorge Batista", "predicate": "authored", "to": "Beyond the Shortest Path", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_621", "file_type": "json", "from": "Jorge Batista", "predicate": "affiliated_with", "to": "Institute of Systems and Robotics - University of Coimbra", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_622", "file_type": "json", "from": "Jorge Batista", "predicate": "is_affiliated_with", "to": "Institute of Sistemas and Robotics - University of Coimbra", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_618", "file_type": "json", "from": "domain adaptation", "predicate": "aims_to", "to": "improve_performance", "type": "conceptual", "width": 0.7}, {"arrows": "to", "chunk_id": "doc_0_chunk_618", "file_type": "json", "from": "spline flow", "predicate": "is_a", "to": "method", "type": "conceptual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_619", "file_type": "json", "from": "domain adaptation paradigm", "predicate": "represents", "to": "source and target domains", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_619", "file_type": "json", "from": "shortest path", "predicate": "is", "to": "geodesic curve", "type": "conceptual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_619", "file_type": "json", "from": "shortest path", "predicate": "is insufficient", "to": "modeling complex domain shifts", "type": "causal", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_619", "file_type": "json", "from": "shortest path", "predicate": "restricts", "to": "use of multiple datasets", "type": "causal", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_619", "file_type": "json", "from": "novel approach", "predicate": "utilizes", "to": "spline curves", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_619", "file_type": "json", "from": "novel approach", "predicate": "allows for", "to": "integration of multiple source domains", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_619", "file_type": "json", "from": "novel approach", "predicate": "models", "to": "domain shifts", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_619", "file_type": "json", "from": "novel approach", "predicate": "demonstrates", "to": "improved performance", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_619", "file_type": "json", "from": "spline curves", "predicate": "computed via", "to": "rolling maps", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_620", "file_type": "json", "from": "Domain Adaptation", "predicate": "addresses", "to": "domain shifts", "type": "conceptual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_620", "file_type": "json", "from": "Domain Adaptation", "predicate": "improves", "to": "performance", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_620", "file_type": "json", "from": "Subspace Representation", "predicate": "used_in", "to": "Domain Adaptation", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_620", "file_type": "json", "from": "Baktas et al. (2013)", "predicate": "proposes", "to": "Domain Invariant Projection", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_620", "file_type": "json", "from": "Gopalan et al. (2013)", "predicate": "focuses_on", "to": "location recognition", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_620", "file_type": "json", "from": "Gopalan et al. (2011)", "predicate": "presents", "to": "Unsupervised approach", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_620", "file_type": "json", "from": "Carreira et al. (2012)", "predicate": "explores", "to": "Semantic segmentation", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_620", "file_type": "json", "from": "Caseiro et al. (2010)", "predicate": "investigates", "to": "cast shadows", "type": "factual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_620", "file_type": "json", "from": "Gopalan et al. (2014)", "predicate": "uses", "to": "intermediate data representations", "type": "factual", "width": 0.87}, {"arrows": "to", "chunk_id": "doc_0_chunk_620", "file_type": "json", "from": "Spline Flow", "predicate": "related_to", "to": "Domain Adaptation", "type": "conceptual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_620", "file_type": "json", "from": "Grasmannn Manifold", "predicate": "related_to", "to": "Domain Adaptation", "type": "conceptual", "width": 0.7}, {"arrows": "to", "chunk_id": "doc_0_chunk_620", "file_type": "json", "from": "Rolling Maps", "predicate": "related_to", "to": "Domain Adaptation", "type": "conceptual", "width": 0.65}, {"arrows": "to", "chunk_id": "doc_0_chunk_621", "file_type": "json", "from": "Gopalan", "predicate": "authored", "to": "Unsupervised adaptation across domain shifts by generating intermediate data representations", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_621", "file_type": "json", "from": "Li", "predicate": "co-authored", "to": "Unsupervised adaptation across domain shifts by generating intermediate data representations", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_621", "file_type": "json", "from": "Griffin", "predicate": "developed", "to": "Caltech-256 object category dataset", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_621", "file_type": "json", "from": "Holub", "predicate": "co-developed", "to": "Cal tech-256 object category dataset", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_621", "file_type": "json", "from": "Caseiro", "predicate": "authored", "to": "non-parametric riemannian framework", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_621", "file_type": "json", "from": "Henriques", "predicate": "co-authored", "to": "non-parametric riemannian framework", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_621", "file_type": "json", "from": "Martins", "predicate": "co-authored", "to": "non-parametric riemannian framework", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_621", "file_type": "json", "from": "Batista", "predicate": "co-authored", "to": "non-parametric riemannian framework", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_621", "file_type": "json", "from": "Hays", "predicate": "authored", "to": "Im2gps", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_621", "file_type": "json", "from": "Efroos", "predicate": "co-authored", "to": "Im2gps", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_621", "file_type": "json", "from": "Pan", "predicate": "authored", "to": "A survey on transfer learning", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_621", "file_type": "json", "from": "Jo\u00e3o F. Henriques", "predicate": "affiliated_with", "to": "Institute of Systems and Robotics - University of Coimbra", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_622", "file_type": "json", "from": "Sparse Composite Quantization", "predicate": "is_publication", "to": "CVPR paper", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_622", "file_type": "json", "from": "Sparse Composite Quantization", "predicate": "published_in", "to": "2015", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_622", "file_type": "json", "from": "Sparse Composite Quantization", "predicate": "addresses", "to": "quantization techniques", "type": "conceptual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_622", "file_type": "json", "from": "Zhang_Sparse_Composite_Quantization_2015_CVPR_paper", "predicate": "is_file_name", "to": "PDF document", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_623", "file_type": "json", "from": "sparse composite quantization", "predicate": "is_approach_for", "to": "approximate nearest neighbor search", "type": "conceptual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_623", "file_type": "json", "from": "sparse composite quantization", "predicate": "achieves", "to": "competitive search accuracy", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_623", "file_type": "json", "from": "sparse composite quantization", "predicate": "builds_on", "to": "product quantization", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_623", "file_type": "json", "from": "sparse composite quantization", "predicate": "builds_on", "to": "Cartesian k-means", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_623", "file_type": "json", "from": "sparse composite quantization", "predicate": "builds_on", "to": "composite quantization", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_623", "file_type": "json", "from": "sparse composite quantization", "predicate": "constructs", "to": "sparse dictionaries", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_623", "file_type": "json", "from": "sparse composite quantization", "predicate": "reduces", "to": "distance table computation time", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_626", "file_type": "json", "from": "Cartesian k-means", "predicate": "is_a", "to": "variation", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_626", "file_type": "json", "from": "Cartesian k-means", "predicate": "introduced_in", "to": "CVPR (2013)", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_623", "file_type": "json", "from": "distance table computation", "predicate": "is_bottleneck_in", "to": "composite quantization", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_624", "file_type": "json", "from": "distance table computation", "predicate": "reduces", "to": "time", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_623", "file_type": "json", "from": "sparse dictionaries", "predicate": "accelerates", "to": "distance evaluation", "type": "factual", "width": 0.87}, {"arrows": "to", "chunk_id": "doc_0_chunk_625", "file_type": "json", "from": "large-scale datasets", "predicate": "contains", "to": "SIFTs", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_625", "file_type": "json", "from": "SIFTs", "predicate": "has_scale", "to": "1M", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_624", "file_type": "json", "from": "SIFTs", "predicate": "scale", "to": "1B", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_624", "file_type": "json", "from": "search times", "predicate": "are", "to": "faster", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_625", "file_type": "json", "from": "SIFTS", "predicate": "has_scale", "to": "1B", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_625", "file_type": "json", "from": "ANN", "predicate": "is_method_for", "to": "nearest neighbor search", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_625", "file_type": "json", "from": "Product Quantization", "predicate": "is_technique", "to": "for nearest neighbor search", "type": "conceptual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_625", "file_type": "json", "from": "Babenko and Lempitsky (2012)", "predicate": "introduces", "to": "inverted multi-index", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_625", "file_type": "json", "from": "Babenko and Lempitsky (2014)", "predicate": "improves", "to": "bilayer product quantization", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_625", "file_type": "json", "from": "Babenko and Lempitsky (2014)", "predicate": "targets", "to": "billion-scale approximate nearest neighbors", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_626", "file_type": "json", "from": "bilayer product quantization", "predicate": "builds_upon", "to": "product quantization", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_626", "file_type": "json", "from": "bilayer product quantization", "predicate": "addresses", "to": "large-scale applications", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_625", "file_type": "json", "from": "High-Dimensional Data", "predicate": "requires", "to": "approximate nearest neighbors", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_626", "file_type": "json", "from": "vocabulary trees", "predicate": "used_in", "to": "scalable recognition", "type": "factual", "width": 0.87}, {"arrows": "to", "chunk_id": "doc_0_chunk_626", "file_type": "json", "from": "Hamming embedding", "predicate": "utilized_for", "to": "large scale image search", "type": "factual", "width": 0.89}, {"arrows": "to", "chunk_id": "doc_0_chunk_626", "file_type": "json", "from": "semi-supervised hashing", "predicate": "applied_to", "to": "large-scale search", "type": "factual", "width": 0.91}, {"arrows": "to", "chunk_id": "doc_0_chunk_627", "file_type": "json", "from": "Re-ranking strategies", "predicate": "addresses", "to": "large-scale search", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_627", "file_type": "json", "from": "Graph-based search methods", "predicate": "focuses on", "to": "search", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_627", "file_type": "json", "from": "Angular quantization", "predicate": "introduces", "to": "binary codes", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_628", "file_type": "json", "from": "Guo-Jun Qi", "predicate": "affiliated_with", "to": "University of Central Florida", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_627", "file_type": "json", "from": "Jinhui Tang", "predicate": "affiliated with", "to": "Unknown", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_628", "file_type": "json", "from": "Jinhui Tang", "predicate": "affiliated_with", "to": "Nanjing University of Science and Technology", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_628", "file_type": "json", "from": "Ting Zhang", "predicate": "affiliated_with", "to": "University of Science and Technology of China", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_628", "file_type": "json", "from": "Jingding Wang", "predicate": "affiliated_with", "to": "Microsoft Research", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_628", "file_type": "json", "from": "Ioannis Gkioulekalas", "predicate": "author_of", "to": "On the Appearance of Translueceny Edges", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_628", "file_type": "json", "from": "On the Appearence of Translueceny Edges", "predicate": "presented_at", "to": "CVPR", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_628", "file_type": "json", "from": "Gkiooulekas_On_the_Appearance_2015_CVPR_paper.pdf", "predicate": "contains", "to": "On the Appearence of Translueceny Edges", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_629", "file_type": "json", "from": "Edges in images", "predicate": "differ from", "to": "Edges in opaque objects", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_629", "file_type": "json", "from": "Edges", "predicate": "caused by", "to": "Discontinuity in surface orientation", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_629", "file_type": "json", "from": "Authors", "predicate": "explain", "to": "Edge patterns", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_629", "file_type": "json", "from": "Edge patterns", "predicate": "result from", "to": "Light Transport", "type": "causal", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_629", "file_type": "json", "from": "Simulations", "predicate": "utilize", "to": "Scattering parameters", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_629", "file_type": "json", "from": "Visual Inference tasks", "predicate": "involve", "to": "Shape estimation", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_629", "file_type": "json", "from": "Visual Inference tasks", "predicate": "involve", "to": "Material estimation", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_630", "file_type": "json", "from": "Light Transport", "predicate": "addressed_in", "to": "Jensen et al. (2001)", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_630", "file_type": "json", "from": "Wave Propagation", "predicate": "described_in", "to": "Ishimaru (1978)", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_630", "file_type": "json", "from": "Stereo Reconstruction", "predicate": "relies_on", "to": "Wave Propagation", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_630", "file_type": "json", "from": "Human Perception", "predicate": "studied_in", "to": "Adelson (2001)", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_630", "file_type": "json", "from": "Machine Vision", "predicate": "informed_by", "to": "Human Perception", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_630", "file_type": "json", "from": "Rendering", "predicate": "requires", "to": "Light Transport", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_630", "file_type": "json", "from": "Reconstruction", "predicate": "requires", "to": "Light Transport", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_630", "file_type": "json", "from": "Translucent Objects", "predicate": "involved_in", "to": "Shape Estimation", "type": "factual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_630", "file_type": "json", "from": "Translucent Objects", "predicate": "involved_in", "to": "Material Estimation", "type": "factual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_630", "file_type": "json", "from": "Material Metameters", "predicate": "related_to", "to": "Translucent Objects", "type": "conceptual", "width": 0.7}, {"arrows": "to", "chunk_id": "doc_0_chunk_631", "file_type": "json", "from": "light transport", "predicate": "occurs_within", "to": "translucient materials", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_631", "file_type": "json", "from": "rendering", "predicate": "requires", "to": "accurate light transport", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_631", "file_type": "json", "from": "translucent materials", "predicate": "exhibit", "to": "translucent appearance", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_632", "file_type": "json", "from": "translucent materials", "predicate": "has_property", "to": "translucency", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_631", "file_type": "json", "from": "phase functions", "predicate": "important_for", "to": "translucent appearance", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_631", "file_type": "json", "from": "reconstruction", "predicate": "requires", "to": "accurate rendering", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_632", "file_type": "json", "from": "phase function", "predicate": "affects", "to": "translucent appearance", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_632", "file_type": "json", "from": "photon diffusion", "predicate": "is_rendering_technique_for", "to": "translucent materials", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_632", "file_type": "json", "from": "Open-surfaces catalog", "predicate": "is_dataset_of", "to": "surface appearances", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_632", "file_type": "json", "from": "surface appearances", "predicate": "used_for", "to": "training", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_632", "file_type": "json", "from": "surface appearances", "predicate": "used_for", "to": "evaluation", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_698", "file_type": "json", "from": "evaluation", "predicate": "uses", "to": "hybrid multi-camera and marker-based capture dataset", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_632", "file_type": "json", "from": "ACM Transactions on Graphics", "predicate": "publishes", "to": "research on phase function", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_633", "file_type": "json", "from": "ACM Transactions on Graphics", "predicate": "publishes", "to": "Bala. Open-surfaces", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_632", "file_type": "json", "from": "Journal of Vision", "predicate": "publishes", "to": "research on translucency", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_632", "file_type": "json", "from": "ACM SIGGRAPH", "predicate": "publishes", "to": "research on rendering techniques", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_633", "file_type": "json", "from": "Bala. Open-surfaces", "predicate": "provides", "to": "dataset of surface appearances", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_633", "file_type": "json", "from": "Gkiouslekas et al.", "predicate": "examines", "to": "phase function in translucent appearance", "type": "conceptual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_633", "file_type": "json", "from": "materials in context database", "predicate": "relevant for", "to": "translucent objects", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_634", "file_type": "json", "from": "Mingkui Tan", "predicate": "author_of", "to": "Learning graph structure...", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_639", "file_type": "json", "from": "Mingkui Tan", "predicate": "affiliated_with", "to": "University of Adelaide", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_634", "file_type": "json", "from": "Learning graph structure...", "predicate": "is_a", "to": "CVPR paper", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_634", "file_type": "json", "from": "Learning graph structure...", "predicate": "file_name", "to": "Tan_Learning_Graph_Structure_2015_CVPR_paper.pdf", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_634", "file_type": "json", "from": "Qinfeng Shi", "predicate": "author_of", "to": "Learning graph structure...", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_639", "file_type": "json", "from": "Qinfeng Shi", "predicate": "affiliated_with", "to": "University of Adelaide", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_634", "file_type": "json", "from": "Fuyuan Hu", "predicate": "author_of", "to": "Learning graph structure...", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_634", "file_type": "json", "from": "Zhen Zhang", "predicate": "author_of", "to": "Learning graph structure...", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_635", "file_type": "json", "from": "Multi-label image classification", "predicate": "improves", "to": "Classification performance", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_637", "file_type": "json", "from": "Multi-label image classification", "predicate": "uses", "to": "Probablistic Graphical Models", "type": "conceptual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_635", "file_type": "json", "from": "Probabilistic Graphical Models", "predicate": "represents", "to": "Label dependency", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_635", "file_type": "json", "from": "Graphical model structure", "predicate": "determined by", "to": "Heuristic methods", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_635", "file_type": "json", "from": "Graphical model structure", "predicate": "learned from", "to": "Limited information", "type": "factual", "width": 0.82}, {"arrows": "to", "chunk_id": "doc_0_chunk_635", "file_type": "json", "from": "Problem", "predicate": "formulated into", "to": "Max-margin framework", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_635", "file_type": "json", "from": "Problem", "predicate": "transformed into", "to": "Convex programming problem", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_637", "file_type": "json", "from": "Max-margin framework", "predicate": "used_in", "to": "learning", "type": "conceptual", "width": 0.65}, {"arrows": "to", "chunk_id": "doc_0_chunk_635", "file_type": "json", "from": "Procedure", "predicate": "activates", "to": "Set of cliques", "type": "factual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_637", "file_type": "json", "from": "performance improvement", "predicate": "over", "to": "methods", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_637", "file_type": "json", "from": "Graph structure learning", "predicate": "related_to", "to": "Probablistic Graphical Models", "type": "conceptual", "width": 0.7}, {"arrows": "to", "chunk_id": "doc_0_chunk_637", "file_type": "json", "from": "Clique generation", "predicate": "related_to", "to": "Graph structure learning", "type": "conceptual", "width": 0.7}, {"arrows": "to", "chunk_id": "doc_0_chunk_637", "file_type": "json", "from": "Boutell et al. (2004)", "predicate": "studied", "to": "scene classification", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_637", "file_type": "json", "from": "Bradley \u0026 Guestrin (2010)", "predicate": "studied", "to": "tree conditional random fields", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_637", "file_type": "json", "from": "Bucak et al. (2009)", "predicate": "studied", "to": "multi-label ranking", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_638", "file_type": "json", "from": "R.", "predicate": "authored", "to": "Efficient multi-label ranking", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_638", "file_type": "json", "from": "Efficient multi-label ranking", "predicate": "presented_at", "to": "IEEE Conference on Computer Vision and Pattern Recognition", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_638", "file_type": "json", "from": "Jain, A. K.", "predicate": "co-authored", "to": "Efficient multi-label ranking", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_638", "file_type": "json", "from": "Cai, X.", "predicate": "authored", "to": "Graph structured sparsity model", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_638", "file_type": "json", "from": "Chow, C.", "predicate": "authored", "to": "Approximating discrete probability distributions", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_638", "file_type": "json", "from": "Approximating discrete probability distributions", "predicate": "published_in", "to": "IEEE Transactions on Information Theory", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_638", "file_type": "json", "from": "Liu, C.", "predicate": "co-authored", "to": "Approximating discrete probability distributions", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_638", "file_type": "json", "from": "Dembczy\u0144ski, K.", "predicate": "authored", "to": "Label dependence and loss minimization", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_638", "file_type": "json", "from": "Label dependence and loss minimization", "predicate": "published_in", "to": "Machine Learning", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_638", "file_type": "json", "from": "Waegeman, W.", "predicate": "co-authored", "to": "Label dependence and loss minimization", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_638", "file_type": "json", "from": "Waegeman, W.", "predicate": "co-authored", "to": "Analysis of chaining", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_638", "file_type": "json", "from": "Cheng, W.", "predicate": "co-authored", "to": "Label dependence and loss minimization", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_638", "file_type": "json", "from": "H\u00fcllermeier, E.", "predicate": "co-authored", "to": "Label dependence and loss minimization", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_638", "file_type": "json", "from": "H\u00fcllermeier, E.", "predicate": "co-authored", "to": "Analysis of chaining", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_638", "file_type": "json", "from": "Dembczynski, K.", "predicate": "authored", "to": "Analysis of chaining", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_638", "file_type": "json", "from": "Analysis of chaining", "predicate": "presented_at", "to": "European Conference on Artificial Intelligence", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_639", "file_type": "json", "from": "Dembczynski", "predicate": "authored", "to": "analysis of chaining", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_639", "file_type": "json", "from": "Dembczynski", "predicate": "affilates_with", "to": "European Conference on Artificial Intelligence", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_639", "file_type": "json", "from": "Everingham", "predicate": "created", "to": "PASUAL Visual Object Classes Challenge 2012", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_640", "file_type": "json", "from": "Bolei Zhou", "predicate": "authored", "to": "ConceptLearner", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_644", "file_type": "json", "from": "Bolei Zhou", "predicate": "affiliated_with", "to": "MIT", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_640", "file_type": "json", "from": "ConceptLearner", "predicate": "is_a", "to": "CVPR paper", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_641", "file_type": "json", "from": "ConceptLearner", "predicate": "proposes", "to": "scalable approach", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_641", "file_type": "json", "from": "ConceptLearner", "predicate": "discovers", "to": "visual concepts", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_641", "file_type": "json", "from": "ConceptLearner", "predicate": "demonstrates", "to": "promising performance", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_641", "file_type": "json", "from": "ConceptLearner", "predicate": "compared to", "to": "fully supervised methods", "type": "factual", "width": 0.82}, {"arrows": "to", "chunk_id": "doc_0_chunk_641", "file_type": "json", "from": "ConceptLearner", "predicate": "compared to", "to": "weakly supervised methods", "type": "factual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_640", "file_type": "json", "from": "Vignesh Jagadeesh", "predicate": "authored", "to": "ConceptLearner", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_644", "file_type": "json", "from": "Vignesh Jagadeesh", "predicate": "affiliated_with", "to": "eBay Research Labs", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_640", "file_type": "json", "from": "Zhou_ConceptLearner_Discovering_Visual_2015_CVPR_paper.pdf", "predicate": "is_file_of", "to": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_640", "file_type": "json", "from": "Charles Sturt University", "predicate": "has_author", "to": "Junbin Gao", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_641", "file_type": "json", "from": "computer vision recognition systems", "predicate": "requires", "to": "visual knowledge", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_641", "file_type": "json", "from": "fully labeled data", "predicate": "is", "to": "expensive", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_641", "file_type": "json", "from": "visual concept detectors", "predicate": "are learned", "to": "automatically", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_641", "file_type": "json", "from": "visual concept detectors", "predicate": "applied to", "to": "image region-level detection", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_642", "file_type": "json", "from": "learned concepts", "predicate": "evaluated_for", "to": "scene recognition", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_642", "file_type": "json", "from": "learned concepts", "predicate": "evaluated_for", "to": "object detection", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_642", "file_type": "json", "from": "fully supervised methods", "predicate": "outperformed_by", "to": "weakly supervised methods", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_642", "file_type": "json", "from": "weakly supervised methods", "predicate": "outperformed_by", "to": "domain-specific supervision", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_642", "file_type": "json", "from": "automatic attribute discovery", "predicate": "characterized_from", "to": "noisy web data", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_642", "file_type": "json", "from": "Im2text", "predicate": "describes", "to": "images", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_642", "file_type": "json", "from": "Im2text", "predicate": "uses", "to": "captioned photographs", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_642", "file_type": "json", "from": "object detectors", "predicate": "adapted_from", "to": "images", "type": "factual", "width": 0.87}, {"arrows": "to", "chunk_id": "doc_0_chunk_642", "file_type": "json", "from": "object detectors", "predicate": "adapted_to", "to": "video", "type": "factual", "width": 0.87}, {"arrows": "to", "chunk_id": "doc_0_chunk_643", "file_type": "json", "from": "Deng et al.", "predicate": "created", "to": "Imaginet", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_643", "file_type": "json", "from": "Places database", "predicate": "used_for", "to": "scene recognition", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_643", "file_type": "json", "from": "Zhou et al.", "predicate": "utilized", "to": "Places database", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_643", "file_type": "json", "from": "Piction", "predicate": "labels", "to": "human faces", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_643", "file_type": "json", "from": "Srihari", "predicate": "developed", "to": "Piction", "type": "factual", "width": 0.91}, {"arrows": "to", "chunk_id": "doc_0_chunk_643", "file_type": "json", "from": "Divvala et al.", "predicate": "developed", "to": "webly-supervised visual concept learning", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_643", "file_type": "json", "from": "Tang et al.", "predicate": "adapted", "to": "object detectors", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_644", "file_type": "json", "from": "AAAI", "predicate": "published_by", "to": "AAAI Press", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_644", "file_type": "json", "from": "AAAI", "predicate": "published_by", "to": "The MIT Press", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_644", "file_type": "json", "from": "Divvala, S. K.", "predicate": "affiliated_with", "to": "CVPR", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_644", "file_type": "json", "from": "Robinson Piramuthu", "predicate": "affiliated_with", "to": "eBay Research Labs", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_644", "file_type": "json", "from": "Yumin Suh", "predicate": "author_of", "to": "Subgraph Matching using Compactness Prior", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_649", "file_type": "json", "from": "Yumin Suh", "predicate": "affiliated_with", "to": "Seoul National University", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_644", "file_type": "json", "from": "Subgraph Matching using Compactness Prior", "predicate": "presented_at", "to": "CVPR", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_644", "file_type": "json", "from": "Kamil Adamczewski", "predicate": "author_of", "to": "Subgraph Matching using Compactness Prior", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_649", "file_type": "json", "from": "Kamil Adamczewski", "predicate": "affiliated_with", "to": "Seoul National University", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_644", "file_type": "json", "from": "Suh_Subgraph_Matching_Using_2015_CVPR_paper.pdf", "predicate": "contains", "to": "Subgraph Matching using Compactness Prior", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_645", "file_type": "json", "from": "Feature correspondence", "predicate": "plays_role_in", "to": "computer vision applications", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_647", "file_type": "json", "from": "Feature correspondence", "predicate": "presented_in", "to": "2009 IEEE 12th International Conference on Computer Vision", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_645", "file_type": "json", "from": "Graph matching", "predicate": "is_formulated_as", "to": "problem", "type": "conceptual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_645", "file_type": "json", "from": "Graph matching algorithms", "predicate": "rarely_considers", "to": "precision", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_645", "file_type": "json", "from": "Solutions", "predicate": "have", "to": "outliers", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_645", "file_type": "json", "from": "Subgraph matching formulation", "predicate": "uses", "to": "compactness prior", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_645", "file_type": "json", "from": "compactness prior", "predicate": "prefers", "to": "sparsity", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_645", "file_type": "json", "from": "compactness prior", "predicate": "eliminates", "to": "outliers", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_645", "file_type": "json", "from": "Meta-algorithm", "predicate": "based_on", "to": "Markov chain Monte Carlo", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_645", "file_type": "json", "from": "Formulation and algorithm", "predicate": "improve", "to": "baseline performance", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_647", "file_type": "json", "from": "Cho, M.", "predicate": "authored", "to": "Learning graphs to match", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_647", "file_type": "json", "from": "Cho, M.", "predicate": "authored", "to": "Feature correspondence", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_647", "file_type": "json", "from": "Cho, M.", "predicate": "authored", "to": "Reweighted random walks", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_648", "file_type": "json", "from": "Cho, M.", "predicate": "author_of", "to": "max-pooling strategy", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_647", "file_type": "json", "from": "Reweighted random walks", "predicate": "presented_in", "to": "Computer Vision\u2013ECCV 2010", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_647", "file_type": "json", "from": "Progressive graph matching", "predicate": "presented_in", "to": "Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on", "type": "factual", "width": 0.91}, {"arrows": "to", "chunk_id": "doc_0_chunk_647", "file_type": "json", "from": "Alahari, K.", "predicate": "co-authored", "to": "Learning graphs to match", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_647", "file_type": "json", "from": "Ponce, J.", "predicate": "co-authored", "to": "Learning graphs to match", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_648", "file_type": "json", "from": "Lee, Kyoung Mu", "predicate": "affiliation", "to": "Seoul National University", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_648", "file_type": "json", "from": "Lee, Kyoung Mu", "predicate": "author_of", "to": "progressive graph matching", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_648", "file_type": "json", "from": "Adamczewski, Kamil", "predicate": "affiliation", "to": "Seoul National University", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_648", "file_type": "json", "from": "Suh, Yumin", "predicate": "affiliation", "to": "Seoul National University", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_648", "file_type": "json", "from": "Duchenne, O.", "predicate": "author_of", "to": "tensor-based algorithm", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_648", "file_type": "json", "from": "Duchenne, O.", "predicate": "affiliation", "to": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_648", "file_type": "json", "from": "Gilks, W. R.", "predicate": "author_of", "to": "Markov chain monte carlo", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_648", "file_type": "json", "from": "Cour, T.", "predicate": "author_of", "to": "balanced graph matching", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_648", "file_type": "json", "from": "progressive graph matching", "predicate": "presented_at", "to": "IEEE Conference on Computer Vision and Pattern Recognition", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_649", "file_type": "json", "from": "Guancong Zhang", "predicate": "author_of", "to": "Good Features to Track for Visual SLAM", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_649", "file_type": "json", "from": "Guancong Zhang", "predicate": "contributed_to", "to": "Zhang_Good_Features_to_2015_CVPR_paper.pdf", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_649", "file_type": "json", "from": "Good Features to Track for Visual SLAM", "predicate": "is_publication_of", "to": "CVPR", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_649", "file_type": "json", "from": "Good Features to Track for Visual SLAM", "predicate": "published_in_year", "to": "2015", "type": "factual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_649", "file_type": "json", "from": "Patricio A. Vela", "predicate": "author_of", "to": "Good Features to Track for Visual SLAM", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_649", "file_type": "json", "from": "Patricio A. Vela", "predicate": "contributed_to", "to": "Zhang_Good_Features_to_2015_CVPR_paper.pdf", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_654", "file_type": "json", "from": "Patricio A. Vela", "predicate": "email", "to": "pvela@gatech.edu", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_649", "file_type": "json", "from": "Zhang_Good_Features_to_2015_CVPR_paper.pdf", "predicate": "is_file_of", "to": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_650", "file_type": "json", "from": "measured features", "predicate": "contribute_to", "to": "accurate localization", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_650", "file_type": "json", "from": "method for selecting features", "predicate": "is_useful_for", "to": "localization", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_650", "file_type": "json", "from": "method for selecting features", "predicate": "is_derived_from", "to": "observability of SLAM", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_650", "file_type": "json", "from": "method for selecting features", "predicate": "integrates_into", "to": "existing SLAM systems", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_650", "file_type": "json", "from": "method for selecting features", "predicate": "improves", "to": "localization accuracy", "type": "causal", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_650", "file_type": "json", "from": "estimation utility", "predicate": "is_formulated_with", "to": "observability indices", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_650", "file_type": "json", "from": "observability indices", "predicate": "are_computed_using", "to": "incremental singular value decomposition (SVD)", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_651", "file_type": "json", "from": "observability indices", "predicate": "calculated_using", "to": "greedy selection", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_651", "file_type": "json", "from": "observability indices", "predicate": "described_by", "to": "incremental singular value decomposition", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_651", "file_type": "json", "from": "greedy selection", "predicate": "is_approximately", "to": "submodular", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_651", "file_type": "json", "from": "greedy selection", "predicate": "is", "to": "near-optimal", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_650", "file_type": "json", "from": "SLAM", "predicate": "related_to", "to": "SfM", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_651", "file_type": "json", "from": "incremental singular value decomposition", "predicate": "used_for", "to": "temporal observability indices", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_651", "file_type": "json", "from": "synthetic experiments", "predicate": "demonstrate", "to": "improved localization accuracy", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_651", "file_type": "json", "from": "SLAM experiments", "predicate": "demonstrate", "to": "improved data association", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_652", "file_type": "json", "from": "Visual SLAM", "predicate": "relies_on", "to": "Data Association", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_652", "file_type": "json", "from": "Data Association", "predicate": "addressed_by", "to": "Incremental SVD", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_652", "file_type": "json", "from": "Observability Analysis", "predicate": "impacts", "to": "map_building", "type": "causal", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_652", "file_type": "json", "from": "MonoSLAM", "predicate": "implements", "to": "real-time SLAM", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_652", "file_type": "json", "from": "LSD-SLAM", "predicate": "uses", "to": "direct monocular SLAM", "type": "factual", "width": 0.87}, {"arrows": "to", "chunk_id": "doc_0_chunk_653", "file_type": "json", "from": "LSD-SLAM", "predicate": "is_a", "to": "Slam_Algorithm", "type": "conceptual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_652", "file_type": "json", "from": "Covariance recovery", "predicate": "solves", "to": "data association", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_652", "file_type": "json", "from": "Feature Selection", "predicate": "influences", "to": "localization accuracy", "type": "causal", "width": 0.82}, {"arrows": "to", "chunk_id": "doc_0_chunk_652", "file_type": "json", "from": "Andrade-Cetto and Sanfeliu", "predicate": "analyzed", "to": "partial observability", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_652", "file_type": "json", "from": "Kaess and Dellaert", "predicate": "proposed", "to": "covariance recovery", "type": "factual", "width": 0.91}, {"arrows": "to", "chunk_id": "doc_0_chunk_652", "file_type": "json", "from": "Davison et al.", "predicate": "developed", "to": "MonoSLAM", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_652", "file_type": "json", "from": "Engel et al.", "predicate": "created", "to": "LSD-SLAM", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_653", "file_type": "json", "from": "Slam_Algorithm", "predicate": "is_field_of", "to": "Machine Intelligence", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_653", "file_type": "json", "from": "iSAM2", "predicate": "is_algorithm", "to": "Bayes Tree", "type": "conceptual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_653", "file_type": "json", "from": "iSAM2", "predicate": "uses", "to": "Incremental Smoothing", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_653", "file_type": "json", "from": "iSAM2", "predicate": "is_variant_of", "to": "Slam", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_653", "file_type": "json", "from": "Active search", "predicate": "is_topic_of", "to": "IEEE International Conference on Computer Vision", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_653", "file_type": "json", "from": "Live dense reconstruction", "predicate": "uses", "to": "Single moving camera", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_654", "file_type": "json", "from": "Zheng Ma", "predicate": "affiliated_with", "to": "School of ECR", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_654", "file_type": "json", "from": "Zheng Ma", "predicate": "email", "to": "zhanggc@gatech.edu", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_660", "file_type": "json", "from": "Zheng Ma", "predicate": "affiliated_with", "to": "City University of Hong Kong", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_654", "file_type": "json", "from": "Lei Yu", "predicate": "affiliated_with", "to": "School of ECR", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_660", "file_type": "json", "from": "Lei Yu", "predicate": "affiliated_with", "to": "City University of Hong Kong", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_654", "file_type": "json", "from": "Antoni B. Chan", "predicate": "affiliated_with", "to": "School of ECR", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_661", "file_type": "json", "from": "Antoni B. Chan", "predicate": "affiliated_with", "to": "City University of Hong Kong", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_654", "file_type": "json", "from": "Georgia Tech", "predicate": "has_affiliation", "to": "School of ECE", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_654", "file_type": "json", "from": "Ma_Small_Instance_Detection_2015_CVPR_paper", "predicate": "published_in", "to": "CVPR", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_654", "file_type": "json", "from": "Ma_Small_Instance_Detection_2015_CVPR_paper", "predicate": "file_path", "to": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers", "type": "factual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_654", "file_type": "json", "from": "Small Instance Detection", "predicate": "uses", "to": "Integer Programming", "type": "conceptual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_654", "file_type": "json", "from": "Small Instance Detection", "predicate": "analyzes", "to": "Object Density Maps", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_657", "file_type": "json", "from": "Small Instance Detection", "predicate": "is_topic", "to": "Computer Vision", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_657", "file_type": "json", "from": "Integer Programming", "predicate": "is_topic", "to": "Computer Vision", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_657", "file_type": "json", "from": "Object Density Maps", "predicate": "is_topic", "to": "Computer Vision", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_655", "file_type": "json", "from": "pedestrians", "predicate": "is_example_of", "to": "partially-occluded small instances", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_655", "file_type": "json", "from": "cells", "predicate": "is_example_of", "to": "partially-occluding small instances", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_656", "file_type": "json", "from": "2D integer programming", "predicate": "is_used_to", "to": "recover object instance locations", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_656", "file_type": "json", "from": "ROI counts", "predicate": "used_in", "to": "2D integer programming", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_656", "file_type": "json", "from": "density map", "predicate": "regularizes", "to": "detection performance", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_656", "file_type": "json", "from": "object instances", "predicate": "located_using", "to": "2D integer programming", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_658", "file_type": "json", "from": "HOG features", "predicate": "is_feature_descriptor_for", "to": "human detection", "type": "conceptual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_680", "file_type": "json", "from": "HOG features", "predicate": "used in", "to": "object detection", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_658", "file_type": "json", "from": "Bayesian regression", "predicate": "is_method_for", "to": "crowd counting", "type": "conceptual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_658", "file_type": "json", "from": "Crowd counting", "predicate": "uses", "to": "low-level features", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_658", "file_type": "json", "from": "Crowd counting", "predicate": "uses", "to": "multiple local features", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_658", "file_type": "json", "from": "IEEE Conf. Computer Vision and Pattern Recognition", "predicate": "publishes", "to": "HOG features", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_658", "file_type": "json", "from": "IEEE Trans. on Image Processing", "predicate": "publishes", "to": "Bayesian regression", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_658", "file_type": "json", "from": "Digital Image Computing: Techniques and Applications", "predicate": "publishes", "to": "Crowd counting using multiple local features", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_658", "file_type": "json", "from": "Human detection", "predicate": "requires", "to": "HOG features", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_659", "file_type": "json", "from": "Sridharan (2009)", "predicate": "addresses", "to": "crowd counting", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_659", "file_type": "json", "from": "Sridharan (2009)", "predicate": "uses", "to": "multiple local features", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_659", "file_type": "json", "from": "Lowe (2004)", "predicate": "introduces", "to": "SIFT features", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_659", "file_type": "json", "from": "Chan (2008)", "predicate": "addresses", "to": "privacy concerns", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_659", "file_type": "json", "from": "Chan (2008)", "predicate": "addresses", "to": "crowd monitoring", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_659", "file_type": "json", "from": "Zhao (2003)", "predicate": "focuses on", "to": "Bayesian segmentation", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_659", "file_type": "json", "from": "Bayesian segmentation", "predicate": "applied to", "to": "crowded scenes", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_660", "file_type": "json", "from": "R. ia", "predicate": "focuses_on", "to": "Bayesian segmentation", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_660", "file_type": "json", "from": "Lemtipsky, V.", "predicate": "works_on", "to": "object counting", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_660", "file_type": "json", "from": "PASCAL VOC challenge", "predicate": "is_a", "to": "benchmark", "type": "conceptual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_660", "file_type": "json", "from": "PASCAL VOC challenge", "predicate": "used_for", "to": "object detection", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_661", "file_type": "json", "from": "City University of Hong Kong", "predicate": "has_member", "to": "Antoni B. Chan", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_660", "file_type": "json", "from": "Mohammad Rastegari", "predicate": "is_author_of", "to": "Computationally Bound Retrieval", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_661", "file_type": "json", "from": "Mohammad Rastegari", "predicate": "author_of", "to": "Computationally Bounded Retrieval", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_666", "file_type": "json", "from": "Mohammad Rastegari", "predicate": "affiliated_with", "to": "University of Maryland", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_661", "file_type": "json", "from": "Computationally Bounded Retrieval", "predicate": "is_paper_from", "to": "CVPR", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_661", "file_type": "json", "from": "Cem Keskin", "predicate": "author_of", "to": "Computationally Bounded Retrieval", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_667", "file_type": "json", "from": "Cem Keskin", "predicate": "affiliated_with", "to": "Microsoft Research", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_661", "file_type": "json", "from": "Shahram Izadi", "predicate": "author_of", "to": "Computationally Bounded Retrieval", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_667", "file_type": "json", "from": "Shahram Izadi", "predicate": "affiliated_with", "to": "Microsoft Research", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_661", "file_type": "json", "from": "Rastegari_Computationally_Bounded_Retrieval_2015_CVPR_paper.pdf", "predicate": "contains", "to": "Computationally Bounded Retrieval", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_661", "file_type": "json", "from": "Rastegari_Computationality_Bounded_Retrieval_2015_CVPR_paper", "predicate": "is_title_of", "to": "Computationally Bounded Retrieval", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_662", "file_type": "json", "from": "large image databases", "predicate": "makes", "to": "efficient retrieval challenging", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_662", "file_type": "json", "from": "high dimensional data", "predicate": "poses", "to": "retrieval challenge", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_662", "file_type": "json", "from": "hashing methods", "predicate": "sacrifices", "to": "accuracy for speed", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_662", "file_type": "json", "from": "orthogonality constraint", "predicate": "reduces", "to": "bit correlation", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_662", "file_type": "json", "from": "iterative scheme", "predicate": "optimizes", "to": "objective", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_664", "file_type": "json", "from": "Near-optimal Hasing Algorithms", "predicate": "solves", "to": "Approximate Nearest Neighbor", "type": "conceptual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_664", "file_type": "json", "from": "Near-optimal Hasing Algorithms", "predicate": "addresses", "to": "High Dimensions", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_664", "file_type": "json", "from": "Datar et al. (2004)", "predicate": "introduces", "to": "Locality-Sensitive Hashing Scheme", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_664", "file_type": "json", "from": "Locality-Sensitive Hashing Scheme", "predicate": "based_on", "to": "P-stable Distributions", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_664", "file_type": "json", "from": "Methods", "predicate": "demonstrates", "to": "Speed-up", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_664", "file_type": "json", "from": "Speed-up", "predicate": "is", "to": "factor of 100", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_665", "file_type": "json", "from": "Gong et al. (2013)", "predicate": "presented_at", "to": "Computer Vision and Pattern Recognition (CVPR), 2013", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_665", "file_type": "json", "from": "Gong et al. (2013)", "predicate": "uses", "to": "bilinear projections", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_665", "file_type": "json", "from": "Gong \u0026 Lazebnik (2011)", "predicate": "presented_at", "to": "2011 IEEE Conference on Computer Vision and Pattern Recognition", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_665", "file_type": "json", "from": "Gong \u0026 Lazebnik (2011)", "predicate": "uses", "to": "iterative quantization", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_665", "file_type": "json", "from": "J\u00e9goeu et al. (2009)", "predicate": "developed", "to": "Searching with quantization", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_665", "file_type": "json", "from": "Searching with quantization", "predicate": "uses", "to": "short codes", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_666", "file_type": "json", "from": "J\u00e9gou et al (2009)", "predicate": "researches", "to": "approximate nearest neighbor search", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_666", "file_type": "json", "from": "Krizhevskya et al (2012)", "predicate": "performs", "to": "ImageNet classification", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_666", "file_type": "json", "from": "Krizhevskya et al (2012)", "predicate": "uses", "to": "deep convolutional neural networks", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_666", "file_type": "json", "from": "Majia et al (2013)", "predicate": "focuses on", "to": "efficient classification", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_666", "file_type": "json", "from": "Norouzia et al (2011)", "predicate": "develops", "to": "minimal loss hashing", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_667", "file_type": "json", "from": "New Insights into Laplacian Similarity Search", "predicate": "presented_at", "to": "CVPR", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_667", "file_type": "json", "from": "Wu_New_Insights_Into_2015_CVPR_paper.pdf", "predicate": "is_document_of", "to": "New Insights into Laplacian Similarity Search", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_668", "file_type": "json", "from": "Graph-based computer vision applications", "predicate": "relies_on", "to": "similarity metrics", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_668", "file_type": "json", "from": "similarity metrics", "predicate": "computes", "to": "pairwise similarity", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_668", "file_type": "json", "from": "pairwise similarity", "predicate": "is_between", "to": "vertices", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_668", "file_type": "json", "from": "(L + \u03b1\u039b)\u22121", "predicate": "includes", "to": "graph Laplacian", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_668", "file_type": "json", "from": "(L + \u03b1\u039b)\u22121", "predicate": "includes", "to": "positive diagonal matrix", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_668", "file_type": "json", "from": "(L + \u03b1\u039b)\u22121", "predicate": "respects", "to": "graph topology", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_668", "file_type": "json", "from": "positive diagonal matrix", "predicate": "acts_as", "to": "regularizer", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_668", "file_type": "json", "from": "regularizer", "predicate": "impacts", "to": "cluster density", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_669", "file_type": "json", "from": "regularizer", "predicate": "denotes", "to": "\u039b", "type": "conceptual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_668", "file_type": "json", "from": "choices", "predicate": "lead_to", "to": "complementary behaviors", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_670", "file_type": "json", "from": "Paper (1999)", "predicate": "discusses", "to": "Pagerank Citation Ranking", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_670", "file_type": "json", "from": "Paper (1999)", "predicate": "validates", "to": "approach", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_670", "file_type": "json", "from": "Pagerank Citation Ranking", "predicate": "aims_to", "to": "bring order to web", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_670", "file_type": "json", "from": "Chung (1997)", "predicate": "authored", "to": "Spectral Graph Theory", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_670", "file_type": "json", "from": "Spectral Graph Theory", "predicate": "covers", "to": "Graph Topology", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_670", "file_type": "json", "from": "Andersen et al. (2006)", "predicate": "uses", "to": "Pagerank Vectors", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_670", "file_type": "json", "from": "Andersen et al. (2006)", "predicate": "addresses", "to": "Local Graph Partitioning", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_670", "file_type": "json", "from": "Belkin \u0026 Niyogi (2001)", "predicate": "proposes", "to": "Laplacian Eigenmaps", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_670", "file_type": "json", "from": "Shi \u0026 Malik (2000)", "predicate": "introduces", "to": "Normalized Cuts", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_671", "file_type": "json", "from": "Laplacianfaces", "predicate": "used_for", "to": "face recognition", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_671", "file_type": "json", "from": "Random walks", "predicate": "used_for", "to": "image segmentation", "type": "factual", "width": 0.91}, {"arrows": "to", "chunk_id": "doc_0_chunk_672", "file_type": "json", "from": "Wu, X.-M.", "predicate": "affiliated_with", "to": "Columbia University", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_671", "file_type": "json", "from": "Wu, X.-M.", "predicate": "works_in", "to": "Electrical Engineering", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_672", "file_type": "json", "from": "Wu, X.-M.", "predicate": "authored", "to": "Analyzing the harmonic structure in graph-based learning", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_672", "file_type": "json", "from": "Wu, X.-M.", "predicate": "works_in", "to": "Department of Electrical Engineering", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_672", "file_type": "json", "from": "Wu, X.-M.", "predicate": "contributes_to", "to": "graph-based learning", "type": "conceptual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_671", "file_type": "json", "from": "graph-based learning", "predicate": "analyzes", "to": "harmonic structure", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_672", "file_type": "json", "from": "Chang, S.-F.", "predicate": "authored", "to": "Analyzing the harmonic structure in graph-based learning", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_673", "file_type": "json", "from": "Wenguan Wang", "predicate": "is_author_of", "to": "Saliency-Aware Geodesic Video Object Segmentation", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_678", "file_type": "json", "from": "Wenguan Wang", "predicate": "affiliated_with", "to": "Beijing Lab of Intelligent Information Technology", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_678", "file_type": "json", "from": "Wenguan Wang", "predicate": "member_of", "to": "School of Computer Science", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_678", "file_type": "json", "from": "Wenguan Wang", "predicate": "located_in", "to": "Beijing Institute of Technology", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_673", "file_type": "json", "from": "Saliency-Aware Geodesic Video Object Segmentation", "predicate": "published_in", "to": "NIPS", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_673", "file_type": "json", "from": "Jianbing Shen", "predicate": "is_author_of", "to": "Saliency-Aware Geodesic Video Object Segmentation", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_678", "file_type": "json", "from": "Jianbing Shen", "predicate": "affiliated_with", "to": "Beijing Lab of Intelligent Information Technology", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_678", "file_type": "json", "from": "Jianbing Shen", "predicate": "located_in", "to": "Beijing Institute of Technology", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_673", "file_type": "json", "from": "Fatih Porikli", "predicate": "is_author_of", "to": "Saliency-Aware Geodesic Video Object Segmentation", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_678", "file_type": "json", "from": "Fatih Porikli", "predicate": "affiliated_with", "to": "Research School of Engineering", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_678", "file_type": "json", "from": "Fatih Porikli", "predicate": "affiliated_with", "to": "Australian National University", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_678", "file_type": "json", "from": "Fatih Porikli", "predicate": "member_of", "to": "NICTA Australia", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_673", "file_type": "json", "from": "Saliency-Aware Geosesic Video Object Segmentation", "predicate": "is_a", "to": "CVPR paper", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_674", "file_type": "json", "from": "unsupervised method", "predicate": "is_based_on", "to": "geodesic distance", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_674", "file_type": "json", "from": "spatial edges", "predicate": "are_indicators_of", "to": "spatiotemporal salience maps", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_675", "file_type": "json", "from": "Video Object Segmentation", "predicate": "relies_on", "to": "Energy Minimization", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_675", "file_type": "json", "from": "Video Object Segmentation", "predicate": "achieves", "to": "spatially and temporally coherent object segmentation", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_675", "file_type": "json", "from": "Geos image segmentation", "predicate": "introduced_in", "to": "ECCV, 2008", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_675", "file_type": "json", "from": "Geos image segmentation", "predicate": "uses", "to": "geodesic image segmentation approach", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_675", "file_type": "json", "from": "Motion Boundaries", "predicate": "relevant_to", "to": "Video Object Segmentation", "type": "conceptual", "width": 0.7}, {"arrows": "to", "chunk_id": "doc_0_chunk_676", "file_type": "json", "from": "Geos", "predicate": "introduces", "to": "geospatial image segmentation approach", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_676", "file_type": "json", "from": "Geos", "predicate": "is_method_of", "to": "image segmentation", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_676", "file_type": "json", "from": "Geodesic graph cut", "predicate": "is_method_of", "to": "interactive image segmentation", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_676", "file_type": "json", "from": "Grabcut", "predicate": "is_method_of", "to": "interactive foreground extraction", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_676", "file_type": "json", "from": "Grabcut", "predicate": "uses", "to": "iterated graph cuts", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_676", "file_type": "json", "from": "Object segmentation", "predicate": "uses", "to": "trajectory analysis", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_677", "file_type": "json", "from": "Video object segmentation", "predicate": "addressed by", "to": "W. Brendel and S. Todorovic", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_677", "file_type": "json", "from": "Geodesic image and video editing", "predicate": "combines", "to": "geodesic methods", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_677", "file_type": "json", "from": "J. Carreira", "predicate": "authored", "to": "Constrained parametric min-cuts", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_677", "file_type": "json", "from": "W. Brendel", "predicate": "authored", "to": "Video object segmentation by tracking regions", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_677", "file_type": "json", "from": "D. Tsai", "predicate": "authored", "to": "Motion coherent tracking using multi-label mrf optimization", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_678", "file_type": "json", "from": "Tali Dekel", "predicate": "author_of", "to": "Best-Buddies Similarity", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_682", "file_type": "json", "from": "Tali Dekel", "predicate": "affiliated_with", "to": "MIT CSAIL", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_678", "file_type": "json", "from": "Best-Buddies Similarity", "predicate": "is_a", "to": "paper", "type": "conceptual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_678", "file_type": "json", "from": "Best-Buddies Similarity", "predicate": "published_in", "to": "CVPR", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_679", "file_type": "json", "from": "template matching", "predicate": "occurs_in", "to": "unconstrained environments", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_679", "file_type": "json", "from": "Best-Buddies Similarity (BBS)", "predicate": "is", "to": "similarity measure", "type": "conceptual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_679", "file_type": "json", "from": "Best-Buddies Similarity (BBS)", "predicate": "is_based_on", "to": "counting Best-Buddie Pairs (BBPs)", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_679", "file_type": "json", "from": "Best-Buddies Similarity (BBS)", "predicate": "is_robust_against", "to": "geometric deformations", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_679", "file_type": "json", "from": "Best-Buddies Similarity (BBS)", "predicate": "is_robust_against", "to": "outliers", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_679", "file_type": "json", "from": "Best-Buddies Similarity (BSS)", "predicate": "is", "to": "parameter-free", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_679", "file_type": "json", "from": "Best-Buddie Pairs (BBPs)", "predicate": "are", "to": "pairs of points", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_680", "file_type": "json", "from": "BBS", "predicate": "solves", "to": "non-rigid object tracking", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_680", "file_type": "json", "from": "BBS", "predicate": "demonstrates", "to": "consistent success", "type": "factual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_680", "file_type": "json", "from": "BBS", "predicate": "uses", "to": "Similarity Measures", "type": "conceptual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_680", "file_type": "json", "from": "BBS", "predicate": "handles", "to": "Outlier Robustness", "type": "conceptual", "width": 0.7}, {"arrows": "to", "chunk_id": "doc_0_chunk_680", "file_type": "json", "from": "BBS", "predicate": "incorporates", "to": "Geometric Deformations", "type": "conceptual", "width": 0.65}, {"arrows": "to", "chunk_id": "doc_0_chunk_680", "file_type": "json", "from": "Comaniciu, D. et al.", "predicate": "authored", "to": "mean shift tracking", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_680", "file_type": "json", "from": "mean shift tracking", "predicate": "addresses", "to": "non-rigid object tracking", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_680", "file_type": "json", "from": "Rubner, Y. et al.", "predicate": "introduced", "to": "Earth Mover\u0027s Distance", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_680", "file_type": "json", "from": "Earth Mover\u0027s Distance", "predicate": "is_a", "to": "metric for image retrieval", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_681", "file_type": "json", "from": "Earth Mover\u0027s Distance", "predicate": "is_metric_for", "to": "image comparison", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_681", "file_type": "json", "from": "Earth Mover\u0027s Distance", "predicate": "introduced_by", "to": "Rubner et al. (2000)", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_681", "file_type": "json", "from": "Pele et al. (2008)", "predicate": "addresses", "to": "robust pattern matching", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_681", "file_type": "json", "from": "Simaov et al. (2008)", "predicate": "explores", "to": "summarizing visual data", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_681", "file_type": "json", "from": "Simaov et al. (2008)", "predicate": "uses", "to": "similarity measures", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_681", "file_type": "json", "from": "Hel-Or et al. (2014)", "predicate": "focuses_on", "to": "photometric invariant template matching", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_681", "file_type": "json", "from": "photometric invariant template matching", "predicate": "is_a", "to": "matching technique", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_681", "file_type": "json", "from": "Tian et al. (2012)", "predicate": "deals_with", "to": "estimating nonrigid image distortions", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_681", "file_type": "json", "from": "nonrigid image distortions", "predicate": "impacts", "to": "image estimation", "type": "conceptual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_682", "file_type": "json", "from": "Tian \u0026 Narasimhan (2012)", "predicate": "deals_with", "to": "nonrigid image distortions", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_682", "file_type": "json", "from": "Korman et al. (2013)", "predicate": "presents", "to": "fast affine template matching algorithm", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_682", "file_type": "json", "from": "Wu et al. (2013)", "predicate": "provides", "to": "online object tracking benchmark", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_682", "file_type": "json", "from": "Olson (2002)", "predicate": "discusses", "to": "maximum-likelihood image matching", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_682", "file_type": "json", "from": "Michael Rubinstein", "predicate": "affiliated_with", "to": "Google Research", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_683", "file_type": "json", "from": "Shai Avidan", "predicate": "affiliated_with", "to": "Tel Aviv University", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_683", "file_type": "json", "from": "Shai Avidan", "predicate": "email", "to": "avidan@eng.tau.ac.il", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_682", "file_type": "json", "from": "Nianyi Li", "predicate": "author_of", "to": "A Weighted Sparse Coding Framework for Saliency Detection", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_684", "file_type": "json", "from": "Nianyi Li", "predicate": "authored", "to": "A Weighted Sparse Coding Framework", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_684", "file_type": "json", "from": "Nianyi Li", "predicate": "affilates_with", "to": "Tel Aviv University", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_682", "file_type": "json", "from": "Bilin Sun", "predicate": "author_of", "to": "A Weighted Sparse Coding Framework for Saliency Detection", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_684", "file_type": "json", "from": "Bilin Sun", "predicate": "authored", "to": "A Weighted Sparse Coding Framework", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_682", "file_type": "json", "from": "Jingyi", "predicate": "author_of", "to": "A Weighted Sparse Coding Framework for Saliency Detection", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_684", "file_type": "json", "from": "A Weighted Sparse Coding Framework", "predicate": "addresses", "to": "Saliency Detection", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_684", "file_type": "json", "from": "A Weighted Sparse Coding Framework", "predicate": "is_paper_type", "to": "CVPR paper", "type": "conceptual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_684", "file_type": "json", "from": "Jingyi Yu", "predicate": "authored", "to": "A Weighted Sparse Coding Framework", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_684", "file_type": "json", "from": "avidan@eng.tau.ac.il", "predicate": "email_associated_with", "to": "Nianyi Li", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_684", "file_type": "json", "from": "billf@mit.edu", "predicate": "email_associated_with", "to": "William T. Freeman", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_684", "file_type": "json", "from": "Li_A_Weighted_Sparse_2015_CVPR_paper", "predicate": "is_file", "to": "pdf", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_685", "file_type": "json", "from": "salience detection", "predicate": "utilizes", "to": "high-dimensional datasets", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_685", "file_type": "json", "from": "dictionaries", "predicate": "uses", "to": "data-speci\ufb01c features", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_685", "file_type": "json", "from": "dictionary", "predicate": "prunes", "to": "outliers", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_686", "file_type": "json", "from": "dictionary", "predicate": "refined_by", "to": "superpixels", "type": "causal", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_686", "file_type": "json", "from": "Liu et al. (2011)", "predicate": "proposes", "to": "Salience Detection", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_686", "file_type": "json", "from": "Liu et al. (2011)", "predicate": "published_in", "to": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_686", "file_type": "json", "from": "Achanta et al. (2012)", "predicate": "compares", "to": "Superpixels", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_686", "file_type": "json", "from": "Achanta et al. (2012)", "predicate": "published_in", "to": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_686", "file_type": "json", "from": "Borji \u0026 Itti (2012)", "predicate": "exploits", "to": "Patch Rarities", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_686", "file_type": "json", "from": "Borji, Sihite, \u0026 Itti (2012)", "predicate": "introduces", "to": "Salient Object Detection Benchmark", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_686", "file_type": "json", "from": "Borji, Sihite, \u0026 Itti (2012)", "predicate": "published_in", "to": "ECCV", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_687", "file_type": "json", "from": "Reynolds \u0026 Desimone", "predicate": "study", "to": "V4", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_687", "file_type": "json", "from": "Reynolds \u0026 Desimone", "predicate": "investigate", "to": "attention", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_688", "file_type": "json", "from": "Reynolds \u0026 Desimone", "predicate": "Published in", "to": "Neuron", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_687", "file_type": "json", "from": "Nothdurft", "predicate": "proposes", "to": "additivity across dimensions", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_687", "file_type": "json", "from": "Nothdurft", "predicate": "studies", "to": "salience from feature contrast", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_687", "file_type": "json", "from": "Perazzi et al.", "predicate": "develops", "to": "salience filters", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_687", "file_type": "json", "from": "Perazzi et al.", "predicate": "uses", "to": "contrast based filtering", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_687", "file_type": "json", "from": "Cheng et al.", "predicate": "develops", "to": "Global contrast based salient region detection", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_687", "file_type": "json", "from": "Borji et al.", "predicate": "presents", "to": "cal and global patch rarities", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_687", "file_type": "json", "from": "Movahedi \u0026 Elder", "predicate": "validates", "to": "performance measures", "type": "factual", "width": 0.91}, {"arrows": "to", "chunk_id": "doc_0_chunk_688", "file_type": "json", "from": "Reynolds \u0026 Desimnone", "predicate": "studied", "to": "V4", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_688", "file_type": "json", "from": "Itti \u0026 Koch", "predicate": "Published in", "to": "Nature Reviews Neuroscience", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_688", "file_type": "json", "from": "Reynolds", "predicate": "Published in", "to": "CVPR", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_688", "file_type": "json", "from": "Souvenir", "predicate": "affiliated with", "to": "University of Delaware", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_688", "file_type": "json", "from": "Souvenir", "predicate": "authored", "to": "Robust Regression on Image Manifolds", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_688", "file_type": "json", "from": "Sun", "predicate": "email", "to": "sunbilin@eecis.udel.edu", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_688", "file_type": "json", "from": "Yu", "predicate": "email", "to": "yu@eecis.udel.edu", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_689", "file_type": "json", "from": "robust regression method", "predicate": "is", "to": "non-parametric", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_689", "file_type": "json", "from": "mis-labeled examples", "predicate": "has", "to": "ordered labels", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_689", "file_type": "json", "from": "label corruption levels", "predicate": "reaches", "to": "80%", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_690", "file_type": "json", "from": "image labels", "predicate": "describe", "to": "associated images", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_690", "file_type": "json", "from": "Ordered Labels", "predicate": "relates_to", "to": "Ordinal Data", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_690", "file_type": "json", "from": "Label Denoising", "predicate": "improves", "to": "accuracy of image labels", "type": "factual", "width": 0.82}, {"arrows": "to", "chunk_id": "doc_0_chunk_690", "file_type": "json", "from": "Building Rome in a day", "predicate": "published_in", "to": "IEEE International Conference on Computer Visions", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_691", "file_type": "json", "from": "R. C. Bolles", "predicate": "author of", "to": "Random sample consensus", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_691", "file_type": "json", "from": "C.-C. Chang", "predicate": "author of", "to": "LIBSVM", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_691", "file_type": "json", "from": "C.-J. Lin", "predicate": "author of", "to": "LIBSVM", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_691", "file_type": "json", "from": "USAC", "predicate": "framework for", "to": "Random sample consensus", "type": "conceptual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_691", "file_type": "json", "from": "R. Raguram", "predicate": "author of", "to": "USAC", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_691", "file_type": "json", "from": "l-curve", "predicate": "used for", "to": "Analysis of discrete ill-posed problems", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_691", "file_type": "json", "from": "P. C. Hansen", "predicate": "author of", "to": "Analysis of discrete ill-posed problems", "type": "factual", "width": 0.91}, {"arrows": "to", "chunk_id": "doc_0_chunk_691", "file_type": "json", "from": "Internet photo collections", "predicate": "used for", "to": "Modeling the world", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_691", "file_type": "json", "from": "N. Snavely", "predicate": "author of", "to": "Modeling the world", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_692", "file_type": "json", "from": "SIAM review", "predicate": "is_published_in", "to": "journal", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_692", "file_type": "json", "from": "Kai Han", "predicate": "is_author_of", "to": "A Fixed Viewpoint Approach", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_697", "file_type": "json", "from": "Kai Han", "predicate": "affiliation", "to": "The University of Hokkaido", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_692", "file_type": "json", "from": "A Fixed Viewpoint Approach", "predicate": "is_publication", "to": "paper", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_692", "file_type": "json", "from": "Kwan-Yee K. Wong", "predicate": "is_author_of", "to": "A Fixed Viewpoint Approach", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_697", "file_type": "json", "from": "Kwan-Yee K. Wong", "predicate": "affiliation", "to": "The University of Hokkaido", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_692", "file_type": "json", "from": "Miaomiao Liu", "predicate": "is_author_of", "to": "A Fixed View Point Approach", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_697", "file_type": "json", "from": "Miaomiao Liu", "predicate": "affiliation", "to": "NICTA", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_697", "file_type": "json", "from": "Miaomiao Liu", "predicate": "affiliation", "to": "CECS, ANU", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_694", "file_type": "json", "from": "light path triangulation", "predicate": "relates_to", "to": "refractive photo-light-path", "type": "conceptual", "width": 0.82}, {"arrows": "to", "chunk_id": "doc_0_chunk_694", "file_type": "json", "from": "M. Ben-Ezra and S. K. Nayr", "predicate": "addresses", "to": "transparency analysis", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_694", "file_type": "json", "from": "G. Eren et al.", "predicate": "proposes", "to": "shape estimation", "type": "factual", "width": 0.86}, {"arrows": "to", "chunk_id": "doc_0_chunk_694", "file_type": "json", "from": "shape estimation", "predicate": "uses", "to": "local surface heating", "type": "factual", "width": 0.84}, {"arrows": "to", "chunk_id": "doc_0_chunk_695", "file_type": "json", "from": "local surface heating", "predicate": "is_method_for", "to": "shape estimation", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_695", "file_type": "json", "from": "Fischler and Bolles", "predicate": "Introduces", "to": "RANSAC", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_695", "file_type": "json", "from": "Hata et al.", "predicate": "Explores", "to": "genetic algorithm", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_695", "file_type": "json", "from": "genetic algorithm", "predicate": "is_used_for", "to": "shape extraction", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_695", "file_type": "json", "from": "Ihrke et al. (2005)", "predicate": "Demonstrates", "to": "geometry reconstruction", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_695", "file_type": "json", "from": "geometry reconstruction", "predicate": "occurs_in", "to": "dynamic environments", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_695", "file_type": "json", "from": "Ihrke et al. (2008)", "predicate": "Provides", "to": "overview", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_695", "file_type": "json", "from": "Transparent objects", "predicate": "requires", "to": "shape estimation", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_697", "file_type": "json", "from": "Benjamin Allain", "predicate": "author_of", "to": "An Efficient Volumetric Framework for Shape Tracking", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_702", "file_type": "json", "from": "Benjamin Allain", "predicate": "affiliated_with", "to": "Inria Grenoble Rh\u02c6one-Alpes - LJK", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_701", "file_type": "json", "from": "Benjamin Allain", "predicate": "affiliation", "to": "Inria Grenoble Rh\u02c6one- Alpes", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_701", "file_type": "json", "from": "Benjamin Allain", "predicate": "is_affiliated_with", "to": "LJK", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_697", "file_type": "json", "from": "An Efficient Volumetric Framework for Shape Tracking", "predicate": "presented_at", "to": "CVPR", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_697", "file_type": "json", "from": "An Efficient Volumetric Framework for Shape Tracking", "predicate": "file_name", "to": "Allain_An_Efficient_Volumetric_2015_CVPR_paper.pdf", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_697", "file_type": "json", "from": "Jean-S\u00e9batian Franco", "predicate": "author_of", "to": "An Efficient Volumetric Framework for Shape Tracking", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_697", "file_type": "json", "from": "Edmond Boyer", "predicate": "author_of", "to": "An Efficient Volumetric Framework for Shape Tracking", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_702", "file_type": "json", "from": "Edmond Boyer", "predicate": "affiliated_with", "to": "Inria Grenoble Rh\u02c6one-Alpes - LJK", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_698", "file_type": "json", "from": "surface-based strategies", "predicate": "can_fail_when", "to": "observations define several feasible surfaces", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_698", "file_type": "json", "from": "this work", "predicate": "investigates", "to": "volumetric shape parametrization", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_698", "file_type": "json", "from": "volumetric shape parametrization", "predicate": "utilizes", "to": "Centroidal Voronoi Tesselations (CVT)", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_698", "file_type": "json", "from": "volumetric deformation model", "predicate": "demonstrates", "to": "improved precision and robustness", "type": "factual", "width": 0.91}, {"arrows": "to", "chunk_id": "doc_0_chunk_698", "file_type": "json", "from": "volumetric deformation model", "predicate": "compared_to", "to": "state-of-the-art methods", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_698", "file_type": "json", "from": "volumetric shape tracking", "predicate": "uses", "to": "Centroidal Voronoi Tesselations (CVT)", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_699", "file_type": "json", "from": "Volumetric Shape Tracking", "predicate": "compares_to", "to": "state-of-the-art methods", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_699", "file_type": "json", "from": "Centroidal Voronoi Tesselations", "predicate": "is_a", "to": "method", "type": "conceptual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_699", "file_type": "json", "from": "Dynamic Shape Capture", "predicate": "is_a", "to": "method", "type": "conceptual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_699", "file_type": "json", "from": "Motion Estimation", "predicate": "is_a", "to": "method", "type": "conceptual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_699", "file_type": "json", "from": "Surface-based Methods", "predicate": "is_a", "to": "approach", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_699", "file_type": "json", "from": "Volume-based Methods", "predicate": "is_a", "to": "approach", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_699", "file_type": "json", "from": "Alexa et al. (2000)", "predicate": "authored", "to": "As-rigid-as-possible shape interpolation", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_699", "file_type": "json", "from": "Allain et al. (2014)", "predicate": "authored", "to": "On mean pose and variability of 3d deformable models", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_699", "file_type": "json", "from": "Ballan \u0026 Cortelazzo (2008)", "predicate": "authored", "to": "Marker-less motion capture of skinned models", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_699", "file_type": "json", "from": "Bishop (2006)", "predicate": "authored", "to": "Pattern Recognition and Machine Learning", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_699", "file_type": "json", "from": "Botsu et al. (2007)", "predicate": "authored", "to": "Adaptive space deformations based on rigid cells", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_700", "file_type": "json", "from": "Adaptive space deformations based on rigid cells", "predicate": "published_in", "to": "Comput. Graph. Forum", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_700", "file_type": "json", "from": "Botsu, M.", "predicate": "authored", "to": "Adaptive space deformations based on rigid cells", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_700", "file_type": "json", "from": "Cagniart, C.", "predicate": "authored", "to": "Free-form mesh tracking: a patch-based approach", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_700", "file_type": "json", "from": "Cagniart, C.", "predicate": "authored", "to": "Probabilistic deformable surface tracking from multiple videos", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_700", "file_type": "json", "from": "Free-form mesh tracking: a patch-based approach", "predicate": "published_in", "to": "CVPR", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_700", "file_type": "json", "from": "Probabilistic deformable surface tracking from multiple videos", "predicate": "published_in", "to": "ECCV", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_700", "file_type": "json", "from": "de Aguiar, E.", "predicate": "authored", "to": "Performance capture from sparse multi-view video", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_700", "file_type": "json", "from": "Performance capture from sparse multi-view video", "predicate": "published_in", "to": "ACM Transactions on Graphics", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_700", "file_type": "json", "from": "de Aguliar, E.", "predicate": "authored", "to": "Marker-less deformable mesh tracking for human shape and motion capture", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_701", "file_type": "json", "from": "Benjamin Allaine", "predicate": "email", "to": "firstname.lastname@inria.fr", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_701", "file_type": "json", "from": "Maximum likelihood", "predicate": "method_used_in", "to": "em algorithm", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_701", "file_type": "json", "from": "em algorithm", "predicate": "used_for", "to": "Maximum likelihood from incomplete data", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_701", "file_type": "json", "from": "Journal of the Royal Statistical Society, series B", "predicate": "publishes", "to": "Maximum likelihood from incomplete data via the em algorithm", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_702", "file_type": "json", "from": "Jean-S\u00e9bastien Franco", "predicate": "affiliated_with", "to": "Inria Grenoble Rh\u02c6one-Alpes - LHK", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_702", "file_type": "json", "from": "Yi-Hsuan Tsai", "predicate": "author_of", "to": "Adaptive Region Pooling for Object Detection", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_704", "file_type": "json", "from": "Yi-Hsuan Tsai", "predicate": "affiliated_with", "to": "UC Merced", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_702", "file_type": "json", "from": "Adaptive Region Pooling for Object Detection", "predicate": "presented_at", "to": "CVPR", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_702", "file_type": "json", "from": "Adaptive Region Pooling for Object Detection", "predicate": "is_located_at", "to": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Tsai_Adaptive_Region_Pooling_2015_CVPR_supplemental.pdf", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_702", "file_type": "json", "from": "Onur C. Hamsici", "predicate": "author_of", "to": "Adaptive Region Pooling for Object Detection", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_704", "file_type": "json", "from": "Onur C. Hamsici", "predicate": "affiliated_with", "to": "Qualcomm Research", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_702", "file_type": "json", "from": "Tsai_Adaptive_Region_Pooling_2015_CVPR_supplemental", "predicate": "is_file_name", "to": "Adaptive Region Pooling for Object Detection", "type": "factual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_703", "file_type": "json", "from": "Adaptive Region Pooling", "predicate": "is_method_for", "to": "object detection", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_703", "file_type": "json", "from": "Adaptive Region Pooling", "predicate": "discovers", "to": "discriminative object parts", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_703", "file_type": "json", "from": "Adaptive Region Pooling", "predicate": "addresses", "to": "challenging conditions", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_704", "file_type": "json", "from": "Adaptive Region Pooling", "predicate": "is_topic", "to": "Object Detection", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_703", "file_type": "json", "from": "Adaptive Region Processing", "predicate": "transfers", "to": "keypoints", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_703", "file_type": "json", "from": "representative parts", "predicate": "transferred_to", "to": "detected objects", "type": "factual", "width": 0.91}, {"arrows": "to", "chunk_id": "doc_0_chunk_703", "file_type": "json", "from": "challenging conditions", "predicate": "includes", "to": "occlusion", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_703", "file_type": "json", "from": "challenging conditions", "predicate": "includes", "to": "varying lighting", "type": "factual", "width": 0.83}, {"arrows": "to", "chunk_id": "doc_0_chunk_703", "file_type": "json", "from": "ARP", "predicate": "compared_with", "to": "ESVM", "type": "factual", "width": 0.87}, {"arrows": "to", "chunk_id": "doc_0_chunk_704", "file_type": "json", "from": "Ensemble of exemplar-svms", "predicate": "compared_to", "to": "method", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_704", "file_type": "json", "from": "Ensemble of exemplar-svms", "predicate": "presented_at", "to": "ICCV", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_704", "file_type": "json", "from": "Keypoint Transfer", "predicate": "is_topic", "to": "Object Detection", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_704", "file_type": "json", "from": "Object Parts Discovery", "predicate": "is_topic", "to": "Object Detection", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_704", "file_type": "json", "from": "Long-term Recurrent Convolutional Networks", "predicate": "aims_to", "to": "Visual Recognition", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_704", "file_type": "json", "from": "Long-term Recurrent Convolutional Networks", "predicate": "aims_to", "to": "Visual Description", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_704", "file_type": "json", "from": "Jeff Donahue", "predicate": "is_author", "to": "Long-term Recurrent Convolutional Networks", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_711", "file_type": "json", "from": "Jeff Donahue", "predicate": "affiliated_with", "to": "UC Berkeley", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_704", "file_type": "json", "from": "Lisa Anne Hendricks", "predicate": "is_author", "to": "Long-term Recurrent Convolutional Networks", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_711", "file_type": "json", "from": "Lisa Anne Hendricks", "predicate": "affiliated_with", "to": "UC Berkeley", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_705", "file_type": "json", "from": "Sergio Guadarrama", "predicate": "contributed_to", "to": "Donahue_Long-Term_Recurrent_Convolutional_2015_CVPR_paper", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_711", "file_type": "json", "from": "Sergio Guadarrama", "predicate": "affiliated_with", "to": "UC Berkeley", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_705", "file_type": "json", "from": "Donahue_Long-Term_Recurrent_Convolutional_2015_CVPR_paper", "predicate": "discusses", "to": "Visual Features", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_705", "file_type": "json", "from": "Donahue_Long-Term_Recurrent_Convolutional_2015_CVPR_paper", "predicate": "generates", "to": "Predictions", "type": "factual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_705", "file_type": "json", "from": "Donahue_Long-Term_Recurrent_Convolutional_2015_CVPR_paper", "predicate": "operates_on", "to": "Visual Input", "type": "factual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_705", "file_type": "json", "from": "Marcus Rohrbach", "predicate": "contributed_to", "to": "Donahue_Long-Term_Recurrent_Convolutional_2015_CVPR_paper", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_705", "file_type": "json", "from": "Subhashini Venugopalan", "predicate": "contributed_to", "to": "Donahue_Long-Term_Recurrent_Convolutional_2015_CVPR_paper", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_711", "file_type": "json", "from": "Subhashini Venugopalan", "predicate": "affiliated_with", "to": "UT Austin", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_705", "file_type": "json", "from": "Kate Saenko", "predicate": "contributed_to", "to": "Donahue_Long-Term_Recurrent_Convolutional_2015_CVPR_paper", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_712", "file_type": "json", "from": "Kate Saenko", "predicate": "affiliation", "to": "UMass Lowell", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_705", "file_type": "json", "from": "Trevor Darrell", "predicate": "contributed_to", "to": "Donahue_Long-Term_Recurrent_Convolutional_2015_CVPR_paper", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_712", "file_type": "json", "from": "Trevor Darrell", "predicate": "affiliation", "to": "UC Berkeley", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_705", "file_type": "json", "from": "Visual Features", "predicate": "is_aspect_of", "to": "Donahue_Long-Term_Recurrent_Convolutional_2015_CVPR_paper", "type": "conceptual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_706", "file_type": "json", "from": "Models", "predicate": "based_on", "to": "deep convolutional networks", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_706", "file_type": "json", "from": "Models", "predicate": "are", "to": "recurrent", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_706", "file_type": "json", "from": "Models", "predicate": "are", "to": "temporally deep", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_706", "file_type": "json", "from": "Models", "predicate": "demonstrate value on", "to": "video recognition tasks", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_706", "file_type": "json", "from": "Models", "predicate": "is_used_for", "to": "image description", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_706", "file_type": "json", "from": "Models", "predicate": "address", "to": "video narration challenges", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_707", "file_type": "json", "from": "recurrent convolutional models", "predicate": "are", "to": "doubly deep", "type": "conceptual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_707", "file_type": "json", "from": "recurrent convolutional models", "predicate": "have_advantage", "to": "complex target concepts", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_707", "file_type": "json", "from": "recurrent convolutional models", "predicate": "have_advantage", "to": "limited training data", "type": "conceptual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_707", "file_type": "json", "from": "network state updates", "predicate": "enable", "to": "long-term dependencies", "type": "causal", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_707", "file_type": "json", "from": "long-term RNN models", "predicate": "map", "to": "variable length outputs", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_707", "file_type": "json", "from": "long-term RNN models", "predicate": "model", "to": "complex temporal dynamics", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_707", "file_type": "json", "from": "recurrent long-term models", "predicate": "are_connected_to", "to": "visual convnet models", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_709", "file_type": "json", "from": "Recurrent Convolutional Networks (LRCNs)", "predicate": "applied_to", "to": "Video Recognition", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_708", "file_type": "json", "from": "Recurrent Convolutional Networks (LRCNs)", "predicate": "addresses", "to": "Long-Term Dependencies", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_708", "file_type": "json", "from": "Recurrent Convolutional Networks (LRCNs)", "predicate": "has_advantage_over", "to": "state-of-the-art models", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_708", "file_type": "json", "from": "Recurrent Convolutional Networks (LRCNs)", "predicate": "improves", "to": "recognition", "type": "factual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_708", "file_type": "json", "from": "Video Recognition", "predicate": "relies_on", "to": "Convolutional Networks", "type": "conceptual", "width": 0.82}, {"arrows": "to", "chunk_id": "doc_0_chunk_709", "file_type": "json", "from": "Video Recognition", "predicate": "related_to", "to": "Sequence Learning", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_709", "file_type": "json", "from": "Long-Term Dependencies", "predicate": "requires", "to": "Recurrent Convolutional Networks (LRCNs)", "type": "conceptual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_708", "file_type": "json", "from": "Sequence Learning", "predicate": "enables", "to": "Long-Term Dependencies", "type": "conceptual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_708", "file_type": "json", "from": "state-of-the-art models", "predicate": "are", "to": "separately optimized", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_709", "file_type": "json", "from": "Action Classification", "predicate": "uses", "to": "Long short-term memory recurrent neural networks", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_709", "file_type": "json", "from": "Action Classification", "predicate": "occurs_in", "to": "soccer videos", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_709", "file_type": "json", "from": "Long short-term memory recurrent neural networks", "predicate": "addresses", "to": "Long-Term Dependencies", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_709", "file_type": "json", "from": "Multimodal neural language models", "predicate": "combines", "to": "visual-semantic embeddings", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_709", "file_type": "json", "from": "Unifying visual-semantic embeddings", "predicate": "achieved_by", "to": "Multimodal neural language models", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_709", "file_type": "json", "from": "Video in sentences out", "predicate": "investigates", "to": "Video Recognition", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_710", "file_type": "json", "from": "Video in sentences out", "predicate": "presented_at", "to": "UAI", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_710", "file_type": "json", "from": "High accuracy optical flow estimation", "predicate": "presented_at", "to": "ECCV", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_710", "file_type": "json", "from": "3D convolutional neural networks", "predicate": "published_in", "to": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_710", "file_type": "json", "from": "Generating sequences", "predicate": "published_as", "to": "arXiv preprint arXiv:1308.0850", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_710", "file_type": "json", "from": "Recurrent neural networks", "predicate": "used_for", "to": "generating sequences", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_711", "file_type": "json", "from": "J. Deng", "predicate": "affilates_with", "to": "CVPR", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_711", "file_type": "json", "from": "W. Dong", "predicate": "affilates_with", "to": "CVPR", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_711", "file_type": "json", "from": "R. Socher", "predicate": "affilates_with", "to": "CVPR", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_711", "file_type": "json", "from": "L.-J. Li", "predicate": "affilages_with", "to": "CVPR", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_711", "file_type": "json", "from": "K. Li", "predicate": "affiliates_with", "to": "CVPR", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_711", "file_type": "json", "from": "L. Fei-Fei", "predicate": "affiliates_with", "to": "CVPR", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_711", "file_type": "json", "from": "A. Frome", "predicate": "affiliates_with", "to": "NIPS", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_712", "file_type": "json", "from": "ubhashini Venugopalan", "predicate": "affiliation", "to": "UT Austin", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_712", "file_type": "json", "from": "SOM", "predicate": "is_a", "to": "Semantic Obviousness Metric", "type": "conceptual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_712", "file_type": "json", "from": "SOM", "predicate": "used_for", "to": "Image Quality Assessment", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_712", "file_type": "json", "from": "SOM", "predicate": "presented_at", "to": "CVPR", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_712", "file_type": "json", "from": "Peng Zhang", "predicate": "author_of", "to": "SOM", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_715", "file_type": "json", "from": "Peng Zhang", "predicate": "has_email", "to": "pzhangoo@mail.ustc.edu.cn", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_712", "file_type": "json", "from": "Wengang Zhou", "predicate": "author_of", "to": "SOM", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_715", "file_type": "json", "from": "Wengang Zhou", "predicate": "has_email", "to": "zhwg@ustc.edu.cn", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_712", "file_type": "json", "from": "Lei Wu", "predicate": "author_of", "to": "SOM", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_715", "file_type": "json", "from": "Lei Wu", "predicate": "has_email", "to": "wuleibig@gmail.com", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_712", "file_type": "json", "from": "Houqiang Li", "predicate": "author_of", "to": "SOM", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_715", "file_type": "json", "from": "Houqiang Li", "predicate": "has_email", "to": "lihq@ustc.edu.cn", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_712", "file_type": "json", "from": "Zhang_SOM_Semantic_Obviousness_2015_CVPR_paper.pdf", "predicate": "contains", "to": "SOM", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_713", "file_type": "json", "from": "Image quality assessment (IQA)", "predicate": "aims_to", "to": "objectively estimate human perception", "type": "definitional", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_713", "file_type": "json", "from": "Image quality assessment (IQA)", "predicate": "targets", "to": "problem", "type": "definitional", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_713", "file_type": "json", "from": "no-referece (NR) image quality assessment (IQA) framework", "predicate": "based_on", "to": "semantic obviousness", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_713", "file_type": "json", "from": "semantic-level factors", "predicate": "affects", "to": "human perception of image quality", "type": "causal", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_714", "file_type": "json", "from": "comparable results", "predicate": "related_to", "to": "state-of-the-art full-referece IQA (FR-IQA) methods", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_714", "file_type": "json", "from": "generalization ability", "predicate": "belongs_to", "to": "approach", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_714", "file_type": "json", "from": "NR-IQA algorithms", "predicate": "are_type_of", "to": "IQA algorithms", "type": "conceptual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_715", "file_type": "json", "from": "NR-IQA algorithms", "predicate": "compared_to", "to": "existing algorithms", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_714", "file_type": "json", "from": "full-referece IQA (FR-IQA) methods", "predicate": "are_type_of", "to": "IQA methods", "type": "conceptual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_715", "file_type": "json", "from": "Image Quality Assessment (IQA)", "predicate": "is_a", "to": "assessment method", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_715", "file_type": "json", "from": "No-Reference Image Quality Assessment (NR-IQA)", "predicate": "is_a", "to": "IQA method", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_715", "file_type": "json", "from": "NR-IQA", "predicate": "achieves", "to": "comparable results", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_715", "file_type": "json", "from": "Damien Teney", "predicate": "is_author", "to": "Learning Similarity Metrics", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_715", "file_type": "json", "from": "Learning Similarity Metrics", "predicate": "addresses", "to": "Dynamic Scene Segmentation", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_715", "file_type": "json", "from": "Matthew Brown", "predicate": "is_author", "to": "Learning Similarity Metrics", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_716", "file_type": "json", "from": "dynamic textures", "predicate": "exhibit", "to": "complex patterns", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_716", "file_type": "json", "from": "metric-learning framework", "predicate": "optimizes", "to": "representation", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_717", "file_type": "json", "from": "object and motion segmentation", "predicate": "is_type_of", "to": "segmentation", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_718", "file_type": "json", "from": "Dynamic textures", "predicate": "used_in", "to": "video segmentation", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_718", "file_type": "json", "from": "Spatio-temporal filters", "predicate": "used_in", "to": "video segmentation", "type": "factual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_718", "file_type": "json", "from": "Metric learning", "predicate": "used_in", "to": "video segmentation", "type": "factual", "width": 0.75}, {"arrows": "to", "chunk_id": "doc_0_chunk_718", "file_type": "json", "from": "Graph-based segmentation", "predicate": "is_type_of", "to": "image segmentation", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_718", "file_type": "json", "from": "Alpert et al. (2007)", "predicate": "published_in", "to": "CVPR", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_718", "file_type": "json", "from": "Brox \u0026 Malik (2010)", "predicate": "published_in", "to": "ECCV", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_718", "file_type": "json", "from": "Chan \u0026 Vasconcelos (2008)", "predicate": "published_in", "to": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_718", "file_type": "json", "from": "Chan \u0026 Vasconcelos (2009)", "predicate": "published_in", "to": "CVPR", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_719", "file_type": "json", "from": "Variational layered dynamic textures", "predicate": "is_a", "to": "dynamic textures", "type": "conceptual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_719", "file_type": "json", "from": "Corso", "predicate": "delivered", "to": "CVPR tutorial on video segmentation", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_719", "file_type": "json", "from": "Spacetime texture representation and recognition", "predicate": "uses", "to": "spatio-temporal orientation analysis", "type": "conceptual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_719", "file_type": "json", "from": "Dynamic texture detection", "predicate": "based_on", "to": "motion analysis", "type": "conceptual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_720", "file_type": "json", "from": "S.", "predicate": "authored", "to": "Dynamic textures", "type": "factual", "width": 6}, {"arrows": "to", "chunk_id": "doc_0_chunk_720", "file_type": "json", "from": "Wu, Y. N.", "predicate": "authored", "to": "Dynamic textures", "type": "factual", "width": 6}, {"arrows": "to", "chunk_id": "doc_0_chunk_720", "file_type": "json", "from": "Fazekas, S.", "predicate": "authored", "to": "Dynamic texture detection", "type": "factual", "width": 6}, {"arrows": "to", "chunk_id": "doc_0_chunk_720", "file_type": "json", "from": "Amiatz, T.", "predicate": "authored", "to": "Dynamic texture detection", "type": "factual", "width": 6}, {"arrows": "to", "chunk_id": "doc_0_chunk_720", "file_type": "json", "from": "Chetverikov, D.", "predicate": "authored", "to": "Dynamic texture detection", "type": "factual", "width": 6}, {"arrows": "to", "chunk_id": "doc_0_chunk_720", "file_type": "json", "from": "Feichtenhofer, C.", "predicate": "authored", "to": "Bags of spacetime energies", "type": "factual", "width": 6}, {"arrows": "to", "chunk_id": "doc_0_chunk_720", "file_type": "json", "from": "Pinz, A.", "predicate": "authored", "to": "Bags of spacetime energies", "type": "factual", "width": 6}, {"arrows": "to", "chunk_id": "doc_0_chunk_720", "file_type": "json", "from": "Wilides, R.", "predicate": "authored", "to": "Bags of spacetime energies", "type": "factual", "width": 6}, {"arrows": "to", "chunk_id": "doc_0_chunk_720", "file_type": "json", "from": "Teney, Damien", "predicate": "affiliated_with", "to": "Carnegie Mellon University", "type": "factual", "width": 6}, {"arrows": "to", "chunk_id": "doc_0_chunk_720", "file_type": "json", "from": "Brown, Matthew", "predicate": "affiliated_with", "to": "University of Bath", "type": "factual", "width": 6}, {"arrows": "to", "chunk_id": "doc_0_chunk_720", "file_type": "json", "from": "Kit, Dimitry", "predicate": "affiliated_with", "to": "University of Bath", "type": "factual", "width": 6}, {"arrows": "to", "chunk_id": "doc_0_chunk_720", "file_type": "json", "from": "Hall, Peter", "predicate": "affiliated_with", "to": "University of Bath", "type": "factual", "width": 6}, {"arrows": "to", "chunk_id": "doc_0_chunk_720", "file_type": "json", "from": "Li, Yang", "predicate": "author_of", "to": "Reliable Patch Tracers", "type": "factual", "width": 6}, {"arrows": "to", "chunk_id": "doc_0_chunk_720", "file_type": "json", "from": "Zhu, Jianke", "predicate": "author_of", "to": "Reliable Patch Tracers", "type": "factual", "width": 6}, {"arrows": "to", "chunk_id": "doc_0_chunk_720", "file_type": "json", "from": "Hoi, Steven C.H.", "predicate": "author_of", "to": "Reliable Patch Tracers", "type": "factual", "width": 6}, {"arrows": "to", "chunk_id": "doc_0_chunk_721", "file_type": "json", "from": "modern trackers", "predicate": "use", "to": "bounding box", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_721", "file_type": "json", "from": "tracking results", "predicate": "sensitive to", "to": "initialization", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_721", "file_type": "json", "from": "Reliable Patch Tracers (RPT)", "predicate": "is", "to": "tracking method", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_721", "file_type": "json", "from": "Reliable Patch Tracers (RPT)", "predicate": "attempts to", "to": "identify reliable patches", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_721", "file_type": "json", "from": "reliable patches", "predicate": "can be", "to": "tracked effectively", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_721", "file_type": "json", "from": "reliable patches", "predicate": "are distributed over", "to": "image", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_721", "file_type": "json", "from": "tracking reliability metric", "predicate": "measures", "to": "reliability of patch", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_721", "file_type": "json", "from": "probability model", "predicate": "estimates", "to": "distribution of reliable patches", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_721", "file_type": "json", "from": "probability model", "predicate": "uses", "to": "sequential Monte Carlo framework", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_721", "file_type": "json", "from": "motion trajectories", "predicate": "distinguish", "to": "reliable patches from background", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_721", "file_type": "json", "from": "visual object", "predicate": "is defined as", "to": "cluster", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_723", "file_type": "json", "from": "source code", "predicate": "is_available", "to": "public", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_723", "file_type": "json", "from": "Adam, A., Rivlin, E., \u0026 Shimshoni, I. (2006)", "predicate": "published", "to": "Robust fragments-based tracking", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_723", "file_type": "json", "from": "Robust fragments-based tracking", "predicate": "uses", "to": "integral histogram", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_723", "file_type": "json", "from": "Lucas, B. D., \u0026 Kanade, T. (1981)", "predicate": "developed", "to": "iterative image registration technique", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_723", "file_type": "json", "from": "Poling, B., Lerman, G., \u0026 Szlarm, A. (2014)", "predicate": "presented", "to": "Better feature tracking", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_723", "file_type": "json", "from": "Cai, Z., Wen, L., Yang, J., Lei, Z., \u0026 Li, S. (2012)", "predicate": "introduced", "to": "Structured visual tracking", "type": "factual", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_723", "file_type": "json", "from": "Sequential Monte Carlo Methods in Practice", "predicate": "describes", "to": "Sequential Monte Carlo Framework", "type": "conceptual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_724", "file_type": "json", "from": "Szlarm", "predicate": "presented_in", "to": "CVPR", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_724", "file_type": "json", "from": "Cai et al.", "predicate": "presented_in", "to": "ACCV", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_724", "file_type": "json", "from": "Mannning et al.", "predicate": "authored", "to": "Introduction to Information Retrieval", "type": "factual", "width": 0.99}, {"arrows": "to", "chunk_id": "doc_0_chunk_724", "file_type": "json", "from": "Danelljan et al.", "predicate": "presented_in", "to": "CVPR", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_724", "file_type": "json", "from": "Everingham et al.", "predicate": "created", "to": "The pascal visual object classes(voc) challenge", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_724", "file_type": "json", "from": "Grundmann et al.", "predicate": "presented_in", "to": "CVPR", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_724", "file_type": "json", "from": "Yang Li", "predicate": "affiliated_with", "to": "College of Computer Science", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_724", "file_type": "json", "from": "Yang Li", "predicate": "affiliated_with", "to": "Zhejiang University", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_724", "file_type": "json", "from": "Yang Li", "predicate": "email", "to": "liyang89@zju.edu.cn", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_724", "file_type": "json", "from": "Jianke Zhu", "predicate": "affiliated_with", "to": "College of Computer Science", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_725", "file_type": "json", "from": "V.", "predicate": "affiliated_with", "to": "CVPR", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_725", "file_type": "json", "from": "Han, M.", "predicate": "affiliated_with", "to": "CVPR", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_725", "file_type": "json", "from": "Essa, I.", "predicate": "affiliated_with", "to": "CVPR", "type": "factual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_726", "file_type": "json", "from": "SALICON", "predicate": "is_effort_to", "to": "understand and predict visual attention", "type": "conceptual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_726", "file_type": "json", "from": "SALICON", "predicate": "focuses_on", "to": "collecting large-scale human data", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_726", "file_type": "json", "from": "SALICON", "predicate": "offers", "to": "new possibilities for visual understanding", "type": "conceptual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_726", "file_type": "json", "from": "mouse-contingent paradigm", "predicate": "replaces", "to": "eye tracker", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_726", "file_type": "json", "from": "SALICON dataset", "predicate": "is_dataset_of", "to": "human \u0027free-viewing\u0027 data", "type": "factual", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_726", "file_type": "json", "from": "SALICON dataset", "predicate": "contains_data_from", "to": "10,000 images", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_726", "file_type": "json", "from": "SALICON dataset", "predicate": "is_based_on", "to": "Microsoft COCO dataset", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_726", "file_type": "json", "from": "SALICON dataset", "predicate": "serves_as", "to": "ground truth for evaluating salience algorithms", "type": "factual", "width": 0.93}, {"arrows": "to", "chunk_id": "doc_0_chunk_726", "file_type": "json", "from": "SALICON dataset", "predicate": "complements", "to": "existing annotations", "type": "factual", "width": 0.88}, {"arrows": "to", "chunk_id": "doc_0_chunk_727", "file_type": "json", "from": "Ming Jiang", "predicate": "affiliated_with", "to": "National University of Singapore", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_727", "file_type": "json", "from": "Shengshen Huang", "predicate": "affiliated_with", "to": "National University of Singapore", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_727", "file_type": "json", "from": "Juanyong Duan", "predicate": "affiliated_with", "to": "National University of Singapore", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_727", "file_type": "json", "from": "Qi Zhao", "predicate": "affiliated_with", "to": "National University of Singapore", "type": "factual", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_727", "file_type": "json", "from": "Deep LAC", "predicate": "focuses_on", "to": "fine-grained recognition", "type": "factual", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_728", "file_type": "json", "from": "Deep LAC", "predicate": "is_a", "to": "Fine-grained Recognition", "type": "conceptual", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_728", "file_type": "json", "from": "Deep LAC", "predicate": "is_author_of", "to": "Xiaoyong Shen", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_728", "file_type": "json", "from": "Deep LAC", "predicate": "presented_at", "to": "CVPR", "type": "factual", "width": 0.9}, {"arrows": "to", "chunk_id": "doc_0_chunk_728", "file_type": "json", "from": "Deep LAC", "predicate": "performs", "to": "Localization", "type": "conceptual", "width": 0.7}, {"arrows": "to", "chunk_id": "doc_0_chunk_728", "file_type": "json", "from": "Deep LAC", "predicate": "performs", "to": "Classification", "type": "conceptual", "width": 0.7}, {"arrows": "to", "chunk_id": "doc_0_chunk_729", "file_type": "json", "from": "Di Lin", "predicate": "author of", "to": "Deep LAC", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_729", "file_type": "json", "from": "Xiaoyong Shen", "predicate": "author of", "to": "Deep LAC", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_729", "file_type": "json", "from": "Cewu Lu", "predicate": "author of", "to": "Deep LAC", "type": "factual", "width": 0.95}, {"arrows": "to", "chunk_id": "doc_0_chunk_728", "file_type": "json", "from": "Lin_Deep_LAC_Deep_2015_CVPR_paper", "predicate": "is_publication", "to": "Deep LAC", "type": "factual", "width": 0.85}, {"arrows": "to", "chunk_id": "doc_0_chunk_728", "file_type": "json", "from": "eleqiz@nus.edu.sg", "predicate": "is_contact_email", "to": "National University of Singapore", "type": "factual", "width": 0.8}, {"arrows": "to", "chunk_id": "doc_0_chunk_729", "file_type": "json", "from": "fine-grained recognition system", "predicate": "incorporates", "to": "part localization", "type": "functional", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_729", "file_type": "json", "from": "fine-grained recognition system", "predicate": "incorporates", "to": "alignment", "type": "functional", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_729", "file_type": "json", "from": "fine-grained recognition system", "predicate": "incorporates", "to": "classification", "type": "functional", "width": 0.98}, {"arrows": "to", "chunk_id": "doc_0_chunk_729", "file_type": "json", "from": "valve linkage function", "predicate": "enables", "to": "back-propagation chaining", "type": "functional", "width": 0.97}, {"arrows": "to", "chunk_id": "doc_0_chunk_729", "file_type": "json", "from": "valve linkage function", "predicate": "compromises", "to": "classification errors", "type": "functional", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_729", "file_type": "json", "from": "valve linkage function", "predicate": "compromises", "to": "alignment errors", "type": "functional", "width": 0.94}, {"arrows": "to", "chunk_id": "doc_0_chunk_729", "file_type": "json", "from": "valve linkage function", "predicate": "helps", "to": "update localization", "type": "functional", "width": 0.92}, {"arrows": "to", "chunk_id": "doc_0_chunk_729", "file_type": "json", "from": "deep LAC system", "predicate": "uses", "to": "valve linkage function", "type": "functional", "width": 0.96}, {"arrows": "to", "chunk_id": "doc_0_chunk_729", "file_type": "json", "from": "LAC system", "predicate": "performs well on", "to": "fine-grained object data", "type": "factual", "width": 0.93}]);

                  nodeColors = {};
                  allNodes = nodes.get({ returnType: "Object" });
                  for (nodeId in allNodes) {
                    nodeColors[nodeId] = allNodes[nodeId].color;
                  }
                  allEdges = edges.get({ returnType: "Object" });
                  // adding nodes and edges to the graph
                  data = {nodes: nodes, edges: edges};

                  var options = {"nodes": {"font": {"size": 15}}, "edges": {"arrows": {"to": {"enabled": true}}, "font": {"size": 12}}, "physics": {"stabilization": {"iterations": 100}}, "interaction": {"navigationButtons": true, "hover": true}};

                  


                  

                  network = new vis.Network(container, data, options);

                  

                  

                  


                  
                      network.on("stabilizationProgress", function(params) {
                          document.getElementById('loadingBar').removeAttribute("style");
                          var maxWidth = 496;
                          var minWidth = 20;
                          var widthFactor = params.iterations/params.total;
                          var width = Math.max(minWidth,maxWidth * widthFactor);
                          document.getElementById('bar').style.width = width + 'px';
                          document.getElementById('text').innerHTML = Math.round(widthFactor*100) + '%';
                      });
                      network.once("stabilizationIterationsDone", function() {
                          document.getElementById('text').innerHTML = '100%';
                          document.getElementById('bar').style.width = '496px';
                          document.getElementById('loadingBar').style.opacity = 0;
                          // really clean the dom element
                          setTimeout(function () {document.getElementById('loadingBar').style.display = 'none';}, 500);
                      });
                  

                  return network;

              }
              drawGraph();
        </script>
    </body>
</html>