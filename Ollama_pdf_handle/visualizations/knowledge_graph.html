<html>
    <head>
        <meta charset="utf-8">
        
            <script src="lib/bindings/utils.js"></script>
            <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/vis-network/9.1.2/dist/dist/vis-network.min.css" integrity="sha512-WgxfT5LWjfszlPHXRmBWHkV2eceiWTOBvrKCNbdgDYTHrT2AeLCGbF4sZlZw3UMN3WtL0tGUoIAKsu8mllg/XA==" crossorigin="anonymous" referrerpolicy="no-referrer" />
            <script src="https://cdnjs.cloudflare.com/ajax/libs/vis-network/9.1.2/dist/vis-network.min.js" integrity="sha512-LnvoEWDFrqGHlHmDD2101OrLcbsfkrzoSpvtSQtxK3RMnRV0eOkhhBN2dXHKRrUU8p2DGRTk35n4O8nWSVe1mQ==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
            
        
<center>
<h1></h1>
</center>

<!-- <link rel="stylesheet" href="../node_modules/vis/dist/vis.min.css" type="text/css" />
<script type="text/javascript" src="../node_modules/vis/dist/vis.js"> </script>-->
        <link
          href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/css/bootstrap.min.css"
          rel="stylesheet"
          integrity="sha384-eOJMYsd53ii+scO/bJGFsiCZc+5NDVN2yr8+0RDqr0Ql0h+rP48ckxlpbzKgwra6"
          crossorigin="anonymous"
        />
        <script
          src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/js/bootstrap.bundle.min.js"
          integrity="sha384-JEW9xMcG8R+pH31jmWH6WWP0WintQrMb4s7ZOdauHnUtxwoG2vI5DkLtS3qm9Ekf"
          crossorigin="anonymous"
        ></script>


        <center>
          <h1></h1>
        </center>
        <style type="text/css">

             #mynetwork {
                 width: 1200px;
                 height: 800px;
                 background-color: #ffffff;
                 border: 1px solid lightgray;
                 position: relative;
                 float: left;
             }

             
             #loadingBar {
                 position:absolute;
                 top:0px;
                 left:0px;
                 width: 1200px;
                 height: 800px;
                 background-color:rgba(200,200,200,0.8);
                 -webkit-transition: all 0.5s ease;
                 -moz-transition: all 0.5s ease;
                 -ms-transition: all 0.5s ease;
                 -o-transition: all 0.5s ease;
                 transition: all 0.5s ease;
                 opacity:1;
             }

             #bar {
                 position:absolute;
                 top:0px;
                 left:0px;
                 width:20px;
                 height:20px;
                 margin:auto auto auto auto;
                 border-radius:11px;
                 border:2px solid rgba(30,30,30,0.05);
                 background: rgb(0, 173, 246); /* Old browsers */
                 box-shadow: 2px 0px 4px rgba(0,0,0,0.4);
             }

             #border {
                 position:absolute;
                 top:10px;
                 left:10px;
                 width:500px;
                 height:23px;
                 margin:auto auto auto auto;
                 box-shadow: 0px 0px 4px rgba(0,0,0,0.2);
                 border-radius:10px;
             }

             #text {
                 position:absolute;
                 top:8px;
                 left:530px;
                 width:30px;
                 height:50px;
                 margin:auto auto auto auto;
                 font-size:22px;
                 color: #000000;
             }

             div.outerBorder {
                 position:relative;
                 top:400px;
                 width:600px;
                 height:44px;
                 margin:auto auto auto auto;
                 border:8px solid rgba(0,0,0,0.1);
                 background: rgb(252,252,252); /* Old browsers */
                 background: -moz-linear-gradient(top,  rgba(252,252,252,1) 0%, rgba(237,237,237,1) 100%); /* FF3.6+ */
                 background: -webkit-gradient(linear, left top, left bottom, color-stop(0%,rgba(252,252,252,1)), color-stop(100%,rgba(237,237,237,1))); /* Chrome,Safari4+ */
                 background: -webkit-linear-gradient(top,  rgba(252,252,252,1) 0%,rgba(237,237,237,1) 100%); /* Chrome10+,Safari5.1+ */
                 background: -o-linear-gradient(top,  rgba(252,252,252,1) 0%,rgba(237,237,237,1) 100%); /* Opera 11.10+ */
                 background: -ms-linear-gradient(top,  rgba(252,252,252,1) 0%,rgba(237,237,237,1) 100%); /* IE10+ */
                 background: linear-gradient(to bottom,  rgba(252,252,252,1) 0%,rgba(237,237,237,1) 100%); /* W3C */
                 filter: progid:DXImageTransform.Microsoft.gradient( startColorstr='#fcfcfc', endColorstr='#ededed',GradientType=0 ); /* IE6-9 */
                 border-radius:72px;
                 box-shadow: 0px 0px 10px rgba(0,0,0,0.2);
             }
             

             

             
        </style>
    </head>


    <body>
        <div class="card" style="width: 100%">
            
            
            <div id="mynetwork" class="card-body"></div>
        </div>

        
            <div id="loadingBar">
              <div class="outerBorder">
                <div id="text">0%</div>
                <div id="border">
                  <div id="bar"></div>
                </div>
              </div>
            </div>
        
        

        <script type="text/javascript">

              // initialize global variables.
              var edges;
              var nodes;
              var allNodes;
              var allEdges;
              var nodeColors;
              var originalNodes;
              var network;
              var container;
              var options, data;
              var filter = {
                  item : '',
                  property : '',
                  value : []
              };

              

              

              // This method is responsible for drawing the graph, returns the drawn network
              function drawGraph() {
                  var container = document.getElementById('mynetwork');

                  

                  // parsing and collecting nodes and edges from the python
                  nodes = new vis.DataSet([{"color": "#66CCFF", "id": "Saurabh Singh", "label": "Saurabh Singh", "shape": "dot", "size": 25, "title": "Saurabh Singh"}, {"color": "#66CCFF", "id": "Learning a Sequential Search for Landmarks", "label": "Learning a Sequential Search for Landmarks", "shape": "dot", "size": 25, "title": "Learning a Sequential Search for Landmarks"}, {"color": "#66CCFF", "id": "Derek Hoiem", "label": "Derek Hoiem", "shape": "dot", "size": 25, "title": "Derek Hoiem"}, {"color": "#66CCFF", "id": "David Forsyth", "label": "David Forsyth", "shape": "dot", "size": 25, "title": "David Forsyth"}, {"color": "#66CCFF", "id": "CVPR", "label": "CVPR", "shape": "dot", "size": 25, "title": "CVPR"}, {"color": "#66CCFF", "id": "2015", "label": "2015", "shape": "dot", "size": 25, "title": "2015"}, {"color": "#66CCFF", "id": "Glaucoma", "label": "Glaucoma", "shape": "dot", "size": 25, "title": "Glaucoma"}, {"color": "#66CCFF", "id": "Sequential Search", "label": "Sequential Search", "shape": "dot", "size": 25, "title": "Sequential Search"}, {"color": "#66CCFF", "id": "Landmarks", "label": "Landmarks", "shape": "dot", "size": 25, "title": "Landmarks"}, {"color": "#66CCFF", "id": "Ollama", "label": "Ollama", "shape": "dot", "size": 25, "title": "Ollama"}, {"color": "#66CCFF", "id": "method", "label": "method", "shape": "dot", "size": 25, "title": "method"}, {"color": "#66CCFF", "id": "landmarks", "label": "landmarks", "shape": "dot", "size": 25, "title": "landmarks"}, {"color": "#66CCFF", "id": "images of objects", "label": "images of objects", "shape": "dot", "size": 25, "title": "images of objects"}, {"color": "#66CCFF", "id": "appearance", "label": "appearance", "shape": "dot", "size": 25, "title": "appearance"}, {"color": "#66CCFF", "id": "parsing human body layouts", "label": "parsing human body layouts", "shape": "dot", "size": 25, "title": "parsing human body layouts"}, {"color": "#66CCFF", "id": "finding landmarks in images of birds", "label": "finding landmarks in images of birds", "shape": "dot", "size": 25, "title": "finding landmarks in images of birds"}, {"color": "#66CCFF", "id": "sequential search", "label": "sequential search", "shape": "dot", "size": 25, "title": "sequential search"}, {"color": "#66CCFF", "id": "landmark addition", "label": "landmark addition", "shape": "dot", "size": 25, "title": "landmark addition"}, {"color": "#66CCFF", "id": "image", "label": "image", "shape": "dot", "size": 25, "title": "image"}, {"color": "#66CCFF", "id": "groups", "label": "groups", "shape": "dot", "size": 25, "title": "groups"}, {"color": "#66CCFF", "id": "learned function", "label": "learned function", "shape": "dot", "size": 25, "title": "learned function"}, {"color": "#66CCFF", "id": "landmark group", "label": "landmark group", "shape": "dot", "size": 25, "title": "landmark group"}, {"color": "#66CCFF", "id": "expand groups", "label": "expand groups", "shape": "dot", "size": 25, "title": "expand groups"}, {"color": "#66CCFF", "id": "scoring function", "label": "scoring function", "shape": "dot", "size": 25, "title": "scoring function"}, {"color": "#66CCFF", "id": "data labelled with landmarks", "label": "data labelled with landmarks", "shape": "dot", "size": 25, "title": "data labelled with landmarks"}, {"color": "#66CCFF", "id": "spatial model", "label": "spatial model", "shape": "dot", "size": 25, "title": "spatial model"}, {"color": "#66CCFF", "id": "kinematics of landmark groups", "label": "kinematics of landmark groups", "shape": "dot", "size": 25, "title": "kinematics of landmark groups"}, {"color": "#66CCFF", "id": "strong performance", "label": "strong performance", "shape": "dot", "size": 25, "title": "strong performance"}, {"color": "#66CCFF", "id": "landmark", "label": "landmark", "shape": "dot", "size": 25, "title": "landmark"}, {"color": "#66CCFF", "id": "initial landmark", "label": "initial landmark", "shape": "dot", "size": 25, "title": "initial landmark"}, {"color": "#66CCFF", "id": "data", "label": "data", "shape": "dot", "size": 25, "title": "data"}, {"color": "#66CCFF", "id": "model problems", "label": "model problems", "shape": "dot", "size": 25, "title": "model problems"}, {"color": "#66CCFF", "id": "Method", "label": "Method", "shape": "dot", "size": 25, "title": "Method"}, {"color": "#66CCFF", "id": "Andriluka et al. (2009)", "label": "Andriluka et al. (2009)", "shape": "dot", "size": 25, "title": "Andriluka et al. (2009)"}, {"color": "#66CCFF", "id": "People detection", "label": "People detection", "shape": "dot", "size": 25, "title": "People detection"}, {"color": "#66CCFF", "id": "articulated pose estimation", "label": "articulated pose estimation", "shape": "dot", "size": 25, "title": "articulated pose estimation"}, {"color": "#66CCFF", "id": "Barto (1998)", "label": "Barto (1998)", "shape": "dot", "size": 25, "title": "Barto (1998)"}, {"color": "#66CCFF", "id": "Reinforcement learning", "label": "Reinforcement learning", "shape": "dot", "size": 25, "title": "Reinforcement learning"}, {"color": "#66CCFF", "id": "Felzenszwalb and Huttenlocher (2005)", "label": "Felzenszwalb and Huttenlocher (2005)", "shape": "dot", "size": 25, "title": "Felzenszwalb and Huttenlocher (2005)"}, {"color": "#66CCFF", "id": "Pictorial structures", "label": "Pictorial structures", "shape": "dot", "size": 25, "title": "Pictorial structures"}, {"color": "#66CCFF", "id": "Fergus et al. (2003)", "label": "Fergus et al. (2003)", "shape": "dot", "size": 25, "title": "Fergus et al. (2003)"}, {"color": "#66CCFF", "id": "Unsupervised scale-invariant learning", "label": "Unsupervised scale-invariant learning", "shape": "dot", "size": 25, "title": "Unsupervised scale-invariant learning"}, {"color": "#66CCFF", "id": "Doll\u00b4ar et al. (2009)", "label": "Doll\u00b4ar et al. (2009)", "shape": "dot", "size": 25, "title": "Doll\u00b4ar et al. (2009)"}, {"color": "#66CCFF", "id": "Integral channel features", "label": "Integral channel features", "shape": "dot", "size": 25, "title": "Integral channel features"}, {"color": "#66CCFF", "id": "Eichner and Ferrari (2012)", "label": "Eichner and Ferrari (2012)", "shape": "dot", "size": 25, "title": "Eichner and Ferrari (2012)"}, {"color": "#66CCFF", "id": "collective human pose estimation", "label": "collective human pose estimation", "shape": "dot", "size": 25, "title": "collective human pose estimation"}, {"color": "#66CCFF", "id": "Doll\u00e1r, P.", "label": "Doll\u00e1r, P.", "shape": "dot", "size": 25, "title": "Doll\u00e1r, P."}, {"color": "#66CCFF", "id": "Eichner, M.", "label": "Eichner, M.", "shape": "dot", "size": 25, "title": "Eichner, M."}, {"color": "#66CCFF", "id": "Appearance sharing", "label": "Appearance sharing", "shape": "dot", "size": 25, "title": "Appearance sharing"}, {"color": "#66CCFF", "id": "Fei-Fei, L.", "label": "Fei-Fei, L.", "shape": "dot", "size": 25, "title": "Fei-Fei, L."}, {"color": "#66CCFF", "id": "One-shot learning", "label": "One-shot learning", "shape": "dot", "size": 25, "title": "One-shot learning"}, {"color": "#66CCFF", "id": "Felzenszwalb, P. F.", "label": "Felzenszwalb, P. F.", "shape": "dot", "size": 25, "title": "Felzenszwalb, P. F."}, {"color": "#66CCFF", "id": "Cascade object detection", "label": "Cascade object detection", "shape": "dot", "size": 25, "title": "Cascade object detection"}, {"color": "#66CCFF", "id": "Fergus, R.", "label": "Fergus, R.", "shape": "dot", "size": 25, "title": "Fergus, R."}, {"color": "#66CCFF", "id": "Sparse object category model", "label": "Sparse object category model", "shape": "dot", "size": 25, "title": "Sparse object category model"}, {"color": "#66CCFF", "id": "Wang, Y.", "label": "Wang, Y.", "shape": "dot", "size": 25, "title": "Wang, Y."}, {"color": "#66CCFF", "id": "Multiple tree models", "label": "Multiple tree models", "shape": "dot", "size": 25, "title": "Multiple tree models"}, {"color": "#66CCFF", "id": "BMVC", "label": "BMVC", "shape": "dot", "size": 25, "title": "BMVC"}, {"color": "#66CCFF", "id": "ECCV", "label": "ECCV", "shape": "dot", "size": 25, "title": "ECCV"}, {"color": "#66CCFF", "id": "University of Illinois, Urbana-Champaign", "label": "University of Illinois, Urbana-Champaign", "shape": "dot", "size": 25, "title": "University of Illinois, Urbana-Champaign"}, {"color": "#66CCFF", "id": "University of Indiana", "label": "University of Indiana", "shape": "dot", "size": 25, "title": "University of Indiana"}, {"color": "#66CCFF", "id": "Gedas Bertasius", "label": "Gedas Bertasius", "shape": "dot", "size": 25, "title": "Gedas Bertasius"}, {"color": "#66CCFF", "id": "DeepEdge", "label": "DeepEdge", "shape": "dot", "size": 25, "title": "DeepEdge"}, {"color": "#66CCFF", "id": "Lorenzo Torresani", "label": "Lorenzo Torresani", "shape": "dot", "size": 25, "title": "Lorenzo Torresani"}, {"color": "#66CCFF", "id": "Deep Network", "label": "Deep Network", "shape": "dot", "size": 25, "title": "Deep Network"}, {"color": "#66CCFF", "id": "Contour Detection", "label": "Contour Detection", "shape": "dot", "size": 25, "title": "Contour Detection"}, {"color": "#66CCFF", "id": "Top-Down Contour Detection", "label": "Top-Down Contour Detection", "shape": "dot", "size": 25, "title": "Top-Down Contour Detection"}, {"color": "#66CCFF", "id": "Bifurcated Deep Network", "label": "Bifurcated Deep Network", "shape": "dot", "size": 25, "title": "Bifurcated Deep Network"}, {"color": "#66CCFF", "id": "Contour detection", "label": "Contour detection", "shape": "dot", "size": 25, "title": "Contour detection"}, {"color": "#66CCFF", "id": "low-level features", "label": "low-level features", "shape": "dot", "size": 25, "title": "low-level features"}, {"color": "#66CCFF", "id": "novel method", "label": "novel method", "shape": "dot", "size": 25, "title": "novel method"}, {"color": "#66CCFF", "id": "state-of-the-art results", "label": "state-of-the-art results", "shape": "dot", "size": 25, "title": "state-of-the-art results"}, {"color": "#66CCFF", "id": "contour detection", "label": "contour detection", "shape": "dot", "size": 25, "title": "contour detection"}, {"color": "#66CCFF", "id": "object recognition", "label": "object recognition", "shape": "dot", "size": 25, "title": "object recognition"}, {"color": "#66CCFF", "id": "feature engineering", "label": "feature engineering", "shape": "dot", "size": 25, "title": "feature engineering"}, {"color": "#66CCFF", "id": "Arbel\u00e1ez et al. (2011)", "label": "Arbel\u00e1ez et al. (2011)", "shape": "dot", "size": 25, "title": "Arbel\u00e1ez et al. (2011)"}, {"color": "#66CCFF", "id": "Contour detection and hierarchical image segmentation", "label": "Contour detection and hierarchical image segmentation", "shape": "dot", "size": 25, "title": "Contour detection and hierarchical image segmentation"}, {"color": "#66CCFF", "id": "Lim et al. (2013)", "label": "Lim et al. (2013)", "shape": "dot", "size": 25, "title": "Lim et al. (2013)"}, {"color": "#66CCFF", "id": "Sketch tokens", "label": "Sketch tokens", "shape": "dot", "size": 25, "title": "Sketch tokens"}, {"color": "#66CCFF", "id": "Long et al. (2014)", "label": "Long et al. (2014)", "shape": "dot", "size": 25, "title": "Long et al. (2014)"}, {"color": "#66CCFF", "id": "Fully convolutional networks", "label": "Fully convolutional networks", "shape": "dot", "size": 25, "title": "Fully convolutional networks"}, {"color": "#66CCFF", "id": "Malik et al. (2001)", "label": "Malik et al. (2001)", "shape": "dot", "size": 25, "title": "Malik et al. (2001)"}, {"color": "#66CCFF", "id": "Contour and texture analysis", "label": "Contour and texture analysis", "shape": "dot", "size": 25, "title": "Contour and texture analysis"}, {"color": "#66CCFF", "id": "semantic segmentation", "label": "semantic segmentation", "shape": "dot", "size": 25, "title": "semantic segmentation"}, {"color": "#66CCFF", "id": "Girshick", "label": "Girshick", "shape": "dot", "size": 25, "title": "Girshick"}, {"color": "#66CCFF", "id": "Rich feature hierarchies", "label": "Rich feature hierarchies", "shape": "dot", "size": 25, "title": "Rich feature hierarchies"}, {"color": "#66CCFF", "id": "Donahue", "label": "Donahue", "shape": "dot", "size": 25, "title": "Donahue"}, {"color": "#66CCFF", "id": "Darrell", "label": "Darrell", "shape": "dot", "size": 25, "title": "Darrell"}, {"color": "#66CCFF", "id": "Hariharan", "label": "Hariharan", "shape": "dot", "size": 25, "title": "Hariharan"}, {"color": "#66CCFF", "id": "Hypercolumns", "label": "Hypercolumns", "shape": "dot", "size": 25, "title": "Hypercolumns"}, {"color": "#66CCFF", "id": "Arbel\u00e1ez", "label": "Arbel\u00e1ez", "shape": "dot", "size": 25, "title": "Arbel\u00e1ez"}, {"color": "#66CCFF", "id": "Iandola", "label": "Iandola", "shape": "dot", "size": 25, "title": "Iandola"}, {"color": "#66CCFF", "id": "Densenet", "label": "Densenet", "shape": "dot", "size": 25, "title": "Densenet"}, {"color": "#66CCFF", "id": "Jia", "label": "Jia", "shape": "dot", "size": 25, "title": "Jia"}, {"color": "#66CCFF", "id": "Caffe", "label": "Caffe", "shape": "dot", "size": 25, "title": "Caffe"}, {"color": "#66CCFF", "id": "Convolutional architecture", "label": "Convolutional architecture", "shape": "dot", "size": 25, "title": "Convolutional architecture"}, {"color": "#66CCFF", "id": "Girshik", "label": "Girshik", "shape": "dot", "size": 25, "title": "Girshik"}, {"color": "#66CCFF", "id": "Object detection", "label": "Object detection", "shape": "dot", "size": 25, "title": "Object detection"}, {"color": "#66CCFF", "id": "Shelhamer et al.", "label": "Shelhamer et al.", "shape": "dot", "size": 25, "title": "Shelhamer et al."}, {"color": "#66CCFF", "id": "Ren et al.", "label": "Ren et al.", "shape": "dot", "size": 25, "title": "Ren et al."}, {"color": "#66CCFF", "id": "Scale-invariant contour completion", "label": "Scale-invariant contour completion", "shape": "dot", "size": 25, "title": "Scale-invariant contour completion"}, {"color": "#66CCFF", "id": "Condition random fields", "label": "Condition random fields", "shape": "dot", "size": 25, "title": "Condition random fields"}, {"color": "#66CCFF", "id": "University of Pennsylvania", "label": "University of Pennsylvania", "shape": "dot", "size": 25, "title": "University of Pennsylvania"}, {"color": "#66CCFF", "id": "Jianbo Shi", "label": "Jianbo Shi", "shape": "dot", "size": 25, "title": "Jianbo Shi"}, {"color": "#66CCFF", "id": "Dartmouth College", "label": "Dartmouth College", "shape": "dot", "size": 25, "title": "Dartmouth College"}, {"color": "#66CCFF", "id": "Wen Wang", "label": "Wen Wang", "shape": "dot", "size": 25, "title": "Wen Wang"}, {"color": "#66CCFF", "id": "Discrimi nant Analysis", "label": "Discrimi nant Analysis", "shape": "dot", "size": 25, "title": "Discrimi nant Analysis"}, {"color": "#66CCFF", "id": "Ruiping Wang", "label": "Ruiping Wang", "shape": "dot", "size": 25, "title": "Ruiping Wang"}, {"color": "#66CCFF", "id": "Gaussian Distributions", "label": "Gaussian Distributions", "shape": "dot", "size": 25, "title": "Gaussian Distributions"}, {"color": "#66CCFF", "id": "Face Recognition", "label": "Face Recognition", "shape": "dot", "size": 25, "title": "Face Recognition"}, {"color": "#66CCFF", "id": "Wang_Discriminant_Analysis_on_2015_CVPR_paper", "label": "Wang_Discriminant_Analysis_on_2015_CVPR_paper", "shape": "dot", "size": 25, "title": "Wang_Discriminant_Analysis_on_2015_CVPR_paper"}, {"color": "#66CCFF", "id": "Zhiwu Huang", "label": "Zhiwu Huang", "shape": "dot", "size": 25, "title": "Zhiwu Huang"}, {"color": "#66CCFF", "id": "Wang_Discribminant_Analysis_on_2015_CVPR_paper", "label": "Wang_Discribminant_Analysis_on_2015_CVPR_paper", "shape": "dot", "size": 25, "title": "Wang_Discribminant_Analysis_on_2015_CVPR_paper"}, {"color": "#66CCFF", "id": "Shiguan Shan", "label": "Shiguan Shan", "shape": "dot", "size": 25, "title": "Shiguan Shan"}, {"color": "#66CCFF", "id": "Xilin Chen", "label": "Xilin Chen", "shape": "dot", "size": 25, "title": "Xilin Chen"}, {"color": "#66CCFF", "id": "DARG", "label": "DARG", "shape": "dot", "size": 25, "title": "DARG"}, {"color": "#66CCFF", "id": "face recognition problem", "label": "face recognition problem", "shape": "dot", "size": 25, "title": "face recognition problem"}, {"color": "#66CCFF", "id": "image sets", "label": "image sets", "shape": "dot", "size": 25, "title": "image sets"}, {"color": "#66CCFF", "id": "Gaussian Mixture Models", "label": "Gaussian Mixture Models", "shape": "dot", "size": 25, "title": "Gaussian Mixture Models"}, {"color": "#66CCFF", "id": "Gaussian distributions", "label": "Gaussian distributions", "shape": "dot", "size": 25, "title": "Gaussian distributions"}, {"color": "#66CCFF", "id": "Riemannian manifold", "label": "Riemannian manifold", "shape": "dot", "size": 25, "title": "Riemannian manifold"}, {"color": "#66CCFF", "id": "Kernel Discrimiant Analysis", "label": "Kernel Discrimiant Analysis", "shape": "dot", "size": 25, "title": "Kernel Discrimiant Analysis"}, {"color": "#66CCFF", "id": "probabilistic kernels", "label": "probabilistic kernels", "shape": "dot", "size": 25, "title": "probabilistic kernels"}, {"color": "#66CCFF", "id": "geometry", "label": "geometry", "shape": "dot", "size": 25, "title": "geometry"}, {"color": "#66CCFF", "id": "Gaussian components", "label": "Gaussian components", "shape": "dot", "size": 25, "title": "Gaussian components"}, {"color": "#66CCFF", "id": "proposed method", "label": "proposed method", "shape": "dot", "size": 25, "title": "proposed method"}, {"color": "#66CCFF", "id": "face recognition databases", "label": "face recognition databases", "shape": "dot", "size": 25, "title": "face recognition databases"}, {"color": "#66CCFF", "id": "superior performance", "label": "superior performance", "shape": "dot", "size": 25, "title": "superior performance"}, {"color": "#66CCFF", "id": "state-of-the-art approaches", "label": "state-of-the-art approaches", "shape": "dot", "size": 25, "title": "state-of-the-art approaches"}, {"color": "#66CCFF", "id": "Russian components", "label": "Russian components", "shape": "dot", "size": 25, "title": "Russian components"}, {"color": "#66CCFF", "id": "different subjects", "label": "different subjects", "shape": "dot", "size": 25, "title": "different subjects"}, {"color": "#66CCFF", "id": "prior probabilities", "label": "prior probabilities", "shape": "dot", "size": 25, "title": "prior probabilities"}, {"color": "#66CCFF", "id": "challenging", "label": "challenging", "shape": "dot", "size": 25, "title": "challenging"}, {"color": "#66CCFF", "id": "Riemannian Manifold", "label": "Riemannian Manifold", "shape": "dot", "size": 25, "title": "Riemannian Manifold"}, {"color": "#66CCFF", "id": "Discriminant Analysis", "label": "Discriminant Analysis", "shape": "dot", "size": 25, "title": "Discriminant Analysis"}, {"color": "#66CCFF", "id": "Kernel Methods", "label": "Kernel Methods", "shape": "dot", "size": 25, "title": "Kernel Methods"}, {"color": "#66CCFF", "id": "Aranndi et al. (2005)", "label": "Aranndi et al. (2005)", "shape": "dot", "size": 25, "title": "Aranndi et al. (2005)"}, {"color": "#66CCFF", "id": "Face recognition with image sets using manifold density divergence", "label": "Face recognition with image sets using manifold density divergence", "shape": "dot", "size": 25, "title": "Face recognition with image sets using manifold density divergence"}, {"color": "#66CCFF", "id": "Amar \u0026 Nagaoka (2000)", "label": "Amar \u0026 Nagaoka (2000)", "shape": "dot", "size": 25, "title": "Amar \u0026 Nagaoka (2000)"}, {"color": "#66CCFF", "id": "Methods of Information Geometry", "label": "Methods of Information Geometry", "shape": "dot", "size": 25, "title": "Methods of Information Geometry"}, {"color": "#66CCFF", "id": "Information Geometry", "label": "Information Geometry", "shape": "dot", "size": 25, "title": "Information Geometry"}, {"color": "#66CCFF", "id": "Chan et al. (2004)", "label": "Chan et al. (2004)", "shape": "dot", "size": 25, "title": "Chan et al. (2004)"}, {"color": "#66CCFF", "id": "Probabilistic Kernels", "label": "Probabilistic Kernels", "shape": "dot", "size": 25, "title": "Probabilistic Kernels"}, {"color": "#66CCFF", "id": "Probabilistic KernELS", "label": "Probabilistic KernELS", "shape": "dot", "size": 25, "title": "Probabilistic KernELS"}, {"color": "#66CCFF", "id": "Information Divergence", "label": "Information Divergence", "shape": "dot", "size": 25, "title": "Information Divergence"}, {"color": "#66CCFF", "id": "Chan, A. B.", "label": "Chan, A. B.", "shape": "dot", "size": 25, "title": "Chan, A. B."}, {"color": "#66CCFF", "id": "Moreno, P. J.", "label": "Moreno, P. J.", "shape": "dot", "size": 25, "title": "Moreno, P. J."}, {"color": "#66CCFF", "id": "Image Sets", "label": "Image Sets", "shape": "dot", "size": 25, "title": "Image Sets"}, {"color": "#66CCFF", "id": "Cevikalp, H.", "label": "Cevikalp, H.", "shape": "dot", "size": 25, "title": "Cevikalp, H."}, {"color": "#66CCFF", "id": "Triggs, B.", "label": "Triggs, B.", "shape": "dot", "size": 25, "title": "Triggs, B."}, {"color": "#66CCFF", "id": "Image Sets Alignment", "label": "Image Sets Alignment", "shape": "dot", "size": 25, "title": "Image Sets Alignment"}, {"color": "#66CCFF", "id": "Video-based Face Recognition", "label": "Video-based Face Recognition", "shape": "dot", "size": 25, "title": "Video-based Face Recognition"}, {"color": "#66CCFF", "id": "Cui, Z.", "label": "Cui, Z.", "shape": "dot", "size": 25, "title": "Cui, Z."}, {"color": "#66CCFF", "id": "Grassmann Discriminant Analysis", "label": "Grassmann Discriminant Analysis", "shape": "dot", "size": 25, "title": "Grassmann Discriminant Analysis"}, {"color": "#66CCFF", "id": "Subspace-based Learning", "label": "Subspace-based Learning", "shape": "dot", "size": 25, "title": "Subspace-based Learning"}, {"color": "#66CCFF", "id": "Hamm, J.", "label": "Hamm, J.", "shape": "dot", "size": 25, "title": "Hamm, J."}, {"color": "#66CCFF", "id": "Lee, D. D.", "label": "Lee, D. D.", "shape": "dot", "size": 25, "title": "Lee, D. D."}, {"color": "#66CCFF", "id": "Sparse Approximated Nearest Points", "label": "Sparse Approximated Nearest Points", "shape": "dot", "size": 25, "title": "Sparse Approximated Nearest Points"}, {"color": "#66CCFF", "id": "Image Set Classification", "label": "Image Set Classification", "shape": "dot", "size": 25, "title": "Image Set Classification"}, {"color": "#66CCFF", "id": "Hu, Y.", "label": "Hu, Y.", "shape": "dot", "size": 25, "title": "Hu, Y."}, {"color": "#66CCFF", "id": "Sparse approximated nearest points", "label": "Sparse approximated nearest points", "shape": "dot", "size": 25, "title": "Sparse approximated nearest points"}, {"color": "#66CCFF", "id": "IEEE International Conference on Computer Vision and Pattern Recognized (CVPR)", "label": "IEEE International Conference on Computer Vision and Pattern Recognized (CVPR)", "shape": "dot", "size": 25, "title": "IEEE International Conference on Computer Vision and Pattern Recognized (CVPR)"}, {"color": "#66CCFF", "id": "Harandi, M. T.", "label": "Harandi, M. T.", "shape": "dot", "size": 25, "title": "Harandi, M. T."}, {"color": "#66CCFF", "id": "Grasmannian kernels", "label": "Grasmannian kernels", "shape": "dot", "size": 25, "title": "Grasmannian kernels"}, {"color": "#66CCFF", "id": "Jayasumana, S.", "label": "Jayasumana, S.", "shape": "dot", "size": 25, "title": "Jayasumana, S."}, {"color": "#66CCFF", "id": "IEEE Computer Society on Computer Vision and Pattern Recognition (CVPR)", "label": "IEEE Computer Society on Computer Vision and Pattern Recognition (CVPR)", "shape": "dot", "size": 25, "title": "IEEE Computer Society on Computer Vision and Pattern Recognition (CVPR)"}, {"color": "#66CCFF", "id": "Kim, M.", "label": "Kim, M.", "shape": "dot", "size": 25, "title": "Kim, M."}, {"color": "#66CCFF", "id": "Face tracking and recognition", "label": "Face tracking and recognition", "shape": "dot", "size": 25, "title": "Face tracking and recognition"}, {"color": "#66CCFF", "id": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)", "label": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)", "shape": "dot", "size": 25, "title": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)"}, {"color": "#66CCFF", "id": "Key Laboratory of Intelligent Information Processing", "label": "Key Laboratory of Intelligent Information Processing", "shape": "dot", "size": 25, "title": "Key Laboratory of Intelligent Information Processing"}, {"color": "#66CCFF", "id": "Chinese Academy of Sciences", "label": "Chinese Academy of Sciences", "shape": "dot", "size": 25, "title": "Chinese Academy of Sciences"}, {"color": "#66CCFF", "id": "Institute of Computing Technology", "label": "Institute of Computing Technology", "shape": "dot", "size": 25, "title": "Institute of Computing Technology"}, {"color": "#66CCFF", "id": "wen.wang@vipl.ict.ac.cn", "label": "wen.wang@vipl.ict.ac.cn", "shape": "dot", "size": 25, "title": "wen.wang@vipl.ict.ac.cn"}, {"color": "#66CCFF", "id": "wangruiping@ict.ac.cn", "label": "wangruiping@ict.ac.cn", "shape": "dot", "size": 25, "title": "wangruiping@ict.ac.cn"}, {"color": "#66CCFF", "id": "zhiwu.huang@vipl.ict.ac.cn", "label": "zhiwu.huang@vipl.ict.ac.cn", "shape": "dot", "size": 25, "title": "zhiwu.huang@vipl.ict.ac.cn"}, {"color": "#66CCFF", "id": "sgshan@ict.ac.cn", "label": "sgshan@ict.ac.cn", "shape": "dot", "size": 25, "title": "sgshan@ict.ac.cn"}, {"color": "#66CCFF", "id": "xlchen@ict.ac.cn", "label": "xlchen@ict.ac.cn", "shape": "dot", "size": 25, "title": "xlchen@ict.ac.cn"}, {"color": "#66CCFF", "id": "Super-resolution Person Re-identi\ufb01cation", "label": "Super-resolution Person Re-identi\ufb01cation", "shape": "dot", "size": 25, "title": "Super-resolution Person Re-identi\ufb01cation"}, {"color": "#66CCFF", "id": "Xiao-Yuan Jing", "label": "Xiao-Yuan Jing", "shape": "dot", "size": 25, "title": "Xiao-Yuan Jing"}, {"color": "#66CCFF", "id": "Xiaoke Zhu", "label": "Xiaoke Zhu", "shape": "dot", "size": 25, "title": "Xiaoke Zhu"}, {"color": "#66CCFF", "id": "Fei Wu", "label": "Fei Wu", "shape": "dot", "size": 25, "title": "Fei Wu"}, {"color": "#66CCFF", "id": "Xinge You", "label": "Xinge You", "shape": "dot", "size": 25, "title": "Xinge You"}, {"color": "#66CCFF", "id": "Qinglong Liu", "label": "Qinglong Liu", "shape": "dot", "size": 25, "title": "Qinglong Liu"}, {"color": "#66CCFF", "id": "Dong Yue", "label": "Dong Yue", "shape": "dot", "size": 25, "title": "Dong Yue"}, {"color": "#66CCFF", "id": "Ruimin Hu", "label": "Ruimin Hu", "shape": "dot", "size": 25, "title": "Ruimin Hu"}, {"color": "#66CCFF", "id": "Baowen Xu", "label": "Baowen Xu", "shape": "dot", "size": 25, "title": "Baowen Xu"}, {"color": "#66CCFF", "id": "Jing_Super-Resolution_Person_Re-Identification_2015_CVPR_paper.pdf", "label": "Jing_Super-Resolution_Person_Re-Identification_2015_CVPR_paper.pdf", "shape": "dot", "size": 25, "title": "Jing_Super-Resolution_Person_Re-Identification_2015_CVPR_paper.pdf"}, {"color": "#66CCFF", "id": "Person re-identification", "label": "Person re-identification", "shape": "dot", "size": 25, "title": "Person re-identification"}, {"color": "#66CCFF", "id": "surveillance applications", "label": "surveillance applications", "shape": "dot", "size": 25, "title": "surveillance applications"}, {"color": "#66CCFF", "id": "forensics applications", "label": "forensics applications", "shape": "dot", "size": 25, "title": "forensics applications"}, {"color": "#66CCFF", "id": "SLD2L", "label": "SLD2L", "shape": "dot", "size": 25, "title": "SLD2L"}, {"color": "#66CCFF", "id": "converting LR probe image features", "label": "converting LR probe image features", "shape": "dot", "size": 25, "title": "converting LR probe image features"}, {"color": "#66CCFF", "id": "effectiveness", "label": "effectiveness", "shape": "dot", "size": 25, "title": "effectiveness"}, {"color": "#66CCFF", "id": "discriminant term", "label": "discriminant term", "shape": "dot", "size": 25, "title": "discriminant term"}, {"color": "#66CCFF", "id": "converted features are far from different-person HR gallery features", "label": "converted features are far from different-person HR gallery features", "shape": "dot", "size": 25, "title": "converted features are far from different-person HR gallery features"}, {"color": "#66CCFF", "id": "low-rank regularization", "label": "low-rank regularization", "shape": "dot", "size": 25, "title": "low-rank regularization"}, {"color": "#66CCFF", "id": "intrinsic feature space", "label": "intrinsic feature space", "shape": "dot", "size": 25, "title": "intrinsic feature space"}, {"color": "#66CCFF", "id": "HR images", "label": "HR images", "shape": "dot", "size": 25, "title": "HR images"}, {"color": "#66CCFF", "id": "LR images", "label": "LR images", "shape": "dot", "size": 25, "title": "LR images"}, {"color": "#66CCFF", "id": "HR gallery images", "label": "HR gallery images", "shape": "dot", "size": 25, "title": "HR gallery images"}, {"color": "#66CCFF", "id": "features", "label": "features", "shape": "dot", "size": 25, "title": "features"}, {"color": "#66CCFF", "id": "LR probe images", "label": "LR probe images", "shape": "dot", "size": 25, "title": "LR probe images"}, {"color": "#66CCFF", "id": "public datasets", "label": "public datasets", "shape": "dot", "size": 25, "title": "public datasets"}, {"color": "#66CCFF", "id": "HR gallery", "label": "HR gallery", "shape": "dot", "size": 25, "title": "HR gallery"}, {"color": "#66CCFF", "id": "Super-resolution person re-identification", "label": "Super-resolution person re-identification", "shape": "dot", "size": 25, "title": "Super-resolution person re-identification"}, {"color": "#66CCFF", "id": "research", "label": "research", "shape": "dot", "size": 25, "title": "research"}, {"color": "#66CCFF", "id": "Low-rank discriminant dictionary learning", "label": "Low-rank discriminant dictionary learning", "shape": "dot", "size": 25, "title": "Low-rank discriminant dictionary learning"}, {"color": "#66CCFF", "id": "machine learning", "label": "machine learning", "shape": "dot", "size": 25, "title": "machine learning"}, {"color": "#66CCFF", "id": "Semi-coupled dictionaries", "label": "Semi-coupled dictionaries", "shape": "dot", "size": 25, "title": "Semi-coupled dictionaries"}, {"color": "#66CCFF", "id": "person re-identification", "label": "person re-identification", "shape": "dot", "size": 25, "title": "person re-identification"}, {"color": "#66CCFF", "id": "Feature representation learning", "label": "Feature representation learning", "shape": "dot", "size": 25, "title": "Feature representation learning"}, {"color": "#66CCFF", "id": "semi-coupled dictionaries", "label": "semi-coupled dictionaries", "shape": "dot", "size": 25, "title": "semi-coupled dictionaries"}, {"color": "#66CCFF", "id": "Bak et al. (2010)", "label": "Bak et al. (2010)", "shape": "dot", "size": 25, "title": "Bak et al. (2010)"}, {"color": "#66CCFF", "id": "Bedagkar-Gala \u0026 Shah (2014)", "label": "Bedagkar-Gala \u0026 Shah (2014)", "shape": "dot", "size": 25, "title": "Bedagkar-Gala \u0026 Shah (2014)"}, {"color": "#66CCFF", "id": "person re-identi\ufb01cation approaches", "label": "person re-identi\ufb01cation approaches", "shape": "dot", "size": 25, "title": "person re-identi\ufb01cation approaches"}, {"color": "#66CCFF", "id": "Liu et al. (2014)", "label": "Liu et al. (2014)", "shape": "dot", "size": 25, "title": "Liu et al. (2014)"}, {"color": "#66CCFF", "id": "semi-supervised coupled dictionary learning", "label": "semi-supervised coupled dictionary learning", "shape": "dot", "size": 25, "title": "semi-supervised coupled dictionary learning"}, {"color": "#66CCFF", "id": "Liu, X.", "label": "Liu, X.", "shape": "dot", "size": 25, "title": "Liu, X."}, {"color": "#66CCFF", "id": "Semi-supervised coupled dictionary learning for person re-identi\ufb01cation", "label": "Semi-supervised coupled dictionary learning for person re-identi\ufb01cation", "shape": "dot", "size": 25, "title": "Semi-supervised coupled dictionary learning for person re-identi\ufb01cation"}, {"color": "#66CCFF", "id": "CVPR, IEEE Conference on", "label": "CVPR, IEEE Conference on", "shape": "dot", "size": 25, "title": "CVPR, IEEE Conference on"}, {"color": "#66CCFF", "id": "Ma, L.", "label": "Ma, L.", "shape": "dot", "size": 25, "title": "Ma, L."}, {"color": "#66CCFF", "id": "Sparse representation for face recognition based on discriminative low-rank dictionary learning", "label": "Sparse representation for face recognition based on discriminative low-rank dictionary learning", "shape": "dot", "size": 25, "title": "Sparse representation for face recognition based on discriminative low-rank dictionary learning"}, {"color": "#66CCFF", "id": "Gray, D.", "label": "Gray, D.", "shape": "dot", "size": 25, "title": "Gray, D."}, {"color": "#66CCFF", "id": "Evaluating appearance models for recognition, reacquisition, and tracking", "label": "Evaluating appearance models for recognition, reacquisition, and tracking", "shape": "dot", "size": 25, "title": "Evaluating appearance models for recognition, reacquisition, and tracking"}, {"color": "#66CCFF", "id": "Performance Evaluation of Tracking and Surveillance, IEEE workshop on", "label": "Performance Evaluation of Tracking and Surveillance, IEEE workshop on", "shape": "dot", "size": 25, "title": "Performance Evaluation of Tracking and Surveillance, IEEE workshop on"}, {"color": "#66CCFF", "id": "Viewpoint invariant pedestrian recognition with an ensemble of localized features", "label": "Viewpoint invariant pedestrian recognition with an ensemble of localized features", "shape": "dot", "size": 25, "title": "Viewpoint invariant pedestrian recognition with an ensemble of localized features"}, {"color": "#66CCFF", "id": "Person re-identi\ufb01cation over camera networks using multi-task distance metric learning", "label": "Person re-identi\ufb01cation over camera networks using multi-task distance metric learning", "shape": "dot", "size": 25, "title": "Person re-identi\ufb01cation over camera networks using multi-task distance metric learning"}, {"color": "#66CCFF", "id": "Image Processing, IEEE Transactions on", "label": "Image Processing, IEEE Transactions on", "shape": "dot", "size": 25, "title": "Image Processing, IEEE Transactions on"}, {"color": "#66CCFF", "id": "Person re-identi\ufb01cation over camera networks", "label": "Person re-identi\ufb01cation over camera networks", "shape": "dot", "size": 25, "title": "Person re-identi\ufb01cation over camera networks"}, {"color": "#66CCFF", "id": "Hirzer, M.", "label": "Hirzer, M.", "shape": "dot", "size": 25, "title": "Hirzer, M."}, {"color": "#66CCFF", "id": "Person re-identi\ufb01cation by descriptive and discriminative classification", "label": "Person re-identi\ufb01cation by descriptive and discriminative classification", "shape": "dot", "size": 25, "title": "Person re-identi\ufb01cation by descriptive and discriminative classification"}, {"color": "#66CCFF", "id": "Zheng, W.-S.", "label": "Zheng, W.-S.", "shape": "dot", "size": 25, "title": "Zheng, W.-S."}, {"color": "#66CCFF", "id": "Reidenti\ufb01cation by relative distance comparison", "label": "Reidenti\ufb01cation by relative distance comparison", "shape": "dot", "size": 25, "title": "Reidenti\ufb01cation by relative distance comparison"}, {"color": "#66CCFF", "id": "State Key Laboratory of Software Engineering", "label": "State Key Laboratory of Software Engineering", "shape": "dot", "size": 25, "title": "State Key Laboratory of Software Engineering"}, {"color": "#66CCFF", "id": "Image super-resolution via sparse representation", "label": "Image super-resolution via sparse representation", "shape": "dot", "size": 25, "title": "Image super-resolution via sparse representation"}, {"color": "#66CCFF", "id": "Image Analysis", "label": "Image Analysis", "shape": "dot", "size": 25, "title": "Image Analysis"}, {"color": "#66CCFF", "id": "Pattern Analysis and Machine Intelligence, IEEE Transactions on", "label": "Pattern Analysis and Machine Intelligence, IEEE Transactions on", "shape": "dot", "size": 25, "title": "Pattern Analysis and Machine Intelligence, IEEE Transactions on"}, {"color": "#66CCFF", "id": "re Engineering", "label": "re Engineering", "shape": "dot", "size": 25, "title": "re Engineering"}, {"color": "#66CCFF", "id": "School of Computer", "label": "School of Computer", "shape": "dot", "size": 25, "title": "School of Computer"}, {"color": "#66CCFF", "id": "Wuhan University", "label": "Wuhan University", "shape": "dot", "size": 25, "title": "Wuhan University"}, {"color": "#66CCFF", "id": "China", "label": "China", "shape": "dot", "size": 25, "title": "China"}, {"color": "#66CCFF", "id": "Huazhong University of Science and Technology", "label": "Huazhong University of Science and Technology", "shape": "dot", "size": 25, "title": "Huazhong University of Science and Technology"}, {"color": "#66CCFF", "id": "School of Electronic Information and Communications", "label": "School of Electronic Information and Communications", "shape": "dot", "size": 25, "title": "School of Electronic Information and Communications"}, {"color": "#66CCFF", "id": "Nanjing University of Posts and Telecommunications", "label": "Nanjing University of Posts and Telecommunications", "shape": "dot", "size": 25, "title": "Nanjing University of Posts and Telecommunications"}, {"color": "#66CCFF", "id": "National Engineering Research Center for Multimedia Software", "label": "National Engineering Research Center for Multimedia Software", "shape": "dot", "size": 25, "title": "National Engineering Research Center for Multimedia Software"}, {"color": "#66CCFF", "id": "Ronan Collobert", "label": "Ronan Collobert", "shape": "dot", "size": 25, "title": "Ronan Collobert"}, {"color": "#66CCFF", "id": "From Image-level to Pixel-level Labeling", "label": "From Image-level to Pixel-level Labeling", "shape": "dot", "size": 25, "title": "From Image-level to Pixel-level Labeling"}, {"color": "#66CCFF", "id": "Multimedia Software", "label": "Multimedia Software", "shape": "dot", "size": 25, "title": "Multimedia Software"}, {"color": "#66CCFF", "id": "We", "label": "We", "shape": "dot", "size": 25, "title": "We"}, {"color": "#66CCFF", "id": "object segmentation", "label": "object segmentation", "shape": "dot", "size": 25, "title": "object segmentation"}, {"color": "#66CCFF", "id": "object class information", "label": "object class information", "shape": "dot", "size": 25, "title": "object class information"}, {"color": "#66CCFF", "id": "weakly supervised segmentation task", "label": "weakly supervised segmentation task", "shape": "dot", "size": 25, "title": "weakly supervised segmentation task"}, {"color": "#66CCFF", "id": "Multiple Instance Learning (MIL) framework", "label": "Multiple Instance Learning (MIL) framework", "shape": "dot", "size": 25, "title": "Multiple Instance Learning (MIL) framework"}, {"color": "#66CCFF", "id": "training image", "label": "training image", "shape": "dot", "size": 25, "title": "training image"}, {"color": "#66CCFF", "id": "pixel corresponding to image class label", "label": "pixel corresponding to image class label", "shape": "dot", "size": 25, "title": "pixel corresponding to image class label"}, {"color": "#66CCFF", "id": "segmentation task", "label": "segmentation task", "shape": "dot", "size": 25, "title": "segmentation task"}, {"color": "#66CCFF", "id": "inferring pixels belonging to class of object", "label": "inferring pixels belonging to class of object", "shape": "dot", "size": 25, "title": "inferring pixels belonging to class of object"}, {"color": "#66CCFF", "id": "model", "label": "model", "shape": "dot", "size": 25, "title": "model"}, {"color": "#66CCFF", "id": "Convolutional Neural Network", "label": "Convolutional Neural Network", "shape": "dot", "size": 25, "title": "Convolutional Neural Network"}, {"color": "#66CCFF", "id": "training", "label": "training", "shape": "dot", "size": 25, "title": "training"}, {"color": "#66CCFF", "id": "pixels important for image classification", "label": "pixels important for image classification", "shape": "dot", "size": 25, "title": "pixels important for image classification"}, {"color": "#66CCFF", "id": "pixels", "label": "pixels", "shape": "dot", "size": 25, "title": "pixels"}, {"color": "#66CCFF", "id": "network-based model", "label": "network-based model", "shape": "dot", "size": 25, "title": "network-based model"}, {"color": "#66CCFF", "id": "important pixels", "label": "important pixels", "shape": "dot", "size": 25, "title": "important pixels"}, {"color": "#66CCFF", "id": "right pixels", "label": "right pixels", "shape": "dot", "size": 25, "title": "right pixels"}, {"color": "#66CCFF", "id": "system", "label": "system", "shape": "dot", "size": 25, "title": "system"}, {"color": "#66CCFF", "id": "Imaginet dataset", "label": "Imaginet dataset", "shape": "dot", "size": 25, "title": "Imaginet dataset"}, {"color": "#66CCFF", "id": "segmentation experiments", "label": "segmentation experiments", "shape": "dot", "size": 25, "title": "segmentation experiments"}, {"color": "#66CCFF", "id": "Pascal VOC dataset", "label": "Pascal VOC dataset", "shape": "dot", "size": 25, "title": "Pascal VOC dataset"}, {"color": "#66CCFF", "id": "state of the art results", "label": "state of the art results", "shape": "dot", "size": 25, "title": "state of the art results"}, {"color": "#66CCFF", "id": "weakly supervised object segmentation task", "label": "weakly supervised object segmentation task", "shape": "dot", "size": 25, "title": "weakly supervised object segmentation task"}, {"color": "#66CCFF", "id": "fully-supervised segmentation approaches", "label": "fully-supervised segmentation approaches", "shape": "dot", "size": 25, "title": "fully-supervised segmentation approaches"}, {"color": "#66CCFF", "id": "Model", "label": "Model", "shape": "dot", "size": 25, "title": "Model"}, {"color": "#66CCFF", "id": "Object Segmentation", "label": "Object Segmentation", "shape": "dot", "size": 25, "title": "Object Segmentation"}, {"color": "#66CCFF", "id": "task", "label": "task", "shape": "dot", "size": 25, "title": "task"}, {"color": "#66CCFF", "id": "Weakly Supervised Segmentation", "label": "Weakly Supervised Segmentation", "shape": "dot", "size": 25, "title": "Weakly Supervised Segmentation"}, {"color": "#66CCFF", "id": "approach", "label": "approach", "shape": "dot", "size": 25, "title": "approach"}, {"color": "#66CCFF", "id": "Convolutional Neural Networks", "label": "Convolutional Neural Networks", "shape": "dot", "size": 25, "title": "Convolutional Neural Networks"}, {"color": "#66CCFF", "id": "network type", "label": "network type", "shape": "dot", "size": 25, "title": "network type"}, {"color": "#66CCFF", "id": "Multiple Instance Learning", "label": "Multiple Instance Learning", "shape": "dot", "size": 25, "title": "Multiple Instance Learning"}, {"color": "#66CCFF", "id": "Image-level Training", "label": "Image-level Training", "shape": "dot", "size": 25, "title": "Image-level Training"}, {"color": "#66CCFF", "id": "training method", "label": "training method", "shape": "dot", "size": 25, "title": "training method"}, {"color": "#66CCFF", "id": "Arbel\u00e1ez et al. (2009)", "label": "Arbel\u00e1ez et al. (2009)", "shape": "dot", "size": 25, "title": "Arbel\u00e1ez et al. (2009)"}, {"color": "#66CCFF", "id": "Multiscale combinatorial grouping", "label": "Multiscale combinatorial grouping", "shape": "dot", "size": 25, "title": "Multiscale combinatorial grouping"}, {"color": "#66CCFF", "id": "Boyd \u0026 Vandenberghe (2004)", "label": "Boyd \u0026 Vandenberghe (2004)", "shape": "dot", "size": 25, "title": "Boyd \u0026 Vandenberghe (2004)"}, {"color": "#66CCFF", "id": "Convex optimization", "label": "Convex optimization", "shape": "dot", "size": 25, "title": "Convex optimization"}, {"color": "#66CCFF", "id": "Bridle (1990)", "label": "Bridle (1990)", "shape": "dot", "size": 25, "title": "Bridle (1990)"}, {"color": "#66CCFF", "id": "Probabilistic interpretation of feedforward classification network outputs", "label": "Probabilistic interpretation of feedforward classification network outputs", "shape": "dot", "size": 25, "title": "Probabilistic interpretation of feedforward classification network outputs"}, {"color": "#66CCFF", "id": "Probabilistic interpretation", "label": "Probabilistic interpretation", "shape": "dot", "size": 25, "title": "Probabilistic interpretation"}, {"color": "#66CCFF", "id": "Statistical pattern recognition", "label": "Statistical pattern recognition", "shape": "dot", "size": 25, "title": "Statistical pattern recognition"}, {"color": "#66CCFF", "id": "Feedforward classification network outputs", "label": "Feedforward classification network outputs", "shape": "dot", "size": 25, "title": "Feedforward classification network outputs"}, {"color": "#66CCFF", "id": "Efficient graph-based image segmentation", "label": "Efficient graph-based image segmentation", "shape": "dot", "size": 25, "title": "Efficient graph-based image segmentation"}, {"color": "#66CCFF", "id": "International Journal of Computer Vision (IJCV)", "label": "International Journal of Computer Vision (IJCV)", "shape": "dot", "size": 25, "title": "International Journal of Computer Vision (IJCV)"}, {"color": "#66CCFF", "id": "Semantic segmentation", "label": "Semantic segmentation", "shape": "dot", "size": 25, "title": "Semantic segmentation"}, {"color": "#66CCFF", "id": "Simultaneous detection and segmentation", "label": "Simultaneous detection and segmentation", "shape": "dot", "size": 25, "title": "Simultaneous detection and segmentation"}, {"color": "#66CCFF", "id": "European Conference on Computer Vision (ECCV)", "label": "European Conference on Computer Vision (ECCV)", "shape": "dot", "size": 25, "title": "European Conference on Computer Vision (ECCV)"}, {"color": "#66CCFF", "id": "Graph-based image segmentation", "label": "Graph-based image segmentation", "shape": "dot", "size": 25, "title": "Graph-based image segmentation"}, {"color": "#66CCFF", "id": "Image segmentation", "label": "Image segmentation", "shape": "dot", "size": 25, "title": "Image segmentation"}, {"color": "#66CCFF", "id": "Hariharan et al.", "label": "Hariharan et al.", "shape": "dot", "size": 25, "title": "Hariharan et al."}, {"color": "#66CCFF", "id": "Krizhevsky et al.", "label": "Krizhevsky et al.", "shape": "dot", "size": 25, "title": "Krizhevsky et al."}, {"color": "#66CCFF", "id": "NIPS", "label": "NIPS", "shape": "dot", "size": 25, "title": "NIPS"}, {"color": "#66CCFF", "id": "LeCun et al.", "label": "LeCun et al.", "shape": "dot", "size": 25, "title": "LeCun et al."}, {"color": "#66CCFF", "id": "Proceedings of the IEEE", "label": "Proceedings of the IEEE", "shape": "dot", "size": 25, "title": "Proceedings of the IEEE"}, {"color": "#66CCFF", "id": "Maron \u0026 Lozano-P\u00e9rez", "label": "Maron \u0026 Lozano-P\u00e9rez", "shape": "dot", "size": 25, "title": "Maron \u0026 Lozano-P\u00e9rez"}, {"color": "#66CCFF", "id": "Pedro O. Pinheiro", "label": "Pedro O. Pinheiro", "shape": "dot", "size": 25, "title": "Pedro O. Pinheiro"}, {"color": "#66CCFF", "id": "Idiap Research Institute", "label": "Idiap Research Institute", "shape": "dot", "size": 25, "title": "Idiap Research Institute"}, {"color": "#66CCFF", "id": "Facebook AI Research", "label": "Facebook AI Research", "shape": "dot", "size": 25, "title": "Facebook AI Research"}, {"color": "#66CCFF", "id": "ronan@coltobert.com", "label": "ronan@coltobert.com", "shape": "dot", "size": 25, "title": "ronan@coltobert.com"}, {"color": "#66CCFF", "id": "Yeqing Li", "label": "Yeqing Li", "shape": "dot", "size": 25, "title": "Yeqing Li"}, {"color": "#66CCFF", "id": "Deep Sparse Representation", "label": "Deep Sparse Representation", "shape": "dot", "size": 25, "title": "Deep Sparse Representation"}, {"color": "#66CCFF", "id": "Chen Chen", "label": "Chen Chen", "shape": "dot", "size": 25, "title": "Chen Chen"}, {"color": "#66CCFF", "id": "Fei Yang", "label": "Fei Yang", "shape": "dot", "size": 25, "title": "Fei Yang"}, {"color": "#66CCFF", "id": "Junzhou Huang", "label": "Junzhou Huang", "shape": "dot", "size": 25, "title": "Junzhou Huang"}, {"color": "#66CCFF", "id": "Menlo Park", "label": "Menlo Park", "shape": "dot", "size": 25, "title": "Menlo Park"}, {"color": "#66CCFF", "id": "USA", "label": "USA", "shape": "dot", "size": 25, "title": "USA"}, {"color": "#66CCFF", "id": "Deep Sparse representation", "label": "Deep Sparse representation", "shape": "dot", "size": 25, "title": "Deep Sparse representation"}, {"color": "#66CCFF", "id": "image registration technique", "label": "image registration technique", "shape": "dot", "size": 25, "title": "image registration technique"}, {"color": "#66CCFF", "id": "Li_Deep_Sparse_Representation_2015_CVPR_paper.pdf", "label": "Li_Deep_Sparse_Representation_2015_CVPR_paper.pdf", "shape": "dot", "size": 25, "title": "Li_Deep_Sparse_Representation_2015_CVPR_paper.pdf"}, {"color": "#66CCFF", "id": "paper", "label": "paper", "shape": "dot", "size": 25, "title": "paper"}, {"color": "#66CCFF", "id": "similarity measure", "label": "similarity measure", "shape": "dot", "size": 25, "title": "similarity measure"}, {"color": "#66CCFF", "id": "deep sparse representation", "label": "deep sparse representation", "shape": "dot", "size": 25, "title": "deep sparse representation"}, {"color": "#66CCFF", "id": "subpixel-level accuracy", "label": "subpixel-level accuracy", "shape": "dot", "size": 25, "title": "subpixel-level accuracy"}, {"color": "#66CCFF", "id": "robustness", "label": "robustness", "shape": "dot", "size": 25, "title": "robustness"}, {"color": "#66CCFF", "id": "images", "label": "images", "shape": "dot", "size": 25, "title": "images"}, {"color": "#66CCFF", "id": "gradient domain", "label": "gradient domain", "shape": "dot", "size": 25, "title": "gradient domain"}, {"color": "#66CCFF", "id": "frequency domain", "label": "frequency domain", "shape": "dot", "size": 25, "title": "frequency domain"}, {"color": "#66CCFF", "id": "sparse error tensors", "label": "sparse error tensors", "shape": "dot", "size": 25, "title": "sparse error tensors"}, {"color": "#66CCFF", "id": "limitations", "label": "limitations", "shape": "dot", "size": 25, "title": "limitations"}, {"color": "#66CCFF", "id": "spatially-varying intensity distortions", "label": "spatially-varying intensity distortions", "shape": "dot", "size": 25, "title": "spatially-varying intensity distortions"}, {"color": "#66CCFF", "id": "traditional methods", "label": "traditional methods", "shape": "dot", "size": 25, "title": "traditional methods"}, {"color": "#66CCFF", "id": "state-of-the-art methods", "label": "state-of-the-art methods", "shape": "dot", "size": 25, "title": "state-of-the-art methods"}, {"color": "#66CCFF", "id": "RASL", "label": "RASL", "shape": "dot", "size": 25, "title": "RASL"}, {"color": "#66CCFF", "id": "accuracy", "label": "accuracy", "shape": "dot", "size": 25, "title": "accuracy"}, {"color": "#66CCFF", "id": "efficiency", "label": "efficiency", "shape": "dot", "size": 25, "title": "efficiency"}, {"color": "#66CCFF", "id": "Deformable medical image registration", "label": "Deformable medical image registration", "shape": "dot", "size": 25, "title": "Deformable medical image registration"}, {"color": "#66CCFF", "id": "IEEE Transactions on Medical Imaging", "label": "IEEE Transactions on Medical Imaging", "shape": "dot", "size": 25, "title": "IEEE Transactions on Medical Imaging"}, {"color": "#66CCFF", "id": "sparse decomposition", "label": "sparse decomposition", "shape": "dot", "size": 25, "title": "sparse decomposition"}, {"color": "#66CCFF", "id": "low-rank decomposition", "label": "low-rank decomposition", "shape": "dot", "size": 25, "title": "low-rank decomposition"}, {"color": "#66CCFF", "id": "Robust principal component analysis", "label": "Robust principal component analysis", "shape": "dot", "size": 25, "title": "Robust principal component analysis"}, {"color": "#66CCFF", "id": "Journal of the ACM", "label": "Journal of the ACM", "shape": "dot", "size": 25, "title": "Journal of the ACM"}, {"color": "#66CCFF", "id": "Deep sparse representation", "label": "Deep sparse representation", "shape": "dot", "size": 25, "title": "Deep sparse representation"}, {"color": "#66CCFF", "id": "Deep Learning", "label": "Deep Learning", "shape": "dot", "size": 25, "title": "Deep Learning"}, {"color": "#66CCFF", "id": "Intensity Distortions", "label": "Intensity Distortions", "shape": "dot", "size": 25, "title": "Intensity Distortions"}, {"color": "#66CCFF", "id": "existing approaches", "label": "existing approaches", "shape": "dot", "size": 25, "title": "existing approaches"}, {"color": "#66CCFF", "id": "journal", "label": "journal", "shape": "dot", "size": 25, "title": "journal"}, {"color": "#66CCFF", "id": "32", "label": "32", "shape": "dot", "size": 25, "title": "32"}, {"color": "#66CCFF", "id": "7", "label": "7", "shape": "dot", "size": 25, "title": "7"}, {"color": "#66CCFF", "id": "58", "label": "58", "shape": "dot", "size": 25, "title": "58"}, {"color": "#66CCFF", "id": "3", "label": "3", "shape": "dot", "size": 25, "title": "3"}, {"color": "#66CCFF", "id": "Pattern Recognition", "label": "Pattern Recognition", "shape": "dot", "size": 25, "title": "Pattern Recognition"}, {"color": "#66CCFF", "id": "35", "label": "35", "shape": "dot", "size": 25, "title": "35"}, {"color": "#66CCFF", "id": "2", "label": "2", "shape": "dot", "size": 25, "title": "2"}, {"color": "#66CCFF", "id": "IEEE Transactions on Geoscience and Remote Sensing", "label": "IEEE Transactions on Geoscience and Remote Sensing", "shape": "dot", "size": 25, "title": "IEEE Transactions on Geoscience and Remote Sensing"}, {"color": "#66CCFF", "id": "46", "label": "46", "shape": "dot", "size": 25, "title": "46"}, {"color": "#66CCFF", "id": "IEEE Transactions on Geoscientific and Remote Sensing", "label": "IEEE Transactions on Geoscientific and Remote Sensing", "shape": "dot", "size": 25, "title": "IEEE Transactions on Geoscientific and Remote Sensing"}, {"color": "#66CCFF", "id": "5", "label": "5", "shape": "dot", "size": 25, "title": "5"}, {"color": "#66CCFF", "id": "Medical image analysis", "label": "Medical image analysis", "shape": "dot", "size": 25, "title": "Medical image analysis"}, {"color": "#66CCFF", "id": "18", "label": "18", "shape": "dot", "size": 25, "title": "18"}, {"color": "#66CCFF", "id": "6", "label": "6", "shape": "dot", "size": 25, "title": "6"}, {"color": "#66CCFF", "id": "H", "label": "H", "shape": "dot", "size": 25, "title": "H"}, {"color": "#66CCFF", "id": "Landmark matching based retinal image alignment", "label": "Landmark matching based retinal image alignment", "shape": "dot", "size": 25, "title": "Landmark matching based retinal image alignment"}, {"color": "#66CCFF", "id": "Maguire", "label": "Maguire", "shape": "dot", "size": 25, "title": "Maguire"}, {"color": "#66CCFF", "id": "Brainard", "label": "Brainard", "shape": "dot", "size": 25, "title": "Brainard"}, {"color": "#66CCFF", "id": "Tzimiropouulos", "label": "Tzimiropouulos", "shape": "dot", "size": 25, "title": "Tzimiropouulos"}, {"color": "#66CCFF", "id": "Robust FFT-based scale-invariant image registration", "label": "Robust FFT-based scale-invariant image registration", "shape": "dot", "size": 25, "title": "Robust FFT-based scale-invariant image registration"}, {"color": "#66CCFF", "id": "Argyriou", "label": "Argyriou", "shape": "dot", "size": 25, "title": "Argyriou"}, {"color": "#66CCFF", "id": "Zafeiriou", "label": "Zafeiriou", "shape": "dot", "size": 25, "title": "Zafeiriou"}, {"color": "#66CCFF", "id": "Stathaki", "label": "Stathaki", "shape": "dot", "size": 25, "title": "Stathaki"}, {"color": "#66CCFF", "id": "Viola", "label": "Viola", "shape": "dot", "size": 25, "title": "Viola"}, {"color": "#66CCFF", "id": "Alignment by maximization of mutual information", "label": "Alignment by maximization of mutual information", "shape": "dot", "size": 25, "title": "Alignment by maximization of mutual information"}, {"color": "#66CCFF", "id": "Wells III", "label": "Wells III", "shape": "dot", "size": 25, "title": "Wells III"}, {"color": "#66CCFF", "id": "Gross", "label": "Gross", "shape": "dot", "size": 25, "title": "Gross"}, {"color": "#66CCFF", "id": "Multi-pie", "label": "Multi-pie", "shape": "dot", "size": 25, "title": "Multi-pie"}, {"color": "#66CCFF", "id": "Matthews", "label": "Matthews", "shape": "dot", "size": 25, "title": "Matthews"}, {"color": "#66CCFF", "id": "Cohn", "label": "Cohn", "shape": "dot", "size": 25, "title": "Cohn"}, {"color": "#66CCFF", "id": "Kanade", "label": "Kanade", "shape": "dot", "size": 25, "title": "Kanade"}, {"color": "#66CCFF", "id": "Baker", "label": "Baker", "shape": "dot", "size": 25, "title": "Baker"}, {"color": "#66CCFF", "id": "Zitova", "label": "Zitova", "shape": "dot", "size": 25, "title": "Zitova"}, {"color": "#66CCFF", "id": "Image registration methods", "label": "Image registration methods", "shape": "dot", "size": 25, "title": "Image registration methods"}, {"color": "#66CCFF", "id": "Flusser", "label": "Flusser", "shape": "dot", "size": 25, "title": "Flusser"}, {"color": "#66CCFF", "id": "University of Texas at Arlington", "label": "University of Texas at Arlington", "shape": "dot", "size": 25, "title": "University of Texas at Arlington"}, {"color": "#66CCFF", "id": "Tsung-Yi Lin", "label": "Tsung-Yi Lin", "shape": "dot", "size": 25, "title": "Tsung-Yi Lin"}, {"color": "#66CCFF", "id": "Lin_Learning_Deep_Representations_2015_CVPR_paper", "label": "Lin_Learning_Deep_Representations_2015_CVPR_paper", "shape": "dot", "size": 25, "title": "Lin_Learning_Deep_Representations_2015_CVPR_paper"}, {"color": "#66CCFF", "id": "Yin Cui", "label": "Yin Cui", "shape": "dot", "size": 25, "title": "Yin Cui"}, {"color": "#66CCFF", "id": "Serge Belongie", "label": "Serge Belongie", "shape": "dot", "size": 25, "title": "Serge Belongie"}, {"color": "#66CCFF", "id": "James Hays", "label": "James Hays", "shape": "dot", "size": 25, "title": "James Hays"}, {"color": "#66CCFF", "id": "Image and vision computing", "label": "Image and vision computing", "shape": "dot", "size": 25, "title": "Image and vision computing"}, {"color": "#66CCFF", "id": "21", "label": "21", "shape": "dot", "size": 25, "title": "21"}, {"color": "#66CCFF", "id": "977\u20131000", "label": "977\u20131000", "shape": "dot", "size": 25, "title": "977\u20131000"}, {"color": "#66CCFF", "id": "geo-tagged images", "label": "geo-tagged images", "shape": "dot", "size": 25, "title": "geo-tagged images"}, {"color": "#66CCFF", "id": "image-based geolocalization algorithms", "label": "image-based geolocalization algorithms", "shape": "dot", "size": 25, "title": "image-based geolocalization algorithms"}, {"color": "#66CCFF", "id": "ground-level images", "label": "ground-level images", "shape": "dot", "size": 25, "title": "ground-level images"}, {"color": "#66CCFF", "id": "limitation", "label": "limitation", "shape": "dot", "size": 25, "title": "limitation"}, {"color": "#66CCFF", "id": "ground-level query images", "label": "ground-level query images", "shape": "dot", "size": 25, "title": "ground-level query images"}, {"color": "#66CCFF", "id": "aerial imagery", "label": "aerial imagery", "shape": "dot", "size": 25, "title": "aerial imagery"}, {"color": "#66CCFF", "id": "Where-CNN", "label": "Where-CNN", "shape": "dot", "size": 25, "title": "Where-CNN"}, {"color": "#66CCFF", "id": "deep learning approach", "label": "deep learning approach", "shape": "dot", "size": 25, "title": "deep learning approach"}, {"color": "#66CCFF", "id": "face verification", "label": "face verification", "shape": "dot", "size": 25, "title": "face verification"}, {"color": "#66CCFF", "id": "dataset", "label": "dataset", "shape": "dot", "size": 25, "title": "dataset"}, {"color": "#66CCFF", "id": "78K aligned cross-view image pairs", "label": "78K aligned cross-view image pairs", "shape": "dot", "size": 25, "title": "78K aligned cross-view image pairs"}, {"color": "#66CCFF", "id": "feature representation", "label": "feature representation", "shape": "dot", "size": 25, "title": "feature representation"}, {"color": "#66CCFF", "id": "matching views", "label": "matching views", "shape": "dot", "size": 25, "title": "matching views"}, {"color": "#66CCFF", "id": "close", "label": "close", "shape": "dot", "size": 25, "title": "close"}, {"color": "#66CCFF", "id": "mismatched views", "label": "mismatched views", "shape": "dot", "size": 25, "title": "mismatched views"}, {"color": "#66CCFF", "id": "far apart", "label": "far apart", "shape": "dot", "size": 25, "title": "far apart"}, {"color": "#66CCFF", "id": "Geolocalization", "label": "Geolocalization", "shape": "dot", "size": 25, "title": "Geolocalization"}, {"color": "#66CCFF", "id": "Aerial Imagery", "label": "Aerial Imagery", "shape": "dot", "size": 25, "title": "Aerial Imagery"}, {"color": "#66CCFF", "id": "Cross-View Matching", "label": "Cross-View Matching", "shape": "dot", "size": 25, "title": "Cross-View Matching"}, {"color": "#66CCFF", "id": "Feature Representation", "label": "Feature Representation", "shape": "dot", "size": 25, "title": "Feature Representation"}, {"color": "#66CCFF", "id": "Deep Convolutional Neural Networks", "label": "Deep Convolutional Neural Networks", "shape": "dot", "size": 25, "title": "Deep Convolutional Neural Networks"}, {"color": "#66CCFF", "id": "Image Classification", "label": "Image Classification", "shape": "dot", "size": 25, "title": "Image Classification"}, {"color": "#66CCFF", "id": "Google Street View", "label": "Google Street View", "shape": "dot", "size": 25, "title": "Google Street View"}, {"color": "#66CCFF", "id": "World", "label": "World", "shape": "dot", "size": 25, "title": "World"}, {"color": "#66CCFF", "id": "Distinctive Image Features", "label": "Distinctive Image Features", "shape": "dot", "size": 25, "title": "Distinctive Image Features"}, {"color": "#66CCFF", "id": "Scale-Invariant Keypoints", "label": "Scale-Invariant Keypoints", "shape": "dot", "size": 25, "title": "Scale-Invariant Keypoints"}, {"color": "#66CCFF", "id": "Deepface", "label": "Deepface", "shape": "dot", "size": 25, "title": "Deepface"}, {"color": "#66CCFF", "id": "Human-Level Performance", "label": "Human-Level Performance", "shape": "dot", "size": 25, "title": "Human-Level Performance"}, {"color": "#66CCFF", "id": "Learning a Similarity Metric", "label": "Learning a Similarity Metric", "shape": "dot", "size": 25, "title": "Learning a Similarity Metric"}, {"color": "#66CCFF", "id": "Face Verification", "label": "Face Verification", "shape": "dot", "size": 25, "title": "Face Verification"}, {"color": "#66CCFF", "id": "Novel Locations", "label": "Novel Locations", "shape": "dot", "size": 25, "title": "Novel Locations"}, {"color": "#66CCFF", "id": "Traditional Deep Features", "label": "Traditional Deep Features", "shape": "dot", "size": 25, "title": "Traditional Deep Features"}, {"color": "#66CCFF", "id": "Chopra et al. (2005)", "label": "Chopra et al. (2005)", "shape": "dot", "size": 25, "title": "Chopra et al. (2005)"}, {"color": "#66CCFF", "id": "Lin et al. (2013)", "label": "Lin et al. (2013)", "shape": "dot", "size": 25, "title": "Lin et al. (2013)"}, {"color": "#66CCFF", "id": "Bansal \u0026 Daniilidis (2014)", "label": "Bansal \u0026 Daniilidis (2014)", "shape": "dot", "size": 25, "title": "Bansal \u0026 Daniilidis (2014)"}, {"color": "#66CCFF", "id": "van der Maaten \u0026 Hinton (2008)", "label": "van der Maaten \u0026 Hinton (2008)", "shape": "dot", "size": 25, "title": "van der Maaten \u0026 Hinton (2008)"}, {"color": "#66CCFF", "id": "JMLR", "label": "JMLR", "shape": "dot", "size": 25, "title": "JMLR"}, {"color": "#66CCFF", "id": "Xiao et al. (2010)", "label": "Xiao et al. (2010)", "shape": "dot", "size": 25, "title": "Xiao et al. (2010)"}, {"color": "#66CCFF", "id": "Felzenszwalb et al. (2010)", "label": "Felzenszwalb et al. (2010)", "shape": "dot", "size": 25, "title": "Felzenszwalb et al. (2010)"}, {"color": "#66CCFF", "id": "PAMI", "label": "PAMI", "shape": "dot", "size": 25, "title": "PAMI"}, {"color": "#66CCFF", "id": "Cornell Tech", "label": "Cornell Tech", "shape": "dot", "size": 25, "title": "Cornell Tech"}, {"color": "#66CCFF", "id": "Brown University", "label": "Brown University", "shape": "dot", "size": 25, "title": "Brown University"}, {"color": "#66CCFF", "id": "Junho Yim", "label": "Junho Yim", "shape": "dot", "size": 25, "title": "Junho Yim"}, {"color": "#66CCFF", "id": "Rotating Your Face Using Multi-task Deep Neural Network", "label": "Rotating Your Face Using Multi-task Deep Neural Network", "shape": "dot", "size": 25, "title": "Rotating Your Face Using Multi-task Deep Neural Network"}, {"color": "#66CCFF", "id": "Heechul Jung", "label": "Heechul Jung", "shape": "dot", "size": 25, "title": "Heechul Jung"}, {"color": "#66CCFF", "id": "Rotating Your face Using Multi-task Deep Neural Network", "label": "Rotating Your face Using Multi-task Deep Neural Network", "shape": "dot", "size": 25, "title": "Rotating Your face Using Multi-task Deep Neural Network"}, {"color": "#66CCFF", "id": "ByungIn Yoo", "label": "ByungIn Yoo", "shape": "dot", "size": 25, "title": "ByungIn Yoo"}, {"color": "#66CCFF", "id": "Changkyu Choi", "label": "Changkyu Choi", "shape": "dot", "size": 25, "title": "Changkyu Choi"}, {"color": "#66CCFF", "id": "Dusik Park", "label": "Dusik Park", "shape": "dot", "size": 25, "title": "Dusik Park"}, {"color": "#66CCFF", "id": "Junmo Kim", "label": "Junmo Kim", "shape": "dot", "size": 25, "title": "Junmo Kim"}, {"color": "#66CCFF", "id": "Yim_Rotating_Your_Face_2015_CVPR_paper.pdf", "label": "Yim_Rotating_Your_Face_2015_CVPR_paper.pdf", "shape": "dot", "size": 25, "title": "Yim_Rotating_Your_Face_2015_CVPR_paper.pdf"}, {"color": "#66CCFF", "id": "CVPR paper", "label": "CVPR paper", "shape": "dot", "size": 25, "title": "CVPR paper"}, {"color": "#66CCFF", "id": "Face recognition", "label": "Face recognition", "shape": "dot", "size": 25, "title": "Face recognition"}, {"color": "#66CCFF", "id": "viewpoint and illumination changes", "label": "viewpoint and illumination changes", "shape": "dot", "size": 25, "title": "viewpoint and illumination changes"}, {"color": "#66CCFF", "id": "new deep architecture", "label": "new deep architecture", "shape": "dot", "size": 25, "title": "new deep architecture"}, {"color": "#66CCFF", "id": "novel type of multitask learning", "label": "novel type of multitask learning", "shape": "dot", "size": 25, "title": "novel type of multitask learning"}, {"color": "#66CCFF", "id": "target-pose face image", "label": "target-pose face image", "shape": "dot", "size": 25, "title": "target-pose face image"}, {"color": "#66CCFF", "id": "arbitrary pose and illumination image", "label": "arbitrary pose and illumination image", "shape": "dot", "size": 25, "title": "arbitrary pose and illumination image"}, {"color": "#66CCFF", "id": "target pose", "label": "target pose", "shape": "dot", "size": 25, "title": "target pose"}, {"color": "#66CCFF", "id": "user\u2019s intention", "label": "user\u2019s intention", "shape": "dot", "size": 25, "title": "user\u2019s intention"}, {"color": "#66CCFF", "id": "multi-task model", "label": "multi-task model", "shape": "dot", "size": 25, "title": "multi-task model"}, {"color": "#66CCFF", "id": "identity preservation", "label": "identity preservation", "shape": "dot", "size": 25, "title": "identity preservation"}, {"color": "#66CCFF", "id": "Controlled Pose Image (CPI)", "label": "Controlled Pose Image (CPI)", "shape": "dot", "size": 25, "title": "Controlled Pose Image (CPI)"}, {"color": "#66CCFF", "id": "pose-illumination- invariant feature", "label": "pose-illumination- invariant feature", "shape": "dot", "size": 25, "title": "pose-illumination- invariant feature"}, {"color": "#66CCFF", "id": "state-of-the-art algorithms", "label": "state-of-the-art algorithms", "shape": "dot", "size": 25, "title": "state-of-the-art algorithms"}, {"color": "#66CCFF", "id": "MultiPIE dataset", "label": "MultiPIE dataset", "shape": "dot", "size": 25, "title": "MultiPIE dataset"}, {"color": "#66CCFF", "id": "Proposed method", "label": "Proposed method", "shape": "dot", "size": 25, "title": "Proposed method"}, {"color": "#66CCFF", "id": "School of Electrical Engineerin, KAIST", "label": "School of Electrical Engineerin, KAIST", "shape": "dot", "size": 25, "title": "School of Electrical Engineerin, KAIST"}, {"color": "#66CCFF", "id": "Samsung Advanced Institute of Technology", "label": "Samsung Advanced Institute of Technology", "shape": "dot", "size": 25, "title": "Samsung Advanced Institute of Technology"}, {"color": "#66CCFF", "id": "School of Electrical Engineering, KAIST", "label": "School of Electrical Engineering, KAIST", "shape": "dot", "size": 25, "title": "School of Electrical Engineering, KAIST"}, {"color": "#66CCFF", "id": "Ramakrishna Vedantam", "label": "Ramakrishna Vedantam", "shape": "dot", "size": 25, "title": "Ramakrishna Vedantam"}, {"color": "#66CCFF", "id": "CIDEr", "label": "CIDEr", "shape": "dot", "size": 25, "title": "CIDEr"}, {"color": "#66CCFF", "id": "C. Lawrence Zitnick", "label": "C. Lawrence Zitnick", "shape": "dot", "size": 25, "title": "C. Lawrence Zitnick"}, {"color": "#66CCFF", "id": "Devi Parikh", "label": "Devi Parikh", "shape": "dot", "size": 25, "title": "Devi Parikh"}, {"color": "#66CCFF", "id": "School of Electrical Engineering", "label": "School of Electrical Engineering", "shape": "dot", "size": 25, "title": "School of Electrical Engineering"}, {"color": "#66CCFF", "id": "junmo.kim@kaisten.ac.kr", "label": "junmo.kim@kaisten.ac.kr", "shape": "dot", "size": 25, "title": "junmo.kim@kaisten.ac.kr"}, {"color": "#66CCFF", "id": "Image Description Evaluation", "label": "Image Description Evaluation", "shape": "dot", "size": 25, "title": "Image Description Evaluation"}, {"color": "#66CCFF", "id": "KAIST", "label": "KAIST", "shape": "dot", "size": 25, "title": "KAIST"}, {"color": "#66CCFF", "id": "Image Description", "label": "Image Description", "shape": "dot", "size": 25, "title": "Image Description"}, {"color": "#66CCFF", "id": "Computer Vision", "label": "Computer Vision", "shape": "dot", "size": 25, "title": "Computer Vision"}, {"color": "#66CCFF", "id": "Natural Language Processing", "label": "Natural Language Processing", "shape": "dot", "size": 25, "title": "Natural Language Processing"}, {"color": "#66CCFF", "id": "Object Detection", "label": "Object Detection", "shape": "dot", "size": 25, "title": "Object Detection"}, {"color": "#66CCFF", "id": "Attribute Classification", "label": "Attribute Classification", "shape": "dot", "size": 25, "title": "Attribute Classification"}, {"color": "#66CCFF", "id": "Paradigm", "label": "Paradigm", "shape": "dot", "size": 25, "title": "Paradigm"}, {"color": "#66CCFF", "id": "Human Consensus", "label": "Human Consensus", "shape": "dot", "size": 25, "title": "Human Consensus"}, {"color": "#66CCFF", "id": "Triplet-based Method", "label": "Triplet-based Method", "shape": "dot", "size": 25, "title": "Triplet-based Method"}, {"color": "#66CCFF", "id": "Automated Metric", "label": "Automated Metric", "shape": "dot", "size": 25, "title": "Automated Metric"}, {"color": "#66CCFF", "id": "Datasets", "label": "Datasets", "shape": "dot", "size": 25, "title": "Datasets"}, {"color": "#66CCFF", "id": "PASCAL-50S", "label": "PASCAL-50S", "shape": "dot", "size": 25, "title": "PASCAL-50S"}, {"color": "#66CCFF", "id": "ABSTRACT-50S", "label": "ABSTRACT-50S", "shape": "dot", "size": 25, "title": "ABSTRACT-50S"}, {"color": "#66CCFF", "id": "Metric", "label": "Metric", "shape": "dot", "size": 25, "title": "Metric"}, {"color": "#66CCFF", "id": "Human Judgment", "label": "Human Judgment", "shape": "dot", "size": 25, "title": "Human Judgment"}, {"color": "#66CCFF", "id": "nsensus", "label": "nsensus", "shape": "dot", "size": 25, "title": "nsensus"}, {"color": "#66CCFF", "id": "metric", "label": "metric", "shape": "dot", "size": 25, "title": "metric"}, {"color": "#66CCFF", "id": "human judgment", "label": "human judgment", "shape": "dot", "size": 25, "title": "human judgment"}, {"color": "#66CCFF", "id": "existing metrics", "label": "existing metrics", "shape": "dot", "size": 25, "title": "existing metrics"}, {"color": "#66CCFF", "id": "sentences", "label": "sentences", "shape": "dot", "size": 25, "title": "sentences"}, {"color": "#66CCFF", "id": "various sources", "label": "various sources", "shape": "dot", "size": 25, "title": "various sources"}, {"color": "#66CCFF", "id": "CIDEr-D", "label": "CIDEr-D", "shape": "dot", "size": 25, "title": "CIDEr-D"}, {"color": "#66CCFF", "id": "MS COCO evaluation server", "label": "MS COCO evaluation server", "shape": "dot", "size": 25, "title": "MS COCO evaluation server"}, {"color": "#66CCFF", "id": "systematic evaluation", "label": "systematic evaluation", "shape": "dot", "size": 25, "title": "systematic evaluation"}, {"color": "#66CCFF", "id": "image description approaches", "label": "image description approaches", "shape": "dot", "size": 25, "title": "image description approaches"}, {"color": "#66CCFF", "id": "protocol", "label": "protocol", "shape": "dot", "size": 25, "title": "protocol"}, {"color": "#66CCFF", "id": "Automated Metrics", "label": "Automated Metrics", "shape": "dot", "size": 25, "title": "Automated Metrics"}, {"color": "#66CCFF", "id": "Microsoft Research", "label": "Microsoft Research", "shape": "dot", "size": 25, "title": "Microsoft Research"}, {"color": "#66CCFF", "id": "Virginia Tech", "label": "Virginia Tech", "shape": "dot", "size": 25, "title": "Virginia Tech"}, {"color": "#66CCFF", "id": "Zhenzhong Lan", "label": "Zhenzhong Lan", "shape": "dot", "size": 25, "title": "Zhenzhong Lan"}, {"color": "#66CCFF", "id": "Beyond Gaussian Pyramid", "label": "Beyond Gaussian Pyramid", "shape": "dot", "size": 25, "title": "Beyond Gaussian Pyramid"}, {"color": "#66CCFF", "id": "Alexander G. Hauptmann", "label": "Alexander G. Hauptmann", "shape": "dot", "size": 25, "title": "Alexander G. Hauptmann"}, {"color": "#66CCFF", "id": "Bhiksha Raj", "label": "Bhiksha Raj", "shape": "dot", "size": 25, "title": "Bhiksha Raj"}, {"color": "#66CCFF", "id": "Lan_Beyond_Gausian_Pyramid_2015_CVPR_supplemental", "label": "Lan_Beyond_Gausian_Pyramid_2015_CVPR_supplemental", "shape": "dot", "size": 25, "title": "Lan_Beyond_Gausian_Pyramid_2015_CVPR_supplemental"}, {"color": "#66CCFF", "id": "Beyond Gaussian Pyramid: Multi-skip Feature Stacking for Action Recognition", "label": "Beyond Gaussian Pyramid: Multi-skip Feature Stacking for Action Recognition", "shape": "dot", "size": 25, "title": "Beyond Gaussian Pyramid: Multi-skip Feature Stacking for Action Recognition"}, {"color": "#66CCFF", "id": "Action Recognition", "label": "Action Recognition", "shape": "dot", "size": 25, "title": "Action Recognition"}, {"color": "#66CCFF", "id": "proof of theorem 1", "label": "proof of theorem 1", "shape": "dot", "size": 25, "title": "proof of theorem 1"}, {"color": "#66CCFF", "id": "proof of theorem 2", "label": "proof of theorem 2", "shape": "dot", "size": 25, "title": "proof of theorem 2"}, {"color": "#66CCFF", "id": "School of Computer Science, Carnegie Mellon University", "label": "School of Computer Science, Carnegie Mellon University", "shape": "dot", "size": 25, "title": "School of Computer Science, Carnegie Mellon University"}, {"color": "#66CCFF", "id": "Ming Lin", "label": "Ming Lin", "shape": "dot", "size": 25, "title": "Ming Lin"}, {"color": "#66CCFF", "id": "Feature Stacking", "label": "Feature Stacking", "shape": "dot", "size": 25, "title": "Feature Stacking"}, {"color": "#66CCFF", "id": "Matrix Bernstein\u0027s Inequality", "label": "Matrix Bernstein\u0027s Inequality", "shape": "dot", "size": 25, "title": "Matrix Bernstein\u0027s Inequality"}, {"color": "#66CCFF", "id": "Condition Number", "label": "Condition Number", "shape": "dot", "size": 25, "title": "Condition Number"}, {"color": "#66CCFF", "id": "Alexander G. Hauptman", "label": "Alexander G. Hauptman", "shape": "dot", "size": 25, "title": "Alexander G. Hauptman"}, {"color": "#66CCFF", "id": "cli@cs.cmu.edu", "label": "cli@cs.cmu.edu", "shape": "dot", "size": 25, "title": "cli@cs.cmu.edu"}, {"color": "#66CCFF", "id": "bhiksha@cs.cmu.edu", "label": "bhiksha@cs.cmu.edu", "shape": "dot", "size": 25, "title": "bhiksha@cs.cmu.edu"}, {"color": "#66CCFF", "id": "Kwang In Kim", "label": "Kwang In Kim", "shape": "dot", "size": 25, "title": "Kwang In Kim"}, {"color": "#66CCFF", "id": "Kim_Local_High-Order_Regularization_2015_CVPR_paper.pdf", "label": "Kim_Local_High-Order_Regularization_2015_CVPR_paper.pdf", "shape": "dot", "size": 25, "title": "Kim_Local_High-Order_Regularization_2015_CVPR_paper.pdf"}, {"color": "#66CCFF", "id": "James Tompkin", "label": "James Tompkin", "shape": "dot", "size": 25, "title": "James Tompkin"}, {"color": "#66CCFF", "id": "Hanspeter Pfister", "label": "Hanspeter Pfister", "shape": "dot", "size": 25, "title": "Hanspeter Pfister"}, {"color": "#66CCFF", "id": "Kim_Local_High-Order_RegularIZATION_2015_CVPR_paper.pdf", "label": "Kim_Local_High-Order_RegularIZATION_2015_CVPR_paper.pdf", "shape": "dot", "size": 25, "title": "Kim_Local_High-Order_RegularIZATION_2015_CVPR_paper.pdf"}, {"color": "#66CCFF", "id": "Christian Theobalt", "label": "Christian Theobalt", "shape": "dot", "size": 25, "title": "Christian Theobalt"}, {"color": "#66CCFF", "id": "Local High-order Regularization on Data Manifolds", "label": "Local High-order Regularization on Data Manifolds", "shape": "dot", "size": 25, "title": "Local High-order Regularization on Data Manifolds"}, {"color": "#66CCFF", "id": "Carnegie Mellon University", "label": "Carnegie Mellon University", "shape": "dot", "size": 25, "title": "Carnegie Mellon University"}, {"color": "#66CCFF", "id": "School of Computer Science", "label": "School of Computer Science", "shape": "dot", "size": 25, "title": "School of Computer Science"}, {"color": "#66CCFF", "id": "Graph Laplacian Regularizer", "label": "Graph Laplacian Regularizer", "shape": "dot", "size": 25, "title": "Graph Laplacian Regularizer"}, {"color": "#66CCFF", "id": "degeneracy", "label": "degeneracy", "shape": "dot", "size": 25, "title": "degeneracy"}, {"color": "#66CCFF", "id": "Iterated Graph Laplacian", "label": "Iterated Graph Laplacian", "shape": "dot", "size": 25, "title": "Iterated Graph Laplacian"}, {"color": "#66CCFF", "id": "computational complexity", "label": "computational complexity", "shape": "dot", "size": 25, "title": "computational complexity"}, {"color": "#66CCFF", "id": "Proposed Regularizer", "label": "Proposed Regularizer", "shape": "dot", "size": 25, "title": "Proposed Regularizer"}, {"color": "#66CCFF", "id": "sparsity", "label": "sparsity", "shape": "dot", "size": 25, "title": "sparsity"}, {"color": "#66CCFF", "id": "Approach", "label": "Approach", "shape": "dot", "size": 25, "title": "Approach"}, {"color": "#66CCFF", "id": "manifold approximation", "label": "manifold approximation", "shape": "dot", "size": 25, "title": "manifold approximation"}, {"color": "#66CCFF", "id": "local derivative evaluations", "label": "local derivative evaluations", "shape": "dot", "size": 25, "title": "local derivative evaluations"}, {"color": "#66CCFF", "id": "Experiments", "label": "Experiments", "shape": "dot", "size": 25, "title": "Experiments"}, {"color": "#66CCFF", "id": "Manifold Approximation", "label": "Manifold Approximation", "shape": "dot", "size": 25, "title": "Manifold Approximation"}, {"color": "#66CCFF", "id": "surrogate geometry", "label": "surrogate geometry", "shape": "dot", "size": 25, "title": "surrogate geometry"}, {"color": "#66CCFF", "id": "Graph Laplacian Regularization", "label": "Graph Laplacian Regularization", "shape": "dot", "size": 25, "title": "Graph Laplacian Regularization"}, {"color": "#66CCFF", "id": "Semi-Supervised Learning", "label": "Semi-Supervised Learning", "shape": "dot", "size": 25, "title": "Semi-Supervised Learning"}, {"color": "#66CCFF", "id": "Laplacian eigenmaps", "label": "Laplacian eigenmaps", "shape": "dot", "size": 25, "title": "Laplacian eigenmaps"}, {"color": "#66CCFF", "id": "dimensionality reduction", "label": "dimensionality reduction", "shape": "dot", "size": 25, "title": "dimensionality reduction"}, {"color": "#66CCFF", "id": "High-Order Derivatives", "label": "High-Order Derivatives", "shape": "dot", "size": 25, "title": "High-Order Derivatives"}, {"color": "#66CCFF", "id": "Hessian eigenmaps", "label": "Hessian eigenmaps", "shape": "dot", "size": 25, "title": "Hessian eigenmaps"}, {"color": "#66CCFF", "id": "Hessian eigenMaps", "label": "Hessian eigenMaps", "shape": "dot", "size": 25, "title": "Hessian eigenMaps"}, {"color": "#66CCFF", "id": "locally linear embedding", "label": "locally linear embedding", "shape": "dot", "size": 25, "title": "locally linear embedding"}, {"color": "#66CCFF", "id": "Normalized cuts", "label": "Normalized cuts", "shape": "dot", "size": 25, "title": "Normalized cuts"}, {"color": "#66CCFF", "id": "image segmentation", "label": "image segmentation", "shape": "dot", "size": 25, "title": "image segmentation"}, {"color": "#66CCFF", "id": "Reproducing Kernel Hilbert Space (RKHS)", "label": "Reproducing Kernel Hilbert Space (RKHS)", "shape": "dot", "size": 25, "title": "Reproducing Kernel Hilbert Space (RKHS)"}, {"color": "#66CCFF", "id": "Spectral Clustering", "label": "Spectral Clustering", "shape": "dot", "size": 25, "title": "Spectral Clustering"}, {"color": "#66CCFF", "id": "Plug-in classifiers", "label": "Plug-in classifiers", "shape": "dot", "size": 25, "title": "Plug-in classifiers"}, {"color": "#66CCFF", "id": "fast learning rates", "label": "fast learning rates", "shape": "dot", "size": 25, "title": "fast learning rates"}, {"color": "#66CCFF", "id": "Supervised Learning", "label": "Supervised Learning", "shape": "dot", "size": 25, "title": "Supervised Learning"}, {"color": "#66CCFF", "id": "MIT Press", "label": "MIT Press", "shape": "dot", "size": 25, "title": "MIT Press"}, {"color": "#66CCFF", "id": "2006", "label": "2006", "shape": "dot", "size": 25, "title": "2006"}, {"color": "#66CCFF", "id": "Statistics and Computing", "label": "Statistics and Computing", "shape": "dot", "size": 25, "title": "Statistics and Computing"}, {"color": "#66CCFF", "id": "Normalized Cuts", "label": "Normalized Cuts", "shape": "dot", "size": 25, "title": "Normalized Cuts"}, {"color": "#66CCFF", "id": "Image Segmentation", "label": "Image Segmentation", "shape": "dot", "size": 25, "title": "Image Segmentation"}, {"color": "#66CCFF", "id": "Real Analysis and Probability", "label": "Real Analysis and Probability", "shape": "dot", "size": 25, "title": "Real Analysis and Probability"}, {"color": "#66CCFF", "id": "Cambridge University Press", "label": "Cambridge University Press", "shape": "dot", "size": 25, "title": "Cambridge University Press"}, {"color": "#66CCFF", "id": "Graphs", "label": "Graphs", "shape": "dot", "size": 25, "title": "Graphs"}, {"color": "#66CCFF", "id": "Manifolds", "label": "Manifolds", "shape": "dot", "size": 25, "title": "Manifolds"}, {"color": "#66CCFF", "id": "Graph Laplacians", "label": "Graph Laplacians", "shape": "dot", "size": 25, "title": "Graph Laplacians"}, {"color": "#66CCFF", "id": "Pointwise Consistency", "label": "Pointwise Consistency", "shape": "dot", "size": 25, "title": "Pointwise Consistency"}, {"color": "#66CCFF", "id": "Jianping Shi", "label": "Jianping Shi", "shape": "dot", "size": 25, "title": "Jianping Shi"}, {"color": "#66CCFF", "id": "Just Noticeable Defocus Blur Detection and Estimation", "label": "Just Noticeable Defocus Blur Detection and Estimation", "shape": "dot", "size": 25, "title": "Just Noticeable Defocus Blur Detection and Estimation"}, {"color": "#66CCFF", "id": "Li Xu", "label": "Li Xu", "shape": "dot", "size": 25, "title": "Li Xu"}, {"color": "#66CCFF", "id": "Jiaya Jia", "label": "Jiaya Jia", "shape": "dot", "size": 25, "title": "Jiaya Jia"}, {"color": "#66CCFF", "id": "Just Noticeable Defocus Blur Detectio and Estimation", "label": "Just Noticeable Defocus Blur Detectio and Estimation", "shape": "dot", "size": 25, "title": "Just Noticeable Defocus Blur Detectio and Estimation"}, {"color": "#66CCFF", "id": "Shi_Just_Noticeable_Defocus_2015_CVPR_paper.pdf", "label": "Shi_Just_Noticeable_Defocus_2015_CVPR_paper.pdf", "shape": "dot", "size": 25, "title": "Shi_Just_Noticeable_Defocus_2015_CVPR_paper.pdf"}, {"color": "#66CCFF", "id": "MPI for Informatics", "label": "MPI for Informatics", "shape": "dot", "size": 25, "title": "MPI for Informatics"}, {"color": "#66CCFF", "id": "just noticeable blur", "label": "just noticeable blur", "shape": "dot", "size": 25, "title": "just noticeable blur"}, {"color": "#66CCFF", "id": "defocus", "label": "defocus", "shape": "dot", "size": 25, "title": "defocus"}, {"color": "#66CCFF", "id": "small number of pixels", "label": "small number of pixels", "shape": "dot", "size": 25, "title": "small number of pixels"}, {"color": "#66CCFF", "id": "slight edge blurriness", "label": "slight edge blurriness", "shape": "dot", "size": 25, "title": "slight edge blurriness"}, {"color": "#66CCFF", "id": "informative clues", "label": "informative clues", "shape": "dot", "size": 25, "title": "informative clues"}, {"color": "#66CCFF", "id": "depth", "label": "depth", "shape": "dot", "size": 25, "title": "depth"}, {"color": "#66CCFF", "id": "blur descriptors", "label": "blur descriptors", "shape": "dot", "size": 25, "title": "blur descriptors"}, {"color": "#66CCFF", "id": "local information", "label": "local information", "shape": "dot", "size": 25, "title": "local information"}, {"color": "#66CCFF", "id": "blur feature", "label": "blur feature", "shape": "dot", "size": 25, "title": "blur feature"}, {"color": "#66CCFF", "id": "sparse representation", "label": "sparse representation", "shape": "dot", "size": 25, "title": "sparse representation"}, {"color": "#66CCFF", "id": "image decomposition", "label": "image decomposition", "shape": "dot", "size": 25, "title": "image decomposition"}, {"color": "#66CCFF", "id": "sparse edge representation", "label": "sparse edge representation", "shape": "dot", "size": 25, "title": "sparse edge representation"}, {"color": "#66CCFF", "id": "blur strength estimation", "label": "blur strength estimation", "shape": "dot", "size": 25, "title": "blur strength estimation"}, {"color": "#66CCFF", "id": "age decomposition", "label": "age decomposition", "shape": "dot", "size": 25, "title": "age decomposition"}, {"color": "#66CCFF", "id": "feature", "label": "feature", "shape": "dot", "size": 25, "title": "feature"}, {"color": "#66CCFF", "id": "generality", "label": "generality", "shape": "dot", "size": 25, "title": "generality"}, {"color": "#66CCFF", "id": "The Chinese University of Hong Kong", "label": "The Chinese University of Hong Kong", "shape": "dot", "size": 25, "title": "The Chinese University of Hong Kong"}, {"color": "#66CCFF", "id": "Lenovo R\u0026T", "label": "Lenovo R\u0026T", "shape": "dot", "size": 25, "title": "Lenovo R\u0026T"}, {"color": "#66CCFF", "id": "Bernt Schiele", "label": "Bernt Schiele", "shape": "dot", "size": 25, "title": "Bernt Schiele"}, {"color": "#66CCFF", "id": "Filtered Channel Features for Pedestrian Detection", "label": "Filtered Channel Features for Pedestrian Detection", "shape": "dot", "size": 25, "title": "Filtered Channel Features for Pedestrian Detection"}, {"color": "#66CCFF", "id": "Shanshan Zhang", "label": "Shanshan Zhang", "shape": "dot", "size": 25, "title": "Shanshan Zhang"}, {"color": "#66CCFF", "id": "Rodrigo Benenson", "label": "Rodrigo Benenson", "shape": "dot", "size": 25, "title": "Rodrigo Benenson"}, {"color": "#66CCFF", "id": "Zhang_Filtered_Feature_Channels_2015_CVPR_supplemental.pdf", "label": "Zhang_Filtered_Feature_Channels_2015_CVPR_supplemental.pdf", "shape": "dot", "size": 25, "title": "Zhang_Filtered_Feature_Channels_2015_CVPR_supplemental.pdf"}, {"color": "#66CCFF", "id": "Zhang_Filtered_Feature_Channels_2015_CVPR_supplemental", "label": "Zhang_Filtered_Feature_Channels_2015_CVPR_supplemental", "shape": "dot", "size": 25, "title": "Zhang_Filtered_Feature_Channels_2015_CVPR_supplemental"}, {"color": "#66CCFF", "id": "Checkerboards4x3 model", "label": "Checkerboards4x3 model", "shape": "dot", "size": 25, "title": "Checkerboards4x3 model"}, {"color": "#66CCFF", "id": "pedestrian detection model", "label": "pedestrian detection model", "shape": "dot", "size": 25, "title": "pedestrian detection model"}, {"color": "#66CCFF", "id": "Roerei model", "label": "Roerei model", "shape": "dot", "size": 25, "title": "Roerei model"}, {"color": "#66CCFF", "id": "weaker model", "label": "weaker model", "shape": "dot", "size": 25, "title": "weaker model"}, {"color": "#66CCFF", "id": "filtered channels", "label": "filtered channels", "shape": "dot", "size": 25, "title": "filtered channels"}, {"color": "#66CCFF", "id": "areas of pedestrian deemed informative", "label": "areas of pedestrian deemed informative", "shape": "dot", "size": 25, "title": "areas of pedestrian deemed informative"}, {"color": "#66CCFF", "id": "extraction of discriminative information", "label": "extraction of discriminative information", "shape": "dot", "size": 25, "title": "extraction of discriminative information"}, {"color": "#66CCFF", "id": "channel U", "label": "channel U", "shape": "dot", "size": 25, "title": "channel U"}, {"color": "#66CCFF", "id": "face", "label": "face", "shape": "dot", "size": 25, "title": "face"}, {"color": "#66CCFF", "id": "channel L", "label": "channel L", "shape": "dot", "size": 25, "title": "channel L"}, {"color": "#66CCFF", "id": "body", "label": "body", "shape": "dot", "size": 25, "title": "body"}, {"color": "#66CCFF", "id": "gradient magnitude channel", "label": "gradient magnitude channel", "shape": "dot", "size": 25, "title": "gradient magnitude channel"}, {"color": "#66CCFF", "id": "filter usage distribution", "label": "filter usage distribution", "shape": "dot", "size": 25, "title": "filter usage distribution"}, {"color": "#66CCFF", "id": "filter bank families", "label": "filter bank families", "shape": "dot", "size": 25, "title": "filter bank families"}, {"color": "#66CCFF", "id": "filter", "label": "filter", "shape": "dot", "size": 25, "title": "filter"}, {"color": "#66CCFF", "id": "feature for decision tree split nodes", "label": "feature for decision tree split nodes", "shape": "dot", "size": 25, "title": "feature for decision tree split nodes"}, {"color": "#66CCFF", "id": "decision tree split node feature", "label": "decision tree split node feature", "shape": "dot", "size": 25, "title": "decision tree split node feature"}, {"color": "#66CCFF", "id": "text", "label": "text", "shape": "dot", "size": 25, "title": "text"}, {"color": "#66CCFF", "id": "Benenson, R., Mathias, M., Tuytelaars, T., \u0026 Van Gools, L. (2013)", "label": "Benenson, R., Mathias, M., Tuytelaars, T., \u0026 Van Gools, L. (2013)", "shape": "dot", "size": 25, "title": "Benenson, R., Mathias, M., Tuytelaars, T., \u0026 Van Gools, L. (2013)"}, {"color": "#66CCFF", "id": "Roerei, et al.", "label": "Roerei, et al.", "shape": "dot", "size": 25, "title": "Roerei, et al."}, {"color": "#66CCFF", "id": "filter usage", "label": "filter usage", "shape": "dot", "size": 25, "title": "filter usage"}, {"color": "#66CCFF", "id": "ACF", "label": "ACF", "shape": "dot", "size": 25, "title": "ACF"}, {"color": "#66CCFF", "id": "decision tree split nodes", "label": "decision tree split nodes", "shape": "dot", "size": 25, "title": "decision tree split nodes"}, {"color": "#66CCFF", "id": "filter features", "label": "filter features", "shape": "dot", "size": 25, "title": "filter features"}, {"color": "#66CCFF", "id": "Benenson, R., Mathias, M., Tuytelaars, T., \u0026 Van Gool, L. (2013)", "label": "Benenson, R., Mathias, M., Tuytelaars, T., \u0026 Van Gool, L. (2013)", "shape": "dot", "size": 25, "title": "Benenson, R., Mathias, M., Tuytelaars, T., \u0026 Van Gool, L. (2013)"}, {"color": "#66CCFF", "id": "decision tree", "label": "decision tree", "shape": "dot", "size": 25, "title": "decision tree"}, {"color": "#66CCFF", "id": "spatial feature distribution", "label": "spatial feature distribution", "shape": "dot", "size": 25, "title": "spatial feature distribution"}, {"color": "#66CCFF", "id": "pedestrian detection", "label": "pedestrian detection", "shape": "dot", "size": 25, "title": "pedestrian detection"}, {"color": "#66CCFF", "id": "parametrization of the trifocal tensor", "label": "parametrization of the trifocal tensor", "shape": "dot", "size": 25, "title": "parametrization of the trifocal tensor"}, {"color": "#66CCFF", "id": "quotient Riemannian manifold", "label": "quotient Riemannian manifold", "shape": "dot", "size": 25, "title": "quotient Riemannian manifold"}, {"color": "#66CCFF", "id": "almost symmetric", "label": "almost symmetric", "shape": "dot", "size": 25, "title": "almost symmetric"}, {"color": "#66CCFF", "id": "preferred camera", "label": "preferred camera", "shape": "dot", "size": 25, "title": "preferred camera"}, {"color": "#66CCFF", "id": "optimization techniques on manifolds", "label": "optimization techniques on manifolds", "shape": "dot", "size": 25, "title": "optimization techniques on manifolds"}, {"color": "#66CCFF", "id": "Riemannian structure", "label": "Riemannian structure", "shape": "dot", "size": 25, "title": "Riemannian structure"}, {"color": "#66CCFF", "id": "notion of distance", "label": "notion of distance", "shape": "dot", "size": 25, "title": "notion of distance"}, {"color": "#66CCFF", "id": "distance between trifocal tensors", "label": "distance between trifocal tensors", "shape": "dot", "size": 25, "title": "distance between trifocal tensors"}, {"color": "#66CCFF", "id": "meaningful results", "label": "meaningful results", "shape": "dot", "size": 25, "title": "meaningful results"}, {"color": "#66CCFF", "id": "Structure from Motion problem", "label": "Structure from Motion problem", "shape": "dot", "size": 25, "title": "Structure from Motion problem"}, {"color": "#66CCFF", "id": "work", "label": "work", "shape": "dot", "size": 25, "title": "work"}, {"color": "#66CCFF", "id": "new formulation of the trifocal tensor", "label": "new formulation of the trifocal tensor", "shape": "dot", "size": 25, "title": "new formulation of the trifocal tensor"}, {"color": "#66CCFF", "id": "trifocal tensor", "label": "trifocal tensor", "shape": "dot", "size": 25, "title": "trifocal tensor"}, {"color": "#66CCFF", "id": "tensor", "label": "tensor", "shape": "dot", "size": 25, "title": "tensor"}, {"color": "#66CCFF", "id": "Structure from Motion", "label": "Structure from Motion", "shape": "dot", "size": 25, "title": "Structure from Motion"}, {"color": "#66CCFF", "id": "Trifocal Tensor", "label": "Trifocal Tensor", "shape": "dot", "size": 25, "title": "Trifocal Tensor"}, {"color": "#66CCFF", "id": "distances", "label": "distances", "shape": "dot", "size": 25, "title": "distances"}, {"color": "#66CCFF", "id": "Geometric Computer Vision", "label": "Geometric Computer Vision", "shape": "dot", "size": 25, "title": "Geometric Computer Vision"}, {"color": "#66CCFF", "id": "Camera Calibration", "label": "Camera Calibration", "shape": "dot", "size": 25, "title": "Camera Calibration"}, {"color": "#66CCFF", "id": "Optimization Algorithms", "label": "Optimization Algorithms", "shape": "dot", "size": 25, "title": "Optimization Algorithms"}, {"color": "#66CCFF", "id": "Absil, Mahony, and Sepulchre", "label": "Absil, Mahony, and Sepulchre", "shape": "dot", "size": 25, "title": "Absil, Mahony, and Sepulchre"}, {"color": "#66CCFF", "id": "Nonlinear Programming", "label": "Nonlinear Programming", "shape": "dot", "size": 25, "title": "Nonlinear Programming"}, {"color": "#66CCFF", "id": "Bertsekas", "label": "Bertsekas", "shape": "dot", "size": 25, "title": "Bertsekas"}, {"color": "#66CCFF", "id": "Manopt", "label": "Manopt", "shape": "dot", "size": 25, "title": "Manopt"}, {"color": "#66CCFF", "id": "Boumal, Mishra, Absil, and Sepulchre", "label": "Boumal, Mishra, Absil, and Sepulchre", "shape": "dot", "size": 25, "title": "Boumal, Mishra, Absil, and Sepulchre"}, {"color": "#66CCFF", "id": "Lines and points", "label": "Lines and points", "shape": "dot", "size": 25, "title": "Lines and points"}, {"color": "#66CCFF", "id": "Hartley\u0027s work", "label": "Hartley\u0027s work", "shape": "dot", "size": 25, "title": "Hartley\u0027s work"}, {"color": "#66CCFF", "id": "Multiple View Geometry", "label": "Multiple View Geometry", "shape": "dot", "size": 25, "title": "Multiple View Geometry"}, {"color": "#66CCFF", "id": "Hartley and Zisserman", "label": "Hartley and Zisserman", "shape": "dot", "size": 25, "title": "Hartley and Zisserman"}, {"color": "#66CCFF", "id": "Hartley, R. I.", "label": "Hartley, R. I.", "shape": "dot", "size": 25, "title": "Hartley, R. I."}, {"color": "#66CCFF", "id": "views and the trifocal tensor", "label": "views and the trifocal tensor", "shape": "dot", "size": 25, "title": "views and the trifocal tensor"}, {"color": "#66CCFF", "id": "Int. J. Comput. Vision", "label": "Int. J. Comput. Vision", "shape": "dot", "size": 25, "title": "Int. J. Comput. Vision"}, {"color": "#66CCFF", "id": "Projective reconstruction from line correspondences", "label": "Projective reconstruction from line correspondences", "shape": "dot", "size": 25, "title": "Projective reconstruction from line correspondences"}, {"color": "#66CCFF", "id": "IEEE Conf. on Computer Vision and Pattern Recognition", "label": "IEEE Conf. on Computer Vision and Pattern Recognition", "shape": "dot", "size": 25, "title": "IEEE Conf. on Computer Vision and Pattern Recognition"}, {"color": "#66CCFF", "id": "Papapdoulo, T.", "label": "Papapdoulo, T.", "shape": "dot", "size": 25, "title": "Papapdoulo, T."}, {"color": "#66CCFF", "id": "A new characterization of the trifocal tensor", "label": "A new characterization of the trifocal tensor", "shape": "dot", "size": 25, "title": "A new characterization of the trifocal tensor"}, {"color": "#66CCFF", "id": "European Conference on Computer Vision", "label": "European Conference on Computer Vision", "shape": "dot", "size": 25, "title": "European Conference on Computer Vision"}, {"color": "#66CCFF", "id": "Kendall, D. G.", "label": "Kendall, D. G.", "shape": "dot", "size": 25, "title": "Kendall, D. G."}, {"color": "#66CCFF", "id": "Shape Manifolds, Procustean Metrics, and Complex Projective Spaces", "label": "Shape Manifolds, Procustean Metrics, and Complex Projective Spaces", "shape": "dot", "size": 25, "title": "Shape Manifolds, Procustean Metrics, and Complex Projective Spaces"}, {"color": "#66CCFF", "id": "Bulletin of the London Mathematical Society", "label": "Bulletin of the London Mathematical Society", "shape": "dot", "size": 25, "title": "Bulletin of the London Mathematical Society"}, {"color": "#66CCFF", "id": "Torr, P.", "label": "Torr, P.", "shape": "dot", "size": 25, "title": "Torr, P."}, {"color": "#66CCFF", "id": "Robust parameterization and computation of the trifocal tensor", "label": "Robust parameterization and computation of the trifocal tensor", "shape": "dot", "size": 25, "title": "Robust parameterization and computation of the trifocal tensor"}, {"color": "#66CCFF", "id": "Image and Vision Computing", "label": "Image and Vision Computing", "shape": "dot", "size": 25, "title": "Image and Vision Computing"}, {"color": "#66CCFF", "id": "Multiple View Geometry in Computer Vision", "label": "Multiple View Geometry in Computer Vision", "shape": "dot", "size": 25, "title": "Multiple View Geometry in Computer Vision"}, {"color": "#66CCFF", "id": "Robust parameterization", "label": "Robust parameterization", "shape": "dot", "size": 25, "title": "Robust parameterization"}, {"color": "#66CCFF", "id": "Weng, J.", "label": "Weng, J.", "shape": "dot", "size": 25, "title": "Weng, J."}, {"color": "#66CCFF", "id": "Motion and structure", "label": "Motion and structure", "shape": "dot", "size": 25, "title": "Motion and structure"}, {"color": "#66CCFF", "id": "Grasp Laboratory", "label": "Grasp Laboratory", "shape": "dot", "size": 25, "title": "Grasp Laboratory"}, {"color": "#66CCFF", "id": "tron@seas.upenn.edu", "label": "tron@seas.upenn.edu", "shape": "dot", "size": 25, "title": "tron@seas.upenn.edu"}, {"color": "#66CCFF", "id": "Roberto Tron", "label": "Roberto Tron", "shape": "dot", "size": 25, "title": "Roberto Tron"}, {"color": "#66CCFF", "id": "kostas@cis.upenn.edu", "label": "kostas@cis.upenn.edu", "shape": "dot", "size": 25, "title": "kostas@cis.upenn.edu"}, {"color": "#66CCFF", "id": "Kostas Daniilidis", "label": "Kostas Daniilidis", "shape": "dot", "size": 25, "title": "Kostas Daniilidis"}, {"color": "#66CCFF", "id": "Fast 2D Border Ownership Assignment", "label": "Fast 2D Border Ownership Assignment", "shape": "dot", "size": 25, "title": "Fast 2D Border Ownership Assignment"}, {"color": "#66CCFF", "id": "Cornelia Ferm\u00fcller", "label": "Cornelia Ferm\u00fcller", "shape": "dot", "size": 25, "title": "Cornelia Ferm\u00fcller"}, {"color": "#66CCFF", "id": "Ching L. Teo", "label": "Ching L. Teo", "shape": "dot", "size": 25, "title": "Ching L. Teo"}, {"color": "#66CCFF", "id": "Yiannis Aloimonos", "label": "Yiannis Aloimonos", "shape": "dot", "size": 25, "title": "Yiannis Aloimonos"}, {"color": "#66CCFF", "id": "Teo_Fast_2D_Border_2015_CVPR_paper.pdf", "label": "Teo_Fast_2D_Border_2015_CVPR_paper.pdf", "shape": "dot", "size": 25, "title": "Teo_Fast_2D_Border_2015_CVPR_paper.pdf"}, {"color": "#66CCFF", "id": "PDF document", "label": "PDF document", "shape": "dot", "size": 25, "title": "PDF document"}, {"color": "#66CCFF", "id": "border ownership assignment", "label": "border ownership assignment", "shape": "dot", "size": 25, "title": "border ownership assignment"}, {"color": "#66CCFF", "id": "Structured Random Forests (SRF)", "label": "Structured Random Forests (SRF)", "shape": "dot", "size": 25, "title": "Structured Random Forests (SRF)"}, {"color": "#66CCFF", "id": "boundary detection", "label": "boundary detection", "shape": "dot", "size": 25, "title": "boundary detection"}, {"color": "#66CCFF", "id": "border ownership structure", "label": "border ownership structure", "shape": "dot", "size": 25, "title": "border ownership structure"}, {"color": "#66CCFF", "id": "shape descriptors", "label": "shape descriptors", "shape": "dot", "size": 25, "title": "shape descriptors"}, {"color": "#66CCFF", "id": "HoG-like descriptors", "label": "HoG-like descriptors", "shape": "dot", "size": 25, "title": "HoG-like descriptors"}, {"color": "#66CCFF", "id": "spectral properties", "label": "spectral properties", "shape": "dot", "size": 25, "title": "spectral properties"}, {"color": "#66CCFF", "id": "PCA", "label": "PCA", "shape": "dot", "size": 25, "title": "PCA"}, {"color": "#66CCFF", "id": "semi-global grouping cues", "label": "semi-global grouping cues", "shape": "dot", "size": 25, "title": "semi-global grouping cues"}, {"color": "#66CCFF", "id": "perceived depth", "label": "perceived depth", "shape": "dot", "size": 25, "title": "perceived depth"}, {"color": "#66CCFF", "id": "Berkeley Segmentation Dataset (BSDS)", "label": "Berkeley Segmentation Dataset (BSDS)", "shape": "dot", "size": 25, "title": "Berkeley Segmentation Dataset (BSDS)"}, {"color": "#66CCFF", "id": "NYU Depth V2 dataset", "label": "NYU Depth V2 dataset", "shape": "dot", "size": 25, "title": "NYU Depth V2 dataset"}, {"color": "#66CCFF", "id": "multi-stage approaches", "label": "multi-stage approaches", "shape": "dot", "size": 25, "title": "multi-stage approaches"}, {"color": "#66CCFF", "id": "Experimental results", "label": "Experimental results", "shape": "dot", "size": 25, "title": "Experimental results"}, {"color": "#66CCFF", "id": "Berkeley Segmentation Dataset", "label": "Berkeley Segmentation Dataset", "shape": "dot", "size": 25, "title": "Berkeley Segmentation Dataset"}, {"color": "#66CCFF", "id": "Feature Extraction", "label": "Feature Extraction", "shape": "dot", "size": 25, "title": "Feature Extraction"}, {"color": "#66CCFF", "id": "HoG", "label": "HoG", "shape": "dot", "size": 25, "title": "HoG"}, {"color": "#66CCFF", "id": "Hierarchical Image Segmentation", "label": "Hierarchical Image Segmentation", "shape": "dot", "size": 25, "title": "Hierarchical Image Segmentation"}, {"color": "#66CCFF", "id": "Cheng et al. (2014)", "label": "Cheng et al. (2014)", "shape": "dot", "size": 25, "title": "Cheng et al. (2014)"}, {"color": "#66CCFF", "id": "Bing", "label": "Bing", "shape": "dot", "size": 25, "title": "Bing"}, {"color": "#66CCFF", "id": "Dalal \u0026 Triggs (2005)", "label": "Dalal \u0026 Triggs (2005)", "shape": "dot", "size": 25, "title": "Dalal \u0026 Triggs (2005)"}, {"color": "#66CCFF", "id": "Histograms of oriented gradients", "label": "Histograms of oriented gradients", "shape": "dot", "size": 25, "title": "Histograms of oriented gradients"}, {"color": "#66CCFF", "id": "human detection", "label": "human detection", "shape": "dot", "size": 25, "title": "human detection"}, {"color": "#66CCFF", "id": "Binarized normed gradients", "label": "Binarized normed gradients", "shape": "dot", "size": 25, "title": "Binarized normed gradients"}, {"color": "#66CCFF", "id": "objectness estimation", "label": "objectness estimation", "shape": "dot", "size": 25, "title": "objectness estimation"}, {"color": "#66CCFF", "id": "300fps", "label": "300fps", "shape": "dot", "size": 25, "title": "300fps"}, {"color": "#66CCFF", "id": "Fast edge detection", "label": "Fast edge detection", "shape": "dot", "size": 25, "title": "Fast edge detection"}, {"color": "#66CCFF", "id": "structured forests", "label": "structured forests", "shape": "dot", "size": 25, "title": "structured forests"}, {"color": "#66CCFF", "id": "Fast feature pyramids", "label": "Fast feature pyramids", "shape": "dot", "size": 25, "title": "Fast feature pyramids"}, {"color": "#66CCFF", "id": "object detection", "label": "object detection", "shape": "dot", "size": 25, "title": "object detection"}, {"color": "#66CCFF", "id": "Category-independent object proposals", "label": "Category-independent object proposals", "shape": "dot", "size": 25, "title": "Category-independent object proposals"}, {"color": "#66CCFF", "id": "diverse ranking", "label": "diverse ranking", "shape": "dot", "size": 25, "title": "diverse ranking"}, {"color": "#66CCFF", "id": "Extremely randomized trees", "label": "Extremely randomized trees", "shape": "dot", "size": 25, "title": "Extremely randomized trees"}, {"color": "#66CCFF", "id": "machine learning algorithm", "label": "machine learning algorithm", "shape": "dot", "size": 25, "title": "machine learning algorithm"}, {"color": "#66CCFF", "id": "Perceptual organization", "label": "Perceptual organization", "shape": "dot", "size": 25, "title": "Perceptual organization"}, {"color": "#66CCFF", "id": "recognition of indoor scenes", "label": "recognition of indoor scenes", "shape": "dot", "size": 25, "title": "recognition of indoor scenes"}, {"color": "#66CCFF", "id": "Random decision forests", "label": "Random decision forests", "shape": "dot", "size": 25, "title": "Random decision forests"}, {"color": "#66CCFF", "id": "Computer Vision Lab", "label": "Computer Vision Lab", "shape": "dot", "size": 25, "title": "Computer Vision Lab"}, {"color": "#66CCFF", "id": "University of Maryland", "label": "University of Maryland", "shape": "dot", "size": 25, "title": "University of Maryland"}, {"color": "#66CCFF", "id": "College Park, MD", "label": "College Park, MD", "shape": "dot", "size": 25, "title": "College Park, MD"}, {"color": "#66CCFF", "id": "Computer Vision\u003c0xC2\u003e\u003c0xA0\u003eLab", "label": "Computer Vision\u003c0xC2\u003e\u003c0xA0\u003eLab", "shape": "dot", "size": 25, "title": "Computer Vision\u003c0xC2\u003e\u003c0xA0\u003eLab"}, {"color": "#66CCFF", "id": "Mostafa Abdelrahman", "label": "Mostafa Abdelrahman", "shape": "dot", "size": 25, "title": "Mostafa Abdelrahman"}, {"color": "#66CCFF", "id": "Heat Diffusion Over Weighted Manifolds", "label": "Heat Diffusion Over Weighted Manifolds", "shape": "dot", "size": 25, "title": "Heat Diffusion Over Weighted Manifolds"}, {"color": "#66CCFF", "id": "Aly Farag", "label": "Aly Farag", "shape": "dot", "size": 25, "title": "Aly Farag"}, {"color": "#66CCFF", "id": "David Swanson", "label": "David Swanson", "shape": "dot", "size": 25, "title": "David Swanson"}, {"color": "#66CCFF", "id": "Moumen T. El-Melegy", "label": "Moumen T. El-Melegy", "shape": "dot", "size": 25, "title": "Moumen T. El-Melegy"}, {"color": "#66CCFF", "id": "descriptor", "label": "descriptor", "shape": "dot", "size": 25, "title": "descriptor"}, {"color": "#66CCFF", "id": "Abdelrahman_Heat_Diffusion_Over_2015_CVPR_paper.pdf", "label": "Abdelrahman_Heat_Diffusion_Over_2015_CVPR_paper.pdf", "shape": "dot", "size": 25, "title": "Abdelrahman_Heat_Diffusion_Over_2015_CVPR_paper.pdf"}, {"color": "#66CCFF", "id": "photometric features", "label": "photometric features", "shape": "dot", "size": 25, "title": "photometric features"}, {"color": "#66CCFF", "id": "existing descriptors", "label": "existing descriptors", "shape": "dot", "size": 25, "title": "existing descriptors"}, {"color": "#66CCFF", "id": "geometric properties", "label": "geometric properties", "shape": "dot", "size": 25, "title": "geometric properties"}, {"color": "#66CCFF", "id": "topological properties", "label": "topological properties", "shape": "dot", "size": 25, "title": "topological properties"}, {"color": "#66CCFF", "id": "Weighted Heat Kernel Signature (W-HKS)", "label": "Weighted Heat Kernel Signature (W-HKS)", "shape": "dot", "size": 25, "title": "Weighted Heat Kernel Signature (W-HKS)"}, {"color": "#66CCFF", "id": "textured 3D non-rigid models", "label": "textured 3D non-rigid models", "shape": "dot", "size": 25, "title": "textured 3D non-rigid models"}, {"color": "#66CCFF", "id": "photometric information", "label": "photometric information", "shape": "dot", "size": 25, "title": "photometric information"}, {"color": "#66CCFF", "id": "shape manifold", "label": "shape manifold", "shape": "dot", "size": 25, "title": "shape manifold"}, {"color": "#66CCFF", "id": "new discretization method", "label": "new discretization method", "shape": "dot", "size": 25, "title": "new discretization method"}, {"color": "#66CCFF", "id": "discretization method", "label": "discretization method", "shape": "dot", "size": 25, "title": "discretization method"}, {"color": "#66CCFF", "id": "finite element approximation", "label": "finite element approximation", "shape": "dot", "size": 25, "title": "finite element approximation"}, {"color": "#66CCFF", "id": "weighted heat kernel signature", "label": "weighted heat kernel signature", "shape": "dot", "size": 25, "title": "weighted heat kernel signature"}, {"color": "#66CCFF", "id": "geometric information", "label": "geometric information", "shape": "dot", "size": 25, "title": "geometric information"}, {"color": "#66CCFF", "id": "method for scale invariance", "label": "method for scale invariance", "shape": "dot", "size": 25, "title": "method for scale invariance"}, {"color": "#66CCFF", "id": "heat kernel signature", "label": "heat kernel signature", "shape": "dot", "size": 25, "title": "heat kernel signature"}, {"color": "#66CCFF", "id": "scale invariance", "label": "scale invariance", "shape": "dot", "size": 25, "title": "scale invariance"}, {"color": "#66CCFF", "id": "high performance", "label": "high performance", "shape": "dot", "size": 25, "title": "high performance"}, {"color": "#66CCFF", "id": "challenges", "label": "challenges", "shape": "dot", "size": 25, "title": "challenges"}, {"color": "#66CCFF", "id": "pure geometric methods", "label": "pure geometric methods", "shape": "dot", "size": 25, "title": "pure geometric methods"}, {"color": "#66CCFF", "id": "pure photometric methods", "label": "pure photometric methods", "shape": "dot", "size": 25, "title": "pure photometric methods"}, {"color": "#66CCFF", "id": "experimental results", "label": "experimental results", "shape": "dot", "size": 25, "title": "experimental results"}, {"color": "#66CCFF", "id": "approach\u0027s performance", "label": "approach\u0027s performance", "shape": "dot", "size": 25, "title": "approach\u0027s performance"}, {"color": "#66CCFF", "id": "textured shape retrieval", "label": "textured shape retrieval", "shape": "dot", "size": 25, "title": "textured shape retrieval"}, {"color": "#66CCFF", "id": "Textured 3D Shape Retrieval", "label": "Textured 3D Shape Retrieval", "shape": "dot", "size": 25, "title": "Textured 3D Shape Retrieval"}, {"color": "#66CCFF", "id": "Challenges", "label": "Challenges", "shape": "dot", "size": 25, "title": "Challenges"}, {"color": "#66CCFF", "id": "Pure Geometric Methods", "label": "Pure Geometric Methods", "shape": "dot", "size": 25, "title": "Pure Geometric Methods"}, {"color": "#66CCFF", "id": "Photometric Shape Descriptors", "label": "Photometric Shape Descriptors", "shape": "dot", "size": 25, "title": "Photometric Shape Descriptors"}, {"color": "#66CCFF", "id": "Weighted Heat Kernel Signature", "label": "Weighted Heat Kernel Signature", "shape": "dot", "size": 25, "title": "Weighted Heat Kernel Signature"}, {"color": "#66CCFF", "id": "Heat Diffusion on Manifold", "label": "Heat Diffusion on Manifold", "shape": "dot", "size": 25, "title": "Heat Diffusion on Manifold"}, {"color": "#66CCFF", "id": "Electrical Engineering Department", "label": "Electrical Engineering Department", "shape": "dot", "size": 25, "title": "Electrical Engineering Department"}, {"color": "#66CCFF", "id": "Assiut University", "label": "Assiut University", "shape": "dot", "size": 25, "title": "Assiut University"}, {"color": "#66CCFF", "id": "CVIP Lab", "label": "CVIP Lab", "shape": "dot", "size": 25, "title": "CVIP Lab"}, {"color": "#66CCFF", "id": "University of Louisville", "label": "University of Louisville", "shape": "dot", "size": 25, "title": "University of Louisville"}, {"color": "#66CCFF", "id": "Department of Mathematics", "label": "Department of Mathematics", "shape": "dot", "size": 25, "title": "Department of Mathematics"}, {"color": "#66CCFF", "id": "Assiut", "label": "Assiut", "shape": "dot", "size": 25, "title": "Assiut"}, {"color": "#66CCFF", "id": "Si Liu", "label": "Si Liu", "shape": "dot", "size": 25, "title": "Si Liu"}, {"color": "#66CCFF", "id": "Matching-CNN Meets KNN", "label": "Matching-CNN Meets KNN", "shape": "dot", "size": 25, "title": "Matching-CNN Meets KNN"}, {"color": "#66CCFF", "id": "Xiaodan Liang", "label": "Xiaodan Liang", "shape": "dot", "size": 25, "title": "Xiaodan Liang"}, {"color": "#66CCFF", "id": "Luoqi Liu", "label": "Luoqi Liu", "shape": "dot", "size": 25, "title": "Luoqi Liu"}, {"color": "#66CCFF", "id": "Xiaohui Shen", "label": "Xiaohui Shen", "shape": "dot", "size": 25, "title": "Xiaohui Shen"}, {"color": "#66CCFF", "id": "Jianchao Yang", "label": "Jianchao Yang", "shape": "dot", "size": 25, "title": "Jianchao Yang"}, {"color": "#66CCFF", "id": "Changshen Xu", "label": "Changshen Xu", "shape": "dot", "size": 25, "title": "Changshen Xu"}, {"color": "#66CCFF", "id": "Liang Lin", "label": "Liang Lin", "shape": "dot", "size": 25, "title": "Liang Lin"}, {"color": "#66CCFF", "id": "Xiaochun Cao", "label": "Xiaochun Cao", "shape": "dot", "size": 25, "title": "Xiaochun Cao"}, {"color": "#66CCFF", "id": "Shuicheng Yan", "label": "Shuicheng Yan", "shape": "dot", "size": 25, "title": "Shuicheng Yan"}, {"color": "#66CCFF", "id": "Work", "label": "Work", "shape": "dot", "size": 25, "title": "Work"}, {"color": "#66CCFF", "id": "Solution", "label": "Solution", "shape": "dot", "size": 25, "title": "Solution"}, {"color": "#66CCFF", "id": "Human Parsing", "label": "Human Parsing", "shape": "dot", "size": 25, "title": "Human Parsing"}, {"color": "#66CCFF", "id": "Quasi-parametric Model", "label": "Quasi-parametric Model", "shape": "dot", "size": 25, "title": "Quasi-parametric Model"}, {"color": "#66CCFF", "id": "KNN Framework", "label": "KNN Framework", "shape": "dot", "size": 25, "title": "KNN Framework"}, {"color": "#66CCFF", "id": "M-CNN", "label": "M-CNN", "shape": "dot", "size": 25, "title": "M-CNN"}, {"color": "#66CCFF", "id": "Matching Confidence", "label": "Matching Confidence", "shape": "dot", "size": 25, "title": "Matching Confidence"}, {"color": "#66CCFF", "id": "Displacements", "label": "Displacements", "shape": "dot", "size": 25, "title": "Displacements"}, {"color": "#66CCFF", "id": "KNN Images", "label": "KNN Images", "shape": "dot", "size": 25, "title": "KNN Images"}, {"color": "#66CCFF", "id": "Semantic Regions", "label": "Semantic Regions", "shape": "dot", "size": 25, "title": "Semantic Regions"}, {"color": "#66CCFF", "id": "Matched Regions", "label": "Matched Regions", "shape": "dot", "size": 25, "title": "Matched Regions"}, {"color": "#66CCFF", "id": "Result", "label": "Result", "shape": "dot", "size": 25, "title": "Result"}, {"color": "#66CCFF", "id": "Evaluations", "label": "Evaluations", "shape": "dot", "size": 25, "title": "Evaluations"}, {"color": "#66CCFF", "id": "Performance Gains", "label": "Performance Gains", "shape": "dot", "size": 25, "title": "Performance Gains"}, {"color": "#66CCFF", "id": "superpixel smoothing", "label": "superpixel smoothing", "shape": "dot", "size": 25, "title": "superpixel smoothing"}, {"color": "#66CCFF", "id": "matched regions", "label": "matched regions", "shape": "dot", "size": 25, "title": "matched regions"}, {"color": "#66CCFF", "id": "performance", "label": "performance", "shape": "dot", "size": 25, "title": "performance"}, {"color": "#66CCFF", "id": "SKLOIs", "label": "SKLOIs", "shape": "dot", "size": 25, "title": "SKLOIs"}, {"color": "#66CCFF", "id": "IIE", "label": "IIE", "shape": "dot", "size": 25, "title": "IIE"}, {"color": "#66CCFF", "id": "National University of Singapore", "label": "National University of Singapore", "shape": "dot", "size": 25, "title": "National University of Singapore"}, {"color": "#66CCFF", "id": "Adobe Research", "label": "Adobe Research", "shape": "dot", "size": 25, "title": "Adobe Research"}, {"color": "#66CCFF", "id": "IA", "label": "IA", "shape": "dot", "size": 25, "title": "IA"}, {"color": "#66CCFF", "id": "Sun Yat-sen University", "label": "Sun Yat-sen University", "shape": "dot", "size": 25, "title": "Sun Yat-sen University"}, {"color": "#66CCFF", "id": "OIS", "label": "OIS", "shape": "dot", "size": 25, "title": "OIS"}, {"color": "#66CCFF", "id": "Yunsheng Jiang", "label": "Yunsheng Jiang", "shape": "dot", "size": 25, "title": "Yunsheng Jiang"}, {"color": "#66CCFF", "id": "Combination Features and Models for Human Detection", "label": "Combination Features and Models for Human Detection", "shape": "dot", "size": 25, "title": "Combination Features and Models for Human Detection"}, {"color": "#66CCFF", "id": "Jinwen Ma", "label": "Jinwen Ma", "shape": "dot", "size": 25, "title": "Jinwen Ma"}, {"color": "#66CCFF", "id": "Combination Features and Models for Human Description", "label": "Combination Features and Models for Human Description", "shape": "dot", "size": 25, "title": "Combination Features and Models for Human Description"}, {"color": "#66CCFF", "id": "Human Detection", "label": "Human Detection", "shape": "dot", "size": 25, "title": "Human Detection"}, {"color": "#66CCFF", "id": "Jiang_Combination_Features_and_2015_CVPR_paper.pdf", "label": "Jiang_Combination_Features_and_2015_CVPR_paper.pdf", "shape": "dot", "size": 25, "title": "Jiang_Combination_Features_and_2015_CVPR_paper.pdf"}, {"color": "#66CCFF", "id": "combination models", "label": "combination models", "shape": "dot", "size": 25, "title": "combination models"}, {"color": "#66CCFF", "id": "complementary features", "label": "complementary features", "shape": "dot", "size": 25, "title": "complementary features"}, {"color": "#66CCFF", "id": "existing features", "label": "existing features", "shape": "dot", "size": 25, "title": "existing features"}, {"color": "#66CCFF", "id": "biases", "label": "biases", "shape": "dot", "size": 25, "title": "biases"}, {"color": "#66CCFF", "id": "HOG-III features", "label": "HOG-III features", "shape": "dot", "size": 25, "title": "HOG-III features"}, {"color": "#66CCFF", "id": "color features", "label": "color features", "shape": "dot", "size": 25, "title": "color features"}, {"color": "#66CCFF", "id": "weighted-NMS fusion algorithm", "label": "weighted-NMS fusion algorithm", "shape": "dot", "size": 25, "title": "weighted-NMS fusion algorithm"}, {"color": "#66CCFF", "id": "approaches", "label": "approaches", "shape": "dot", "size": 25, "title": "approaches"}, {"color": "#66CCFF", "id": "detection performance", "label": "detection performance", "shape": "dot", "size": 25, "title": "detection performance"}, {"color": "#66CCFF", "id": "computational efficiency", "label": "computational efficiency", "shape": "dot", "size": 25, "title": "computational efficiency"}, {"color": "#66CCFF", "id": "experiments", "label": "experiments", "shape": "dot", "size": 25, "title": "experiments"}, {"color": "#66CCFF", "id": "PASCAL VOC datasets", "label": "PASCAL VOC datasets", "shape": "dot", "size": 25, "title": "PASCAL VOC datasets"}, {"color": "#66CCFF", "id": "Belongie et al. (2001)", "label": "Belongie et al. (2001)", "shape": "dot", "size": 25, "title": "Belongie et al. (2001)"}, {"color": "#66CCFF", "id": "IEEE Int\u0027l Conf. on Computer Vision (IC CV)", "label": "IEEE Int\u0027l Conf. on Computer Vision (IC CV)", "shape": "dot", "size": 25, "title": "IEEE Int\u0027l Conf. on Computer Vision (IC CV)"}, {"color": "#66CCFF", "id": "Hubel (1995)", "label": "Hubel (1995)", "shape": "dot", "size": 25, "title": "Hubel (1995)"}, {"color": "#66CCFF", "id": "Eye, brain, and vision", "label": "Eye, brain, and vision", "shape": "dot", "size": 25, "title": "Eye, brain, and vision"}, {"color": "#66CCFF", "id": "Ioffe \u0026 Forsyth (2001)", "label": "Ioffe \u0026 Forsyth (2001)", "shape": "dot", "size": 25, "title": "Ioffe \u0026 Forsyth (2001)"}, {"color": "#66CCFF", "id": "IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)", "label": "IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)", "shape": "dot", "size": 25, "title": "IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)"}, {"color": "#66CCFF", "id": "Dalal (2006)", "label": "Dalal (2006)", "shape": "dot", "size": 25, "title": "Dalal (2006)"}, {"color": "#66CCFF", "id": "Finding people in images and videos", "label": "Finding people in images and videos", "shape": "dot", "size": 25, "title": "Finding people in images and videos"}, {"color": "#66CCFF", "id": "Felzenszwalb et al. (2008)", "label": "Felzenszwalb et al. (2008)", "shape": "dot", "size": 25, "title": "Felzenszwalb et al. (2008)"}, {"color": "#66CCFF", "id": "HOG-III Features", "label": "HOG-III Features", "shape": "dot", "size": 25, "title": "HOG-III Features"}, {"color": "#66CCFF", "id": "Model Fusion", "label": "Model Fusion", "shape": "dot", "size": 25, "title": "Model Fusion"}, {"color": "#66CCFF", "id": "Weighted-NMS", "label": "Weighted-NMS", "shape": "dot", "size": 25, "title": "Weighted-NMS"}, {"color": "#66CCFF", "id": "Matching Shapes", "label": "Matching Shapes", "shape": "dot", "size": 25, "title": "Matching Shapes"}, {"color": "#66CCFF", "id": "elzenszwalb, P. F.", "label": "elzenszwalb, P. F.", "shape": "dot", "size": 25, "title": "elzenszwalb, P. F."}, {"color": "#66CCFF", "id": "A discriminatively trained, multiscale, deformable part model", "label": "A discriminatively trained, multiscale, deformable part model", "shape": "dot", "size": 25, "title": "A discriminatively trained, multiscale, deformable part model"}, {"color": "#66CCFF", "id": "Girshick, R. B.", "label": "Girshick, R. B.", "shape": "dot", "size": 25, "title": "Girshick, R. B."}, {"color": "#66CCFF", "id": "Viola, P.", "label": "Viola, P.", "shape": "dot", "size": 25, "title": "Viola, P."}, {"color": "#66CCFF", "id": "Robust real-time face detection", "label": "Robust real-time face detection", "shape": "dot", "size": 25, "title": "Robust real-time face detection"}, {"color": "#66CCFF", "id": "Object detection grammar", "label": "Object detection grammar", "shape": "dot", "size": 25, "title": "Object detection grammar"}, {"color": "#66CCFF", "id": "Object detection with grammar models", "label": "Object detection with grammar models", "shape": "dot", "size": 25, "title": "Object detection with grammar models"}, {"color": "#66CCFF", "id": "A discriminatively trained, mult scale, deformable part model", "label": "A discriminatively trained, mult scale, deformable part model", "shape": "dot", "size": 25, "title": "A discriminatively trained, mult scale, deformable part model"}, {"color": "#66CCFF", "id": "International Journal of Computer Vision", "label": "International Journal of Computer Vision", "shape": "dot", "size": 25, "title": "International Journal of Computer Vision"}, {"color": "#66CCFF", "id": "IEEE Int\u2019l Conf. on Computer Vision (ICCV) Workshops", "label": "IEEE Int\u2019l Conf. on Computer Vision (ICCV) Workshops", "shape": "dot", "size": 25, "title": "IEEE Int\u2019l Conf. on Computer Vision (ICCV) Workshops"}, {"color": "#66CCFF", "id": "Advances in Neural Information Processing Systems", "label": "Advances in Neural Information Processing Systems", "shape": "dot", "size": 25, "title": "Advances in Neural Information Processing Systems"}, {"color": "#66CCFF", "id": "Gkioxari, G.", "label": "Gkioxari, G.", "shape": "dot", "size": 25, "title": "Gkioxari, G."}, {"color": "#66CCFF", "id": "Using k-poselets for detecting people and localizing their keypoints", "label": "Using k-poselets for detecting people and localizing their keypoints", "shape": "dot", "size": 25, "title": "Using k-poselets for detecting people and localizing their keypoints"}, {"color": "#66CCFF", "id": "Hariharan, B.", "label": "Hariharan, B.", "shape": "dot", "size": 25, "title": "Hariharan, B."}, {"color": "#66CCFF", "id": "GkioxARI, G.", "label": "GkioxARI, G.", "shape": "dot", "size": 25, "title": "GkioxARI, G."}, {"color": "#66CCFF", "id": "Girshick, R.", "label": "Girshick, R.", "shape": "dot", "size": 25, "title": "Girshick, R."}, {"color": "#66CCFF", "id": "Malik, J.", "label": "Malik, J.", "shape": "dot", "size": 25, "title": "Malik, J."}, {"color": "#66CCFF", "id": "YunshEng Jiang", "label": "YunshEng Jiang", "shape": "dot", "size": 25, "title": "YunshEng Jiang"}, {"color": "#66CCFF", "id": "Peking University", "label": "Peking University", "shape": "dot", "size": 25, "title": "Peking University"}, {"color": "#66CCFF", "id": "Department of Information Science", "label": "Department of Information Science", "shape": "dot", "size": 25, "title": "Department of Information Science"}, {"color": "#66CCFF", "id": "Cheng", "label": "Cheng", "shape": "dot", "size": 25, "title": "Cheng"}, {"color": "#66CCFF", "id": "Effective Learning-Based Illuminant Estimation Using Simple Features", "label": "Effective Learning-Based Illuminant Estimation Using Simple Features", "shape": "dot", "size": 25, "title": "Effective Learning-Based Illuminant Estimation Using Simple Features"}, {"color": "#66CCFF", "id": "Price", "label": "Price", "shape": "dot", "size": 25, "title": "Price"}, {"color": "#66CCFF", "id": "Cohen", "label": "Cohen", "shape": "dot", "size": 25, "title": "Cohen"}, {"color": "#66CCFF", "id": "Michael S. Brown", "label": "Michael S. Brown", "shape": "dot", "size": 25, "title": "Michael S. Brown"}, {"color": "#66CCFF", "id": "Illumination estimation", "label": "Illumination estimation", "shape": "dot", "size": 25, "title": "Illumination estimation"}, {"color": "#66CCFF", "id": "determining chromaticity", "label": "determining chromaticity", "shape": "dot", "size": 25, "title": "determining chromaticity"}, {"color": "#66CCFF", "id": "white-balancing", "label": "white-balancing", "shape": "dot", "size": 25, "title": "white-balancing"}, {"color": "#66CCFF", "id": "computational color constancy", "label": "computational color constancy", "shape": "dot", "size": 25, "title": "computational color constancy"}, {"color": "#66CCFF", "id": "computer vision", "label": "computer vision", "shape": "dot", "size": 25, "title": "computer vision"}, {"color": "#66CCFF", "id": "problem", "label": "problem", "shape": "dot", "size": 25, "title": "problem"}, {"color": "#66CCFF", "id": "ill-posed", "label": "ill-posed", "shape": "dot", "size": 25, "title": "ill-posed"}, {"color": "#66CCFF", "id": "four simple color features", "label": "four simple color features", "shape": "dot", "size": 25, "title": "four simple color features"}, {"color": "#66CCFF", "id": "regression trees", "label": "regression trees", "shape": "dot", "size": 25, "title": "regression trees"}, {"color": "#66CCFF", "id": "existing learning-based methods", "label": "existing learning-based methods", "shape": "dot", "size": 25, "title": "existing learning-based methods"}, {"color": "#66CCFF", "id": "best results", "label": "best results", "shape": "dot", "size": 25, "title": "best results"}, {"color": "#66CCFF", "id": "results", "label": "results", "shape": "dot", "size": 25, "title": "results"}, {"color": "#66CCFF", "id": "color constancy data sets", "label": "color constancy data sets", "shape": "dot", "size": 25, "title": "color constancy data sets"}, {"color": "#66CCFF", "id": "our approach", "label": "our approach", "shape": "dot", "size": 25, "title": "our approach"}, {"color": "#66CCFF", "id": "modern color constancy data sets", "label": "modern color constancy data sets", "shape": "dot", "size": 25, "title": "modern color constancy data sets"}, {"color": "#66CCFF", "id": "Forsyth, D. A.", "label": "Forsyth, D. A.", "shape": "dot", "size": 25, "title": "Forsyth, D. A."}, {"color": "#66CCFF", "id": "novel algorithm", "label": "novel algorithm", "shape": "dot", "size": 25, "title": "novel algorithm"}, {"color": "#66CCFF", "id": "Bani\u0107, N.", "label": "Bani\u0107, N.", "shape": "dot", "size": 25, "title": "Bani\u0107, N."}, {"color": "#66CCFF", "id": "Color dog", "label": "Color dog", "shape": "dot", "size": 25, "title": "Color dog"}, {"color": "#66CCFF", "id": "global illumination estimation", "label": "global illumination estimation", "shape": "dot", "size": 25, "title": "global illumination estimation"}, {"color": "#66CCFF", "id": "Funt, B.", "label": "Funt, B.", "shape": "dot", "size": 25, "title": "Funt, B."}, {"color": "#66CCFF", "id": "support vector regression", "label": "support vector regression", "shape": "dot", "size": 25, "title": "support vector regression"}, {"color": "#66CCFF", "id": "illumination chromaticity", "label": "illumination chromaticity", "shape": "dot", "size": 25, "title": "illumination chromaticity"}, {"color": "#66CCFF", "id": "Gao, S.", "label": "Gao, S.", "shape": "dot", "size": 25, "title": "Gao, S."}, {"color": "#66CCFF", "id": "color constancy", "label": "color constancy", "shape": "dot", "size": 25, "title": "color constancy"}, {"color": "#66CCFF", "id": "local surface reflectance statistics", "label": "local surface reflectance statistics", "shape": "dot", "size": 25, "title": "local surface reflectance statistics"}, {"color": "#66CCFF", "id": "Learning-Based Methods", "label": "Learning-Based Methods", "shape": "dot", "size": 25, "title": "Learning-Based Methods"}, {"color": "#66CCFF", "id": "slower than", "label": "slower than", "shape": "dot", "size": 25, "title": "slower than"}, {"color": "#66CCFF", "id": "Color and Imaging Conference", "label": "Color and Imaging Conference", "shape": "dot", "size": 25, "title": "Color and Imaging Conference"}, {"color": "#66CCFF", "id": "via support vector regression", "label": "via support vector regression", "shape": "dot", "size": 25, "title": "via support vector regression"}, {"color": "#66CCFF", "id": "Ef\ufb01cient color constancy with local surface re\ufb02ectance statistics", "label": "Ef\ufb01cient color constancy with local surface re\ufb02ectance statistics", "shape": "dot", "size": 25, "title": "Ef\ufb01cient color constancy with local surface re\ufb02ectance statistics"}, {"color": "#66CCFF", "id": "Barnard, K.", "label": "Barnard, K.", "shape": "dot", "size": 25, "title": "Barnard, K."}, {"color": "#66CCFF", "id": "A comparison of computational color constancy algorithms", "label": "A comparison of computational color constancy algorithms", "shape": "dot", "size": 25, "title": "A comparison of computational color constancy algorithms"}, {"color": "#66CCFF", "id": "TIP", "label": "TIP", "shape": "dot", "size": 25, "title": "TIP"}, {"color": "#66CCFF", "id": "A data set for color research", "label": "A data set for color research", "shape": "dot", "size": 25, "title": "A data set for color research"}, {"color": "#66CCFF", "id": "Color Research \u0026 Application", "label": "Color Research \u0026 Application", "shape": "dot", "size": 25, "title": "Color Research \u0026 Application"}, {"color": "#66CCFF", "id": "Gehler, P. V.", "label": "Gehler, P. V.", "shape": "dot", "size": 25, "title": "Gehler, P. V."}, {"color": "#66CCFF", "id": "Bayesian color constancy revisited", "label": "Bayesian color constancy revisited", "shape": "dot", "size": 25, "title": "Bayesian color constancy revisited"}, {"color": "#66CCFF", "id": "Bianco, S.", "label": "Bianco, S.", "shape": "dot", "size": 25, "title": "Bianco, S."}, {"color": "#66CCFF", "id": "Improving color constancy using indoor - outdoor image classification", "label": "Improving color constancy using indoor - outdoor image classification", "shape": "dot", "size": 25, "title": "Improving color constancy using indoor - outdoor image classification"}, {"color": "#66CCFF", "id": "Automatic color constancy algorithm selection and combination", "label": "Automatic color constancy algorithm selection and combination", "shape": "dot", "size": 25, "title": "Automatic color constancy algorithm selection and combination"}, {"color": "#66CCFF", "id": "IEEE Transactions on Image Processing", "label": "IEEE Transactions on Image Processing", "shape": "dot", "size": 25, "title": "IEEE Transactions on Image Processing"}, {"color": "#66CCFF", "id": "Botev, Z.", "label": "Botev, Z.", "shape": "dot", "size": 25, "title": "Botev, Z."}, {"color": "#66CCFF", "id": "Kernel density estimation via diffusion", "label": "Kernel density estimation via diffusion", "shape": "dot", "size": 25, "title": "Kernel density estimation via diffusion"}, {"color": "#66CCFF", "id": "The Annals of Statistics", "label": "The Annals of Statistics", "shape": "dot", "size": 25, "title": "The Annals of Statistics"}, {"color": "#66CCFF", "id": "Dongliang Cheng", "label": "Dongliang Cheng", "shape": "dot", "size": 25, "title": "Dongliang Cheng"}, {"color": "#66CCFF", "id": "Brian Price", "label": "Brian Price", "shape": "dot", "size": 25, "title": "Brian Price"}, {"color": "#66CCFF", "id": "Scott Cohen", "label": "Scott Cohen", "shape": "dot", "size": 25, "title": "Scott Cohen"}, {"color": "#66CCFF", "id": "Hui Wu", "label": "Hui Wu", "shape": "dot", "size": 25, "title": "Hui Wu"}, {"color": "#66CCFF", "id": "Robust Regression on Image Manifolds for Ordered Label Denoising", "label": "Robust Regression on Image Manifolds for Ordered Label Denoising", "shape": "dot", "size": 25, "title": "Robust Regression on Image Manifolds for Ordered Label Denoising"}, {"color": "#66CCFF", "id": "Richard Souvenir", "label": "Richard Souvenir", "shape": "dot", "size": 25, "title": "Richard Souvenir"}, {"color": "#66CCFF", "id": "Robust Regression on Image Manifold for Ordered Label Denoising", "label": "Robust Regression on Image Manifold for Ordered Label Denoising", "shape": "dot", "size": 25, "title": "Robust Regression on Image Manifold for Ordered Label Denoising"}, {"color": "#66CCFF", "id": "Wu_Robust_Regression_on_2015_CVPR_supplemental.pdf", "label": "Wu_Robust_Regression_on_2015_CVPR_supplemental.pdf", "shape": "dot", "size": 25, "title": "Wu_Robust_Regression_on_2015_CVPR_supplemental.pdf"}, {"color": "#66CCFF", "id": "supplemental material", "label": "supplemental material", "shape": "dot", "size": 25, "title": "supplemental material"}, {"color": "#66CCFF", "id": "Wu_Robust_Regression_on_2015_CVPR_supplemental", "label": "Wu_Robust_Regression_on_2015_CVPR_supplemental", "shape": "dot", "size": 25, "title": "Wu_Robust_Regression_on_2015_CVPR_supplemental"}, {"color": "#66CCFF", "id": "Robust Regression", "label": "Robust Regression", "shape": "dot", "size": 25, "title": "Robust Regression"}, {"color": "#66CCFF", "id": "Wu_Robust_Reservation_on_2015_CVPR_supplemental", "label": "Wu_Robust_Reservation_on_2015_CVPR_supplemental", "shape": "dot", "size": 25, "title": "Wu_Robust_Reservation_on_2015_CVPR_supplemental"}, {"color": "#66CCFF", "id": "Ordered Label Denoising", "label": "Ordered Label Denoising", "shape": "dot", "size": 25, "title": "Ordered Label Denoising"}, {"color": "#66CCFF", "id": "Figures 1-4", "label": "Figures 1-4", "shape": "dot", "size": 25, "title": "Figures 1-4"}, {"color": "#66CCFF", "id": "RANSC", "label": "RANSC", "shape": "dot", "size": 25, "title": "RANSC"}, {"color": "#66CCFF", "id": "K-NN", "label": "K-NN", "shape": "dot", "size": 25, "title": "K-NN"}, {"color": "#66CCFF", "id": "RBFN", "label": "RBFN", "shape": "dot", "size": 25, "title": "RBFN"}, {"color": "#66CCFF", "id": "SVR", "label": "SVR", "shape": "dot", "size": 25, "title": "SVR"}, {"color": "#66CCFF", "id": "KSPCA", "label": "KSPCA", "shape": "dot", "size": 25, "title": "KSPCA"}, {"color": "#66CCFF", "id": "H3R", "label": "H3R", "shape": "dot", "size": 25, "title": "H3R"}, {"color": "#66CCFF", "id": "Statue Data Set", "label": "Statue Data Set", "shape": "dot", "size": 25, "title": "Statue Data Set"}, {"color": "#66CCFF", "id": "Image Manifolds", "label": "Image Manifolds", "shape": "dot", "size": 25, "title": "Image Manifolds"}, {"color": "#66CCFF", "id": "Face Pose Estimation", "label": "Face Pose Estimation", "shape": "dot", "size": 25, "title": "Face Pose Estimation"}, {"color": "#66CCFF", "id": "University of North Carolina at Charlotte", "label": "University of North Carolina at Charlotte", "shape": "dot", "size": 25, "title": "University of North Carolina at Charlotte"}, {"color": "#66CCFF", "id": "Y. Cheng", "label": "Y. Cheng", "shape": "dot", "size": 25, "title": "Y. Cheng"}, {"color": "#66CCFF", "id": "University of North Carolin", "label": "University of North Carolin", "shape": "dot", "size": 25, "title": "University of North Carolin"}, {"color": "#66CCFF", "id": "J. A. Lopez", "label": "J. A. Lopez", "shape": "dot", "size": 25, "title": "J. A. Lopez"}, {"color": "#66CCFF", "id": "O. Camps", "label": "O. Camps", "shape": "dot", "size": 25, "title": "O. Camps"}, {"color": "#66CCFF", "id": "M. Sznaier", "label": "M. Sznaier", "shape": "dot", "size": 25, "title": "M. Sznaier"}, {"color": "#66CCFF", "id": "A Convex Optimization Approach", "label": "A Convex Optimization Approach", "shape": "dot", "size": 25, "title": "A Convex Optimization Approach"}, {"color": "#66CCFF", "id": "Cheng_A_Convex_Optimization_2015_CVPR_paper", "label": "Cheng_A_Convex_Optimization_2015_CVPR_paper", "shape": "dot", "size": 25, "title": "Cheng_A_Convex_Optimization_2015_CVPR_paper"}, {"color": "#66CCFF", "id": "hwu13@uncc.edu", "label": "hwu13@uncc.edu", "shape": "dot", "size": 25, "title": "hwu13@uncc.edu"}, {"color": "#66CCFF", "id": "souvenir@uncc.edu", "label": "souvenir@uncc.edu", "shape": "dot", "size": 25, "title": "souvenir@uncc.edu"}, {"color": "#66CCFF", "id": "fundamental matrix estimation problem", "label": "fundamental matrix estimation problem", "shape": "dot", "size": 25, "title": "fundamental matrix estimation problem"}, {"color": "#66CCFF", "id": "framework", "label": "framework", "shape": "dot", "size": 25, "title": "framework"}, {"color": "#66CCFF", "id": "general nonconvex", "label": "general nonconvex", "shape": "dot", "size": 25, "title": "general nonconvex"}, {"color": "#66CCFF", "id": "rank-2 constraint", "label": "rank-2 constraint", "shape": "dot", "size": 25, "title": "rank-2 constraint"}, {"color": "#66CCFF", "id": "noise", "label": "noise", "shape": "dot", "size": 25, "title": "noise"}, {"color": "#66CCFF", "id": "nonconvex", "label": "nonconvex", "shape": "dot", "size": 25, "title": "nonconvex"}, {"color": "#66CCFF", "id": "sequence of convex semi-de\ufb01nite programs", "label": "sequence of convex semi-de\ufb01nite programs", "shape": "dot", "size": 25, "title": "sequence of convex semi-de\ufb01nite programs"}, {"color": "#66CCFF", "id": "algorithm", "label": "algorithm", "shape": "dot", "size": 25, "title": "algorithm"}, {"color": "#66CCFF", "id": "extensible", "label": "extensible", "shape": "dot", "size": 25, "title": "extensible"}, {"color": "#66CCFF", "id": "partially labeled correspondences", "label": "partially labeled correspondences", "shape": "dot", "size": 25, "title": "partially labeled correspondences"}, {"color": "#66CCFF", "id": "co-occurrence information", "label": "co-occurrence information", "shape": "dot", "size": 25, "title": "co-occurrence information"}, {"color": "#66CCFF", "id": "high percentage of outliers", "label": "high percentage of outliers", "shape": "dot", "size": 25, "title": "high percentage of outliers"}, {"color": "#66CCFF", "id": "optimization", "label": "optimization", "shape": "dot", "size": 25, "title": "optimization"}, {"color": "#66CCFF", "id": "robust", "label": "robust", "shape": "dot", "size": 25, "title": "robust"}, {"color": "#66CCFF", "id": "Multiple view geometry", "label": "Multiple view geometry", "shape": "dot", "size": 25, "title": "Multiple view geometry"}, {"color": "#66CCFF", "id": "topic", "label": "topic", "shape": "dot", "size": 25, "title": "topic"}, {"color": "#66CCFF", "id": "Hartley, R. \u0026 Zisserman, A.", "label": "Hartley, R. \u0026 Zisserman, A.", "shape": "dot", "size": 25, "title": "Hartley, R. \u0026 Zisserman, A."}, {"color": "#66CCFF", "id": "Mohan, K. \u0026 Fazel, M.", "label": "Mohan, K. \u0026 Fazel, M.", "shape": "dot", "size": 25, "title": "Mohan, K. \u0026 Fazel, M."}, {"color": "#66CCFF", "id": "Iterative reweighted algorithms", "label": "Iterative reweighted algorithms", "shape": "dot", "size": 25, "title": "Iterative reweighted algorithms"}, {"color": "#66CCFF", "id": "Lasserre, J. B.", "label": "Lasserre, J. B.", "shape": "dot", "size": 25, "title": "Lasserre, J. B."}, {"color": "#66CCFF", "id": "Global optimization with polynomials", "label": "Global optimization with polynomials", "shape": "dot", "size": 25, "title": "Global optimization with polynomials"}, {"color": "#66CCFF", "id": "polynomial optimization methods", "label": "polynomial optimization methods", "shape": "dot", "size": 25, "title": "polynomial optimization methods"}, {"color": "#66CCFF", "id": "outliers", "label": "outliers", "shape": "dot", "size": 25, "title": "outliers"}, {"color": "#66CCFF", "id": "Method\u0027s effectiveness", "label": "Method\u0027s effectiveness", "shape": "dot", "size": 25, "title": "Method\u0027s effectiveness"}, {"color": "#66CCFF", "id": "Fundamental Matrix Estimation", "label": "Fundamental Matrix Estimation", "shape": "dot", "size": 25, "title": "Fundamental Matrix Estimation"}, {"color": "#66CCFF", "id": "Robust Optimization", "label": "Robust Optimization", "shape": "dot", "size": 25, "title": "Robust Optimization"}, {"color": "#66CCFF", "id": "Rank-Constrained Optimization", "label": "Rank-Constrained Optimization", "shape": "dot", "size": 25, "title": "Rank-Constrained Optimization"}, {"color": "#66CCFF", "id": "Global optimization", "label": "Global optimization", "shape": "dot", "size": 25, "title": "Global optimization"}, {"color": "#66CCFF", "id": "polynomial methods", "label": "polynomial methods", "shape": "dot", "size": 25, "title": "polynomial methods"}, {"color": "#66CCFF", "id": "optimization methods", "label": "optimization methods", "shape": "dot", "size": 25, "title": "optimization methods"}, {"color": "#66CCFF", "id": "Lasserre", "label": "Lasserre", "shape": "dot", "size": 25, "title": "Lasserre"}, {"color": "#66CCFF", "id": "Global optimization with polynomials and the problem of moments", "label": "Global optimization with polynomials and the problem of moments", "shape": "dot", "size": 25, "title": "Global optimization with polynomials and the problem of moments"}, {"color": "#66CCFF", "id": "Bugarin et al.", "label": "Bugarin et al.", "shape": "dot", "size": 25, "title": "Bugarin et al."}, {"color": "#66CCFF", "id": "polynomial global optimization", "label": "polynomial global optimization", "shape": "dot", "size": 25, "title": "polynomial global optimization"}, {"color": "#66CCFF", "id": "fundamental matrix estimation", "label": "fundamental matrix estimation", "shape": "dot", "size": 25, "title": "fundamental matrix estimation"}, {"color": "#66CCFF", "id": "eight-point algorithm", "label": "eight-point algorithm", "shape": "dot", "size": 25, "title": "eight-point algorithm"}, {"color": "#66CCFF", "id": "Torr \u0026 Murray", "label": "Torr \u0026 Murray", "shape": "dot", "size": 25, "title": "Torr \u0026 Murray"}, {"color": "#66CCFF", "id": "fundamental matrix estimation methods", "label": "fundamental matrix estimation methods", "shape": "dot", "size": 25, "title": "fundamental matrix estimation methods"}, {"color": "#66CCFF", "id": "computer vision technique", "label": "computer vision technique", "shape": "dot", "size": 25, "title": "computer vision technique"}, {"color": "#66CCFF", "id": "International journal of computer vision", "label": "International journal of computer vision", "shape": "dot", "size": 25, "title": "International journal of computer vision"}, {"color": "#66CCFF", "id": "Computer Vision and Pattern Recognition (CVPR)", "label": "Computer Vision and Pattern Recognition (CVPR)", "shape": "dot", "size": 25, "title": "Computer Vision and Pattern Recognition (CVPR)"}, {"color": "#66CCFF", "id": "Mlesac", "label": "Mlesac", "shape": "dot", "size": 25, "title": "Mlesac"}, {"color": "#66CCFF", "id": "robust estimator", "label": "robust estimator", "shape": "dot", "size": 25, "title": "robust estimator"}, {"color": "#66CCFF", "id": "Computer Vision and Image Understanding", "label": "Computer Vision and Image Understanding", "shape": "dot", "size": 25, "title": "Computer Vision and Image Understanding"}, {"color": "#66CCFF", "id": "SDP relaxations", "label": "SDP relaxations", "shape": "dot", "size": 25, "title": "SDP relaxations"}, {"color": "#66CCFF", "id": "SIAM Journal on Optimization", "label": "SIAM Journal on Optimization", "shape": "dot", "size": 25, "title": "SIAM Journal on Optimization"}, {"color": "#66CCFF", "id": "Zheng, Y., Sugimoto, S., \u0026 Okutomi, M.", "label": "Zheng, Y., Sugimoto, S., \u0026 Okutomi, M.", "shape": "dot", "size": 25, "title": "Zheng, Y., Sugimoto, S., \u0026 Okutomi, M."}, {"color": "#66CCFF", "id": "fundamental matrix estimation method", "label": "fundamental matrix estimation method", "shape": "dot", "size": 25, "title": "fundamental matrix estimation method"}, {"color": "#66CCFF", "id": "Lasserre (2006)", "label": "Lasserre (2006)", "shape": "dot", "size": 25, "title": "Lasserre (2006)"}, {"color": "#66CCFF", "id": "Sugaya \u0026 Kanatani (2007)", "label": "Sugaya \u0026 Kanatani (2007)", "shape": "dot", "size": 25, "title": "Sugaya \u0026 Kanatani (2007)"}, {"color": "#66CCFF", "id": "high-accuracy computation", "label": "high-accuracy computation", "shape": "dot", "size": 25, "title": "high-accuracy computation"}, {"color": "#66CCFF", "id": "Northeastern University", "label": "Northeastern University", "shape": "dot", "size": 25, "title": "Northeastern University"}, {"color": "#66CCFF", "id": "polynomial optimization", "label": "polynomial optimization", "shape": "dot", "size": 25, "title": "polynomial optimization"}, {"color": "#66CCFF", "id": "RANSA", "label": "RANSA", "shape": "dot", "size": 25, "title": "RANSA"}, {"color": "#66CCFF", "id": "image analysis", "label": "image analysis", "shape": "dot", "size": 25, "title": "image analysis"}, {"color": "#66CCFF", "id": "University", "label": "University", "shape": "dot", "size": 25, "title": "University"}, {"color": "#66CCFF", "id": "Boston", "label": "Boston", "shape": "dot", "size": 25, "title": "Boston"}, {"color": "#66CCFF", "id": "Massachusetts", "label": "Massachusetts", "shape": "dot", "size": 25, "title": "Massachusetts"}, {"color": "#66CCFF", "id": "Jian Sun", "label": "Jian Sun", "shape": "dot", "size": 25, "title": "Jian Sun"}, {"color": "#66CCFF", "id": "Learning a Convolutional Neural Network", "label": "Learning a Convolutional Neural Network", "shape": "dot", "size": 25, "title": "Learning a Convolutional Neural Network"}, {"color": "#66CCFF", "id": "Wenfei Cao", "label": "Wenfei Cao", "shape": "dot", "size": 25, "title": "Wenfei Cao"}, {"color": "#66CCFF", "id": "Jean Ponce", "label": "Jean Ponce", "shape": "dot", "size": 25, "title": "Jean Ponce"}, {"color": "#66CCFF", "id": "Non-uniform Motion Blur Removal", "label": "Non-uniform Motion Blur Removal", "shape": "dot", "size": 25, "title": "Non-uniform Motion Blur Removal"}, {"color": "#66CCFF", "id": "lopez.jo@husky.neu.edu", "label": "lopez.jo@husky.neu.edu", "shape": "dot", "size": 25, "title": "lopez.jo@husky.neu.edu"}, {"color": "#66CCFF", "id": "camps@coe.neu.edu", "label": "camps@coe.neu.edu", "shape": "dot", "size": 25, "title": "camps@coe.neu.edu"}, {"color": "#66CCFF", "id": "msznaier@coe.neu.edu", "label": "msznaier@coe.neu.edu", "shape": "dot", "size": 25, "title": "msznaier@coe.neu.edu"}, {"color": "#66CCFF", "id": "problem of estimating and removing non-uniform motion blur", "label": "problem of estimating and removing non-uniform motion blur", "shape": "dot", "size": 25, "title": "problem of estimating and removing non-uniform motion blur"}, {"color": "#66CCFF", "id": "authors", "label": "authors", "shape": "dot", "size": 25, "title": "authors"}, {"color": "#66CCFF", "id": "CNN", "label": "CNN", "shape": "dot", "size": 25, "title": "CNN"}, {"color": "#66CCFF", "id": "convolutional neural network", "label": "convolutional neural network", "shape": "dot", "size": 25, "title": "convolutional neural network"}, {"color": "#66CCFF", "id": "motion kernels", "label": "motion kernels", "shape": "dot", "size": 25, "title": "motion kernels"}, {"color": "#66CCFF", "id": "image rotations", "label": "image rotations", "shape": "dot", "size": 25, "title": "image rotations"}, {"color": "#66CCFF", "id": "Markov random field model", "label": "Markov random field model", "shape": "dot", "size": 25, "title": "Markov random field model"}, {"color": "#66CCFF", "id": "inferring dense non-uniform motion blur field", "label": "inferring dense non-uniform motion blur field", "shape": "dot", "size": 25, "title": "inferring dense non-uniform motion blur field"}, {"color": "#66CCFF", "id": "motion smoothness", "label": "motion smoothness", "shape": "dot", "size": 25, "title": "motion smoothness"}, {"color": "#66CCFF", "id": "deblurring model", "label": "deblurring model", "shape": "dot", "size": 25, "title": "deblurring model"}, {"color": "#66CCFF", "id": "motion blur", "label": "motion blur", "shape": "dot", "size": 25, "title": "motion blur"}, {"color": "#66CCFF", "id": "patch-level image prior", "label": "patch-level image prior", "shape": "dot", "size": 25, "title": "patch-level image prior"}, {"color": "#66CCFF", "id": "complex non-uniform motion blur", "label": "complex non-uniform motion blur", "shape": "dot", "size": 25, "title": "complex non-uniform motion blur"}, {"color": "#66CCFF", "id": "ion blur", "label": "ion blur", "shape": "dot", "size": 25, "title": "ion blur"}, {"color": "#66CCFF", "id": "non-uniform model", "label": "non-uniform model", "shape": "dot", "size": 25, "title": "non-uniform model"}, {"color": "#66CCFF", "id": "non-uniform motion blur", "label": "non-uniform motion blur", "shape": "dot", "size": 25, "title": "non-uniform motion blur"}, {"color": "#66CCFF", "id": "patch-based image processing", "label": "patch-based image processing", "shape": "dot", "size": 25, "title": "patch-based image processing"}, {"color": "#66CCFF", "id": "image artifact", "label": "image artifact", "shape": "dot", "size": 25, "title": "image artifact"}, {"color": "#66CCFF", "id": "image quality", "label": "image quality", "shape": "dot", "size": 25, "title": "image quality"}, {"color": "#66CCFF", "id": "Object detection systems", "label": "Object detection systems", "shape": "dot", "size": 25, "title": "Object detection systems"}, {"color": "#66CCFF", "id": "large number of classes", "label": "large number of classes", "shape": "dot", "size": 25, "title": "large number of classes"}, {"color": "#66CCFF", "id": "extensive convolution operations", "label": "extensive convolution operations", "shape": "dot", "size": 25, "title": "extensive convolution operations"}, {"color": "#66CCFF", "id": "long detection times", "label": "long detection times", "shape": "dot", "size": 25, "title": "long detection times"}, {"color": "#66CCFF", "id": "sparse coding methods", "label": "sparse coding methods", "shape": "dot", "size": 25, "title": "sparse coding methods"}, {"color": "#66CCFF", "id": "Regularized Sparse Coding", "label": "Regularized Sparse Coding", "shape": "dot", "size": 25, "title": "Regularized Sparse Coding"}, {"color": "#66CCFF", "id": "filter functionality reconstruction", "label": "filter functionality reconstruction", "shape": "dot", "size": 25, "title": "filter functionality reconstruction"}, {"color": "#66CCFF", "id": "score map error", "label": "score map error", "shape": "dot", "size": 25, "title": "score map error"}, {"color": "#66CCFF", "id": "16x speedup", "label": "16x speedup", "shape": "dot", "size": 25, "title": "16x speedup"}, {"color": "#66CCFF", "id": "ILSVIRC 2013", "label": "ILSVIRC 2013", "shape": "dot", "size": 25, "title": "ILSVIRC 2013"}, {"color": "#66CCFF", "id": "0.04 mAP drop", "label": "0.04 mAP drop", "shape": "dot", "size": 25, "title": "0.04 mAP drop"}, {"color": "#66CCFF", "id": "Deformable Part Model", "label": "Deformable Part Model", "shape": "dot", "size": 25, "title": "Deformable Part Model"}, {"color": "#66CCFF", "id": "parallel computing", "label": "parallel computing", "shape": "dot", "size": 25, "title": "parallel computing"}, {"color": "#66CCFF", "id": "GPUs", "label": "GPUs", "shape": "dot", "size": 25, "title": "GPUs"}, {"color": "#66CCFF", "id": "Ting-Hsuan Chao", "label": "Ting-Hsuan Chao", "shape": "dot", "size": 25, "title": "Ting-Hsuan Chao"}, {"color": "#66CCFF", "id": "National Taiwan University", "label": "National Taiwan University", "shape": "dot", "size": 25, "title": "National Taiwan University"}, {"color": "#66CCFF", "id": "Yen-Liang Lin", "label": "Yen-Liang Lin", "shape": "dot", "size": 25, "title": "Yen-Liang Lin"}, {"color": "#66CCFF", "id": "Yin-Hsi Kuo", "label": "Yin-Hsi Kuo", "shape": "dot", "size": 25, "title": "Yin-Hsi Kuo"}, {"color": "#66CCFF", "id": "Winston H. Hsu", "label": "Winston H. Hsu", "shape": "dot", "size": 25, "title": "Winston H. Hsu"}, {"color": "#66CCFF", "id": "Xiao-Ming Wu", "label": "Xiao-Ming Wu", "shape": "dot", "size": 25, "title": "Xiao-Ming Wu"}, {"color": "#66CCFF", "id": "author", "label": "author", "shape": "dot", "size": 25, "title": "author"}, {"color": "#66CCFF", "id": "Zhenguo Li", "label": "Zhenguo Li", "shape": "dot", "size": 25, "title": "Zhenguo Li"}, {"color": "#66CCFF", "id": "Shih-Fu Chang", "label": "Shih-Fu Chang", "shape": "dot", "size": 25, "title": "Shih-Fu Chang"}, {"color": "#66CCFF", "id": "X.-M. Wu", "label": "X.-M. Wu", "shape": "dot", "size": 25, "title": "X.-M. Wu"}, {"color": "#66CCFF", "id": "Department of Electrical Engineering, Columbia University", "label": "Department of Electrical Engineering, Columbia University", "shape": "dot", "size": 25, "title": "Department of Electrical Engineering, Columbia University"}, {"color": "#66CCFF", "id": "Huawei Noah\u2019s Ark Lab, Hong Kong", "label": "Huawei Noah\u2019s Ark Lab, Hong Kong", "shape": "dot", "size": 25, "title": "Huawei Noah\u2019s Ark Lab, Hong Kong"}, {"color": "#66CCFF", "id": "xmwu@ee.columbia.edu", "label": "xmwu@ee.columbia.edu", "shape": "dot", "size": 25, "title": "xmwu@ee.columbia.edu"}, {"color": "#66CCFF", "id": "li.zhenguo@huawei.com", "label": "li.zhenguo@huawei.com", "shape": "dot", "size": 25, "title": "li.zhenguo@huawei.com"}, {"color": "#66CCFF", "id": "sfchang@ee.columbia.edu", "label": "sfchang@ee.columbia.edu", "shape": "dot", "size": 25, "title": "sfchang@ee.columbia.edu"}, {"color": "#66CCFF", "id": "Analyzing the harmonic structure in graph-based learning", "label": "Analyzing the harmonic structure in graph-based learning", "shape": "dot", "size": 25, "title": "Analyzing the harmonic structure in graph-based learning"}, {"color": "#66CCFF", "id": "New insights into laplacian similarity search", "label": "New insights into laplacian similarity search", "shape": "dot", "size": 25, "title": "New insights into laplacian similarity search"}, {"color": "#66CCFF", "id": "Xuan Dong", "label": "Xuan Dong", "shape": "dot", "size": 25, "title": "Xuan Dong"}, {"color": "#66CCFF", "id": "Region-based Temporally Consistent Video Post-processing", "label": "Region-based Temporally Consistent Video Post-processing", "shape": "dot", "size": 25, "title": "Region-based Temporally Consistent Video Post-processing"}, {"color": "#66CCFF", "id": "Boyan Bonev", "label": "Boyan Bonev", "shape": "dot", "size": 25, "title": "Boyan Bonev"}, {"color": "#66CCFF", "id": "Region-based Temporately Consistent Video Post-processing", "label": "Region-based Temporately Consistent Video Post-processing", "shape": "dot", "size": 25, "title": "Region-based Temporately Consistent Video Post-processing"}, {"color": "#66CCFF", "id": "Yu Zhu", "label": "Yu Zhu", "shape": "dot", "size": 25, "title": "Yu Zhu"}, {"color": "#66CCFF", "id": "Alan L. Yuille", "label": "Alan L. Yuille", "shape": "dot", "size": 25, "title": "Alan L. Yuille"}, {"color": "#66CCFF", "id": "Region-based Temporically Consistent Video Post-processing", "label": "Region-based Temporically Consistent Video Post-processing", "shape": "dot", "size": 25, "title": "Region-based Temporically Consistent Video Post-processing"}, {"color": "#66CCFF", "id": "Columbia University", "label": "Columbia University", "shape": "dot", "size": 25, "title": "Columbia University"}, {"color": "#66CCFF", "id": "Department of Electrical Engineering", "label": "Department of Electrical Engineering", "shape": "dot", "size": 25, "title": "Department of Electrical Engineering"}, {"color": "#66CCFF", "id": "Dong_Region-Based_Temporally_Consistent_2015_CVPR_paper.pdf", "label": "Dong_Region-Based_Temporally_Consistent_2015_CVPR_paper.pdf", "shape": "dot", "size": 25, "title": "Dong_Region-Based_Temporally_Consistent_2015_CVPR_paper.pdf"}, {"color": "#66CCFF", "id": "temporally consistent video post-processing", "label": "temporally consistent video post-processing", "shape": "dot", "size": 25, "title": "temporally consistent video post-processing"}, {"color": "#66CCFF", "id": "goal", "label": "goal", "shape": "dot", "size": 25, "title": "goal"}, {"color": "#66CCFF", "id": "fidelity", "label": "fidelity", "shape": "dot", "size": 25, "title": "fidelity"}, {"color": "#66CCFF", "id": "enhancement algorithms", "label": "enhancement algorithms", "shape": "dot", "size": 25, "title": "enhancement algorithms"}, {"color": "#66CCFF", "id": "spatially consistent prior", "label": "spatially consistent prior", "shape": "dot", "size": 25, "title": "spatially consistent prior"}, {"color": "#66CCFF", "id": "pixels with same RGB values", "label": "pixels with same RGB values", "shape": "dot", "size": 25, "title": "pixels with same RGB values"}, {"color": "#66CCFF", "id": "frame", "label": "frame", "shape": "dot", "size": 25, "title": "frame"}, {"color": "#66CCFF", "id": "enhancement of regions", "label": "enhancement of regions", "shape": "dot", "size": 25, "title": "enhancement of regions"}, {"color": "#66CCFF", "id": "temporal consistency", "label": "temporal consistency", "shape": "dot", "size": 25, "title": "temporal consistency"}, {"color": "#66CCFF", "id": "spatial consistency", "label": "spatial consistency", "shape": "dot", "size": 25, "title": "spatial consistency"}, {"color": "#66CCFF", "id": "high fidelity", "label": "high fidelity", "shape": "dot", "size": 25, "title": "high fidelity"}, {"color": "#66CCFF", "id": "original enhancement algorithms are unknown", "label": "original enhancement algorithms are unknown", "shape": "dot", "size": 25, "title": "original enhancement algorithms are unknown"}, {"color": "#66CCFF", "id": "original enhancement algorithms are inaccessible", "label": "original enhancement algorithms are inaccessible", "shape": "dot", "size": 25, "title": "original enhancement algorithms are inaccessible"}, {"color": "#66CCFF", "id": "frames", "label": "frames", "shape": "dot", "size": 25, "title": "frames"}, {"color": "#66CCFF", "id": "Slic superpixels", "label": "Slic superpixels", "shape": "dot", "size": 25, "title": "Slic superpixels"}, {"color": "#66CCFF", "id": "superpixel methods", "label": "superpixel methods", "shape": "dot", "size": 25, "title": "superpixel methods"}, {"color": "#66CCFF", "id": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "label": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "shape": "dot", "size": 25, "title": "IEEE Transactions on Pattern Analysis and Machine Intelligence"}, {"color": "#66CCFF", "id": "tone management", "label": "tone management", "shape": "dot", "size": 25, "title": "tone management"}, {"color": "#66CCFF", "id": "ACM Trans. on Graph.", "label": "ACM Trans. on Graph.", "shape": "dot", "size": 25, "title": "ACM Trans. on Graph."}, {"color": "#66CCFF", "id": "video color grading", "label": "video color grading", "shape": "dot", "size": 25, "title": "video color grading"}, {"color": "#66CCFF", "id": "color transformation", "label": "color transformation", "shape": "dot", "size": 25, "title": "color transformation"}, {"color": "#66CCFF", "id": "Image Enhancement Algorithms", "label": "Image Enhancement Algorithms", "shape": "dot", "size": 25, "title": "Image Enhancement Algorithms"}, {"color": "#66CCFF", "id": "Y. Chang", "label": "Y. Chang", "shape": "dot", "size": 25, "title": "Y. Chang"}, {"color": "#66CCFF", "id": "Example-based color transformation of image and video using basic color categories", "label": "Example-based color transformation of image and video using basic color categories", "shape": "dot", "size": 25, "title": "Example-based color transformation of image and video using basic color categories"}, {"color": "#66CCFF", "id": "S. Saito", "label": "S. Saito", "shape": "dot", "size": 25, "title": "S. Saito"}, {"color": "#66CCFF", "id": "M. Nakajima", "label": "M. Nakajima", "shape": "dot", "size": 25, "title": "M. Nakajima"}, {"color": "#66CCFF", "id": "16", "label": "16", "shape": "dot", "size": 25, "title": "16"}, {"color": "#66CCFF", "id": "article", "label": "article", "shape": "dot", "size": 25, "title": "article"}, {"color": "#66CCFF", "id": "1\u201311", "label": "1\u201311", "shape": "dot", "size": 25, "title": "1\u201311"}, {"color": "#66CCFF", "id": "2013", "label": "2013", "shape": "dot", "size": 25, "title": "2013"}, {"color": "#66CCFF", "id": "Example-based color transformation", "label": "Example-based color transformation", "shape": "dot", "size": 25, "title": "Example-based color transformation"}, {"color": "#66CCFF", "id": "Example-based color transform", "label": "Example-based color transform", "shape": "dot", "size": 25, "title": "Example-based color transform"}, {"color": "#66CCFF", "id": "Z. Farbman", "label": "Z. Farbman", "shape": "dot", "size": 25, "title": "Z. Farbman"}, {"color": "#66CCFF", "id": "Tonal stabilization of video", "label": "Tonal stabilization of video", "shape": "dot", "size": 25, "title": "Tonal stabilization of video"}, {"color": "#66CCFF", "id": "D. Lischinski", "label": "D. Lischinski", "shape": "dot", "size": 25, "title": "D. Lischinski"}, {"color": "#66CCFF", "id": "P. F. Felzenszwalb", "label": "P. F. Felzenszwalb", "shape": "dot", "size": 25, "title": "P. F. Felzenszwalb"}, {"color": "#66CCFF", "id": "Efficient belief propagation", "label": "Efficient belief propagation", "shape": "dot", "size": 25, "title": "Efficient belief propagation"}, {"color": "#66CCFF", "id": "D. P. Huttenlocher", "label": "D. P. Huttenlocher", "shape": "dot", "size": 25, "title": "D. P. Huttenlocher"}, {"color": "#66CCFF", "id": "M. Grundmann", "label": "M. Grundmann", "shape": "dot", "size": 25, "title": "M. Grundmann"}, {"color": "#66CCFF", "id": "Post-processing approach", "label": "Post-processing approach", "shape": "dot", "size": 25, "title": "Post-processing approach"}, {"color": "#66CCFF", "id": "Y. Hacohen", "label": "Y. Hacohen", "shape": "dot", "size": 25, "title": "Y. Hacohen"}, {"color": "#66CCFF", "id": "Non-rigid dense correspondence", "label": "Non-rigid dense correspondence", "shape": "dot", "size": 25, "title": "Non-rigid dense correspondence"}, {"color": "#66CCFF", "id": "N. K. Kalantari", "label": "N. K. Kalantari", "shape": "dot", "size": 25, "title": "N. K. Kalantari"}, {"color": "#66CCFF", "id": "Patch-based high dynamic range video", "label": "Patch-based high dynamic range video", "shape": "dot", "size": 25, "title": "Patch-based high dynamic range video"}, {"color": "#66CCFF", "id": "age enhancement", "label": "age enhancement", "shape": "dot", "size": 25, "title": "age enhancement"}, {"color": "#66CCFF", "id": "ACM Trans. Graph.", "label": "ACM Trans. Graph.", "shape": "dot", "size": 25, "title": "ACM Trans. Graph."}, {"color": "#66CCFF", "id": "N. K. Kalantria", "label": "N. K. Kalantria", "shape": "dot", "size": 25, "title": "N. K. Kalantria"}, {"color": "#66CCFF", "id": "S. B. Kang", "label": "S. B. Kang", "shape": "dot", "size": 25, "title": "S. B. Kang"}, {"color": "#66CCFF", "id": "High dynamic range video", "label": "High dynamic range video", "shape": "dot", "size": 25, "title": "High dynamic range video"}, {"color": "#66CCFF", "id": "Tsinghua University", "label": "Tsinghua University", "shape": "dot", "size": 25, "title": "Tsinghua University"}, {"color": "#66CCFF", "id": "UC Los Angeles", "label": "UC Los Angeles", "shape": "dot", "size": 25, "title": "UC Los Angeles"}, {"color": "#66CCFF", "id": "Northwestern Polytechnical University", "label": "Northwestern Polytechnical University", "shape": "dot", "size": 25, "title": "Northwestern Polytechnical University"}, {"color": "#66CCFF", "id": "Lionel Gueguen", "label": "Lionel Gueguen", "shape": "dot", "size": 25, "title": "Lionel Gueguen"}, {"color": "#66CCFF", "id": "Large-Scale Damage Detection Using Satellite Imagery", "label": "Large-Scale Damage Detection Using Satellite Imagery", "shape": "dot", "size": 25, "title": "Large-Scale Damage Detection Using Satellite Imagery"}, {"color": "#66CCFF", "id": "Raffay Hamid", "label": "Raffay Hamid", "shape": "dot", "size": 25, "title": "Raffay Hamid"}, {"color": "#66CCFF", "id": "Satellite imagery", "label": "Satellite imagery", "shape": "dot", "size": 25, "title": "Satellite imagery"}, {"color": "#66CCFF", "id": "assessing damages", "label": "assessing damages", "shape": "dot", "size": 25, "title": "assessing damages"}, {"color": "#66CCFF", "id": "Manual inspection", "label": "Manual inspection", "shape": "dot", "size": 25, "title": "Manual inspection"}, {"color": "#66CCFF", "id": "vast amount of data", "label": "vast amount of data", "shape": "dot", "size": 25, "title": "vast amount of data"}, {"color": "#66CCFF", "id": "damage detection", "label": "damage detection", "shape": "dot", "size": 25, "title": "damage detection"}, {"color": "#66CCFF", "id": "semi-supervised learning", "label": "semi-supervised learning", "shape": "dot", "size": 25, "title": "semi-supervised learning"}, {"color": "#66CCFF", "id": "study", "label": "study", "shape": "dot", "size": 25, "title": "study"}, {"color": "#66CCFF", "id": "88 million images", "label": "88 million images", "shape": "dot", "size": 25, "title": "88 million images"}, {"color": "#66CCFF", "id": "4,665 KM2", "label": "4,665 KM2", "shape": "dot", "size": 25, "title": "4,665 KM2"}, {"color": "#66CCFF", "id": "12 locations", "label": "12 locations", "shape": "dot", "size": 25, "title": "12 locations"}, {"color": "#66CCFF", "id": "sun angle", "label": "sun angle", "shape": "dot", "size": 25, "title": "sun angle"}, {"color": "#66CCFF", "id": "sensor resolution", "label": "sensor resolution", "shape": "dot", "size": 25, "title": "sensor resolution"}, {"color": "#66CCFF", "id": "registration differences", "label": "registration differences", "shape": "dot", "size": 25, "title": "registration differences"}, {"color": "#66CCFF", "id": "user study", "label": "user study", "shape": "dot", "size": 25, "title": "user study"}, {"color": "#66CCFF", "id": "ten-fold reduction", "label": "ten-fold reduction", "shape": "dot", "size": 25, "title": "ten-fold reduction"}, {"color": "#66CCFF", "id": "human annotation time", "label": "human annotation time", "shape": "dot", "size": 25, "title": "human annotation time"}, {"color": "#66CCFF", "id": "hierarchical shape features", "label": "hierarchical shape features", "shape": "dot", "size": 25, "title": "hierarchical shape features"}, {"color": "#66CCFF", "id": "bag-of-visual words setting", "label": "bag-of-visual words setting", "shape": "dot", "size": 25, "title": "bag-of-visual words setting"}, {"color": "#66CCFF", "id": "representation", "label": "representation", "shape": "dot", "size": 25, "title": "representation"}, {"color": "#66CCFF", "id": "five alternatives", "label": "five alternatives", "shape": "dot", "size": 25, "title": "five alternatives"}, {"color": "#66CCFF", "id": "detection accuracy", "label": "detection accuracy", "shape": "dot", "size": 25, "title": "detection accuracy"}, {"color": "#66CCFF", "id": "manual inspection", "label": "manual inspection", "shape": "dot", "size": 25, "title": "manual inspection"}, {"color": "#66CCFF", "id": "minimal loss", "label": "minimal loss", "shape": "dot", "size": 25, "title": "minimal loss"}, {"color": "#66CCFF", "id": "time efficiency", "label": "time efficiency", "shape": "dot", "size": 25, "title": "time efficiency"}, {"color": "#66CCFF", "id": "annotation process", "label": "annotation process", "shape": "dot", "size": 25, "title": "annotation process"}, {"color": "#66CCFF", "id": "User study", "label": "User study", "shape": "dot", "size": 25, "title": "User study"}, {"color": "#66CCFF", "id": "ten-fold reduction in annotation time", "label": "ten-fold reduction in annotation time", "shape": "dot", "size": 25, "title": "ten-fold reduction in annotation time"}, {"color": "#66CCFF", "id": "minimal loss in detection accuracy", "label": "minimal loss in detection accuracy", "shape": "dot", "size": 25, "title": "minimal loss in detection accuracy"}, {"color": "#66CCFF", "id": "Damage detection", "label": "Damage detection", "shape": "dot", "size": 25, "title": "Damage detection"}, {"color": "#66CCFF", "id": "Hierarchical shape features", "label": "Hierarchical shape features", "shape": "dot", "size": 25, "title": "Hierarchical shape features"}, {"color": "#66CCFF", "id": "Semi-supervised learning", "label": "Semi-supervised learning", "shape": "dot", "size": 25, "title": "Semi-supervised learning"}, {"color": "#66CCFF", "id": "Novelty detection", "label": "Novelty detection", "shape": "dot", "size": 25, "title": "Novelty detection"}, {"color": "#66CCFF", "id": "Xia et al. (2010)", "label": "Xia et al. (2010)", "shape": "dot", "size": 25, "title": "Xia et al. (2010)"}, {"color": "#66CCFF", "id": "Shape-based invariant texture indexing", "label": "Shape-based invariant texture indexing", "shape": "dot", "size": 25, "title": "Shape-based invariant texture indexing"}, {"color": "#66CCFF", "id": "Markou \u0026 Singh (2003)", "label": "Markou \u0026 Singh (2003)", "shape": "dot", "size": 25, "title": "Markou \u0026 Singh (2003)"}, {"color": "#66CCFF", "id": "Blanchard et al. (2010)", "label": "Blanchard et al. (2010)", "shape": "dot", "size": 25, "title": "Blanchard et al. (2010)"}, {"color": "#66CCFF", "id": "Semi-supervised novelty detection", "label": "Semi-supervised novelty detection", "shape": "dot", "size": 25, "title": "Semi-supervised novelty detection"}, {"color": "#66CCFF", "id": "Bruzone \u0026 Prieto (2000)", "label": "Bruzone \u0026 Prieto (2000)", "shape": "dot", "size": 25, "title": "Bruzone \u0026 Prieto (2000)"}, {"color": "#66CCFF", "id": "Difference image", "label": "Difference image", "shape": "dot", "size": 25, "title": "Difference image"}, {"color": "#66CCFF", "id": "Unsupervised change detection", "label": "Unsupervised change detection", "shape": "dot", "size": 25, "title": "Unsupervised change detection"}, {"color": "#66CCFF", "id": "Satellite imagery analysis", "label": "Satellite imagery analysis", "shape": "dot", "size": 25, "title": "Satellite imagery analysis"}, {"color": "#66CCFF", "id": "Bruzzone", "label": "Bruzzone", "shape": "dot", "size": 25, "title": "Bruzzone"}, {"color": "#66CCFF", "id": "Automatic analysis of the difference image", "label": "Automatic analysis of the difference image", "shape": "dot", "size": 25, "title": "Automatic analysis of the difference image"}, {"color": "#66CCFF", "id": "Prieto", "label": "Prieto", "shape": "dot", "size": 25, "title": "Prieto"}, {"color": "#66CCFF", "id": "Gerard", "label": "Gerard", "shape": "dot", "size": 25, "title": "Gerard"}, {"color": "#66CCFF", "id": "A quasi-linear algorithm", "label": "A quasi-linear algorithm", "shape": "dot", "size": 25, "title": "A quasi-linear algorithm"}, {"color": "#66CCFF", "id": "International Symposium on Mathematical Morphology", "label": "International Symposium on Mathematical Morphology", "shape": "dot", "size": 25, "title": "International Symposium on Mathematical Morphology"}, {"color": "#66CCFF", "id": "Monasse", "label": "Monasse", "shape": "dot", "size": 25, "title": "Monasse"}, {"color": "#66CCFF", "id": "Fast computation of a contrast-invariant image representation", "label": "Fast computation of a contrast-invariant image representation", "shape": "dot", "size": 25, "title": "Fast computation of a contrast-invariant image representation"}, {"color": "#66CCFF", "id": "Guichard", "label": "Guichard", "shape": "dot", "size": 25, "title": "Guichard"}, {"color": "#66CCFF", "id": "Nielsen", "label": "Nielsen", "shape": "dot", "size": 25, "title": "Nielsen"}, {"color": "#66CCFF", "id": "The regularized iteratively reweighted mad method", "label": "The regularized iteratively reweighted mad method", "shape": "dot", "size": 25, "title": "The regularized iteratively reweighted mad method"}, {"color": "#66CCFF", "id": "Vaduva", "label": "Vaduva", "shape": "dot", "size": 25, "title": "Vaduva"}, {"color": "#66CCFF", "id": "A latent analysis of earth surface dynamic evolution", "label": "A latent analysis of earth surface dynamic evolution", "shape": "dot", "size": 25, "title": "A latent analysis of earth surface dynamic evolution"}, {"color": "#66CCFF", "id": "at", "label": "at", "shape": "dot", "size": 25, "title": "at"}, {"color": "#66CCFF", "id": "A latent analysis of earth surface dynamic evolution using change map time series", "label": "A latent analysis of earth surface dynamic evolution using change map time series", "shape": "dot", "size": 25, "title": "A latent analysis of earth surface dynamic evolution using change map time series"}, {"color": "#66CCFF", "id": "Lazarescu", "label": "Lazarescu", "shape": "dot", "size": 25, "title": "Lazarescu"}, {"color": "#66CCFF", "id": "Datcu", "label": "Datcu", "shape": "dot", "size": 25, "title": "Datcu"}, {"color": "#66CCFF", "id": "A latent anisotropy of earth surface dynamic evolution using change map time series", "label": "A latent anisotropy of earth surface dynamic evolution using change map time series", "shape": "dot", "size": 25, "title": "A latent anisotropy of earth surface dynamic evolution using change map time series"}, {"color": "#66CCFF", "id": "Gomez-Chova", "label": "Gomez-Chova", "shape": "dot", "size": 25, "title": "Gomez-Chova"}, {"color": "#66CCFF", "id": "Kernel-based framework for multitemporal and multisource remote sensing data classification and change detection", "label": "Kernel-based framework for multitemporal and multisource remote sensing data classification and change detection", "shape": "dot", "size": 25, "title": "Kernel-based framework for multitemporal and multisource remote sensing data classification and change detection"}, {"color": "#66CCFF", "id": "Wang", "label": "Wang", "shape": "dot", "size": 25, "title": "Wang"}, {"color": "#66CCFF", "id": "Locality-constrained linear coding for image classification", "label": "Locality-constrained linear coding for image classification", "shape": "dot", "size": 25, "title": "Locality-constrained linear coding for image classification"}, {"color": "#66CCFF", "id": "Yang", "label": "Yang", "shape": "dot", "size": 25, "title": "Yang"}, {"color": "#66CCFF", "id": "Song", "label": "Song", "shape": "dot", "size": 25, "title": "Song"}, {"color": "#66CCFF", "id": "Fusing Subcategory Probabilities for Texture Classi\ufb01cation", "label": "Fusing Subcategory Probabilities for Texture Classi\ufb01cation", "shape": "dot", "size": 25, "title": "Fusing Subcategory Probabilities for Texture Classi\ufb01cation"}, {"color": "#66CCFF", "id": "Cai", "label": "Cai", "shape": "dot", "size": 25, "title": "Cai"}, {"color": "#66CCFF", "id": "Gueguen", "label": "Gueguen", "shape": "dot", "size": 25, "title": "Gueguen"}, {"color": "#66CCFF", "id": "DigitalGlobe Inc.", "label": "DigitalGlobe Inc.", "shape": "dot", "size": 25, "title": "DigitalGlobe Inc."}, {"color": "#66CCFF", "id": "Hamid", "label": "Hamid", "shape": "dot", "size": 25, "title": "Hamid"}, {"color": "#66CCFF", "id": "mhamid@digitalGlobe.com", "label": "mhamid@digitalGlobe.com", "shape": "dot", "size": 25, "title": "mhamid@digitalGlobe.com"}, {"color": "#66CCFF", "id": "Yang Song", "label": "Yang Song", "shape": "dot", "size": 25, "title": "Yang Song"}, {"color": "#66CCFF", "id": "Fusing Subcategory Probabilities for Texture Classification", "label": "Fusing Subcategory Probabilities for Texture Classification", "shape": "dot", "size": 25, "title": "Fusing Subcategory Probabilities for Texture Classification"}, {"color": "#66CCFF", "id": "Qing Li", "label": "Qing Li", "shape": "dot", "size": 25, "title": "Qing Li"}, {"color": "#66CCFF", "id": "Fusing SubCategory Probabilities for Texture Classification", "label": "Fusing SubCategory Probabilities for Texture Classification", "shape": "dot", "size": 25, "title": "Fusing SubCategory Probabilities for Texture Classification"}, {"color": "#66CCFF", "id": "Fan Zhang", "label": "Fan Zhang", "shape": "dot", "size": 25, "title": "Fan Zhang"}, {"color": "#66CCFF", "id": "David Dagan Feng", "label": "David Dagan Feng", "shape": "dot", "size": 25, "title": "David Dagan Feng"}, {"color": "#66CCFF", "id": "Heng Huang", "label": "Heng Huang", "shape": "dot", "size": 25, "title": "Heng Huang"}, {"color": "#66CCFF", "id": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers", "label": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers", "shape": "dot", "size": 25, "title": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers"}, {"color": "#66CCFF", "id": "Song_Fusing_Subcategory_Probabilities_2015_CVPR_paper.pdf", "label": "Song_Fusing_Subcategory_Probabilities_2015_CVPR_paper.pdf", "shape": "dot", "size": 25, "title": "Song_Fusing_Subcategory_Probabilities_2015_CVPR_paper.pdf"}, {"color": "#66CCFF", "id": "Texture classification", "label": "Texture classification", "shape": "dot", "size": 25, "title": "Texture classification"}, {"color": "#66CCFF", "id": "high intra-class variation", "label": "high intra-class variation", "shape": "dot", "size": 25, "title": "high intra-class variation"}, {"color": "#66CCFF", "id": "sub-categorization model", "label": "sub-categorization model", "shape": "dot", "size": 25, "title": "sub-categorization model"}, {"color": "#66CCFF", "id": "texture classification", "label": "texture classification", "shape": "dot", "size": 25, "title": "texture classification"}, {"color": "#66CCFF", "id": "class", "label": "class", "shape": "dot", "size": 25, "title": "class"}, {"color": "#66CCFF", "id": "subcategories", "label": "subcategories", "shape": "dot", "size": 25, "title": "subcategories"}, {"color": "#66CCFF", "id": "distinctiveness", "label": "distinctiveness", "shape": "dot", "size": 25, "title": "distinctiveness"}, {"color": "#66CCFF", "id": "representativeness", "label": "representativeness", "shape": "dot", "size": 25, "title": "representativeness"}, {"color": "#66CCFF", "id": "subcategory probabilities", "label": "subcategory probabilities", "shape": "dot", "size": 25, "title": "subcategory probabilities"}, {"color": "#66CCFF", "id": "contribution levels", "label": "contribution levels", "shape": "dot", "size": 25, "title": "contribution levels"}, {"color": "#66CCFF", "id": "cluster qualities", "label": "cluster qualities", "shape": "dot", "size": 25, "title": "cluster qualities"}, {"color": "#66CCFF", "id": "fused probability", "label": "fused probability", "shape": "dot", "size": 25, "title": "fused probability"}, {"color": "#66CCFF", "id": "multiclass classification probability", "label": "multiclass classification probability", "shape": "dot", "size": 25, "title": "multiclass classification probability"}, {"color": "#66CCFF", "id": "KTH-TIPS2 dataset", "label": "KTH-TIPS2 dataset", "shape": "dot", "size": 25, "title": "KTH-TIPS2 dataset"}, {"color": "#66CCFF", "id": "FMD dataset", "label": "FMD dataset", "shape": "dot", "size": 25, "title": "FMD dataset"}, {"color": "#66CCFF", "id": "DTD dataset", "label": "DTD dataset", "shape": "dot", "size": 25, "title": "DTD dataset"}, {"color": "#66CCFF", "id": "Texture Classification", "label": "Texture Classification", "shape": "dot", "size": 25, "title": "Texture Classification"}, {"color": "#66CCFF", "id": "KTH-TIPS2", "label": "KTH-TIPS2", "shape": "dot", "size": 25, "title": "KTH-TIPS2"}, {"color": "#66CCFF", "id": "FMD", "label": "FMD", "shape": "dot", "size": 25, "title": "FMD"}, {"color": "#66CCFF", "id": "DTD", "label": "DTD", "shape": "dot", "size": 25, "title": "DTD"}, {"color": "#66CCFF", "id": "State-of-the-art approaches", "label": "State-of-the-art approaches", "shape": "dot", "size": 25, "title": "State-of-the-art approaches"}, {"color": "#66CCFF", "id": "BMIT Research Group", "label": "BMIT Research Group", "shape": "dot", "size": 25, "title": "BMIT Research Group"}, {"color": "#66CCFF", "id": "University of Sydney", "label": "University of Sydney", "shape": "dot", "size": 25, "title": "University of Sydney"}, {"color": "#66CCFF", "id": "School of IT", "label": "School of IT", "shape": "dot", "size": 25, "title": "School of IT"}, {"color": "#66CCFF", "id": "Australia", "label": "Australia", "shape": "dot", "size": 25, "title": "Australia"}, {"color": "#66CCFF", "id": "Department of Computer Science and Engineering", "label": "Department of Computer Science and Engineering", "shape": "dot", "size": 25, "title": "Department of Computer Science and Engineering"}, {"color": "#66CCFF", "id": "University of Texas, Arlington", "label": "University of Texas, Arlington", "shape": "dot", "size": 25, "title": "University of Texas, Arlington"}, {"color": "#66CCFF", "id": "Manohar Paluri", "label": "Manohar Paluri", "shape": "dot", "size": 25, "title": "Manohar Paluri"}, {"color": "#66CCFF", "id": "Beyond Frontal Faces", "label": "Beyond Frontal Faces", "shape": "dot", "size": 25, "title": "Beyond Frontal Faces"}, {"color": "#66CCFF", "id": "Yaniv Taigman", "label": "Yaniv Taigman", "shape": "dot", "size": 25, "title": "Yaniv Taigman"}, {"color": "#66CCFF", "id": "Rob Fergus", "label": "Rob Fergus", "shape": "dot", "size": 25, "title": "Rob Fergus"}, {"color": "#66CCFF", "id": "Lubomir Bourdev", "label": "Lubomir Bourdev", "shape": "dot", "size": 25, "title": "Lubomir Bourdev"}, {"color": "#66CCFF", "id": "Zhang_Beyond_Frontal_Faces_2015_CVPR_paper", "label": "Zhang_Beyond_Frontal_Faces_2015_CVPR_paper", "shape": "dot", "size": 25, "title": "Zhang_Beyond_Frontal_Faces_2015_CVPR_paper"}, {"color": "#66CCFF", "id": "recognizing peoples\u2019 identities", "label": "recognizing peoples\u2019 identities", "shape": "dot", "size": 25, "title": "recognizing peoples\u2019 identities"}, {"color": "#66CCFF", "id": "PIPA dataset", "label": "PIPA dataset", "shape": "dot", "size": 25, "title": "PIPA dataset"}, {"color": "#66CCFF", "id": "60000 instances", "label": "60000 instances", "shape": "dot", "size": 25, "title": "60000 instances"}, {"color": "#66CCFF", "id": "2000 individuals", "label": "2000 individuals", "shape": "dot", "size": 25, "title": "2000 individuals"}, {"color": "#66CCFF", "id": "PIPER method", "label": "PIPER method", "shape": "dot", "size": 25, "title": "PIPER method"}, {"color": "#66CCFF", "id": "face recognizer", "label": "face recognizer", "shape": "dot", "size": 25, "title": "face recognizer"}, {"color": "#66CCFF", "id": "global recognizer", "label": "global recognizer", "shape": "dot", "size": 25, "title": "global recognizer"}, {"color": "#66CCFF", "id": "person images", "label": "person images", "shape": "dot", "size": 25, "title": "person images"}, {"color": "#66CCFF", "id": "frontal face", "label": "frontal face", "shape": "dot", "size": 25, "title": "frontal face"}, {"color": "#66CCFF", "id": "person recognizers", "label": "person recognizers", "shape": "dot", "size": 25, "title": "person recognizers"}, {"color": "#66CCFF", "id": "deep convolutional networks", "label": "deep convolutional networks", "shape": "dot", "size": 25, "title": "deep convolutional networks"}, {"color": "#66CCFF", "id": "discount pose variations", "label": "discount pose variations", "shape": "dot", "size": 25, "title": "discount pose variations"}, {"color": "#66CCFF", "id": "PIPER", "label": "PIPER", "shape": "dot", "size": 25, "title": "PIPER"}, {"color": "#66CCFF", "id": "DeepFace", "label": "DeepFace", "shape": "dot", "size": 25, "title": "DeepFace"}, {"color": "#66CCFF", "id": "best face recognizers", "label": "best face recognizers", "shape": "dot", "size": 25, "title": "best face recognizers"}, {"color": "#66CCFF", "id": "LFW dataset", "label": "LFW dataset", "shape": "dot", "size": 25, "title": "LFW dataset"}, {"color": "#66CCFF", "id": "unconstrained setup", "label": "unconstrained setup", "shape": "dot", "size": 25, "title": "unconstrained setup"}, {"color": "#66CCFF", "id": "Pose Invariant Recognition", "label": "Pose Invariant Recognition", "shape": "dot", "size": 25, "title": "Pose Invariant Recognition"}, {"color": "#66CCFF", "id": "Paul Wohlhart", "label": "Paul Wohlhart", "shape": "dot", "size": 25, "title": "Paul Wohlhart"}, {"color": "#66CCFF", "id": "Institute for Computer Vision and Graphics, Graz University of Technology, Austria", "label": "Institute for Computer Vision and Graphics, Graz University of Technology, Austria", "shape": "dot", "size": 25, "title": "Institute for Computer Vision and Graphics, Graz University of Technology, Austria"}, {"color": "#66CCFF", "id": "Learning Descriptors for Object Recognition and 3D Pose Estimation", "label": "Learning Descriptors for Object Recognition and 3D Pose Estimation", "shape": "dot", "size": 25, "title": "Learning Descriptors for Object Recognition and 3D Pose Estimation"}, {"color": "#66CCFF", "id": "cvpr_papers", "label": "cvpr_papers", "shape": "dot", "size": 25, "title": "cvpr_papers"}, {"color": "#66CCFF", "id": "Institute for Computer Vision and Graphics", "label": "Institute for Computer Vision and Graphics", "shape": "dot", "size": 25, "title": "Institute for Computer Vision and Graphics"}, {"color": "#66CCFF", "id": "Graz University of Technology", "label": "Graz University of Technology", "shape": "dot", "size": 25, "title": "Graz University of Technology"}, {"color": "#66CCFF", "id": "Austria", "label": "Austria", "shape": "dot", "size": 25, "title": "Austria"}, {"color": "#66CCFF", "id": "{wohlhart}@icg.tugraz.at", "label": "{wohlhart}@icg.tugraz.at", "shape": "dot", "size": 25, "title": "{wohlhart}@icg.tugraz.at"}, {"color": "#66CCFF", "id": "2015 CVPR paper", "label": "2015 CVPR paper", "shape": "dot", "size": 25, "title": "2015 CVPR paper"}, {"color": "#66CCFF", "id": "Learning Descriptors", "label": "Learning Descriptors", "shape": "dot", "size": 25, "title": "Learning Descriptors"}, {"color": "#66CCFF", "id": "Computer Graphics", "label": "Computer Graphics", "shape": "dot", "size": 25, "title": "Computer Graphics"}, {"color": "#66CCFF", "id": "IEEE", "label": "IEEE", "shape": "dot", "size": 25, "title": "IEEE"}, {"color": "#66CCFF", "id": "Vincent LePetit", "label": "Vincent LePetit", "shape": "dot", "size": 25, "title": "Vincent LePetit"}, {"color": "#66CCFF", "id": "Joe Yue-Hei Ng", "label": "Joe Yue-Hei Ng", "shape": "dot", "size": 25, "title": "Joe Yue-Hei Ng"}, {"color": "#66CCFF", "id": "Beyond Short Snippets", "label": "Beyond Short Snippets", "shape": "dot", "size": 25, "title": "Beyond Short Snippets"}, {"color": "#66CCFF", "id": "Matthew Hausknecht", "label": "Matthew Hausknecht", "shape": "dot", "size": 25, "title": "Matthew Hausknecht"}, {"color": "#66CCFF", "id": "Sudheendra Vijayanarasimhan", "label": "Sudheendra Vijayanarasimhan", "shape": "dot", "size": 25, "title": "Sudheendra Vijayanarasimhan"}, {"color": "#66CCFF", "id": "Oriol Vinyals", "label": "Oriol Vinyals", "shape": "dot", "size": 25, "title": "Oriol Vinyals"}, {"color": "#66CCFF", "id": "Rajat Monga", "label": "Rajat Monga", "shape": "dot", "size": 25, "title": "Rajat Monga"}, {"color": "#66CCFF", "id": "Beyond Short Snppets", "label": "Beyond Short Snppets", "shape": "dot", "size": 25, "title": "Beyond Short Snppets"}, {"color": "#66CCFF", "id": "George Toderici", "label": "George Toderici", "shape": "dot", "size": 25, "title": "George Toderici"}, {"color": "#66CCFF", "id": "Convolutional neural networks (CNNs)", "label": "Convolutional neural networks (CNNs)", "shape": "dot", "size": 25, "title": "Convolutional neural networks (CNNs)"}, {"color": "#66CCFF", "id": "image recognition problems", "label": "image recognition problems", "shape": "dot", "size": 25, "title": "image recognition problems"}, {"color": "#66CCFF", "id": "recognition", "label": "recognition", "shape": "dot", "size": 25, "title": "recognition"}, {"color": "#66CCFF", "id": "detection", "label": "detection", "shape": "dot", "size": 25, "title": "detection"}, {"color": "#66CCFF", "id": "retrieval", "label": "retrieval", "shape": "dot", "size": 25, "title": "retrieval"}, {"color": "#66CCFF", "id": "deep neural network architectures", "label": "deep neural network architectures", "shape": "dot", "size": 25, "title": "deep neural network architectures"}, {"color": "#66CCFF", "id": "image information", "label": "image information", "shape": "dot", "size": 25, "title": "image information"}, {"color": "#66CCFF", "id": "longer time periods", "label": "longer time periods", "shape": "dot", "size": 25, "title": "longer time periods"}, {"color": "#66CCFF", "id": "convolutional temporal feature pooling architectures", "label": "convolutional temporal feature pooling architectures", "shape": "dot", "size": 25, "title": "convolutional temporal feature pooling architectures"}, {"color": "#66CCFF", "id": "video", "label": "video", "shape": "dot", "size": 25, "title": "video"}, {"color": "#66CCFF", "id": "ordered sequence of frames", "label": "ordered sequence of frames", "shape": "dot", "size": 25, "title": "ordered sequence of frames"}, {"color": "#66CCFF", "id": "recurrent neural network", "label": "recurrent neural network", "shape": "dot", "size": 25, "title": "recurrent neural network"}, {"color": "#66CCFF", "id": "Long Short-Term Memory (LSTM) cells", "label": "Long Short-Term Memory (LSTM) cells", "shape": "dot", "size": 25, "title": "Long Short-Term Memory (LSTM) cells"}, {"color": "#66CCFF", "id": "LSTM cells", "label": "LSTM cells", "shape": "dot", "size": 25, "title": "LSTM cells"}, {"color": "#66CCFF", "id": "output of CNN", "label": "output of CNN", "shape": "dot", "size": 25, "title": "output of CNN"}, {"color": "#66CCFF", "id": "networks", "label": "networks", "shape": "dot", "size": 25, "title": "networks"}, {"color": "#66CCFF", "id": "performance improvements", "label": "performance improvements", "shape": "dot", "size": 25, "title": "performance improvements"}, {"color": "#66CCFF", "id": "UCF-101 datasets", "label": "UCF-101 datasets", "shape": "dot", "size": 25, "title": "UCF-101 datasets"}, {"color": "#66CCFF", "id": "Sports 1 million dataset", "label": "Sports 1 million dataset", "shape": "dot", "size": 25, "title": "Sports 1 million dataset"}, {"color": "#66CCFF", "id": "73.1%", "label": "73.1%", "shape": "dot", "size": 25, "title": "73.1%"}, {"color": "#66CCFF", "id": "60.9%", "label": "60.9%", "shape": "dot", "size": 25, "title": "60.9%"}, {"color": "#66CCFF", "id": "88.6%", "label": "88.6%", "shape": "dot", "size": 25, "title": "88.6%"}, {"color": "#66CCFF", "id": "88.0%", "label": "88.0%", "shape": "dot", "size": 25, "title": "88.0%"}, {"color": "#66CCFF", "id": "optical flow information", "label": "optical flow information", "shape": "dot", "size": 25, "title": "optical flow information"}, {"color": "#66CCFF", "id": "73.0%", "label": "73.0%", "shape": "dot", "size": 25, "title": "73.0%"}, {"color": "#66CCFF", "id": "82.6%", "label": "82.6%", "shape": "dot", "size": 25, "title": "82.6%"}, {"color": "#66CCFF", "id": "University of Maryland, College Park", "label": "University of Maryland, College Park", "shape": "dot", "size": 25, "title": "University of Maryland, College Park"}, {"color": "#66CCFF", "id": "University of Texas at Austin", "label": "University of Texas at Austin", "shape": "dot", "size": 25, "title": "University of Texas at Austin"}, {"color": "#66CCFF", "id": "Google, Inc.", "label": "Google, Inc.", "shape": "dot", "size": 25, "title": "Google, Inc."}, {"color": "#66CCFF", "id": "Ioannis Gkiouslekas", "label": "Ioannis Gkiouslekas", "shape": "dot", "size": 25, "title": "Ioannis Gkiouslekas"}, {"color": "#66CCFF", "id": "On the Appearance of Translucnt Edges", "label": "On the Appearance of Translucnt Edges", "shape": "dot", "size": 25, "title": "On the Appearance of Translucnt Edges"}, {"color": "#66CCFF", "id": "Gkiouslekas", "label": "Gkiouslekas", "shape": "dot", "size": 25, "title": "Gkiouslekas"}, {"color": "#66CCFF", "id": "Harvard SEAS", "label": "Harvard SEAS", "shape": "dot", "size": 25, "title": "Harvard SEAS"}, {"color": "#66CCFF", "id": "Bruce Walter", "label": "Bruce Walter", "shape": "dot", "size": 25, "title": "Bruce Walter"}, {"color": "#66CCFF", "id": "Cornell University", "label": "Cornell University", "shape": "dot", "size": 25, "title": "Cornell University"}, {"color": "#66CCFF", "id": "Edward H. Adelson", "label": "Edward H. Adelson", "shape": "dot", "size": 25, "title": "Edward H. Adelson"}, {"color": "#66CCFF", "id": "Massachusetts Institute of Technology", "label": "Massachusetts Institute of Technology", "shape": "dot", "size": 25, "title": "Massachusetts Institute of Technology"}, {"color": "#66CCFF", "id": "Edge Radiance Profiles", "label": "Edge Radiance Profiles", "shape": "dot", "size": 25, "title": "Edge Radiance Profiles"}, {"color": "#66CCFF", "id": "Walter", "label": "Walter", "shape": "dot", "size": 25, "title": "Walter"}, {"color": "#66CCFF", "id": "Microfacet models", "label": "Microfacet models", "shape": "dot", "size": 25, "title": "Microfacet models"}, {"color": "#66CCFF", "id": "refraction through rough surfaces", "label": "refraction through rough surfaces", "shape": "dot", "size": 25, "title": "refraction through rough surfaces"}, {"color": "#66CCFF", "id": "EGSR", "label": "EGSR", "shape": "dot", "size": 25, "title": "EGSR"}, {"color": "#66CCFF", "id": "Ioannis Gkioleakas", "label": "Ioannis Gkioleakas", "shape": "dot", "size": 25, "title": "Ioannis Gkioleakas"}, {"color": "#66CCFF", "id": "Higher Education", "label": "Higher Education", "shape": "dot", "size": 25, "title": "Higher Education"}, {"color": "#66CCFF", "id": "igkio@seas.harvard.edu", "label": "igkio@seas.harvard.edu", "shape": "dot", "size": 25, "title": "igkio@seas.harvard.edu"}, {"color": "#66CCFF", "id": "bruce.walter@cornell.edu", "label": "bruce.walter@cornell.edu", "shape": "dot", "size": 25, "title": "bruce.walter@cornell.edu"}, {"color": "#66CCFF", "id": "adelson@cail.mit.edu", "label": "adelson@cail.mit.edu", "shape": "dot", "size": 25, "title": "adelson@cail.mit.edu"}, {"color": "#66CCFF", "id": "Academic Degrees", "label": "Academic Degrees", "shape": "dot", "size": 25, "title": "Academic Degrees"}, {"color": "#66CCFF", "id": "Ioannis Gkiousleas", "label": "Ioannis Gkiousleas", "shape": "dot", "size": 25, "title": "Ioannis Gkiousleas"}, {"color": "#66CCFF", "id": "Todd Zickler", "label": "Todd Zickler", "shape": "dot", "size": 25, "title": "Todd Zickler"}, {"color": "#66CCFF", "id": "Kavita Bala", "label": "Kavita Bala", "shape": "dot", "size": 25, "title": "Kavita Bala"}, {"color": "#66CCFF", "id": "Naeemullah Khan", "label": "Naeemullah Khan", "shape": "dot", "size": 25, "title": "Naeemullah Khan"}, {"color": "#66CCFF", "id": "Shape-Tailored Local Descriptors", "label": "Shape-Tailored Local Descriptors", "shape": "dot", "size": 25, "title": "Shape-Tailored Local Descriptors"}, {"color": "#66CCFF", "id": "Marei Algarni", "label": "Marei Algarni", "shape": "dot", "size": 25, "title": "Marei Algarni"}, {"color": "#66CCFF", "id": "Anthony Yezzi", "label": "Anthony Yezzi", "shape": "dot", "size": 25, "title": "Anthony Yezzi"}, {"color": "#66CCFF", "id": "Shape-Tailed Local Descriptors", "label": "Shape-Tailed Local Descriptors", "shape": "dot", "size": 25, "title": "Shape-Tailed Local Descriptors"}, {"color": "#66CCFF", "id": "Ganesh Sundaramoorthi", "label": "Ganesh Sundaramoorthi", "shape": "dot", "size": 25, "title": "Ganesh Sundaramoorthi"}, {"color": "#66CCFF", "id": "Segmentation", "label": "Segmentation", "shape": "dot", "size": 25, "title": "Segmentation"}, {"color": "#66CCFF", "id": "Tracking", "label": "Tracking", "shape": "dot", "size": 25, "title": "Tracking"}, {"color": "#66CCFF", "id": "descriptors", "label": "descriptors", "shape": "dot", "size": 25, "title": "descriptors"}, {"color": "#66CCFF", "id": "dense descriptors", "label": "dense descriptors", "shape": "dot", "size": 25, "title": "dense descriptors"}, {"color": "#66CCFF", "id": "texture segmentation", "label": "texture segmentation", "shape": "dot", "size": 25, "title": "texture segmentation"}, {"color": "#66CCFF", "id": "shape-dependent scale spaces", "label": "shape-dependent scale spaces", "shape": "dot", "size": 25, "title": "shape-dependent scale spaces"}, {"color": "#66CCFF", "id": "aggregate image data across boundary", "label": "aggregate image data across boundary", "shape": "dot", "size": 25, "title": "aggregate image data across boundary"}, {"color": "#66CCFF", "id": "Mumford-Shah energy", "label": "Mumford-Shah energy", "shape": "dot", "size": 25, "title": "Mumford-Shah energy"}, {"color": "#66CCFF", "id": "datasets", "label": "datasets", "shape": "dot", "size": 25, "title": "datasets"}, {"color": "#66CCFF", "id": "accurate segmentation", "label": "accurate segmentation", "shape": "dot", "size": 25, "title": "accurate segmentation"}, {"color": "#66CCFF", "id": "shape-dependent", "label": "shape-dependent", "shape": "dot", "size": 25, "title": "shape-dependent"}, {"color": "#66CCFF", "id": "oriented gradients", "label": "oriented gradients", "shape": "dot", "size": 25, "title": "oriented gradients"}, {"color": "#66CCFF", "id": "state-of-the-art", "label": "state-of-the-art", "shape": "dot", "size": 25, "title": "state-of-the-art"}, {"color": "#66CCFF", "id": "non-shape dependent", "label": "non-shape dependent", "shape": "dot", "size": 25, "title": "non-shape dependent"}, {"color": "#66CCFF", "id": "more accurate segmentation", "label": "more accurate segmentation", "shape": "dot", "size": 25, "title": "more accurate segmentation"}, {"color": "#66CCFF", "id": "textured object tracking", "label": "textured object tracking", "shape": "dot", "size": 25, "title": "textured object tracking"}, {"color": "#66CCFF", "id": "Shape-Tailored Descriptors (STLD)", "label": "Shape-Tailored Descriptors (STLD)", "shape": "dot", "size": 25, "title": "Shape-Tailored Descriptors (STLD)"}, {"color": "#66CCFF", "id": "Shape-Tailori Descriptors (STLD)", "label": "Shape-Tailori Descriptors (STLD)", "shape": "dot", "size": 25, "title": "Shape-Tailori Descriptors (STLD)"}, {"color": "#66CCFF", "id": "non-shape dependent descriptors", "label": "non-shape dependent descriptors", "shape": "dot", "size": 25, "title": "non-shape dependent descriptors"}, {"color": "#66CCFF", "id": "Local Descriptors", "label": "Local Descriptors", "shape": "dot", "size": 25, "title": "Local Descriptors"}, {"color": "#66CCFF", "id": "King Abdullah University of Science \u0026 Technology (KAUST)", "label": "King Abdullah University of Science \u0026 Technology (KAUST)", "shape": "dot", "size": 25, "title": "King Abdullah University of Science \u0026 Technology (KAUST)"}, {"color": "#66CCFF", "id": "Georgia Institute of Technology", "label": "Georgia Institute of Technology", "shape": "dot", "size": 25, "title": "Georgia Institute of Technology"}, {"color": "#66CCFF", "id": "De-An Huang", "label": "De-An Huang", "shape": "dot", "size": 25, "title": "De-An Huang"}, {"color": "#66CCFF", "id": "How Do We Use Our Hands?", "label": "How Do We Use Our Hands?", "shape": "dot", "size": 25, "title": "How Do We Use Our Hands?"}, {"color": "#66CCFF", "id": "common grasps", "label": "common grasps", "shape": "dot", "size": 25, "title": "common grasps"}, {"color": "#66CCFF", "id": "Partial Differential Equations (PDE)", "label": "Partial Differential Equations (PDE)", "shape": "dot", "size": 25, "title": "Partial Differential Equations (PDE)"}, {"color": "#66CCFF", "id": "Minghuang Ma", "label": "Minghuang Ma", "shape": "dot", "size": 25, "title": "Minghuang Ma"}, {"color": "#66CCFF", "id": "How DoWe Use Our Hands?", "label": "How DoWe Use Our Hands?", "shape": "dot", "size": 25, "title": "How DoWe Use Our Hands?"}, {"color": "#66CCFF", "id": "Wei-Chiu Ma", "label": "Wei-Chiu Ma", "shape": "dot", "size": 25, "title": "Wei-Chiu Ma"}, {"color": "#66CCFF", "id": "Kris M. Kitani", "label": "Kris M. Kitani", "shape": "dot", "size": 25, "title": "Kris M. Kitani"}, {"color": "#66CCFF", "id": "KAUST", "label": "KAUST", "shape": "dot", "size": 25, "title": "KAUST"}, {"color": "#66CCFF", "id": "Saudi Arabia", "label": "Saudi Arabia", "shape": "dot", "size": 25, "title": "Saudi Arabia"}, {"color": "#66CCFF", "id": "ganesh.sundaramoorthi@kust.edu.sa", "label": "ganesh.sundaramoorthi@kust.edu.sa", "shape": "dot", "size": 25, "title": "ganesh.sundaramoorthi@kust.edu.sa"}, {"color": "#66CCFF", "id": "Huang_How_Do_We2015_CVPR_paper.pdf", "label": "Huang_How_Do_We2015_CVPR_paper.pdf", "shape": "dot", "size": 25, "title": "Huang_How_Do_We2015_CVPR_paper.pdf"}, {"color": "#66CCFF", "id": "Common Grasps", "label": "Common Grasps", "shape": "dot", "size": 25, "title": "Common Grasps"}, {"color": "#66CCFF", "id": "Common Graspt", "label": "Common Graspt", "shape": "dot", "size": 25, "title": "Common Graspt"}, {"color": "#66CCFF", "id": "computer vision techniques", "label": "computer vision techniques", "shape": "dot", "size": 25, "title": "computer vision techniques"}, {"color": "#66CCFF", "id": "advance prehensile analysis", "label": "advance prehensile analysis", "shape": "dot", "size": 25, "title": "advance prehensile analysis"}, {"color": "#66CCFF", "id": "prehensile analysis", "label": "prehensile analysis", "shape": "dot", "size": 25, "title": "prehensile analysis"}, {"color": "#66CCFF", "id": "multi-disciplinary field", "label": "multi-disciplinary field", "shape": "dot", "size": 25, "title": "multi-disciplinary field"}, {"color": "#66CCFF", "id": "researchers", "label": "researchers", "shape": "dot", "size": 25, "title": "researchers"}, {"color": "#66CCFF", "id": "hand-object interaction videos", "label": "hand-object interaction videos", "shape": "dot", "size": 25, "title": "hand-object interaction videos"}, {"color": "#66CCFF", "id": "wearable cameras", "label": "wearable cameras", "shape": "dot", "size": 25, "title": "wearable cameras"}, {"color": "#66CCFF", "id": "automatically discover common modes of human hand use", "label": "automatically discover common modes of human hand use", "shape": "dot", "size": 25, "title": "automatically discover common modes of human hand use"}, {"color": "#66CCFF", "id": "unsupervised clustering techniques", "label": "unsupervised clustering techniques", "shape": "dot", "size": 25, "title": "unsupervised clustering techniques"}, {"color": "#66CCFF", "id": "first-person point-of-view camera", "label": "first-person point-of-view camera", "shape": "dot", "size": 25, "title": "first-person point-of-view camera"}, {"color": "#66CCFF", "id": "observing human hand use", "label": "observing human hand use", "shape": "dot", "size": 25, "title": "observing human hand use"}, {"color": "#66CCFF", "id": "common modes of human hand use", "label": "common modes of human hand use", "shape": "dot", "size": 25, "title": "common modes of human hand use"}, {"color": "#66CCFF", "id": "first-person point-of-view videos", "label": "first-person point-of-view videos", "shape": "dot", "size": 25, "title": "first-person point-of-view videos"}, {"color": "#66CCFF", "id": "choreographed scenarios", "label": "choreographed scenarios", "shape": "dot", "size": 25, "title": "choreographed scenarios"}, {"color": "#66CCFF", "id": "Cutkosky grasp taxonomy", "label": "Cutkosky grasp taxonomy", "shape": "dot", "size": 25, "title": "Cutkosky grasp taxonomy"}, {"color": "#66CCFF", "id": "Grasp Taxonomy", "label": "Grasp Taxonomy", "shape": "dot", "size": 25, "title": "Grasp Taxonomy"}, {"color": "#66CCFF", "id": "hand-object interaction", "label": "hand-object interaction", "shape": "dot", "size": 25, "title": "hand-object interaction"}, {"color": "#66CCFF", "id": "Hand-Object Interaction", "label": "Hand-Object Interaction", "shape": "dot", "size": 25, "title": "Hand-Object Interaction"}, {"color": "#66CCFF", "id": "Determinantal Point Process (DPP)", "label": "Determinantal Point Process (DPP)", "shape": "dot", "size": 25, "title": "Determinantal Point Process (DPP)"}, {"color": "#66CCFF", "id": "N. Ailon", "label": "N. Ailon", "shape": "dot", "size": 25, "title": "N. Ailon"}, {"color": "#66CCFF", "id": "Streaming k-means approximation", "label": "Streaming k-means approximation", "shape": "dot", "size": 25, "title": "Streaming k-means approximation"}, {"color": "#66CCFF", "id": "W. Barbakh", "label": "W. Barbakh", "shape": "dot", "size": 25, "title": "W. Barbakh"}, {"color": "#66CCFF", "id": "Online clustering algorithms", "label": "Online clustering algorithms", "shape": "dot", "size": 25, "title": "Online clustering algorithms"}, {"color": "#66CCFF", "id": "A. Fathi", "label": "A. Fathi", "shape": "dot", "size": 25, "title": "A. Fathi"}, {"color": "#66CCFF", "id": "Social interactions: A first-person perspective", "label": "Social interactions: A first-person perspective", "shape": "dot", "size": 25, "title": "Social interactions: A first-person perspective"}, {"color": "#66CCFF", "id": "R. Filipovych", "label": "R. Filipovych", "shape": "dot", "size": 25, "title": "R. Filipovych"}, {"color": "#66CCFF", "id": "Recognizing primitive interactions", "label": "Recognizing primitive interactions", "shape": "dot", "size": 25, "title": "Recognizing primitive interactions"}, {"color": "#66CCFF", "id": "J. Case-Smith", "label": "J. Case-Smith", "shape": "dot", "size": 25, "title": "J. Case-Smith"}, {"color": "#66CCFF", "id": "Development of hand skills in children", "label": "Development of hand skills in children", "shape": "dot", "size": 25, "title": "Development of hand skills in children"}, {"color": "#66CCFF", "id": "L. Cheng", "label": "L. Cheng", "shape": "dot", "size": 25, "title": "L. Cheng"}, {"color": "#66CCFF", "id": "Pixel-level hand detection in ego-centric videos", "label": "Pixel-level hand detection in ego-centric videos", "shape": "dot", "size": 25, "title": "Pixel-level hand detection in ego-centric videos"}, {"color": "#66CCFF", "id": "American Occupational Therapy Association", "label": "American Occupational Therapy Association", "shape": "dot", "size": 25, "title": "American Occupational Therapy Association"}, {"color": "#66CCFF", "id": "M. Cutkosky", "label": "M. Cutkosky", "shape": "dot", "size": 25, "title": "M. Cutkosky"}, {"color": "#66CCFF", "id": "On grasp choice, grasp models", "label": "On grasp choice, grasp models", "shape": "dot", "size": 25, "title": "On grasp choice, grasp models"}, {"color": "#66CCFF", "id": "H. N. Djidjev", "label": "H. N. Djidjev", "shape": "dot", "size": 25, "title": "H. N. Djidjev"}, {"color": "#66CCFF", "id": "Computing shortest paths", "label": "Computing shortest paths", "shape": "dot", "size": 25, "title": "Computing shortest paths"}, {"color": "#66CCFF", "id": "C. Desai", "label": "C. Desai", "shape": "dot", "size": 25, "title": "C. Desai"}, {"color": "#66CCFF", "id": "Discriminaitive models for static human-object interactions", "label": "Discriminaitive models for static human-object interactions", "shape": "dot", "size": 25, "title": "Discriminaitive models for static human-object interactions"}, {"color": "#66CCFF", "id": "Cnegie Mellon University", "label": "Cnegie Mellon University", "shape": "dot", "size": 25, "title": "Cnegie Mellon University"}, {"color": "#66CCFF", "id": "Kris M. Kitan", "label": "Kris M. Kitan", "shape": "dot", "size": 25, "title": "Kris M. Kitan"}, {"color": "#66CCFF", "id": "hand skills", "label": "hand skills", "shape": "dot", "size": 25, "title": "hand skills"}, {"color": "#66CCFF", "id": "Canezie Mellon University", "label": "Canezie Mellon University", "shape": "dot", "size": 25, "title": "Canezie Mellon University"}, {"color": "#66CCFF", "id": "deanh@andrew.cmu.edu", "label": "deanh@andrew.cmu.edu", "shape": "dot", "size": 25, "title": "deanh@andrew.cmu.edu"}, {"color": "#66CCFF", "id": "minghuam@andrew.cmu.edu", "label": "minghuam@andrew.cmu.edu", "shape": "dot", "size": 25, "title": "minghuam@andrew.cmu.edu"}, {"color": "#66CCFF", "id": "weichium@andrew.cmu.edu", "label": "weichium@andrew.cmu.edu", "shape": "dot", "size": 25, "title": "weichium@andrew.cmu.edu"}, {"color": "#66CCFF", "id": "Edward Johns", "label": "Edward Johns", "shape": "dot", "size": 25, "title": "Edward Johns"}, {"color": "#66CCFF", "id": "Becoming the Expert", "label": "Becoming the Expert", "shape": "dot", "size": 25, "title": "Becoming the Expert"}, {"color": "#66CCFF", "id": "Oisin Mac Aodha", "label": "Oisin Mac Aodha", "shape": "dot", "size": 25, "title": "Oisin Mac Aodha"}, {"color": "#66CCFF", "id": "Becoming the Efficient", "label": "Becoming the Efficient", "shape": "dot", "size": 25, "title": "Becoming the Efficient"}, {"color": "#66CCFF", "id": "Gabriel J. Brostow", "label": "Gabriel J. Brostow", "shape": "dot", "size": 25, "title": "Gabriel J. Brostow"}, {"color": "#66CCFF", "id": "Machine Teaching", "label": "Machine Teaching", "shape": "dot", "size": 25, "title": "Machine Teaching"}, {"color": "#66CCFF", "id": "Johns_Becoming_the_Expert_2015_CVPR_paper.pdf", "label": "Johns_Becoming_the_Expert_2015_CVPR_paper.pdf", "shape": "dot", "size": 25, "title": "Johns_Becoming_the_Expert_2015_CVPR_paper.pdf"}, {"color": "#66CCFF", "id": "Interactive Machine Teaching algorithm", "label": "Interactive Machine Teaching algorithm", "shape": "dot", "size": 25, "title": "Interactive Machine Teaching algorithm"}, {"color": "#66CCFF", "id": "computer", "label": "computer", "shape": "dot", "size": 25, "title": "computer"}, {"color": "#66CCFF", "id": "challenging visual concepts", "label": "challenging visual concepts", "shape": "dot", "size": 25, "title": "challenging visual concepts"}, {"color": "#66CCFF", "id": "labeled images", "label": "labeled images", "shape": "dot", "size": 25, "title": "labeled images"}, {"color": "#66CCFF", "id": "teaching strategy", "label": "teaching strategy", "shape": "dot", "size": 25, "title": "teaching strategy"}, {"color": "#66CCFF", "id": "experts", "label": "experts", "shape": "dot", "size": 25, "title": "experts"}, {"color": "#66CCFF", "id": "challenge", "label": "challenge", "shape": "dot", "size": 25, "title": "challenge"}, {"color": "#66CCFF", "id": "annotators", "label": "annotators", "shape": "dot", "size": 25, "title": "annotators"}, {"color": "#66CCFF", "id": "expertise", "label": "expertise", "shape": "dot", "size": 25, "title": "expertise"}, {"color": "#66CCFF", "id": "Adaptive Algorithms", "label": "Adaptive Algorithms", "shape": "dot", "size": 25, "title": "Adaptive Algorithms"}, {"color": "#66CCFF", "id": "Interactive Machine Teaching", "label": "Interactive Machine Teaching", "shape": "dot", "size": 25, "title": "Interactive Machine Teaching"}, {"color": "#66CCFF", "id": "Human Learning", "label": "Human Learning", "shape": "dot", "size": 25, "title": "Human Learning"}, {"color": "#66CCFF", "id": "Visual Classification", "label": "Visual Classification", "shape": "dot", "size": 25, "title": "Visual Classification"}, {"color": "#66CCFF", "id": "Bruner", "label": "Bruner", "shape": "dot", "size": 25, "title": "Bruner"}, {"color": "#66CCFF", "id": "The Process of Education", "label": "The Process of Education", "shape": "dot", "size": 25, "title": "The Process of Education"}, {"color": "#66CCFF", "id": "foundational concepts", "label": "foundational concepts", "shape": "dot", "size": 25, "title": "foundational concepts"}, {"color": "#66CCFF", "id": "teaching and learning", "label": "teaching and learning", "shape": "dot", "size": 25, "title": "teaching and learning"}, {"color": "#66CCFF", "id": "Curriculum learning", "label": "Curriculum learning", "shape": "dot", "size": 25, "title": "Curriculum learning"}, {"color": "#66CCFF", "id": "teaching strategies", "label": "teaching strategies", "shape": "dot", "size": 25, "title": "teaching strategies"}, {"color": "#66CCFF", "id": "Love", "label": "Love", "shape": "dot", "size": 25, "title": "Love"}, {"color": "#66CCFF", "id": "Categorization", "label": "Categorization", "shape": "dot", "size": 25, "title": "Categorization"}, {"color": "#66CCFF", "id": "cognitive neuroscience", "label": "cognitive neuroscience", "shape": "dot", "size": 25, "title": "cognitive neuroscience"}, {"color": "#66CCFF", "id": "core tasks", "label": "core tasks", "shape": "dot", "size": 25, "title": "core tasks"}, {"color": "#66CCFF", "id": "active learning", "label": "active learning", "shape": "dot", "size": 25, "title": "active learning"}, {"color": "#66CCFF", "id": "Machine teaching", "label": "Machine teaching", "shape": "dot", "size": 25, "title": "Machine teaching"}, {"color": "#66CCFF", "id": "education", "label": "education", "shape": "dot", "size": 25, "title": "education"}, {"color": "#66CCFF", "id": "overview", "label": "overview", "shape": "dot", "size": 25, "title": "overview"}, {"color": "#66CCFF", "id": "technique", "label": "technique", "shape": "dot", "size": 25, "title": "technique"}, {"color": "#66CCFF", "id": "sampling estimation", "label": "sampling estimation", "shape": "dot", "size": 25, "title": "sampling estimation"}, {"color": "#66CCFF", "id": "error", "label": "error", "shape": "dot", "size": 25, "title": "error"}, {"color": "#66CCFF", "id": "machine teaching", "label": "machine teaching", "shape": "dot", "size": 25, "title": "machine teaching"}, {"color": "#66CCFF", "id": "approach toward optimal education", "label": "approach toward optimal education", "shape": "dot", "size": 25, "title": "approach toward optimal education"}, {"color": "#66CCFF", "id": "paper\u0027s design", "label": "paper\u0027s design", "shape": "dot", "size": 25, "title": "paper\u0027s design"}, {"color": "#66CCFF", "id": "learners with limited cognitive capacity", "label": "learners with limited cognitive capacity", "shape": "dot", "size": 25, "title": "learners with limited cognitive capacity"}, {"color": "#66CCFF", "id": "algorithmic teaching", "label": "algorithmic teaching", "shape": "dot", "size": 25, "title": "algorithmic teaching"}, {"color": "#66CCFF", "id": "background", "label": "background", "shape": "dot", "size": 25, "title": "background"}, {"color": "#66CCFF", "id": "Love \u0026 Patil", "label": "Love \u0026 Patil", "shape": "dot", "size": 25, "title": "Love \u0026 Patil"}, {"color": "#66CCFF", "id": "teaching learners", "label": "teaching learners", "shape": "dot", "size": 25, "title": "teaching learners"}, {"color": "#66CCFF", "id": "Balbach \u0026 Zeugmann", "label": "Balbach \u0026 Zeugmann", "shape": "dot", "size": 25, "title": "Balbach \u0026 Zeugmann"}, {"color": "#66CCFF", "id": "algorithmic teaching methods", "label": "algorithmic teaching methods", "shape": "dot", "size": 25, "title": "algorithmic teaching methods"}, {"color": "#66CCFF", "id": "Basu \u0026 Christensen", "label": "Basu \u0026 Christensen", "shape": "dot", "size": 25, "title": "Basu \u0026 Christensen"}, {"color": "#66CCFF", "id": "teaching classification boundaries", "label": "teaching classification boundaries", "shape": "dot", "size": 25, "title": "teaching classification boundaries"}, {"color": "#66CCFF", "id": "Optimal teaching for limited-capacity human learners", "label": "Optimal teaching for limited-capacity human learners", "shape": "dot", "size": 25, "title": "Optimal teaching for limited-capacity human learners"}, {"color": "#66CCFF", "id": "Language and Automata Theory and Applications", "label": "Language and Automata Theory and Applications", "shape": "dot", "size": 25, "title": "Language and Automata Theory and Applications"}, {"color": "#66CCFF", "id": "Recent developments in algorithmic teaching", "label": "Recent developments in algorithmic teaching", "shape": "dot", "size": 25, "title": "Recent developments in algorithmic teaching"}, {"color": "#66CCFF", "id": "teaching classification tasks", "label": "teaching classification tasks", "shape": "dot", "size": 25, "title": "teaching classification tasks"}, {"color": "#66CCFF", "id": "Gigu`ere \u0026 Love", "label": "Gigu`ere \u0026 Love", "shape": "dot", "size": 25, "title": "Gigu`ere \u0026 Love"}, {"color": "#66CCFF", "id": "cognitive limitations", "label": "cognitive limitations", "shape": "dot", "size": 25, "title": "cognitive limitations"}, {"color": "#66CCFF", "id": "Chin", "label": "Chin", "shape": "dot", "size": 25, "title": "Chin"}, {"color": "#66CCFF", "id": "University College London", "label": "University College London", "shape": "dot", "size": 25, "title": "University College London"}, {"color": "#66CCFF", "id": "Eriksson", "label": "Eriksson", "shape": "dot", "size": 25, "title": "Eriksson"}, {"color": "#66CCFF", "id": "Suter", "label": "Suter", "shape": "dot", "size": 25, "title": "Suter"}, {"color": "#66CCFF", "id": "Chin_Efficient_Globally_Optimal_2015_CVPR_paper.pdf", "label": "Chin_Efficient_Globally_Optimal_2015_CVPR_paper.pdf", "shape": "dot", "size": 25, "title": "Chin_Efficient_Globally_Optimal_2015_CVPR_paper.pdf"}, {"color": "#66CCFF", "id": "Efficient Globally Optimal Consensus Maximisation", "label": "Efficient Globally Optimal Consensus Maximisation", "shape": "dot", "size": 25, "title": "Efficient Globally Optimal Consensus Maximisation"}, {"color": "#66CCFF", "id": "Maximum Consensus", "label": "Maximum Consensus", "shape": "dot", "size": 25, "title": "Maximum Consensus"}, {"color": "#66CCFF", "id": "Robust Estimation", "label": "Robust Estimation", "shape": "dot", "size": 25, "title": "Robust Estimation"}, {"color": "#66CCFF", "id": "randomized sample-and-test techniques", "label": "randomized sample-and-test techniques", "shape": "dot", "size": 25, "title": "randomized sample-and-test techniques"}, {"color": "#66CCFF", "id": "optimality", "label": "optimality", "shape": "dot", "size": 25, "title": "optimality"}, {"color": "#66CCFF", "id": "globally optimal algorithms", "label": "globally optimal algorithms", "shape": "dot", "size": 25, "title": "globally optimal algorithms"}, {"color": "#66CCFF", "id": "randomized methods", "label": "randomized methods", "shape": "dot", "size": 25, "title": "randomized methods"}, {"color": "#66CCFF", "id": "global maximization of consensus", "label": "global maximization of consensus", "shape": "dot", "size": 25, "title": "global maximization of consensus"}, {"color": "#66CCFF", "id": "tree search problem", "label": "tree search problem", "shape": "dot", "size": 25, "title": "tree search problem"}, {"color": "#66CCFF", "id": "LP-type methods", "label": "LP-type methods", "shape": "dot", "size": 25, "title": "LP-type methods"}, {"color": "#66CCFF", "id": "A* search", "label": "A* search", "shape": "dot", "size": 25, "title": "A* search"}, {"color": "#66CCFF", "id": "orders of magnitude faster performance", "label": "orders of magnitude faster performance", "shape": "dot", "size": 25, "title": "orders of magnitude faster performance"}, {"color": "#66CCFF", "id": "previous exact methods", "label": "previous exact methods", "shape": "dot", "size": 25, "title": "previous exact methods"}, {"color": "#66CCFF", "id": "Optimization Problems", "label": "Optimization Problems", "shape": "dot", "size": 25, "title": "Optimization Problems"}, {"color": "#66CCFF", "id": "A* Search", "label": "A* Search", "shape": "dot", "size": 25, "title": "A* Search"}, {"color": "#66CCFF", "id": "Search Algorithm", "label": "Search Algorithm", "shape": "dot", "size": 25, "title": "Search Algorithm"}, {"color": "#66CCFF", "id": "Tree Search", "label": "Tree Search", "shape": "dot", "size": 25, "title": "Tree Search"}, {"color": "#66CCFF", "id": "LP-type Methods", "label": "LP-type Methods", "shape": "dot", "size": 25, "title": "LP-type Methods"}, {"color": "#66CCFF", "id": "N. Amenta", "label": "N. Amenta", "shape": "dot", "size": 25, "title": "N. Amenta"}, {"color": "#66CCFF", "id": "Optimal Point Placement for Mesh Smoothing", "label": "Optimal Point Placement for Mesh Smoothing", "shape": "dot", "size": 25, "title": "Optimal Point Placement for Mesh Smoothing"}, {"color": "#66CCFF", "id": "M. Bern", "label": "M. Bern", "shape": "dot", "size": 25, "title": "M. Bern"}, {"color": "#66CCFF", "id": "D. Eppstein", "label": "D. Eppstein", "shape": "dot", "size": 25, "title": "D. Eppstein"}, {"color": "#66CCFF", "id": "B. Chazelle", "label": "B. Chazelle", "shape": "dot", "size": 25, "title": "B. Chazelle"}, {"color": "#66CCFF", "id": "On Linear-Time Deterministic Algorithms", "label": "On Linear-Time Deterministic Algorithms", "shape": "dot", "size": 25, "title": "On Linear-Time Deterministic Algorithms"}, {"color": "#66CCFF", "id": "J. Matou\u02c7sek", "label": "J. Matou\u02c7sek", "shape": "dot", "size": 25, "title": "J. Matou\u02c7sek"}, {"color": "#66CCFF", "id": "Quasiconvex Programming", "label": "Quasiconvex Programming", "shape": "dot", "size": 25, "title": "Quasiconvex Programming"}, {"color": "#66CCFF", "id": "Optimization Technique", "label": "Optimization Technique", "shape": "dot", "size": 25, "title": "Optimization Technique"}, {"color": "#66CCFF", "id": "globally optimal results", "label": "globally optimal results", "shape": "dot", "size": 25, "title": "globally optimal results"}, {"color": "#66CCFF", "id": "optimization algorithms", "label": "optimization algorithms", "shape": "dot", "size": 25, "title": "optimization algorithms"}, {"color": "#66CCFF", "id": "core theme", "label": "core theme", "shape": "dot", "size": 25, "title": "core theme"}, {"color": "#66CCFF", "id": "RANSA algorithm", "label": "RANSA algorithm", "shape": "dot", "size": 25, "title": "RANSA algorithm"}, {"color": "#66CCFF", "id": "outlier rejection", "label": "outlier rejection", "shape": "dot", "size": 25, "title": "outlier rejection"}, {"color": "#66CCFF", "id": "multiple view geometry", "label": "multiple view geometry", "shape": "dot", "size": 25, "title": "multiple view geometry"}, {"color": "#66CCFF", "id": "central topic", "label": "central topic", "shape": "dot", "size": 25, "title": "central topic"}, {"color": "#66CCFF", "id": "l\u221e triangulation", "label": "l\u221e triangulation", "shape": "dot", "size": 25, "title": "l\u221e triangulation"}, {"color": "#66CCFF", "id": "outlier handling", "label": "outlier handling", "shape": "dot", "size": 25, "title": "outlier handling"}, {"color": "#66CCFF", "id": "automated cartography", "label": "automated cartography", "shape": "dot", "size": 25, "title": "automated cartography"}, {"color": "#66CCFF", "id": "H. Li", "label": "H. Li", "shape": "dot", "size": 25, "title": "H. Li"}, {"color": "#66CCFF", "id": "algorithm for l\u221e triangulation", "label": "algorithm for l\u221e triangulation", "shape": "dot", "size": 25, "title": "algorithm for l\u221e triangulation"}, {"color": "#66CCFF", "id": "geometric optimization", "label": "geometric optimization", "shape": "dot", "size": 25, "title": "geometric optimization"}, {"color": "#66CCFF", "id": "constraints", "label": "constraints", "shape": "dot", "size": 25, "title": "constraints"}, {"color": "#66CCFF", "id": "C. Olsson, O. Enqvist, and F. Kahl", "label": "C. Olsson, O. Enqvist, and F. Kahl", "shape": "dot", "size": 25, "title": "C. Olsson, O. Enqvist, and F. Kahl"}, {"color": "#66CCFF", "id": "outlier handling in matching", "label": "outlier handling in matching", "shape": "dot", "size": 25, "title": "outlier handling in matching"}, {"color": "#66CCFF", "id": "matching", "label": "matching", "shape": "dot", "size": 25, "title": "matching"}, {"color": "#66CCFF", "id": "registration", "label": "registration", "shape": "dot", "size": 25, "title": "registration"}, {"color": "#66CCFF", "id": "C. Olsson, A. Eriksson, and F. Kahl", "label": "C. Olsson, A. Eriksson, and F. Kahl", "shape": "dot", "size": 25, "title": "C. Olsson, A. Eriksson, and F. Kahl"}, {"color": "#66CCFF", "id": "optimization techniques", "label": "optimization techniques", "shape": "dot", "size": 25, "title": "optimization techniques"}, {"color": "#66CCFF", "id": "l\u221e-norm problems", "label": "l\u221e-norm problems", "shape": "dot", "size": 25, "title": "l\u221e-norm problems"}, {"color": "#66CCFF", "id": "Tat-Jun Chin", "label": "Tat-Jun Chin", "shape": "dot", "size": 25, "title": "Tat-Jun Chin"}, {"color": "#66CCFF", "id": "University of Adelaide", "label": "University of Adelaide", "shape": "dot", "size": 25, "title": "University of Adelaide"}, {"color": "#66CCFF", "id": "Anders Eriksson", "label": "Anders Eriksson", "shape": "dot", "size": 25, "title": "Anders Eriksson"}, {"color": "#66CCFF", "id": "School of Computer Science, The University of Adelaide", "label": "School of Computer Science, The University of Adelaide", "shape": "dot", "size": 25, "title": "School of Computer Science, The University of Adelaide"}, {"color": "#66CCFF", "id": "Pulak Purkait", "label": "Pulak Purkait", "shape": "dot", "size": 25, "title": "Pulak Purkait"}, {"color": "#66CCFF", "id": "School of Electrical Engineering and Computer Science, Queensland University of Technology", "label": "School of Electrical Engineering and Computer Science, Queensland University of Technology", "shape": "dot", "size": 25, "title": "School of Electrical Engineering and Computer Science, Queensland University of Technology"}, {"color": "#66CCFF", "id": "Julian Straub", "label": "Julian Straub", "shape": "dot", "size": 25, "title": "Julian Straub"}, {"color": "#66CCFF", "id": "Small-Variance Nonparametric Clustering on the Hypsphere", "label": "Small-Variance Nonparametric Clustering on the Hypsphere", "shape": "dot", "size": 25, "title": "Small-Variance Nonparametric Clustering on the Hypsphere"}, {"color": "#66CCFF", "id": "Trevor Campbell", "label": "Trevor Campbell", "shape": "dot", "size": 25, "title": "Trevor Campbell"}, {"color": "#66CCFF", "id": "Jonathan P. How", "label": "Jonathan P. How", "shape": "dot", "size": 25, "title": "Jonathan P. How"}, {"color": "#66CCFF", "id": "John W. Fisher III", "label": "John W. Fisher III", "shape": "dot", "size": 25, "title": "John W. Fisher III"}, {"color": "#66CCFF", "id": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Straub_Small-Variance_Nonparametric_Clustering_2015_CVPR_paper.pdf", "label": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Straub_Small-Variance_Nonparametric_Clustering_2015_CVPR_paper.pdf", "shape": "dot", "size": 25, "title": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Straub_Small-Variance_Nonparametric_Clustering_2015_CVPR_paper.pdf"}, {"color": "#66CCFF", "id": "Straub_Small-Variance_Nonparametric_Clustering_2015_CVPR_paper.pdf", "label": "Straub_Small-Variance_Nonparametric_Clustering_2015_CVPR_paper.pdf", "shape": "dot", "size": 25, "title": "Straub_Small-Variance_Nonparametric_Clustering_2015_CVPR_paper.pdf"}, {"color": "#66CCFF", "id": "Small-Variance Nonparametric Clustering on the Hypshere", "label": "Small-Variance Nonparametric Clustering on the Hypshere", "shape": "dot", "size": 25, "title": "Small-Variance Nonparametric Clustering on the Hypshere"}, {"color": "#66CCFF", "id": "surface normals", "label": "surface normals", "shape": "dot", "size": 25, "title": "surface normals"}, {"color": "#66CCFF", "id": "distribution of structural regularities", "label": "distribution of structural regularities", "shape": "dot", "size": 25, "title": "distribution of structural regularities"}, {"color": "#66CCFF", "id": "algorithms", "label": "algorithms", "shape": "dot", "size": 25, "title": "algorithms"}, {"color": "#66CCFF", "id": "Bayesian nonparametric von-Mises-Fisher (vMF) mixture distributions", "label": "Bayesian nonparametric von-Mises-Fisher (vMF) mixture distributions", "shape": "dot", "size": 25, "title": "Bayesian nonparametric von-Mises-Fisher (vMF) mixture distributions"}, {"color": "#66CCFF", "id": "DDP-vMF-means", "label": "DDP-vMF-means", "shape": "dot", "size": 25, "title": "DDP-vMF-means"}, {"color": "#66CCFF", "id": "temporally evolving cluster structure", "label": "temporally evolving cluster structure", "shape": "dot", "size": 25, "title": "temporally evolving cluster structure"}, {"color": "#66CCFF", "id": "geometry of directional data", "label": "geometry of directional data", "shape": "dot", "size": 25, "title": "geometry of directional data"}, {"color": "#66CCFF", "id": "directional data", "label": "directional data", "shape": "dot", "size": 25, "title": "directional data"}, {"color": "#66CCFF", "id": "unit sphere", "label": "unit sphere", "shape": "dot", "size": 25, "title": "unit sphere"}, {"color": "#66CCFF", "id": "DP-vMF-means", "label": "DP-vMF-means", "shape": "dot", "size": 25, "title": "DP-vMF-means"}, {"color": "#66CCFF", "id": "Dirichlet process (DP) vMF mixture", "label": "Dirichlet process (DP) vMF mixture", "shape": "dot", "size": 25, "title": "Dirichlet process (DP) vMF mixture"}, {"color": "#66CCFF", "id": "streaming data", "label": "streaming data", "shape": "dot", "size": 25, "title": "streaming data"}, {"color": "#66CCFF", "id": "synthetic directional data", "label": "synthetic directional data", "shape": "dot", "size": 25, "title": "synthetic directional data"}, {"color": "#66CCFF", "id": "real 3D surface normals", "label": "real 3D surface normals", "shape": "dot", "size": 25, "title": "real 3D surface normals"}, {"color": "#66CCFF", "id": "3D surface normals", "label": "3D surface normals", "shape": "dot", "size": 25, "title": "3D surface normals"}, {"color": "#66CCFF", "id": "high dimensional directional data", "label": "high dimensional directional data", "shape": "dot", "size": 25, "title": "high dimensional directional data"}, {"color": "#66CCFF", "id": "protein backbone configurations", "label": "protein backbone configurations", "shape": "dot", "size": 25, "title": "protein backbone configurations"}, {"color": "#66CCFF", "id": "semantic word vectors", "label": "semantic word vectors", "shape": "dot", "size": 25, "title": "semantic word vectors"}, {"color": "#66CCFF", "id": "Algorithms", "label": "Algorithms", "shape": "dot", "size": 25, "title": "Algorithms"}, {"color": "#66CCFF", "id": "RGB-D sensors", "label": "RGB-D sensors", "shape": "dot", "size": 25, "title": "RGB-D sensors"}, {"color": "#66CCFF", "id": "Surface Normals", "label": "Surface Normals", "shape": "dot", "size": 25, "title": "Surface Normals"}, {"color": "#66CCFF", "id": "Bayesian Nonparametric Clustering", "label": "Bayesian Nonparametric Clustering", "shape": "dot", "size": 25, "title": "Bayesian Nonparametric Clustering"}, {"color": "#66CCFF", "id": "von-Mises-Fisher Distributions", "label": "von-Mises-Fisher Distributions", "shape": "dot", "size": 25, "title": "von-Mises-Fisher Distributions"}, {"color": "#66CCFF", "id": "Streaming Data Analysis", "label": "Streaming Data Analysis", "shape": "dot", "size": 25, "title": "Streaming Data Analysis"}, {"color": "#66CCFF", "id": "Abramowitz \u0026 Stegun", "label": "Abramowitz \u0026 Stegun", "shape": "dot", "size": 25, "title": "Abramowitz \u0026 Stegun"}, {"color": "#66CCFF", "id": "mathematical background", "label": "mathematical background", "shape": "dot", "size": 25, "title": "mathematical background"}, {"color": "#66CCFF", "id": "Neal", "label": "Neal", "shape": "dot", "size": 25, "title": "Neal"}, {"color": "#66CCFF", "id": "Markov chain sampling", "label": "Markov chain sampling", "shape": "dot", "size": 25, "title": "Markov chain sampling"}, {"color": "#66CCFF", "id": "Jiang, Kulis, \u0026 Jordan", "label": "Jiang, Kulis, \u0026 Jordan", "shape": "dot", "size": 25, "title": "Jiang, Kulis, \u0026 Jordan"}, {"color": "#66CCFF", "id": "theoretical analysis", "label": "theoretical analysis", "shape": "dot", "size": 25, "title": "theoretical analysis"}, {"color": "#66CCFF", "id": "DPMMs", "label": "DPMMs", "shape": "dot", "size": 25, "title": "DPMMs"}, {"color": "#66CCFF", "id": "Jiang, K., Kulis, B., and Jordan, M.", "label": "Jiang, K., Kulis, B., and Jordan, M.", "shape": "dot", "size": 25, "title": "Jiang, K., Kulis, B., and Jordan, M."}, {"color": "#66CCFF", "id": "Latent Dirichlet Allocation", "label": "Latent Dirichlet Allocation", "shape": "dot", "size": 25, "title": "Latent Dirichlet Allocation"}, {"color": "#66CCFF", "id": "spherical topic models", "label": "spherical topic models", "shape": "dot", "size": 25, "title": "spherical topic models"}, {"color": "#66CCFF", "id": "k-means", "label": "k-means", "shape": "dot", "size": 25, "title": "k-means"}, {"color": "#66CCFF", "id": "Bayesian nonparametrics", "label": "Bayesian nonparametrics", "shape": "dot", "size": 25, "title": "Bayesian nonparametrics"}, {"color": "#66CCFF", "id": "Directional statistics", "label": "Directional statistics", "shape": "dot", "size": 25, "title": "Directional statistics"}, {"color": "#66CCFF", "id": "Mardia, K. V. and Jupp, P. E.", "label": "Mardia, K. V. and Jupp, P. E.", "shape": "dot", "size": 25, "title": "Mardia, K. V. and Jupp, P. E."}, {"color": "#66CCFF", "id": "Bayesian nonparametric methods", "label": "Bayesian nonparametric methods", "shape": "dot", "size": 25, "title": "Bayesian nonparametric methods"}, {"color": "#66CCFF", "id": "Ferguson, T.", "label": "Ferguson, T.", "shape": "dot", "size": 25, "title": "Ferguson, T."}, {"color": "#66CCFF", "id": "John Wiley \u0026 Sons", "label": "John Wiley \u0026 Sons", "shape": "dot", "size": 25, "title": "John Wiley \u0026 Sons"}, {"color": "#66CCFF", "id": "Ferguson distributions", "label": "Ferguson distributions", "shape": "dot", "size": 25, "title": "Ferguson distributions"}, {"color": "#66CCFF", "id": "p\u00b4olya urn schemes", "label": "p\u00b4olya urn schemes", "shape": "dot", "size": 25, "title": "p\u00b4olya urn schemes"}, {"color": "#66CCFF", "id": "spherical data", "label": "spherical data", "shape": "dot", "size": 25, "title": "spherical data"}, {"color": "#66CCFF", "id": "Dirichlet processes", "label": "Dirichlet processes", "shape": "dot", "size": 25, "title": "Dirichlet processes"}, {"color": "#66CCFF", "id": "Encyclopedia of Machine Learning", "label": "Encyclopedia of Machine Learning", "shape": "dot", "size": 25, "title": "Encyclopedia of Machine Learning"}, {"color": "#66CCFF", "id": "CSAIL", "label": "CSAIL", "shape": "dot", "size": 25, "title": "CSAIL"}, {"color": "#66CCFF", "id": "LIDS", "label": "LIDS", "shape": "dot", "size": 25, "title": "LIDS"}, {"color": "#66CCFF", "id": "Bayesian analysis", "label": "Bayesian analysis", "shape": "dot", "size": 25, "title": "Bayesian analysis"}, {"color": "#66CCFF", "id": "nonparametric problems", "label": "nonparametric problems", "shape": "dot", "size": 25, "title": "nonparametric problems"}, {"color": "#66CCFF", "id": "fisher@csaill.mit.edu", "label": "fisher@csaill.mit.edu", "shape": "dot", "size": 25, "title": "fisher@csaill.mit.edu"}, {"color": "#66CCFF", "id": "Dingwen Zhang", "label": "Dingwen Zhang", "shape": "dot", "size": 25, "title": "Dingwen Zhang"}, {"color": "#66CCFF", "id": "Co-Saliency Detection via Looking Deep and Wide", "label": "Co-Saliency Detection via Looking Deep and Wide", "shape": "dot", "size": 25, "title": "Co-Saliency Detection via Looking Deep and Wide"}, {"color": "#66CCFF", "id": "Junwei Han", "label": "Junwei Han", "shape": "dot", "size": 25, "title": "Junwei Han"}, {"color": "#66CCFF", "id": "Chao Li", "label": "Chao Li", "shape": "dot", "size": 25, "title": "Chao Li"}, {"color": "#66CCFF", "id": "Jingdong Wang", "label": "Jingdong Wang", "shape": "dot", "size": 25, "title": "Jingdong Wang"}, {"color": "#66CCFF", "id": "cvpr_papers/Zhang_Co-Saliency_Detection_via_2015_CVPR_paper.pdf", "label": "cvpr_papers/Zhang_Co-Saliency_Detection_via_2015_CVPR_paper.pdf", "shape": "dot", "size": 25, "title": "cvpr_papers/Zhang_Co-Saliency_Detection_via_2015_CVPR_paper.pdf"}, {"color": "#66CCFF", "id": "co-saliency detection", "label": "co-saliency detection", "shape": "dot", "size": 25, "title": "co-saliency detection"}, {"color": "#66CCFF", "id": "video foreground extraction", "label": "video foreground extraction", "shape": "dot", "size": 25, "title": "video foreground extraction"}, {"color": "#66CCFF", "id": "surveillance", "label": "surveillance", "shape": "dot", "size": 25, "title": "surveillance"}, {"color": "#66CCFF", "id": "image retrieval", "label": "image retrieval", "shape": "dot", "size": 25, "title": "image retrieval"}, {"color": "#66CCFF", "id": "image annotation", "label": "image annotation", "shape": "dot", "size": 25, "title": "image annotation"}, {"color": "#66CCFF", "id": "unified co-saliency detection framework", "label": "unified co-saliency detection framework", "shape": "dot", "size": 25, "title": "unified co-saliency detection framework"}, {"color": "#66CCFF", "id": "novel insights", "label": "novel insights", "shape": "dot", "size": 25, "title": "novel insights"}, {"color": "#66CCFF", "id": "network", "label": "network", "shape": "dot", "size": 25, "title": "network"}, {"color": "#66CCFF", "id": "representation of co-salient objects", "label": "representation of co-salient objects", "shape": "dot", "size": 25, "title": "representation of co-salient objects"}, {"color": "#66CCFF", "id": "visually similar neighbors", "label": "visually similar neighbors", "shape": "dot", "size": 25, "title": "visually similar neighbors"}, {"color": "#66CCFF", "id": "neighbors", "label": "neighbors", "shape": "dot", "size": 25, "title": "neighbors"}, {"color": "#66CCFF", "id": "common background regions", "label": "common background regions", "shape": "dot", "size": 25, "title": "common background regions"}, {"color": "#66CCFF", "id": "deep information", "label": "deep information", "shape": "dot", "size": 25, "title": "deep information"}, {"color": "#66CCFF", "id": "wide information", "label": "wide information", "shape": "dot", "size": 25, "title": "wide information"}, {"color": "#66CCFF", "id": "co-salience scores", "label": "co-salience scores", "shape": "dot", "size": 25, "title": "co-salience scores"}, {"color": "#66CCFF", "id": "intra-image contrast", "label": "intra-image contrast", "shape": "dot", "size": 25, "title": "intra-image contrast"}, {"color": "#66CCFF", "id": "intra-group consistency", "label": "intra-group consistency", "shape": "dot", "size": 25, "title": "intra-group consistency"}, {"color": "#66CCFF", "id": "window-level co-salience scores", "label": "window-level co-salience scores", "shape": "dot", "size": 25, "title": "window-level co-salience scores"}, {"color": "#66CCFF", "id": "superpixel-level co-salience maps", "label": "superpixel-level co-salience maps", "shape": "dot", "size": 25, "title": "superpixel-level co-salience maps"}, {"color": "#66CCFF", "id": "foreground region agreement strategy", "label": "foreground region agreement strategy", "shape": "dot", "size": 25, "title": "foreground region agreement strategy"}, {"color": "#66CCFF", "id": "consistent performance gain", "label": "consistent performance gain", "shape": "dot", "size": 25, "title": "consistent performance gain"}, {"color": "#66CCFF", "id": "object proposal windows", "label": "object proposal windows", "shape": "dot", "size": 25, "title": "object proposal windows"}, {"color": "#66CCFF", "id": "Bayesian formulation", "label": "Bayesian formulation", "shape": "dot", "size": 25, "title": "Bayesian formulation"}, {"color": "#66CCFF", "id": "l-level co-saliency maps", "label": "l-level co-saliency maps", "shape": "dot", "size": 25, "title": "l-level co-saliency maps"}, {"color": "#66CCFF", "id": "foreground region agreement", "label": "foreground region agreement", "shape": "dot", "size": 25, "title": "foreground region agreement"}, {"color": "#66CCFF", "id": "proposed approach", "label": "proposed approach", "shape": "dot", "size": 25, "title": "proposed approach"}, {"color": "#66CCFF", "id": "Co-salient object detection", "label": "Co-salient object detection", "shape": "dot", "size": 25, "title": "Co-salient object detection"}, {"color": "#66CCFF", "id": "multiple images", "label": "multiple images", "shape": "dot", "size": 25, "title": "multiple images"}, {"color": "#66CCFF", "id": "Unified approach", "label": "Unified approach", "shape": "dot", "size": 25, "title": "Unified approach"}, {"color": "#66CCFF", "id": "low rank matrix recovery", "label": "low rank matrix recovery", "shape": "dot", "size": 25, "title": "low rank matrix recovery"}, {"color": "#66CCFF", "id": "iCoseg", "label": "iCoseg", "shape": "dot", "size": 25, "title": "iCoseg"}, {"color": "#66CCFF", "id": "intelligent scribble guidance", "label": "intelligent scribble guidance", "shape": "dot", "size": 25, "title": "intelligent scribble guidance"}, {"color": "#66CCFF", "id": "Co-salience detection", "label": "Co-salience detection", "shape": "dot", "size": 25, "title": "Co-salience detection"}, {"color": "#66CCFF", "id": "Convolutional Neural Networks (CNNs)", "label": "Convolutional Neural Networks (CNNs)", "shape": "dot", "size": 25, "title": "Convolutional Neural Networks (CNNs)"}, {"color": "#66CCFF", "id": "Image Group Consistency", "label": "Image Group Consistency", "shape": "dot", "size": 25, "title": "Image Group Consistency"}, {"color": "#66CCFF", "id": "Bayesian Formulation", "label": "Bayesian Formulation", "shape": "dot", "size": 25, "title": "Bayesian Formulation"}, {"color": "#66CCFF", "id": "salient object detection", "label": "salient object detection", "shape": "dot", "size": 25, "title": "salient object detection"}, {"color": "#66CCFF", "id": "Visual Attention", "label": "Visual Attention", "shape": "dot", "size": 25, "title": "Visual Attention"}, {"color": "#66CCFF", "id": "co-salience detection", "label": "co-salience detection", "shape": "dot", "size": 25, "title": "co-salience detection"}, {"color": "#66CCFF", "id": "alient object detection", "label": "alient object detection", "shape": "dot", "size": 25, "title": "alient object detection"}, {"color": "#66CCFF", "id": "CVPR (Conference)", "label": "CVPR (Conference)", "shape": "dot", "size": 25, "title": "CVPR (Conference)"}, {"color": "#66CCFF", "id": "Xie, Y.", "label": "Xie, Y.", "shape": "dot", "size": 25, "title": "Xie, Y."}, {"color": "#66CCFF", "id": "Bayesian salience", "label": "Bayesian salience", "shape": "dot", "size": 25, "title": "Bayesian salience"}, {"color": "#66CCFF", "id": "low and mid level cues", "label": "low and mid level cues", "shape": "dot", "size": 25, "title": "low and mid level cues"}, {"color": "#66CCFF", "id": "discriminative regional feature integration", "label": "discriminative regional feature integration", "shape": "dot", "size": 25, "title": "discriminative regional feature integration"}, {"color": "#66CCFF", "id": "Han, J.", "label": "Han, J.", "shape": "dot", "size": 25, "title": "Han, J."}, {"color": "#66CCFF", "id": "object-oriented visual salieny detection framework", "label": "object-oriented visual salieny detection framework", "shape": "dot", "size": 25, "title": "object-oriented visual salieny detection framework"}, {"color": "#66CCFF", "id": "sparse coding representations", "label": "sparse coding representations", "shape": "dot", "size": 25, "title": "sparse coding representations"}, {"color": "#66CCFF", "id": "Rubinstein, M.", "label": "Rubinstein, M.", "shape": "dot", "size": 25, "title": "Rubinstein, M."}, {"color": "#66CCFF", "id": "joint object discovery", "label": "joint object discovery", "shape": "dot", "size": 25, "title": "joint object discovery"}, {"color": "#66CCFF", "id": "internet images", "label": "internet images", "shape": "dot", "size": 25, "title": "internet images"}, {"color": "#66CCFF", "id": "Jiang, H.", "label": "Jiang, H.", "shape": "dot", "size": 25, "title": "Jiang, H."}, {"color": "#66CCFF", "id": "salient object segmentation", "label": "salient object segmentation", "shape": "dot", "size": 25, "title": "salient object segmentation"}, {"color": "#66CCFF", "id": "context prior", "label": "context prior", "shape": "dot", "size": 25, "title": "context prior"}, {"color": "#66CCFF", "id": "IEEE Trans. Image Process.", "label": "IEEE Trans. Image Process.", "shape": "dot", "size": 25, "title": "IEEE Trans. Image Process."}, {"color": "#66CCFF", "id": "Image Processing", "label": "Image Processing", "shape": "dot", "size": 25, "title": "Image Processing"}, {"color": "#66CCFF", "id": "is_author_of", "label": "is_author_of", "shape": "dot", "size": 25, "title": "is_author_of"}, {"color": "#66CCFF", "id": "Cong Zhang", "label": "Cong Zhang", "shape": "dot", "size": 25, "title": "Cong Zhang"}, {"color": "#66CCFF", "id": "Hongsheng Li", "label": "Hongsheng Li", "shape": "dot", "size": 25, "title": "Hongsheng Li"}, {"color": "#66CCFF", "id": "Xiaogang Wang", "label": "Xiaogang Wang", "shape": "dot", "size": 25, "title": "Xiaogang Wang"}, {"color": "#66CCFF", "id": "Xiaokang Yang", "label": "Xiaokang Yang", "shape": "dot", "size": 25, "title": "Xiaokang Yang"}, {"color": "#66CCFF", "id": "Automatic salient object segmentation", "label": "Automatic salient object segmentation", "shape": "dot", "size": 25, "title": "Automatic salient object segmentation"}, {"color": "#66CCFF", "id": "Self-Adaptively Weighted Co-Saliency Detection", "label": "Self-Adaptively Weighted Co-Saliency Detection", "shape": "dot", "size": 25, "title": "Self-Adaptively Weighted Co-Saliency Detection"}, {"color": "#66CCFF", "id": "jingdw@microsoft.com", "label": "jingdw@microsoft.com", "shape": "dot", "size": 25, "title": "jingdw@microsoft.com"}, {"color": "#66CCFF", "id": "Cross-Scene Crowd Counting", "label": "Cross-Scene Crowd Counting", "shape": "dot", "size": 25, "title": "Cross-Scene Crowd Counting"}, {"color": "#66CCFF", "id": "Zhang_Cross-Scene_Crowd_Counting_2015_CVPR_paper.pdf", "label": "Zhang_Cross-Scene_Crowd_Counting_2015_CVPR_paper.pdf", "shape": "dot", "size": 25, "title": "Zhang_Cross-Scene_Crowd_Counting_2015_CVPR_paper.pdf"}, {"color": "#66CCFF", "id": "Cross-scene crowd counting", "label": "Cross-scene crowd counting", "shape": "dot", "size": 25, "title": "Cross-scene crowd counting"}, {"color": "#66CCFF", "id": "crowd counting", "label": "crowd counting", "shape": "dot", "size": 25, "title": "crowd counting"}, {"color": "#66CCFF", "id": "no laborious data annotation", "label": "no laborious data annotation", "shape": "dot", "size": 25, "title": "no laborious data annotation"}, {"color": "#66CCFF", "id": "existing crowd counting methods", "label": "existing crowd counting methods", "shape": "dot", "size": 25, "title": "existing crowd counting methods"}, {"color": "#66CCFF", "id": "significant performance drop", "label": "significant performance drop", "shape": "dot", "size": 25, "title": "significant performance drop"}, {"color": "#66CCFF", "id": "deep convolutional neural network (CNN)", "label": "deep convolutional neural network (CNN)", "shape": "dot", "size": 25, "title": "deep convolutional neural network (CNN)"}, {"color": "#66CCFF", "id": "crowd density", "label": "crowd density", "shape": "dot", "size": 25, "title": "crowd density"}, {"color": "#66CCFF", "id": "switchable learning approach", "label": "switchable learning approach", "shape": "dot", "size": 25, "title": "switchable learning approach"}, {"color": "#66CCFF", "id": "better local optimum", "label": "better local optimum", "shape": "dot", "size": 25, "title": "better local optimum"}, {"color": "#66CCFF", "id": "data-driven method", "label": "data-driven method", "shape": "dot", "size": 25, "title": "data-driven method"}, {"color": "#66CCFF", "id": "fine-tune CNN model", "label": "fine-tune CNN model", "shape": "dot", "size": 25, "title": "fine-tune CNN model"}, {"color": "#66CCFF", "id": "108 crowd scenes", "label": "108 crowd scenes", "shape": "dot", "size": 25, "title": "108 crowd scenes"}, {"color": "#66CCFF", "id": "nearly 200,000 head", "label": "nearly 200,000 head", "shape": "dot", "size": 25, "title": "nearly 200,000 head"}, {"color": "#66CCFF", "id": "unseen target crowd scene", "label": "unseen target crowd scene", "shape": "dot", "size": 25, "title": "unseen target crowd scene"}, {"color": "#66CCFF", "id": "trained CNN model", "label": "trained CNN model", "shape": "dot", "size": 25, "title": "trained CNN model"}, {"color": "#66CCFF", "id": "200,000 head annotations", "label": "200,000 head annotations", "shape": "dot", "size": 25, "title": "200,000 head annotations"}, {"color": "#66CCFF", "id": "cross-scene crowd counting methods", "label": "cross-scene crowd counting methods", "shape": "dot", "size": 25, "title": "cross-scene crowd counting methods"}, {"color": "#66CCFF", "id": "reliability", "label": "reliability", "shape": "dot", "size": 25, "title": "reliability"}, {"color": "#66CCFF", "id": "CNN model", "label": "CNN model", "shape": "dot", "size": 25, "title": "CNN model"}, {"color": "#66CCFF", "id": "trained", "label": "trained", "shape": "dot", "size": 25, "title": "trained"}, {"color": "#66CCFF", "id": "new", "label": "new", "shape": "dot", "size": 25, "title": "new"}, {"color": "#66CCFF", "id": "Crowd Counting", "label": "Crowd Counting", "shape": "dot", "size": 25, "title": "Crowd Counting"}, {"color": "#66CCFF", "id": "Deep Convolutional Neural Networks (CNNs)", "label": "Deep Convolutional Neural Networks (CNNs)", "shape": "dot", "size": 25, "title": "Deep Convolutional Neural Networks (CNNs)"}, {"color": "#66CCFF", "id": "object counting", "label": "object counting", "shape": "dot", "size": 25, "title": "object counting"}, {"color": "#66CCFF", "id": "Chen et al. (2013)", "label": "Chen et al. (2013)", "shape": "dot", "size": 25, "title": "Chen et al. (2013)"}, {"color": "#66CCFF", "id": "cumulative attribute space", "label": "cumulative attribute space", "shape": "dot", "size": 25, "title": "cumulative attribute space"}, {"color": "#66CCFF", "id": "crowd density estimation", "label": "crowd density estimation", "shape": "dot", "size": 25, "title": "crowd density estimation"}, {"color": "#66CCFF", "id": "Lempitsky \u0026 Zisserman (2010)", "label": "Lempitsky \u0026 Zisserman (2010)", "shape": "dot", "size": 25, "title": "Lempitsky \u0026 Zisserman (2010)"}, {"color": "#66CCFF", "id": "Chen et al. (2012)", "label": "Chen et al. (2012)", "shape": "dot", "size": 25, "title": "Chen et al. (2012)"}, {"color": "#66CCFF", "id": "feature mining", "label": "feature mining", "shape": "dot", "size": 25, "title": "feature mining"}, {"color": "#66CCFF", "id": "localized crowd counting", "label": "localized crowd counting", "shape": "dot", "size": 25, "title": "localized crowd counting"}, {"color": "#66CCFF", "id": "Loy et al. (2012) research", "label": "Loy et al. (2012) research", "shape": "dot", "size": 25, "title": "Loy et al. (2012) research"}, {"color": "#66CCFF", "id": "An et al. (2007) research", "label": "An et al. (2007) research", "shape": "dot", "size": 25, "title": "An et al. (2007) research"}, {"color": "#66CCFF", "id": "kernel ridge regression", "label": "kernel ridge regression", "shape": "dot", "size": 25, "title": "kernel ridge regression"}, {"color": "#66CCFF", "id": "vision tasks", "label": "vision tasks", "shape": "dot", "size": 25, "title": "vision tasks"}, {"color": "#66CCFF", "id": "Kai et al. (2014) research", "label": "Kai et al. (2014) research", "shape": "dot", "size": 25, "title": "Kai et al. (2014) research"}, {"color": "#66CCFF", "id": "fully convolutional network", "label": "fully convolutional network", "shape": "dot", "size": 25, "title": "fully convolutional network"}, {"color": "#66CCFF", "id": "crowd segmentation", "label": "crowd segmentation", "shape": "dot", "size": 25, "title": "crowd segmentation"}, {"color": "#66CCFF", "id": "Kong et al. (2006) research", "label": "Kong et al. (2006) research", "shape": "dot", "size": 25, "title": "Kong et al. (2006) research"}, {"color": "#66CCFF", "id": "viewpoint invariance", "label": "viewpoint invariance", "shape": "dot", "size": 25, "title": "viewpoint invariance"}, {"color": "#66CCFF", "id": "neural network", "label": "neural network", "shape": "dot", "size": 25, "title": "neural network"}, {"color": "#66CCFF", "id": "Kong et al. (2006)", "label": "Kong et al. (2006)", "shape": "dot", "size": 25, "title": "Kong et al. (2006)"}, {"color": "#66CCFF", "id": "viewpoint invariance in crowd counting", "label": "viewpoint invariance in crowd counting", "shape": "dot", "size": 25, "title": "viewpoint invariance in crowd counting"}, {"color": "#66CCFF", "id": "practical consideration", "label": "practical consideration", "shape": "dot", "size": 25, "title": "practical consideration"}, {"color": "#66CCFF", "id": "Jing et al. (2015)", "label": "Jing et al. (2015)", "shape": "dot", "size": 25, "title": "Jing et al. (2015)"}, {"color": "#66CCFF", "id": "deep learning for attribute extraction", "label": "deep learning for attribute extraction", "shape": "dot", "size": 25, "title": "deep learning for attribute extraction"}, {"color": "#66CCFF", "id": "attribute extraction", "label": "attribute extraction", "shape": "dot", "size": 25, "title": "attribute extraction"}, {"color": "#66CCFF", "id": "crowd scene understanding", "label": "crowd scene understanding", "shape": "dot", "size": 25, "title": "crowd scene understanding"}, {"color": "#66CCFF", "id": "Fiaschi et al. (2012)", "label": "Fiaschi et al. (2012)", "shape": "dot", "size": 25, "title": "Fiaschi et al. (2012)"}, {"color": "#66CCFF", "id": "regression forest for counting", "label": "regression forest for counting", "shape": "dot", "size": 25, "title": "regression forest for counting"}, {"color": "#66CCFF", "id": "neural networks", "label": "neural networks", "shape": "dot", "size": 25, "title": "neural networks"}, {"color": "#66CCFF", "id": "Loy et al. (2013)", "label": "Loy et al. (2013)", "shape": "dot", "size": 25, "title": "Loy et al. (2013)"}, {"color": "#66CCFF", "id": "semi-supervised learning for crowd counting", "label": "semi-supervised learning for crowd counting", "shape": "dot", "size": 25, "title": "semi-supervised learning for crowd counting"}, {"color": "#66CCFF", "id": "transfer learning for crowd counting", "label": "transfer learning for crowd counting", "shape": "dot", "size": 25, "title": "transfer learning for crowd counting"}, {"color": "#66CCFF", "id": "ICPR", "label": "ICPR", "shape": "dot", "size": 25, "title": "ICPR"}, {"color": "#66CCFF", "id": "conference", "label": "conference", "shape": "dot", "size": 25, "title": "conference"}, {"color": "#66CCFF", "id": "ICCV", "label": "ICCV", "shape": "dot", "size": 25, "title": "ICCV"}, {"color": "#66CCFF", "id": "Gong, S.", "label": "Gong, S.", "shape": "dot", "size": 25, "title": "Gong, S."}, {"color": "#66CCFF", "id": "From semi-supervised to transfer counting of crowds", "label": "From semi-supervised to transfer counting of crowds", "shape": "dot", "size": 25, "title": "From semi-supervised to transfer counting of crowds"}, {"color": "#66CCFF", "id": "Xiang, T.", "label": "Xiang, T.", "shape": "dot", "size": 25, "title": "Xiang, T."}, {"color": "#66CCFF", "id": "Lowe, D. G.", "label": "Lowe, D. G.", "shape": "dot", "size": 25, "title": "Lowe, D. G."}, {"color": "#66CCFF", "id": "Distinctive image features from scale-invariant keypoints", "label": "Distinctive image features from scale-invariant keypoints", "shape": "dot", "size": 25, "title": "Distinctive image features from scale-invariant keypoints"}, {"color": "#66CCFF", "id": "SIFT features", "label": "SIFT features", "shape": "dot", "size": 25, "title": "SIFT features"}, {"color": "#66CCFF", "id": "crowd analysis", "label": "crowd analysis", "shape": "dot", "size": 25, "title": "crowd analysis"}, {"color": "#66CCFF", "id": "Institute of Image Communication and Network Engineering, Shanghai Jiao Tong University", "label": "Institute of Image Communication and Network Engineering, Shanghai Jiao Tong University", "shape": "dot", "size": 25, "title": "Institute of Image Communication and Network Engineering, Shanghai Jiao Tong University"}, {"color": "#66CCFF", "id": "Department of Electronic Engineering, The Chinese University of Hong Kong", "label": "Department of Electronic Engineering, The Chinese University of Hong Kong", "shape": "dot", "size": 25, "title": "Department of Electronic Engineering, The Chinese University of Hong Kong"}, {"color": "#66CCFF", "id": "Chinese University of Hong Kong", "label": "Chinese University of Hong Kong", "shape": "dot", "size": 25, "title": "Chinese University of Hong Kong"}, {"color": "#66CCFF", "id": "xgwang@ee.cuhk.edu.hk", "label": "xgwang@ee.cuhk.edu.hk", "shape": "dot", "size": 25, "title": "xgwang@ee.cuhk.edu.hk"}, {"color": "#66CCFF", "id": "Yongzhen Huang", "label": "Yongzhen Huang", "shape": "dot", "size": 25, "title": "Yongzhen Huang"}, {"color": "#66CCFF", "id": "Deep SemanticRanking Based Hashing", "label": "Deep SemanticRanking Based Hashing", "shape": "dot", "size": 25, "title": "Deep SemanticRanking Based Hashing"}, {"color": "#66CCFF", "id": "Liang Wang", "label": "Liang Wang", "shape": "dot", "size": 25, "title": "Liang Wang"}, {"color": "#66CCFF", "id": "Deep Semantic Ranking Based Hashing", "label": "Deep Semantic Ranking Based Hashing", "shape": "dot", "size": 25, "title": "Deep Semantic Ranking Based Hashing"}, {"color": "#66CCFF", "id": "Tieniu Tan", "label": "Tieniu Tan", "shape": "dot", "size": 25, "title": "Tieniu Tan"}, {"color": "#66CCFF", "id": "Institute of Image Communication and Network Engineering", "label": "Institute of Image Communication and Network Engineering", "shape": "dot", "size": 25, "title": "Institute of Image Communication and Network Engineering"}, {"color": "#66CCFF", "id": "Shanghai Jiao Tong University", "label": "Shanghai Jiao Tong University", "shape": "dot", "size": 25, "title": "Shanghai Jiao Tong University"}, {"color": "#66CCFF", "id": "xk yang@sjtu.edu.cn", "label": "xk yang@sjtu.edu.cn", "shape": "dot", "size": 25, "title": "xk yang@sjtu.edu.cn"}, {"color": "#66CCFF", "id": "Multi-Label Image Retrieval", "label": "Multi-Label Image Retrieval", "shape": "dot", "size": 25, "title": "Multi-Label Image Retrieval"}, {"color": "#66CCFF", "id": "deep hash functions", "label": "deep hash functions", "shape": "dot", "size": 25, "title": "deep hash functions"}, {"color": "#66CCFF", "id": "hand-crafted features", "label": "hand-crafted features", "shape": "dot", "size": 25, "title": "hand-crafted features"}, {"color": "#66CCFF", "id": "ranking list", "label": "ranking list", "shape": "dot", "size": 25, "title": "ranking list"}, {"color": "#66CCFF", "id": "multilevel similarity information", "label": "multilevel similarity information", "shape": "dot", "size": 25, "title": "multilevel similarity information"}, {"color": "#66CCFF", "id": "state-of-the-art hashing methods", "label": "state-of-the-art hashing methods", "shape": "dot", "size": 25, "title": "state-of-the-art hashing methods"}, {"color": "#66CCFF", "id": "semantic representation", "label": "semantic representation", "shape": "dot", "size": 25, "title": "semantic representation"}, {"color": "#66CCFF", "id": "hash codes", "label": "hash codes", "shape": "dot", "size": 25, "title": "hash codes"}, {"color": "#66CCFF", "id": "Deep Hash Functions", "label": "Deep Hash Functions", "shape": "dot", "size": 25, "title": "Deep Hash Functions"}, {"color": "#66CCFF", "id": "Similarity Information", "label": "Similarity Information", "shape": "dot", "size": 25, "title": "Similarity Information"}, {"color": "#66CCFF", "id": "Proposed Approach", "label": "Proposed Approach", "shape": "dot", "size": 25, "title": "Proposed Approach"}, {"color": "#66CCFF", "id": "Hashing Methods", "label": "Hashing Methods", "shape": "dot", "size": 25, "title": "Hashing Methods"}, {"color": "#66CCFF", "id": "ImageNet Classification", "label": "ImageNet Classification", "shape": "dot", "size": 25, "title": "ImageNet Classification"}, {"color": "#66CCFF", "id": "Deep Convolutional Ranking", "label": "Deep Convolutional Ranking", "shape": "dot", "size": 25, "title": "Deep Convolutional Ranking"}, {"color": "#66CCFF", "id": "Multi-label Image Annotation", "label": "Multi-label Image Annotation", "shape": "dot", "size": 25, "title": "Multi-label Image Annotation"}, {"color": "#66CCFF", "id": "Iterative Quantization", "label": "Iterative Quantization", "shape": "dot", "size": 25, "title": "Iterative Quantization"}, {"color": "#66CCFF", "id": "Binary Codes", "label": "Binary Codes", "shape": "dot", "size": 25, "title": "Binary Codes"}, {"color": "#66CCFF", "id": "Image Retrieval", "label": "Image Retrieval", "shape": "dot", "size": 25, "title": "Image Retrieval"}, {"color": "#66CCFF", "id": "Krizhevsky", "label": "Krizhevsky", "shape": "dot", "size": 25, "title": "Krizhevsky"}, {"color": "#66CCFF", "id": "Gong", "label": "Gong", "shape": "dot", "size": 25, "title": "Gong"}, {"color": "#66CCFF", "id": "Perronnin", "label": "Perronnin", "shape": "dot", "size": 25, "title": "Perronnin"}, {"color": "#66CCFF", "id": "Iterative quantization", "label": "Iterative quantization", "shape": "dot", "size": 25, "title": "Iterative quantization"}, {"color": "#66CCFF", "id": "binary codes", "label": "binary codes", "shape": "dot", "size": 25, "title": "binary codes"}, {"color": "#66CCFF", "id": "Norouzi", "label": "Norouzi", "shape": "dot", "size": 25, "title": "Norouzi"}, {"color": "#66CCFF", "id": "Hamming distance metric learning", "label": "Hamming distance metric learning", "shape": "dot", "size": 25, "title": "Hamming distance metric learning"}, {"color": "#66CCFF", "id": "Semi-supervised hashing", "label": "Semi-supervised hashing", "shape": "dot", "size": 25, "title": "Semi-supervised hashing"}, {"color": "#66CCFF", "id": "scalable image retrieval", "label": "scalable image retrieval", "shape": "dot", "size": 25, "title": "scalable image retrieval"}, {"color": "#66CCFF", "id": "Torralba", "label": "Torralba", "shape": "dot", "size": 25, "title": "Torralba"}, {"color": "#66CCFF", "id": "Small codes", "label": "Small codes", "shape": "dot", "size": 25, "title": "Small codes"}, {"color": "#66CCFF", "id": "image databases", "label": "image databases", "shape": "dot", "size": 25, "title": "image databases"}, {"color": "#66CCFF", "id": "Deep convolutional ranking", "label": "Deep convolutional ranking", "shape": "dot", "size": 25, "title": "Deep convolutional ranking"}, {"color": "#66CCFF", "id": "multilabel image annotation", "label": "multilabel image annotation", "shape": "dot", "size": 25, "title": "multilabel image annotation"}, {"color": "#66CCFF", "id": "Gong, Y.", "label": "Gong, Y.", "shape": "dot", "size": 25, "title": "Gong, Y."}, {"color": "#66CCFF", "id": "multilable image annotation", "label": "multilable image annotation", "shape": "dot", "size": 25, "title": "multilable image annotation"}, {"color": "#66CCFF", "id": "Krizhevsky, A.", "label": "Krizhevsky, A.", "shape": "dot", "size": 25, "title": "Krizhevsky, A."}, {"color": "#66CCFF", "id": "One weird trick", "label": "One weird trick", "shape": "dot", "size": 25, "title": "One weird trick"}, {"color": "#66CCFF", "id": "parallelizing convolutional neural networks", "label": "parallelizing convolutional neural networks", "shape": "dot", "size": 25, "title": "parallelizing convolutional neural networks"}, {"color": "#66CCFF", "id": "Minimal loss hashing", "label": "Minimal loss hashing", "shape": "dot", "size": 25, "title": "Minimal loss hashing"}, {"color": "#66CCFF", "id": "compact binary codes", "label": "compact binary codes", "shape": "dot", "size": 25, "title": "compact binary codes"}, {"color": "#66CCFF", "id": "Lin, G.", "label": "Lin, G.", "shape": "dot", "size": 25, "title": "Lin, G."}, {"color": "#66CCFF", "id": "Optimizing ranking measures", "label": "Optimizing ranking measures", "shape": "dot", "size": 25, "title": "Optimizing ranking measures"}, {"color": "#66CCFF", "id": "compact binary code learning", "label": "compact binary code learning", "shape": "dot", "size": 25, "title": "compact binary code learning"}, {"color": "#66CCFF", "id": "Fang Zhao", "label": "Fang Zhao", "shape": "dot", "size": 25, "title": "Fang Zhao"}, {"color": "#66CCFF", "id": "Center for Research on Intelligent Perception and Computing", "label": "Center for Research on Intelligent Perception and Computing", "shape": "dot", "size": 25, "title": "Center for Research on Intelligent Perception and Computing"}, {"color": "#66CCFF", "id": "Institute of Automation", "label": "Institute of Automation", "shape": "dot", "size": 25, "title": "Institute of Automation"}, {"color": "#66CCFF", "id": "David Perra", "label": "David Perra", "shape": "dot", "size": 25, "title": "David Perra"}, {"color": "#66CCFF", "id": "Adaptive Eye-Camera Calibration", "label": "Adaptive Eye-Camera Calibration", "shape": "dot", "size": 25, "title": "Adaptive Eye-Camera Calibration"}, {"color": "#66CCFF", "id": "Rohit Kumar Gupta", "label": "Rohit Kumar Gupta", "shape": "dot", "size": 25, "title": "Rohit Kumar Gupta"}, {"color": "#66CCFF", "id": "fang.zhao@nlpr.ia.ac.cn", "label": "fang.zhao@nlpr.ia.ac.cn", "shape": "dot", "size": 25, "title": "fang.zhao@nlpr.ia.ac.cn"}, {"color": "#66CCFF", "id": "Perra_Adaptive_Eye-Camera_Calibration_2015_CVPR_paper.pdf", "label": "Perra_Adaptive_Eye-Camera_Calibration_2015_CVPR_paper.pdf", "shape": "dot", "size": 25, "title": "Perra_Adaptive_Eye-Camera_Calibration_2015_CVPR_paper.pdf"}, {"color": "#66CCFF", "id": "calibration scheme", "label": "calibration scheme", "shape": "dot", "size": 25, "title": "calibration scheme"}, {"color": "#66CCFF", "id": "globally optimal model", "label": "globally optimal model", "shape": "dot", "size": 25, "title": "globally optimal model"}, {"color": "#66CCFF", "id": "calibration schemes", "label": "calibration schemes", "shape": "dot", "size": 25, "title": "calibration schemes"}, {"color": "#66CCFF", "id": "per-user basis", "label": "per-user basis", "shape": "dot", "size": 25, "title": "per-user basis"}, {"color": "#66CCFF", "id": "changes in calibration", "label": "changes in calibration", "shape": "dot", "size": 25, "title": "changes in calibration"}, {"color": "#66CCFF", "id": "locally optimal eye-device transformation", "label": "locally optimal eye-device transformation", "shape": "dot", "size": 25, "title": "locally optimal eye-device transformation"}, {"color": "#66CCFF", "id": "local window of previous frames", "label": "local window of previous frames", "shape": "dot", "size": 25, "title": "local window of previous frames"}, {"color": "#66CCFF", "id": "interest regions", "label": "interest regions", "shape": "dot", "size": 25, "title": "interest regions"}, {"color": "#66CCFF", "id": "user\u0027s passive participation", "label": "user\u0027s passive participation", "shape": "dot", "size": 25, "title": "user\u0027s passive participation"}, {"color": "#66CCFF", "id": "continuous", "label": "continuous", "shape": "dot", "size": 25, "title": "continuous"}, {"color": "#66CCFF", "id": "locally optimal", "label": "locally optimal", "shape": "dot", "size": 25, "title": "locally optimal"}, {"color": "#66CCFF", "id": "eye-device transformation", "label": "eye-device transformation", "shape": "dot", "size": 25, "title": "eye-device transformation"}, {"color": "#66CCFF", "id": "user\u2019s environment", "label": "user\u2019s environment", "shape": "dot", "size": 25, "title": "user\u2019s environment"}, {"color": "#66CCFF", "id": "without user participation", "label": "without user participation", "shape": "dot", "size": 25, "title": "without user participation"}, {"color": "#66CCFF", "id": "proposed calibration scheme", "label": "proposed calibration scheme", "shape": "dot", "size": 25, "title": "proposed calibration scheme"}, {"color": "#66CCFF", "id": "existing state of the art systems", "label": "existing state of the art systems", "shape": "dot", "size": 25, "title": "existing state of the art systems"}, {"color": "#66CCFF", "id": "less restrictive", "label": "less restrictive", "shape": "dot", "size": 25, "title": "less restrictive"}, {"color": "#66CCFF", "id": "environment", "label": "environment", "shape": "dot", "size": 25, "title": "environment"}, {"color": "#66CCFF", "id": "calibration", "label": "calibration", "shape": "dot", "size": 25, "title": "calibration"}, {"color": "#66CCFF", "id": "state of the art systems", "label": "state of the art systems", "shape": "dot", "size": 25, "title": "state of the art systems"}, {"color": "#66CCFF", "id": "Alnajar et al.", "label": "Alnajar et al.", "shape": "dot", "size": 25, "title": "Alnajar et al."}, {"color": "#66CCFF", "id": "Calibration-free gaze estimation", "label": "Calibration-free gaze estimation", "shape": "dot", "size": 25, "title": "Calibration-free gaze estimation"}, {"color": "#66CCFF", "id": "Chen and Ji", "label": "Chen and Ji", "shape": "dot", "size": 25, "title": "Chen and Ji"}, {"color": "#66CCFF", "id": "Probabilistic gaze estimation", "label": "Probabilistic gaze estimation", "shape": "dot", "size": 25, "title": "Probabilistic gaze estimation"}, {"color": "#66CCFF", "id": "Corno et al.", "label": "Corno et al.", "shape": "dot", "size": 25, "title": "Corno et al."}, {"color": "#66CCFF", "id": "cost-effective solution", "label": "cost-effective solution", "shape": "dot", "size": 25, "title": "cost-effective solution"}, {"color": "#66CCFF", "id": "human gaze patterns", "label": "human gaze patterns", "shape": "dot", "size": 25, "title": "human gaze patterns"}, {"color": "#66CCFF", "id": "active personal calibration", "label": "active personal calibration", "shape": "dot", "size": 25, "title": "active personal calibration"}, {"color": "#66CCFF", "id": "eye-gaze assistive technology", "label": "eye-gaze assistive technology", "shape": "dot", "size": 25, "title": "eye-gaze assistive technology"}, {"color": "#66CCFF", "id": "eye-camera calibration", "label": "eye-camera calibration", "shape": "dot", "size": 25, "title": "eye-camera calibration"}, {"color": "#66CCFF", "id": "gaze tracking", "label": "gaze tracking", "shape": "dot", "size": 25, "title": "gaze tracking"}, {"color": "#66CCFF", "id": "F. Corno", "label": "F. Corno", "shape": "dot", "size": 25, "title": "F. Corno"}, {"color": "#66CCFF", "id": "IEEE International Conference on Multimedia and Expo", "label": "IEEE International Conference on Multimedia and Expo", "shape": "dot", "size": 25, "title": "IEEE International Conference on Multimedia and Expo"}, {"color": "#66CCFF", "id": "E. Guestrin", "label": "E. Guestrin", "shape": "dot", "size": 25, "title": "E. Guestrin"}, {"color": "#66CCFF", "id": "remote gaze estimation", "label": "remote gaze estimation", "shape": "dot", "size": 25, "title": "remote gaze estimation"}, {"color": "#66CCFF", "id": "D. Hansen", "label": "D. Hansen", "shape": "dot", "size": 25, "title": "D. Hansen"}, {"color": "#66CCFF", "id": "models for eyes and gaze", "label": "models for eyes and gaze", "shape": "dot", "size": 25, "title": "models for eyes and gaze"}, {"color": "#66CCFF", "id": "J. Harel", "label": "J. Harel", "shape": "dot", "size": 25, "title": "J. Harel"}, {"color": "#66CCFF", "id": "graph-based visual saliency", "label": "graph-based visual saliency", "shape": "dot", "size": 25, "title": "graph-based visual saliency"}, {"color": "#66CCFF", "id": "X. Hou", "label": "X. Hou", "shape": "dot", "size": 25, "title": "X. Hou"}, {"color": "#66CCFF", "id": "image signature", "label": "image signature", "shape": "dot", "size": 25, "title": "image signature"}, {"color": "#66CCFF", "id": "pupil center", "label": "pupil center", "shape": "dot", "size": 25, "title": "pupil center"}, {"color": "#66CCFF", "id": "neural processing method", "label": "neural processing method", "shape": "dot", "size": 25, "title": "neural processing method"}, {"color": "#66CCFF", "id": "sparse salient regions", "label": "sparse salient regions", "shape": "dot", "size": 25, "title": "sparse salient regions"}, {"color": "#66CCFF", "id": "IEEE Transactions on Biomedical Engineering", "label": "IEEE Transactions on Biomedical Engineering", "shape": "dot", "size": 25, "title": "IEEE Transactions on Biomedical Engineering"}, {"color": "#66CCFF", "id": "research on remote gaze estimation", "label": "research on remote gaze estimation", "shape": "dot", "size": 25, "title": "research on remote gaze estimation"}, {"color": "#66CCFF", "id": "research on assistive technology", "label": "research on assistive technology", "shape": "dot", "size": 25, "title": "research on assistive technology"}, {"color": "#66CCFF", "id": "U. Lahiri", "label": "U. Lahiri", "shape": "dot", "size": 25, "title": "U. Lahiri"}, {"color": "#66CCFF", "id": "Neural Systems and Rehabilitation Engineering, IEEE Transactions on", "label": "Neural Systems and Rehabilitation Engineering, IEEE Transactions on", "shape": "dot", "size": 25, "title": "Neural Systems and Rehabilitation Engineering, IEEE Transactions on"}, {"color": "#66CCFF", "id": "Virtual Rehabilitation (ICVR)", "label": "Virtual Rehabilitation (ICVR)", "shape": "dot", "size": 25, "title": "Virtual Rehabilitation (ICVR)"}, {"color": "#66CCFF", "id": "R. Kumar", "label": "R. Kumar", "shape": "dot", "size": 25, "title": "R. Kumar"}, {"color": "#66CCFF", "id": "Computer Vision and Pattern Recognition", "label": "Computer Vision and Pattern Recognition", "shape": "dot", "size": 25, "title": "Computer Vision and Pattern Recognition"}, {"color": "#66CCFF", "id": "Google Inc.", "label": "Google Inc.", "shape": "dot", "size": 25, "title": "Google Inc."}, {"color": "#66CCFF", "id": "The University of North Carolina at Chapel Hill", "label": "The University of North Carolina at Chapel Hill", "shape": "dot", "size": 25, "title": "The University of North Carolina at Chapel Hill"}, {"color": "#66CCFF", "id": "Jan-Micheal Frahm", "label": "Jan-Micheal Frahm", "shape": "dot", "size": 25, "title": "Jan-Micheal Frahm"}, {"color": "#66CCFF", "id": "The University of North Carolian at Chapel Hill", "label": "The University of North Carolian at Chapel Hill", "shape": "dot", "size": 25, "title": "The University of North Carolian at Chapel Hill"}, {"color": "#66CCFF", "id": "children with autism", "label": "children with autism", "shape": "dot", "size": 25, "title": "children with autism"}, {"color": "#66CCFF", "id": "social communication", "label": "social communication", "shape": "dot", "size": 25, "title": "social communication"}, {"color": "#66CCFF", "id": "rkgupta@cs.unc.edu", "label": "rkgupta@cs.unc.edu", "shape": "dot", "size": 25, "title": "rkgupta@cs.unc.edu"}, {"color": "#66CCFF", "id": "jmf@cs.unc.edu", "label": "jmf@cs.unc.edu", "shape": "dot", "size": 25, "title": "jmf@cs.unc.edu"}, {"color": "#66CCFF", "id": "Nianuan Jiang", "label": "Nianuan Jiang", "shape": "dot", "size": 25, "title": "Nianuan Jiang"}, {"color": "#66CCFF", "id": "Direct Structure Estimation for 3D Reconstruction", "label": "Direct Structure Estimation for 3D Reconstruction", "shape": "dot", "size": 25, "title": "Direct Structure Estimation for 3D Reconstruction"}, {"color": "#66CCFF", "id": "Wen-Yan Lin", "label": "Wen-Yan Lin", "shape": "dot", "size": 25, "title": "Wen-Yan Lin"}, {"color": "#66CCFF", "id": "Direct Structure Estimated for 3D Reconstruction", "label": "Direct Structure Estimated for 3D Reconstruction", "shape": "dot", "size": 25, "title": "Direct Structure Estimated for 3D Reconstruction"}, {"color": "#66CCFF", "id": "Minh N. Do", "label": "Minh N. Do", "shape": "dot", "size": 25, "title": "Minh N. Do"}, {"color": "#66CCFF", "id": "Jiangbo Lu", "label": "Jiangbo Lu", "shape": "dot", "size": 25, "title": "Jiangbo Lu"}, {"color": "#66CCFF", "id": "Jiang_Direct_Structure_Estimation_2015_CVPR_paper.pdf", "label": "Jiang_Direct_Structure_Estimation_2015_CVPR_paper.pdf", "shape": "dot", "size": 25, "title": "Jiang_Direct_Structure_Estimation_2015_CVPR_paper.pdf"}, {"color": "#66CCFF", "id": "Structure from Motion (SFM)", "label": "Structure from Motion (SFM)", "shape": "dot", "size": 25, "title": "Structure from Motion (SFM)"}, {"color": "#66CCFF", "id": "camera pose estimation", "label": "camera pose estimation", "shape": "dot", "size": 25, "title": "camera pose estimation"}, {"color": "#66CCFF", "id": "Euclidean Rigidity", "label": "Euclidean Rigidity", "shape": "dot", "size": 25, "title": "Euclidean Rigidity"}, {"color": "#66CCFF", "id": "Direct StructureEstimation (DSE)", "label": "Direct StructureEstimation (DSE)", "shape": "dot", "size": 25, "title": "Direct StructureEstimation (DSE)"}, {"color": "#66CCFF", "id": "homography estimation", "label": "homography estimation", "shape": "dot", "size": 25, "title": "homography estimation"}, {"color": "#66CCFF", "id": "Direct Structure Estimation (DSE)", "label": "Direct Structure Estimation (DSE)", "shape": "dot", "size": 25, "title": "Direct Structure Estimation (DSE)"}, {"color": "#66CCFF", "id": "formulation for scene structure recovery", "label": "formulation for scene structure recovery", "shape": "dot", "size": 25, "title": "formulation for scene structure recovery"}, {"color": "#66CCFF", "id": "scene structure recovery", "label": "scene structure recovery", "shape": "dot", "size": 25, "title": "scene structure recovery"}, {"color": "#66CCFF", "id": "recovering scene structure", "label": "recovering scene structure", "shape": "dot", "size": 25, "title": "recovering scene structure"}, {"color": "#66CCFF", "id": "recovering camera poses", "label": "recovering camera poses", "shape": "dot", "size": 25, "title": "recovering camera poses"}, {"color": "#66CCFF", "id": "scene structure", "label": "scene structure", "shape": "dot", "size": 25, "title": "scene structure"}, {"color": "#66CCFF", "id": "sideway motion", "label": "sideway motion", "shape": "dot", "size": 25, "title": "sideway motion"}, {"color": "#66CCFF", "id": "planar or general man-made scenes", "label": "planar or general man-made scenes", "shape": "dot", "size": 25, "title": "planar or general man-made scenes"}, {"color": "#66CCFF", "id": "scene reconstruction", "label": "scene reconstruction", "shape": "dot", "size": 25, "title": "scene reconstruction"}, {"color": "#66CCFF", "id": "Homography Estimation", "label": "Homography Estimation", "shape": "dot", "size": 25, "title": "Homography Estimation"}, {"color": "#66CCFF", "id": "Camera Pose Estimation", "label": "Camera Pose Estimation", "shape": "dot", "size": 25, "title": "Camera Pose Estimation"}, {"color": "#66CCFF", "id": "D. Nist\u00e9r", "label": "D. Nist\u00e9r", "shape": "dot", "size": 25, "title": "D. Nist\u00e9r"}, {"color": "#66CCFF", "id": "solution to five-point relative pose problem", "label": "solution to five-point relative pose problem", "shape": "dot", "size": 25, "title": "solution to five-point relative pose problem"}, {"color": "#66CCFF", "id": "D. G. Aliaga", "label": "D. G. Aliaga", "shape": "dot", "size": 25, "title": "D. G. Aliaga"}, {"color": "#66CCFF", "id": "simplifying reconstruction of 3d models", "label": "simplifying reconstruction of 3d models", "shape": "dot", "size": 25, "title": "simplifying reconstruction of 3d models"}, {"color": "#66CCFF", "id": "K. S. Arun", "label": "K. S. Arun", "shape": "dot", "size": 25, "title": "K. S. Arun"}, {"color": "#66CCFF", "id": "least-squares fitting of two 3-d point sets", "label": "least-squares fitting of two 3-d point sets", "shape": "dot", "size": 25, "title": "least-squares fitting of two 3-d point sets"}, {"color": "#66CCFF", "id": "D. Crandall", "label": "D. Crandall", "shape": "dot", "size": 25, "title": "D. Crandall"}, {"color": "#66CCFF", "id": "discrete-continuous optimization", "label": "discrete-continuous optimization", "shape": "dot", "size": 25, "title": "discrete-continuous optimization"}, {"color": "#66CCFF", "id": "D. W. Eggert", "label": "D. W. Eggert", "shape": "dot", "size": 25, "title": "D. W. Eggert"}, {"color": "#66CCFF", "id": "comparison of four major algorithms", "label": "comparison of four major algorithms", "shape": "dot", "size": 25, "title": "comparison of four major algorithms"}, {"color": "#66CCFF", "id": "least-squares fitting", "label": "least-squares fitting", "shape": "dot", "size": 25, "title": "least-squares fitting"}, {"color": "#66CCFF", "id": "3d point set alignment", "label": "3d point set alignment", "shape": "dot", "size": 25, "title": "3d point set alignment"}, {"color": "#66CCFF", "id": "Estimating 3-d rigid body transformations", "label": "Estimating 3-d rigid body transformations", "shape": "dot", "size": 25, "title": "Estimating 3-d rigid body transformations"}, {"color": "#66CCFF", "id": "M. A. Fischler", "label": "M. A. Fischler", "shape": "dot", "size": 25, "title": "M. A. Fischler"}, {"color": "#66CCFF", "id": "Random sample consensus", "label": "Random sample consensus", "shape": "dot", "size": 25, "title": "Random sample consensus"}, {"color": "#66CCFF", "id": "Communications of the ACM", "label": "Communications of the ACM", "shape": "dot", "size": 25, "title": "Communications of the ACM"}, {"color": "#66CCFF", "id": "Advanced Digital Sciences Center", "label": "Advanced Digital Sciences Center", "shape": "dot", "size": 25, "title": "Advanced Digital Sciences Center"}, {"color": "#66CCFF", "id": "Structure from motion", "label": "Structure from motion", "shape": "dot", "size": 25, "title": "Structure from motion"}, {"color": "#66CCFF", "id": "R. Hartley", "label": "R. Hartley", "shape": "dot", "size": 25, "title": "R. Hartley"}, {"color": "#66CCFF", "id": "In defense of the eight-point algorithm", "label": "In defense of the eight-point algorithm", "shape": "dot", "size": 25, "title": "In defense of the eight-point algorithm"}, {"color": "#66CCFF", "id": "H. Isack", "label": "H. Isack", "shape": "dot", "size": 25, "title": "H. Isack"}, {"color": "#66CCFF", "id": "Energy-based geometric multi-model fitting", "label": "Energy-based geometric multi-model fitting", "shape": "dot", "size": 25, "title": "Energy-based geometric multi-model fitting"}, {"color": "#66CCFF", "id": "N. Jiang", "label": "N. Jiang", "shape": "dot", "size": 25, "title": "N. Jiang"}, {"color": "#66CCFF", "id": "A global linear method for camera pose registration", "label": "A global linear method for camera pose registration", "shape": "dot", "size": 25, "title": "A global linear method for camera pose registration"}, {"color": "#66CCFF", "id": "Advanced Digital Sciences Center, Singapore", "label": "Advanced Digital Sciences Center, Singapore", "shape": "dot", "size": 25, "title": "Advanced Digital Sciences Center, Singapore"}, {"color": "#66CCFF", "id": "University of Illinois at Urbana-Champaign", "label": "University of Illinois at Urbana-Champaign", "shape": "dot", "size": 25, "title": "University of Illinois at Urbana-Champaign"}, {"color": "#66CCFF", "id": "Stefan Roth", "label": "Stefan Roth", "shape": "dot", "size": 25, "title": "Stefan Roth"}, {"color": "#66CCFF", "id": "Richter_Discriminative_Shape_From_2015_CVPR_supplemental", "label": "Richter_Discriminative_Shape_From_2015_CVPR_supplemental", "shape": "dot", "size": 25, "title": "Richter_Discriminative_Shape_From_2015_CVPR_supplemental"}, {"color": "#66CCFF", "id": "Discriminative Shape from Shading", "label": "Discriminative Shape from Shading", "shape": "dot", "size": 25, "title": "Discriminative Shape from Shading"}, {"color": "#66CCFF", "id": "Shape from shading method", "label": "Shape from shading method", "shape": "dot", "size": 25, "title": "Shape from shading method"}, {"color": "#66CCFF", "id": "other approaches", "label": "other approaches", "shape": "dot", "size": 25, "title": "other approaches"}, {"color": "#66CCFF", "id": "local context", "label": "local context", "shape": "dot", "size": 25, "title": "local context"}, {"color": "#66CCFF", "id": "learning framework", "label": "learning framework", "shape": "dot", "size": 25, "title": "learning framework"}, {"color": "#66CCFF", "id": "improved reconstructions", "label": "improved reconstructions", "shape": "dot", "size": 25, "title": "improved reconstructions"}, {"color": "#66CCFF", "id": "smooth local context", "label": "smooth local context", "shape": "dot", "size": 25, "title": "smooth local context"}, {"color": "#66CCFF", "id": "Results", "label": "Results", "shape": "dot", "size": 25, "title": "Results"}, {"color": "#66CCFF", "id": "real images", "label": "real images", "shape": "dot", "size": 25, "title": "real images"}, {"color": "#66CCFF", "id": "unknown reflectance maps", "label": "unknown reflectance maps", "shape": "dot", "size": 25, "title": "unknown reflectance maps"}, {"color": "#66CCFF", "id": "fine detail", "label": "fine detail", "shape": "dot", "size": 25, "title": "fine detail"}, {"color": "#66CCFF", "id": "Dataset", "label": "Dataset", "shape": "dot", "size": 25, "title": "Dataset"}, {"color": "#66CCFF", "id": "ground truth dataset", "label": "ground truth dataset", "shape": "dot", "size": 25, "title": "ground truth dataset"}, {"color": "#66CCFF", "id": "real objects", "label": "real objects", "shape": "dot", "size": 25, "title": "real objects"}, {"color": "#66CCFF", "id": "Shape from Shading", "label": "Shape from Shading", "shape": "dot", "size": 25, "title": "Shape from Shading"}, {"color": "#66CCFF", "id": "Surface Reconstruction", "label": "Surface Reconstruction", "shape": "dot", "size": 25, "title": "Surface Reconstruction"}, {"color": "#66CCFF", "id": "Illumination Estimation", "label": "Illumination Estimation", "shape": "dot", "size": 25, "title": "Illumination Estimation"}, {"color": "#66CCFF", "id": "Local and Global Context", "label": "Local and Global Context", "shape": "dot", "size": 25, "title": "Local and Global Context"}, {"color": "#66CCFF", "id": "Machine Learning", "label": "Machine Learning", "shape": "dot", "size": 25, "title": "Machine Learning"}, {"color": "#66CCFF", "id": "Barron, J. T., \u0026 Malik, J.", "label": "Barron, J. T., \u0026 Malik, J.", "shape": "dot", "size": 25, "title": "Barron, J. T., \u0026 Malik, J."}, {"color": "#66CCFF", "id": "Color Constancy", "label": "Color Constancy", "shape": "dot", "size": 25, "title": "Color Constancy"}, {"color": "#66CCFF", "id": "Johnson, M. K., \u0026 Adelson, E. H.", "label": "Johnson, M. K., \u0026 Adelson, E. H.", "shape": "dot", "size": 25, "title": "Johnson, M. K., \u0026 Adelson, E. H."}, {"color": "#66CCFF", "id": "Shape Estimation", "label": "Shape Estimation", "shape": "dot", "size": 25, "title": "Shape Estimation"}, {"color": "#66CCFF", "id": "Natural Illumination", "label": "Natural Illumination", "shape": "dot", "size": 25, "title": "Natural Illumination"}, {"color": "#66CCFF", "id": "Reflectance Maps", "label": "Reflectance Maps", "shape": "dot", "size": 25, "title": "Reflectance Maps"}, {"color": "#66CCFF", "id": "Fine Detail", "label": "Fine Detail", "shape": "dot", "size": 25, "title": "Fine Detail"}, {"color": "#66CCFF", "id": "Real Objects", "label": "Real Objects", "shape": "dot", "size": 25, "title": "Real Objects"}, {"color": "#66CCFF", "id": "Johnson \u0026 Adelson (2011) paper", "label": "Johnson \u0026 Adelson (2011) paper", "shape": "dot", "size": 25, "title": "Johnson \u0026 Adelson (2011) paper"}, {"color": "#66CCFF", "id": "reference [4]", "label": "reference [4]", "shape": "dot", "size": 25, "title": "reference [4]"}, {"color": "#66CCFF", "id": "reference [3]", "label": "reference [3]", "shape": "dot", "size": 25, "title": "reference [3]"}, {"color": "#66CCFF", "id": "reference [1]", "label": "reference [1]", "shape": "dot", "size": 25, "title": "reference [1]"}, {"color": "#66CCFF", "id": "Jacobs, D. W.", "label": "Jacobs, D. W.", "shape": "dot", "size": 25, "title": "Jacobs, D. W."}, {"color": "#66CCFF", "id": "Basri, R.", "label": "Basri, R.", "shape": "dot", "size": 25, "title": "Basri, R."}, {"color": "#66CCFF", "id": "Stephan R. Richter", "label": "Stephan R. Richter", "shape": "dot", "size": 25, "title": "Stephan R. Richter"}, {"color": "#66CCFF", "id": "Department of Computer Science, TU Darmstadt", "label": "Department of Computer Science, TU Darmstadt", "shape": "dot", "size": 25, "title": "Department of Computer Science, TU Darmstadt"}, {"color": "#66CCFF", "id": "TU Darmstadt", "label": "TU Darmstadt", "shape": "dot", "size": 25, "title": "TU Darmstadt"}, {"color": "#66CCFF", "id": "Andr\u00e1s B\u00f3dis-Sz\u0151m\u0151ru", "label": "Andr\u00e1s B\u00f3dis-Sz\u0151m\u0151ru", "shape": "dot", "size": 25, "title": "Andr\u00e1s B\u00f3dis-Sz\u0151m\u0151ru"}, {"color": "#66CCFF", "id": "Superpixel Meshes", "label": "Superpixel Meshes", "shape": "dot", "size": 25, "title": "Superpixel Meshes"}, {"color": "#66CCFF", "id": "Hayko Riemenschneider", "label": "Hayko Riemenschneider", "shape": "dot", "size": 25, "title": "Hayko Riemenschneider"}, {"color": "#66CCFF", "id": "Superpixel Meses", "label": "Superpixel Meses", "shape": "dot", "size": 25, "title": "Superpixel Meses"}, {"color": "#66CCFF", "id": "Luc Van Gool", "label": "Luc Van Gool", "shape": "dot", "size": 25, "title": "Luc Van Gool"}, {"color": "#66CCFF", "id": "surface reconstruction", "label": "surface reconstruction", "shape": "dot", "size": 25, "title": "surface reconstruction"}, {"color": "#66CCFF", "id": "edges", "label": "edges", "shape": "dot", "size": 25, "title": "edges"}, {"color": "#66CCFF", "id": "Bodis-Szomoru_Superpixel_Meshes_for_2015_CVPR_supplemental.pdf", "label": "Bodis-Szomoru_Superpixel_Meshes_for_2015_CVPR_supplemental.pdf", "shape": "dot", "size": 25, "title": "Bodis-Szomoru_Superpixel_Meshes_for_2015_CVPR_supplemental.pdf"}, {"color": "#66CCFF", "id": "Multi-View-Stereo methods", "label": "Multi-View-Stereo methods", "shape": "dot", "size": 25, "title": "Multi-View-Stereo methods"}, {"color": "#66CCFF", "id": "highest detail", "label": "highest detail", "shape": "dot", "size": 25, "title": "highest detail"}, {"color": "#66CCFF", "id": "surface reconstruction method", "label": "surface reconstruction method", "shape": "dot", "size": 25, "title": "surface reconstruction method"}, {"color": "#66CCFF", "id": "image edges", "label": "image edges", "shape": "dot", "size": 25, "title": "image edges"}, {"color": "#66CCFF", "id": "second-order smoothness constraints", "label": "second-order smoothness constraints", "shape": "dot", "size": 25, "title": "second-order smoothness constraints"}, {"color": "#66CCFF", "id": "meshes", "label": "meshes", "shape": "dot", "size": 25, "title": "meshes"}, {"color": "#66CCFF", "id": "classic MVS surfaces", "label": "classic MVS surfaces", "shape": "dot", "size": 25, "title": "classic MVS surfaces"}, {"color": "#66CCFF", "id": "dense depth optimization", "label": "dense depth optimization", "shape": "dot", "size": 25, "title": "dense depth optimization"}, {"color": "#66CCFF", "id": "Ground Control Points", "label": "Ground Control Points", "shape": "dot", "size": 25, "title": "Ground Control Points"}, {"color": "#66CCFF", "id": "view pairing", "label": "view pairing", "shape": "dot", "size": 25, "title": "view pairing"}, {"color": "#66CCFF", "id": "stereo depth estimation", "label": "stereo depth estimation", "shape": "dot", "size": 25, "title": "stereo depth estimation"}, {"color": "#66CCFF", "id": "per-image paralleization", "label": "per-image paralleization", "shape": "dot", "size": 25, "title": "per-image paralleization"}, {"color": "#66CCFF", "id": "SfM points", "label": "SfM points", "shape": "dot", "size": 25, "title": "SfM points"}, {"color": "#66CCFF", "id": "GCPs", "label": "GCPs", "shape": "dot", "size": 25, "title": "GCPs"}, {"color": "#66CCFF", "id": "LiDAR", "label": "LiDAR", "shape": "dot", "size": 25, "title": "LiDAR"}, {"color": "#66CCFF", "id": "RGB-D", "label": "RGB-D", "shape": "dot", "size": 25, "title": "RGB-D"}, {"color": "#66CCFF", "id": "edge-aligned", "label": "edge-aligned", "shape": "dot", "size": 25, "title": "edge-aligned"}, {"color": "#66CCFF", "id": "Structure-from-Motion (SfM) points", "label": "Structure-from-Motion (SfM) points", "shape": "dot", "size": 25, "title": "Structure-from-Motion (SfM) points"}, {"color": "#66CCFF", "id": "compact", "label": "compact", "shape": "dot", "size": 25, "title": "compact"}, {"color": "#66CCFF", "id": "image gradients", "label": "image gradients", "shape": "dot", "size": 25, "title": "image gradients"}, {"color": "#66CCFF", "id": "renderings", "label": "renderings", "shape": "dot", "size": 25, "title": "renderings"}, {"color": "#66CCFF", "id": "lightweight", "label": "lightweight", "shape": "dot", "size": 25, "title": "lightweight"}, {"color": "#66CCFF", "id": "per-face flat", "label": "per-face flat", "shape": "dot", "size": 25, "title": "per-face flat"}, {"color": "#66CCFF", "id": "superiority in speed", "label": "superiority in speed", "shape": "dot", "size": 25, "title": "superiority in speed"}, {"color": "#66CCFF", "id": "competitive surface quality", "label": "competitive surface quality", "shape": "dot", "size": 25, "title": "competitive surface quality"}, {"color": "#66CCFF", "id": "Zhang_Light_Field_From_2015_CVPR_paper", "label": "Zhang_Light_Field_From_2015_CVPR_paper", "shape": "dot", "size": 25, "title": "Zhang_Light_Field_From_2015_CVPR_paper"}, {"color": "#66CCFF", "id": "Superpixels", "label": "Superpixels", "shape": "dot", "size": 25, "title": "Superpixels"}, {"color": "#66CCFF", "id": "Zhang_Light_Light_Field_From_2015_CVPR_paper", "label": "Zhang_Light_Light_Field_From_2015_CVPR_paper", "shape": "dot", "size": 25, "title": "Zhang_Light_Light_Field_From_2015_CVPR_paper"}, {"color": "#66CCFF", "id": "Structure-from-Motion (SfM)", "label": "Structure-from-Motion (SfM)", "shape": "dot", "size": 25, "title": "Structure-from-Motion (SfM)"}, {"color": "#66CCFF", "id": "Mesh Generation", "label": "Mesh Generation", "shape": "dot", "size": 25, "title": "Mesh Generation"}, {"color": "#66CCFF", "id": "Edge-Preerving Methods", "label": "Edge-Preerving Methods", "shape": "dot", "size": 25, "title": "Edge-Preerving Methods"}, {"color": "#66CCFF", "id": "Andr\u00b4as B\u00b4odis-Szomor\u00b4u", "label": "Andr\u00b4as B\u00b4odis-Szomor\u00b4u", "shape": "dot", "size": 25, "title": "Andr\u00b4as B\u00b4odis-Szomor\u00b4u"}, {"color": "#66CCFF", "id": "ETH Zurich, Computer Vision Lab", "label": "ETH Zurich, Computer Vision Lab", "shape": "dot", "size": 25, "title": "ETH Zurich, Computer Vision Lab"}, {"color": "#66CCFF", "id": "PSI-VISICS, KU Leuven", "label": "PSI-VISICS, KU Leuven", "shape": "dot", "size": 25, "title": "PSI-VISICS, KU Leuven"}, {"color": "#66CCFF", "id": "Didyk et.al", "label": "Didyk et.al", "shape": "dot", "size": 25, "title": "Didyk et.al"}, {"color": "#66CCFF", "id": "surface quality", "label": "surface quality", "shape": "dot", "size": 25, "title": "surface quality"}, {"color": "#66CCFF", "id": "depth estimation", "label": "depth estimation", "shape": "dot", "size": 25, "title": "depth estimation"}, {"color": "#66CCFF", "id": "synthesizing intermediate views", "label": "synthesizing intermediate views", "shape": "dot", "size": 25, "title": "synthesizing intermediate views"}, {"color": "#66CCFF", "id": "intermediate views", "label": "intermediate views", "shape": "dot", "size": 25, "title": "intermediate views"}, {"color": "#66CCFF", "id": "light field synthesis", "label": "light field synthesis", "shape": "dot", "size": 25, "title": "light field synthesis"}, {"color": "#66CCFF", "id": "work on depth", "label": "work on depth", "shape": "dot", "size": 25, "title": "work on depth"}, {"color": "#66CCFF", "id": "synthesized right views", "label": "synthesized right views", "shape": "dot", "size": 25, "title": "synthesized right views"}, {"color": "#66CCFF", "id": "disparity refinement", "label": "disparity refinement", "shape": "dot", "size": 25, "title": "disparity refinement"}, {"color": "#66CCFF", "id": "depth perception", "label": "depth perception", "shape": "dot", "size": 25, "title": "depth perception"}, {"color": "#66CCFF", "id": "improvements", "label": "improvements", "shape": "dot", "size": 25, "title": "improvements"}, {"color": "#66CCFF", "id": "Depth Estimation", "label": "Depth Estimation", "shape": "dot", "size": 25, "title": "Depth Estimation"}, {"color": "#66CCFF", "id": "existing methods", "label": "existing methods", "shape": "dot", "size": 25, "title": "existing methods"}, {"color": "#66CCFF", "id": "View Synthesis", "label": "View Synthesis", "shape": "dot", "size": 25, "title": "View Synthesis"}, {"color": "#66CCFF", "id": "Light Field Reconstruction", "label": "Light Field Reconstruction", "shape": "dot", "size": 25, "title": "Light Field Reconstruction"}, {"color": "#66CCFF", "id": "Disparity Refinement", "label": "Disparity Refinement", "shape": "dot", "size": 25, "title": "Disparity Refinement"}, {"color": "#66CCFF", "id": "Iterative View Generation", "label": "Iterative View Generation", "shape": "dot", "size": 25, "title": "Iterative View Generation"}, {"color": "#66CCFF", "id": "Jan Hosang", "label": "Jan Hosang", "shape": "dot", "size": 25, "title": "Jan Hosang"}, {"color": "#66CCFF", "id": "Taking a Deeper Look at Pedestrians", "label": "Taking a Deeper Look at Pedestrians", "shape": "dot", "size": 25, "title": "Taking a Deeper Look at Pedestrians"}, {"color": "#66CCFF", "id": "Mohamed Omran", "label": "Mohamed Omran", "shape": "dot", "size": 25, "title": "Mohamed Omran"}, {"color": "#66CCFF", "id": "impact of parameters", "label": "impact of parameters", "shape": "dot", "size": 25, "title": "impact of parameters"}, {"color": "#66CCFF", "id": "CifarNet", "label": "CifarNet", "shape": "dot", "size": 25, "title": "CifarNet"}, {"color": "#66CCFF", "id": "AlexNet", "label": "AlexNet", "shape": "dot", "size": 25, "title": "AlexNet"}, {"color": "#66CCFF", "id": "filter size", "label": "filter size", "shape": "dot", "size": 25, "title": "filter size"}, {"color": "#66CCFF", "id": "layer width", "label": "layer width", "shape": "dot", "size": 25, "title": "layer width"}, {"color": "#66CCFF", "id": "learning rate policies", "label": "learning rate policies", "shape": "dot", "size": 25, "title": "learning rate policies"}, {"color": "#66CCFF", "id": "pedestrian heights", "label": "pedestrian heights", "shape": "dot", "size": 25, "title": "pedestrian heights"}, {"color": "#66CCFF", "id": "Calttech dataset", "label": "Calttech dataset", "shape": "dot", "size": 25, "title": "Calttech dataset"}, {"color": "#66CCFF", "id": "KITTI dataset", "label": "KITTI dataset", "shape": "dot", "size": 25, "title": "KITTI dataset"}, {"color": "#66CCFF", "id": "transferability", "label": "transferability", "shape": "dot", "size": 25, "title": "transferability"}, {"color": "#66CCFF", "id": "neural network training", "label": "neural network training", "shape": "dot", "size": 25, "title": "neural network training"}, {"color": "#66CCFF", "id": "parameter choices", "label": "parameter choices", "shape": "dot", "size": 25, "title": "parameter choices"}, {"color": "#66CCFF", "id": "parameter optimization", "label": "parameter optimization", "shape": "dot", "size": 25, "title": "parameter optimization"}, {"color": "#66CCFF", "id": "optimal results", "label": "optimal results", "shape": "dot", "size": 25, "title": "optimal results"}, {"color": "#66CCFF", "id": "Pedestrian Detection", "label": "Pedestrian Detection", "shape": "dot", "size": 25, "title": "Pedestrian Detection"}, {"color": "#66CCFF", "id": "Neural Network Training", "label": "Neural Network Training", "shape": "dot", "size": 25, "title": "Neural Network Training"}, {"color": "#66CCFF", "id": "Parameter Optimization", "label": "Parameter Optimization", "shape": "dot", "size": 25, "title": "Parameter Optimization"}, {"color": "#66CCFF", "id": "Dataset Analysis", "label": "Dataset Analysis", "shape": "dot", "size": 25, "title": "Dataset Analysis"}, {"color": "#66CCFF", "id": "Transfer Learning", "label": "Transfer Learning", "shape": "dot", "size": 25, "title": "Transfer Learning"}, {"color": "#66CCFF", "id": "Benenson, R.", "label": "Benenson, R.", "shape": "dot", "size": 25, "title": "Benenson, R."}, {"color": "#66CCFF", "id": "Max Planck Institute for Informatics", "label": "Max Planck Institute for Informatics", "shape": "dot", "size": 25, "title": "Max Planck Institute for Informatics"}, {"color": "#66CCFF", "id": "Omran, M.", "label": "Omran, M.", "shape": "dot", "size": 25, "title": "Omran, M."}, {"color": "#66CCFF", "id": "Schiele, B.", "label": "Schiele, B.", "shape": "dot", "size": 25, "title": "Schiele, B."}, {"color": "#66CCFF", "id": "Hosang, J.", "label": "Hosang, J.", "shape": "dot", "size": 25, "title": "Hosang, J."}, {"color": "#66CCFF", "id": "jan.hosang@mpi-inf.mpg.de", "label": "jan.hosang@mpi-inf.mpg.de", "shape": "dot", "size": 25, "title": "jan.hosang@mpi-inf.mpg.de"}, {"color": "#66CCFF", "id": "mohamed.omran@mpi-inf.mpg.de", "label": "mohamed.omran@mpi-inf.mpg.de", "shape": "dot", "size": 25, "title": "mohamed.omran@mpi-inf.mpg.de"}, {"color": "#66CCFF", "id": "rodrigo.benenson@mpi-inf.mpg.de", "label": "rodrigo.benenson@mpi-inf.mpg.de", "shape": "dot", "size": 25, "title": "rodrigo.benenson@mpi-inf.mpg.de"}, {"color": "#66CCFF", "id": "unspecified", "label": "unspecified", "shape": "dot", "size": 25, "title": "unspecified"}, {"color": "#66CCFF", "id": "Kiyoshi Matsuo", "label": "Kiyoshi Matsuo", "shape": "dot", "size": 25, "title": "Kiyoshi Matsuo"}, {"color": "#66CCFF", "id": "Depth Image Enhancement Using Local Tangent Plane Approximations", "label": "Depth Image Enhancement Using Local Tangent Plane Approximations", "shape": "dot", "size": 25, "title": "Depth Image Enhancement Using Local Tangent Plane Approximations"}, {"color": "#66CCFF", "id": "informatics", "label": "informatics", "shape": "dot", "size": 25, "title": "informatics"}, {"color": "#66CCFF", "id": "Yoshimitsu Aoki", "label": "Yoshimitsu Aoki", "shape": "dot", "size": 25, "title": "Yoshimitsu Aoki"}, {"color": "#66CCFF", "id": "depth image enhancement method", "label": "depth image enhancement method", "shape": "dot", "size": 25, "title": "depth image enhancement method"}, {"color": "#66CCFF", "id": "consumer RGB-D cameras", "label": "consumer RGB-D cameras", "shape": "dot", "size": 25, "title": "consumer RGB-D cameras"}, {"color": "#66CCFF", "id": "pixel-coordinates", "label": "pixel-coordinates", "shape": "dot", "size": 25, "title": "pixel-coordinates"}, {"color": "#66CCFF", "id": "handling local geometries", "label": "handling local geometries", "shape": "dot", "size": 25, "title": "handling local geometries"}, {"color": "#66CCFF", "id": "local tangent planes", "label": "local tangent planes", "shape": "dot", "size": 25, "title": "local tangent planes"}, {"color": "#66CCFF", "id": "two steps", "label": "two steps", "shape": "dot", "size": 25, "title": "two steps"}, {"color": "#66CCFF", "id": "calculation of local tangents", "label": "calculation of local tangents", "shape": "dot", "size": 25, "title": "calculation of local tangents"}, {"color": "#66CCFF", "id": "depth image enhancement", "label": "depth image enhancement", "shape": "dot", "size": 25, "title": "depth image enhancement"}, {"color": "#66CCFF", "id": "local geometries", "label": "local geometries", "shape": "dot", "size": 25, "title": "local geometries"}, {"color": "#66CCFF", "id": "high completion rate", "label": "high completion rate", "shape": "dot", "size": 25, "title": "high completion rate"}, {"color": "#66CCFF", "id": "lowest errors", "label": "lowest errors", "shape": "dot", "size": 25, "title": "lowest errors"}, {"color": "#66CCFF", "id": "noisy cases", "label": "noisy cases", "shape": "dot", "size": 25, "title": "noisy cases"}, {"color": "#66CCFF", "id": "Local Tangent Planes", "label": "Local Tangent Planes", "shape": "dot", "size": 25, "title": "Local Tangent Planes"}, {"color": "#66CCFF", "id": "Depth Image Enhancement", "label": "Depth Image Enhancement", "shape": "dot", "size": 25, "title": "Depth Image Enhancement"}, {"color": "#66CCFF", "id": "RGB-D Cameras", "label": "RGB-D Cameras", "shape": "dot", "size": 25, "title": "RGB-D Cameras"}, {"color": "#66CCFF", "id": "Noise Reduction", "label": "Noise Reduction", "shape": "dot", "size": 25, "title": "Noise Reduction"}, {"color": "#66CCFF", "id": "Completion Rate", "label": "Completion Rate", "shape": "dot", "size": 25, "title": "Completion Rate"}, {"color": "#66CCFF", "id": "Asus Xtion Pro Live", "label": "Asus Xtion Pro Live", "shape": "dot", "size": 25, "title": "Asus Xtion Pro Live"}, {"color": "#66CCFF", "id": "Kim et al. (2013)", "label": "Kim et al. (2013)", "shape": "dot", "size": 25, "title": "Kim et al. (2013)"}, {"color": "#66CCFF", "id": "joint intensity and depth analysis model", "label": "joint intensity and depth analysis model", "shape": "dot", "size": 25, "title": "joint intensity and depth analysis model"}, {"color": "#66CCFF", "id": "Kim et al. (2014)", "label": "Kim et al. (2014)", "shape": "dot", "size": 25, "title": "Kim et al. (2014)"}, {"color": "#66CCFF", "id": "depth map upsampling", "label": "depth map upsampling", "shape": "dot", "size": 25, "title": "depth map upsampling"}, {"color": "#66CCFF", "id": "Lee et al.", "label": "Lee et al.", "shape": "dot", "size": 25, "title": "Lee et al."}, {"color": "#66CCFF", "id": "Journal of Signal Processing Systems", "label": "Journal of Signal Processing Systems", "shape": "dot", "size": 25, "title": "Journal of Signal Processing Systems"}, {"color": "#66CCFF", "id": "depth map upsampling method", "label": "depth map upsampling method", "shape": "dot", "size": 25, "title": "depth map upsampling method"}, {"color": "#66CCFF", "id": "misalignment of depth and color boundaries", "label": "misalignment of depth and color boundaries", "shape": "dot", "size": 25, "title": "misalignment of depth and color boundaries"}, {"color": "#66CCFF", "id": "Kopf et al.", "label": "Kopf et al.", "shape": "dot", "size": 25, "title": "Kopf et al."}, {"color": "#66CCFF", "id": "Joint bilateral upsampling", "label": "Joint bilateral upsampling", "shape": "dot", "size": 25, "title": "Joint bilateral upsampling"}, {"color": "#66CCFF", "id": "Li et al.", "label": "Li et al.", "shape": "dot", "size": 25, "title": "Li et al."}, {"color": "#66CCFF", "id": "Joint example-based depth map super-resolution", "label": "Joint example-based depth map super-resolution", "shape": "dot", "size": 25, "title": "Joint example-based depth map super-resolution"}, {"color": "#66CCFF", "id": "Lu et al.", "label": "Lu et al.", "shape": "dot", "size": 25, "title": "Lu et al."}, {"color": "#66CCFF", "id": "Depth enhancement", "label": "Depth enhancement", "shape": "dot", "size": 25, "title": "Depth enhancement"}, {"color": "#66CCFF", "id": "low-rank matrix completion", "label": "low-rank matrix completion", "shape": "dot", "size": 25, "title": "low-rank matrix completion"}, {"color": "#66CCFF", "id": "upsampling method", "label": "upsampling method", "shape": "dot", "size": 25, "title": "upsampling method"}, {"color": "#66CCFF", "id": "depth map resolution", "label": "depth map resolution", "shape": "dot", "size": 25, "title": "depth map resolution"}, {"color": "#66CCFF", "id": "Joint geodesic up-sampling", "label": "Joint geodesic up-sampling", "shape": "dot", "size": 25, "title": "Joint geodesic up-sampling"}, {"color": "#66CCFF", "id": "depth images", "label": "depth images", "shape": "dot", "size": 25, "title": "depth images"}, {"color": "#66CCFF", "id": "S. Lu", "label": "S. Lu", "shape": "dot", "size": 25, "title": "S. Lu"}, {"color": "#66CCFF", "id": "Depth enhancement via low-rank matrix completion", "label": "Depth enhancement via low-rank matrix completion", "shape": "dot", "size": 25, "title": "Depth enhancement via low-rank matrix completion"}, {"color": "#66CCFF", "id": "CVPR 2014", "label": "CVPR 2014", "shape": "dot", "size": 25, "title": "CVPR 2014"}, {"color": "#66CCFF", "id": "D. Scharstein", "label": "D. Scharstein", "shape": "dot", "size": 25, "title": "D. Scharstein"}, {"color": "#66CCFF", "id": "Learning conditional random fields for stereo", "label": "Learning conditional random fields for stereo", "shape": "dot", "size": 25, "title": "Learning conditional random fields for stereo"}, {"color": "#66CCFF", "id": "J. Papon", "label": "J. Papon", "shape": "dot", "size": 25, "title": "J. Papon"}, {"color": "#66CCFF", "id": "Point cloud video object segmentation", "label": "Point cloud video object segmentation", "shape": "dot", "size": 25, "title": "Point cloud video object segmentation"}, {"color": "#66CCFF", "id": "IROS 2013", "label": "IROS 2013", "shape": "dot", "size": 25, "title": "IROS 2013"}, {"color": "#66CCFF", "id": "Keio University", "label": "Keio University", "shape": "dot", "size": 25, "title": "Keio University"}, {"color": "#66CCFF", "id": "Hokuyo Automatic Co., LTD.", "label": "Hokuyo Automatic Co., LTD.", "shape": "dot", "size": 25, "title": "Hokuyo Automatic Co., LTD."}, {"color": "#66CCFF", "id": "Sean Bell", "label": "Sean Bell", "shape": "dot", "size": 25, "title": "Sean Bell"}, {"color": "#66CCFF", "id": "Material Recognition in the Wild", "label": "Material Recognition in the Wild", "shape": "dot", "size": 25, "title": "Material Recognition in the Wild"}, {"color": "#66CCFF", "id": "Paul Upchurch", "label": "Paul Upchurch", "shape": "dot", "size": 25, "title": "Paul Upchurch"}, {"color": "#66CCFF", "id": "Materials in Context Database", "label": "Materials in Context Database", "shape": "dot", "size": 25, "title": "Materials in Context Database"}, {"color": "#66CCFF", "id": "Material Recognition in 2015 CVPR paper", "label": "Material Recognition in 2015 CVPR paper", "shape": "dot", "size": 25, "title": "Material Recognition in 2015 CVPR paper"}, {"color": "#66CCFF", "id": "Noah Snavely", "label": "Noah Snavely", "shape": "dot", "size": 25, "title": "Noah Snavely"}, {"color": "#66CCFF", "id": "Material recognition in 2015 CVPR paper", "label": "Material recognition in 2015 CVPR paper", "shape": "dot", "size": 25, "title": "Material recognition in 2015 CVPR paper"}, {"color": "#66CCFF", "id": "Material Recognition", "label": "Material Recognition", "shape": "dot", "size": 25, "title": "Material Recognition"}, {"color": "#66CCFF", "id": "Bell_Material_Recognition_in_2015_CVPR_paper", "label": "Bell_Material_Recognition_in_2015_CVPR_paper", "shape": "dot", "size": 25, "title": "Bell_Material_Recognition_in_2015_CVPR_paper"}, {"color": "#66CCFF", "id": "Bell_Material_Detection_in_2015_CVPR_paper", "label": "Bell_Material_Detection_in_2015_CVPR_paper", "shape": "dot", "size": 25, "title": "Bell_Material_Detection_in_2015_CVPR_paper"}, {"color": "#66CCFF", "id": "material recognition", "label": "material recognition", "shape": "dot", "size": 25, "title": "material recognition"}, {"color": "#66CCFF", "id": "rich surface texture", "label": "rich surface texture", "shape": "dot", "size": 25, "title": "rich surface texture"}, {"color": "#66CCFF", "id": "lighting conditions", "label": "lighting conditions", "shape": "dot", "size": 25, "title": "lighting conditions"}, {"color": "#66CCFF", "id": "MINC", "label": "MINC", "shape": "dot", "size": 25, "title": "MINC"}, {"color": "#66CCFF", "id": "large-scale", "label": "large-scale", "shape": "dot", "size": 25, "title": "large-scale"}, {"color": "#66CCFF", "id": "open", "label": "open", "shape": "dot", "size": 25, "title": "open"}, {"color": "#66CCFF", "id": "CNNs", "label": "CNNs", "shape": "dot", "size": 25, "title": "CNNs"}, {"color": "#66CCFF", "id": "material classification", "label": "material classification", "shape": "dot", "size": 25, "title": "material classification"}, {"color": "#66CCFF", "id": "material segmentation", "label": "material segmentation", "shape": "dot", "size": 25, "title": "material segmentation"}, {"color": "#66CCFF", "id": "patch-based classification", "label": "patch-based classification", "shape": "dot", "size": 25, "title": "patch-based classification"}, {"color": "#66CCFF", "id": "85.2% mean class accuracy", "label": "85.2% mean class accuracy", "shape": "dot", "size": 25, "title": "85.2% mean class accuracy"}, {"color": "#66CCFF", "id": "full image segmentation", "label": "full image segmentation", "shape": "dot", "size": 25, "title": "full image segmentation"}, {"color": "#66CCFF", "id": "73.1% mean class accuracy", "label": "73.1% mean class accuracy", "shape": "dot", "size": 25, "title": "73.1% mean class accuracy"}, {"color": "#66CCFF", "id": "large, well-sampled datasets", "label": "large, well-sampled datasets", "shape": "dot", "size": 25, "title": "large, well-sampled datasets"}, {"color": "#66CCFF", "id": "Dataset Creation", "label": "Dataset Creation", "shape": "dot", "size": 25, "title": "Dataset Creation"}, {"color": "#66CCFF", "id": "G. Patterson et al.", "label": "G. Patterson et al.", "shape": "dot", "size": 25, "title": "G. Patterson et al."}, {"color": "#66CCFF", "id": "The SUN Attribute Database", "label": "The SUN Attribute Database", "shape": "dot", "size": 25, "title": "The SUN Attribute Database"}, {"color": "#66CCFF", "id": "Deeper Scene Understanding", "label": "Deeper Scene Understanding", "shape": "dot", "size": 25, "title": "Deeper Scene Understanding"}, {"color": "#66CCFF", "id": "S. Bell et al.", "label": "S. Bell et al.", "shape": "dot", "size": 25, "title": "S. Bell et al."}, {"color": "#66CCFF", "id": "OpenSurposes", "label": "OpenSurposes", "shape": "dot", "size": 25, "title": "OpenSurposes"}, {"color": "#66CCFF", "id": "OpenSurfaces", "label": "OpenSurfaces", "shape": "dot", "size": 25, "title": "OpenSurfaces"}, {"color": "#66CCFF", "id": "richly annotated catalog", "label": "richly annotated catalog", "shape": "dot", "size": 25, "title": "richly annotated catalog"}, {"color": "#66CCFF", "id": "X. Qi et al.", "label": "X. Qi et al.", "shape": "dot", "size": 25, "title": "X. Qi et al."}, {"color": "#66CCFF", "id": "Pairwise rotation invariant co-occurrence local binary pattern", "label": "Pairwise rotation invariant co-occurrence local binary pattern", "shape": "dot", "size": 25, "title": "Pairwise rotation invariant co-occurrence local binary pattern"}, {"color": "#66CCFF", "id": "B. Caputo et al.", "label": "B. Caputo et al.", "shape": "dot", "size": 25, "title": "B. Caputo et al."}, {"color": "#66CCFF", "id": "Class-speci\ufb01c material categorisation", "label": "Class-speci\ufb01c material categorisation", "shape": "dot", "size": 25, "title": "Class-speci\ufb01c material categorisation"}, {"color": "#66CCFF", "id": "variant co-occurrence local binary pattern", "label": "variant co-occurrence local binary pattern", "shape": "dot", "size": 25, "title": "variant co-occurrence local binary pattern"}, {"color": "#66CCFF", "id": "Class-specific material categorisation", "label": "Class-specific material categorisation", "shape": "dot", "size": 25, "title": "Class-specific material categorisation"}, {"color": "#66CCFF", "id": "ImageNet Large Scale Visual Recognition Challenge", "label": "ImageNet Large Scale Visual Recognition Challenge", "shape": "dot", "size": 25, "title": "ImageNet Large Scale Visual Recognition Challenge"}, {"color": "#66CCFF", "id": "visual recognition", "label": "visual recognition", "shape": "dot", "size": 25, "title": "visual recognition"}, {"color": "#66CCFF", "id": "ACM Transactions on Graphics (TOG)", "label": "ACM Transactions on Graphics (TOG)", "shape": "dot", "size": 25, "title": "ACM Transactions on Graphics (TOG)"}, {"color": "#66CCFF", "id": "Re\ufb02ectence and texture of real-world surfaces", "label": "Re\ufb02ectence and texture of real-world surfaces", "shape": "dot", "size": 25, "title": "Re\ufb02ectence and texture of real-world surfaces"}, {"color": "#66CCFF", "id": "LabelMe", "label": "LabelMe", "shape": "dot", "size": 25, "title": "LabelMe"}, {"color": "#66CCFF", "id": "Re\ufb02ectance", "label": "Re\ufb02ectance", "shape": "dot", "size": 25, "title": "Re\ufb02ectance"}, {"color": "#66CCFF", "id": "real-world surfaces", "label": "real-world surfaces", "shape": "dot", "size": 25, "title": "real-world surfaces"}, {"color": "#66CCFF", "id": "texture", "label": "texture", "shape": "dot", "size": 25, "title": "texture"}, {"color": "#66CCFF", "id": "Describing textures in the wild", "label": "Describing textures in the wild", "shape": "dot", "size": 25, "title": "Describing textures in the wild"}, {"color": "#66CCFF", "id": "image processing", "label": "image processing", "shape": "dot", "size": 25, "title": "image processing"}, {"color": "#66CCFF", "id": "TOG", "label": "TOG", "shape": "dot", "size": 25, "title": "TOG"}, {"color": "#66CCFF", "id": "Graphics", "label": "Graphics", "shape": "dot", "size": 25, "title": "Graphics"}, {"color": "#66CCFF", "id": "Pascal VOC Challenge", "label": "Pascal VOC Challenge", "shape": "dot", "size": 25, "title": "Pascal VOC Challenge"}, {"color": "#66CCFF", "id": "Visual Object Classes", "label": "Visual Object Classes", "shape": "dot", "size": 25, "title": "Visual Object Classes"}, {"color": "#66CCFF", "id": "Department of Computer Science, Cornell University", "label": "Department of Computer Science, Cornell University", "shape": "dot", "size": 25, "title": "Department of Computer Science, Cornell University"}, {"color": "#66CCFF", "id": "sbell@cs.cornell.edu", "label": "sbell@cs.cornell.edu", "shape": "dot", "size": 25, "title": "sbell@cs.cornell.edu"}, {"color": "#66CCFF", "id": "paulu@cs.cornell.edu", "label": "paulu@cs.cornell.edu", "shape": "dot", "size": 25, "title": "paulu@cs.cornell.edu"}, {"color": "#66CCFF", "id": "snavely@cs.cornell.edu", "label": "snavely@cs.cornell.edu", "shape": "dot", "size": 25, "title": "snavely@cs.cornell.edu"}, {"color": "#66CCFF", "id": "kb@cs.cornell.edu", "label": "kb@cs.cornell.edu", "shape": "dot", "size": 25, "title": "kb@cs.cornell.edu"}, {"color": "#66CCFF", "id": "Department of Computer Science", "label": "Department of Computer Science", "shape": "dot", "size": 25, "title": "Department of Computer Science"}, {"color": "#66CCFF", "id": "Wonmin Byeon", "label": "Wonmin Byeon", "shape": "dot", "size": 25, "title": "Wonmin Byeon"}, {"color": "#66CCFF", "id": "Scene Labeling with LSTM Recurrent Neural Networks", "label": "Scene Labeling with LSTM Recurrent Neural Networks", "shape": "dot", "size": 25, "title": "Scene Labeling with LSTM Recurrent Neural Networks"}, {"color": "#66CCFF", "id": "Thomas M. Breuel", "label": "Thomas M. Breuel", "shape": "dot", "size": 25, "title": "Thomas M. Breuel"}, {"color": "#66CCFF", "id": "Federico Raue", "label": "Federico Raue", "shape": "dot", "size": 25, "title": "Federico Raue"}, {"color": "#66CCFF", "id": "Byeon_Scene_Labeling_With_2015_CVPR_paper.pdf", "label": "Byeon_Scene_Labeling_With_2015_CVPR_paper.pdf", "shape": "dot", "size": 25, "title": "Byeon_Scene_Labeling_With_2015_CVPR_paper.pdf"}, {"color": "#66CCFF", "id": "Accurate scene labeling", "label": "Accurate scene labeling", "shape": "dot", "size": 25, "title": "Accurate scene labeling"}, {"color": "#66CCFF", "id": "image understanding", "label": "image understanding", "shape": "dot", "size": 25, "title": "image understanding"}, {"color": "#66CCFF", "id": "pixel-level segmentation", "label": "pixel-level segmentation", "shape": "dot", "size": 25, "title": "pixel-level segmentation"}, {"color": "#66CCFF", "id": "Long Short Term Memory (LSTM) recurrent neural networks", "label": "Long Short Term Memory (LSTM) recurrent neural networks", "shape": "dot", "size": 25, "title": "Long Short Term Memory (LSTM) recurrent neural networks"}, {"color": "#66CCFF", "id": "LSTM recurrent neural networks", "label": "LSTM recurrent neural networks", "shape": "dot", "size": 25, "title": "LSTM recurrent neural networks"}, {"color": "#66CCFF", "id": "lower computational complexity", "label": "lower computational complexity", "shape": "dot", "size": 25, "title": "lower computational complexity"}, {"color": "#66CCFF", "id": "state-of-the-art performance", "label": "state-of-the-art performance", "shape": "dot", "size": 25, "title": "state-of-the-art performance"}, {"color": "#66CCFF", "id": "Stanford Background dataset", "label": "Stanford Background dataset", "shape": "dot", "size": 25, "title": "Stanford Background dataset"}, {"color": "#66CCFF", "id": "SIFT Flow datasets", "label": "SIFT Flow datasets", "shape": "dot", "size": 25, "title": "SIFT Flow datasets"}, {"color": "#66CCFF", "id": "local contextual information", "label": "local contextual information", "shape": "dot", "size": 25, "title": "local contextual information"}, {"color": "#66CCFF", "id": "global contextual information", "label": "global contextual information", "shape": "dot", "size": 25, "title": "global contextual information"}, {"color": "#66CCFF", "id": "RGB values", "label": "RGB values", "shape": "dot", "size": 25, "title": "RGB values"}, {"color": "#66CCFF", "id": "Networks", "label": "Networks", "shape": "dot", "size": 25, "title": "Networks"}, {"color": "#66CCFF", "id": "raw RGB values", "label": "raw RGB values", "shape": "dot", "size": 25, "title": "raw RGB values"}, {"color": "#66CCFF", "id": "complex scene images", "label": "complex scene images", "shape": "dot", "size": 25, "title": "complex scene images"}, {"color": "#66CCFF", "id": "German Research Center for Arti\ufb01cial Intelligence (DFKI)", "label": "German Research Center for Arti\ufb01cial Intelligence (DFKI)", "shape": "dot", "size": 25, "title": "German Research Center for Arti\ufb01cial Intelligence (DFKI)"}, {"color": "#66CCFF", "id": "University of Kaiserslautern", "label": "University of Kaiserslautern", "shape": "dot", "size": 25, "title": "University of Kaiserslautern"}, {"color": "#66CCFF", "id": "Thalaiyasingam Ajanthan", "label": "Thalaiyasingam Ajanthan", "shape": "dot", "size": 25, "title": "Thalaiyasingam Ajanthan"}, {"color": "#66CCFF", "id": "Iteratively Reweighted Graph Cut", "label": "Iteratively Reweighted Graph Cut", "shape": "dot", "size": 25, "title": "Iteratively Reweighted Graph Cut"}, {"color": "#66CCFF", "id": "Multi-label MRFs", "label": "Multi-label MRFs", "shape": "dot", "size": 25, "title": "Multi-label MRFs"}, {"color": "#66CCFF", "id": "energy", "label": "energy", "shape": "dot", "size": 25, "title": "energy"}, {"color": "#66CCFF", "id": "Multi-label Markov Random Fields (MRFs)", "label": "Multi-label Markov Random Fields (MRFs)", "shape": "dot", "size": 25, "title": "Multi-label Markov Random Fields (MRFs)"}, {"color": "#66CCFF", "id": "multi-label graph cut algorithm", "label": "multi-label graph cut algorithm", "shape": "dot", "size": 25, "title": "multi-label graph cut algorithm"}, {"color": "#66CCFF", "id": "Iterively Reweighted Least Squares (IRLS) algorithm", "label": "Iterively Reweighted Least Squares (IRLS) algorithm", "shape": "dot", "size": 25, "title": "Iterively Reweighted Least Squares (IRLS) algorithm"}, {"color": "#66CCFF", "id": "stereo correspondence estimation", "label": "stereo correspondence estimation", "shape": "dot", "size": 25, "title": "stereo correspondence estimation"}, {"color": "#66CCFF", "id": "image inpainting problems", "label": "image inpainting problems", "shape": "dot", "size": 25, "title": "image inpainting problems"}, {"color": "#66CCFF", "id": "graph-cut-based algorithms", "label": "graph-cut-based algorithms", "shape": "dot", "size": 25, "title": "graph-cut-based algorithms"}, {"color": "#66CCFF", "id": "lower energy values", "label": "lower energy values", "shape": "dot", "size": 25, "title": "lower energy values"}, {"color": "#66CCFF", "id": "MRFs", "label": "MRFs", "shape": "dot", "size": 25, "title": "MRFs"}, {"color": "#66CCFF", "id": "Non-convex Priors", "label": "Non-convex Priors", "shape": "dot", "size": 25, "title": "Non-convex Priors"}, {"color": "#66CCFF", "id": "Multi-label Markov Random Fields", "label": "Multi-label Markov Random Fields", "shape": "dot", "size": 25, "title": "Multi-label Markov Random Fields"}, {"color": "#66CCFF", "id": "Graph Cut Optimization", "label": "Graph Cut Optimization", "shape": "dot", "size": 25, "title": "Graph Cut Optimization"}, {"color": "#66CCFF", "id": "Ishikawa, H.", "label": "Ishikawa, H.", "shape": "dot", "size": 25, "title": "Ishikawa, H."}, {"color": "#66CCFF", "id": "Exact optimization for Markov random fields with convex priors", "label": "Exact optimization for Markov random fields with convex priors", "shape": "dot", "size": 25, "title": "Exact optimization for Markov random fields with convex priors"}, {"color": "#66CCFF", "id": "MRF optimization", "label": "MRF optimization", "shape": "dot", "size": 25, "title": "MRF optimization"}, {"color": "#66CCFF", "id": "Boykov, Y.", "label": "Boykov, Y.", "shape": "dot", "size": 25, "title": "Boykov, Y."}, {"color": "#66CCFF", "id": "Fast approximate energy minimization via graph cuts", "label": "Fast approximate energy minimization via graph cuts", "shape": "dot", "size": 25, "title": "Fast approximate energy minimization via graph cuts"}, {"color": "#66CCFF", "id": "graph cut methods", "label": "graph cut methods", "shape": "dot", "size": 25, "title": "graph cut methods"}, {"color": "#66CCFF", "id": "Kolmogorov, V.", "label": "Kolmogorov, V.", "shape": "dot", "size": 25, "title": "Kolmogorov, V."}, {"color": "#66CCFF", "id": "Convergent tree-reweighted message passing", "label": "Convergent tree-reweighted message passing", "shape": "dot", "size": 25, "title": "Convergent tree-reweighted message passing"}, {"color": "#66CCFF", "id": "reweighted message passing", "label": "reweighted message passing", "shape": "dot", "size": 25, "title": "reweighted message passing"}, {"color": "#66CCFF", "id": "Iteratively Reweighted Algorithms", "label": "Iteratively Reweighted Algorithms", "shape": "dot", "size": 25, "title": "Iteratively Reweighted Algorithms"}, {"color": "#66CCFF", "id": "Stereo/Inpainting", "label": "Stereo/Inpainting", "shape": "dot", "size": 25, "title": "Stereo/Inpainting"}, {"color": "#66CCFF", "id": "energy minimization", "label": "energy minimization", "shape": "dot", "size": 25, "title": "energy minimization"}, {"color": "#66CCFF", "id": "geometric relationships", "label": "geometric relationships", "shape": "dot", "size": 25, "title": "geometric relationships"}, {"color": "#66CCFF", "id": "energy minimization techniques", "label": "energy minimization techniques", "shape": "dot", "size": 25, "title": "energy minimization techniques"}, {"color": "#66CCFF", "id": "comparative study", "label": "comparative study", "shape": "dot", "size": 25, "title": "comparative study"}, {"color": "#66CCFF", "id": "Markov random fields", "label": "Markov random fields", "shape": "dot", "size": 25, "title": "Markov random fields"}, {"color": "#66CCFF", "id": "energy minimization methods", "label": "energy minimization methods", "shape": "dot", "size": 25, "title": "energy minimization methods"}, {"color": "#66CCFF", "id": "smoothness-based priors", "label": "smoothness-based priors", "shape": "dot", "size": 25, "title": "smoothness-based priors"}, {"color": "#66CCFF", "id": "pattern analysis", "label": "pattern analysis", "shape": "dot", "size": 25, "title": "pattern analysis"}, {"color": "#66CCFF", "id": "Pattern Analysis and Machine Intelligence", "label": "Pattern Analysis and Machine Intelligence", "shape": "dot", "size": 25, "title": "Pattern Analysis and Machine Intelligence"}, {"color": "#66CCFF", "id": "ds with smoothness-based priors", "label": "ds with smoothness-based priors", "shape": "dot", "size": 25, "title": "ds with smoothness-based priors"}, {"color": "#66CCFF", "id": "Boykov et al.", "label": "Boykov et al.", "shape": "dot", "size": 25, "title": "Boykov et al."}, {"color": "#66CCFF", "id": "An experimental comparison", "label": "An experimental comparison", "shape": "dot", "size": 25, "title": "An experimental comparison"}, {"color": "#66CCFF", "id": "min-cut/max-flow algorithms", "label": "min-cut/max-flow algorithms", "shape": "dot", "size": 25, "title": "min-cut/max-flow algorithms"}, {"color": "#66CCFF", "id": "Pock et al.", "label": "Pock et al.", "shape": "dot", "size": 25, "title": "Pock et al."}, {"color": "#66CCFF", "id": "convex formulation", "label": "convex formulation", "shape": "dot", "size": 25, "title": "convex formulation"}, {"color": "#66CCFF", "id": "multi-label problems", "label": "multi-label problems", "shape": "dot", "size": 25, "title": "multi-label problems"}, {"color": "#66CCFF", "id": "Computer Vision\u2013ECCV 2008", "label": "Computer Vision\u2013ECCV 2008", "shape": "dot", "size": 25, "title": "Computer Vision\u2013ECCV 2008"}, {"color": "#66CCFF", "id": "H. f.", "label": "H. f.", "shape": "dot", "size": 25, "title": "H. f."}, {"color": "#66CCFF", "id": "Kappes et al.", "label": "Kappes et al.", "shape": "dot", "size": 25, "title": "Kappes et al."}, {"color": "#66CCFF", "id": "inference techniques", "label": "inference techniques", "shape": "dot", "size": 25, "title": "inference techniques"}, {"color": "#66CCFF", "id": "Scharstein \u0026 Szeliski", "label": "Scharstein \u0026 Szeliski", "shape": "dot", "size": 25, "title": "Scharstein \u0026 Szeliski"}, {"color": "#66CCFF", "id": "dense two-frame stereo correspondence algorithms", "label": "dense two-frame stereo correspondence algorithms", "shape": "dot", "size": 25, "title": "dense two-frame stereo correspondence algorithms"}, {"color": "#66CCFF", "id": "understanding", "label": "understanding", "shape": "dot", "size": 25, "title": "understanding"}, {"color": "#66CCFF", "id": "taxonomies", "label": "taxonomies", "shape": "dot", "size": 25, "title": "taxonomies"}, {"color": "#66CCFF", "id": "stereo correspondence algorithms", "label": "stereo correspondence algorithms", "shape": "dot", "size": 25, "title": "stereo correspondence algorithms"}, {"color": "#66CCFF", "id": "Vekler", "label": "Vekler", "shape": "dot", "size": 25, "title": "Vekler"}, {"color": "#66CCFF", "id": "Multi-label moves for mrfs", "label": "Multi-label moves for mrfs", "shape": "dot", "size": 25, "title": "Multi-label moves for mrfs"}, {"color": "#66CCFF", "id": "truncated convex priors", "label": "truncated convex priors", "shape": "dot", "size": 25, "title": "truncated convex priors"}, {"color": "#66CCFF", "id": "Australian National University", "label": "Australian National University", "shape": "dot", "size": 25, "title": "Australian National University"}, {"color": "#66CCFF", "id": "Mathieu Salzmann", "label": "Mathieu Salzmann", "shape": "dot", "size": 25, "title": "Mathieu Salzmann"}, {"color": "#66CCFF", "id": "Hongdong Li", "label": "Hongdong Li", "shape": "dot", "size": 25, "title": "Hongdong Li"}, {"color": "#66CCFF", "id": "Rui Zhao", "label": "Rui Zhao", "shape": "dot", "size": 25, "title": "Rui Zhao"}, {"color": "#66CCFF", "id": "Saliency Detection by Multi-Context Deep Learning", "label": "Saliency Detection by Multi-Context Deep Learning", "shape": "dot", "size": 25, "title": "Saliency Detection by Multi-Context Deep Learning"}, {"color": "#66CCFF", "id": "Wanli Ouyang", "label": "Wanli Ouyang", "shape": "dot", "size": 25, "title": "Wanli Ouyang"}, {"color": "#66CCFF", "id": "Saliency Detection by Multi-Content Deep Learning", "label": "Saliency Detection by Multi-Content Deep Learning", "shape": "dot", "size": 25, "title": "Saliency Detection by Multi-Content Deep Learning"}, {"color": "#66CCFF", "id": "Multi-Context Deep Learning", "label": "Multi-Context Deep Learning", "shape": "dot", "size": 25, "title": "Multi-Context Deep Learning"}, {"color": "#66CCFF", "id": "Image salience detection", "label": "Image salience detection", "shape": "dot", "size": 25, "title": "Image salience detection"}, {"color": "#66CCFF", "id": "highlight visually salient regions", "label": "highlight visually salient regions", "shape": "dot", "size": 25, "title": "highlight visually salient regions"}, {"color": "#66CCFF", "id": "Conventional approaches", "label": "Conventional approaches", "shape": "dot", "size": 25, "title": "Conventional approaches"}, {"color": "#66CCFF", "id": "salient objects in low-contrast backgrounds", "label": "salient objects in low-contrast backgrounds", "shape": "dot", "size": 25, "title": "salient objects in low-contrast backgrounds"}, {"color": "#66CCFF", "id": "multi-context deep learning framework", "label": "multi-context deep learning framework", "shape": "dot", "size": 25, "title": "multi-context deep learning framework"}, {"color": "#66CCFF", "id": "salience", "label": "salience", "shape": "dot", "size": 25, "title": "salience"}, {"color": "#66CCFF", "id": "global context", "label": "global context", "shape": "dot", "size": 25, "title": "global context"}, {"color": "#66CCFF", "id": "pre-training scheme", "label": "pre-training scheme", "shape": "dot", "size": 25, "title": "pre-training scheme"}, {"color": "#66CCFF", "id": "Task-specific pre-training scheme", "label": "Task-specific pre-training scheme", "shape": "dot", "size": 25, "title": "Task-specific pre-training scheme"}, {"color": "#66CCFF", "id": "Frequency-tuned salient region detection", "label": "Frequency-tuned salient region detection", "shape": "dot", "size": 25, "title": "Frequency-tuned salient region detection"}, {"color": "#66CCFF", "id": "Training products of experts", "label": "Training products of experts", "shape": "dot", "size": 25, "title": "Training products of experts"}, {"color": "#66CCFF", "id": "Neural computation", "label": "Neural computation", "shape": "dot", "size": 25, "title": "Neural computation"}, {"color": "#66CCFF", "id": "Saliency detection", "label": "Saliency detection", "shape": "dot", "size": 25, "title": "Saliency detection"}, {"color": "#66CCFF", "id": "Category-independent object-level saliency detection", "label": "Category-independent object-level saliency detection", "shape": "dot", "size": 25, "title": "Category-independent object-level saliency detection"}, {"color": "#66CCFF", "id": "arXiv preprint", "label": "arXiv preprint", "shape": "dot", "size": 25, "title": "arXiv preprint"}, {"color": "#66CCFF", "id": "Graph-based visual saliency", "label": "Graph-based visual saliency", "shape": "dot", "size": 25, "title": "Graph-based visual saliency"}, {"color": "#66CCFF", "id": "Salient Object Detection", "label": "Salient Object Detection", "shape": "dot", "size": 25, "title": "Salient Object Detection"}, {"color": "#66CCFF", "id": "Multi-Context Modeling", "label": "Multi-Context Modeling", "shape": "dot", "size": 25, "title": "Multi-Context Modeling"}, {"color": "#66CCFF", "id": "ik", "label": "ik", "shape": "dot", "size": 25, "title": "ik"}, {"color": "#66CCFF", "id": "arXiv", "label": "arXiv", "shape": "dot", "size": 25, "title": "arXiv"}, {"color": "#66CCFF", "id": "Graph-based visual salience", "label": "Graph-based visual salience", "shape": "dot", "size": 25, "title": "Graph-based visual salience"}, {"color": "#66CCFF", "id": "A. Borji", "label": "A. Borji", "shape": "dot", "size": 25, "title": "A. Borji"}, {"color": "#66CCFF", "id": "Boosting bottom-up and top-down visual features", "label": "Boosting bottom-up and top-down visual features", "shape": "dot", "size": 25, "title": "Boosting bottom-up and top-down visual features"}, {"color": "#66CCFF", "id": "M.-M. Cheng", "label": "M.-M. Cheng", "shape": "dot", "size": 25, "title": "M.-M. Cheng"}, {"color": "#66CCFF", "id": "Global contrast based salient region detection", "label": "Global contrast based salient region detection", "shape": "dot", "size": 25, "title": "Global contrast based salient region detection"}, {"color": "#66CCFF", "id": "R. Mairon", "label": "R. Mairon", "shape": "dot", "size": 25, "title": "R. Mairon"}, {"color": "#66CCFF", "id": "A closer look at context", "label": "A closer look at context", "shape": "dot", "size": 25, "title": "A closer look at context"}, {"color": "#66CCFF", "id": "Shenzhen Institutes of Advanced Technology", "label": "Shenzhen Institutes of Advanced Technology", "shape": "dot", "size": 25, "title": "Shenzhen Institutes of Advanced Technology"}, {"color": "#66CCFF", "id": "Department of Electronic Engineering", "label": "Department of Electronic Engineering", "shape": "dot", "size": 25, "title": "Department of Electronic Engineering"}, {"color": "#66CCFF", "id": "wlouyang@ee.cuhk.edu.hk", "label": "wlouyang@ee.cuhk.edu.hk", "shape": "dot", "size": 25, "title": "wlouyang@ee.cuhk.edu.hk"}, {"color": "#66CCFF", "id": "hsli@ee.cuhk.edu.hk", "label": "hsli@ee.cuhk.edu.hk", "shape": "dot", "size": 25, "title": "hsli@ee.cuhk.edu.hk"}, {"color": "#66CCFF", "id": "Advanced Technology", "label": "Advanced Technology", "shape": "dot", "size": 25, "title": "Advanced Technology"}, {"color": "#66CCFF", "id": "Baohua Li", "label": "Baohua Li", "shape": "dot", "size": 25, "title": "Baohua Li"}, {"color": "#66CCFF", "id": "Subspace Clustering by Mixture of Gaussian Regression", "label": "Subspace Clustering by Mixture of Gaussian Regression", "shape": "dot", "size": 25, "title": "Subspace Clustering by Mixture of Gaussian Regression"}, {"color": "#66CCFF", "id": "Ying Zhang", "label": "Ying Zhang", "shape": "dot", "size": 25, "title": "Ying Zhang"}, {"color": "#66CCFF", "id": "Zhouchen Lin", "label": "Zhouchen Lin", "shape": "dot", "size": 25, "title": "Zhouchen Lin"}, {"color": "#66CCFF", "id": "Subclone Clustering by Mixture of Gaussian Regression", "label": "Subclone Clustering by Mixture of Gaussian Regression", "shape": "dot", "size": 25, "title": "Subclone Clustering by Mixture of Gaussian Regression"}, {"color": "#66CCFF", "id": "Huchuan Lu", "label": "Huchuan Lu", "shape": "dot", "size": 25, "title": "Huchuan Lu"}, {"color": "#66CCFF", "id": "Subspace clustering", "label": "Subspace clustering", "shape": "dot", "size": 25, "title": "Subspace clustering"}, {"color": "#66CCFF", "id": "multi-subspace representation", "label": "multi-subspace representation", "shape": "dot", "size": 25, "title": "multi-subspace representation"}, {"color": "#66CCFF", "id": "high-dimensional space", "label": "high-dimensional space", "shape": "dot", "size": 25, "title": "high-dimensional space"}, {"color": "#66CCFF", "id": "Existing methods", "label": "Existing methods", "shape": "dot", "size": 25, "title": "Existing methods"}, {"color": "#66CCFF", "id": "norms", "label": "norms", "shape": "dot", "size": 25, "title": "norms"}, {"color": "#66CCFF", "id": "MoG Regression", "label": "MoG Regression", "shape": "dot", "size": 25, "title": "MoG Regression"}, {"color": "#66CCFF", "id": "subspace clustering", "label": "subspace clustering", "shape": "dot", "size": 25, "title": "subspace clustering"}, {"color": "#66CCFF", "id": "Mixture of Gausians (MoG)", "label": "Mixture of Gausians (MoG)", "shape": "dot", "size": 25, "title": "Mixture of Gausians (MoG)"}, {"color": "#66CCFF", "id": "affinity matrix", "label": "affinity matrix", "shape": "dot", "size": 25, "title": "affinity matrix"}, {"color": "#66CCFF", "id": "clustering performance", "label": "clustering performance", "shape": "dot", "size": 25, "title": "clustering performance"}, {"color": "#66CCFF", "id": "MoG Regression outperforms subspace clustering methods", "label": "MoG Regression outperforms subspace clustering methods", "shape": "dot", "size": 25, "title": "MoG Regression outperforms subspace clustering methods"}, {"color": "#66CCFF", "id": "noise distributions", "label": "noise distributions", "shape": "dot", "size": 25, "title": "noise distributions"}, {"color": "#66CCFF", "id": "Noise Modeling", "label": "Noise Modeling", "shape": "dot", "size": 25, "title": "Noise Modeling"}, {"color": "#66CCFF", "id": "Subspace Clustering", "label": "Subspace Clustering", "shape": "dot", "size": 25, "title": "Subspace Clustering"}, {"color": "#66CCFF", "id": "state-of-the-art subspace clustering methods", "label": "state-of-the-art subspace clustering methods", "shape": "dot", "size": 25, "title": "state-of-the-art subspace clustering methods"}, {"color": "#66CCFF", "id": "K-plane clustering", "label": "K-plane clustering", "shape": "dot", "size": 25, "title": "K-plane clustering"}, {"color": "#66CCFF", "id": "segmentation", "label": "segmentation", "shape": "dot", "size": 25, "title": "segmentation"}, {"color": "#66CCFF", "id": "graph-based image segmentation", "label": "graph-based image segmentation", "shape": "dot", "size": 25, "title": "graph-based image segmentation"}, {"color": "#66CCFF", "id": "power factorization", "label": "power factorization", "shape": "dot", "size": 25, "title": "power factorization"}, {"color": "#66CCFF", "id": "motion segmentation", "label": "motion segmentation", "shape": "dot", "size": 25, "title": "motion segmentation"}, {"color": "#66CCFF", "id": "GPCA", "label": "GPCA", "shape": "dot", "size": 25, "title": "GPCA"}, {"color": "#66CCFF", "id": "Mixture of Gaussian Regression", "label": "Mixture of Gaussian Regression", "shape": "dot", "size": 25, "title": "Mixture of Gaussian Regression"}, {"color": "#66CCFF", "id": "clustering", "label": "clustering", "shape": "dot", "size": 25, "title": "clustering"}, {"color": "#66CCFF", "id": "Affinity Matrix Construction", "label": "Affinity Matrix Construction", "shape": "dot", "size": 25, "title": "Affinity Matrix Construction"}, {"color": "#66CCFF", "id": "Journal of Global Optimization", "label": "Journal of Global Optimization", "shape": "dot", "size": 25, "title": "Journal of Global Optimization"}, {"color": "#66CCFF", "id": "referenced papers", "label": "referenced papers", "shape": "dot", "size": 25, "title": "referenced papers"}, {"color": "#66CCFF", "id": "Spectral clustering", "label": "Spectral clustering", "shape": "dot", "size": 25, "title": "Spectral clustering"}, {"color": "#66CCFF", "id": "tutorial", "label": "tutorial", "shape": "dot", "size": 25, "title": "tutorial"}, {"color": "#66CCFF", "id": "Sparse representation", "label": "Sparse representation", "shape": "dot", "size": 25, "title": "Sparse representation"}, {"color": "#66CCFF", "id": "Wright et al.", "label": "Wright et al.", "shape": "dot", "size": 25, "title": "Wright et al."}, {"color": "#66CCFF", "id": "face recognition approach", "label": "face recognition approach", "shape": "dot", "size": 25, "title": "face recognition approach"}, {"color": "#66CCFF", "id": "Approaches", "label": "Approaches", "shape": "dot", "size": 25, "title": "Approaches"}, {"color": "#66CCFF", "id": "EM algorithm", "label": "EM algorithm", "shape": "dot", "size": 25, "title": "EM algorithm"}, {"color": "#66CCFF", "id": "Convergence properties", "label": "Convergence properties", "shape": "dot", "size": 25, "title": "Convergence properties"}, {"color": "#66CCFF", "id": "Motion segmentation", "label": "Motion segmentation", "shape": "dot", "size": 25, "title": "Motion segmentation"}, {"color": "#66CCFF", "id": "Framework", "label": "Framework", "shape": "dot", "size": 25, "title": "Framework"}, {"color": "#66CCFF", "id": "Types of motion", "label": "Types of motion", "shape": "dot", "size": 25, "title": "Types of motion"}, {"color": "#66CCFF", "id": "Gaussian mixtures", "label": "Gaussian mixtures", "shape": "dot", "size": 25, "title": "Gaussian mixtures"}, {"color": "#66CCFF", "id": "Pattern Analysis", "label": "Pattern Analysis", "shape": "dot", "size": 25, "title": "Pattern Analysis"}, {"color": "#66CCFF", "id": "various types of motion", "label": "various types of motion", "shape": "dot", "size": 25, "title": "various types of motion"}, {"color": "#66CCFF", "id": "lossy data compression", "label": "lossy data compression", "shape": "dot", "size": 25, "title": "lossy data compression"}, {"color": "#66CCFF", "id": "Dalian University of Technology", "label": "Dalian University of Technology", "shape": "dot", "size": 25, "title": "Dalian University of Technology"}, {"color": "#66CCFF", "id": "Dalian\u003c0xC2\u003e\u003c0xA0\u003eUniversity of Technology", "label": "Dalian\u003c0xC2\u003e\u003c0xA0\u003eUniversity of Technology", "shape": "dot", "size": 25, "title": "Dalian\u003c0xC2\u003e\u003c0xA0\u003eUniversity of Technology"}, {"color": "#66CCFF", "id": "robust PCA", "label": "robust PCA", "shape": "dot", "size": 25, "title": "robust PCA"}, {"color": "#66CCFF", "id": "broad framework", "label": "broad framework", "shape": "dot", "size": 25, "title": "broad framework"}, {"color": "#66CCFF", "id": "School of EECS", "label": "School of EECS", "shape": "dot", "size": 25, "title": "School of EECS"}, {"color": "#66CCFF", "id": "A Convolutional Neural Network Cascade", "label": "A Convolutional Neural Network Cascade", "shape": "dot", "size": 25, "title": "A Convolutional Neural Network Cascade"}, {"color": "#66CCFF", "id": "Face Detection", "label": "Face Detection", "shape": "dot", "size": 25, "title": "Face Detection"}, {"color": "#66CCFF", "id": "Haoxiang Li", "label": "Haoxiang Li", "shape": "dot", "size": 25, "title": "Haoxiang Li"}, {"color": "#66CCFF", "id": "Zhe Lin", "label": "Zhe Lin", "shape": "dot", "size": 25, "title": "Zhe Lin"}, {"color": "#66CCFF", "id": "Jonathan Brandt", "label": "Jonathan Brandt", "shape": "dot", "size": 25, "title": "Jonathan Brandt"}, {"color": "#66CCFF", "id": "Gang Hua", "label": "Gang Hua", "shape": "dot", "size": 25, "title": "Gang Hua"}, {"color": "#66CCFF", "id": "Face detection", "label": "Face detection", "shape": "dot", "size": 25, "title": "Face detection"}, {"color": "#66CCFF", "id": "large visual variations", "label": "large visual variations", "shape": "dot", "size": 25, "title": "large visual variations"}, {"color": "#66CCFF", "id": "large search space", "label": "large search space", "shape": "dot", "size": 25, "title": "large search space"}, {"color": "#66CCFF", "id": "Advanced models", "label": "Advanced models", "shape": "dot", "size": 25, "title": "Advanced models"}, {"color": "#66CCFF", "id": "visual variations", "label": "visual variations", "shape": "dot", "size": 25, "title": "visual variations"}, {"color": "#66CCFF", "id": "computationally expensive", "label": "computationally expensive", "shape": "dot", "size": 25, "title": "computationally expensive"}, {"color": "#66CCFF", "id": "Paper", "label": "Paper", "shape": "dot", "size": 25, "title": "Paper"}, {"color": "#66CCFF", "id": "CNN cascade architecture", "label": "CNN cascade architecture", "shape": "dot", "size": 25, "title": "CNN cascade architecture"}, {"color": "#66CCFF", "id": "conflicting demands", "label": "conflicting demands", "shape": "dot", "size": 25, "title": "conflicting demands"}, {"color": "#66CCFF", "id": "Cascade", "label": "Cascade", "shape": "dot", "size": 25, "title": "Cascade"}, {"color": "#66CCFF", "id": "background regions", "label": "background regions", "shape": "dot", "size": 25, "title": "background regions"}, {"color": "#66CCFF", "id": "CNN-based calibration stage", "label": "CNN-based calibration stage", "shape": "dot", "size": 25, "title": "CNN-based calibration stage"}, {"color": "#66CCFF", "id": "localization effectiveness", "label": "localization effectiveness", "shape": "dot", "size": 25, "title": "localization effectiveness"}, {"color": "#66CCFF", "id": "state-of-the-art detection performance", "label": "state-of-the-art detection performance", "shape": "dot", "size": 25, "title": "state-of-the-art detection performance"}, {"color": "#66CCFF", "id": "14 FPS", "label": "14 FPS", "shape": "dot", "size": 25, "title": "14 FPS"}, {"color": "#66CCFF", "id": "100 FPS", "label": "100 FPS", "shape": "dot", "size": 25, "title": "100 FPS"}, {"color": "#66CCFF", "id": "Real-time Performance", "label": "Real-time Performance", "shape": "dot", "size": 25, "title": "Real-time Performance"}, {"color": "#66CCFF", "id": "Cascade Architecture", "label": "Cascade Architecture", "shape": "dot", "size": 25, "title": "Cascade Architecture"}, {"color": "#66CCFF", "id": "Bounding Box Calibration", "label": "Bounding Box Calibration", "shape": "dot", "size": 25, "title": "Bounding Box Calibration"}, {"color": "#66CCFF", "id": "GPU", "label": "GPU", "shape": "dot", "size": 25, "title": "GPU"}, {"color": "#66CCFF", "id": "Rapid object detection", "label": "Rapid object detection", "shape": "dot", "size": 25, "title": "Rapid object detection"}, {"color": "#66CCFF", "id": "LeCun", "label": "LeCun", "shape": "dot", "size": 25, "title": "LeCun"}, {"color": "#66CCFF", "id": "Convolutional networks", "label": "Convolutional networks", "shape": "dot", "size": 25, "title": "Convolutional networks"}, {"color": "#66CCFF", "id": "speech", "label": "speech", "shape": "dot", "size": 25, "title": "speech"}, {"color": "#66CCFF", "id": "Rowley", "label": "Rowley", "shape": "dot", "size": 25, "title": "Rowley"}, {"color": "#66CCFF", "id": "Neural network-based face detection", "label": "Neural network-based face detection", "shape": "dot", "size": 25, "title": "Neural network-based face detection"}, {"color": "#66CCFF", "id": "Felzenszwalb", "label": "Felzenszwalb", "shape": "dot", "size": 25, "title": "Felzenszwalb"}, {"color": "#66CCFF", "id": "part-based models", "label": "part-based models", "shape": "dot", "size": 25, "title": "part-based models"}, {"color": "#66CCFF", "id": "P. F.", "label": "P. F.", "shape": "dot", "size": 25, "title": "P. F."}, {"color": "#66CCFF", "id": "Object detection with discriminatatively trained part-based models", "label": "Object detection with discriminatatively trained part-based models", "shape": "dot", "size": 25, "title": "Object detection with discriminatatively trained part-based models"}, {"color": "#66CCFF", "id": "Object detection with discriminatively trained part-based models", "label": "Object detection with discriminatively trained part-based models", "shape": "dot", "size": 25, "title": "Object detection with discriminatively trained part-based models"}, {"color": "#66CCFF", "id": "Jain, V.", "label": "Jain, V.", "shape": "dot", "size": 25, "title": "Jain, V."}, {"color": "#66CCFF", "id": "Fddb: A benchmark for face detection in unconstrained settings", "label": "Fddb: A benchmark for face detection in unconstrained settings", "shape": "dot", "size": 25, "title": "Fddb: A benchmark for face detection in unconstrained settings"}, {"color": "#66CCFF", "id": "Rich feature hierarchies for accurate object detection and semantic segmentation", "label": "Rich feature hierarchies for accurate object detection and semantic segmentation", "shape": "dot", "size": 25, "title": "Rich feature hierarchies for accurate object detection and semantic segmentation"}, {"color": "#66CCFF", "id": "ImageNet classification with deep convolutional neural networks", "label": "ImageNet classification with deep convolutional neural networks", "shape": "dot", "size": 25, "title": "ImageNet classification with deep convolutional neural networks"}, {"color": "#66CCFF", "id": "Jia, Y.", "label": "Jia, Y.", "shape": "dot", "size": 25, "title": "Jia, Y."}, {"color": "#66CCFF", "id": "Caffe: Convolutional architecture for fast feature embedding", "label": "Caffe: Convolutional architecture for fast feature embedding", "shape": "dot", "size": 25, "title": "Caffe: Convolutional architecture for fast feature embedding"}, {"color": "#66CCFF", "id": "fast feature embedding", "label": "fast feature embedding", "shape": "dot", "size": 25, "title": "fast feature embedding"}, {"color": "#66CCFF", "id": "University of Massachusetts, Amherst", "label": "University of Massachusetts, Amherst", "shape": "dot", "size": 25, "title": "University of Massachusetts, Amherst"}, {"color": "#66CCFF", "id": "arXiv preprint arXiv:1311.2524", "label": "arXiv preprint arXiv:1311.2524", "shape": "dot", "size": 25, "title": "arXiv preprint arXiv:1311.2524"}, {"color": "#66CCFF", "id": "Advances in neural information processing systems", "label": "Advances in neural information processing systems", "shape": "dot", "size": 25, "title": "Advances in neural information processing systems"}, {"color": "#66CCFF", "id": "helhamer, E.", "label": "helhamer, E.", "shape": "dot", "size": 25, "title": "helhamer, E."}, {"color": "#66CCFF", "id": "Vaillant, R.", "label": "Vaillant, R.", "shape": "dot", "size": 25, "title": "Vaillant, R."}, {"color": "#66CCFF", "id": "object localization approach", "label": "object localization approach", "shape": "dot", "size": 25, "title": "object localization approach"}, {"color": "#66CCFF", "id": "Yang, B.", "label": "Yang, B.", "shape": "dot", "size": 25, "title": "Yang, B."}, {"color": "#66CCFF", "id": "multi-view face detection method", "label": "multi-view face detection method", "shape": "dot", "size": 25, "title": "multi-view face detection method"}, {"color": "#66CCFF", "id": "bootstrap learning", "label": "bootstrap learning", "shape": "dot", "size": 25, "title": "bootstrap learning"}, {"color": "#66CCFF", "id": "Stevens Institute of Technology", "label": "Stevens Institute of Technology", "shape": "dot", "size": 25, "title": "Stevens Institute of Technology"}, {"color": "#66CCFF", "id": "Na Tong", "label": "Na Tong", "shape": "dot", "size": 25, "title": "Na Tong"}, {"color": "#66CCFF", "id": "Salient Object Detectio", "label": "Salient Object Detectio", "shape": "dot", "size": 25, "title": "Salient Object Detectio"}, {"color": "#66CCFF", "id": "Xiang Ruan", "label": "Xiang Ruan", "shape": "dot", "size": 25, "title": "Xiang Ruan"}, {"color": "#66CCFF", "id": "Ming-Hsuan Yang", "label": "Ming-Hsuan Yang", "shape": "dot", "size": 25, "title": "Ming-Hsuan Yang"}, {"color": "#66CCFF", "id": "Bootstrap Learning", "label": "Bootstrap Learning", "shape": "dot", "size": 25, "title": "Bootstrap Learning"}, {"color": "#66CCFF", "id": "Tong_Salient_Object_Detection_2015_CVPR_paper.pdf", "label": "Tong_Salient_Object_Detection_2015_CVPR_paper.pdf", "shape": "dot", "size": 25, "title": "Tong_Salient_Object_Detection_2015_CVPR_paper.pdf"}, {"color": "#66CCFF", "id": "bootstrap learning algorithm", "label": "bootstrap learning algorithm", "shape": "dot", "size": 25, "title": "bootstrap learning algorithm"}, {"color": "#66CCFF", "id": "weak models", "label": "weak models", "shape": "dot", "size": 25, "title": "weak models"}, {"color": "#66CCFF", "id": "strong models", "label": "strong models", "shape": "dot", "size": 25, "title": "strong models"}, {"color": "#66CCFF", "id": "weak salience map", "label": "weak salience map", "shape": "dot", "size": 25, "title": "weak salience map"}, {"color": "#66CCFF", "id": "image priors", "label": "image priors", "shape": "dot", "size": 25, "title": "image priors"}, {"color": "#66CCFF", "id": "training samples", "label": "training samples", "shape": "dot", "size": 25, "title": "training samples"}, {"color": "#66CCFF", "id": "strong classi\ufb01er", "label": "strong classi\ufb01er", "shape": "dot", "size": 25, "title": "strong classi\ufb01er"}, {"color": "#66CCFF", "id": "salient pixels", "label": "salient pixels", "shape": "dot", "size": 25, "title": "salient pixels"}, {"color": "#66CCFF", "id": "input image", "label": "input image", "shape": "dot", "size": 25, "title": "input image"}, {"color": "#66CCFF", "id": "multiscale salience maps", "label": "multiscale salience maps", "shape": "dot", "size": 25, "title": "multiscale salience maps"}, {"color": "#66CCFF", "id": "state-of-the-art salience detection methods", "label": "state-of-the-art salience detection methods", "shape": "dot", "size": 25, "title": "state-of-the-art salience detection methods"}, {"color": "#66CCFF", "id": "bootstrap learning approach", "label": "bootstrap learning approach", "shape": "dot", "size": 25, "title": "bootstrap learning approach"}, {"color": "#66CCFF", "id": "signi\ufb01cant improvement", "label": "signi\ufb01cant improvement", "shape": "dot", "size": 25, "title": "signi\ufb01cant improvement"}, {"color": "#66CCFF", "id": "Salieny Detection Methods", "label": "Salieny Detection Methods", "shape": "dot", "size": 25, "title": "Salieny Detection Methods"}, {"color": "#66CCFF", "id": "Bootstrap Learning Approach", "label": "Bootstrap Learning Approach", "shape": "dot", "size": 25, "title": "Bootstrap Learning Approach"}, {"color": "#66CCFF", "id": "Bottom-up Salieny Models", "label": "Bottom-up Salieny Models", "shape": "dot", "size": 25, "title": "Bottom-up Salieny Models"}, {"color": "#66CCFF", "id": "Li et al. (2014)", "label": "Li et al. (2014)", "shape": "dot", "size": 25, "title": "Li et al. (2014)"}, {"color": "#66CCFF", "id": "Salient Object Segmentation", "label": "Salient Object Segmentation", "shape": "dot", "size": 25, "title": "Salient Object Segmentation"}, {"color": "#66CCFF", "id": "Achanta et al. (2009)", "label": "Achanta et al. (2009)", "shape": "dot", "size": 25, "title": "Achanta et al. (2009)"}, {"color": "#66CCFF", "id": "Frequency-tuned Salient Region Detection", "label": "Frequency-tuned Salient Region Detection", "shape": "dot", "size": 25, "title": "Frequency-tuned Salient Region Detection"}, {"color": "#66CCFF", "id": "Movahedi \u0026 Elder (2010)", "label": "Movahedi \u0026 Elder (2010)", "shape": "dot", "size": 25, "title": "Movahedi \u0026 Elder (2010)"}, {"color": "#66CCFF", "id": "Performance Measures", "label": "Performance Measures", "shape": "dot", "size": 25, "title": "Performance Measures"}, {"color": "#66CCFF", "id": "Achanta et al. (2010)", "label": "Achanta et al. (2010)", "shape": "dot", "size": 25, "title": "Achanta et al. (2010)"}, {"color": "#66CCFF", "id": "Slic Superpixels", "label": "Slic Superpixels", "shape": "dot", "size": 25, "title": "Slic Superpixels"}, {"color": "#66CCFF", "id": "Achanta, R.", "label": "Achanta, R.", "shape": "dot", "size": 25, "title": "Achanta, R."}, {"color": "#66CCFF", "id": "Ojala, T.", "label": "Ojala, T.", "shape": "dot", "size": 25, "title": "Ojala, T."}, {"color": "#66CCFF", "id": "Multiresolution gray-scale texture classification", "label": "Multiresolution gray-scale texture classification", "shape": "dot", "size": 25, "title": "Multiresolution gray-scale texture classification"}, {"color": "#66CCFF", "id": "Bach, F. R.", "label": "Bach, F. R.", "shape": "dot", "size": 25, "title": "Bach, F. R."}, {"color": "#66CCFF", "id": "Multiple kernel learning", "label": "Multiple kernel learning", "shape": "dot", "size": 25, "title": "Multiple kernel learning"}, {"color": "#66CCFF", "id": "Perazzi, F.", "label": "Perazzi, F.", "shape": "dot", "size": 25, "title": "Perazzi, F."}, {"color": "#66CCFF", "id": "Saliency filters", "label": "Saliency filters", "shape": "dot", "size": 25, "title": "Saliency filters"}, {"color": "#66CCFF", "id": "Borji, A.", "label": "Borji, A.", "shape": "dot", "size": 25, "title": "Borji, A."}, {"color": "#66CCFF", "id": "Salient object detection benchmark", "label": "Salient object detection benchmark", "shape": "dot", "size": 25, "title": "Salient object detection benchmark"}, {"color": "#66CCFF", "id": "authored", "label": "authored", "shape": "dot", "size": 25, "title": "authored"}, {"color": "#66CCFF", "id": "Rahtu, E.", "label": "Rahtu, E.", "shape": "dot", "size": 25, "title": "Rahtu, E."}, {"color": "#66CCFF", "id": "Salient object segmentation", "label": "Salient object segmentation", "shape": "dot", "size": 25, "title": "Salient object segmentation"}, {"color": "#66CCFF", "id": "performance measures", "label": "performance measures", "shape": "dot", "size": 25, "title": "performance measures"}, {"color": "#66CCFF", "id": "superpixels", "label": "superpixels", "shape": "dot", "size": 25, "title": "superpixels"}, {"color": "#66CCFF", "id": "Local binary patterns", "label": "Local binary patterns", "shape": "dot", "size": 25, "title": "Local binary patterns"}, {"color": "#66CCFF", "id": "SMO algorithm", "label": "SMO algorithm", "shape": "dot", "size": 25, "title": "SMO algorithm"}, {"color": "#66CCFF", "id": "J. Heikkil\u00e4", "label": "J. Heikkil\u00e4", "shape": "dot", "size": 25, "title": "J. Heikkil\u00e4"}, {"color": "#66CCFF", "id": "Segmenting salient objects from images and videos", "label": "Segmenting salient objects from images and videos", "shape": "dot", "size": 25, "title": "Segmenting salient objects from images and videos"}, {"color": "#66CCFF", "id": "Kaiming He", "label": "Kaiming He", "shape": "dot", "size": 25, "title": "Kaiming He"}, {"color": "#66CCFF", "id": "Convolutional Neural Networks at Constrained Time Cost", "label": "Convolutional Neural Networks at Constrained Time Cost", "shape": "dot", "size": 25, "title": "Convolutional Neural Networks at Constrained Time Cost"}, {"color": "#66CCFF", "id": "Convolutional Neural Networks at Constained Time Cost", "label": "Convolutional Neural Networks at Constained Time Cost", "shape": "dot", "size": 25, "title": "Convolutional Neural Networks at Constained Time Cost"}, {"color": "#66CCFF", "id": "OMRON Corporation", "label": "OMRON Corporation", "shape": "dot", "size": 25, "title": "OMRON Corporation"}, {"color": "#66CCFF", "id": "University of California at Merced", "label": "University of California at Merced", "shape": "dot", "size": 25, "title": "University of California at Merced"}, {"color": "#66CCFF", "id": "image recognition accuracy", "label": "image recognition accuracy", "shape": "dot", "size": 25, "title": "image recognition accuracy"}, {"color": "#66CCFF", "id": "complex", "label": "complex", "shape": "dot", "size": 25, "title": "complex"}, {"color": "#66CCFF", "id": "time-consuming", "label": "time-consuming", "shape": "dot", "size": 25, "title": "time-consuming"}, {"color": "#66CCFF", "id": "CNN architectures", "label": "CNN architectures", "shape": "dot", "size": 25, "title": "CNN architectures"}, {"color": "#66CCFF", "id": "architecture", "label": "architecture", "shape": "dot", "size": 25, "title": "architecture"}, {"color": "#66CCFF", "id": "competitive accuracy", "label": "competitive accuracy", "shape": "dot", "size": 25, "title": "competitive accuracy"}, {"color": "#66CCFF", "id": "20% faster than AlexNet", "label": "20% faster than AlexNet", "shape": "dot", "size": 25, "title": "20% faster than AlexNet"}, {"color": "#66CCFF", "id": "time constraints during offline training", "label": "time constraints during offline training", "shape": "dot", "size": 25, "title": "time constraints during offline training"}, {"color": "#66CCFF", "id": "accuracy improvements", "label": "accuracy improvements", "shape": "dot", "size": 25, "title": "accuracy improvements"}, {"color": "#66CCFF", "id": "factors", "label": "factors", "shape": "dot", "size": 25, "title": "factors"}, {"color": "#66CCFF", "id": "ImageNet dataset", "label": "ImageNet dataset", "shape": "dot", "size": 25, "title": "ImageNet dataset"}, {"color": "#66CCFF", "id": "11.8% top-5 error", "label": "11.8% top-5 error", "shape": "dot", "size": 25, "title": "11.8% top-5 error"}, {"color": "#66CCFF", "id": "Offline Training", "label": "Offline Training", "shape": "dot", "size": 25, "title": "Offline Training"}, {"color": "#66CCFF", "id": "Time Constraints", "label": "Time Constraints", "shape": "dot", "size": 25, "title": "Time Constraints"}, {"color": "#66CCFF", "id": "Accuracy Improvements", "label": "Accuracy Improvements", "shape": "dot", "size": 25, "title": "Accuracy Improvements"}, {"color": "#66CCFF", "id": "Factors", "label": "Factors", "shape": "dot", "size": 25, "title": "Factors"}, {"color": "#66CCFF", "id": "Architecture", "label": "Architecture", "shape": "dot", "size": 25, "title": "Architecture"}, {"color": "#66CCFF", "id": "ImageNet Dataset", "label": "ImageNet Dataset", "shape": "dot", "size": 25, "title": "ImageNet Dataset"}, {"color": "#66CCFF", "id": "Computer Vision Tasks", "label": "Computer Vision Tasks", "shape": "dot", "size": 25, "title": "Computer Vision Tasks"}, {"color": "#66CCFF", "id": "Deng et al. (2009)", "label": "Deng et al. (2009)", "shape": "dot", "size": 25, "title": "Deng et al. (2009)"}, {"color": "#66CCFF", "id": "Limited Time Budget", "label": "Limited Time Budget", "shape": "dot", "size": 25, "title": "Limited Time Budget"}, {"color": "#66CCFF", "id": "Layer Replacement", "label": "Layer Replacement", "shape": "dot", "size": 25, "title": "Layer Replacement"}, {"color": "#66CCFF", "id": "Architecture Optimization", "label": "Architecture Optimization", "shape": "dot", "size": 25, "title": "Architecture Optimization"}, {"color": "#66CCFF", "id": "Image Database", "label": "Image Database", "shape": "dot", "size": 25, "title": "Image Database"}, {"color": "#66CCFF", "id": "ImageNet", "label": "ImageNet", "shape": "dot", "size": 25, "title": "ImageNet"}, {"color": "#66CCFF", "id": "computer vision tasks", "label": "computer vision tasks", "shape": "dot", "size": 25, "title": "computer vision tasks"}, {"color": "#66CCFF", "id": "hierarchical image database", "label": "hierarchical image database", "shape": "dot", "size": 25, "title": "hierarchical image database"}, {"color": "#66CCFF", "id": "very deep convolutional networks", "label": "very deep convolutional networks", "shape": "dot", "size": 25, "title": "very deep convolutional networks"}, {"color": "#66CCFF", "id": "development", "label": "development", "shape": "dot", "size": 25, "title": "development"}, {"color": "#66CCFF", "id": "multi-column deep neural networks", "label": "multi-column deep neural networks", "shape": "dot", "size": 25, "title": "multi-column deep neural networks"}, {"color": "#66CCFF", "id": "power of deep learning", "label": "power of deep learning", "shape": "dot", "size": 25, "title": "power of deep learning"}, {"color": "#66CCFF", "id": "image classification", "label": "image classification", "shape": "dot", "size": 25, "title": "image classification"}, {"color": "#66CCFF", "id": "rich feature hierarchies", "label": "rich feature hierarchies", "shape": "dot", "size": 25, "title": "rich feature hierarchies"}, {"color": "#66CCFF", "id": "network architecture", "label": "network architecture", "shape": "dot", "size": 25, "title": "network architecture"}, {"color": "#66CCFF", "id": "deep learning", "label": "deep learning", "shape": "dot", "size": 25, "title": "deep learning"}, {"color": "#66CCFF", "id": "Malik et al. (2014)", "label": "Malik et al. (2014)", "shape": "dot", "size": 25, "title": "Malik et al. (2014)"}, {"color": "#66CCFF", "id": "Zeiler et al. (2014)", "label": "Zeiler et al. (2014)", "shape": "dot", "size": 25, "title": "Zeiler et al. (2014)"}, {"color": "#66CCFF", "id": "convolutional neural networks", "label": "convolutional neural networks", "shape": "dot", "size": 25, "title": "convolutional neural networks"}, {"color": "#66CCFF", "id": "interpretability", "label": "interpretability", "shape": "dot", "size": 25, "title": "interpretability"}, {"color": "#66CCFF", "id": "Eigen et al. (2013)", "label": "Eigen et al. (2013)", "shape": "dot", "size": 25, "title": "Eigen et al. (2013)"}, {"color": "#66CCFF", "id": "deep architectures", "label": "deep architectures", "shape": "dot", "size": 25, "title": "deep architectures"}, {"color": "#66CCFF", "id": "recursive convolutional networks", "label": "recursive convolutional networks", "shape": "dot", "size": 25, "title": "recursive convolutional networks"}, {"color": "#66CCFF", "id": "Chatfield et al. (2014)", "label": "Chatfield et al. (2014)", "shape": "dot", "size": 25, "title": "Chatfield et al. (2014)"}, {"color": "#66CCFF", "id": "convolutional networks", "label": "convolutional networks", "shape": "dot", "size": 25, "title": "convolutional networks"}, {"color": "#66CCFF", "id": "details", "label": "details", "shape": "dot", "size": 25, "title": "details"}, {"color": "#66CCFF", "id": "Return of the devil in the details", "label": "Return of the devil in the details", "shape": "dot", "size": 25, "title": "Return of the devil in the details"}, {"color": "#66CCFF", "id": "Overfeat", "label": "Overfeat", "shape": "dot", "size": 25, "title": "Overfeat"}, {"color": "#66CCFF", "id": "integrated recognition, localization, and detection system", "label": "integrated recognition, localization, and detection system", "shape": "dot", "size": 25, "title": "integrated recognition, localization, and detection system"}, {"color": "#66CCFF", "id": "Going deeper with convolutions", "label": "Going deeper with convolutions", "shape": "dot", "size": 25, "title": "Going deeper with convolutions"}, {"color": "#66CCFF", "id": "convolutions", "label": "convolutions", "shape": "dot", "size": 25, "title": "convolutions"}, {"color": "#66CCFF", "id": "Lei Zhang", "label": "Lei Zhang", "shape": "dot", "size": 25, "title": "Lei Zhang"}, {"color": "#66CCFF", "id": "Reweighted Laplace Prior Based Hyperspectral Compressive Sensing", "label": "Reweighted Laplace Prior Based Hyperspectral Compressive Sensing", "shape": "dot", "size": 25, "title": "Reweighted Laplace Prior Based Hyperspectral Compressive Sensing"}, {"color": "#66CCFF", "id": "Yanning Zhang", "label": "Yanning Zhang", "shape": "dot", "size": 25, "title": "Yanning Zhang"}, {"color": "#66CCFF", "id": "Chunna Tian", "label": "Chunna Tian", "shape": "dot", "size": 25, "title": "Chunna Tian"}, {"color": "#66CCFF", "id": "Fei Li", "label": "Fei Li", "shape": "dot", "size": 25, "title": "Fei Li"}, {"color": "#66CCFF", "id": "Wei Wei", "label": "Wei Wei", "shape": "dot", "size": 25, "title": "Wei Wei"}, {"color": "#66CCFF", "id": "Reweighted Laplace Prior Based Hyperspectra Compressives Sensing", "label": "Reweighted Laplace Prior Based Hyperspectra Compressives Sensing", "shape": "dot", "size": 25, "title": "Reweighted Laplace Prior Based Hyperspectra Compressives Sensing"}, {"color": "#66CCFF", "id": "Reweighted Laplace Prior Based Hyperspectral Compressives Sensing", "label": "Reweighted Laplace Prior Based Hyperspectral Compressives Sensing", "shape": "dot", "size": 25, "title": "Reweighted Laplace Prior Based Hyperspectral Compressives Sensing"}, {"color": "#66CCFF", "id": "Hyperspectral Compressives Sensing", "label": "Hyperspectral Compressives Sensing", "shape": "dot", "size": 25, "title": "Hyperspectral Compressives Sensing"}, {"color": "#66CCFF", "id": "Laplace Prior", "label": "Laplace Prior", "shape": "dot", "size": 25, "title": "Laplace Prior"}, {"color": "#66CCFF", "id": "Zhang_Reweighted_Laplace_Prior_2015_CVPR_supplemental.pdf", "label": "Zhang_Reweighted_Laplace_Prior_2015_CVPR_supplemental.pdf", "shape": "dot", "size": 25, "title": "Zhang_Reweighted_Laplace_Prior_2015_CVPR_supplemental.pdf"}, {"color": "#66CCFF", "id": "Zhang_Reweighted_Lapless_Prior_2015_CVPR_supplemental.pdf", "label": "Zhang_Reweighted_Lapless_Prior_2015_CVPR_supplemental.pdf", "shape": "dot", "size": 25, "title": "Zhang_Reweighted_Lapless_Prior_2015_CVPR_supplemental.pdf"}, {"color": "#66CCFF", "id": "hyperspectral compressive sensing method", "label": "hyperspectral compressive sensing method", "shape": "dot", "size": 25, "title": "hyperspectral compressive sensing method"}, {"color": "#66CCFF", "id": "reweighted Laplace prior", "label": "reweighted Laplace prior", "shape": "dot", "size": 25, "title": "reweighted Laplace prior"}, {"color": "#66CCFF", "id": "unknown sparsity", "label": "unknown sparsity", "shape": "dot", "size": 25, "title": "unknown sparsity"}, {"color": "#66CCFF", "id": "optimization procedure", "label": "optimization procedure", "shape": "dot", "size": 25, "title": "optimization procedure"}, {"color": "#66CCFF", "id": "matrix algebra manipulations", "label": "matrix algebra manipulations", "shape": "dot", "size": 25, "title": "matrix algebra manipulations"}, {"color": "#66CCFF", "id": "conjugate functions", "label": "conjugate functions", "shape": "dot", "size": 25, "title": "conjugate functions"}, {"color": "#66CCFF", "id": "non-convex optimization problems", "label": "non-convex optimization problems", "shape": "dot", "size": 25, "title": "non-convex optimization problems"}, {"color": "#66CCFF", "id": "sparsity learning over \u03b3", "label": "sparsity learning over \u03b3", "shape": "dot", "size": 25, "title": "sparsity learning over \u03b3"}, {"color": "#66CCFF", "id": "noise estimation over \u03bb", "label": "noise estimation over \u03bb", "shape": "dot", "size": 25, "title": "noise estimation over \u03bb"}, {"color": "#66CCFF", "id": "Hyberspectral Compressive Sensing", "label": "Hyberspectral Compressive Sensing", "shape": "dot", "size": 25, "title": "Hyberspectral Compressive Sensing"}, {"color": "#66CCFF", "id": "Reweighted Laplace Prior", "label": "Reweighted Laplace Prior", "shape": "dot", "size": 25, "title": "Reweighted Laplace Prior"}, {"color": "#66CCFF", "id": "optimization technique", "label": "optimization technique", "shape": "dot", "size": 25, "title": "optimization technique"}, {"color": "#66CCFF", "id": "Sparsity Learning", "label": "Sparsity Learning", "shape": "dot", "size": 25, "title": "Sparsity Learning"}, {"color": "#66CCFF", "id": "Noise Estimation", "label": "Noise Estimation", "shape": "dot", "size": 25, "title": "Noise Estimation"}, {"color": "#66CCFF", "id": "Fang_Collaborative_Feature_Learning_2015_CVPR_paper", "label": "Fang_Collaborative_Feature_Learning_2015_CVPR_paper", "shape": "dot", "size": 25, "title": "Fang_Collaborative_Feature_Learning_2015_CVPR_paper"}, {"color": "#66CCFF", "id": "research paper", "label": "research paper", "shape": "dot", "size": 25, "title": "research paper"}, {"color": "#66CCFF", "id": "Paper Abstract", "label": "Paper Abstract", "shape": "dot", "size": 25, "title": "Paper Abstract"}, {"color": "#66CCFF", "id": "readable text", "label": "readable text", "shape": "dot", "size": 25, "title": "readable text"}, {"color": "#66CCFF", "id": "Research", "label": "Research", "shape": "dot", "size": 25, "title": "Research"}, {"color": "#66CCFF", "id": "Data Encoding/Decoding", "label": "Data Encoding/Decoding", "shape": "dot", "size": 25, "title": "Data Encoding/Decoding"}, {"color": "#66CCFF", "id": "Text Corruption/Error Correction", "label": "Text Corruption/Error Correction", "shape": "dot", "size": 25, "title": "Text Corruption/Error Correction"}, {"color": "#66CCFF", "id": "Information Retrieval", "label": "Information Retrieval", "shape": "dot", "size": 25, "title": "Information Retrieval"}, {"color": "#66CCFF", "id": "Victor Escorcia", "label": "Victor Escorcia", "shape": "dot", "size": 25, "title": "Victor Escorcia"}, {"color": "#66CCFF", "id": "On the Relationship between Visual Attributes and Convolutional Networks", "label": "On the Relationship between Visual Attributes and Convolutional Networks", "shape": "dot", "size": 25, "title": "On the Relationship between Visual Attributes and Convolutional Networks"}, {"color": "#66CCFF", "id": "Juan Carlos Niebles", "label": "Juan Carlos Niebles", "shape": "dot", "size": 25, "title": "Juan Carlos Niebles"}, {"color": "#66CCFF", "id": "Bernard Ghanem", "label": "Bernard Ghanem", "shape": "dot", "size": 25, "title": "Bernard Ghanem"}, {"color": "#66CCFF", "id": "Convolutional Networks", "label": "Convolutional Networks", "shape": "dot", "size": 25, "title": "Convolutional Networks"}, {"color": "#66CCFF", "id": "Visual Attributes", "label": "Visual Attributes", "shape": "dot", "size": 25, "title": "Visual Attributes"}, {"color": "#66CCFF", "id": "visual attributes", "label": "visual attributes", "shape": "dot", "size": 25, "title": "visual attributes"}, {"color": "#66CCFF", "id": "Escorcia_On_the_Relationship_2015_CVPR_paper.pdf", "label": "Escorcia_On_the_Relationship_2015_CVPR_paper.pdf", "shape": "dot", "size": 25, "title": "Escorcia_On_the_Relationship_2015_CVPR_paper.pdf"}, {"color": "#66CCFF", "id": "/mnt/DATA/Glucomaa/LLM/Ollama_pdf_handle/cvpr_papers", "label": "/mnt/DATA/Glucomaa/LLM/Ollama_pdf_handle/cvpr_papers", "shape": "dot", "size": 25, "title": "/mnt/DATA/Glucomaa/LLM/Ollama_pdf_handle/cvpr_papers"}, {"color": "#66CCFF", "id": "conv-nets", "label": "conv-nets", "shape": "dot", "size": 25, "title": "conv-nets"}, {"color": "#66CCFF", "id": "abstract concepts", "label": "abstract concepts", "shape": "dot", "size": 25, "title": "abstract concepts"}, {"color": "#66CCFF", "id": "objects in images", "label": "objects in images", "shape": "dot", "size": 25, "title": "objects in images"}, {"color": "#66CCFF", "id": "semantic visual attributes", "label": "semantic visual attributes", "shape": "dot", "size": 25, "title": "semantic visual attributes"}, {"color": "#66CCFF", "id": "object description", "label": "object description", "shape": "dot", "size": 25, "title": "object description"}, {"color": "#66CCFF", "id": "Attribute Centric Nodes (ACNs)", "label": "Attribute Centric Nodes (ACNs)", "shape": "dot", "size": 25, "title": "Attribute Centric Nodes (ACNs)"}, {"color": "#66CCFF", "id": "conv-net", "label": "conv-net", "shape": "dot", "size": 25, "title": "conv-net"}, {"color": "#66CCFF", "id": "objects", "label": "objects", "shape": "dot", "size": 25, "title": "objects"}, {"color": "#66CCFF", "id": "visual attribute representation", "label": "visual attribute representation", "shape": "dot", "size": 25, "title": "visual attribute representation"}, {"color": "#66CCFF", "id": "discrimination", "label": "discrimination", "shape": "dot", "size": 25, "title": "discrimination"}, {"color": "#66CCFF", "id": "conv-net nodes", "label": "conv-net nodes", "shape": "dot", "size": 25, "title": "conv-net nodes"}, {"color": "#66CCFF", "id": "information", "label": "information", "shape": "dot", "size": 25, "title": "information"}, {"color": "#66CCFF", "id": "layers", "label": "layers", "shape": "dot", "size": 25, "title": "layers"}, {"color": "#66CCFF", "id": "sparsely distributed", "label": "sparsely distributed", "shape": "dot", "size": 25, "title": "sparsely distributed"}, {"color": "#66CCFF", "id": "unevenly distributed", "label": "unevenly distributed", "shape": "dot", "size": 25, "title": "unevenly distributed"}, {"color": "#66CCFF", "id": "visual attribute representation and discrimination", "label": "visual attribute representation and discrimination", "shape": "dot", "size": 25, "title": "visual attribute representation and discrimination"}, {"color": "#66CCFF", "id": "conv-net based object recognition", "label": "conv-net based object recognition", "shape": "dot", "size": 25, "title": "conv-net based object recognition"}, {"color": "#66CCFF", "id": "Zero-Shot Object Recognition", "label": "Zero-Shot Object Recognition", "shape": "dot", "size": 25, "title": "Zero-Shot Object Recognition"}, {"color": "#66CCFF", "id": "Semantic Manifold Distance", "label": "Semantic Manifold Distance", "shape": "dot", "size": 25, "title": "Semantic Manifold Distance"}, {"color": "#66CCFF", "id": "Zhenyong Fu", "label": "Zhenyong Fu", "shape": "dot", "size": 25, "title": "Zhenyong Fu"}, {"color": "#66CCFF", "id": "Tao Xiang", "label": "Tao Xiang", "shape": "dot", "size": 25, "title": "Tao Xiang"}, {"color": "#66CCFF", "id": "Elyor Kodirov", "label": "Elyor Kodirov", "shape": "dot", "size": 25, "title": "Elyor Kodirov"}, {"color": "#66CCFF", "id": "Zero-Shot Object Research", "label": "Zero-Shot Object Research", "shape": "dot", "size": 25, "title": "Zero-Shot Object Research"}, {"color": "#66CCFF", "id": "Shaogang Gong", "label": "Shaogang Gong", "shape": "dot", "size": 25, "title": "Shaogang Gong"}, {"color": "#66CCFF", "id": "King Abdullah University of Science and Technology", "label": "King Abdullah University of Science and Technology", "shape": "dot", "size": 25, "title": "King Abdullah University of Science and Technology"}, {"color": "#66CCFF", "id": "Universidad del Norte", "label": "Universidad del Norte", "shape": "dot", "size": 25, "title": "Universidad del Norte"}, {"color": "#66CCFF", "id": "Zero-shot learning", "label": "Zero-shot learning", "shape": "dot", "size": 25, "title": "Zero-shot learning"}, {"color": "#66CCFF", "id": "recognise objects", "label": "recognise objects", "shape": "dot", "size": 25, "title": "recognise objects"}, {"color": "#66CCFF", "id": "knowledge transfer", "label": "knowledge transfer", "shape": "dot", "size": 25, "title": "knowledge transfer"}, {"color": "#66CCFF", "id": "existing works", "label": "existing works", "shape": "dot", "size": 25, "title": "existing works"}, {"color": "#66CCFF", "id": "similarity", "label": "similarity", "shape": "dot", "size": 25, "title": "similarity"}, {"color": "#66CCFF", "id": "semantic embedding space", "label": "semantic embedding space", "shape": "dot", "size": 25, "title": "semantic embedding space"}, {"color": "#66CCFF", "id": "distance metrics", "label": "distance metrics", "shape": "dot", "size": 25, "title": "distance metrics"}, {"color": "#66CCFF", "id": "intrinsic structure", "label": "intrinsic structure", "shape": "dot", "size": 25, "title": "intrinsic structure"}, {"color": "#66CCFF", "id": "semantic categories", "label": "semantic categories", "shape": "dot", "size": 25, "title": "semantic categories"}, {"color": "#66CCFF", "id": "semantic class label graph", "label": "semantic class label graph", "shape": "dot", "size": 25, "title": "semantic class label graph"}, {"color": "#66CCFF", "id": "absorbing Markov chain process", "label": "absorbing Markov chain process", "shape": "dot", "size": 25, "title": "absorbing Markov chain process"}, {"color": "#66CCFF", "id": "semantic manifold distance", "label": "semantic manifold distance", "shape": "dot", "size": 25, "title": "semantic manifold distance"}, {"color": "#66CCFF", "id": "existing ZSL algorithms", "label": "existing ZSL algorithms", "shape": "dot", "size": 25, "title": "existing ZSL algorithms"}, {"color": "#66CCFF", "id": "performance gains", "label": "performance gains", "shape": "dot", "size": 25, "title": "performance gains"}, {"color": "#66CCFF", "id": "AwA datasets", "label": "AwA datasets", "shape": "dot", "size": 25, "title": "AwA datasets"}, {"color": "#66CCFF", "id": "proposed model", "label": "proposed model", "shape": "dot", "size": 25, "title": "proposed model"}, {"color": "#66CCFF", "id": "ZSL algorithms", "label": "ZSL algorithms", "shape": "dot", "size": 25, "title": "ZSL algorithms"}, {"color": "#66CCFF", "id": "semantic manifold", "label": "semantic manifold", "shape": "dot", "size": 25, "title": "semantic manifold"}, {"color": "#66CCFF", "id": "AMP", "label": "AMP", "shape": "dot", "size": 25, "title": "AMP"}, {"color": "#66CCFF", "id": "Label-embedding", "label": "Label-embedding", "shape": "dot", "size": 25, "title": "Label-embedding"}, {"color": "#66CCFF", "id": "Akata et al. (2013)", "label": "Akata et al. (2013)", "shape": "dot", "size": 25, "title": "Akata et al. (2013)"}, {"color": "#66CCFF", "id": "Single-example learning", "label": "Single-example learning", "shape": "dot", "size": 25, "title": "Single-example learning"}, {"color": "#66CCFF", "id": "Bart \u0026 Ullman (2005)", "label": "Bart \u0026 Ullman (2005)", "shape": "dot", "size": 25, "title": "Bart \u0026 Ullman (2005)"}, {"color": "#66CCFF", "id": "Cluster kernels", "label": "Cluster kernels", "shape": "dot", "size": 25, "title": "Cluster kernels"}, {"color": "#66CCFF", "id": "Chapelle et al. (2002)", "label": "Chapelle et al. (2002)", "shape": "dot", "size": 25, "title": "Chapelle et al. (2002)"}, {"color": "#66CCFF", "id": "Zero-shot learning (ZSL)", "label": "Zero-shot learning (ZSL)", "shape": "dot", "size": 25, "title": "Zero-shot learning (ZSL)"}, {"color": "#66CCFF", "id": "Chapelle", "label": "Chapelle", "shape": "dot", "size": 25, "title": "Chapelle"}, {"color": "#66CCFF", "id": "Weston", "label": "Weston", "shape": "dot", "size": 25, "title": "Weston"}, {"color": "#66CCFF", "id": "Sch\u00f6lkopf", "label": "Sch\u00f6lkopf", "shape": "dot", "size": 25, "title": "Sch\u00f6lkopf"}, {"color": "#66CCFF", "id": "Deng", "label": "Deng", "shape": "dot", "size": 25, "title": "Deng"}, {"color": "#66CCFF", "id": "Large-scale object classification", "label": "Large-scale object classification", "shape": "dot", "size": 25, "title": "Large-scale object classification"}, {"color": "#66CCFF", "id": "Ding", "label": "Ding", "shape": "dot", "size": 25, "title": "Ding"}, {"color": "#66CCFF", "id": "Label relation graphs", "label": "Label relation graphs", "shape": "dot", "size": 25, "title": "Label relation graphs"}, {"color": "#66CCFF", "id": "Dong", "label": "Dong", "shape": "dot", "size": 25, "title": "Dong"}, {"color": "#66CCFF", "id": "Frome", "label": "Frome", "shape": "dot", "size": 25, "title": "Frome"}, {"color": "#66CCFF", "id": "Devise", "label": "Devise", "shape": "dot", "size": 25, "title": "Devise"}, {"color": "#66CCFF", "id": "embedding model", "label": "embedding model", "shape": "dot", "size": 25, "title": "embedding model"}, {"color": "#66CCFF", "id": "Transductive multi-view embedding", "label": "Transductive multi-view embedding", "shape": "dot", "size": 25, "title": "Transductive multi-view embedding"}, {"color": "#66CCFF", "id": "Zero shot recognition with unreliable attributes", "label": "Zero shot recognition with unreliable attributes", "shape": "dot", "size": 25, "title": "Zero shot recognition with unreliable attributes"}, {"color": "#66CCFF", "id": "ImageNet classification", "label": "ImageNet classification", "shape": "dot", "size": 25, "title": "ImageNet classification"}, {"color": "#66CCFF", "id": "Efficient estimation of word representations", "label": "Efficient estimation of word representations", "shape": "dot", "size": 25, "title": "Efficient estimation of word representations"}, {"color": "#66CCFF", "id": "Queen Mary, University of London", "label": "Queen Mary, University of London", "shape": "dot", "size": 25, "title": "Queen Mary, University of London"}, {"color": "#66CCFF", "id": "visual-semantic embedding", "label": "visual-semantic embedding", "shape": "dot", "size": 25, "title": "visual-semantic embedding"}, {"color": "#66CCFF", "id": "zero-shot recognition", "label": "zero-shot recognition", "shape": "dot", "size": 25, "title": "zero-shot recognition"}, {"color": "#66CCFF", "id": "QueenMary, University of London", "label": "QueenMary, University of London", "shape": "dot", "size": 25, "title": "QueenMary, University of London"}, {"color": "#66CCFF", "id": "Sakrapee Paisitkriangkrai", "label": "Sakrapee Paisitkriangkrai", "shape": "dot", "size": 25, "title": "Sakrapee Paisitkriangkrai"}, {"color": "#66CCFF", "id": "Learning to rank in person re-identi\ufb01cation with metric ensembles", "label": "Learning to rank in person re-identi\ufb01cation with metric ensembles", "shape": "dot", "size": 25, "title": "Learning to rank in person re-identi\ufb01cation with metric ensembles"}, {"color": "#66CCFF", "id": "Chunhua Shen", "label": "Chunhua Shen", "shape": "dot", "size": 25, "title": "Chunhua Shen"}, {"color": "#66CCFF", "id": "Anton van den Hengel", "label": "Anton van den Hengel", "shape": "dot", "size": 25, "title": "Anton van den Hengel"}, {"color": "#66CCFF", "id": "Paisitkriangrai_Learning_to_Rank_2015_CVPR_paper.pdf", "label": "Paisitkriangrai_Learning_to_Rank_2015_CVPR_paper.pdf", "shape": "dot", "size": 25, "title": "Paisitkriangrai_Learning_to_Rank_2015_CVPR_paper.pdf"}, {"color": "#66CCFF", "id": "match pedestrian images", "label": "match pedestrian images", "shape": "dot", "size": 25, "title": "match pedestrian images"}, {"color": "#66CCFF", "id": "predefined weights", "label": "predefined weights", "shape": "dot", "size": 25, "title": "predefined weights"}, {"color": "#66CCFF", "id": "not adaptable", "label": "not adaptable", "shape": "dot", "size": 25, "title": "not adaptable"}, {"color": "#66CCFF", "id": "two principled approaches", "label": "two principled approaches", "shape": "dot", "size": 25, "title": "two principled approaches"}, {"color": "#66CCFF", "id": "relative distance", "label": "relative distance", "shape": "dot", "size": 25, "title": "relative distance"}, {"color": "#66CCFF", "id": "average rank-k recognition rate", "label": "average rank-k recognition rate", "shape": "dot", "size": 25, "title": "average rank-k recognition rate"}, {"color": "#66CCFF", "id": "multiple visual features", "label": "multiple visual features", "shape": "dot", "size": 25, "title": "multiple visual features"}, {"color": "#66CCFF", "id": "flexible", "label": "flexible", "shape": "dot", "size": 25, "title": "flexible"}, {"color": "#66CCFF", "id": "RMS methods", "label": "RMS methods", "shape": "dot", "size": 25, "title": "RMS methods"}, {"color": "#66CCFF", "id": "rank-1 recognition rates", "label": "rank-1 recognition rates", "shape": "dot", "size": 25, "title": "rank-1 recognition rates"}, {"color": "#66CCFF", "id": "Ensemble-based approaches", "label": "Ensemble-based approaches", "shape": "dot", "size": 25, "title": "Ensemble-based approaches"}, {"color": "#66CCFF", "id": "linear metrics", "label": "linear metrics", "shape": "dot", "size": 25, "title": "linear metrics"}, {"color": "#66CCFF", "id": "non-linear metrics", "label": "non-linear metrics", "shape": "dot", "size": 25, "title": "non-linear metrics"}, {"color": "#66CCFF", "id": "Similarity metric", "label": "Similarity metric", "shape": "dot", "size": 25, "title": "Similarity metric"}, {"color": "#66CCFF", "id": "Person Re-Identification", "label": "Person Re-Identification", "shape": "dot", "size": 25, "title": "Person Re-Identification"}, {"color": "#66CCFF", "id": "Gong et al. (2014)", "label": "Gong et al. (2014)", "shape": "dot", "size": 25, "title": "Gong et al. (2014)"}, {"color": "#66CCFF", "id": "techniques", "label": "techniques", "shape": "dot", "size": 25, "title": "techniques"}, {"color": "#66CCFF", "id": "Gong, S., Crisitan, M., Yan, S., and Loy, C. C.", "label": "Gong, S., Crisitan, M., Yan, S., and Loy, C. C.", "shape": "dot", "size": 25, "title": "Gong, S., Crisitan, M., Yan, S., and Loy, C. C."}, {"color": "#66CCFF", "id": "support vector method", "label": "support vector method", "shape": "dot", "size": 25, "title": "support vector method"}, {"color": "#66CCFF", "id": "Joachims, T.", "label": "Joachims, T.", "shape": "dot", "size": 25, "title": "Joachims, T."}, {"color": "#66CCFF", "id": "Krizhevsky, A., Sutskever, I., and Hinton, G. E.", "label": "Krizhevsky, A., Sutskever, I., and Hinton, G. E.", "shape": "dot", "size": 25, "title": "Krizhevsky, A., Sutskever, I., and Hinton, G. E."}, {"color": "#66CCFF", "id": "re-identification models", "label": "re-identification models", "shape": "dot", "size": 25, "title": "re-identification models"}, {"color": "#66CCFF", "id": "re-identification systems", "label": "re-identification systems", "shape": "dot", "size": 25, "title": "re-identification systems"}, {"color": "#66CCFF", "id": "Imaginet classification", "label": "Imaginet classification", "shape": "dot", "size": 25, "title": "Imaginet classification"}, {"color": "#66CCFF", "id": "deep convolutional neural networks", "label": "deep convolutional neural networks", "shape": "dot", "size": 25, "title": "deep convolutional neural networks"}, {"color": "#66CCFF", "id": "Mahalanobis distance", "label": "Mahalanobis distance", "shape": "dot", "size": 25, "title": "Mahalanobis distance"}, {"color": "#66CCFF", "id": "metric learning", "label": "metric learning", "shape": "dot", "size": 25, "title": "metric learning"}, {"color": "#66CCFF", "id": "scalability", "label": "scalability", "shape": "dot", "size": 25, "title": "scalability"}, {"color": "#66CCFF", "id": "distance metric learning", "label": "distance metric learning", "shape": "dot", "size": 25, "title": "distance metric learning"}, {"color": "#66CCFF", "id": "computational challenges", "label": "computational challenges", "shape": "dot", "size": 25, "title": "computational challenges"}, {"color": "#66CCFF", "id": "paper (Felzenszwalb et al., 2010)", "label": "paper (Felzenszwalb et al., 2010)", "shape": "dot", "size": 25, "title": "paper (Felzenszwalb et al., 2010)"}, {"color": "#66CCFF", "id": "part-based model", "label": "part-based model", "shape": "dot", "size": 25, "title": "part-based model"}, {"color": "#66CCFF", "id": "re-identification", "label": "re-identification", "shape": "dot", "size": 25, "title": "re-identification"}, {"color": "#66CCFF", "id": "image representation techniques", "label": "image representation techniques", "shape": "dot", "size": 25, "title": "image representation techniques"}, {"color": "#66CCFF", "id": "shape and appearance information", "label": "shape and appearance information", "shape": "dot", "size": 25, "title": "shape and appearance information"}, {"color": "#66CCFF", "id": "robust re-identification", "label": "robust re-identification", "shape": "dot", "size": 25, "title": "robust re-identification"}, {"color": "#66CCFF", "id": "kernel methods", "label": "kernel methods", "shape": "dot", "size": 25, "title": "kernel methods"}, {"color": "#66CCFF", "id": "The University of Adelaide", "label": "The University of Adelaide", "shape": "dot", "size": 25, "title": "The University of Adelaide"}, {"color": "#66CCFF", "id": "Australian Centre for Robotic Vision", "label": "Australian Centre for Robotic Vision", "shape": "dot", "size": 25, "title": "Australian Centre for Robotic Vision"}, {"color": "#66CCFF", "id": "keypoints", "label": "keypoints", "shape": "dot", "size": 25, "title": "keypoints"}, {"color": "#66CCFF", "id": "Adelaide, Australia", "label": "Adelaide, Australia", "shape": "dot", "size": 25, "title": "Adelaide, Australia"}, {"color": "#66CCFF", "id": "The University of Adelaide, Australia", "label": "The University of Adelaide, Australia", "shape": "dot", "size": 25, "title": "The University of Adelaide, Australia"}, {"color": "#66CCFF", "id": "Dengxin Dai", "label": "Dengxin Dai", "shape": "dot", "size": 25, "title": "Dengxin Dai"}, {"color": "#66CCFF", "id": "Metric imitation by manifold transfer", "label": "Metric imitation by manifold transfer", "shape": "dot", "size": 25, "title": "Metric imitation by manifold transfer"}, {"color": "#66CCFF", "id": "Till Kroeger", "label": "Till Kroeger", "shape": "dot", "size": 25, "title": "Till Kroeger"}, {"color": "#66CCFF", "id": "Radu Timofte", "label": "Radu Timofte", "shape": "dot", "size": 25, "title": "Radu Timofte"}, {"color": "#66CCFF", "id": "efficient vision applications", "label": "efficient vision applications", "shape": "dot", "size": 25, "title": "efficient vision applications"}, {"color": "#66CCFF", "id": "Metric Imitation", "label": "Metric Imitation", "shape": "dot", "size": 25, "title": "Metric Imitation"}, {"color": "#66CCFF", "id": "improve performance", "label": "improve performance", "shape": "dot", "size": 25, "title": "improve performance"}, {"color": "#66CCFF", "id": "manifold structure", "label": "manifold structure", "shape": "dot", "size": 25, "title": "manifold structure"}, {"color": "#66CCFF", "id": "vision applications", "label": "vision applications", "shape": "dot", "size": 25, "title": "vision applications"}, {"color": "#66CCFF", "id": "GIST features", "label": "GIST features", "shape": "dot", "size": 25, "title": "GIST features"}, {"color": "#66CCFF", "id": "target features", "label": "target features", "shape": "dot", "size": 25, "title": "target features"}, {"color": "#66CCFF", "id": "SIFT-llc", "label": "SIFT-llc", "shape": "dot", "size": 25, "title": "SIFT-llc"}, {"color": "#66CCFF", "id": "source features", "label": "source features", "shape": "dot", "size": 25, "title": "source features"}, {"color": "#66CCFF", "id": "object-bank", "label": "object-bank", "shape": "dot", "size": 25, "title": "object-bank"}, {"color": "#66CCFF", "id": "CNN features", "label": "CNN features", "shape": "dot", "size": 25, "title": "CNN features"}, {"color": "#66CCFF", "id": "instance-based object retrieval", "label": "instance-based object retrieval", "shape": "dot", "size": 25, "title": "instance-based object retrieval"}, {"color": "#66CCFF", "id": "image clustering", "label": "image clustering", "shape": "dot", "size": 25, "title": "image clustering"}, {"color": "#66CCFF", "id": "category-based image retrieval", "label": "category-based image retrieval", "shape": "dot", "size": 25, "title": "category-based image retrieval"}, {"color": "#66CCFF", "id": "better performance", "label": "better performance", "shape": "dot", "size": 25, "title": "better performance"}, {"color": "#66CCFF", "id": "Metric Imitation (MI)", "label": "Metric Imitation (MI)", "shape": "dot", "size": 25, "title": "Metric Imitation (MI)"}, {"color": "#66CCFF", "id": "original target features", "label": "original target features", "shape": "dot", "size": 25, "title": "original target features"}, {"color": "#66CCFF", "id": "Bosch, A.", "label": "Bosch, A.", "shape": "dot", "size": 25, "title": "Bosch, A."}, {"color": "#66CCFF", "id": "Image classification using random forests and ferns", "label": "Image classification using random forests and ferns", "shape": "dot", "size": 25, "title": "Image classification using random forests and ferns"}, {"color": "#66CCFF", "id": "Chatfield, K.", "label": "Chatfield, K.", "shape": "dot", "size": 25, "title": "Chatfield, K."}, {"color": "#66CCFF", "id": "Dai, D.", "label": "Dai, D.", "shape": "dot", "size": 25, "title": "Dai, D."}, {"color": "#66CCFF", "id": "Ensemble partitioning", "label": "Ensemble partitioning", "shape": "dot", "size": 25, "title": "Ensemble partitioning"}, {"color": "#66CCFF", "id": "llc", "label": "llc", "shape": "dot", "size": 25, "title": "llc"}, {"color": "#66CCFF", "id": "Object Retrieval", "label": "Object Retrieval", "shape": "dot", "size": 25, "title": "Object Retrieval"}, {"color": "#66CCFF", "id": "Object-bank (OB)", "label": "Object-bank (OB)", "shape": "dot", "size": 25, "title": "Object-bank (OB)"}, {"color": "#66CCFF", "id": "Image Clustering", "label": "Image Clustering", "shape": "dot", "size": 25, "title": "Image Clustering"}, {"color": "#66CCFF", "id": "Dana et al.", "label": "Dana et al.", "shape": "dot", "size": 25, "title": "Dana et al."}, {"color": "#66CCFF", "id": "Reflectance and texture of real-world surfaces", "label": "Reflectance and texture of real-world surfaces", "shape": "dot", "size": 25, "title": "Reflectance and texture of real-world surfaces"}, {"color": "#66CCFF", "id": "*ACM Trans. Graph.*", "label": "*ACM Trans. Graph.*", "shape": "dot", "size": 25, "title": "*ACM Trans. Graph.*"}, {"color": "#66CCFF", "id": "Fei-Fei et al.", "label": "Fei-Fei et al.", "shape": "dot", "size": 25, "title": "Fei-Fei et al."}, {"color": "#66CCFF", "id": "Learning generative visual models", "label": "Learning generative visual models", "shape": "dot", "size": 25, "title": "Learning generative visual models"}, {"color": "#66CCFF", "id": "Workshop on Generative-Model Based Vision", "label": "Workshop on Generative-Model Based Vision", "shape": "dot", "size": 25, "title": "Workshop on Generative-Model Based Vision"}, {"color": "#66CCFF", "id": "Lazebnik et al.", "label": "Lazebnik et al.", "shape": "dot", "size": 25, "title": "Lazebnik et al."}, {"color": "#66CCFF", "id": "Spatial pyramid matching", "label": "Spatial pyramid matching", "shape": "dot", "size": 25, "title": "Spatial pyramid matching"}, {"color": "#66CCFF", "id": "recognizing natural scene categories", "label": "recognizing natural scene categories", "shape": "dot", "size": 25, "title": "recognizing natural scene categories"}, {"color": "#66CCFF", "id": "Li \u0026 Fei-Fei", "label": "Li \u0026 Fei-Fei", "shape": "dot", "size": 25, "title": "Li \u0026 Fei-Fei"}, {"color": "#66CCFF", "id": "What, where and who?", "label": "What, where and who?", "shape": "dot", "size": 25, "title": "What, where and who?"}, {"color": "#66CCFF", "id": "Li et al. (2010)", "label": "Li et al. (2010)", "shape": "dot", "size": 25, "title": "Li et al. (2010)"}, {"color": "#66CCFF", "id": "Object bank", "label": "Object bank", "shape": "dot", "size": 25, "title": "Object bank"}, {"color": "#66CCFF", "id": "scene classification", "label": "scene classification", "shape": "dot", "size": 25, "title": "scene classification"}, {"color": "#66CCFF", "id": "Li, L.-J. et al. (2010)", "label": "Li, L.-J. et al. (2010)", "shape": "dot", "size": 25, "title": "Li, L.-J. et al. (2010)"}, {"color": "#66CCFF", "id": "*NIPS*", "label": "*NIPS*", "shape": "dot", "size": 25, "title": "*NIPS*"}, {"color": "#66CCFF", "id": "Nist\u00b4er, D. \u0026 Stew\u00b4enius, H. (2006)", "label": "Nist\u00b4er, D. \u0026 Stew\u00b4enius, H. (2006)", "shape": "dot", "size": 25, "title": "Nist\u00b4er, D. \u0026 Stew\u00b4enius, H. (2006)"}, {"color": "#66CCFF", "id": "*CVPR*", "label": "*CVPR*", "shape": "dot", "size": 25, "title": "*CVPR*"}, {"color": "#66CCFF", "id": "Vocabulary tree", "label": "Vocabulary tree", "shape": "dot", "size": 25, "title": "Vocabulary tree"}, {"color": "#66CCFF", "id": "scalable recognition", "label": "scalable recognition", "shape": "dot", "size": 25, "title": "scalable recognition"}, {"color": "#66CCFF", "id": "Oliva, A. \u0026 Torralba, A. (2001)", "label": "Oliva, A. \u0026 Torralba, A. (2001)", "shape": "dot", "size": 25, "title": "Oliva, A. \u0026 Torralba, A. (2001)"}, {"color": "#66CCFF", "id": "*IJCV*", "label": "*IJCV*", "shape": "dot", "size": 25, "title": "*IJCV*"}, {"color": "#66CCFF", "id": "Spatial envelope", "label": "Spatial envelope", "shape": "dot", "size": 25, "title": "Spatial envelope"}, {"color": "#66CCFF", "id": "shape of the scene", "label": "shape of the scene", "shape": "dot", "size": 25, "title": "shape of the scene"}, {"color": "#66CCFF", "id": "Tuytelaars, T. et al. (2009)", "label": "Tuytelaars, T. et al. (2009)", "shape": "dot", "size": 25, "title": "Tuytelaars, T. et al. (2009)"}, {"color": "#66CCFF", "id": "*IJCLP*", "label": "*IJCLP*", "shape": "dot", "size": 25, "title": "*IJCLP*"}, {"color": "#66CCFF", "id": "Object discovery", "label": "Object discovery", "shape": "dot", "size": 25, "title": "Object discovery"}, {"color": "#66CCFF", "id": "unsupervised", "label": "unsupervised", "shape": "dot", "size": 25, "title": "unsupervised"}, {"color": "#66CCFF", "id": "Wang, J. et al. (2010)", "label": "Wang, J. et al. (2010)", "shape": "dot", "size": 25, "title": "Wang, J. et al. (2010)"}, {"color": "#66CCFF", "id": "Locality-constrained linear coding", "label": "Locality-constrained linear coding", "shape": "dot", "size": 25, "title": "Locality-constrained linear coding"}, {"color": "#66CCFF", "id": "Computer Vision Lab, ETH Zurich", "label": "Computer Vision Lab, ETH Zurich", "shape": "dot", "size": 25, "title": "Computer Vision Lab, ETH Zurich"}, {"color": "#66CCFF", "id": "ETH Zurich", "label": "ETH Zurich", "shape": "dot", "size": 25, "title": "ETH Zurich"}, {"color": "#66CCFF", "id": "VISICS, ESAT/PSI, KU Leuven", "label": "VISICS, ESAT/PSI, KU Leuven", "shape": "dot", "size": 25, "title": "VISICS, ESAT/PSI, KU Leuven"}, {"color": "#66CCFF", "id": "Francesco Pittaluca", "label": "Francesco Pittaluca", "shape": "dot", "size": 25, "title": "Francesco Pittaluca"}, {"color": "#66CCFF", "id": "Sanjeev J. Koppal", "label": "Sanjeev J. Koppal", "shape": "dot", "size": 25, "title": "Sanjeev J. Koppal"}, {"color": "#66CCFF", "id": "Pittaluga_Privacy_Preserving_Optics_2015_CVPR_supplemental.pdf", "label": "Pittaluga_Privacy_Preserving_Optics_2015_CVPR_supplemental.pdf", "shape": "dot", "size": 25, "title": "Pittaluga_Privacy_Preserving_Optics_2015_CVPR_supplemental.pdf"}, {"color": "#66CCFF", "id": "Privacy Preserving Optics for Miniature Vision Sensors", "label": "Privacy Preserving Optics for Miniature Vision Sensors", "shape": "dot", "size": 25, "title": "Privacy Preserving Optics for Miniature Vision Sensors"}, {"color": "#66CCFF", "id": "Privacy Preseving Optics for Miniature Vision Sensors", "label": "Privacy Preseving Optics for Miniature Vision Sensors", "shape": "dot", "size": 25, "title": "Privacy Preseving Optics for Miniature Vision Sensors"}, {"color": "#66CCFF", "id": "supplementary material", "label": "supplementary material", "shape": "dot", "size": 25, "title": "supplementary material"}, {"color": "#66CCFF", "id": "privacy-preserving optics", "label": "privacy-preserving optics", "shape": "dot", "size": 25, "title": "privacy-preserving optics"}, {"color": "#66CCFF", "id": "minature vision sensors", "label": "minature vision sensors", "shape": "dot", "size": 25, "title": "minature vision sensors"}, {"color": "#66CCFF", "id": "impact of defocusing optics", "label": "impact of defocusing optics", "shape": "dot", "size": 25, "title": "impact of defocusing optics"}, {"color": "#66CCFF", "id": "performance of face recognition algorithms", "label": "performance of face recognition algorithms", "shape": "dot", "size": 25, "title": "performance of face recognition algorithms"}, {"color": "#66CCFF", "id": "angular support", "label": "angular support", "shape": "dot", "size": 25, "title": "angular support"}, {"color": "#66CCFF", "id": "FLIR One thermal sensor", "label": "FLIR One thermal sensor", "shape": "dot", "size": 25, "title": "FLIR One thermal sensor"}, {"color": "#66CCFF", "id": "Kinect time-of-flight sensor", "label": "Kinect time-of-flight sensor", "shape": "dot", "size": 25, "title": "Kinect time-of-flight sensor"}, {"color": "#66CCFF", "id": "effect of blur", "label": "effect of blur", "shape": "dot", "size": 25, "title": "effect of blur"}, {"color": "#66CCFF", "id": "face recognition rates", "label": "face recognition rates", "shape": "dot", "size": 25, "title": "face recognition rates"}, {"color": "#66CCFF", "id": "geometric derivations", "label": "geometric derivations", "shape": "dot", "size": 25, "title": "geometric derivations"}, {"color": "#66CCFF", "id": "determining angular support", "label": "determining angular support", "shape": "dot", "size": 25, "title": "determining angular support"}, {"color": "#66CCFF", "id": "Privacy-preserving optics", "label": "Privacy-preserving optics", "shape": "dot", "size": 25, "title": "Privacy-preserving optics"}, {"color": "#66CCFF", "id": "Facial images", "label": "Facial images", "shape": "dot", "size": 25, "title": "Facial images"}, {"color": "#66CCFF", "id": "Feret methodology", "label": "Feret methodology", "shape": "dot", "size": 25, "title": "Feret methodology"}, {"color": "#66CCFF", "id": "CSU face identification evaluation system", "label": "CSU face identification evaluation system", "shape": "dot", "size": 25, "title": "CSU face identification evaluation system"}, {"color": "#66CCFF", "id": "Face identification", "label": "Face identification", "shape": "dot", "size": 25, "title": "Face identification"}, {"color": "#66CCFF", "id": "Angular support derivation", "label": "Angular support derivation", "shape": "dot", "size": 25, "title": "Angular support derivation"}, {"color": "#66CCFF", "id": "Bolme, D. S. et al.", "label": "Bolme, D. S. et al.", "shape": "dot", "size": 25, "title": "Bolme, D. S. et al."}, {"color": "#66CCFF", "id": "Newton, E. et al.", "label": "Newton, E. et al.", "shape": "dot", "size": 25, "title": "Newton, E. et al."}, {"color": "#66CCFF", "id": "Privacy-preserving techniques", "label": "Privacy-preserving techniques", "shape": "dot", "size": 25, "title": "Privacy-preserving techniques"}, {"color": "#66CCFF", "id": "Phillips, P. J. et al.", "label": "Phillips, P. J. et al.", "shape": "dot", "size": 25, "title": "Phillips, P. J. et al."}, {"color": "#66CCFF", "id": "Feret evaluation methodology", "label": "Feret evaluation methodology", "shape": "dot", "size": 25, "title": "Feret evaluation methodology"}, {"color": "#66CCFF", "id": "Face recognition evaluation", "label": "Face recognition evaluation", "shape": "dot", "size": 25, "title": "Face recognition evaluation"}, {"color": "#66CCFF", "id": "Sensor positioning", "label": "Sensor positioning", "shape": "dot", "size": 25, "title": "Sensor positioning"}, {"color": "#66CCFF", "id": "face recognition evaluation", "label": "face recognition evaluation", "shape": "dot", "size": 25, "title": "face recognition evaluation"}, {"color": "#66CCFF", "id": "Park, Min-Gyu", "label": "Park, Min-Gyu", "shape": "dot", "size": 25, "title": "Park, Min-Gyu"}, {"color": "#66CCFF", "id": "Leveraging Stereo Matching with Learning-based Con\ufb01dence Measures", "label": "Leveraging Stereo Matching with Learning-based Con\ufb01dence Measures", "shape": "dot", "size": 25, "title": "Leveraging Stereo Matching with Learning-based Con\ufb01dence Measures"}, {"color": "#66CCFF", "id": "Yoon, Kuk-Jin", "label": "Yoon, Kuk-Jin", "shape": "dot", "size": 25, "title": "Yoon, Kuk-Jin"}, {"color": "#66CCFF", "id": "Leverging Stereo Matching with Learning-based Con\ufb01dence Measures", "label": "Leverging Stereo Matching with Learning-based Con\ufb01dence Measures", "shape": "dot", "size": 25, "title": "Leverging Stereo Matching with Learning-based Con\ufb01dence Measures"}, {"color": "#66CCFF", "id": "Pittaluga, Francesco", "label": "Pittaluga, Francesco", "shape": "dot", "size": 25, "title": "Pittaluga, Francesco"}, {"color": "#66CCFF", "id": "P. J. The furet evaluation methodology", "label": "P. J. The furet evaluation methodology", "shape": "dot", "size": 25, "title": "P. J. The furet evaluation methodology"}, {"color": "#66CCFF", "id": "Random Forests", "label": "Random Forests", "shape": "dot", "size": 25, "title": "Random Forests"}, {"color": "#66CCFF", "id": "Breiman, L.", "label": "Breiman, L.", "shape": "dot", "size": 25, "title": "Breiman, L."}, {"color": "#66CCFF", "id": "Random forests", "label": "Random forests", "shape": "dot", "size": 25, "title": "Random forests"}, {"color": "#66CCFF", "id": "University of Florida", "label": "University of Florida", "shape": "dot", "size": 25, "title": "University of Florida"}, {"color": "#66CCFF", "id": "Koppal, Sanjeev J.", "label": "Koppal, Sanjeev J.", "shape": "dot", "size": 25, "title": "Koppal, Sanjeev J."}, {"color": "#66CCFF", "id": "Breiman", "label": "Breiman", "shape": "dot", "size": 25, "title": "Breiman"}, {"color": "#66CCFF", "id": "stereo confidence metric", "label": "stereo confidence metric", "shape": "dot", "size": 25, "title": "stereo confidence metric"}, {"color": "#66CCFF", "id": "key aspect of stereo vision", "label": "key aspect of stereo vision", "shape": "dot", "size": 25, "title": "key aspect of stereo vision"}, {"color": "#66CCFF", "id": "Egnal", "label": "Egnal", "shape": "dot", "size": 25, "title": "Egnal"}, {"color": "#66CCFF", "id": "Hirschm\u00fcller", "label": "Hirschm\u00fcller", "shape": "dot", "size": 25, "title": "Hirschm\u00fcller"}, {"color": "#66CCFF", "id": "semiglobal matching", "label": "semiglobal matching", "shape": "dot", "size": 25, "title": "semiglobal matching"}, {"color": "#66CCFF", "id": "mutual information", "label": "mutual information", "shape": "dot", "size": 25, "title": "mutual information"}, {"color": "#66CCFF", "id": "stereo vision", "label": "stereo vision", "shape": "dot", "size": 25, "title": "stereo vision"}, {"color": "#66CCFF", "id": "stereo processing", "label": "stereo processing", "shape": "dot", "size": 25, "title": "stereo processing"}, {"color": "#66CCFF", "id": "stereo matching", "label": "stereo matching", "shape": "dot", "size": 25, "title": "stereo matching"}, {"color": "#66CCFF", "id": "ground control points", "label": "ground control points", "shape": "dot", "size": 25, "title": "ground control points"}, {"color": "#66CCFF", "id": "benchmark", "label": "benchmark", "shape": "dot", "size": 25, "title": "benchmark"}, {"color": "#66CCFF", "id": "vision", "label": "vision", "shape": "dot", "size": 25, "title": "vision"}, {"color": "#66CCFF", "id": "robotics", "label": "robotics", "shape": "dot", "size": 25, "title": "robotics"}, {"color": "#66CCFF", "id": "vision and robotics", "label": "vision and robotics", "shape": "dot", "size": 25, "title": "vision and robotics"}, {"color": "#66CCFF", "id": "Hu, X.", "label": "Hu, X.", "shape": "dot", "size": 25, "title": "Hu, X."}, {"color": "#66CCFF", "id": "quantitative evaluation", "label": "quantitative evaluation", "shape": "dot", "size": 25, "title": "quantitative evaluation"}, {"color": "#66CCFF", "id": "confidence measures", "label": "confidence measures", "shape": "dot", "size": 25, "title": "confidence measures"}, {"color": "#66CCFF", "id": "dense matching", "label": "dense matching", "shape": "dot", "size": 25, "title": "dense matching"}, {"color": "#66CCFF", "id": "complex scenes", "label": "complex scenes", "shape": "dot", "size": 25, "title": "complex scenes"}, {"color": "#66CCFF", "id": "Manduchi, R.", "label": "Manduchi, R.", "shape": "dot", "size": 25, "title": "Manduchi, R."}, {"color": "#66CCFF", "id": "distinctiveness maps", "label": "distinctiveness maps", "shape": "dot", "size": 25, "title": "distinctiveness maps"}, {"color": "#66CCFF", "id": "image matching", "label": "image matching", "shape": "dot", "size": 25, "title": "image matching"}, {"color": "#66CCFF", "id": "pose estimation", "label": "pose estimation", "shape": "dot", "size": 25, "title": "pose estimation"}, {"color": "#66CCFF", "id": "determining viewpoint", "label": "determining viewpoint", "shape": "dot", "size": 25, "title": "determining viewpoint"}, {"color": "#66CCFF", "id": "viewpoint", "label": "viewpoint", "shape": "dot", "size": 25, "title": "viewpoint"}, {"color": "#66CCFF", "id": "coarse pose", "label": "coarse pose", "shape": "dot", "size": 25, "title": "coarse pose"}, {"color": "#66CCFF", "id": "keypoint prediction", "label": "keypoint prediction", "shape": "dot", "size": 25, "title": "keypoint prediction"}, {"color": "#66CCFF", "id": "finer details", "label": "finer details", "shape": "dot", "size": 25, "title": "finer details"}, {"color": "#66CCFF", "id": "constrained setting", "label": "constrained setting", "shape": "dot", "size": 25, "title": "constrained setting"}, {"color": "#66CCFF", "id": "bounding boxes", "label": "bounding boxes", "shape": "dot", "size": 25, "title": "bounding boxes"}, {"color": "#66CCFF", "id": "detection setting", "label": "detection setting", "shape": "dot", "size": 25, "title": "detection setting"}, {"color": "#66CCFF", "id": "viewpoint estimates", "label": "viewpoint estimates", "shape": "dot", "size": 25, "title": "viewpoint estimates"}, {"color": "#66CCFF", "id": "keypoint predictions", "label": "keypoint predictions", "shape": "dot", "size": 25, "title": "keypoint predictions"}, {"color": "#66CCFF", "id": "object characteristics", "label": "object characteristics", "shape": "dot", "size": 25, "title": "object characteristics"}, {"color": "#66CCFF", "id": "Tulsiani_Viewpoints_and_Keypoints_2015_CVPR_paper", "label": "Tulsiani_Viewpoints_and_Keypoints_2015_CVPR_paper", "shape": "dot", "size": 25, "title": "Tulsiani_Viewpoints_and_Keypoints_2015_CVPR_paper"}, {"color": "#66CCFF", "id": "Shubham Tulsiani", "label": "Shubham Tulsiani", "shape": "dot", "size": 25, "title": "Shubham Tulsiani"}, {"color": "#66CCFF", "id": "Jitendra Malik", "label": "Jitendra Malik", "shape": "dot", "size": 25, "title": "Jitendra Malik"}, {"color": "#66CCFF", "id": "effort", "label": "effort", "shape": "dot", "size": 25, "title": "effort"}, {"color": "#66CCFF", "id": "future efforts", "label": "future efforts", "shape": "dot", "size": 25, "title": "future efforts"}, {"color": "#66CCFF", "id": "analysis", "label": "analysis", "shape": "dot", "size": 25, "title": "analysis"}, {"color": "#66CCFF", "id": "error modes", "label": "error modes", "shape": "dot", "size": 25, "title": "error modes"}, {"color": "#66CCFF", "id": "effect", "label": "effect", "shape": "dot", "size": 25, "title": "effect"}, {"color": "#66CCFF", "id": "Pose Estimation", "label": "Pose Estimation", "shape": "dot", "size": 25, "title": "Pose Estimation"}, {"color": "#66CCFF", "id": "University of California, Berkeley", "label": "University of California, Berkeley", "shape": "dot", "size": 25, "title": "University of California, Berkeley"}, {"color": "#66CCFF", "id": "Song_Joint_Multi-Feature_Spatial_2015_CVPR_paper", "label": "Song_Joint_Multi-Feature_Spatial_2015_CVPR_paper", "shape": "dot", "size": 25, "title": "Song_Joint_Multi-Feature_Spatial_2015_CVPR_paper"}, {"color": "#66CCFF", "id": "Abed Malti", "label": "Abed Malti", "shape": "dot", "size": 25, "title": "Abed Malti"}, {"color": "#66CCFF", "id": "Malti_A_Linear_Least-Squares_2015_CVPR_paper", "label": "Malti_A_Linear_Least-Squares_2015_CVPR_paper", "shape": "dot", "size": 25, "title": "Malti_A_Linear_Least-Squares_2015_CVPR_paper"}, {"color": "#66CCFF", "id": "Adrien Bartoli", "label": "Adrien Bartoli", "shape": "dot", "size": 25, "title": "Adrien Bartoli"}, {"color": "#66CCFF", "id": "Maiti_A_Linear_Least-Squares_2015_CVPR_paper", "label": "Maiti_A_Linear_Least-Squares_2015_CVPR_paper", "shape": "dot", "size": 25, "title": "Maiti_A_Linear_Least-Squares_2015_CVPR_paper"}, {"color": "#66CCFF", "id": "Richard Hartley", "label": "Richard Hartley", "shape": "dot", "size": 25, "title": "Richard Hartley"}, {"color": "#66CCFF", "id": "Keypoint Prediction", "label": "Keypoint Prediction", "shape": "dot", "size": 25, "title": "Keypoint Prediction"}, {"color": "#66CCFF", "id": "Viewpoint Prediction", "label": "Viewpoint Prediction", "shape": "dot", "size": 25, "title": "Viewpoint Prediction"}, {"color": "#66CCFF", "id": "Shape-from-Template methods", "label": "Shape-from-Template methods", "shape": "dot", "size": 25, "title": "Shape-from-Template methods"}, {"color": "#66CCFF", "id": "balancing accuracy, speed, and robustness", "label": "balancing accuracy, speed, and robustness", "shape": "dot", "size": 25, "title": "balancing accuracy, speed, and robustness"}, {"color": "#66CCFF", "id": "non-linear optimization", "label": "non-linear optimization", "shape": "dot", "size": 25, "title": "non-linear optimization"}, {"color": "#66CCFF", "id": "existing approach", "label": "existing approach", "shape": "dot", "size": 25, "title": "existing approach"}, {"color": "#66CCFF", "id": "Kalman filtering", "label": "Kalman filtering", "shape": "dot", "size": 25, "title": "Kalman filtering"}, {"color": "#66CCFF", "id": "computational cost", "label": "computational cost", "shape": "dot", "size": 25, "title": "computational cost"}, {"color": "#66CCFF", "id": "initialization", "label": "initialization", "shape": "dot", "size": 25, "title": "initialization"}, {"color": "#66CCFF", "id": "error accumulation", "label": "error accumulation", "shape": "dot", "size": 25, "title": "error accumulation"}, {"color": "#66CCFF", "id": "proposed solution", "label": "proposed solution", "shape": "dot", "size": 25, "title": "proposed solution"}, {"color": "#66CCFF", "id": "mechanical constraints", "label": "mechanical constraints", "shape": "dot", "size": 25, "title": "mechanical constraints"}, {"color": "#66CCFF", "id": "finite element methods", "label": "finite element methods", "shape": "dot", "size": 25, "title": "finite element methods"}, {"color": "#66CCFF", "id": "surface", "label": "surface", "shape": "dot", "size": 25, "title": "surface"}, {"color": "#66CCFF", "id": "deformation", "label": "deformation", "shape": "dot", "size": 25, "title": "deformation"}, {"color": "#66CCFF", "id": "accurate reconstruction", "label": "accurate reconstruction", "shape": "dot", "size": 25, "title": "accurate reconstruction"}, {"color": "#66CCFF", "id": "efficient reconstruction", "label": "efficient reconstruction", "shape": "dot", "size": 25, "title": "efficient reconstruction"}, {"color": "#66CCFF", "id": "linear least-squares SfT method", "label": "linear least-squares SfT method", "shape": "dot", "size": 25, "title": "linear least-squares SfT method"}, {"color": "#66CCFF", "id": "Shape-from-Template (SfT)", "label": "Shape-from-Template (SfT)", "shape": "dot", "size": 25, "title": "Shape-from-Template (SfT)"}, {"color": "#66CCFF", "id": "elastic deformations", "label": "elastic deformations", "shape": "dot", "size": 25, "title": "elastic deformations"}, {"color": "#66CCFF", "id": "linear least-squares estimation", "label": "linear least-squares estimation", "shape": "dot", "size": 25, "title": "linear least-squares estimation"}, {"color": "#66CCFF", "id": "Finite Element Methods (FEM)", "label": "Finite Element Methods (FEM)", "shape": "dot", "size": 25, "title": "Finite Element Methods (FEM)"}, {"color": "#66CCFF", "id": "non-rigid EKF monocular SLAM", "label": "non-rigid EKF monocular SLAM", "shape": "dot", "size": 25, "title": "non-rigid EKF monocular SLAM"}, {"color": "#66CCFF", "id": "sequential bayesian non-rigid structure from motion", "label": "sequential bayesian non-rigid structure from motion", "shape": "dot", "size": 25, "title": "sequential bayesian non-rigid structure from motion"}, {"color": "#66CCFF", "id": "Agudo, A.", "label": "Agudo, A.", "shape": "dot", "size": 25, "title": "Agudo, A."}, {"color": "#66CCFF", "id": "FEM models to code non-rigid EKF monocular SLAM", "label": "FEM models to code non-rigid EKF monocular SLAM", "shape": "dot", "size": 25, "title": "FEM models to code non-rigid EKF monocular SLAM"}, {"color": "#66CCFF", "id": "Finite element based sequential bayesian non-rigid structure from motion", "label": "Finite element based sequential bayesian non-rigid structure from motion", "shape": "dot", "size": 25, "title": "Finite element based sequential bayesian non-rigid structure from motion"}, {"color": "#66CCFF", "id": "FEM models", "label": "FEM models", "shape": "dot", "size": 25, "title": "FEM models"}, {"color": "#66CCFF", "id": "frequently", "label": "frequently", "shape": "dot", "size": 25, "title": "frequently"}, {"color": "#66CCFF", "id": "fully linear least-squares SfT method", "label": "fully linear least-squares SfT method", "shape": "dot", "size": 25, "title": "fully linear least-squares SfT method"}, {"color": "#66CCFF", "id": "Agudo et al.", "label": "Agudo et al.", "shape": "dot", "size": 25, "title": "Agudo et al."}, {"color": "#66CCFF", "id": "related work", "label": "related work", "shape": "dot", "size": 25, "title": "related work"}, {"color": "#66CCFF", "id": "Bartoli et al.", "label": "Bartoli et al.", "shape": "dot", "size": 25, "title": "Bartoli et al."}, {"color": "#66CCFF", "id": "methodology", "label": "methodology", "shape": "dot", "size": 25, "title": "methodology"}, {"color": "#66CCFF", "id": "Salzmann and Urtasun", "label": "Salzmann and Urtasun", "shape": "dot", "size": 25, "title": "Salzmann and Urtasun"}, {"color": "#66CCFF", "id": "Moreno-Noguer and Porta", "label": "Moreno-Noguer and Porta", "shape": "dot", "size": 25, "title": "Moreno-Noguer and Porta"}, {"color": "#66CCFF", "id": "Finite Element Methods", "label": "Finite Element Methods", "shape": "dot", "size": 25, "title": "Finite Element Methods"}, {"color": "#66CCFF", "id": "Chaskalovic", "label": "Chaskalovic", "shape": "dot", "size": 25, "title": "Chaskalovic"}, {"color": "#66CCFF", "id": "structure from motion", "label": "structure from motion", "shape": "dot", "size": 25, "title": "structure from motion"}, {"color": "#66CCFF", "id": "Salzmann and Urutasun", "label": "Salzmann and Urutasun", "shape": "dot", "size": 25, "title": "Salzmann and Urutasun"}, {"color": "#66CCFF", "id": "3D reconstruction", "label": "3D reconstruction", "shape": "dot", "size": 25, "title": "3D reconstruction"}, {"color": "#66CCFF", "id": "shape recovery", "label": "shape recovery", "shape": "dot", "size": 25, "title": "shape recovery"}, {"color": "#66CCFF", "id": "ape recovery", "label": "ape recovery", "shape": "dot", "size": 25, "title": "ape recovery"}, {"color": "#66CCFF", "id": "Chaskaloric", "label": "Chaskaloric", "shape": "dot", "size": 25, "title": "Chaskaloric"}, {"color": "#66CCFF", "id": "Finite Elements Methods for Engineering Sciences", "label": "Finite Elements Methods for Engineering Sciences", "shape": "dot", "size": 25, "title": "Finite Elements Methods for Engineering Sciences"}, {"color": "#66CCFF", "id": "background knowledge", "label": "background knowledge", "shape": "dot", "size": 25, "title": "background knowledge"}, {"color": "#66CCFF", "id": "Reconstructing sharply folding surfaces", "label": "Reconstructing sharply folding surfaces", "shape": "dot", "size": 25, "title": "Reconstructing sharply folding surfaces"}, {"color": "#66CCFF", "id": "feature extraction", "label": "feature extraction", "shape": "dot", "size": 25, "title": "feature extraction"}, {"color": "#66CCFF", "id": "Salzmann, M., and Fua, P.", "label": "Salzmann, M., and Fua, P.", "shape": "dot", "size": 25, "title": "Salzmann, M., and Fua, P."}, {"color": "#66CCFF", "id": "Linear local models for monocular reconstruction", "label": "Linear local models for monocular reconstruction", "shape": "dot", "size": 25, "title": "Linear local models for monocular reconstruction"}, {"color": "#66CCFF", "id": "Matthews, I., and Baker, S.", "label": "Matthews, I., and Baker, S.", "shape": "dot", "size": 25, "title": "Matthews, I., and Baker, S."}, {"color": "#66CCFF", "id": "Active appearance models revisited", "label": "Active appearance models revisited", "shape": "dot", "size": 25, "title": "Active appearance models revisited"}, {"color": "#66CCFF", "id": "modeling techniques", "label": "modeling techniques", "shape": "dot", "size": 25, "title": "modeling techniques"}, {"color": "#66CCFF", "id": "Fluminance/INRIA", "label": "Fluminance/INRIA", "shape": "dot", "size": 25, "title": "Fluminance/INRIA"}, {"color": "#66CCFF", "id": "appearance models", "label": "appearance models", "shape": "dot", "size": 25, "title": "appearance models"}, {"color": "#66CCFF", "id": "Fuminance/INRIA", "label": "Fuminance/INRIA", "shape": "dot", "size": 25, "title": "Fuminance/INRIA"}, {"color": "#66CCFF", "id": "ALCoV/ISIT", "label": "ALCoV/ISIT", "shape": "dot", "size": 25, "title": "ALCoV/ISIT"}, {"color": "#66CCFF", "id": "NICTA", "label": "NICTA", "shape": "dot", "size": 25, "title": "NICTA"}, {"color": "#66CCFF", "id": "Riemannian Coding", "label": "Riemannian Coding", "shape": "dot", "size": 25, "title": "Riemannian Coding"}, {"color": "#66CCFF", "id": "Kernels", "label": "Kernels", "shape": "dot", "size": 25, "title": "Kernels"}, {"color": "#66CCFF", "id": "Mehrtash Harandi", "label": "Mehrtash Harandi", "shape": "dot", "size": 25, "title": "Mehrtash Harandi"}, {"color": "#66CCFF", "id": "Riemannian Coding and Dictionary Learning", "label": "Riemannian Coding and Dictionary Learning", "shape": "dot", "size": 25, "title": "Riemannian Coding and Dictionary Learning"}, {"color": "#66CCFF", "id": "coding and dictionary learning", "label": "coding and dictionary learning", "shape": "dot", "size": 25, "title": "coding and dictionary learning"}, {"color": "#66CCFF", "id": "covariance descriptors", "label": "covariance descriptors", "shape": "dot", "size": 25, "title": "covariance descriptors"}, {"color": "#66CCFF", "id": "Riemannian manifolds", "label": "Riemannian manifolds", "shape": "dot", "size": 25, "title": "Riemannian manifolds"}, {"color": "#66CCFF", "id": "normalized histograms", "label": "normalized histograms", "shape": "dot", "size": 25, "title": "normalized histograms"}, {"color": "#66CCFF", "id": "linear subspaces", "label": "linear subspaces", "shape": "dot", "size": 25, "title": "linear subspaces"}, {"color": "#66CCFF", "id": "Riemannianmanifolds", "label": "Riemannianmanifolds", "shape": "dot", "size": 25, "title": "Riemannianmanifolds"}, {"color": "#66CCFF", "id": "2D shape outlines", "label": "2D shape outlines", "shape": "dot", "size": 25, "title": "2D shape outlines"}, {"color": "#66CCFF", "id": "existing solutions", "label": "existing solutions", "shape": "dot", "size": 25, "title": "existing solutions"}, {"color": "#66CCFF", "id": "dedicated to specific manifolds", "label": "dedicated to specific manifolds", "shape": "dot", "size": 25, "title": "dedicated to specific manifolds"}, {"color": "#66CCFF", "id": "optimization problems", "label": "optimization problems", "shape": "dot", "size": 25, "title": "optimization problems"}, {"color": "#66CCFF", "id": "difficult to solve", "label": "difficult to solve", "shape": "dot", "size": 25, "title": "difficult to solve"}, {"color": "#66CCFF", "id": "kernels", "label": "kernels", "shape": "dot", "size": 25, "title": "kernels"}, {"color": "#66CCFF", "id": "general Riemannian coding framework", "label": "general Riemannian coding framework", "shape": "dot", "size": 25, "title": "general Riemannian coding framework"}, {"color": "#66CCFF", "id": "kernel-based counterpart", "label": "kernel-based counterpart", "shape": "dot", "size": 25, "title": "kernel-based counterpart"}, {"color": "#66CCFF", "id": "generalization beyond sparse coding", "label": "generalization beyond sparse coding", "shape": "dot", "size": 25, "title": "generalization beyond sparse coding"}, {"color": "#66CCFF", "id": "Riemannian coding framework", "label": "Riemannian coding framework", "shape": "dot", "size": 25, "title": "Riemannian coding framework"}, {"color": "#66CCFF", "id": "learning of kernel parameters", "label": "learning of kernel parameters", "shape": "dot", "size": 25, "title": "learning of kernel parameters"}, {"color": "#66CCFF", "id": "dictionary learning", "label": "dictionary learning", "shape": "dot", "size": 25, "title": "dictionary learning"}, {"color": "#66CCFF", "id": "non-flat manifolds", "label": "non-flat manifolds", "shape": "dot", "size": 25, "title": "non-flat manifolds"}, {"color": "#66CCFF", "id": "Euclidean spaces", "label": "Euclidean spaces", "shape": "dot", "size": 25, "title": "Euclidean spaces"}, {"color": "#66CCFF", "id": "kernel parameters", "label": "kernel parameters", "shape": "dot", "size": 25, "title": "kernel parameters"}, {"color": "#66CCFF", "id": "sparse coding", "label": "sparse coding", "shape": "dot", "size": 25, "title": "sparse coding"}, {"color": "#66CCFF", "id": "flat data", "label": "flat data", "shape": "dot", "size": 25, "title": "flat data"}, {"color": "#66CCFF", "id": "coding schemes", "label": "coding schemes", "shape": "dot", "size": 25, "title": "coding schemes"}, {"color": "#66CCFF", "id": "efficient solutions", "label": "efficient solutions", "shape": "dot", "size": 25, "title": "efficient solutions"}, {"color": "#66CCFF", "id": "Riemannian Manifolds", "label": "Riemannian Manifolds", "shape": "dot", "size": 25, "title": "Riemannian Manifolds"}, {"color": "#66CCFF", "id": "Dictionary Learning", "label": "Dictionary Learning", "shape": "dot", "size": 25, "title": "Dictionary Learning"}, {"color": "#66CCFF", "id": "Coding Theory", "label": "Coding Theory", "shape": "dot", "size": 25, "title": "Coding Theory"}, {"color": "#66CCFF", "id": "Mehrtas Harandi", "label": "Mehrtas Harandi", "shape": "dot", "size": 25, "title": "Mehrtas Harandi"}, {"color": "#66CCFF", "id": "NICITA", "label": "NICITA", "shape": "dot", "size": 25, "title": "NICITA"}, {"color": "#66CCFF", "id": "Yuting Zhang", "label": "Yuting Zhang", "shape": "dot", "size": 25, "title": "Yuting Zhang"}, {"color": "#66CCFF", "id": "Improving Object Detection", "label": "Improving Object Detection", "shape": "dot", "size": 25, "title": "Improving Object Detection"}, {"color": "#66CCFF", "id": "Kihyuk Sohn", "label": "Kihyuk Sohn", "shape": "dot", "size": 25, "title": "Kihyuk Sohn"}, {"color": "#66CCFF", "id": "Improving ObjectDetection", "label": "Improving ObjectDetection", "shape": "dot", "size": 25, "title": "Improving ObjectDetection"}, {"color": "#66CCFF", "id": "Ruben Villegas", "label": "Ruben Villegas", "shape": "dot", "size": 25, "title": "Ruben Villegas"}, {"color": "#66CCFF", "id": "Gang Pan", "label": "Gang Pan", "shape": "dot", "size": 25, "title": "Gang Pan"}, {"color": "#66CCFF", "id": "Honglak Lee", "label": "Honglak Lee", "shape": "dot", "size": 25, "title": "Honglak Lee"}, {"color": "#66CCFF", "id": "object detection benchmarks", "label": "object detection benchmarks", "shape": "dot", "size": 25, "title": "object detection benchmarks"}, {"color": "#66CCFF", "id": "discriminative for categorization", "label": "discriminative for categorization", "shape": "dot", "size": 25, "title": "discriminative for categorization"}, {"color": "#66CCFF", "id": "inaccurate localization", "label": "inaccurate localization", "shape": "dot", "size": 25, "title": "inaccurate localization"}, {"color": "#66CCFF", "id": "major source of error", "label": "major source of error", "shape": "dot", "size": 25, "title": "major source of error"}, {"color": "#66CCFF", "id": "search algorithm", "label": "search algorithm", "shape": "dot", "size": 25, "title": "search algorithm"}, {"color": "#66CCFF", "id": "Bayesian optimization", "label": "Bayesian optimization", "shape": "dot", "size": 25, "title": "Bayesian optimization"}, {"color": "#66CCFF", "id": "candidate regions", "label": "candidate regions", "shape": "dot", "size": 25, "title": "candidate regions"}, {"color": "#66CCFF", "id": "structured loss", "label": "structured loss", "shape": "dot", "size": 25, "title": "structured loss"}, {"color": "#66CCFF", "id": "localization inaccuracy", "label": "localization inaccuracy", "shape": "dot", "size": 25, "title": "localization inaccuracy"}, {"color": "#66CCFF", "id": "methods", "label": "methods", "shape": "dot", "size": 25, "title": "methods"}, {"color": "#66CCFF", "id": "baseline method", "label": "baseline method", "shape": "dot", "size": 25, "title": "baseline method"}, {"color": "#66CCFF", "id": "PASPAL VOC 2", "label": "PASPAL VOC 2", "shape": "dot", "size": 25, "title": "PASPAL VOC 2"}, {"color": "#66CCFF", "id": "loss", "label": "loss", "shape": "dot", "size": 25, "title": "loss"}, {"color": "#66CCFF", "id": "proposed methods", "label": "proposed methods", "shape": "dot", "size": 25, "title": "proposed methods"}, {"color": "#66CCFF", "id": "PASPAL VOC 2007", "label": "PASPAL VOC 2007", "shape": "dot", "size": 25, "title": "PASPAL VOC 2007"}, {"color": "#66CCFF", "id": "two methods", "label": "two methods", "shape": "dot", "size": 25, "title": "two methods"}, {"color": "#66CCFF", "id": "complementary", "label": "complementary", "shape": "dot", "size": 25, "title": "complementary"}, {"color": "#66CCFF", "id": "combined methods", "label": "combined methods", "shape": "dot", "size": 25, "title": "combined methods"}, {"color": "#66CCFF", "id": "PASCAL VOC 2007", "label": "PASCAL VOC 2007", "shape": "dot", "size": 25, "title": "PASCAL VOC 2007"}, {"color": "#66CCFF", "id": "PASCUAL VOC 2012", "label": "PASCUAL VOC 2012", "shape": "dot", "size": 25, "title": "PASCUAL VOC 2012"}, {"color": "#66CCFF", "id": "Dimensionality Reduction", "label": "Dimensionality Reduction", "shape": "dot", "size": 25, "title": "Dimensionality Reduction"}, {"color": "#66CCFF", "id": "Bayesian Optimization", "label": "Bayesian Optimization", "shape": "dot", "size": 25, "title": "Bayesian Optimization"}, {"color": "#66CCFF", "id": "Structured Prediction (Structured SVM)", "label": "Structured Prediction (Structured SVM)", "shape": "dot", "size": 25, "title": "Structured Prediction (Structured SVM)"}, {"color": "#66CCFF", "id": "Localization Accuracy", "label": "Localization Accuracy", "shape": "dot", "size": 25, "title": "Localization Accuracy"}, {"color": "#66CCFF", "id": "Deep Networks", "label": "Deep Networks", "shape": "dot", "size": 25, "title": "Deep Networks"}, {"color": "#66CCFF", "id": "Greedy Layer-Wise Training", "label": "Greedy Layer-Wise Training", "shape": "dot", "size": 25, "title": "Greedy Layer-Wise Training"}, {"color": "#66CCFF", "id": "Local Binary Patterns", "label": "Local Binary Patterns", "shape": "dot", "size": 25, "title": "Local Binary Patterns"}, {"color": "#66CCFF", "id": "Representation Learning", "label": "Representation Learning", "shape": "dot", "size": 25, "title": "Representation Learning"}, {"color": "#66CCFF", "id": "Bengio, Y.", "label": "Bengio, Y.", "shape": "dot", "size": 25, "title": "Bengio, Y."}, {"color": "#66CCFF", "id": "Girschick, R.", "label": "Girschick, R.", "shape": "dot", "size": 25, "title": "Girschick, R."}, {"color": "#66CCFF", "id": "Rich Feature Hierarchies", "label": "Rich Feature Hierarchies", "shape": "dot", "size": 25, "title": "Rich Feature Hierarchies"}, {"color": "#66CCFF", "id": "Everingham, M.", "label": "Everingham, M.", "shape": "dot", "size": 25, "title": "Everingham, M."}, {"color": "#66CCFF", "id": "VOC2007", "label": "VOC2007", "shape": "dot", "size": 25, "title": "VOC2007"}, {"color": "#66CCFF", "id": "Deng, J.", "label": "Deng, J.", "shape": "dot", "size": 25, "title": "Deng, J."}, {"color": "#66CCFF", "id": "Hierarchical Image Database", "label": "Hierarchical Image Database", "shape": "dot", "size": 25, "title": "Hierarchical Image Database"}, {"color": "#66CCFF", "id": "Donahue, J.", "label": "Donahue, J.", "shape": "dot", "size": 25, "title": "Donahue, J."}, {"color": "#66CCFF", "id": "DeCAF", "label": "DeCAF", "shape": "dot", "size": 25, "title": "DeCAF"}, {"color": "#66CCFF", "id": "Visual Recognition", "label": "Visual Recognition", "shape": "dot", "size": 25, "title": "Visual Recognition"}, {"color": "#66CCFF", "id": "CoRR", "label": "CoRR", "shape": "dot", "size": 25, "title": "CoRR"}, {"color": "#66CCFF", "id": "Erhan", "label": "Erhan", "shape": "dot", "size": 25, "title": "Erhan"}, {"color": "#66CCFF", "id": "Erhan, D.", "label": "Erhan, D.", "shape": "dot", "size": 25, "title": "Erhan, D."}, {"color": "#66CCFF", "id": "Department of Computer Science, Zhejiang University", "label": "Department of Computer Science, Zhejiang University", "shape": "dot", "size": 25, "title": "Department of Computer Science, Zhejiang University"}, {"color": "#66CCFF", "id": "Department of Electrical Engineering and Computer Science, University of Michigan", "label": "Department of Electrical Engineering and Computer Science, University of Michigan", "shape": "dot", "size": 25, "title": "Department of Electrical Engineering and Computer Science, University of Michigan"}, {"color": "#66CCFF", "id": "Dongping Li", "label": "Dongping Li", "shape": "dot", "size": 25, "title": "Dongping Li"}, {"color": "#66CCFF", "id": "A Geodesic-Prepreserving Method for Image Warping", "label": "A Geodesic-Prepreserving Method for Image Warping", "shape": "dot", "size": 25, "title": "A Geodesic-Prepreserving Method for Image Warping"}, {"color": "#66CCFF", "id": "Department of Electrical Engineering and Computer Science", "label": "Department of Electrical Engineering and Computer Science", "shape": "dot", "size": 25, "title": "Department of Electrical Engineering and Computer Science"}, {"color": "#66CCFF", "id": "University of Michigan", "label": "University of Michigan", "shape": "dot", "size": 25, "title": "University of Michigan"}, {"color": "#66CCFF", "id": "A Geodesic-Preserving Method for Image Warping", "label": "A Geodesic-Preserving Method for Image Warping", "shape": "dot", "size": 25, "title": "A Geodesic-Preserving Method for Image Warping"}, {"color": "#66CCFF", "id": "A Geolesic-Preerving Method for Image Warping", "label": "A Geolesic-Preerving Method for Image Warping", "shape": "dot", "size": 25, "title": "A Geolesic-Preerving Method for Image Warping"}, {"color": "#66CCFF", "id": "Kun Zhou", "label": "Kun Zhou", "shape": "dot", "size": 25, "title": "Kun Zhou"}, {"color": "#66CCFF", "id": "honglak@umich.edu", "label": "honglak@umich.edu", "shape": "dot", "size": 25, "title": "honglak@umich.edu"}, {"color": "#66CCFF", "id": "Li_A_Geodesic-Preserving_Method_2015_CVPR_supplemental.pdf", "label": "Li_A_Geodesic-Preserving_Method_2015_CVPR_supplemental.pdf", "shape": "dot", "size": 25, "title": "Li_A_Geodesic-Preserving_Method_2015_CVPR_supplemental.pdf"}, {"color": "#66CCFF", "id": "Image Warping", "label": "Image Warping", "shape": "dot", "size": 25, "title": "Image Warping"}, {"color": "#66CCFF", "id": "geodesic-preserving method", "label": "geodesic-preserving method", "shape": "dot", "size": 25, "title": "geodesic-preserving method"}, {"color": "#66CCFF", "id": "maintain shape", "label": "maintain shape", "shape": "dot", "size": 25, "title": "maintain shape"}, {"color": "#66CCFF", "id": "distortions", "label": "distortions", "shape": "dot", "size": 25, "title": "distortions"}, {"color": "#66CCFF", "id": "core of method", "label": "core of method", "shape": "dot", "size": 25, "title": "core of method"}, {"color": "#66CCFF", "id": "preserving geodesic distances", "label": "preserving geodesic distances", "shape": "dot", "size": 25, "title": "preserving geodesic distances"}, {"color": "#66CCFF", "id": "local smoothness", "label": "local smoothness", "shape": "dot", "size": 25, "title": "local smoothness"}, {"color": "#66CCFF", "id": "unwanted artifacts", "label": "unwanted artifacts", "shape": "dot", "size": 25, "title": "unwanted artifacts"}, {"color": "#66CCFF", "id": "energy function", "label": "energy function", "shape": "dot", "size": 25, "title": "energy function"}, {"color": "#66CCFF", "id": "shape preservation terms", "label": "shape preservation terms", "shape": "dot", "size": 25, "title": "shape preservation terms"}, {"color": "#66CCFF", "id": "boundary preservation terms", "label": "boundary preservation terms", "shape": "dot", "size": 25, "title": "boundary preservation terms"}, {"color": "#66CCFF", "id": "geodesic preservation terms", "label": "geodesic preservation terms", "shape": "dot", "size": 25, "title": "geodesic preservation terms"}, {"color": "#66CCFF", "id": "Gauss-Newton method", "label": "Gauss-Newton method", "shape": "dot", "size": 25, "title": "Gauss-Newton method"}, {"color": "#66CCFF", "id": "high-quality warped images", "label": "high-quality warped images", "shape": "dot", "size": 25, "title": "high-quality warped images"}, {"color": "#66CCFF", "id": "Shape and Boundary Preservation", "label": "Shape and Boundary Preservation", "shape": "dot", "size": 25, "title": "Shape and Boundary Preservation"}, {"color": "#66CCFF", "id": "Energy Minimization", "label": "Energy Minimization", "shape": "dot", "size": 25, "title": "Energy Minimization"}, {"color": "#66CCFF", "id": "Gausss-Newton Method", "label": "Gausss-Newton Method", "shape": "dot", "size": 25, "title": "Gausss-Newton Method"}, {"color": "#66CCFF", "id": "Zhang, G. et al.", "label": "Zhang, G. et al.", "shape": "dot", "size": 25, "title": "Zhang, G. et al."}, {"color": "#66CCFF", "id": "A shape-preserving approach to image resizing", "label": "A shape-preserving approach to image resizing", "shape": "dot", "size": 25, "title": "A shape-preserving approach to image resizing"}, {"color": "#66CCFF", "id": "A shape-preserivng approach to image resizing", "label": "A shape-preserivng approach to image resizing", "shape": "dot", "size": 25, "title": "A shape-preserivng approach to image resizing"}, {"color": "#66CCFF", "id": "Rotation matrix R\u03b8,\u03c6", "label": "Rotation matrix R\u03b8,\u03c6", "shape": "dot", "size": 25, "title": "Rotation matrix R\u03b8,\u03c6"}, {"color": "#66CCFF", "id": "Eqn. (1)", "label": "Eqn. (1)", "shape": "dot", "size": 25, "title": "Eqn. (1)"}, {"color": "#66CCFF", "id": "Shape-preserving term ES(V)", "label": "Shape-preserving term ES(V)", "shape": "dot", "size": 25, "title": "Shape-preserving term ES(V)"}, {"color": "#66CCFF", "id": "Eqn. (7)", "label": "Eqn. (7)", "shape": "dot", "size": 25, "title": "Eqn. (7)"}, {"color": "#66CCFF", "id": "shape-preserving approach", "label": "shape-preserving approach", "shape": "dot", "size": 25, "title": "shape-preserving approach"}, {"color": "#66CCFF", "id": "Eqn. (7) - Shape-preserving term ES(V)", "label": "Eqn. (7) - Shape-preserving term ES(V)", "shape": "dot", "size": 25, "title": "Eqn. (7) - Shape-preserving term ES(V)"}, {"color": "#66CCFF", "id": "local smoothness preservation", "label": "local smoothness preservation", "shape": "dot", "size": 25, "title": "local smoothness preservation"}, {"color": "#66CCFF", "id": "Eqn. (4) - Local smoothness preservation EC(V)", "label": "Eqn. (4) - Local smoothness preservation EC(V)", "shape": "dot", "size": 25, "title": "Eqn. (4) - Local smoothness preservation EC(V)"}, {"color": "#66CCFF", "id": "Eqn. (5) - Combined energy function E(V)", "label": "Eqn. (5) - Combined energy function E(V)", "shape": "dot", "size": 25, "title": "Eqn. (5) - Combined energy function E(V)"}, {"color": "#66CCFF", "id": "overall energy function", "label": "overall energy function", "shape": "dot", "size": 25, "title": "overall energy function"}, {"color": "#66CCFF", "id": "Q", "label": "Q", "shape": "dot", "size": 25, "title": "Q"}, {"color": "#66CCFF", "id": "orthogonal matrices", "label": "orthogonal matrices", "shape": "dot", "size": 25, "title": "orthogonal matrices"}, {"color": "#66CCFF", "id": "mathematical concept", "label": "mathematical concept", "shape": "dot", "size": 25, "title": "mathematical concept"}, {"color": "#66CCFF", "id": "E(V)", "label": "E(V)", "shape": "dot", "size": 25, "title": "E(V)"}, {"color": "#66CCFF", "id": "various terms", "label": "various terms", "shape": "dot", "size": 25, "title": "various terms"}, {"color": "#66CCFF", "id": "geodesic-preserving term", "label": "geodesic-preserving term", "shape": "dot", "size": 25, "title": "geodesic-preserving term"}, {"color": "#66CCFF", "id": "Antonio Agudo", "label": "Antonio Agudo", "shape": "dot", "size": 25, "title": "Antonio Agudo"}, {"color": "#66CCFF", "id": "Francesc Moreno-Noguer", "label": "Francesc Moreno-Noguer", "shape": "dot", "size": 25, "title": "Francesc Moreno-Noguer"}, {"color": "#66CCFF", "id": "Zhejiang University", "label": "Zhejiang University", "shape": "dot", "size": 25, "title": "Zhejiang University"}, {"color": "#66CCFF", "id": "Simultaneous Pose and Non-Rigid Shape", "label": "Simultaneous Pose and Non-Rigid Shape", "shape": "dot", "size": 25, "title": "Simultaneous Pose and Non-Rigid Shape"}, {"color": "#66CCFF", "id": "particle dynamics", "label": "particle dynamics", "shape": "dot", "size": 25, "title": "particle dynamics"}, {"color": "#66CCFF", "id": "mathematics", "label": "mathematics", "shape": "dot", "size": 25, "title": "mathematics"}, {"color": "#66CCFF", "id": "geodesic preservation", "label": "geodesic preservation", "shape": "dot", "size": 25, "title": "geodesic preservation"}, {"color": "#66CCFF", "id": "Simultaneous Pose and Non-Rigid Shape with Particle Dynamics", "label": "Simultaneous Pose and Non-Rigid Shape with Particle Dynamics", "shape": "dot", "size": 25, "title": "Simultaneous Pose and Non-Rigid Shape with Particle Dynamics"}, {"color": "#66CCFF", "id": "non-rigid shape", "label": "non-rigid shape", "shape": "dot", "size": 25, "title": "non-rigid shape"}, {"color": "#66CCFF", "id": "simultaneous pose", "label": "simultaneous pose", "shape": "dot", "size": 25, "title": "simultaneous pose"}, {"color": "#66CCFF", "id": "solution", "label": "solution", "shape": "dot", "size": 25, "title": "solution"}, {"color": "#66CCFF", "id": "camera pose", "label": "camera pose", "shape": "dot", "size": 25, "title": "camera pose"}, {"color": "#66CCFF", "id": "global representations", "label": "global representations", "shape": "dot", "size": 25, "title": "global representations"}, {"color": "#66CCFF", "id": "object", "label": "object", "shape": "dot", "size": 25, "title": "object"}, {"color": "#66CCFF", "id": "ensemble of particles", "label": "ensemble of particles", "shape": "dot", "size": 25, "title": "ensemble of particles"}, {"color": "#66CCFF", "id": "particle", "label": "particle", "shape": "dot", "size": 25, "title": "particle"}, {"color": "#66CCFF", "id": "Newton\u2019s second law of motion", "label": "Newton\u2019s second law of motion", "shape": "dot", "size": 25, "title": "Newton\u2019s second law of motion"}, {"color": "#66CCFF", "id": "dynamic model", "label": "dynamic model", "shape": "dot", "size": 25, "title": "dynamic model"}, {"color": "#66CCFF", "id": "bundle adjustment framework", "label": "bundle adjustment framework", "shape": "dot", "size": 25, "title": "bundle adjustment framework"}, {"color": "#66CCFF", "id": "noisy data", "label": "noisy data", "shape": "dot", "size": 25, "title": "noisy data"}, {"color": "#66CCFF", "id": "missing data", "label": "missing data", "shape": "dot", "size": 25, "title": "missing data"}, {"color": "#66CCFF", "id": "sudden camera motions", "label": "sudden camera motions", "shape": "dot", "size": 25, "title": "sudden camera motions"}, {"color": "#66CCFF", "id": "training data", "label": "training data", "shape": "dot", "size": 25, "title": "training data"}, {"color": "#66CCFF", "id": "efficient", "label": "efficient", "shape": "dot", "size": 25, "title": "efficient"}, {"color": "#66CCFF", "id": "no training data", "label": "no training data", "shape": "dot", "size": 25, "title": "no training data"}, {"color": "#66CCFF", "id": "validation", "label": "validation", "shape": "dot", "size": 25, "title": "validation"}, {"color": "#66CCFF", "id": "real video sequences", "label": "real video sequences", "shape": "dot", "size": 25, "title": "real video sequences"}, {"color": "#66CCFF", "id": "motion", "label": "motion", "shape": "dot", "size": 25, "title": "motion"}, {"color": "#66CCFF", "id": "articulated", "label": "articulated", "shape": "dot", "size": 25, "title": "articulated"}, {"color": "#66CCFF", "id": "non-rigid", "label": "non-rigid", "shape": "dot", "size": 25, "title": "non-rigid"}, {"color": "#66CCFF", "id": "shapes", "label": "shapes", "shape": "dot", "size": 25, "title": "shapes"}, {"color": "#66CCFF", "id": "discontinuous", "label": "discontinuous", "shape": "dot", "size": 25, "title": "discontinuous"}, {"color": "#66CCFF", "id": "comparably to batch methods", "label": "comparably to batch methods", "shape": "dot", "size": 25, "title": "comparably to batch methods"}, {"color": "#66CCFF", "id": "batch methods", "label": "batch methods", "shape": "dot", "size": 25, "title": "batch methods"}, {"color": "#66CCFF", "id": "sequential methods", "label": "sequential methods", "shape": "dot", "size": 25, "title": "sequential methods"}, {"color": "#66CCFF", "id": "System", "label": "System", "shape": "dot", "size": 25, "title": "System"}, {"color": "#66CCFF", "id": "Competing Batch Methods", "label": "Competing Batch Methods", "shape": "dot", "size": 25, "title": "Competing Batch Methods"}, {"color": "#66CCFF", "id": "Non-Rigid Structure from Motion", "label": "Non-Rigid Structure from Motion", "shape": "dot", "size": 25, "title": "Non-Rigid Structure from Motion"}, {"color": "#66CCFF", "id": "Particle Dynamics", "label": "Particle Dynamics", "shape": "dot", "size": 25, "title": "Particle Dynamics"}, {"color": "#66CCFF", "id": "Bundle Adjustment", "label": "Bundle Adjustment", "shape": "dot", "size": 25, "title": "Bundle Adjustment"}, {"color": "#66CCFF", "id": "Monocular Video Analysis", "label": "Monocular Video Analysis", "shape": "dot", "size": 25, "title": "Monocular Video Analysis"}, {"color": "#66CCFF", "id": "Instituto de Invesigaci\u00b4on en Ingenier\u00b4\u0131a de Arag\u00b4on (I3A)", "label": "Instituto de Invesigaci\u00b4on en Ingenier\u00b4\u0131a de Arag\u00b4on (I3A)", "shape": "dot", "size": 25, "title": "Instituto de Invesigaci\u00b4on en Ingenier\u00b4\u0131a de Arag\u00b4on (I3A)"}, {"color": "#66CCFF", "id": "Instituto de Invesigaci\u00b4on en Ingenier\u00b4\u0131a de Arag\u00b4on (I3a)", "label": "Instituto de Invesigaci\u00b4on en Ingenier\u00b4\u0131a de Arag\u00b4on (I3a)", "shape": "dot", "size": 25, "title": "Instituto de Invesigaci\u00b4on en Ingenier\u00b4\u0131a de Arag\u00b4on (I3a)"}, {"color": "#66CCFF", "id": "Universidad de Zaragoza", "label": "Universidad de Zaragoza", "shape": "dot", "size": 25, "title": "Universidad de Zaragoza"}, {"color": "#66CCFF", "id": "Institut de Rob`otica i Inform`atica Industrial (CSI-UPC)", "label": "Institut de Rob`otica i Inform`atica Industrial (CSI-UPC)", "shape": "dot", "size": 25, "title": "Institut de Rob`otica i Inform`atica Industrial (CSI-UPC)"}, {"color": "#66CCFF", "id": "Simone Frintrop", "label": "Simone Frintrop", "shape": "dot", "size": 25, "title": "Simone Frintrop"}, {"color": "#66CCFF", "id": "Thomas Werner", "label": "Thomas Werner", "shape": "dot", "size": 25, "title": "Thomas Werner"}, {"color": "#66CCFF", "id": "Germ\u00e1n M. Garc\u00eda", "label": "Germ\u00e1n M. Garc\u00eda", "shape": "dot", "size": 25, "title": "Germ\u00e1n M. Garc\u00eda"}, {"color": "#66CCFF", "id": "salience model", "label": "salience model", "shape": "dot", "size": 25, "title": "salience model"}, {"color": "#66CCFF", "id": "Itti et al.", "label": "Itti et al.", "shape": "dot", "size": 25, "title": "Itti et al."}, {"color": "#66CCFF", "id": "adaptations", "label": "adaptations", "shape": "dot", "size": 25, "title": "adaptations"}, {"color": "#66CCFF", "id": "scale-space structure", "label": "scale-space structure", "shape": "dot", "size": 25, "title": "scale-space structure"}, {"color": "#66CCFF", "id": "twin pyramid", "label": "twin pyramid", "shape": "dot", "size": 25, "title": "twin pyramid"}, {"color": "#66CCFF", "id": "elegant structure", "label": "elegant structure", "shape": "dot", "size": 25, "title": "elegant structure"}, {"color": "#66CCFF", "id": "speed", "label": "speed", "shape": "dot", "size": 25, "title": "speed"}, {"color": "#66CCFF", "id": "pixel-level salience computation", "label": "pixel-level salience computation", "shape": "dot", "size": 25, "title": "pixel-level salience computation"}, {"color": "#66CCFF", "id": "object proposal generation framework", "label": "object proposal generation framework", "shape": "dot", "size": 25, "title": "object proposal generation framework"}, {"color": "#66CCFF", "id": "foundational approaches", "label": "foundational approaches", "shape": "dot", "size": 25, "title": "foundational approaches"}, {"color": "#66CCFF", "id": "VOCUS2", "label": "VOCUS2", "shape": "dot", "size": 25, "title": "VOCUS2"}, {"color": "#66CCFF", "id": "ration framework", "label": "ration framework", "shape": "dot", "size": 25, "title": "ration framework"}, {"color": "#66CCFF", "id": "segment-based salience maps", "label": "segment-based salience maps", "shape": "dot", "size": 25, "title": "segment-based salience maps"}, {"color": "#66CCFF", "id": "benchmark datasets", "label": "benchmark datasets", "shape": "dot", "size": 25, "title": "benchmark datasets"}, {"color": "#66CCFF", "id": "importance", "label": "importance", "shape": "dot", "size": 25, "title": "importance"}, {"color": "#66CCFF", "id": "revisiting foundational approaches", "label": "revisiting foundational approaches", "shape": "dot", "size": 25, "title": "revisiting foundational approaches"}, {"color": "#66CCFF", "id": "adaptation", "label": "adaptation", "shape": "dot", "size": 25, "title": "adaptation"}, {"color": "#66CCFF", "id": "modern applications", "label": "modern applications", "shape": "dot", "size": 25, "title": "modern applications"}, {"color": "#66CCFF", "id": "t-based salience maps", "label": "t-based salience maps", "shape": "dot", "size": 25, "title": "t-based salience maps"}, {"color": "#66CCFF", "id": "Saliency Models", "label": "Saliency Models", "shape": "dot", "size": 25, "title": "Saliency Models"}, {"color": "#66CCFF", "id": "Itti Model", "label": "Itti Model", "shape": "dot", "size": 25, "title": "Itti Model"}, {"color": "#66CCFF", "id": "A cognitive approach for object discovery", "label": "A cognitive approach for object discovery", "shape": "dot", "size": 25, "title": "A cognitive approach for object discovery"}, {"color": "#66CCFF", "id": "Discriminant salieny", "label": "Discriminant salieny", "shape": "dot", "size": 25, "title": "Discriminant salieny"}, {"color": "#66CCFF", "id": "TPAMI", "label": "TPAMI", "shape": "dot", "size": 25, "title": "TPAMI"}, {"color": "#66CCFF", "id": "probabilistic bottom-up aggregation", "label": "probabilistic bottom-up aggregation", "shape": "dot", "size": 25, "title": "probabilistic bottom-up aggregation"}, {"color": "#66CCFF", "id": "State-of-the-art in visual attention modeling", "label": "State-of-the-art in visual attention modeling", "shape": "dot", "size": 25, "title": "State-of-the-art in visual attention modeling"}, {"color": "#66CCFF", "id": "Scale-space representation", "label": "Scale-space representation", "shape": "dot", "size": 25, "title": "Scale-space representation"}, {"color": "#66CCFF", "id": "asri", "label": "asri", "shape": "dot", "size": 25, "title": "asri"}, {"color": "#66CCFF", "id": "visual attention modeling", "label": "visual attention modeling", "shape": "dot", "size": 25, "title": "visual attention modeling"}, {"color": "#66CCFF", "id": "L. Itti", "label": "L. Itti", "shape": "dot", "size": 25, "title": "L. Itti"}, {"color": "#66CCFF", "id": "salienicy-based visual attention model", "label": "salienicy-based visual attention model", "shape": "dot", "size": 25, "title": "salienicy-based visual attention model"}, {"color": "#66CCFF", "id": "N. D. B. Bruce", "label": "N. D. B. Bruce", "shape": "dot", "size": 25, "title": "N. D. B. Bruce"}, {"color": "#66CCFF", "id": "information theoretic approach", "label": "information theoretic approach", "shape": "dot", "size": 25, "title": "information theoretic approach"}, {"color": "#66CCFF", "id": "L. Hurvich", "label": "L. Hurvich", "shape": "dot", "size": 25, "title": "L. Hurvich"}, {"color": "#66CCFF", "id": "opponent-process theory", "label": "opponent-process theory", "shape": "dot", "size": 25, "title": "opponent-process theory"}, {"color": "#66CCFF", "id": "Rheinische Friedrich-Wilhelms-Universit\u00a8at Bonn", "label": "Rheinische Friedrich-Wilhelms-Universit\u00a8at Bonn", "shape": "dot", "size": 25, "title": "Rheinische Friedrich-Wilhelms-Universit\u00a8at Bonn"}, {"color": "#66CCFF", "id": "Rheinische Friedrich-Wilhelms-Universit\u00e4t Bonn", "label": "Rheinische Friedrich-Wilhelms-Universit\u00e4t Bonn", "shape": "dot", "size": 25, "title": "Rheinische Friedrich-Wilhelms-Universit\u00e4t Bonn"}, {"color": "#66CCFF", "id": "Jiaolong Yang", "label": "Jiaolong Yang", "shape": "dot", "size": 25, "title": "Jiaolong Yang"}, {"color": "#66CCFF", "id": "Dense, Accurate Optical Flow Estimation", "label": "Dense, Accurate Optical Flow Estimation", "shape": "dot", "size": 25, "title": "Dense, Accurate Optical Flow Estimation"}, {"color": "#66CCFF", "id": "Yang_Dense_Accurate_Optical_2015_CVPR_paper", "label": "Yang_Dense_Accurate_Optical_2015_CVPR_paper", "shape": "dot", "size": 25, "title": "Yang_Dense_Accurate_Optical_2015_CVPR_paper"}, {"color": "#66CCFF", "id": "frintrop@iai.uni-bonn.de", "label": "frintrop@iai.uni-bonn.de", "shape": "dot", "size": 25, "title": "frintrop@iai.uni-bonn.de"}, {"color": "#66CCFF", "id": "Bonn", "label": "Bonn", "shape": "dot", "size": 25, "title": "Bonn"}, {"color": "#66CCFF", "id": "optical flow field", "label": "optical flow field", "shape": "dot", "size": 25, "title": "optical flow field"}, {"color": "#66CCFF", "id": "piecewise parametric flow model", "label": "piecewise parametric flow model", "shape": "dot", "size": 25, "title": "piecewise parametric flow model"}, {"color": "#66CCFF", "id": "multi-model fitting scheme", "label": "multi-model fitting scheme", "shape": "dot", "size": 25, "title": "multi-model fitting scheme"}, {"color": "#66CCFF", "id": "piecewise constant model assumption", "label": "piecewise constant model assumption", "shape": "dot", "size": 25, "title": "piecewise constant model assumption"}, {"color": "#66CCFF", "id": "flow field continuity constraint", "label": "flow field continuity constraint", "shape": "dot", "size": 25, "title": "flow field continuity constraint"}, {"color": "#66CCFF", "id": "homogeneous motions", "label": "homogeneous motions", "shape": "dot", "size": 25, "title": "homogeneous motions"}, {"color": "#66CCFF", "id": "complex motions", "label": "complex motions", "shape": "dot", "size": 25, "title": "complex motions"}, {"color": "#66CCFF", "id": "KITTI benchmark", "label": "KITTI benchmark", "shape": "dot", "size": 25, "title": "KITTI benchmark"}, {"color": "#66CCFF", "id": "MPI Sintel benchmark", "label": "MPI Sintel benchmark", "shape": "dot", "size": 25, "title": "MPI Sintel benchmark"}, {"color": "#66CCFF", "id": "Middlebury benchmark", "label": "Middlebury benchmark", "shape": "dot", "size": 25, "title": "Middlebury benchmark"}, {"color": "#66CCFF", "id": "top-tier performances", "label": "top-tier performances", "shape": "dot", "size": 25, "title": "top-tier performances"}, {"color": "#66CCFF", "id": "state of the art", "label": "state of the art", "shape": "dot", "size": 25, "title": "state of the art"}, {"color": "#66CCFF", "id": "equity constraint", "label": "equity constraint", "shape": "dot", "size": 25, "title": "equity constraint"}, {"color": "#66CCFF", "id": "Optical flow benchmarks", "label": "Optical flow benchmarks", "shape": "dot", "size": 25, "title": "Optical flow benchmarks"}, {"color": "#66CCFF", "id": "KITTI", "label": "KITTI", "shape": "dot", "size": 25, "title": "KITTI"}, {"color": "#66CCFF", "id": "MPI Sintel", "label": "MPI Sintel", "shape": "dot", "size": 25, "title": "MPI Sintel"}, {"color": "#66CCFF", "id": "Optical flow estimation", "label": "Optical flow estimation", "shape": "dot", "size": 25, "title": "Optical flow estimation"}, {"color": "#66CCFF", "id": "Piecewise parametric models", "label": "Piecewise parametric models", "shape": "dot", "size": 25, "title": "Piecewise parametric models"}, {"color": "#66CCFF", "id": "Energy minimization", "label": "Energy minimization", "shape": "dot", "size": 25, "title": "Energy minimization"}, {"color": "#66CCFF", "id": "Homography transformation", "label": "Homography transformation", "shape": "dot", "size": 25, "title": "Homography transformation"}, {"color": "#66CCFF", "id": "Baker et al. (2011)", "label": "Baker et al. (2011)", "shape": "dot", "size": 25, "title": "Baker et al. (2011)"}, {"color": "#66CCFF", "id": "database and evaluation methodology", "label": "database and evaluation methodology", "shape": "dot", "size": 25, "title": "database and evaluation methodology"}, {"color": "#66CCFF", "id": "Bao et al. (2014)", "label": "Bao et al. (2014)", "shape": "dot", "size": 25, "title": "Bao et al. (2014)"}, {"color": "#66CCFF", "id": "Fast edge-preserving patchmatch", "label": "Fast edge-preserving patchmatch", "shape": "dot", "size": 25, "title": "Fast edge-preserving patchmatch"}, {"color": "#66CCFF", "id": "Barnes et al. (2009)", "label": "Barnes et al. (2009)", "shape": "dot", "size": 25, "title": "Barnes et al. (2009)"}, {"color": "#66CCFF", "id": "PatchMatch", "label": "PatchMatch", "shape": "dot", "size": 25, "title": "PatchMatch"}, {"color": "#66CCFF", "id": "structural image editing", "label": "structural image editing", "shape": "dot", "size": 25, "title": "structural image editing"}, {"color": "#66CCFF", "id": "Piecewise image registration", "label": "Piecewise image registration", "shape": "dot", "size": 25, "title": "Piecewise image registration"}, {"color": "#66CCFF", "id": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "label": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "shape": "dot", "size": 25, "title": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"}, {"color": "#66CCFF", "id": "Multiway cut", "label": "Multiway cut", "shape": "dot", "size": 25, "title": "Multiway cut"}, {"color": "#66CCFF", "id": "stereo and motion", "label": "stereo and motion", "shape": "dot", "size": 25, "title": "stereo and motion"}, {"color": "#66CCFF", "id": "slanted surfaces", "label": "slanted surfaces", "shape": "dot", "size": 25, "title": "slanted surfaces"}, {"color": "#66CCFF", "id": "robust estimation", "label": "robust estimation", "shape": "dot", "size": 25, "title": "robust estimation"}, {"color": "#66CCFF", "id": "multiple motions", "label": "multiple motions", "shape": "dot", "size": 25, "title": "multiple motions"}, {"color": "#66CCFF", "id": "Black", "label": "Black", "shape": "dot", "size": 25, "title": "Black"}, {"color": "#66CCFF", "id": "Anandan", "label": "Anandan", "shape": "dot", "size": 25, "title": "Anandan"}, {"color": "#66CCFF", "id": "parametric flow fields", "label": "parametric flow fields", "shape": "dot", "size": 25, "title": "parametric flow fields"}, {"color": "#66CCFF", "id": "piecewise-smooth flow fields", "label": "piecewise-smooth flow fields", "shape": "dot", "size": 25, "title": "piecewise-smooth flow fields"}, {"color": "#66CCFF", "id": "Black, M. J.", "label": "Black, M. J.", "shape": "dot", "size": 25, "title": "Black, M. J."}, {"color": "#66CCFF", "id": "The robust estimation of multiple motions", "label": "The robust estimation of multiple motions", "shape": "dot", "size": 25, "title": "The robust estimation of multiple motions"}, {"color": "#66CCFF", "id": "Estimating optical flow in segmented images", "label": "Estimating optical flow in segmented images", "shape": "dot", "size": 25, "title": "Estimating optical flow in segmented images"}, {"color": "#66CCFF", "id": "Anandan, P.", "label": "Anandan, P.", "shape": "dot", "size": 25, "title": "Anandan, P."}, {"color": "#66CCFF", "id": "Jepson, A. D.", "label": "Jepson, A. D.", "shape": "dot", "size": 25, "title": "Jepson, A. D."}, {"color": "#66CCFF", "id": "Bleyer, M.", "label": "Bleyer, M.", "shape": "dot", "size": 25, "title": "Bleyer, M."}, {"color": "#66CCFF", "id": "PatchMatch Stereo", "label": "PatchMatch Stereo", "shape": "dot", "size": 25, "title": "PatchMatch Stereo"}, {"color": "#66CCFF", "id": "Rhemann, C.", "label": "Rhemann, C.", "shape": "dot", "size": 25, "title": "Rhemann, C."}, {"color": "#66CCFF", "id": "Rother, C.", "label": "Rother, C.", "shape": "dot", "size": 25, "title": "Rother, C."}, {"color": "#66CCFF", "id": "PatchMatch Stere", "label": "PatchMatch Stere", "shape": "dot", "size": 25, "title": "PatchMatch Stere"}, {"color": "#66CCFF", "id": "Fast approximate energy minimization", "label": "Fast approximate energy minimization", "shape": "dot", "size": 25, "title": "Fast approximate energy minimization"}, {"color": "#66CCFF", "id": "IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI)", "label": "IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI)", "shape": "dot", "size": 25, "title": "IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI)"}, {"color": "#66CCFF", "id": "Braux-Zin et al.", "label": "Braux-Zin et al.", "shape": "dot", "size": 25, "title": "Braux-Zin et al."}, {"color": "#66CCFF", "id": "A general dense image matching framework", "label": "A general dense image matching framework", "shape": "dot", "size": 25, "title": "A general dense image matching framework"}, {"color": "#66CCFF", "id": "Beijing Lab of Intelligent Information Technology", "label": "Beijing Lab of Intelligent Information Technology", "shape": "dot", "size": 25, "title": "Beijing Lab of Intelligent Information Technology"}, {"color": "#66CCFF", "id": "Beijing Institute of Technology", "label": "Beijing Institute of Technology", "shape": "dot", "size": 25, "title": "Beijing Institute of Technology"}, {"color": "#66CCFF", "id": "Research School of Engineering", "label": "Research School of Engineering", "shape": "dot", "size": 25, "title": "Research School of Engineering"}, {"color": "#66CCFF", "id": "The Australian National University (ANU)", "label": "The Australian National University (ANU)", "shape": "dot", "size": 25, "title": "The Australian National University (ANU)"}, {"color": "#66CCFF", "id": "Thomas Mauthner", "label": "Thomas Mauthner", "shape": "dot", "size": 25, "title": "Thomas Mauthner"}, {"color": "#66CCFF", "id": "Encoding Based Saliency Detection for Videos and Images", "label": "Encoding Based Saliency Detection for Videos and Images", "shape": "dot", "size": 25, "title": "Encoding Based Saliency Detection for Videos and Images"}, {"color": "#66CCFF", "id": "Horst Possegger", "label": "Horst Possegger", "shape": "dot", "size": 25, "title": "Horst Possegger"}, {"color": "#66CCFF", "id": "Georg Waltner", "label": "Georg Waltner", "shape": "dot", "size": 25, "title": "Georg Waltner"}, {"color": "#66CCFF", "id": "Horst Bischof", "label": "Horst Bischof", "shape": "dot", "size": 25, "title": "Horst Bischof"}, {"color": "#66CCFF", "id": "Mauthner_Encoding_Based_Saliency_2015_CVPR_paper", "label": "Mauthner_Encoding_Based_Saliency_2015_CVPR_paper", "shape": "dot", "size": 25, "title": "Mauthner_Encoding_Based_Saliency_2015_CVPR_paper"}, {"color": "#66CCFF", "id": "Mauthner_Encoding_Based_Salieney_2015_CVPR_paper", "label": "Mauthner_Encoding_Based_Salieney_2015_CVPR_paper", "shape": "dot", "size": 25, "title": "Mauthner_Encoding_Based_Salieney_2015_CVPR_paper"}, {"color": "#66CCFF", "id": "Mauthne_Encoding_Based_Salieney_2015_CVPR_paper", "label": "Mauthne_Encoding_Based_Salieney_2015_CVPR_paper", "shape": "dot", "size": 25, "title": "Mauthne_Encoding_Based_Salieney_2015_CVPR_paper"}, {"color": "#66CCFF", "id": "predicting human gaze", "label": "predicting human gaze", "shape": "dot", "size": 25, "title": "predicting human gaze"}, {"color": "#66CCFF", "id": "eye-traking data", "label": "eye-traking data", "shape": "dot", "size": 25, "title": "eye-traking data"}, {"color": "#66CCFF", "id": "reliance on human gaze", "label": "reliance on human gaze", "shape": "dot", "size": 25, "title": "reliance on human gaze"}, {"color": "#66CCFF", "id": "bias", "label": "bias", "shape": "dot", "size": 25, "title": "bias"}, {"color": "#66CCFF", "id": "unsupervised video salieny detection method", "label": "unsupervised video salieny detection method", "shape": "dot", "size": 25, "title": "unsupervised video salieny detection method"}, {"color": "#66CCFF", "id": "human activity recognition", "label": "human activity recognition", "shape": "dot", "size": 25, "title": "human activity recognition"}, {"color": "#66CCFF", "id": "training of activity detection algorithms", "label": "training of activity detection algorithms", "shape": "dot", "size": 25, "title": "training of activity detection algorithms"}, {"color": "#66CCFF", "id": "encoding method", "label": "encoding method", "shape": "dot", "size": 25, "title": "encoding method"}, {"color": "#66CCFF", "id": "joint feature distributions", "label": "joint feature distributions", "shape": "dot", "size": 25, "title": "joint feature distributions"}, {"color": "#66CCFF", "id": "salience computation", "label": "salience computation", "shape": "dot", "size": 25, "title": "salience computation"}, {"color": "#66CCFF", "id": "Gestalt principle of figure-ground segregation", "label": "Gestalt principle of figure-ground segregation", "shape": "dot", "size": 25, "title": "Gestalt principle of figure-ground segregation"}, {"color": "#66CCFF", "id": "challenging datasets", "label": "challenging datasets", "shape": "dot", "size": 25, "title": "challenging datasets"}, {"color": "#66CCFF", "id": "favorable performance", "label": "favorable performance", "shape": "dot", "size": 25, "title": "favorable performance"}, {"color": "#66CCFF", "id": "ground-truth eye-gaze", "label": "ground-truth eye-gaze", "shape": "dot", "size": 25, "title": "ground-truth eye-gaze"}, {"color": "#66CCFF", "id": "re-ground segregation", "label": "re-ground segregation", "shape": "dot", "size": 25, "title": "re-ground segregation"}, {"color": "#66CCFF", "id": "Video Salience Detection", "label": "Video Salience Detection", "shape": "dot", "size": 25, "title": "Video Salience Detection"}, {"color": "#66CCFF", "id": "Gestalt Principles", "label": "Gestalt Principles", "shape": "dot", "size": 25, "title": "Gestalt Principles"}, {"color": "#66CCFF", "id": "Encoding Methods", "label": "Encoding Methods", "shape": "dot", "size": 25, "title": "Encoding Methods"}, {"color": "#66CCFF", "id": "Itti, L.", "label": "Itti, L.", "shape": "dot", "size": 25, "title": "Itti, L."}, {"color": "#66CCFF", "id": "model of salience-based visual attention", "label": "model of salience-based visual attention", "shape": "dot", "size": 25, "title": "model of salience-based visual attention"}, {"color": "#66CCFF", "id": "Alexe, B.", "label": "Alexe, B.", "shape": "dot", "size": 25, "title": "Alexe, B."}, {"color": "#66CCFF", "id": "What is an object?", "label": "What is an object?", "shape": "dot", "size": 25, "title": "What is an object?"}, {"color": "#66CCFF", "id": "Liu, T.", "label": "Liu, T.", "shape": "dot", "size": 25, "title": "Liu, T."}, {"color": "#66CCFF", "id": "Learning to Detect A Salient Object", "label": "Learning to Detect A Salient Object", "shape": "dot", "size": 25, "title": "Learning to Detect A Salient Object"}, {"color": "#66CCFF", "id": "Johansson, G.", "label": "Johansson, G.", "shape": "dot", "size": 25, "title": "Johansson, G."}, {"color": "#66CCFF", "id": "model for analysis of biological motion", "label": "model for analysis of biological motion", "shape": "dot", "size": 25, "title": "model for analysis of biological motion"}, {"color": "#66CCFF", "id": "Gorelick, L.", "label": "Gorelick, L.", "shape": "dot", "size": 25, "title": "Gorelick, L."}, {"color": "#66CCFF", "id": "Actions as Space-Time Shapes", "label": "Actions as Space-Time Shapes", "shape": "dot", "size": 25, "title": "Actions as Space-Time Shapes"}, {"color": "#66CCFF", "id": "Salient Object Detection: A Benchmark", "label": "Salient Object Detection: A Benchmark", "shape": "dot", "size": 25, "title": "Salient Object Detection: A Benchmark"}, {"color": "#66CCFF", "id": "Human Activity Recognition", "label": "Human Activity Recognition", "shape": "dot", "size": 25, "title": "Human Activity Recognition"}, {"color": "#66CCFF", "id": "Gorelick", "label": "Gorelick", "shape": "dot", "size": 25, "title": "Gorelick"}, {"color": "#66CCFF", "id": "Blank", "label": "Blank", "shape": "dot", "size": 25, "title": "Blank"}, {"color": "#66CCFF", "id": "Shechtman", "label": "Shechtman", "shape": "dot", "size": 25, "title": "Shechtman"}, {"color": "#66CCFF", "id": "Irani", "label": "Irani", "shape": "dot", "size": 25, "title": "Irani"}, {"color": "#66CCFF", "id": "Basri", "label": "Basri", "shape": "dot", "size": 25, "title": "Basri"}, {"color": "#66CCFF", "id": "Borji", "label": "Borji", "shape": "dot", "size": 25, "title": "Borji"}, {"color": "#66CCFF", "id": "Sihte", "label": "Sihte", "shape": "dot", "size": 25, "title": "Sihte"}, {"color": "#66CCFF", "id": "Salient Object Determination: A Benchmark", "label": "Salient Object Determination: A Benchmark", "shape": "dot", "size": 25, "title": "Salient Object Determination: A Benchmark"}, {"color": "#66CCFF", "id": "Itti", "label": "Itti", "shape": "dot", "size": 25, "title": "Itti"}, {"color": "#66CCFF", "id": "Guo", "label": "Guo", "shape": "dot", "size": 25, "title": "Guo"}, {"color": "#66CCFF", "id": "Spatio-temporal Saliency detection", "label": "Spatio-temporal Saliency detection", "shape": "dot", "size": 25, "title": "Spatio-temporal Saliency detection"}, {"color": "#66CCFF", "id": "Zhang", "label": "Zhang", "shape": "dot", "size": 25, "title": "Zhang"}, {"color": "#66CCFF", "id": "Judd", "label": "Judd", "shape": "dot", "size": 25, "title": "Judd"}, {"color": "#66CCFF", "id": "Learning to Predict Where Humans Look", "label": "Learning to Predict Where Humans Look", "shape": "dot", "size": 25, "title": "Learning to Predict Where Humans Look"}, {"color": "#66CCFF", "id": "Ehinger", "label": "Ehinger", "shape": "dot", "size": 25, "title": "Ehinger"}, {"color": "#66CCFF", "id": "Durand", "label": "Durand", "shape": "dot", "size": 25, "title": "Durand"}, {"color": "#66CCFF", "id": "Harel", "label": "Harel", "shape": "dot", "size": 25, "title": "Harel"}, {"color": "#66CCFF", "id": "Graph-based Visual Saliency", "label": "Graph-based Visual Saliency", "shape": "dot", "size": 25, "title": "Graph-based Visual Saliency"}, {"color": "#66CCFF", "id": "Koch", "label": "Koch", "shape": "dot", "size": 25, "title": "Koch"}, {"color": "#66CCFF", "id": "Perona", "label": "Perona", "shape": "dot", "size": 25, "title": "Perona"}, {"color": "#66CCFF", "id": "Rahtu", "label": "Rahtu", "shape": "dot", "size": 25, "title": "Rahtu"}, {"color": "#66CCFF", "id": "Segmenting Salient Objects from Images and Videos", "label": "Segmenting Salient Objects from Images and Videos", "shape": "dot", "size": 25, "title": "Segmenting Salient Objects from Images and Videos"}, {"color": "#66CCFF", "id": "Kannala", "label": "Kannala", "shape": "dot", "size": 25, "title": "Kannala"}, {"color": "#66CCFF", "id": "Salo", "label": "Salo", "shape": "dot", "size": 25, "title": "Salo"}, {"color": "#66CCFF", "id": "Heikkil\u00a8a", "label": "Heikkil\u00a8a", "shape": "dot", "size": 25, "title": "Heikkil\u00a8a"}, {"color": "#66CCFF", "id": "Mauthner", "label": "Mauthner", "shape": "dot", "size": 25, "title": "Mauthner"}, {"color": "#66CCFF", "id": "Institute for Computer Graphics and Vision, Graz University of Technology", "label": "Institute for Computer Graphics and Vision, Graz University of Technology", "shape": "dot", "size": 25, "title": "Institute for Computer Graphics and Vision, Graz University of Technology"}, {"color": "#66CCFF", "id": "Possegger", "label": "Possegger", "shape": "dot", "size": 25, "title": "Possegger"}, {"color": "#66CCFF", "id": "Waltner", "label": "Waltner", "shape": "dot", "size": 25, "title": "Waltner"}, {"color": "#66CCFF", "id": "Institute for Computer Graphics and Vision", "label": "Institute for Computer Graphics and Vision", "shape": "dot", "size": 25, "title": "Institute for Computer Graphics and Vision"}, {"color": "#66CCFF", "id": "Guanbin Li", "label": "Guanbin Li", "shape": "dot", "size": 25, "title": "Guanbin Li"}, {"color": "#66CCFF", "id": "Visual Saliency Based on Multiscale Deep Features", "label": "Visual Saliency Based on Multiscale Deep Features", "shape": "dot", "size": 25, "title": "Visual Saliency Based on Multiscale Deep Features"}, {"color": "#66CCFF", "id": "Yizhou Yu", "label": "Yizhou Yu", "shape": "dot", "size": 25, "title": "Yizhou Yu"}, {"color": "#66CCFF", "id": "possegger@icg.tugraz.at", "label": "possegger@icg.tugraz.at", "shape": "dot", "size": 25, "title": "possegger@icg.tugraz.at"}, {"color": "#66CCFF", "id": "waltner@icg.tugraz.at", "label": "waltner@icg.tugraz.at", "shape": "dot", "size": 25, "title": "waltner@icg.tugraz.at"}, {"color": "#66CCFF", "id": "bischof@icg.tugraz.at", "label": "bischof@icg.tugraz.at", "shape": "dot", "size": 25, "title": "bischof@icg.tugraz.at"}, {"color": "#66CCFF", "id": "Deep Features", "label": "Deep Features", "shape": "dot", "size": 25, "title": "Deep Features"}, {"color": "#66CCFF", "id": "Multiple Scales", "label": "Multiple Scales", "shape": "dot", "size": 25, "title": "Multiple Scales"}, {"color": "#66CCFF", "id": "SED dataset", "label": "SED dataset", "shape": "dot", "size": 25, "title": "SED dataset"}, {"color": "#66CCFF", "id": "HKU-IS dataset", "label": "HKU-IS dataset", "shape": "dot", "size": 25, "title": "HKU-IS dataset"}, {"color": "#66CCFF", "id": "Superior Performance", "label": "Superior Performance", "shape": "dot", "size": 25, "title": "Superior Performance"}, {"color": "#66CCFF", "id": "Precision", "label": "Precision", "shape": "dot", "size": 25, "title": "Precision"}, {"color": "#66CCFF", "id": "Recall", "label": "Recall", "shape": "dot", "size": 25, "title": "Recall"}, {"color": "#66CCFF", "id": "F-measure", "label": "F-measure", "shape": "dot", "size": 25, "title": "F-measure"}, {"color": "#66CCFF", "id": "Mean Absolute Error", "label": "Mean Absolute Error", "shape": "dot", "size": 25, "title": "Mean Absolute Error"}, {"color": "#66CCFF", "id": "MDF approach", "label": "MDF approach", "shape": "dot", "size": 25, "title": "MDF approach"}, {"color": "#66CCFF", "id": "Salience maps", "label": "Salience maps", "shape": "dot", "size": 25, "title": "Salience maps"}, {"color": "#66CCFF", "id": "accurate salience maps", "label": "accurate salience maps", "shape": "dot", "size": 25, "title": "accurate salience maps"}, {"color": "#66CCFF", "id": "Spectral residual approach", "label": "Spectral residual approach", "shape": "dot", "size": 25, "title": "Spectral residual approach"}, {"color": "#66CCFF", "id": "Salience detection", "label": "Salience detection", "shape": "dot", "size": 25, "title": "Salience detection"}, {"color": "#66CCFF", "id": "Salient object detection", "label": "Salient object detection", "shape": "dot", "size": 25, "title": "Salient object detection"}, {"color": "#66CCFF", "id": "Deep Learning Features", "label": "Deep Learning Features", "shape": "dot", "size": 25, "title": "Deep Learning Features"}, {"color": "#66CCFF", "id": "Multiscale Analysis", "label": "Multiscale Analysis", "shape": "dot", "size": 25, "title": "Multiscale Analysis"}, {"color": "#66CCFF", "id": "TRAMI", "label": "TRAMI", "shape": "dot", "size": 25, "title": "TRAMI"}, {"color": "#66CCFF", "id": "Yuan et al. (2013)", "label": "Yuan et al. (2013)", "shape": "dot", "size": 25, "title": "Yuan et al. (2013)"}, {"color": "#66CCFF", "id": "Perazzi et al. (2012)", "label": "Perazzi et al. (2012)", "shape": "dot", "size": 25, "title": "Perazzi et al. (2012)"}, {"color": "#66CCFF", "id": "salient region detection", "label": "salient region detection", "shape": "dot", "size": 25, "title": "salient region detection"}, {"color": "#66CCFF", "id": "Wei et al. (2012)", "label": "Wei et al. (2012)", "shape": "dot", "size": 25, "title": "Wei et al. (2012)"}, {"color": "#66CCFF", "id": "Yan et al. (2013)", "label": "Yan et al. (2013)", "shape": "dot", "size": 25, "title": "Yan et al. (2013)"}, {"color": "#66CCFF", "id": "Yang et al. (2013)", "label": "Yang et al. (2013)", "shape": "dot", "size": 25, "title": "Yang et al. (2013)"}, {"color": "#66CCFF", "id": "Salient region detection", "label": "Salient region detection", "shape": "dot", "size": 25, "title": "Salient region detection"}, {"color": "#66CCFF", "id": "Salience filters", "label": "Salience filters", "shape": "dot", "size": 25, "title": "Salience filters"}, {"color": "#66CCFF", "id": "Zhu et al. (2014)", "label": "Zhu et al. (2014)", "shape": "dot", "size": 25, "title": "Zhu et al. (2014)"}, {"color": "#66CCFF", "id": "Sun et al. (2014)", "label": "Sun et al. (2014)", "shape": "dot", "size": 25, "title": "Sun et al. (2014)"}, {"color": "#66CCFF", "id": "discriminative manifold-based approach", "label": "discriminative manifold-based approach", "shape": "dot", "size": 25, "title": "discriminative manifold-based approach"}, {"color": "#66CCFF", "id": "Minsu Cho", "label": "Minsu Cho", "shape": "dot", "size": 25, "title": "Minsu Cho"}, {"color": "#66CCFF", "id": "Unsupervised Object Discovery and Localization", "label": "Unsupervised Object Discovery and Localization", "shape": "dot", "size": 25, "title": "Unsupervised Object Discovery and Localization"}, {"color": "#66CCFF", "id": "Suha Kwak", "label": "Suha Kwak", "shape": "dot", "size": 25, "title": "Suha Kwak"}, {"color": "#66CCFF", "id": "Cordelia Schmid", "label": "Cordelia Schmid", "shape": "dot", "size": 25, "title": "Cordelia Schmid"}, {"color": "#66CCFF", "id": "Unsupervised Object Detection and Localization", "label": "Unsupervised Object Detection and Localization", "shape": "dot", "size": 25, "title": "Unsupervised Object Detection and Localization"}, {"color": "#66CCFF", "id": "bottom-up region proposals", "label": "bottom-up region proposals", "shape": "dot", "size": 25, "title": "bottom-up region proposals"}, {"color": "#66CCFF", "id": "Unsupervised Object Discovery and Localization in the Wild", "label": "Unsupervised Object Discovery and Localization in the Wild", "shape": "dot", "size": 25, "title": "Unsupervised Object Discovery and Localization in the Wild"}, {"color": "#66CCFF", "id": "object discovery", "label": "object discovery", "shape": "dot", "size": 25, "title": "object discovery"}, {"color": "#66CCFF", "id": "object localization", "label": "object localization", "shape": "dot", "size": 25, "title": "object localization"}, {"color": "#66CCFF", "id": "part-based matching", "label": "part-based matching", "shape": "dot", "size": 25, "title": "part-based matching"}, {"color": "#66CCFF", "id": "Unsupervised Object Localization", "label": "Unsupervised Object Localization", "shape": "dot", "size": 25, "title": "Unsupervised Object Localization"}, {"color": "#66CCFF", "id": "Unsupervised Object Discovery", "label": "Unsupervised Object Discovery", "shape": "dot", "size": 25, "title": "Unsupervised Object Discovery"}, {"color": "#66CCFF", "id": "discovery problem", "label": "discovery problem", "shape": "dot", "size": 25, "title": "discovery problem"}, {"color": "#66CCFF", "id": "localization", "label": "localization", "shape": "dot", "size": 25, "title": "localization"}, {"color": "#66CCFF", "id": "setting", "label": "setting", "shape": "dot", "size": 25, "title": "setting"}, {"color": "#66CCFF", "id": "fully unsupervised", "label": "fully unsupervised", "shape": "dot", "size": 25, "title": "fully unsupervised"}, {"color": "#66CCFF", "id": "part-based region matching", "label": "part-based region matching", "shape": "dot", "size": 25, "title": "part-based region matching"}, {"color": "#66CCFF", "id": "region proposals", "label": "region proposals", "shape": "dot", "size": 25, "title": "region proposals"}, {"color": "#66CCFF", "id": "candidate bounding boxes", "label": "candidate bounding boxes", "shape": "dot", "size": 25, "title": "candidate bounding boxes"}, {"color": "#66CCFF", "id": "correspondence", "label": "correspondence", "shape": "dot", "size": 25, "title": "correspondence"}, {"color": "#66CCFF", "id": "probabilistic Hough transform", "label": "probabilistic Hough transform", "shape": "dot", "size": 25, "title": "probabilistic Hough transform"}, {"color": "#66CCFF", "id": "Hough transform", "label": "Hough transform", "shape": "dot", "size": 25, "title": "Hough transform"}, {"color": "#66CCFF", "id": "mixed-class datasets", "label": "mixed-class datasets", "shape": "dot", "size": 25, "title": "mixed-class datasets"}, {"color": "#66CCFF", "id": "candidate correspondence", "label": "candidate correspondence", "shape": "dot", "size": 25, "title": "candidate correspondence"}, {"color": "#66CCFF", "id": "dominant objects", "label": "dominant objects", "shape": "dot", "size": 25, "title": "dominant objects"}, {"color": "#66CCFF", "id": "comparing scores", "label": "comparing scores", "shape": "dot", "size": 25, "title": "comparing scores"}, {"color": "#66CCFF", "id": "selecting regions", "label": "selecting regions", "shape": "dot", "size": 25, "title": "selecting regions"}, {"color": "#66CCFF", "id": "regions", "label": "regions", "shape": "dot", "size": 25, "title": "regions"}, {"color": "#66CCFF", "id": "evaluations", "label": "evaluations", "shape": "dot", "size": 25, "title": "evaluations"}, {"color": "#66CCFF", "id": "standard benchmarks", "label": "standard benchmarks", "shape": "dot", "size": 25, "title": "standard benchmarks"}, {"color": "#66CCFF", "id": "current state of the art", "label": "current state of the art", "shape": "dot", "size": 25, "title": "current state of the art"}, {"color": "#66CCFF", "id": "robust object discovery", "label": "robust object discovery", "shape": "dot", "size": 25, "title": "robust object discovery"}, {"color": "#66CCFF", "id": "rd benchmarks", "label": "rd benchmarks", "shape": "dot", "size": 25, "title": "rd benchmarks"}, {"color": "#66CCFF", "id": "Probabiltistic Hough transform", "label": "Probabiltistic Hough transform", "shape": "dot", "size": 25, "title": "Probabiltistic Hough transform"}, {"color": "#66CCFF", "id": "multiple object detection", "label": "multiple object detection", "shape": "dot", "size": 25, "title": "multiple object detection"}, {"color": "#66CCFF", "id": "Alexe et al.", "label": "Alexe et al.", "shape": "dot", "size": 25, "title": "Alexe et al."}, {"color": "#66CCFF", "id": "Measuring the object-ness of image windows", "label": "Measuring the object-ness of image windows", "shape": "dot", "size": 25, "title": "Measuring the object-ness of image windows"}, {"color": "#66CCFF", "id": "Discriminative decorrelation for clustering and classification", "label": "Discriminative decorrelation for clustering and classification", "shape": "dot", "size": 25, "title": "Discriminative decorrelation for clustering and classification"}, {"color": "#66CCFF", "id": "Ballard", "label": "Ballard", "shape": "dot", "size": 25, "title": "Ballard"}, {"color": "#66CCFF", "id": "Generalizing the Hough transform", "label": "Generalizing the Hough transform", "shape": "dot", "size": 25, "title": "Generalizing the Hough transform"}, {"color": "#66CCFF", "id": "Joulin et al.", "label": "Joulin et al.", "shape": "dot", "size": 25, "title": "Joulin et al."}, {"color": "#66CCFF", "id": "Discriminative clustering for image co-segmentation", "label": "Discriminative clustering for image co-segmentation", "shape": "dot", "size": 25, "title": "Discriminative clustering for image co-segmentation"}, {"color": "#66CCFF", "id": "Cho et al.", "label": "Cho et al.", "shape": "dot", "size": 25, "title": "Cho et al."}, {"color": "#66CCFF", "id": "Learning graphs to match", "label": "Learning graphs to match", "shape": "dot", "size": 25, "title": "Learning graphs to match"}, {"color": "#66CCFF", "id": "Unsupervised object localization", "label": "Unsupervised object localization", "shape": "dot", "size": 25, "title": "Unsupervised object localization"}, {"color": "#66CCFF", "id": "Image Co-segmentation", "label": "Image Co-segmentation", "shape": "dot", "size": 25, "title": "Image Co-segmentation"}, {"color": "#66CCFF", "id": "Inria", "label": "Inria", "shape": "dot", "size": 25, "title": "Inria"}, {"color": "#66CCFF", "id": "\u00c9cole Normale Sup\u00e9rieure", "label": "\u00c9cole Normale Sup\u00e9rieure", "shape": "dot", "size": 25, "title": "\u00c9cole Normale Sup\u00e9rieure"}, {"color": "#66CCFF", "id": "Learning Graphs", "label": "Learning Graphs", "shape": "dot", "size": 25, "title": "Learning Graphs"}, {"color": "#66CCFF", "id": "Efficient Image Localization", "label": "Efficient Image Localization", "shape": "dot", "size": 25, "title": "Efficient Image Localization"}, {"color": "#66CCFF", "id": "Pictoiral Structures", "label": "Pictoiral Structures", "shape": "dot", "size": 25, "title": "Pictoiral Structures"}, {"color": "#66CCFF", "id": "IJCV", "label": "IJCV", "shape": "dot", "size": 25, "title": "IJCV"}, {"color": "#66CCFF", "id": "PSL Research University", "label": "PSL Research University", "shape": "dot", "size": 25, "title": "PSL Research University"}, {"color": "#66CCFF", "id": "Ijaz Akhter", "label": "Ijaz Akhter", "shape": "dot", "size": 25, "title": "Ijaz Akhter"}, {"color": "#66CCFF", "id": "Pose-Conditioned Joint Angle Limits", "label": "Pose-Conditioned Joint Angle Limits", "shape": "dot", "size": 25, "title": "Pose-Conditioned Joint Angle Limits"}, {"color": "#66CCFF", "id": "Michael J. Black", "label": "Michael J. Black", "shape": "dot", "size": 25, "title": "Michael J. Black"}, {"color": "#66CCFF", "id": "Pose-Conditioning Joint Angle Limits", "label": "Pose-Conditioning Joint Angle Limits", "shape": "dot", "size": 25, "title": "Pose-Conditioning Joint Angle Limits"}, {"color": "#66CCFF", "id": "Akhter_Pose-Conditioned_Joint_Angle_2015_CVPR_paper.pdf", "label": "Akhter_Pose-Conditioned_Joint_Angle_2015_CVPR_paper.pdf", "shape": "dot", "size": 25, "title": "Akhter_Pose-Conditioned_Joint_Angle_2015_CVPR_paper.pdf"}, {"color": "#66CCFF", "id": "3D human pose estimation", "label": "3D human pose estimation", "shape": "dot", "size": 25, "title": "3D human pose estimation"}, {"color": "#66CCFF", "id": "analysis of people in images and video", "label": "analysis of people in images and video", "shape": "dot", "size": 25, "title": "analysis of people in images and video"}, {"color": "#66CCFF", "id": "inherently ill-posed", "label": "inherently ill-posed", "shape": "dot", "size": 25, "title": "inherently ill-posed"}, {"color": "#66CCFF", "id": "prior over human poses", "label": "prior over human poses", "shape": "dot", "size": 25, "title": "prior over human poses"}, {"color": "#66CCFF", "id": "joint limits", "label": "joint limits", "shape": "dot", "size": 25, "title": "joint limits"}, {"color": "#66CCFF", "id": "pose", "label": "pose", "shape": "dot", "size": 25, "title": "pose"}, {"color": "#66CCFF", "id": "motion capture dataset", "label": "motion capture dataset", "shape": "dot", "size": 25, "title": "motion capture dataset"}, {"color": "#66CCFF", "id": "range of human poses", "label": "range of human poses", "shape": "dot", "size": 25, "title": "range of human poses"}, {"color": "#66CCFF", "id": "pose-dependent model of joint limits", "label": "pose-dependent model of joint limits", "shape": "dot", "size": 25, "title": "pose-dependent model of joint limits"}, {"color": "#66CCFF", "id": "research purposes", "label": "research purposes", "shape": "dot", "size": 25, "title": "research purposes"}, {"color": "#66CCFF", "id": "3D pose from 2D joint locations", "label": "3D pose from 2D joint locations", "shape": "dot", "size": 25, "title": "3D pose from 2D joint locations"}, {"color": "#66CCFF", "id": "over-complete dictionary of poses", "label": "over-complete dictionary of poses", "shape": "dot", "size": 25, "title": "over-complete dictionary of poses"}, {"color": "#66CCFF", "id": "good generalization", "label": "good generalization", "shape": "dot", "size": 25, "title": "good generalization"}, {"color": "#66CCFF", "id": "body pose", "label": "body pose", "shape": "dot", "size": 25, "title": "body pose"}, {"color": "#66CCFF", "id": "3D pose", "label": "3D pose", "shape": "dot", "size": 25, "title": "3D pose"}, {"color": "#66CCFF", "id": "2D joint locations", "label": "2D joint locations", "shape": "dot", "size": 25, "title": "2D joint locations"}, {"color": "#66CCFF", "id": "over-completes dictionary of poses", "label": "over-completes dictionary of poses", "shape": "dot", "size": 25, "title": "over-completes dictionary of poses"}, {"color": "#66CCFF", "id": "impossible poses", "label": "impossible poses", "shape": "dot", "size": 25, "title": "impossible poses"}, {"color": "#66CCFF", "id": "recent work", "label": "recent work", "shape": "dot", "size": 25, "title": "recent work"}, {"color": "#66CCFF", "id": "CMU mocap dataset", "label": "CMU mocap dataset", "shape": "dot", "size": 25, "title": "CMU mocap dataset"}, {"color": "#66CCFF", "id": "manual annotations", "label": "manual annotations", "shape": "dot", "size": 25, "title": "manual annotations"}, {"color": "#66CCFF", "id": "detections", "label": "detections", "shape": "dot", "size": 25, "title": "detections"}, {"color": "#66CCFF", "id": "Leeds sports pose dataset", "label": "Leeds sports pose dataset", "shape": "dot", "size": 25, "title": "Leeds sports pose dataset"}, {"color": "#66CCFF", "id": "he-art results", "label": "he-art results", "shape": "dot", "size": 25, "title": "he-art results"}, {"color": "#66CCFF", "id": "2D to 3D pose estimation", "label": "2D to 3D pose estimation", "shape": "dot", "size": 25, "title": "2D to 3D pose estimation"}, {"color": "#66CCFF", "id": "superior results", "label": "superior results", "shape": "dot", "size": 25, "title": "superior results"}, {"color": "#66CCFF", "id": "automatic detections", "label": "automatic detections", "shape": "dot", "size": 25, "title": "automatic detections"}, {"color": "#66CCFF", "id": "Andriluka, M. et al. (2010)", "label": "Andriluka, M. et al. (2010)", "shape": "dot", "size": 25, "title": "Andriluka, M. et al. (2010)"}, {"color": "#66CCFF", "id": "monocular 3D pose estimation and tracking", "label": "monocular 3D pose estimation and tracking", "shape": "dot", "size": 25, "title": "monocular 3D pose estimation and tracking"}, {"color": "#66CCFF", "id": "Barr`on, C. \u0026 Kakadiaris, I. (2001)", "label": "Barr`on, C. \u0026 Kakadiaris, I. (2001)", "shape": "dot", "size": 25, "title": "Barr`on, C. \u0026 Kakadiaris, I. (2001)"}, {"color": "#66CCFF", "id": "estimating anthropometry and pose", "label": "estimating anthropometry and pose", "shape": "dot", "size": 25, "title": "estimating anthropometry and pose"}, {"color": "#66CCFF", "id": "BenAbdelkader, C. \u0026 Yacoob, Y. (2008)", "label": "BenAbdelkader, C. \u0026 Yacoob, Y. (2008)", "shape": "dot", "size": 25, "title": "BenAbdelkader, C. \u0026 Yacoob, Y. (2008)"}, {"color": "#66CCFF", "id": "statistical estimation of human anthropometry", "label": "statistical estimation of human anthropometry", "shape": "dot", "size": 25, "title": "statistical estimation of human anthropometry"}, {"color": "#66CCFF", "id": "Human Pose Reconstruction", "label": "Human Pose Reconstruction", "shape": "dot", "size": 25, "title": "Human Pose Reconstruction"}, {"color": "#66CCFF", "id": "Motion Capture Data", "label": "Motion Capture Data", "shape": "dot", "size": 25, "title": "Motion Capture Data"}, {"color": "#66CCFF", "id": "Prior Models", "label": "Prior Models", "shape": "dot", "size": 25, "title": "Prior Models"}, {"color": "#66CCFF", "id": "Bourdev \u0026 Malik", "label": "Bourdev \u0026 Malik", "shape": "dot", "size": 25, "title": "Bourdev \u0026 Malik"}, {"color": "#66CCFF", "id": "International Conference on Computer Vision", "label": "International Conference on Computer Vision", "shape": "dot", "size": 25, "title": "International Conference on Computer Vision"}, {"color": "#66CCFF", "id": "Poselets", "label": "Poselets", "shape": "dot", "size": 25, "title": "Poselets"}, {"color": "#66CCFF", "id": "body part detector", "label": "body part detector", "shape": "dot", "size": 25, "title": "body part detector"}, {"color": "#66CCFF", "id": "3D human pose annotations", "label": "3D human pose annotations", "shape": "dot", "size": 25, "title": "3D human pose annotations"}, {"color": "#66CCFF", "id": "Chen, Nie, \u0026 Ji", "label": "Chen, Nie, \u0026 Ji", "shape": "dot", "size": 25, "title": "Chen, Nie, \u0026 Ji"}, {"color": "#66CCFF", "id": "Guan et al.", "label": "Guan et al.", "shape": "dot", "size": 25, "title": "Guan et al."}, {"color": "#66CCFF", "id": "Int. Conf. on Computer Vision (ICCV)", "label": "Int. Conf. on Computer Vision (ICCV)", "shape": "dot", "size": 25, "title": "Int. Conf. on Computer Vision (ICCV)"}, {"color": "#66CCFF", "id": "Estimating human shape", "label": "Estimating human shape", "shape": "dot", "size": 25, "title": "Estimating human shape"}, {"color": "#66CCFF", "id": "single image", "label": "single image", "shape": "dot", "size": 25, "title": "single image"}, {"color": "#66CCFF", "id": "Grochow et al.", "label": "Grochow et al.", "shape": "dot", "size": 25, "title": "Grochow et al."}, {"color": "#66CCFF", "id": "Style-based inverse kinematics", "label": "Style-based inverse kinematics", "shape": "dot", "size": 25, "title": "Style-based inverse kinematics"}, {"color": "#66CCFF", "id": "human anthropometry", "label": "human anthropometry", "shape": "dot", "size": 25, "title": "human anthropometry"}, {"color": "#66CCFF", "id": "single uncalibrated image", "label": "single uncalibrated image", "shape": "dot", "size": 25, "title": "single uncalibrated image"}, {"color": "#66CCFF", "id": "Computer Vision Conference", "label": "Computer Vision Conference", "shape": "dot", "size": 25, "title": "Computer Vision Conference"}, {"color": "#66CCFF", "id": "three-dimensional multivariate model", "label": "three-dimensional multivariate model", "shape": "dot", "size": 25, "title": "three-dimensional multivariate model"}, {"color": "#66CCFF", "id": "Clinical Biomechanics", "label": "Clinical Biomechanics", "shape": "dot", "size": 25, "title": "Clinical Biomechanics"}, {"color": "#66CCFF", "id": "Herda et al.", "label": "Herda et al.", "shape": "dot", "size": 25, "title": "Herda et al."}, {"color": "#66CCFF", "id": "Hierarchical implicit surface joint limits", "label": "Hierarchical implicit surface joint limits", "shape": "dot", "size": 25, "title": "Hierarchical implicit surface joint limits"}, {"color": "#66CCFF", "id": "Lin et al.", "label": "Lin et al.", "shape": "dot", "size": 25, "title": "Lin et al."}, {"color": "#66CCFF", "id": "sketching interface", "label": "sketching interface", "shape": "dot", "size": 25, "title": "sketching interface"}, {"color": "#66CCFF", "id": "IEEE Transactions on Visualization and Computer Graphics", "label": "IEEE Transactions on Visualization and Computer Graphics", "shape": "dot", "size": 25, "title": "IEEE Transactions on Visualization and Computer Graphics"}, {"color": "#66CCFF", "id": "Max Planck Institute for Intelligent Systems", "label": "Max Planck Institute for Intelligent Systems", "shape": "dot", "size": 25, "title": "Max Planck Institute for Intelligent Systems"}, {"color": "#66CCFF", "id": "ijaz.akhter@tuebingen.mpg.de", "label": "ijaz.akhter@tuebingen.mpg.de", "shape": "dot", "size": 25, "title": "ijaz.akhter@tuebingen.mpg.de"}, {"color": "#66CCFF", "id": "Ran Tao", "label": "Ran Tao", "shape": "dot", "size": 25, "title": "Ran Tao"}, {"color": "#66CCFF", "id": "Attributes and Categories for Generic Instance Search from One Example", "label": "Attributes and Categories for Generic Instance Search from One Example", "shape": "dot", "size": 25, "title": "Attributes and Categories for Generic Instance Search from One Example"}, {"color": "#66CCFF", "id": "Arnold W.M. Smeulders", "label": "Arnold W.M. Smeulders", "shape": "dot", "size": 25, "title": "Arnold W.M. Smeulders"}, {"color": "#66CCFF", "id": "Tao_Attributes_and_Categories_2015_CVPR_paper", "label": "Tao_Attributes_and_Categories_2015_CVPR_paper", "shape": "dot", "size": 25, "title": "Tao_Attributes_and_Categories_2015_CVPR_paper"}, {"color": "#66CCFF", "id": "Tuebingen", "label": "Tuebingen", "shape": "dot", "size": 25, "title": "Tuebingen"}, {"color": "#66CCFF", "id": "generic instance search problem", "label": "generic instance search problem", "shape": "dot", "size": 25, "title": "generic instance search problem"}, {"color": "#66CCFF", "id": "instance search methods", "label": "instance search methods", "shape": "dot", "size": 25, "title": "instance search methods"}, {"color": "#66CCFF", "id": "arbitrary 3D objects", "label": "arbitrary 3D objects", "shape": "dot", "size": 25, "title": "arbitrary 3D objects"}, {"color": "#66CCFF", "id": "shoes", "label": "shoes", "shape": "dot", "size": 25, "title": "shoes"}, {"color": "#66CCFF", "id": "category-specific attributes", "label": "category-specific attributes", "shape": "dot", "size": 25, "title": "category-specific attributes"}, {"color": "#66CCFF", "id": "appearance variations", "label": "appearance variations", "shape": "dot", "size": 25, "title": "appearance variations"}, {"color": "#66CCFF", "id": "object search", "label": "object search", "shape": "dot", "size": 25, "title": "object search"}, {"color": "#66CCFF", "id": "category-level information", "label": "category-level information", "shape": "dot", "size": 25, "title": "category-level information"}, {"color": "#66CCFF", "id": "combination", "label": "combination", "shape": "dot", "size": 25, "title": "combination"}, {"color": "#66CCFF", "id": "approaches relying on low-level features", "label": "approaches relying on low-level features", "shape": "dot", "size": 25, "title": "approaches relying on low-level features"}, {"color": "#66CCFF", "id": "core challenge", "label": "core challenge", "shape": "dot", "size": 25, "title": "core challenge"}, {"color": "#66CCFF", "id": "representing query image", "label": "representing query image", "shape": "dot", "size": 25, "title": "representing query image"}, {"color": "#66CCFF", "id": "query image", "label": "query image", "shape": "dot", "size": 25, "title": "query image"}, {"color": "#66CCFF", "id": "robust representation", "label": "robust representation", "shape": "dot", "size": 25, "title": "robust representation"}, {"color": "#66CCFF", "id": "rich representation", "label": "rich representation", "shape": "dot", "size": 25, "title": "rich representation"}, {"color": "#66CCFF", "id": "distinction from similar instances", "label": "distinction from similar instances", "shape": "dot", "size": 25, "title": "distinction from similar instances"}, {"color": "#66CCFF", "id": "distinction", "label": "distinction", "shape": "dot", "size": 25, "title": "distinction"}, {"color": "#66CCFF", "id": "similar instances", "label": "similar instances", "shape": "dot", "size": 25, "title": "similar instances"}, {"color": "#66CCFF", "id": "Generic Instance Search", "label": "Generic Instance Search", "shape": "dot", "size": 25, "title": "Generic Instance Search"}, {"color": "#66CCFF", "id": "Core Challenge", "label": "Core Challenge", "shape": "dot", "size": 25, "title": "Core Challenge"}, {"color": "#66CCFF", "id": "Appearance Variation", "label": "Appearance Variation", "shape": "dot", "size": 25, "title": "Appearance Variation"}, {"color": "#66CCFF", "id": "Attribute Representation", "label": "Attribute Representation", "shape": "dot", "size": 25, "title": "Attribute Representation"}, {"color": "#66CCFF", "id": "Large Scale Visual Recognition Challenge", "label": "Large Scale Visual Recognition Challenge", "shape": "dot", "size": 25, "title": "Large Scale Visual Recognition Challenge"}, {"color": "#66CCFF", "id": "attribute-based classification", "label": "attribute-based classification", "shape": "dot", "size": 25, "title": "attribute-based classification"}, {"color": "#66CCFF", "id": "Attribute Transfer", "label": "Attribute Transfer", "shape": "dot", "size": 25, "title": "Attribute Transfer"}, {"color": "#66CCFF", "id": "detecting unseen object classes", "label": "detecting unseen object classes", "shape": "dot", "size": 25, "title": "detecting unseen object classes"}, {"color": "#66CCFF", "id": "Multiple queries", "label": "Multiple queries", "shape": "dot", "size": 25, "title": "Multiple queries"}, {"color": "#66CCFF", "id": "large scale specific object retrieval", "label": "large scale specific object retrieval", "shape": "dot", "size": 25, "title": "large scale specific object retrieval"}, {"color": "#66CCFF", "id": "Aranandjelovic", "label": "Aranandjelovic", "shape": "dot", "size": 25, "title": "Aranandjelovic"}, {"color": "#66CCFF", "id": "Zisserman", "label": "Zisserman", "shape": "dot", "size": 25, "title": "Zisserman"}, {"color": "#66CCFF", "id": "Naphade", "label": "Naphade", "shape": "dot", "size": 25, "title": "Naphade"}, {"color": "#66CCFF", "id": "concept ontology", "label": "concept ontology", "shape": "dot", "size": 25, "title": "concept ontology"}, {"color": "#66CCFF", "id": "IEEE MultiMedia", "label": "IEEE MultiMedia", "shape": "dot", "size": 25, "title": "IEEE MultiMedia"}, {"color": "#66CCFF", "id": "Perdoch", "label": "Perdoch", "shape": "dot", "size": 25, "title": "Perdoch"}, {"color": "#66CCFF", "id": "efficient representation", "label": "efficient representation", "shape": "dot", "size": 25, "title": "efficient representation"}, {"color": "#66CCFF", "id": "Visual Object Classes Challenge", "label": "Visual Object Classes Challenge", "shape": "dot", "size": 25, "title": "Visual Object Classes Challenge"}, {"color": "#66CCFF", "id": "Farhad", "label": "Farhad", "shape": "dot", "size": 25, "title": "Farhad"}, {"color": "#66CCFF", "id": "describing objects", "label": "describing objects", "shape": "dot", "size": 25, "title": "describing objects"}, {"color": "#66CCFF", "id": "object attributes", "label": "object attributes", "shape": "dot", "size": 25, "title": "object attributes"}, {"color": "#66CCFF", "id": "Pascal Network", "label": "Pascal Network", "shape": "dot", "size": 25, "title": "Pascal Network"}, {"color": "#66CCFF", "id": "ISLA", "label": "ISLA", "shape": "dot", "size": 25, "title": "ISLA"}, {"color": "#66CCFF", "id": "Recognition algorithms", "label": "Recognition algorithms", "shape": "dot", "size": 25, "title": "Recognition algorithms"}, {"color": "#66CCFF", "id": "output of last layer", "label": "output of last layer", "shape": "dot", "size": 25, "title": "output of last layer"}, {"color": "#66CCFF", "id": "last layer output", "label": "last layer output", "shape": "dot", "size": 25, "title": "last layer output"}, {"color": "#66CCFF", "id": "spatially coarse", "label": "spatially coarse", "shape": "dot", "size": 25, "title": "spatially coarse"}, {"color": "#66CCFF", "id": "earlier layers", "label": "earlier layers", "shape": "dot", "size": 25, "title": "earlier layers"}, {"color": "#66CCFF", "id": "precise in localization", "label": "precise in localization", "shape": "dot", "size": 25, "title": "precise in localization"}, {"color": "#66CCFF", "id": "semantics", "label": "semantics", "shape": "dot", "size": 25, "title": "semantics"}, {"color": "#66CCFF", "id": "hypercolumn", "label": "hypercolumn", "shape": "dot", "size": 25, "title": "hypercolumn"}, {"color": "#66CCFF", "id": "vector of activations", "label": "vector of activations", "shape": "dot", "size": 25, "title": "vector of activations"}, {"color": "#66CCFF", "id": "pixel", "label": "pixel", "shape": "dot", "size": 25, "title": "pixel"}, {"color": "#66CCFF", "id": "CNN units", "label": "CNN units", "shape": "dot", "size": 25, "title": "CNN units"}, {"color": "#66CCFF", "id": "hypercolumns", "label": "hypercolumns", "shape": "dot", "size": 25, "title": "hypercolumns"}, {"color": "#66CCFF", "id": "pixel descriptors", "label": "pixel descriptors", "shape": "dot", "size": 25, "title": "pixel descriptors"}, {"color": "#66CCFF", "id": "simultaneous detection", "label": "simultaneous detection", "shape": "dot", "size": 25, "title": "simultaneous detection"}, {"color": "#66CCFF", "id": "localization task", "label": "localization task", "shape": "dot", "size": 25, "title": "localization task"}, {"color": "#66CCFF", "id": "keypoint localization", "label": "keypoint localization", "shape": "dot", "size": 25, "title": "keypoint localization"}, {"color": "#66CCFF", "id": "part labeling", "label": "part labeling", "shape": "dot", "size": 25, "title": "part labeling"}, {"color": "#66CCFF", "id": "Fine-grained Localization", "label": "Fine-grained Localization", "shape": "dot", "size": 25, "title": "Fine-grained Localization"}, {"color": "#66CCFF", "id": "Keypoint Localization", "label": "Keypoint Localization", "shape": "dot", "size": 25, "title": "Keypoint Localization"}, {"color": "#66CCFF", "id": "Deep Convolutional Networks", "label": "Deep Convolutional Networks", "shape": "dot", "size": 25, "title": "Deep Convolutional Networks"}, {"color": "#66CCFF", "id": "Spatial Pyramid Pooling", "label": "Spatial Pyramid Pooling", "shape": "dot", "size": 25, "title": "Spatial Pyramid Pooling"}, {"color": "#66CCFF", "id": "Multi-scale Feature Integration", "label": "Multi-scale Feature Integration", "shape": "dot", "size": 25, "title": "Multi-scale Feature Integration"}, {"color": "#66CCFF", "id": "Arbel\u00e1ez, P.", "label": "Arbel\u00e1ez, P.", "shape": "dot", "size": 25, "title": "Arbel\u00e1ez, P."}, {"color": "#66CCFF", "id": "Multiscale Combinatorial Grouping", "label": "Multiscale Combinatorial Grouping", "shape": "dot", "size": 25, "title": "Multiscale Combinatorial Grouping"}, {"color": "#66CCFF", "id": "He, K.", "label": "He, K.", "shape": "dot", "size": 25, "title": "He, K."}, {"color": "#66CCFF", "id": "Barron, J. T.", "label": "Barron, J. T.", "shape": "dot", "size": 25, "title": "Barron, J. T."}, {"color": "#66CCFF", "id": "Volumetric Semantic Segmentation", "label": "Volumetric Semantic Segmentation", "shape": "dot", "size": 25, "title": "Volumetric Semantic Segmentation"}, {"color": "#66CCFF", "id": "Hypercolumn Representation", "label": "Hypercolumn Representation", "shape": "dot", "size": 25, "title": "Hypercolumn Representation"}, {"color": "#66CCFF", "id": "Barron et al.", "label": "Barron et al.", "shape": "dot", "size": 25, "title": "Barron et al."}, {"color": "#66CCFF", "id": "Hubel \u0026 Wiesel", "label": "Hubel \u0026 Wiesel", "shape": "dot", "size": 25, "title": "Hubel \u0026 Wiesel"}, {"color": "#66CCFF", "id": "The Journal of Physiology", "label": "The Journal of Physiology", "shape": "dot", "size": 25, "title": "The Journal of Physiology"}, {"color": "#66CCFF", "id": "Bo \u0026 Fowlakes", "label": "Bo \u0026 Fowlakes", "shape": "dot", "size": 25, "title": "Bo \u0026 Fowlakes"}, {"color": "#66CCFF", "id": "Ionescu et al.", "label": "Ionescu et al.", "shape": "dot", "size": 25, "title": "Ionescu et al."}, {"color": "#66CCFF", "id": "Jones \u0026 Malik", "label": "Jones \u0026 Malik", "shape": "dot", "size": 25, "title": "Jones \u0026 Malik"}, {"color": "#66CCFF", "id": "Koenderink \u0026 van Doorn", "label": "Koenderink \u0026 van Doorn", "shape": "dot", "size": 25, "title": "Koenderink \u0026 van Doorn"}, {"color": "#66CCFF", "id": "Biological Cybernetics", "label": "Biological Cybernetics", "shape": "dot", "size": 25, "title": "Biological Cybernetics"}, {"color": "#66CCFF", "id": "Volumetric semantic segmentation", "label": "Volumetric semantic segmentation", "shape": "dot", "size": 25, "title": "Volumetric semantic segmentation"}, {"color": "#66CCFF", "id": "pyramid context features", "label": "pyramid context features", "shape": "dot", "size": 25, "title": "pyramid context features"}, {"color": "#66CCFF", "id": "Visual cortex", "label": "Visual cortex", "shape": "dot", "size": 25, "title": "Visual cortex"}, {"color": "#66CCFF", "id": "Hubel \u0026 Wiesel\u0027s work", "label": "Hubel \u0026 Wiesel\u0027s work", "shape": "dot", "size": 25, "title": "Hubel \u0026 Wiesel\u0027s work"}, {"color": "#66CCFF", "id": "Pedestrian parsing", "label": "Pedestrian parsing", "shape": "dot", "size": 25, "title": "Pedestrian parsing"}, {"color": "#66CCFF", "id": "shape-based methods", "label": "shape-based methods", "shape": "dot", "size": 25, "title": "shape-based methods"}, {"color": "#66CCFF", "id": "Biological cybernetics", "label": "Biological cybernetics", "shape": "dot", "size": 25, "title": "Biological cybernetics"}, {"color": "#66CCFF", "id": "visual system", "label": "visual system", "shape": "dot", "size": 25, "title": "visual system"}, {"color": "#66CCFF", "id": "Hinton", "label": "Hinton", "shape": "dot", "size": 25, "title": "Hinton"}, {"color": "#66CCFF", "id": "Universidad de los Andes", "label": "Universidad de los Andes", "shape": "dot", "size": 25, "title": "Universidad de los Andes"}, {"color": "#66CCFF", "id": "Malik", "label": "Malik", "shape": "dot", "size": 25, "title": "Malik"}, {"color": "#66CCFF", "id": "Xie", "label": "Xie", "shape": "dot", "size": 25, "title": "Xie"}, {"color": "#66CCFF", "id": "DeepShape", "label": "DeepShape", "shape": "dot", "size": 25, "title": "DeepShape"}, {"color": "#66CCFF", "id": "shape descriptor", "label": "shape descriptor", "shape": "dot", "size": 25, "title": "shape descriptor"}, {"color": "#66CCFF", "id": "3D shape matching", "label": "3D shape matching", "shape": "dot", "size": 25, "title": "3D shape matching"}, {"color": "#66CCFF", "id": "3D shape retrieval", "label": "3D shape retrieval", "shape": "dot", "size": 25, "title": "3D shape retrieval"}, {"color": "#66CCFF", "id": "3D model", "label": "3D model", "shape": "dot", "size": 25, "title": "3D model"}, {"color": "#66CCFF", "id": "3D shape matching and retrieval", "label": "3D shape matching and retrieval", "shape": "dot", "size": 25, "title": "3D shape matching and retrieval"}, {"color": "#66CCFF", "id": "shape feature learning scheme", "label": "shape feature learning scheme", "shape": "dot", "size": 25, "title": "shape feature learning scheme"}, {"color": "#66CCFF", "id": "auto-encoder", "label": "auto-encoder", "shape": "dot", "size": 25, "title": "auto-encoder"}, {"color": "#66CCFF", "id": "discriminative deep auto-encoder", "label": "discriminative deep auto-encoder", "shape": "dot", "size": 25, "title": "discriminative deep auto-encoder"}, {"color": "#66CCFF", "id": "shape distribution", "label": "shape distribution", "shape": "dot", "size": 25, "title": "shape distribution"}, {"color": "#66CCFF", "id": "Fisher discrimination criterion", "label": "Fisher discrimination criterion", "shape": "dot", "size": 25, "title": "Fisher discrimination criterion"}, {"color": "#66CCFF", "id": "neurons", "label": "neurons", "shape": "dot", "size": 25, "title": "neurons"}, {"color": "#66CCFF", "id": "multiscale shape distribution", "label": "multiscale shape distribution", "shape": "dot", "size": 25, "title": "multiscale shape distribution"}, {"color": "#66CCFF", "id": "hidden layer neurons", "label": "hidden layer neurons", "shape": "dot", "size": 25, "title": "hidden layer neurons"}, {"color": "#66CCFF", "id": "multiple discriminative auto-encoders", "label": "multiple discriminative auto-encoders", "shape": "dot", "size": 25, "title": "multiple discriminative auto-encoders"}, {"color": "#66CCFF", "id": "Mcgill dataset", "label": "Mcgill dataset", "shape": "dot", "size": 25, "title": "Mcgill dataset"}, {"color": "#66CCFF", "id": "3D models", "label": "3D models", "shape": "dot", "size": 25, "title": "3D models"}, {"color": "#66CCFF", "id": "geometric variations", "label": "geometric variations", "shape": "dot", "size": 25, "title": "geometric variations"}, {"color": "#66CCFF", "id": "Proposed Method", "label": "Proposed Method", "shape": "dot", "size": 25, "title": "Proposed Method"}, {"color": "#66CCFF", "id": "Agathos et al. (2009)", "label": "Agathos et al. (2009)", "shape": "dot", "size": 25, "title": "Agathos et al. (2009)"}, {"color": "#66CCFF", "id": "Retrieval of 3D articulated objects", "label": "Retrieval of 3D articulated objects", "shape": "dot", "size": 25, "title": "Retrieval of 3D articulated objects"}, {"color": "#66CCFF", "id": "Assfalg et al. (2007)", "label": "Assfalg et al. (2007)", "shape": "dot", "size": 25, "title": "Assfalg et al. (2007)"}, {"color": "#66CCFF", "id": "Content-based retrieval of 3D objects", "label": "Content-based retrieval of 3D objects", "shape": "dot", "size": 25, "title": "Content-based retrieval of 3D objects"}, {"color": "#66CCFF", "id": "Belongie et al. (2000)", "label": "Belongie et al. (2000)", "shape": "dot", "size": 25, "title": "Belongie et al. (2000)"}, {"color": "#66CCFF", "id": "Shape context", "label": "Shape context", "shape": "dot", "size": 25, "title": "Shape context"}, {"color": "#66CCFF", "id": "shape matching and object recognition", "label": "shape matching and object recognition", "shape": "dot", "size": 25, "title": "shape matching and object recognition"}, {"color": "#66CCFF", "id": "Geometric Feature Learning", "label": "Geometric Feature Learning", "shape": "dot", "size": 25, "title": "Geometric Feature Learning"}, {"color": "#66CCFF", "id": "Deep Auto-encoders", "label": "Deep Auto-encoders", "shape": "dot", "size": 25, "title": "Deep Auto-encoders"}, {"color": "#66CCFF", "id": "Ric variations", "label": "Ric variations", "shape": "dot", "size": 25, "title": "Ric variations"}, {"color": "#66CCFF", "id": "SHREC\u002710 Shape dataset", "label": "SHREC\u002710 Shape dataset", "shape": "dot", "size": 25, "title": "SHREC\u002710 Shape dataset"}, {"color": "#66CCFF", "id": "Shape Context", "label": "Shape Context", "shape": "dot", "size": 25, "title": "Shape Context"}, {"color": "#66CCFF", "id": "shape matching", "label": "shape matching", "shape": "dot", "size": 25, "title": "shape matching"}, {"color": "#66CCFF", "id": "Deep Architectures", "label": "Deep Architectures", "shape": "dot", "size": 25, "title": "Deep Architectures"}, {"color": "#66CCFF", "id": "AI", "label": "AI", "shape": "dot", "size": 25, "title": "AI"}, {"color": "#66CCFF", "id": "Shape Google", "label": "Shape Google", "shape": "dot", "size": 25, "title": "Shape Google"}, {"color": "#66CCFF", "id": "geometric words", "label": "geometric words", "shape": "dot", "size": 25, "title": "geometric words"}, {"color": "#66CCFF", "id": "Isometry-invariant distances", "label": "Isometry-invariant distances", "shape": "dot", "size": 25, "title": "Isometry-invariant distances"}, {"color": "#66CCFF", "id": "Bronstein et al. (2006)", "label": "Bronstein et al. (2006)", "shape": "dot", "size": 25, "title": "Bronstein et al. (2006)"}, {"color": "#66CCFF", "id": "Gromov-Hausdorff framework", "label": "Gromov-Hausdorff framework", "shape": "dot", "size": 25, "title": "Gromov-Hausdorff framework"}, {"color": "#66CCFF", "id": "non-rigid shape matching", "label": "non-rigid shape matching", "shape": "dot", "size": 25, "title": "non-rigid shape matching"}, {"color": "#66CCFF", "id": "Diffusion geometry", "label": "Diffusion geometry", "shape": "dot", "size": 25, "title": "Diffusion geometry"}, {"color": "#66CCFF", "id": "topologically-robust matching", "label": "topologically-robust matching", "shape": "dot", "size": 25, "title": "topologically-robust matching"}, {"color": "#66CCFF", "id": "Bronstein et al. (2011)", "label": "Bronstein et al. (2011)", "shape": "dot", "size": 25, "title": "Bronstein et al. (2011)"}, {"color": "#66CCFF", "id": "Mahmoudi, M.", "label": "Mahmoudi, M.", "shape": "dot", "size": 25, "title": "Mahmoudi, M."}, {"color": "#66CCFF", "id": "A Gromov-Hausdorff framework", "label": "A Gromov-Hausdorff framework", "shape": "dot", "size": 25, "title": "A Gromov-Hausdorff framework"}, {"color": "#66CCFF", "id": "Chen, D.-Y.", "label": "Chen, D.-Y.", "shape": "dot", "size": 25, "title": "Chen, D.-Y."}, {"color": "#66CCFF", "id": "On visual similarity based 3D model retrieval", "label": "On visual similarity based 3D model retrieval", "shape": "dot", "size": 25, "title": "On visual similarity based 3D model retrieval"}, {"color": "#66CCFF", "id": "3D model retrieval", "label": "3D model retrieval", "shape": "dot", "size": 25, "title": "3D model retrieval"}, {"color": "#66CCFF", "id": "Chen, X.", "label": "Chen, X.", "shape": "dot", "size": 25, "title": "Chen, X."}, {"color": "#66CCFF", "id": "A benchmark for 3D mesh segmentation", "label": "A benchmark for 3D mesh segmentation", "shape": "dot", "size": 25, "title": "A benchmark for 3D mesh segmentation"}, {"color": "#66CCFF", "id": "A benchmark", "label": "A benchmark", "shape": "dot", "size": 25, "title": "A benchmark"}, {"color": "#66CCFF", "id": "3D mesh segmentation", "label": "3D mesh segmentation", "shape": "dot", "size": 25, "title": "3D mesh segmentation"}, {"color": "#66CCFF", "id": "De Goes, F.", "label": "De Goes, F.", "shape": "dot", "size": 25, "title": "De Goes, F."}, {"color": "#66CCFF", "id": "A hierarchical segmentation", "label": "A hierarchical segmentation", "shape": "dot", "size": 25, "title": "A hierarchical segmentation"}, {"color": "#66CCFF", "id": "articulated bodies", "label": "articulated bodies", "shape": "dot", "size": 25, "title": "articulated bodies"}, {"color": "#66CCFF", "id": "Jin Xie", "label": "Jin Xie", "shape": "dot", "size": 25, "title": "Jin Xie"}, {"color": "#66CCFF", "id": "New York University Abu Dhabi", "label": "New York University Abu Dhabi", "shape": "dot", "size": 25, "title": "New York University Abu Dhabi"}, {"color": "#66CCFF", "id": "jin.xie@nyu.edu", "label": "jin.xie@nyu.edu", "shape": "dot", "size": 25, "title": "jin.xie@nyu.edu"}, {"color": "#66CCFF", "id": "Yi Fang", "label": "Yi Fang", "shape": "dot", "size": 25, "title": "Yi Fang"}, {"color": "#66CCFF", "id": "yfang@nyu.edu", "label": "yfang@nyu.edu", "shape": "dot", "size": 25, "title": "yfang@nyu.edu"}, {"color": "#66CCFF", "id": "Fan Zhu", "label": "Fan Zhu", "shape": "dot", "size": 25, "title": "Fan Zhu"}, {"color": "#66CCFF", "id": "Department of Electrical and Computer Engineering", "label": "Department of Electrical and Computer Engineering", "shape": "dot", "size": 25, "title": "Department of Electrical and Computer Engineering"}, {"color": "#66CCFF", "id": "Department of Electrical and ComputerEngineering", "label": "Department of Electrical and ComputerEngineering", "shape": "dot", "size": 25, "title": "Department of Electrical and ComputerEngineering"}, {"color": "#66CCFF", "id": "Edward Wong", "label": "Edward Wong", "shape": "dot", "size": 25, "title": "Edward Wong"}, {"color": "#66CCFF", "id": "Polytechnic School of Engineering", "label": "Polytechnic School of Engineering", "shape": "dot", "size": 25, "title": "Polytechnic School of Engineering"}, {"color": "#66CCFF", "id": "New York University", "label": "New York University", "shape": "dot", "size": 25, "title": "New York University"}, {"color": "#66CCFF", "id": "Bingbing Ni", "label": "Bingbing Ni", "shape": "dot", "size": 25, "title": "Bingbing Ni"}, {"color": "#66CCFF", "id": "Motion Part Regularization", "label": "Motion Part Regularization", "shape": "dot", "size": 25, "title": "Motion Part Regularization"}, {"color": "#66CCFF", "id": "Pierre Moulin", "label": "Pierre Moulin", "shape": "dot", "size": 25, "title": "Pierre Moulin"}, {"color": "#66CCFF", "id": "Trajectory Group Selection", "label": "Trajectory Group Selection", "shape": "dot", "size": 25, "title": "Trajectory Group Selection"}, {"color": "#66CCFF", "id": "golf", "label": "golf", "shape": "dot", "size": 25, "title": "golf"}, {"color": "#66CCFF", "id": "Action", "label": "Action", "shape": "dot", "size": 25, "title": "Action"}, {"color": "#66CCFF", "id": "punch", "label": "punch", "shape": "dot", "size": 25, "title": "punch"}, {"color": "#66CCFF", "id": "Fisher vector", "label": "Fisher vector", "shape": "dot", "size": 25, "title": "Fisher vector"}, {"color": "#66CCFF", "id": "Ni_Motion_Part_Regularization_2015_CVPR_paper", "label": "Ni_Motion_Part_Regularization_2015_CVPR_paper", "shape": "dot", "size": 25, "title": "Ni_Motion_Part_Regularization_2015_CVPR_paper"}, {"color": "#66CCFF", "id": "Motion Part Regularization framework", "label": "Motion Part Regularization framework", "shape": "dot", "size": 25, "title": "Motion Part Regularization framework"}, {"color": "#66CCFF", "id": "action recognition", "label": "action recognition", "shape": "dot", "size": 25, "title": "action recognition"}, {"color": "#66CCFF", "id": "dense trajectories", "label": "dense trajectories", "shape": "dot", "size": 25, "title": "dense trajectories"}, {"color": "#66CCFF", "id": "discriminativeness weighted Fisher vector representation", "label": "discriminativeness weighted Fisher vector representation", "shape": "dot", "size": 25, "title": "discriminativeness weighted Fisher vector representation"}, {"color": "#66CCFF", "id": "traditional Fisher vector", "label": "traditional Fisher vector", "shape": "dot", "size": 25, "title": "traditional Fisher vector"}, {"color": "#66CCFF", "id": "generating motion part candidates", "label": "generating motion part candidates", "shape": "dot", "size": 25, "title": "generating motion part candidates"}, {"color": "#66CCFF", "id": "objective function", "label": "objective function", "shape": "dot", "size": 25, "title": "objective function"}, {"color": "#66CCFF", "id": "sparse selection of trajectory groups", "label": "sparse selection of trajectory groups", "shape": "dot", "size": 25, "title": "sparse selection of trajectory groups"}, {"color": "#66CCFF", "id": "action class discriminative term", "label": "action class discriminative term", "shape": "dot", "size": 25, "title": "action class discriminative term"}, {"color": "#66CCFF", "id": "optimization algorithm", "label": "optimization algorithm", "shape": "dot", "size": 25, "title": "optimization algorithm"}, {"color": "#66CCFF", "id": "auxiliary variables", "label": "auxiliary variables", "shape": "dot", "size": 25, "title": "auxiliary variables"}, {"color": "#66CCFF", "id": "Optimization Algorithm", "label": "Optimization Algorithm", "shape": "dot", "size": 25, "title": "Optimization Algorithm"}, {"color": "#66CCFF", "id": "Motion Part", "label": "Motion Part", "shape": "dot", "size": 25, "title": "Motion Part"}, {"color": "#66CCFF", "id": "Discriminative Weights", "label": "Discriminative Weights", "shape": "dot", "size": 25, "title": "Discriminative Weights"}, {"color": "#66CCFF", "id": "State-of-the-Art Performance", "label": "State-of-the-Art Performance", "shape": "dot", "size": 25, "title": "State-of-the-Art Performance"}, {"color": "#66CCFF", "id": "Dense Trajectories", "label": "Dense Trajectories", "shape": "dot", "size": 25, "title": "Dense Trajectories"}, {"color": "#66CCFF", "id": "Wang et al. (2011)", "label": "Wang et al. (2011)", "shape": "dot", "size": 25, "title": "Wang et al. (2011)"}, {"color": "#66CCFF", "id": "Behavior Recognition", "label": "Behavior Recognition", "shape": "dot", "size": 25, "title": "Behavior Recognition"}, {"color": "#66CCFF", "id": "Spatio-Temporal Grouping", "label": "Spatio-Temporal Grouping", "shape": "dot", "size": 25, "title": "Spatio-Temporal Grouping"}, {"color": "#66CCFF", "id": "LIBSVM", "label": "LIBSVM", "shape": "dot", "size": 25, "title": "LIBSVM"}, {"color": "#66CCFF", "id": "library", "label": "library", "shape": "dot", "size": 25, "title": "library"}, {"color": "#66CCFF", "id": "Fisher Vector Representation", "label": "Fisher Vector Representation", "shape": "dot", "size": 25, "title": "Fisher Vector Representation"}, {"color": "#66CCFF", "id": "Automatic Annotation", "label": "Automatic Annotation", "shape": "dot", "size": 25, "title": "Automatic Annotation"}, {"color": "#66CCFF", "id": "Human Actions", "label": "Human Actions", "shape": "dot", "size": 25, "title": "Human Actions"}, {"color": "#66CCFF", "id": "Trajectories", "label": "Trajectories", "shape": "dot", "size": 25, "title": "Trajectories"}, {"color": "#66CCFF", "id": "O. Duchenne", "label": "O. Duchenne", "shape": "dot", "size": 25, "title": "O. Duchenne"}, {"color": "#66CCFF", "id": "Automatic annotation of human actions in video", "label": "Automatic annotation of human actions in video", "shape": "dot", "size": 25, "title": "Automatic annotation of human actions in video"}, {"color": "#66CCFF", "id": "I. Laptev", "label": "I. Laptev", "shape": "dot", "size": 25, "title": "I. Laptev"}, {"color": "#66CCFF", "id": "J. Sivic", "label": "J. Sivic", "shape": "dot", "size": 25, "title": "J. Sivic"}, {"color": "#66CCFF", "id": "J. Ponce", "label": "J. Ponce", "shape": "dot", "size": 25, "title": "J. Ponce"}, {"color": "#66CCFF", "id": "H. Wang", "label": "H. Wang", "shape": "dot", "size": 25, "title": "H. Wang"}, {"color": "#66CCFF", "id": "Action recognition with improved trajectories", "label": "Action recognition with improved trajectories", "shape": "dot", "size": 25, "title": "Action recognition with improved trajectories"}, {"color": "#66CCFF", "id": "C. Schmid", "label": "C. Schmid", "shape": "dot", "size": 25, "title": "C. Schmid"}, {"color": "#66CCFF", "id": "P. Felzenszwalb", "label": "P. Felzenszwalb", "shape": "dot", "size": 25, "title": "P. Felzenszwalb"}, {"color": "#66CCFF", "id": "Object detection with discriminatively trained part based models", "label": "Object detection with discriminatively trained part based models", "shape": "dot", "size": 25, "title": "Object detection with discriminatively trained part based models"}, {"color": "#66CCFF", "id": "R. Girshick", "label": "R. Girshick", "shape": "dot", "size": 25, "title": "R. Girshick"}, {"color": "#66CCFF", "id": "D. McAlleser", "label": "D. McAlleser", "shape": "dot", "size": 25, "title": "D. McAlleser"}, {"color": "#66CCFF", "id": "D. Ramanan", "label": "D. Ramanan", "shape": "dot", "size": 25, "title": "D. Ramanan"}, {"color": "#66CCFF", "id": "Object description with discriminatively trained part based models", "label": "Object description with discriminatively trained part based models", "shape": "dot", "size": 25, "title": "Object description with discriminatively trained part based models"}, {"color": "#66CCFF", "id": "J. Wang", "label": "J. Wang", "shape": "dot", "size": 25, "title": "J. Wang"}, {"color": "#66CCFF", "id": "Mining actionlet ensemble for action recognition with depth cameras", "label": "Mining actionlet ensemble for action recognition with depth cameras", "shape": "dot", "size": 25, "title": "Mining actionlet ensemble for action recognition with depth cameras"}, {"color": "#66CCFF", "id": "Z. Liu", "label": "Z. Liu", "shape": "dot", "size": 25, "title": "Z. Liu"}, {"color": "#66CCFF", "id": "Y. Wu", "label": "Y. Wu", "shape": "dot", "size": 25, "title": "Y. Wu"}, {"color": "#66CCFF", "id": "J. Yuan", "label": "J. Yuan", "shape": "dot", "size": 25, "title": "J. Yuan"}, {"color": "#66CCFF", "id": "M. Jain", "label": "M. Jain", "shape": "dot", "size": 25, "title": "M. Jain"}, {"color": "#66CCFF", "id": "Better exploiting motion for better action recognition", "label": "Better exploiting motion for better action recognition", "shape": "dot", "size": 25, "title": "Better exploiting motion for better action recognition"}, {"color": "#66CCFF", "id": "H. Jegou", "label": "H. Jegou", "shape": "dot", "size": 25, "title": "H. Jegou"}, {"color": "#66CCFF", "id": "P. Bouthemy", "label": "P. Bouthemy", "shape": "dot", "size": 25, "title": "P. Bouthemy"}, {"color": "#66CCFF", "id": "Y.-G. Jiang", "label": "Y.-G. Jiang", "shape": "dot", "size": 25, "title": "Y.-G. Jiang"}, {"color": "#66CCFF", "id": "Trajectory-based modeling of human actions with motion reference points", "label": "Trajectory-based modeling of human actions with motion reference points", "shape": "dot", "size": 25, "title": "Trajectory-based modeling of human actions with motion reference points"}, {"color": "#66CCFF", "id": "Q. Dai", "label": "Q. Dai", "shape": "dot", "size": 25, "title": "Q. Dai"}, {"color": "#66CCFF", "id": "X. Xue", "label": "X. Xue", "shape": "dot", "size": 25, "title": "X. Xue"}, {"color": "#66CCFF", "id": "W. Liu", "label": "W. Liu", "shape": "dot", "size": 25, "title": "W. Liu"}, {"color": "#66CCFF", "id": "C.-W. Ngo", "label": "C.-W. Ngo", "shape": "dot", "size": 25, "title": "C.-W. Ngo"}, {"color": "#66CCFF", "id": "ADSC Singapore", "label": "ADSC Singapore", "shape": "dot", "size": 25, "title": "ADSC Singapore"}, {"color": "#66CCFF", "id": "Pierre Bounameaux", "label": "Pierre Bounameaux", "shape": "dot", "size": 25, "title": "Pierre Bounameaux"}, {"color": "#66CCFF", "id": "gaze correction solutions", "label": "gaze correction solutions", "shape": "dot", "size": 25, "title": "gaze correction solutions"}, {"color": "#66CCFF", "id": "additional hardware", "label": "additional hardware", "shape": "dot", "size": 25, "title": "additional hardware"}, {"color": "#66CCFF", "id": "supervised machine learning", "label": "supervised machine learning", "shape": "dot", "size": 25, "title": "supervised machine learning"}, {"color": "#66CCFF", "id": "synthesize images", "label": "synthesize images", "shape": "dot", "size": 25, "title": "synthesize images"}, {"color": "#66CCFF", "id": "altered gaze direction", "label": "altered gaze direction", "shape": "dot", "size": 25, "title": "altered gaze direction"}, {"color": "#66CCFF", "id": "redirection of gaze", "label": "redirection of gaze", "shape": "dot", "size": 25, "title": "redirection of gaze"}, {"color": "#66CCFF", "id": "computationally efficient", "label": "computationally efficient", "shape": "dot", "size": 25, "title": "computationally efficient"}, {"color": "#66CCFF", "id": "laptop", "label": "laptop", "shape": "dot", "size": 25, "title": "laptop"}, {"color": "#66CCFF", "id": "uncanny valley effect", "label": "uncanny valley effect", "shape": "dot", "size": 25, "title": "uncanny valley effect"}, {"color": "#66CCFF", "id": "pixel replacement operations", "label": "pixel replacement operations", "shape": "dot", "size": 25, "title": "pixel replacement operations"}, {"color": "#66CCFF", "id": "eyes", "label": "eyes", "shape": "dot", "size": 25, "title": "eyes"}, {"color": "#66CCFF", "id": "system\u0027s performance", "label": "system\u0027s performance", "shape": "dot", "size": 25, "title": "system\u0027s performance"}, {"color": "#66CCFF", "id": "Monocular Gaze Correction", "label": "Monocular Gaze Correction", "shape": "dot", "size": 25, "title": "Monocular Gaze Correction"}, {"color": "#66CCFF", "id": "Uncanny Valley Effect", "label": "Uncanny Valley Effect", "shape": "dot", "size": 25, "title": "Uncanny Valley Effect"}, {"color": "#66CCFF", "id": "Localized Pixel Replacement", "label": "Localized Pixel Replacement", "shape": "dot", "size": 25, "title": "Localized Pixel Replacement"}, {"color": "#66CCFF", "id": "Amit", "label": "Amit", "shape": "dot", "size": 25, "title": "Amit"}, {"color": "#66CCFF", "id": "Shape Quantization", "label": "Shape Quantization", "shape": "dot", "size": 25, "title": "Shape Quantization"}, {"color": "#66CCFF", "id": "Doll\u00e1r", "label": "Doll\u00e1r", "shape": "dot", "size": 25, "title": "Doll\u00e1r"}, {"color": "#66CCFF", "id": "Structured Forests", "label": "Structured Forests", "shape": "dot", "size": 25, "title": "Structured Forests"}, {"color": "#66CCFF", "id": "Fast Edge Detection", "label": "Fast Edge Detection", "shape": "dot", "size": 25, "title": "Fast Edge Detection"}, {"color": "#66CCFF", "id": "Fanelli", "label": "Fanelli", "shape": "dot", "size": 25, "title": "Fanelli"}, {"color": "#66CCFF", "id": "Random Forests for 3D Face Analysis", "label": "Random Forests for 3D Face Analysis", "shape": "dot", "size": 25, "title": "Random Forests for 3D Face Analysis"}, {"color": "#66CCFF", "id": "Qualitative Evaluations", "label": "Qualitative Evaluations", "shape": "dot", "size": 25, "title": "Qualitative Evaluations"}, {"color": "#66CCFF", "id": "1841\u20131848", "label": "1841\u20131848", "shape": "dot", "size": 25, "title": "1841\u20131848"}, {"color": "#66CCFF", "id": "Fanelli, G.", "label": "Fanelli, G.", "shape": "dot", "size": 25, "title": "Fanelli, G."}, {"color": "#66CCFF", "id": "Random forests for real time 3d face analysis", "label": "Random forests for real time 3d face analysis", "shape": "dot", "size": 25, "title": "Random forests for real time 3d face analysis"}, {"color": "#66CCFF", "id": "real time 3d face analysis", "label": "real time 3d face analysis", "shape": "dot", "size": 25, "title": "real time 3d face analysis"}, {"color": "#66CCFF", "id": "Gaze correction", "label": "Gaze correction", "shape": "dot", "size": 25, "title": "Gaze correction"}, {"color": "#66CCFF", "id": "single webcam", "label": "single webcam", "shape": "dot", "size": 25, "title": "single webcam"}, {"color": "#66CCFF", "id": "Gall, J.", "label": "Gall, J.", "shape": "dot", "size": 25, "title": "Gall, J."}, {"color": "#66CCFF", "id": "Class-specific hough forests for object detection", "label": "Class-specific hough forests for object detection", "shape": "dot", "size": 25, "title": "Class-specific hough forests for object detection"}, {"color": "#66CCFF", "id": "Hough forests", "label": "Hough forests", "shape": "dot", "size": 25, "title": "Hough forests"}, {"color": "#66CCFF", "id": "Jones, A.", "label": "Jones, A.", "shape": "dot", "size": 25, "title": "Jones, A."}, {"color": "#66CCFF", "id": "Achieving eye contact in a one-to-many 3D video teleconferecing system", "label": "Achieving eye contact in a one-to-many 3D video teleconferecing system", "shape": "dot", "size": 25, "title": "Achieving eye contact in a one-to-many 3D video teleconferecing system"}, {"color": "#66CCFF", "id": "eye contact", "label": "eye contact", "shape": "dot", "size": 25, "title": "eye contact"}, {"color": "#66CCFF", "id": "3D video teleconferencing system", "label": "3D video teleconferencing system", "shape": "dot", "size": 25, "title": "3D video teleconferencing system"}, {"color": "#66CCFF", "id": "Kazemi, V.", "label": "Kazemi, V.", "shape": "dot", "size": 25, "title": "Kazemi, V."}, {"color": "#66CCFF", "id": "One milliseccond face alignment with an ensemble of regression trees", "label": "One milliseccond face alignment with an ensemble of regression trees", "shape": "dot", "size": 25, "title": "One milliseccond face alignment with an ensemble of regression trees"}, {"color": "#66CCFF", "id": "face alignment", "label": "face alignment", "shape": "dot", "size": 25, "title": "face alignment"}, {"color": "#66CCFF", "id": "One milliseccond face alignment", "label": "One milliseccond face alignment", "shape": "dot", "size": 25, "title": "One milliseccond face alignment"}, {"color": "#66CCFF", "id": "Sullivan, J.", "label": "Sullivan, J.", "shape": "dot", "size": 25, "title": "Sullivan, J."}, {"color": "#66CCFF", "id": "Kuster, C.", "label": "Kuster, C.", "shape": "dot", "size": 25, "title": "Kuster, C."}, {"color": "#66CCFF", "id": "Ren, S.", "label": "Ren, S.", "shape": "dot", "size": 25, "title": "Ren, S."}, {"color": "#66CCFF", "id": "Face alignment", "label": "Face alignment", "shape": "dot", "size": 25, "title": "Face alignment"}, {"color": "#66CCFF", "id": "Cao, X.", "label": "Cao, X.", "shape": "dot", "size": 25, "title": "Cao, X."}, {"color": "#66CCFF", "id": "Kononenko, Daniil", "label": "Kononenko, Daniil", "shape": "dot", "size": 25, "title": "Kononenko, Daniil"}, {"color": "#66CCFF", "id": "Skolkovo Institute of Science and Technology (Skoltech)", "label": "Skolkovo Institute of Science and Technology (Skoltech)", "shape": "dot", "size": 25, "title": "Skolkovo Institute of Science and Technology (Skoltech)"}, {"color": "#66CCFF", "id": "Lempitsky, Victor", "label": "Lempitsky, Victor", "shape": "dot", "size": 25, "title": "Lempitsky, Victor"}, {"color": "#66CCFF", "id": "Skolkovo Institute of Science and Technology (Skeltech)", "label": "Skolkovo Institute of Science and Technology (Skeltech)", "shape": "dot", "size": 25, "title": "Skolkovo Institute of Science and Technology (Skeltech)"}, {"color": "#66CCFF", "id": "Zhao, Kaili", "label": "Zhao, Kaili", "shape": "dot", "size": 25, "title": "Zhao, Kaili"}, {"color": "#66CCFF", "id": "Joint Patch and Multi-label Learning", "label": "Joint Patch and Multi-label Learning", "shape": "dot", "size": 25, "title": "Joint Patch and Multi-label Learning"}, {"color": "#66CCFF", "id": "Facial Action Unit Detection", "label": "Facial Action Unit Detection", "shape": "dot", "size": 25, "title": "Facial Action Unit Detection"}, {"color": "#66CCFF", "id": "Zhao_Joint_Patch_and_2015_CVPR_paper.pdf", "label": "Zhao_Joint_Patch_and_2015_CVPR_paper.pdf", "shape": "dot", "size": 25, "title": "Zhao_Joint_Patch_and_2015_CVPR_paper.pdf"}, {"color": "#66CCFF", "id": "Facial Action Coding System", "label": "Facial Action Coding System", "shape": "dot", "size": 25, "title": "Facial Action Coding System"}, {"color": "#66CCFF", "id": "describing facial movements", "label": "describing facial movements", "shape": "dot", "size": 25, "title": "describing facial movements"}, {"color": "#66CCFF", "id": "Action Units", "label": "Action Units", "shape": "dot", "size": 25, "title": "Action Units"}, {"color": "#66CCFF", "id": "JPML", "label": "JPML", "shape": "dot", "size": 25, "title": "JPML"}, {"color": "#66CCFF", "id": "Multi-label Learning", "label": "Multi-label Learning", "shape": "dot", "size": 25, "title": "Multi-label Learning"}, {"color": "#66CCFF", "id": "highest average F1 scores", "label": "highest average F1 scores", "shape": "dot", "size": 25, "title": "highest average F1 scores"}, {"color": "#66CCFF", "id": "CK+", "label": "CK+", "shape": "dot", "size": 25, "title": "CK+"}, {"color": "#66CCFF", "id": "BP4D", "label": "BP4D", "shape": "dot", "size": 25, "title": "BP4D"}, {"color": "#66CCFF", "id": "Machine learning", "label": "Machine learning", "shape": "dot", "size": 25, "title": "Machine learning"}, {"color": "#66CCFF", "id": "facial expression recognition", "label": "facial expression recognition", "shape": "dot", "size": 25, "title": "facial expression recognition"}, {"color": "#66CCFF", "id": "FACIAL Action Coding System (FACS)", "label": "FACIAL Action Coding System (FACS)", "shape": "dot", "size": 25, "title": "FACIAL Action Coding System (FACS)"}, {"color": "#66CCFF", "id": "Action Unit (AU) Detection", "label": "Action Unit (AU) Detection", "shape": "dot", "size": 25, "title": "Action Unit (AU) Detection"}, {"color": "#66CCFF", "id": "Patch Learning", "label": "Patch Learning", "shape": "dot", "size": 25, "title": "Patch Learning"}, {"color": "#66CCFF", "id": "Alternating direction method of multipliers", "label": "Alternating direction method of multipliers", "shape": "dot", "size": 25, "title": "Alternating direction method of multipliers"}, {"color": "#66CCFF", "id": "Alternating Direction Method of Multipliers", "label": "Alternating Direction Method of Multipliers", "shape": "dot", "size": 25, "title": "Alternating Direction Method of Multipliers"}, {"color": "#66CCFF", "id": "Affective Computing", "label": "Affective Computing", "shape": "dot", "size": 25, "title": "Affective Computing"}, {"color": "#66CCFF", "id": "Facial Action Unit Event Detection", "label": "Facial Action Unit Event Detection", "shape": "dot", "size": 25, "title": "Facial Action Unit Event Detection"}, {"color": "#66CCFF", "id": "human face", "label": "human face", "shape": "dot", "size": 25, "title": "human face"}, {"color": "#66CCFF", "id": "X. Ding", "label": "X. Ding", "shape": "dot", "size": 25, "title": "X. Ding"}, {"color": "#66CCFF", "id": "cascade of tasks", "label": "cascade of tasks", "shape": "dot", "size": 25, "title": "cascade of tasks"}, {"color": "#66CCFF", "id": "P. Ekman", "label": "P. Ekman", "shape": "dot", "size": 25, "title": "P. Ekman"}, {"color": "#66CCFF", "id": "facial action units", "label": "facial action units", "shape": "dot", "size": 25, "title": "facial action units"}, {"color": "#66CCFF", "id": "F. De la Torre", "label": "F. De la Torre", "shape": "dot", "size": 25, "title": "F. De la Torre"}, {"color": "#66CCFF", "id": "Intraface", "label": "Intraface", "shape": "dot", "size": 25, "title": "Intraface"}, {"color": "#66CCFF", "id": "facial action unit detection", "label": "facial action unit detection", "shape": "dot", "size": 25, "title": "facial action unit detection"}, {"color": "#66CCFF", "id": "J. C. Hager", "label": "J. C. Hager", "shape": "dot", "size": 25, "title": "J. C. Hager"}, {"color": "#66CCFF", "id": "Selective transfer machine", "label": "Selective transfer machine", "shape": "dot", "size": 25, "title": "Selective transfer machine"}, {"color": "#66CCFF", "id": "personalization in facial action unit detection", "label": "personalization in facial action unit detection", "shape": "dot", "size": 25, "title": "personalization in facial action unit detection"}, {"color": "#66CCFF", "id": "W.-S. Chu", "label": "W.-S. Chu", "shape": "dot", "size": 25, "title": "W.-S. Chu"}, {"color": "#66CCFF", "id": "J. F. Cohn", "label": "J. F. Cohn", "shape": "dot", "size": 25, "title": "J. F. Cohn"}, {"color": "#66CCFF", "id": "Facing imbalanced data", "label": "Facing imbalanced data", "shape": "dot", "size": 25, "title": "Facing imbalanced data"}, {"color": "#66CCFF", "id": "imbalanced datasets", "label": "imbalanced datasets", "shape": "dot", "size": 25, "title": "imbalanced datasets"}, {"color": "#66CCFF", "id": "L. A. Jeni", "label": "L. A. Jeni", "shape": "dot", "size": 25, "title": "L. A. Jeni"}, {"color": "#66CCFF", "id": "Data-free prior model", "label": "Data-free prior model", "shape": "dot", "size": 25, "title": "Data-free prior model"}, {"color": "#66CCFF", "id": "data-free approach", "label": "data-free approach", "shape": "dot", "size": 25, "title": "data-free approach"}, {"color": "#66CCFF", "id": "Y. Li", "label": "Y. Li", "shape": "dot", "size": 25, "title": "Y. Li"}, {"color": "#66CCFF", "id": "Y. Zhao", "label": "Y. Zhao", "shape": "dot", "size": 25, "title": "Y. Zhao"}, {"color": "#66CCFF", "id": "School of Comm. and Info. Engineering", "label": "School of Comm. and Info. Engineering", "shape": "dot", "size": 25, "title": "School of Comm. and Info. Engineering"}, {"color": "#66CCFF", "id": "Beijing University of Posts and Telecom.", "label": "Beijing University of Posts and Telecom.", "shape": "dot", "size": 25, "title": "Beijing University of Posts and Telecom."}, {"color": "#66CCFF", "id": "Beijing", "label": "Beijing", "shape": "dot", "size": 25, "title": "Beijing"}, {"color": "#66CCFF", "id": "facial action unit recognition", "label": "facial action unit recognition", "shape": "dot", "size": 25, "title": "facial action unit recognition"}, {"color": "#66CCFF", "id": "G. Littlewort", "label": "G. Littlewort", "shape": "dot", "size": 25, "title": "G. Littlewort"}, {"color": "#66CCFF", "id": "Dynamics of facial expression", "label": "Dynamics of facial expression", "shape": "dot", "size": 25, "title": "Dynamics of facial expression"}, {"color": "#66CCFF", "id": "AU-cascades", "label": "AU-cascades", "shape": "dot", "size": 25, "title": "AU-cascades"}, {"color": "#66CCFF", "id": "action unit detection", "label": "action unit detection", "shape": "dot", "size": 25, "title": "action unit detection"}, {"color": "#66CCFF", "id": "Wen-Sheng Chu", "label": "Wen-Sheng Chu", "shape": "dot", "size": 25, "title": "Wen-Sheng Chu"}, {"color": "#66CCFF", "id": "Robotics Institute", "label": "Robotics Institute", "shape": "dot", "size": 25, "title": "Robotics Institute"}, {"color": "#66CCFF", "id": "Fernando De la Torre", "label": "Fernando De la Torre", "shape": "dot", "size": 25, "title": "Fernando De la Torre"}, {"color": "#66CCFF", "id": "Jeffrey F. Cohn", "label": "Jeffrey F. Cohn", "shape": "dot", "size": 25, "title": "Jeffrey F. Cohn"}, {"color": "#66CCFF", "id": "Robotic Institute", "label": "Robotic Institute", "shape": "dot", "size": 25, "title": "Robotic Institute"}, {"color": "#66CCFF", "id": "Pittsburgh", "label": "Pittsburgh", "shape": "dot", "size": 25, "title": "Pittsburgh"}, {"color": "#66CCFF", "id": "Honggang Zhang", "label": "Honggang Zhang", "shape": "dot", "size": 25, "title": "Honggang Zhang"}, {"color": "#66CCFF", "id": "Beijing University of Posts and Telecom", "label": "Beijing University of Posts and Telecom", "shape": "dot", "size": 25, "title": "Beijing University of Posts and Telecom"}, {"color": "#66CCFF", "id": "TVSum", "label": "TVSum", "shape": "dot", "size": 25, "title": "TVSum"}, {"color": "#66CCFF", "id": "Web Videos", "label": "Web Videos", "shape": "dot", "size": 25, "title": "Web Videos"}, {"color": "#66CCFF", "id": "Titles", "label": "Titles", "shape": "dot", "size": 25, "title": "Titles"}, {"color": "#66CCFF", "id": "Yale Song", "label": "Yale Song", "shape": "dot", "size": 25, "title": "Yale Song"}, {"color": "#66CCFF", "id": "Jordi Vallmitjana", "label": "Jordi Vallmitjana", "shape": "dot", "size": 25, "title": "Jordi Vallmitjana"}, {"color": "#66CCFF", "id": "Amanda Stent", "label": "Amanda Stent", "shape": "dot", "size": 25, "title": "Amanda Stent"}, {"color": "#66CCFF", "id": "Alejandro Jaimes", "label": "Alejandro Jaimes", "shape": "dot", "size": 25, "title": "Alejandro Jaimes"}, {"color": "#66CCFF", "id": "Song_TVSum_Summarizing_Web_2015_CVPR_paper.pdf", "label": "Song_TVSum_Summarizing_Web_2015_CVPR_paper.pdf", "shape": "dot", "size": 25, "title": "Song_TVSum_Summarizing_Web_2015_CVPR_paper.pdf"}, {"color": "#66CCFF", "id": "Video summarization", "label": "Video summarization", "shape": "dot", "size": 25, "title": "Video summarization"}, {"color": "#66CCFF", "id": "need for prior knowledge", "label": "need for prior knowledge", "shape": "dot", "size": 25, "title": "need for prior knowledge"}, {"color": "#66CCFF", "id": "video summarization framework", "label": "video summarization framework", "shape": "dot", "size": 25, "title": "video summarization framework"}, {"color": "#66CCFF", "id": "title-based image search results", "label": "title-based image search results", "shape": "dot", "size": 25, "title": "title-based image search results"}, {"color": "#66CCFF", "id": "video titles", "label": "video titles", "shape": "dot", "size": 25, "title": "video titles"}, {"color": "#66CCFF", "id": "descriptive", "label": "descriptive", "shape": "dot", "size": 25, "title": "descriptive"}, {"color": "#66CCFF", "id": "co-archetypal analysis", "label": "co-archetypal analysis", "shape": "dot", "size": 25, "title": "co-archetypal analysis"}, {"color": "#66CCFF", "id": "novel technique", "label": "novel technique", "shape": "dot", "size": 25, "title": "novel technique"}, {"color": "#66CCFF", "id": "visual concepts", "label": "visual concepts", "shape": "dot", "size": 25, "title": "visual concepts"}, {"color": "#66CCFF", "id": "video and images", "label": "video and images", "shape": "dot", "size": 25, "title": "video and images"}, {"color": "#66CCFF", "id": "TVSum50", "label": "TVSum50", "shape": "dot", "size": 25, "title": "TVSum50"}, {"color": "#66CCFF", "id": "benchmark dataset", "label": "benchmark dataset", "shape": "dot", "size": 25, "title": "benchmark dataset"}, {"color": "#66CCFF", "id": "superior quality summaries", "label": "superior quality summaries", "shape": "dot", "size": 25, "title": "superior quality summaries"}, {"color": "#66CCFF", "id": "image search results", "label": "image search results", "shape": "dot", "size": 25, "title": "image search results"}, {"color": "#66CCFF", "id": "noise and variance", "label": "noise and variance", "shape": "dot", "size": 25, "title": "noise and variance"}, {"color": "#66CCFF", "id": "summaries", "label": "summaries", "shape": "dot", "size": 25, "title": "summaries"}, {"color": "#66CCFF", "id": "superior", "label": "superior", "shape": "dot", "size": 25, "title": "superior"}, {"color": "#66CCFF", "id": "Co-Archetypal Analysis", "label": "Co-Archetypal Analysis", "shape": "dot", "size": 25, "title": "Co-Archetypal Analysis"}, {"color": "#66CCFF", "id": "analysis method", "label": "analysis method", "shape": "dot", "size": 25, "title": "analysis method"}, {"color": "#66CCFF", "id": "Canonical Visual Concepts", "label": "Canonical Visual Concepts", "shape": "dot", "size": 25, "title": "Canonical Visual Concepts"}, {"color": "#66CCFF", "id": "concept", "label": "concept", "shape": "dot", "size": 25, "title": "concept"}, {"color": "#66CCFF", "id": "M. Basseville", "label": "M. Basseville", "shape": "dot", "size": 25, "title": "M. Basseville"}, {"color": "#66CCFF", "id": "Detection of abrupt changes", "label": "Detection of abrupt changes", "shape": "dot", "size": 25, "title": "Detection of abrupt changes"}, {"color": "#66CCFF", "id": "A. Beck", "label": "A. Beck", "shape": "dot", "size": 25, "title": "A. Beck"}, {"color": "#66CCFF", "id": "shrinkage-thresholding algorithm", "label": "shrinkage-thresholding algorithm", "shape": "dot", "size": 25, "title": "shrinkage-thresholding algorithm"}, {"color": "#66CCFF", "id": "K. Bleakley", "label": "K. Bleakley", "shape": "dot", "size": 25, "title": "K. Bleakley"}, {"color": "#66CCFF", "id": "group fused lasso", "label": "group fused lasso", "shape": "dot", "size": 25, "title": "group fused lasso"}, {"color": "#66CCFF", "id": "Y. Chen", "label": "Y. Chen", "shape": "dot", "size": 25, "title": "Y. Chen"}, {"color": "#66CCFF", "id": "archetypal analysis", "label": "archetypal analysis", "shape": "dot", "size": 25, "title": "archetypal analysis"}, {"color": "#66CCFF", "id": "S. Fidler", "label": "S. Fidler", "shape": "dot", "size": 25, "title": "S. Fidler"}, {"color": "#66CCFF", "id": "sentence is worth a thousand pixels", "label": "sentence is worth a thousand pixels", "shape": "dot", "size": 25, "title": "sentence is worth a thousand pixels"}, {"color": "#66CCFF", "id": "Airal", "label": "Airal", "shape": "dot", "size": 25, "title": "Airal"}, {"color": "#66CCFF", "id": "Fast and robust archetypal analysis", "label": "Fast and robust archetypal analysis", "shape": "dot", "size": 25, "title": "Fast and robust archetypal analysis"}, {"color": "#66CCFF", "id": "A sentence is worth a thousand pixels", "label": "A sentence is worth a thousand pixels", "shape": "dot", "size": 25, "title": "A sentence is worth a thousand pixels"}, {"color": "#66CCFF", "id": "M. Gygli", "label": "M. Gygli", "shape": "dot", "size": 25, "title": "M. Gygli"}, {"color": "#66CCFF", "id": "Creating summaries from user videos", "label": "Creating summaries from user videos", "shape": "dot", "size": 25, "title": "Creating summaries from user videos"}, {"color": "#66CCFF", "id": "Y. Jia", "label": "Y. Jia", "shape": "dot", "size": 25, "title": "Y. Jia"}, {"color": "#66CCFF", "id": "Visual concept learning", "label": "Visual concept learning", "shape": "dot", "size": 25, "title": "Visual concept learning"}, {"color": "#66CCFF", "id": "Y. J. Lee", "label": "Y. J. Lee", "shape": "dot", "size": 25, "title": "Y. J. Lee"}, {"color": "#66CCFF", "id": "Discovering important people and objects", "label": "Discovering important people and objects", "shape": "dot", "size": 25, "title": "Discovering important people and objects"}, {"color": "#66CCFF", "id": "L. Li", "label": "L. Li", "shape": "dot", "size": 25, "title": "L. Li"}, {"color": "#66CCFF", "id": "Video summarization via transferrable structured learning", "label": "Video summarization via transferrable structured learning", "shape": "dot", "size": 25, "title": "Video summarization via transferrable structured learning"}, {"color": "#66CCFF", "id": "WWW", "label": "WWW", "shape": "dot", "size": 25, "title": "WWW"}, {"color": "#66CCFF", "id": "D. Lin", "label": "D. Lin", "shape": "dot", "size": 25, "title": "D. Lin"}, {"color": "#66CCFF", "id": "Visual semantic search", "label": "Visual semantic search", "shape": "dot", "size": 25, "title": "Visual semantic search"}, {"color": "#66CCFF", "id": "Yahoo Labs", "label": "Yahoo Labs", "shape": "dot", "size": 25, "title": "Yahoo Labs"}, {"color": "#66CCFF", "id": "Retrieving videos", "label": "Retrieving videos", "shape": "dot", "size": 25, "title": "Retrieving videos"}, {"color": "#66CCFF", "id": "complex textual queries", "label": "complex textual queries", "shape": "dot", "size": 25, "title": "complex textual queries"}, {"color": "#66CCFF", "id": "yalessong@yahoo-inc.com", "label": "yalessong@yahoo-inc.com", "shape": "dot", "size": 25, "title": "yalessong@yahoo-inc.com"}, {"color": "#66CCFF", "id": "research institution", "label": "research institution", "shape": "dot", "size": 25, "title": "research institution"}, {"color": "#66CCFF", "id": "isual semantic search", "label": "isual semantic search", "shape": "dot", "size": 25, "title": "isual semantic search"}, {"color": "#66CCFF", "id": "video retrieval", "label": "video retrieval", "shape": "dot", "size": 25, "title": "video retrieval"}, {"color": "#66CCFF", "id": "Tianjun Xiao", "label": "Tianjun Xiao", "shape": "dot", "size": 25, "title": "Tianjun Xiao"}, {"color": "#66CCFF", "id": "The Application of Two-level Attention Models", "label": "The Application of Two-level Attention Models", "shape": "dot", "size": 25, "title": "The Application of Two-level Attention Models"}, {"color": "#66CCFF", "id": "Yichong Xu", "label": "Yichong Xu", "shape": "dot", "size": 25, "title": "Yichong Xu"}, {"color": "#66CCFF", "id": "Kuiyuan Yang", "label": "Kuiyuan Yang", "shape": "dot", "size": 25, "title": "Kuiyuan Yang"}, {"color": "#66CCFF", "id": "Jiaxing Zhang", "label": "Jiaxing Zhang", "shape": "dot", "size": 25, "title": "Jiaxing Zhang"}, {"color": "#66CCFF", "id": "Yuxin Peng", "label": "Yuxin Peng", "shape": "dot", "size": 25, "title": "Yuxin Peng"}, {"color": "#66CCFF", "id": "The Application of Two-level Attack Models", "label": "The Application of Two-level Attack Models", "shape": "dot", "size": 25, "title": "The Application of Two-level Attack Models"}, {"color": "#66CCFF", "id": "Zheng Zhang", "label": "Zheng Zhang", "shape": "dot", "size": 25, "title": "Zheng Zhang"}, {"color": "#66CCFF", "id": "Deep Convolutional Neural Network", "label": "Deep Convolutional Neural Network", "shape": "dot", "size": 25, "title": "Deep Convolutional Neural Network"}, {"color": "#66CCFF", "id": "Fine-grained classification", "label": "Fine-grained classification", "shape": "dot", "size": 25, "title": "Fine-grained classification"}, {"color": "#66CCFF", "id": "subtle differences between categories", "label": "subtle differences between categories", "shape": "dot", "size": 25, "title": "subtle differences between categories"}, {"color": "#66CCFF", "id": "visual attention", "label": "visual attention", "shape": "dot", "size": 25, "title": "visual attention"}, {"color": "#66CCFF", "id": "deep neural networks", "label": "deep neural networks", "shape": "dot", "size": 25, "title": "deep neural networks"}, {"color": "#66CCFF", "id": "pipeline", "label": "pipeline", "shape": "dot", "size": 25, "title": "pipeline"}, {"color": "#66CCFF", "id": "bottom-up attention", "label": "bottom-up attention", "shape": "dot", "size": 25, "title": "bottom-up attention"}, {"color": "#66CCFF", "id": "object-level top-down attention", "label": "object-level top-down attention", "shape": "dot", "size": 25, "title": "object-level top-down attention"}, {"color": "#66CCFF", "id": "part-level top-down attention", "label": "part-level top-down attention", "shape": "dot", "size": 25, "title": "part-level top-down attention"}, {"color": "#66CCFF", "id": "expensive annotations", "label": "expensive annotations", "shape": "dot", "size": 25, "title": "expensive annotations"}, {"color": "#66CCFF", "id": "significant improvements", "label": "significant improvements", "shape": "dot", "size": 25, "title": "significant improvements"}, {"color": "#66CCFF", "id": "competitive performance", "label": "competitive performance", "shape": "dot", "size": 25, "title": "competitive performance"}, {"color": "#66CCFF", "id": "additional annotations", "label": "additional annotations", "shape": "dot", "size": 25, "title": "additional annotations"}, {"color": "#66CCFF", "id": "Fine-grained image classification", "label": "Fine-grained image classification", "shape": "dot", "size": 25, "title": "Fine-grained image classification"}, {"color": "#66CCFF", "id": "Visual attention models", "label": "Visual attention models", "shape": "dot", "size": 25, "title": "Visual attention models"}, {"color": "#66CCFF", "id": "Deep convolutional neural networks", "label": "Deep convolutional neural networks", "shape": "dot", "size": 25, "title": "Deep convolutional neural networks"}, {"color": "#66CCFF", "id": "Weak supervision", "label": "Weak supervision", "shape": "dot", "size": 25, "title": "Weak supervision"}, {"color": "#66CCFF", "id": "Institute of Computer Science and Technologies", "label": "Institute of Computer Science and Technologies", "shape": "dot", "size": 25, "title": "Institute of Computer Science and Technologies"}, {"color": "#66CCFF", "id": "Institute of Computer Science and Technology", "label": "Institute of Computer Science and Technology", "shape": "dot", "size": 25, "title": "Institute of Computer Science and Technology"}, {"color": "#66CCFF", "id": "New York University Shanghai", "label": "New York University Shanghai", "shape": "dot", "size": 25, "title": "New York University Shanghai"}, {"color": "#66CCFF", "id": "Geodesic Exponential Kernel", "label": "Geodesic Exponential Kernel", "shape": "dot", "size": 25, "title": "Geodesic Exponential Kernel"}, {"color": "#66CCFF", "id": "curvature and linearity conflict", "label": "curvature and linearity conflict", "shape": "dot", "size": 25, "title": "curvature and linearity conflict"}, {"color": "#66CCFF", "id": "Aasa Feragen", "label": "Aasa Feragen", "shape": "dot", "size": 25, "title": "Aasa Feragen"}, {"color": "#66CCFF", "id": "Fran\u00e7ois Lauze", "label": "Fran\u00e7ois Lauze", "shape": "dot", "size": 25, "title": "Fran\u00e7ois Lauze"}, {"color": "#66CCFF", "id": "S\u00f8ren Hauberg", "label": "S\u00f8ren Hauberg", "shape": "dot", "size": 25, "title": "S\u00f8ren Hauberg"}, {"color": "#66CCFF", "id": "geodesic metric spaces", "label": "geodesic metric spaces", "shape": "dot", "size": 25, "title": "geodesic metric spaces"}, {"color": "#66CCFF", "id": "Gaussian kernel", "label": "Gaussian kernel", "shape": "dot", "size": 25, "title": "Gaussian kernel"}, {"color": "#66CCFF", "id": "positive definite kernel", "label": "positive definite kernel", "shape": "dot", "size": 25, "title": "positive definite kernel"}, {"color": "#66CCFF", "id": "flat space", "label": "flat space", "shape": "dot", "size": 25, "title": "flat space"}, {"color": "#66CCFF", "id": "geodesic Gaussian kernel", "label": "geodesic Gaussian kernel", "shape": "dot", "size": 25, "title": "geodesic Gaussian kernel"}, {"color": "#66CCFF", "id": "Riemannian manifold is Euclidean", "label": "Riemannian manifold is Euclidean", "shape": "dot", "size": 25, "title": "Riemannian manifold is Euclidean"}, {"color": "#66CCFF", "id": "geodesic Laplacian kernel", "label": "geodesic Laplacian kernel", "shape": "dot", "size": 25, "title": "geodesic Laplacian kernel"}, {"color": "#66CCFF", "id": "positive de\ufb01niteness", "label": "positive de\ufb01niteness", "shape": "dot", "size": 25, "title": "positive de\ufb01niteness"}, {"color": "#66CCFF", "id": "curved spaces", "label": "curved spaces", "shape": "dot", "size": 25, "title": "curved spaces"}, {"color": "#66CCFF", "id": "spheres", "label": "spheres", "shape": "dot", "size": 25, "title": "spheres"}, {"color": "#66CCFF", "id": "spaces", "label": "spaces", "shape": "dot", "size": 25, "title": "spaces"}, {"color": "#66CCFF", "id": "conditionally negative de\ufb01nite distances", "label": "conditionally negative de\ufb01nite distances", "shape": "dot", "size": 25, "title": "conditionally negative de\ufb01nite distances"}, {"color": "#66CCFF", "id": "Feragen", "label": "Feragen", "shape": "dot", "size": 25, "title": "Feragen"}, {"color": "#66CCFF", "id": "hyperbolic spaces", "label": "hyperbolic spaces", "shape": "dot", "size": 25, "title": "hyperbolic spaces"}, {"color": "#66CCFF", "id": "theoretical results", "label": "theoretical results", "shape": "dot", "size": 25, "title": "theoretical results"}, {"color": "#66CCFF", "id": "empirically", "label": "empirically", "shape": "dot", "size": 25, "title": "empirically"}, {"color": "#66CCFF", "id": "kernel", "label": "kernel", "shape": "dot", "size": 25, "title": "kernel"}, {"color": "#66CCFF", "id": "geodesic Laplacian kernels", "label": "geodesic Laplacian kernels", "shape": "dot", "size": 25, "title": "geodesic Laplacian kernels"}, {"color": "#66CCFF", "id": "Gaussian kernels", "label": "Gaussian kernels", "shape": "dot", "size": 25, "title": "Gaussian kernels"}, {"color": "#66CCFF", "id": "Laplacian kernels", "label": "Laplacian kernels", "shape": "dot", "size": 25, "title": "Laplacian kernels"}, {"color": "#66CCFF", "id": "M. Alamgir and U. von Luxburg", "label": "M. Alamgir and U. von Luxburg", "shape": "dot", "size": 25, "title": "M. Alamgir and U. von Luxburg"}, {"color": "#66CCFF", "id": "Shortest path distance in random k-nearest neighbor graphs", "label": "Shortest path distance in random k-nearest neighbor graphs", "shape": "dot", "size": 25, "title": "Shortest path distance in random k-nearest neighbor graphs"}, {"color": "#66CCFF", "id": "N. Dalal and B. Triggs", "label": "N. Dalal and B. Triggs", "shape": "dot", "size": 25, "title": "N. Dalal and B. Triggs"}, {"color": "#66CCFF", "id": "Histograms of oriented gradients for human detection", "label": "Histograms of oriented gradients for human detection", "shape": "dot", "size": 25, "title": "Histograms of oriented gradients for human detection"}, {"color": "#66CCFF", "id": "S. Amar\u00ed and H. Nagaoka", "label": "S. Amar\u00ed and H. Nagaoka", "shape": "dot", "size": 25, "title": "S. Amar\u00ed and H. Nagaoka"}, {"color": "#66CCFF", "id": "Methods of information geometry", "label": "Methods of information geometry", "shape": "dot", "size": 25, "title": "Methods of information geometry"}, {"color": "#66CCFF", "id": "Arsigny et al.", "label": "Arsigny et al.", "shape": "dot", "size": 25, "title": "Arsigny et al."}, {"color": "#66CCFF", "id": "Fast and simple calculus on tensors", "label": "Fast and simple calculus on tensors", "shape": "dot", "size": 25, "title": "Fast and simple calculus on tensors"}, {"color": "#66CCFF", "id": "MICCAI", "label": "MICCAI", "shape": "dot", "size": 25, "title": "MICCAI"}, {"color": "#66CCFF", "id": "Feragen et al.", "label": "Feragen et al.", "shape": "dot", "size": 25, "title": "Feragen et al."}, {"color": "#66CCFF", "id": "Means in spaces of tree-like shapes", "label": "Means in spaces of tree-like shapes", "shape": "dot", "size": 25, "title": "Means in spaces of tree-like shapes"}, {"color": "#66CCFF", "id": "Scalable kernels for graphs", "label": "Scalable kernels for graphs", "shape": "dot", "size": 25, "title": "Scalable kernels for graphs"}, {"color": "#66CCFF", "id": "Bekka and de la Harple", "label": "Bekka and de la Harple", "shape": "dot", "size": 25, "title": "Bekka and de la Harple"}, {"color": "#66CCFF", "id": "Kazhdan\u2019s Property (T)", "label": "Kazhdan\u2019s Property (T)", "shape": "dot", "size": 25, "title": "Kazhdan\u2019s Property (T)"}, {"color": "#66CCFF", "id": "Bridson and Hae\ufb02iger", "label": "Bridson and Hae\ufb02iger", "shape": "dot", "size": 25, "title": "Bridson and Hae\ufb02iger"}, {"color": "#66CCFF", "id": "Metric spaces of non-positive curvature", "label": "Metric spaces of non-positive curvature", "shape": "dot", "size": 25, "title": "Metric spaces of non-positive curvature"}, {"color": "#66CCFF", "id": "mathematical_property", "label": "mathematical_property", "shape": "dot", "size": 25, "title": "mathematical_property"}, {"color": "#66CCFF", "id": "New Mathematical Monographs", "label": "New Mathematical Monographs", "shape": "dot", "size": 25, "title": "New Mathematical Monographs"}, {"color": "#66CCFF", "id": "Ganzhao Yuan", "label": "Ganzhao Yuan", "shape": "dot", "size": 25, "title": "Ganzhao Yuan"}, {"color": "#66CCFF", "id": "\u21130TV: A New Method", "label": "\u21130TV: A New Method", "shape": "dot", "size": 25, "title": "\u21130TV: A New Method"}, {"color": "#66CCFF", "id": "Image Restoration", "label": "Image Restoration", "shape": "dot", "size": 25, "title": "Image Restoration"}, {"color": "#66CCFF", "id": "DIKU, University of Copenhagen", "label": "DIKU, University of Copenhagen", "shape": "dot", "size": 25, "title": "DIKU, University of Copenhagen"}, {"color": "#66CCFF", "id": "DTU Compute", "label": "DTU Compute", "shape": "dot", "size": 25, "title": "DTU Compute"}, {"color": "#66CCFF", "id": "Yuan_L0TV_A_New_2015_CVPR_paper", "label": "Yuan_L0TV_A_New_2015_CVPR_paper", "shape": "dot", "size": 25, "title": "Yuan_L0TV_A_New_2015_CVPR_paper"}, {"color": "#66CCFF", "id": "image restoration", "label": "image restoration", "shape": "dot", "size": 25, "title": "image restoration"}, {"color": "#66CCFF", "id": "\u21130TV-PADMM", "label": "\u21130TV-PADMM", "shape": "dot", "size": 25, "title": "\u21130TV-PADMM"}, {"color": "#66CCFF", "id": "TV-based restoration problem", "label": "TV-based restoration problem", "shape": "dot", "size": 25, "title": "TV-based restoration problem"}, {"color": "#66CCFF", "id": "\u21130-norm data fidelity", "label": "\u21130-norm data fidelity", "shape": "dot", "size": 25, "title": "\u21130-norm data fidelity"}, {"color": "#66CCFF", "id": "MPEC", "label": "MPEC", "shape": "dot", "size": 25, "title": "MPEC"}, {"color": "#66CCFF", "id": "PADMM", "label": "PADMM", "shape": "dot", "size": 25, "title": "PADMM"}, {"color": "#66CCFF", "id": "state-of-the-art image restoration methods", "label": "state-of-the-art image restoration methods", "shape": "dot", "size": 25, "title": "state-of-the-art image restoration methods"}, {"color": "#66CCFF", "id": "Impulse Noise", "label": "Impulse Noise", "shape": "dot", "size": 25, "title": "Impulse Noise"}, {"color": "#66CCFF", "id": "Total Variation", "label": "Total Variation", "shape": "dot", "size": 25, "title": "Total Variation"}, {"color": "#66CCFF", "id": "South China University of Technology (SCUT)", "label": "South China University of Technology (SCUT)", "shape": "dot", "size": 25, "title": "South China University of Technology (SCUT)"}, {"color": "#66CCFF", "id": "yuan Ganzhao@gmail.com", "label": "yuan Ganzhao@gmail.com", "shape": "dot", "size": 25, "title": "yuan Ganzhao@gmail.com"}, {"color": "#66CCFF", "id": "King Abdullah University of Science and Technology (KAUST)", "label": "King Abdullah University of Science and Technology (KAUST)", "shape": "dot", "size": 25, "title": "King Abdullah University of Science and Technology (KAUST)"}, {"color": "#66CCFF", "id": "bernard.ghanem@kust.edu.sa", "label": "bernard.ghanem@kust.edu.sa", "shape": "dot", "size": 25, "title": "bernard.ghanem@kust.edu.sa"}, {"color": "#66CCFF", "id": "Ejaz Ahmed", "label": "Ejaz Ahmed", "shape": "dot", "size": 25, "title": "Ejaz Ahmed"}, {"color": "#66CCFF", "id": "An Improved Deep Learning Architecture", "label": "An Improved Deep Learning Architecture", "shape": "dot", "size": 25, "title": "An Improved Deep Learning Architecture"}, {"color": "#66CCFF", "id": "Michael Jones", "label": "Michael Jones", "shape": "dot", "size": 25, "title": "Michael Jones"}, {"color": "#66CCFF", "id": "Tim K. Marks", "label": "Tim K. Marks", "shape": "dot", "size": 25, "title": "Tim K. Marks"}, {"color": "#66CCFF", "id": "learning features", "label": "learning features", "shape": "dot", "size": 25, "title": "learning features"}, {"color": "#66CCFF", "id": "learning similarity metric", "label": "learning similarity metric", "shape": "dot", "size": 25, "title": "learning similarity metric"}, {"color": "#66CCFF", "id": "similarity value", "label": "similarity value", "shape": "dot", "size": 25, "title": "similarity value"}, {"color": "#66CCFF", "id": "same person", "label": "same person", "shape": "dot", "size": 25, "title": "same person"}, {"color": "#66CCFF", "id": "layer", "label": "layer", "shape": "dot", "size": 25, "title": "layer"}, {"color": "#66CCFF", "id": "cross-input neighborhood differences", "label": "cross-input neighborhood differences", "shape": "dot", "size": 25, "title": "cross-input neighborhood differences"}, {"color": "#66CCFF", "id": "local relationships", "label": "local relationships", "shape": "dot", "size": 25, "title": "local relationships"}, {"color": "#66CCFF", "id": "mid-level features", "label": "mid-level features", "shape": "dot", "size": 25, "title": "mid-level features"}, {"color": "#66CCFF", "id": "patch summary features", "label": "patch summary features", "shape": "dot", "size": 25, "title": "patch summary features"}, {"color": "#66CCFF", "id": "layer of patch summary features", "label": "layer of patch summary features", "shape": "dot", "size": 25, "title": "layer of patch summary features"}, {"color": "#66CCFF", "id": "high-level summary", "label": "high-level summary", "shape": "dot", "size": 25, "title": "high-level summary"}, {"color": "#66CCFF", "id": "over-fitting", "label": "over-fitting", "shape": "dot", "size": 25, "title": "over-fitting"}, {"color": "#66CCFF", "id": "small target data set", "label": "small target data set", "shape": "dot", "size": 25, "title": "small target data set"}, {"color": "#66CCFF", "id": "CUHK03", "label": "CUHK03", "shape": "dot", "size": 25, "title": "CUHK03"}, {"color": "#66CCFF", "id": "large data set", "label": "large data set", "shape": "dot", "size": 25, "title": "large data set"}, {"color": "#66CCFF", "id": "CUHK01", "label": "CUHK01", "shape": "dot", "size": 25, "title": "CUHK01"}, {"color": "#66CCFF", "id": "medium-sized data set", "label": "medium-sized data set", "shape": "dot", "size": 25, "title": "medium-sized data set"}, {"color": "#66CCFF", "id": "VIPeR", "label": "VIPeR", "shape": "dot", "size": 25, "title": "VIPeR"}, {"color": "#66CCFF", "id": "small data set", "label": "small data set", "shape": "dot", "size": 25, "title": "small data set"}, {"color": "#66CCFF", "id": "initial training", "label": "initial training", "shape": "dot", "size": 25, "title": "initial training"}, {"color": "#66CCFF", "id": "fine-tuning", "label": "fine-tuning", "shape": "dot", "size": 25, "title": "fine-tuning"}, {"color": "#66CCFF", "id": "Deep Convolutional Architecture", "label": "Deep Convolutional Architecture", "shape": "dot", "size": 25, "title": "Deep Convolutional Architecture"}, {"color": "#66CCFF", "id": "Similarity Metric Learning", "label": "Similarity Metric Learning", "shape": "dot", "size": 25, "title": "Similarity Metric Learning"}, {"color": "#66CCFF", "id": "Neighborhood Difference Layer", "label": "Neighborhood Difference Layer", "shape": "dot", "size": 25, "title": "Neighborhood Difference Layer"}, {"color": "#66CCFF", "id": "Metric Learning", "label": "Metric Learning", "shape": "dot", "size": 25, "title": "Metric Learning"}, {"color": "#66CCFF", "id": "[Li, W., \u0026 Wang, X. (2013)]", "label": "[Li, W., \u0026 Wang, X. (2013)]", "shape": "dot", "size": 25, "title": "[Li, W., \u0026 Wang, X. (2013)]"}, {"color": "#66CCFF", "id": "person re-identi\ufb01cation", "label": "person re-identi\ufb01cation", "shape": "dot", "size": 25, "title": "person re-identi\ufb01cation"}, {"color": "#66CCFF", "id": "Li, Z.", "label": "Li, Z.", "shape": "dot", "size": 25, "title": "Li, Z."}, {"color": "#66CCFF", "id": "Learning locally-adaptive decision functions", "label": "Learning locally-adaptive decision functions", "shape": "dot", "size": 25, "title": "Learning locally-adaptive decision functions"}, {"color": "#66CCFF", "id": "Bazzani, L.", "label": "Bazzani, L.", "shape": "dot", "size": 25, "title": "Bazzani, L."}, {"color": "#66CCFF", "id": "Multiple-shot person re-identi\ufb01cation", "label": "Multiple-shot person re-identi\ufb01cation", "shape": "dot", "size": 25, "title": "Multiple-shot person re-identi\ufb01cation"}, {"color": "#66CCFF", "id": "Bottou, L.", "label": "Bottou, L.", "shape": "dot", "size": 25, "title": "Bottou, L."}, {"color": "#66CCFF", "id": "Stochastic gradient tricks", "label": "Stochastic gradient tricks", "shape": "dot", "size": 25, "title": "Stochastic gradient tricks"}, {"color": "#66CCFF", "id": "Davis, J. V.", "label": "Davis, J. V.", "shape": "dot", "size": 25, "title": "Davis, J. V."}, {"color": "#66CCFF", "id": "Information-theoretic metric learning", "label": "Information-theoretic metric learning", "shape": "dot", "size": 25, "title": "Information-theoretic metric learning"}, {"color": "#66CCFF", "id": "Farenzena, M.", "label": "Farenzena, M.", "shape": "dot", "size": 25, "title": "Farenzena, M."}, {"color": "#66CCFF", "id": "Person re-identi\ufb01cation by symmetry-driven accumulation", "label": "Person re-identi\ufb01cation by symmetry-driven accumulation", "shape": "dot", "size": 25, "title": "Person re-identi\ufb01cation by symmetry-driven accumulation"}, {"color": "#66CCFF", "id": "chromatic analyses", "label": "chromatic analyses", "shape": "dot", "size": 25, "title": "chromatic analyses"}, {"color": "#66CCFF", "id": "Person re-identi\ufb01cation", "label": "Person re-identi\ufb01cation", "shape": "dot", "size": 25, "title": "Person re-identi\ufb01cation"}, {"color": "#66CCFF", "id": "local features", "label": "local features", "shape": "dot", "size": 25, "title": "local features"}, {"color": "#66CCFF", "id": "McAllester, D.", "label": "McAllester, D.", "shape": "dot", "size": 25, "title": "McAllester, D."}, {"color": "#66CCFF", "id": "IEEE Trans. Pattern Anal. Mach. Intell", "label": "IEEE Trans. Pattern Anal. Mach. Intell", "shape": "dot", "size": 25, "title": "IEEE Trans. Pattern Anal. Mach. Intell"}, {"color": "#66CCFF", "id": "Mitsubishi Electric Research Labs", "label": "Mitsubishi Electric Research Labs", "shape": "dot", "size": 25, "title": "Mitsubishi Electric Research Labs"}, {"color": "#66CCFF", "id": "Vassileios Balntas", "label": "Vassileios Balntas", "shape": "dot", "size": 25, "title": "Vassileios Balntas"}, {"color": "#66CCFF", "id": "BOLD", "label": "BOLD", "shape": "dot", "size": 25, "title": "BOLD"}, {"color": "#66CCFF", "id": "Lilian Tang", "label": "Lilian Tang", "shape": "dot", "size": 25, "title": "Lilian Tang"}, {"color": "#66CCFF", "id": "Krystian Mikolajczyk", "label": "Krystian Mikolajczyk", "shape": "dot", "size": 25, "title": "Krystian Mikolajczyk"}, {"color": "#66CCFF", "id": "Binary Online Learned Descriptor", "label": "Binary Online Learned Descriptor", "shape": "dot", "size": 25, "title": "Binary Online Learned Descriptor"}, {"color": "#66CCFF", "id": "BOLD (Binary Online Learned Descriptor)", "label": "BOLD (Binary Online Learned Descriptor)", "shape": "dot", "size": 25, "title": "BOLD (Binary Online Learned Descriptor)"}, {"color": "#66CCFF", "id": "image patch", "label": "image patch", "shape": "dot", "size": 25, "title": "image patch"}, {"color": "#66CCFF", "id": "linear discriminant embedding", "label": "linear discriminant embedding", "shape": "dot", "size": 25, "title": "linear discriminant embedding"}, {"color": "#66CCFF", "id": "binary tests", "label": "binary tests", "shape": "dot", "size": 25, "title": "binary tests"}, {"color": "#66CCFF", "id": "binary strings", "label": "binary strings", "shape": "dot", "size": 25, "title": "binary strings"}, {"color": "#66CCFF", "id": "test results", "label": "test results", "shape": "dot", "size": 25, "title": "test results"}, {"color": "#66CCFF", "id": "subset of robust tests", "label": "subset of robust tests", "shape": "dot", "size": 25, "title": "subset of robust tests"}, {"color": "#66CCFF", "id": "masked Hamming distance calculation", "label": "masked Hamming distance calculation", "shape": "dot", "size": 25, "title": "masked Hamming distance calculation"}, {"color": "#66CCFF", "id": "per-patch optimization", "label": "per-patch optimization", "shape": "dot", "size": 25, "title": "per-patch optimization"}, {"color": "#66CCFF", "id": "global optimization", "label": "global optimization", "shape": "dot", "size": 25, "title": "global optimization"}, {"color": "#66CCFF", "id": "Masked Hamming distance", "label": "Masked Hamming distance", "shape": "dot", "size": 25, "title": "Masked Hamming distance"}, {"color": "#66CCFF", "id": "tests", "label": "tests", "shape": "dot", "size": 25, "title": "tests"}, {"color": "#66CCFF", "id": "Per-patch optimization", "label": "Per-patch optimization", "shape": "dot", "size": 25, "title": "Per-patch optimization"}, {"color": "#66CCFF", "id": "D. G. Lowe", "label": "D. G. Lowe", "shape": "dot", "size": 25, "title": "D. G. Lowe"}, {"color": "#66CCFF", "id": "IJCV, 60:91\u2013110, 2004", "label": "IJCV, 60:91\u2013110, 2004", "shape": "dot", "size": 25, "title": "IJCV, 60:91\u2013110, 2004"}, {"color": "#66CCFF", "id": "Local descriptors", "label": "Local descriptors", "shape": "dot", "size": 25, "title": "Local descriptors"}, {"color": "#66CCFF", "id": "IEEE TPAMI, 27(10):1615\u20131630, 2005", "label": "IEEE TPAMI, 27(10):1615\u20131630, 2005", "shape": "dot", "size": 25, "title": "IEEE TPAMI, 27(10):1615\u20131630, 2005"}, {"color": "#66CCFF", "id": "SURF descriptor", "label": "SURF descriptor", "shape": "dot", "size": 25, "title": "SURF descriptor"}, {"color": "#66CCFF", "id": "ECCV, 2006", "label": "ECCV, 2006", "shape": "dot", "size": 25, "title": "ECCV, 2006"}, {"color": "#66CCFF", "id": "Local image descriptors", "label": "Local image descriptors", "shape": "dot", "size": 25, "title": "Local image descriptors"}, {"color": "#66CCFF", "id": "Discriminative learning", "label": "Discriminative learning", "shape": "dot", "size": 25, "title": "Discriminative learning"}, {"color": "#66CCFF", "id": "IEEE TPAMI, 33(1):43\u201357, 2010", "label": "IEEE TPAMI, 33(1):43\u201357, 2010", "shape": "dot", "size": 25, "title": "IEEE TPAMI, 33(1):43\u201357, 2010"}, {"color": "#66CCFF", "id": "Binary descriptors", "label": "Binary descriptors", "shape": "dot", "size": 25, "title": "Binary descriptors"}, {"color": "#66CCFF", "id": "Image matching", "label": "Image matching", "shape": "dot", "size": 25, "title": "Image matching"}, {"color": "#66CCFF", "id": "Online descriptor optimization", "label": "Online descriptor optimization", "shape": "dot", "size": 25, "title": "Online descriptor optimization"}, {"color": "#66CCFF", "id": "G. H. M. Brown and S. Winder", "label": "G. H. M. Brown and S. Winder", "shape": "dot", "size": 25, "title": "G. H. M. Brown and S. Winder"}, {"color": "#66CCFF", "id": "discriminative learning", "label": "discriminative learning", "shape": "dot", "size": 25, "title": "discriminative learning"}, {"color": "#66CCFF", "id": "interest point detection", "label": "interest point detection", "shape": "dot", "size": 25, "title": "interest point detection"}, {"color": "#66CCFF", "id": "K. Mikolajczyk and C. Schmid", "label": "K. Mikolajczyk and C. Schmid", "shape": "dot", "size": 25, "title": "K. Mikolajczyk and C. Schmid"}, {"color": "#66CCFF", "id": "tracking applications", "label": "tracking applications", "shape": "dot", "size": 25, "title": "tracking applications"}, {"color": "#66CCFF", "id": "Struck", "label": "Struck", "shape": "dot", "size": 25, "title": "Struck"}, {"color": "#66CCFF", "id": "Tracking-learning-detection", "label": "Tracking-learning-detection", "shape": "dot", "size": 25, "title": "Tracking-learning-detection"}, {"color": "#66CCFF", "id": "integration", "label": "integration", "shape": "dot", "size": 25, "title": "integration"}, {"color": "#66CCFF", "id": "keypoint recognition", "label": "keypoint recognition", "shape": "dot", "size": 25, "title": "keypoint recognition"}, {"color": "#66CCFF", "id": "random ferns", "label": "random ferns", "shape": "dot", "size": 25, "title": "random ferns"}, {"color": "#66CCFF", "id": "M. Ozuysal", "label": "M. Ozuysal", "shape": "dot", "size": 25, "title": "M. Ozuysal"}, {"color": "#66CCFF", "id": "tracking", "label": "tracking", "shape": "dot", "size": 25, "title": "tracking"}, {"color": "#66CCFF", "id": "Fast keypoint recognition method", "label": "Fast keypoint recognition method", "shape": "dot", "size": 25, "title": "Fast keypoint recognition method"}, {"color": "#66CCFF", "id": "IEEE TPAMI", "label": "IEEE TPAMI", "shape": "dot", "size": 25, "title": "IEEE TPAMI"}, {"color": "#66CCFF", "id": "ORB", "label": "ORB", "shape": "dot", "size": 25, "title": "ORB"}, {"color": "#66CCFF", "id": "SIFT", "label": "SIFT", "shape": "dot", "size": 25, "title": "SIFT"}, {"color": "#66CCFF", "id": "SURF", "label": "SURF", "shape": "dot", "size": 25, "title": "SURF"}, {"color": "#66CCFF", "id": "V. L. T. Trzcinski", "label": "V. L. T. Trzcinski", "shape": "dot", "size": 25, "title": "V. L. T. Trzcinski"}, {"color": "#66CCFF", "id": "Boosting Binary Keypoint Descriptors", "label": "Boosting Binary Keypoint Descriptors", "shape": "dot", "size": 25, "title": "Boosting Binary Keypoint Descriptors"}, {"color": "#66CCFF", "id": "University of Surrey, UK", "label": "University of Surrey, UK", "shape": "dot", "size": 25, "title": "University of Surrey, UK"}, {"color": "#66CCFF", "id": "Jiajun Wu", "label": "Jiajun Wu", "shape": "dot", "size": 25, "title": "Jiajun Wu"}, {"color": "#66CCFF", "id": "Deep Multiple Instance Learning for Image Classi\ufb01cation and Auto-Annotation", "label": "Deep Multiple Instance Learning for Image Classi\ufb01cation and Auto-Annotation", "shape": "dot", "size": 25, "title": "Deep Multiple Instance Learning for Image Classi\ufb01cation and Auto-Annotation"}, {"color": "#66CCFF", "id": "Deep Multiple Instance Learning", "label": "Deep Multiple Instance Learning", "shape": "dot", "size": 25, "title": "Deep Multiple Instance Learning"}, {"color": "#66CCFF", "id": "Yinan Yu", "label": "Yinan Yu", "shape": "dot", "size": 25, "title": "Yinan Yu"}, {"color": "#66CCFF", "id": "Kai Yu", "label": "Kai Yu", "shape": "dot", "size": 25, "title": "Kai Yu"}, {"color": "#66CCFF", "id": "Deep Multiple instance Learning", "label": "Deep Multiple instance Learning", "shape": "dot", "size": 25, "title": "Deep Multiple instance Learning"}, {"color": "#66CCFF", "id": "Learning Algorithm", "label": "Learning Algorithm", "shape": "dot", "size": 25, "title": "Learning Algorithm"}, {"color": "#66CCFF", "id": "Wu_Deep_Multiple_Instance_2015_CVPR_paper.pdf", "label": "Wu_Deep_Multiple_Instance_2015_CVPR_paper.pdf", "shape": "dot", "size": 25, "title": "Wu_Deep_Multiple_Instance_2015_CVPR_paper.pdf"}, {"color": "#66CCFF", "id": "Deep learning", "label": "Deep learning", "shape": "dot", "size": 25, "title": "Deep learning"}, {"color": "#66CCFF", "id": "tremendous improvements", "label": "tremendous improvements", "shape": "dot", "size": 25, "title": "tremendous improvements"}, {"color": "#66CCFF", "id": "early stage", "label": "early stage", "shape": "dot", "size": 25, "title": "early stage"}, {"color": "#66CCFF", "id": "multiple instance learning framework", "label": "multiple instance learning framework", "shape": "dot", "size": 25, "title": "multiple instance learning framework"}, {"color": "#66CCFF", "id": "object proposals", "label": "object proposals", "shape": "dot", "size": 25, "title": "object proposals"}, {"color": "#66CCFF", "id": "instance sets", "label": "instance sets", "shape": "dot", "size": 25, "title": "instance sets"}, {"color": "#66CCFF", "id": "text annotations", "label": "text annotations", "shape": "dot", "size": 25, "title": "text annotations"}, {"color": "#66CCFF", "id": "systems", "label": "systems", "shape": "dot", "size": 25, "title": "systems"}, {"color": "#66CCFF", "id": "MIL property", "label": "MIL property", "shape": "dot", "size": 25, "title": "MIL property"}, {"color": "#66CCFF", "id": "deep learning strategies", "label": "deep learning strategies", "shape": "dot", "size": 25, "title": "deep learning strategies"}, {"color": "#66CCFF", "id": "relationship", "label": "relationship", "shape": "dot", "size": 25, "title": "relationship"}, {"color": "#66CCFF", "id": "region-keyword pairs", "label": "region-keyword pairs", "shape": "dot", "size": 25, "title": "region-keyword pairs"}, {"color": "#66CCFF", "id": "convincing", "label": "convincing", "shape": "dot", "size": 25, "title": "convincing"}, {"color": "#66CCFF", "id": "classification", "label": "classification", "shape": "dot", "size": 25, "title": "classification"}, {"color": "#66CCFF", "id": "extraction", "label": "extraction", "shape": "dot", "size": 25, "title": "extraction"}, {"color": "#66CCFF", "id": "little supervision", "label": "little supervision", "shape": "dot", "size": 25, "title": "little supervision"}, {"color": "#66CCFF", "id": "reasonable", "label": "reasonable", "shape": "dot", "size": 25, "title": "reasonable"}, {"color": "#66CCFF", "id": "annotation proposals", "label": "annotation proposals", "shape": "dot", "size": 25, "title": "annotation proposals"}, {"color": "#66CCFF", "id": "convincing performance", "label": "convincing performance", "shape": "dot", "size": 25, "title": "convincing performance"}, {"color": "#66CCFF", "id": "Region-keyword pairs", "label": "Region-keyword pairs", "shape": "dot", "size": 25, "title": "Region-keyword pairs"}, {"color": "#66CCFF", "id": "Convolutional deep belief networks", "label": "Convolutional deep belief networks", "shape": "dot", "size": 25, "title": "Convolutional deep belief networks"}, {"color": "#66CCFF", "id": "Multiple Instance Learning (MIL)", "label": "Multiple Instance Learning (MIL)", "shape": "dot", "size": 25, "title": "Multiple Instance Learning (MIL)"}, {"color": "#66CCFF", "id": "Andrews et al.", "label": "Andrews et al.", "shape": "dot", "size": 25, "title": "Andrews et al."}, {"color": "#66CCFF", "id": "Support vector machines", "label": "Support vector machines", "shape": "dot", "size": 25, "title": "Support vector machines"}, {"color": "#66CCFF", "id": "Li \u0026 Wang", "label": "Li \u0026 Wang", "shape": "dot", "size": 25, "title": "Li \u0026 Wang"}, {"color": "#66CCFF", "id": "computerized annotation", "label": "computerized annotation", "shape": "dot", "size": 25, "title": "computerized annotation"}, {"color": "#66CCFF", "id": "Barnard et al.", "label": "Barnard et al.", "shape": "dot", "size": 25, "title": "Barnard et al."}, {"color": "#66CCFF", "id": "Matching words and pictures", "label": "Matching words and pictures", "shape": "dot", "size": 25, "title": "Matching words and pictures"}, {"color": "#66CCFF", "id": "Image Annotation", "label": "Image Annotation", "shape": "dot", "size": 25, "title": "Image Annotation"}, {"color": "#66CCFF", "id": "annotation of pictures", "label": "annotation of pictures", "shape": "dot", "size": 25, "title": "annotation of pictures"}, {"color": "#66CCFF", "id": "Barnard", "label": "Barnard", "shape": "dot", "size": 25, "title": "Barnard"}, {"color": "#66CCFF", "id": "Li, L.-J.", "label": "Li, L.-J.", "shape": "dot", "size": 25, "title": "Li, L.-J."}, {"color": "#66CCFF", "id": "Chen", "label": "Chen", "shape": "dot", "size": 25, "title": "Chen"}, {"color": "#66CCFF", "id": "Hierarchical matching", "label": "Hierarchical matching", "shape": "dot", "size": 25, "title": "Hierarchical matching"}, {"color": "#66CCFF", "id": "Li, Q.", "label": "Li, Q.", "shape": "dot", "size": 25, "title": "Li, Q."}, {"color": "#66CCFF", "id": "Harvesting mid-level visual concepts", "label": "Harvesting mid-level visual concepts", "shape": "dot", "size": 25, "title": "Harvesting mid-level visual concepts"}, {"color": "#66CCFF", "id": "Bing: Binarized normed gradients", "label": "Bing: Binarized normed gradients", "shape": "dot", "size": 25, "title": "Bing: Binarized normed gradients"}, {"color": "#66CCFF", "id": "Imaginet", "label": "Imaginet", "shape": "dot", "size": 25, "title": "Imaginet"}, {"color": "#66CCFF", "id": "hierarchical image", "label": "hierarchical image", "shape": "dot", "size": 25, "title": "hierarchical image"}, {"color": "#66CCFF", "id": "Rochan", "label": "Rochan", "shape": "dot", "size": 25, "title": "Rochan"}, {"color": "#66CCFF", "id": "Weakly Supervised Localization", "label": "Weakly Supervised Localization", "shape": "dot", "size": 25, "title": "Weakly Supervised Localization"}, {"color": "#66CCFF", "id": "Mrigank Rochan", "label": "Mrigank Rochan", "shape": "dot", "size": 25, "title": "Mrigank Rochan"}, {"color": "#66CCFF", "id": "Institute of Deep Learning", "label": "Institute of Deep Learning", "shape": "dot", "size": 25, "title": "Institute of Deep Learning"}, {"color": "#66CCFF", "id": "Chang Huang", "label": "Chang Huang", "shape": "dot", "size": 25, "title": "Chang Huang"}, {"color": "#66CCFF", "id": "training images", "label": "training images", "shape": "dot", "size": 25, "title": "training images"}, {"color": "#66CCFF", "id": "object bounding boxes", "label": "object bounding boxes", "shape": "dot", "size": 25, "title": "object bounding boxes"}, {"color": "#66CCFF", "id": "weakly labeled data", "label": "weakly labeled data", "shape": "dot", "size": 25, "title": "weakly labeled data"}, {"color": "#66CCFF", "id": "YouTube videos", "label": "YouTube videos", "shape": "dot", "size": 25, "title": "YouTube videos"}, {"color": "#66CCFF", "id": "user-generated tags", "label": "user-generated tags", "shape": "dot", "size": 25, "title": "user-generated tags"}, {"color": "#66CCFF", "id": "image search", "label": "image search", "shape": "dot", "size": 25, "title": "image search"}, {"color": "#66CCFF", "id": "weakly labeled images", "label": "weakly labeled images", "shape": "dot", "size": 25, "title": "weakly labeled images"}, {"color": "#66CCFF", "id": "localize object", "label": "localize object", "shape": "dot", "size": 25, "title": "localize object"}, {"color": "#66CCFF", "id": "collection of images", "label": "collection of images", "shape": "dot", "size": 25, "title": "collection of images"}, {"color": "#66CCFF", "id": "object category", "label": "object category", "shape": "dot", "size": 25, "title": "object category"}, {"color": "#66CCFF", "id": "bounding box", "label": "bounding box", "shape": "dot", "size": 25, "title": "bounding box"}, {"color": "#66CCFF", "id": "videos", "label": "videos", "shape": "dot", "size": 25, "title": "videos"}, {"color": "#66CCFF", "id": "novel object", "label": "novel object", "shape": "dot", "size": 25, "title": "novel object"}, {"color": "#66CCFF", "id": "object appearance model", "label": "object appearance model", "shape": "dot", "size": 25, "title": "object appearance model"}, {"color": "#66CCFF", "id": "familiar objects", "label": "familiar objects", "shape": "dot", "size": 25, "title": "familiar objects"}, {"color": "#66CCFF", "id": "image datasets", "label": "image datasets", "shape": "dot", "size": 25, "title": "image datasets"}, {"color": "#66CCFF", "id": "video datasets", "label": "video datasets", "shape": "dot", "size": 25, "title": "video datasets"}, {"color": "#66CCFF", "id": "unseen objects", "label": "unseen objects", "shape": "dot", "size": 25, "title": "unseen objects"}, {"color": "#66CCFF", "id": "Lampert et al. (2009)", "label": "Lampert et al. (2009)", "shape": "dot", "size": 25, "title": "Lampert et al. (2009)"}, {"color": "#66CCFF", "id": "unseen object classes", "label": "unseen object classes", "shape": "dot", "size": 25, "title": "unseen object classes"}, {"color": "#66CCFF", "id": "Object-graphs for context-aware category discovery", "label": "Object-graphs for context-aware category discovery", "shape": "dot", "size": 25, "title": "Object-graphs for context-aware category discovery"}, {"color": "#66CCFF", "id": "K. Grauman", "label": "K. Grauman", "shape": "dot", "size": 25, "title": "K. Grauman"}, {"color": "#66CCFF", "id": "R. G. Cinbis", "label": "R. G. Cinbis", "shape": "dot", "size": 25, "title": "R. G. Cinbis"}, {"color": "#66CCFF", "id": "Multi-fold mil training", "label": "Multi-fold mil training", "shape": "dot", "size": 25, "title": "Multi-fold mil training"}, {"color": "#66CCFF", "id": "J. Verbeek", "label": "J. Verbeek", "shape": "dot", "size": 25, "title": "J. Verbeek"}, {"color": "#66CCFF", "id": "T. Mikolov", "label": "T. Mikolov", "shape": "dot", "size": 25, "title": "T. Mikolov"}, {"color": "#66CCFF", "id": "Distributed representations of words and phrases", "label": "Distributed representations of words and phrases", "shape": "dot", "size": 25, "title": "Distributed representations of words and phrases"}, {"color": "#66CCFF", "id": "I. Sutskever", "label": "I. Sutskever", "shape": "dot", "size": 25, "title": "I. Sutskever"}, {"color": "#66CCFF", "id": "M. H. Nguyen", "label": "M. H. Nguyen", "shape": "dot", "size": 25, "title": "M. H. Nguyen"}, {"color": "#66CCFF", "id": "Weakly supervised discrimiative localization", "label": "Weakly supervised discrimiative localization", "shape": "dot", "size": 25, "title": "Weakly supervised discrimiative localization"}, {"color": "#66CCFF", "id": "A. Papazoglou", "label": "A. Papazoglou", "shape": "dot", "size": 25, "title": "A. Papazoglou"}, {"color": "#66CCFF", "id": "Fast object segmentation", "label": "Fast object segmentation", "shape": "dot", "size": 25, "title": "Fast object segmentation"}, {"color": "#66CCFF", "id": "V. Ferrari", "label": "V. Ferrari", "shape": "dot", "size": 25, "title": "V. Ferrari"}, {"color": "#66CCFF", "id": "Object-graphs", "label": "Object-graphs", "shape": "dot", "size": 25, "title": "Object-graphs"}, {"color": "#66CCFF", "id": "context-aware category discovery", "label": "context-aware category discovery", "shape": "dot", "size": 25, "title": "context-aware category discovery"}, {"color": "#66CCFF", "id": "Distributed representations", "label": "Distributed representations", "shape": "dot", "size": 25, "title": "Distributed representations"}, {"color": "#66CCFF", "id": "compositionality", "label": "compositionality", "shape": "dot", "size": 25, "title": "compositionality"}, {"color": "#66CCFF", "id": "IEEE International Conference on Computer Vision", "label": "IEEE International Conference on Computer Vision", "shape": "dot", "size": 25, "title": "IEEE International Conference on Computer Vision"}, {"color": "#66CCFF", "id": "A. Prest", "label": "A. Prest", "shape": "dot", "size": 25, "title": "A. Prest"}, {"color": "#66CCFF", "id": "Learning object class detectors", "label": "Learning object class detectors", "shape": "dot", "size": 25, "title": "Learning object class detectors"}, {"color": "#66CCFF", "id": "M. Rohrbach", "label": "M. Rohrbach", "shape": "dot", "size": 25, "title": "M. Rohrbach"}, {"color": "#66CCFF", "id": "Evaluating knowledge transfer", "label": "Evaluating knowledge transfer", "shape": "dot", "size": 25, "title": "Evaluating knowledge transfer"}, {"color": "#66CCFF", "id": "IEEE Conference on Computer Vision and Pattern Recognition", "label": "IEEE Conference on Computer Vision and Pattern Recognition", "shape": "dot", "size": 25, "title": "IEEE Conference on Computer Vision and Pattern Recognition"}, {"color": "#66CCFF", "id": "What helps where", "label": "What helps where", "shape": "dot", "size": 25, "title": "What helps where"}, {"color": "#66CCFF", "id": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition", "label": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition", "shape": "dot", "size": 25, "title": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition"}, {"color": "#66CCFF", "id": "University of Manitoba", "label": "University of Manitoba", "shape": "dot", "size": 25, "title": "University of Manitoba"}, {"color": "#66CCFF", "id": "Yang Wang", "label": "Yang Wang", "shape": "dot", "size": 25, "title": "Yang Wang"}, {"color": "#66CCFF", "id": "Mriganka Rochan", "label": "Mriganka Rochan", "shape": "dot", "size": 25, "title": "Mriganka Rochan"}, {"color": "#66CCFF", "id": "mrochan@cs.umanitoba.ca", "label": "mrochan@cs.umanitoba.ca", "shape": "dot", "size": 25, "title": "mrochan@cs.umanitoba.ca"}, {"color": "#66CCFF", "id": "Canada", "label": "Canada", "shape": "dot", "size": 25, "title": "Canada"}, {"color": "#66CCFF", "id": "rigank Rochan", "label": "rigank Rochan", "shape": "dot", "size": 25, "title": "rigank Rochan"}, {"color": "#66CCFF", "id": "Wei Liu", "label": "Wei Liu", "shape": "dot", "size": 25, "title": "Wei Liu"}, {"color": "#66CCFF", "id": "Towards 3D Object Detection", "label": "Towards 3D Object Detection", "shape": "dot", "size": 25, "title": "Towards 3D Object Detection"}, {"color": "#66CCFF", "id": "Rongrong Ji", "label": "Rongrong Ji", "shape": "dot", "size": 25, "title": "Rongrong Ji"}, {"color": "#66CCFF", "id": "Shaozi Li", "label": "Shaozi Li", "shape": "dot", "size": 25, "title": "Shaozi Li"}, {"color": "#66CCFF", "id": "ywang@cs.umanitoba.ca", "label": "ywang@cs.umanitoba.ca", "shape": "dot", "size": 25, "title": "ywang@cs.umanitoba.ca"}, {"color": "#66CCFF", "id": "Towards 3D ObjectDetection", "label": "Towards 3D ObjectDetection", "shape": "dot", "size": 25, "title": "Towards 3D ObjectDetection"}, {"color": "#66CCFF", "id": "3D scenes", "label": "3D scenes", "shape": "dot", "size": 25, "title": "3D scenes"}, {"color": "#66CCFF", "id": "accurate detection algorithm", "label": "accurate detection algorithm", "shape": "dot", "size": 25, "title": "accurate detection algorithm"}, {"color": "#66CCFF", "id": "RGB and depth modalities", "label": "RGB and depth modalities", "shape": "dot", "size": 25, "title": "RGB and depth modalities"}, {"color": "#66CCFF", "id": "correlated", "label": "correlated", "shape": "dot", "size": 25, "title": "correlated"}, {"color": "#66CCFF", "id": "cross-modality deep learning framework", "label": "cross-modality deep learning framework", "shape": "dot", "size": 25, "title": "cross-modality deep learning framework"}, {"color": "#66CCFF", "id": "deep Boltzmann Machines", "label": "deep Boltzmann Machines", "shape": "dot", "size": 25, "title": "deep Boltzmann Machines"}, {"color": "#66CCFF", "id": "lack of 3D training data", "label": "lack of 3D training data", "shape": "dot", "size": 25, "title": "lack of 3D training data"}, {"color": "#66CCFF", "id": "labeled 2D samples", "label": "labeled 2D samples", "shape": "dot", "size": 25, "title": "labeled 2D samples"}, {"color": "#66CCFF", "id": "existing datasets", "label": "existing datasets", "shape": "dot", "size": 25, "title": "existing datasets"}, {"color": "#66CCFF", "id": "3D CAD models", "label": "3D CAD models", "shape": "dot", "size": 25, "title": "3D CAD models"}, {"color": "#66CCFF", "id": "RMRC dataset", "label": "RMRC dataset", "shape": "dot", "size": 25, "title": "RMRC dataset"}, {"color": "#66CCFF", "id": "cross-modality features", "label": "cross-modality features", "shape": "dot", "size": 25, "title": "cross-modality features"}, {"color": "#66CCFF", "id": "RGBD data", "label": "RGBD data", "shape": "dot", "size": 25, "title": "RGBD data"}, {"color": "#66CCFF", "id": "models", "label": "models", "shape": "dot", "size": 25, "title": "models"}, {"color": "#66CCFF", "id": "k", "label": "k", "shape": "dot", "size": 25, "title": "k"}, {"color": "#66CCFF", "id": "effectiveness of approach", "label": "effectiveness of approach", "shape": "dot", "size": 25, "title": "effectiveness of approach"}, {"color": "#66CCFF", "id": "Semantic labeling", "label": "Semantic labeling", "shape": "dot", "size": 25, "title": "Semantic labeling"}, {"color": "#66CCFF", "id": "3d point clouds", "label": "3d point clouds", "shape": "dot", "size": 25, "title": "3d point clouds"}, {"color": "#66CCFF", "id": "Learning rich features", "label": "Learning rich features", "shape": "dot", "size": 25, "title": "Learning rich features"}, {"color": "#66CCFF", "id": "RGBD images", "label": "RGBD images", "shape": "dot", "size": 25, "title": "RGBD images"}, {"color": "#66CCFF", "id": "ILSVRC2012", "label": "ILSVRC2012", "shape": "dot", "size": 25, "title": "ILSVRC2012"}, {"color": "#66CCFF", "id": "Efficient 3d scene labeling", "label": "Efficient 3d scene labeling", "shape": "dot", "size": 25, "title": "Efficient 3d scene labeling"}, {"color": "#66CCFF", "id": "field-s of trees", "label": "field-s of trees", "shape": "dot", "size": 25, "title": "field-s of trees"}, {"color": "#66CCFF", "id": "geNet", "label": "geNet", "shape": "dot", "size": 25, "title": "geNet"}, {"color": "#66CCFF", "id": "2012", "label": "2012", "shape": "dot", "size": 25, "title": "2012"}, {"color": "#66CCFF", "id": "O. Kahler", "label": "O. Kahler", "shape": "dot", "size": 25, "title": "O. Kahler"}, {"color": "#66CCFF", "id": "N. Srivastava", "label": "N. Srivastava", "shape": "dot", "size": 25, "title": "N. Srivastava"}, {"color": "#66CCFF", "id": "Multimodal learning", "label": "Multimodal learning", "shape": "dot", "size": 25, "title": "Multimodal learning"}, {"color": "#66CCFF", "id": "K. Lai", "label": "K. Lai", "shape": "dot", "size": 25, "title": "K. Lai"}, {"color": "#66CCFF", "id": "Detection-based object labeling", "label": "Detection-based object labeling", "shape": "dot", "size": 25, "title": "Detection-based object labeling"}, {"color": "#66CCFF", "id": "IEEE International Conference on Robotics and Automation", "label": "IEEE International Conference on Robotics and Automation", "shape": "dot", "size": 25, "title": "IEEE International Conference on Robotics and Automation"}, {"color": "#66CCFF", "id": "L. Bo", "label": "L. Bo", "shape": "dot", "size": 25, "title": "L. Bo"}, {"color": "#66CCFF", "id": "Unsupervised feature learning", "label": "Unsupervised feature learning", "shape": "dot", "size": 25, "title": "Unsupervised feature learning"}, {"color": "#66CCFF", "id": "rgb-d based object recognition", "label": "rgb-d based object recognition", "shape": "dot", "size": 25, "title": "rgb-d based object recognition"}, {"color": "#66CCFF", "id": "X. Xiong", "label": "X. Xiong", "shape": "dot", "size": 25, "title": "X. Xiong"}, {"color": "#66CCFF", "id": "3-d scene analysis", "label": "3-d scene analysis", "shape": "dot", "size": 25, "title": "3-d scene analysis"}, {"color": "#66CCFF", "id": "sequenced predictions", "label": "sequenced predictions", "shape": "dot", "size": 25, "title": "sequenced predictions"}, {"color": "#66CCFF", "id": "A. Wang", "label": "A. Wang", "shape": "dot", "size": 25, "title": "A. Wang"}, {"color": "#66CCFF", "id": "Multi-modal unsupervised feature learning", "label": "Multi-modal unsupervised feature learning", "shape": "dot", "size": 25, "title": "Multi-modal unsupervised feature learning"}, {"color": "#66CCFF", "id": "rgb-d scene labeling", "label": "rgb-d scene labeling", "shape": "dot", "size": 25, "title": "rgb-d scene labeling"}, {"color": "#66CCFF", "id": "Dep. of Cognitive Science", "label": "Dep. of Cognitive Science", "shape": "dot", "size": 25, "title": "Dep. of Cognitive Science"}, {"color": "#66CCFF", "id": "Xiamen University", "label": "Xiamen University", "shape": "dot", "size": 25, "title": "Xiamen University"}, {"color": "#66CCFF", "id": "Dep. of Cognitive Space", "label": "Dep. of Cognitive Space", "shape": "dot", "size": 25, "title": "Dep. of Cognitive Space"}, {"color": "#66CCFF", "id": "Abhishek Sharma", "label": "Abhishek Sharma", "shape": "dot", "size": 25, "title": "Abhishek Sharma"}, {"color": "#66CCFF", "id": "Deep Hierarchical Parsing", "label": "Deep Hierarchical Parsing", "shape": "dot", "size": 25, "title": "Deep Hierarchical Parsing"}, {"color": "#66CCFF", "id": "Oncel Tuzel", "label": "Oncel Tuzel", "shape": "dot", "size": 25, "title": "Oncel Tuzel"}, {"color": "#66CCFF", "id": "David W. Jacobs", "label": "David W. Jacobs", "shape": "dot", "size": 25, "title": "David W. Jacobs"}, {"color": "#66CCFF", "id": "Sliding shapes", "label": "Sliding shapes", "shape": "dot", "size": 25, "title": "Sliding shapes"}, {"color": "#66CCFF", "id": "Sharma_Deep_Hierarial_Parsing_2015_CVPR_paper", "label": "Sharma_Deep_Hierarial_Parsing_2015_CVPR_paper", "shape": "dot", "size": 25, "title": "Sharma_Deep_Hierarial_Parsing_2015_CVPR_paper"}, {"color": "#66CCFF", "id": "improvements to RCPN", "label": "improvements to RCPN", "shape": "dot", "size": 25, "title": "improvements to RCPN"}, {"color": "#66CCFF", "id": "RCPN", "label": "RCPN", "shape": "dot", "size": 25, "title": "RCPN"}, {"color": "#66CCFF", "id": "deep feed-forward neural network", "label": "deep feed-forward neural network", "shape": "dot", "size": 25, "title": "deep feed-forward neural network"}, {"color": "#66CCFF", "id": "bypass error paths", "label": "bypass error paths", "shape": "dot", "size": 25, "title": "bypass error paths"}, {"color": "#66CCFF", "id": "contextual propagation", "label": "contextual propagation", "shape": "dot", "size": 25, "title": "contextual propagation"}, {"color": "#66CCFF", "id": "modifications", "label": "modifications", "shape": "dot", "size": 25, "title": "modifications"}, {"color": "#66CCFF", "id": "classification loss", "label": "classification loss", "shape": "dot", "size": 25, "title": "classification loss"}, {"color": "#66CCFF", "id": "random parse trees", "label": "random parse trees", "shape": "dot", "size": 25, "title": "random parse trees"}, {"color": "#66CCFF", "id": "tree-style MRF", "label": "tree-style MRF", "shape": "dot", "size": 25, "title": "tree-style MRF"}, {"color": "#66CCFF", "id": "hierarchical dependencies", "label": "hierarchical dependencies", "shape": "dot", "size": 25, "title": "hierarchical dependencies"}, {"color": "#66CCFF", "id": "Tree-Style MRF", "label": "Tree-Style MRF", "shape": "dot", "size": 25, "title": "Tree-Style MRF"}, {"color": "#66CCFF", "id": "Modifications", "label": "Modifications", "shape": "dot", "size": 25, "title": "Modifications"}, {"color": "#66CCFF", "id": "Semantic Segmentation", "label": "Semantic Segmentation", "shape": "dot", "size": 25, "title": "Semantic Segmentation"}, {"color": "#66CCFF", "id": "Deep Neural Networks", "label": "Deep Neural Networks", "shape": "dot", "size": 25, "title": "Deep Neural Networks"}, {"color": "#66CCFF", "id": "Recursive Context Propagation Network (RCPN)", "label": "Recursive Context Propagation Network (RCPN)", "shape": "dot", "size": 25, "title": "Recursive Context Propagation Network (RCPN)"}, {"color": "#66CCFF", "id": "Contextual Propagation", "label": "Contextual Propagation", "shape": "dot", "size": 25, "title": "Contextual Propagation"}, {"color": "#66CCFF", "id": "Socher et al. (2011)", "label": "Socher et al. (2011)", "shape": "dot", "size": 25, "title": "Socher et al. (2011)"}, {"color": "#66CCFF", "id": "Recursive Neural Networks", "label": "Recursive Neural Networks", "shape": "dot", "size": 25, "title": "Recursive Neural Networks"}, {"color": "#66CCFF", "id": "Farabet et al. (2013)", "label": "Farabet et al. (2013)", "shape": "dot", "size": 25, "title": "Farabet et al. (2013)"}, {"color": "#66CCFF", "id": "scene labeling", "label": "scene labeling", "shape": "dot", "size": 25, "title": "scene labeling"}, {"color": "#66CCFF", "id": "Fergus and Eigen (2012)", "label": "Fergus and Eigen (2012)", "shape": "dot", "size": 25, "title": "Fergus and Eigen (2012)"}, {"color": "#66CCFF", "id": "image parsing", "label": "image parsing", "shape": "dot", "size": 25, "title": "image parsing"}, {"color": "#66CCFF", "id": "Markov Random Fields (MRF)", "label": "Markov Random Fields (MRF)", "shape": "dot", "size": 25, "title": "Markov Random Fields (MRF)"}, {"color": "#66CCFF", "id": "Najman", "label": "Najman", "shape": "dot", "size": 25, "title": "Najman"}, {"color": "#66CCFF", "id": "Learning hierarchical features for scene labeling", "label": "Learning hierarchical features for scene labeling", "shape": "dot", "size": 25, "title": "Learning hierarchical features for scene labeling"}, {"color": "#66CCFF", "id": "R. Fergus", "label": "R. Fergus", "shape": "dot", "size": 25, "title": "R. Fergus"}, {"color": "#66CCFF", "id": "Nonparametric image parsing", "label": "Nonparametric image parsing", "shape": "dot", "size": 25, "title": "Nonparametric image parsing"}, {"color": "#66CCFF", "id": "A. Torralba", "label": "A. Torralba", "shape": "dot", "size": 25, "title": "A. Torralba"}, {"color": "#66CCFF", "id": "Context-based vision system", "label": "Context-based vision system", "shape": "dot", "size": 25, "title": "Context-based vision system"}, {"color": "#66CCFF", "id": "P. H. O. Pinheiro", "label": "P. H. O. Pinheiro", "shape": "dot", "size": 25, "title": "P. H. O. Pinheiro"}, {"color": "#66CCFF", "id": "Recurrent convolutional neural networks", "label": "Recurrent convolutional neural networks", "shape": "dot", "size": 25, "title": "Recurrent convolutional neural networks"}, {"color": "#66CCFF", "id": "A. Sharma", "label": "A. Sharma", "shape": "dot", "size": 25, "title": "A. Sharma"}, {"color": "#66CCFF", "id": "Recursive context propagation network", "label": "Recursive context propagation network", "shape": "dot", "size": 25, "title": "Recursive context propagation network"}, {"color": "#66CCFF", "id": "J. Tighe", "label": "J. Tighe", "shape": "dot", "size": 25, "title": "J. Tighe"}, {"color": "#66CCFF", "id": "Finding things", "label": "Finding things", "shape": "dot", "size": 25, "title": "Finding things"}, {"color": "#66CCFF", "id": "R. Mottaghi", "label": "R. Mottaghi", "shape": "dot", "size": 25, "title": "R. Mottaghi"}, {"color": "#66CCFF", "id": "Analyzing semantic segmentation", "label": "Analyzing semantic segmentation", "shape": "dot", "size": 25, "title": "Analyzing semantic segmentation"}, {"color": "#66CCFF", "id": "Superparsing", "label": "Superparsing", "shape": "dot", "size": 25, "title": "Superparsing"}, {"color": "#66CCFF", "id": "Computer Science Department", "label": "Computer Science Department", "shape": "dot", "size": 25, "title": "Computer Science Department"}, {"color": "#66CCFF", "id": "bhokaal@cs.umd.edu", "label": "bhokaal@cs.umd.edu", "shape": "dot", "size": 25, "title": "bhokaal@cs.umd.edu"}, {"color": "#66CCFF", "id": "IEEE TPAM", "label": "IEEE TPAM", "shape": "dot", "size": 25, "title": "IEEE TPAM"}, {"color": "#66CCFF", "id": "IEEE CVPR", "label": "IEEE CVPR", "shape": "dot", "size": 25, "title": "IEEE CVPR"}, {"color": "#66CCFF", "id": "ICML", "label": "ICML", "shape": "dot", "size": 25, "title": "ICML"}, {"color": "#66CCFF", "id": "Junlin Hu", "label": "Junlin Hu", "shape": "dot", "size": 25, "title": "Junlin Hu"}, {"color": "#66CCFF", "id": "Deep Transfer Metric Learning", "label": "Deep Transfer Metric Learning", "shape": "dot", "size": 25, "title": "Deep Transfer Metric Learning"}, {"color": "#66CCFF", "id": "Jiwen Lu", "label": "Jiwen Lu", "shape": "dot", "size": 25, "title": "Jiwen Lu"}, {"color": "#66CCFF", "id": "MERL", "label": "MERL", "shape": "dot", "size": 25, "title": "MERL"}, {"color": "#66CCFF", "id": "metric learning methods", "label": "metric learning methods", "shape": "dot", "size": 25, "title": "metric learning methods"}, {"color": "#66CCFF", "id": "similar scenarios", "label": "similar scenarios", "shape": "dot", "size": 25, "title": "similar scenarios"}, {"color": "#66CCFF", "id": "real-world visual recognition applications", "label": "real-world visual recognition applications", "shape": "dot", "size": 25, "title": "real-world visual recognition applications"}, {"color": "#66CCFF", "id": "DTML method", "label": "DTML method", "shape": "dot", "size": 25, "title": "DTML method"}, {"color": "#66CCFF", "id": "discriminative knowledge", "label": "discriminative knowledge", "shape": "dot", "size": 25, "title": "discriminative knowledge"}, {"color": "#66CCFF", "id": "inter-class variations", "label": "inter-class variations", "shape": "dot", "size": 25, "title": "inter-class variations"}, {"color": "#66CCFF", "id": "intra-class variations", "label": "intra-class variations", "shape": "dot", "size": 25, "title": "intra-class variations"}, {"color": "#66CCFF", "id": "distribution divergence", "label": "distribution divergence", "shape": "dot", "size": 25, "title": "distribution divergence"}, {"color": "#66CCFF", "id": "DSTML method", "label": "DSTML method", "shape": "dot", "size": 25, "title": "DSTML method"}, {"color": "#66CCFF", "id": "outputs of hidden and top layers", "label": "outputs of hidden and top layers", "shape": "dot", "size": 25, "title": "outputs of hidden and top layers"}, {"color": "#66CCFF", "id": "deeply supervised transfer metric learning", "label": "deeply supervised transfer metric learning", "shape": "dot", "size": 25, "title": "deeply supervised transfer metric learning"}, {"color": "#66CCFF", "id": "vergence", "label": "vergence", "shape": "dot", "size": 25, "title": "vergence"}, {"color": "#66CCFF", "id": "source and target domains", "label": "source and target domains", "shape": "dot", "size": 25, "title": "source and target domains"}, {"color": "#66CCFF", "id": "transfer metric learning method", "label": "transfer metric learning method", "shape": "dot", "size": 25, "title": "transfer metric learning method"}, {"color": "#66CCFF", "id": "effectiveness of proposed methods", "label": "effectiveness of proposed methods", "shape": "dot", "size": 25, "title": "effectiveness of proposed methods"}, {"color": "#66CCFF", "id": "cross-dataset task", "label": "cross-dataset task", "shape": "dot", "size": 25, "title": "cross-dataset task"}, {"color": "#66CCFF", "id": "cross-dataset tasks", "label": "cross-dataset tasks", "shape": "dot", "size": 25, "title": "cross-dataset tasks"}, {"color": "#66CCFF", "id": "hidden layers", "label": "hidden layers", "shape": "dot", "size": 25, "title": "hidden layers"}, {"color": "#66CCFF", "id": "top layers", "label": "top layers", "shape": "dot", "size": 25, "title": "top layers"}, {"color": "#66CCFF", "id": "Deep Transfer Metric Learning (DTML)", "label": "Deep Transfer Metric Learning (DTML)", "shape": "dot", "size": 25, "title": "Deep Transfer Metric Learning (DTML)"}, {"color": "#66CCFF", "id": "Cross-Domain Visual Recognition", "label": "Cross-Domain Visual Recognition", "shape": "dot", "size": 25, "title": "Cross-Domain Visual Recognition"}, {"color": "#66CCFF", "id": "Distribution Divergence", "label": "Distribution Divergence", "shape": "dot", "size": 25, "title": "Distribution Divergence"}, {"color": "#66CCFF", "id": "Face Description", "label": "Face Description", "shape": "dot", "size": 25, "title": "Face Description"}, {"color": "#66CCFF", "id": "Learning Deep Architectures", "label": "Learning Deep Architectures", "shape": "dot", "size": 25, "title": "Learning Deep Architectures"}, {"color": "#66CCFF", "id": "Predictive Structures", "label": "Predictive Structures", "shape": "dot", "size": 25, "title": "Predictive Structures"}, {"color": "#66CCFF", "id": "Multiple Tasks", "label": "Multiple Tasks", "shape": "dot", "size": 25, "title": "Multiple Tasks"}, {"color": "#66CCFF", "id": "Journal of Machine Learning Research", "label": "Journal of Machine Learning Research", "shape": "dot", "size": 25, "title": "Journal of Machine Learning Research"}, {"color": "#66CCFF", "id": "Learning deep architectures for AI", "label": "Learning deep architectures for AI", "shape": "dot", "size": 25, "title": "Learning deep architectures for AI"}, {"color": "#66CCFF", "id": "Foundations and Trends in Machine Learning", "label": "Foundations and Trends in Machine Learning", "shape": "dot", "size": 25, "title": "Foundations and Trends in Machine Learning"}, {"color": "#66CCFF", "id": "ACM Multimedia", "label": "ACM Multimedia", "shape": "dot", "size": 25, "title": "ACM Multimedia"}, {"color": "#66CCFF", "id": "ACM", "label": "ACM", "shape": "dot", "size": 25, "title": "ACM"}, {"color": "#66CCFF", "id": "Chen, D.", "label": "Chen, D.", "shape": "dot", "size": 25, "title": "Chen, D."}, {"color": "#66CCFF", "id": "Bayesian face revisited", "label": "Bayesian face revisited", "shape": "dot", "size": 25, "title": "Bayesian face revisited"}, {"color": "#66CCFF", "id": "Duan, L.", "label": "Duan, L.", "shape": "dot", "size": 25, "title": "Duan, L."}, {"color": "#66CCFF", "id": "Domain transfer SVM", "label": "Domain transfer SVM", "shape": "dot", "size": 25, "title": "Domain transfer SVM"}, {"color": "#66CCFF", "id": "Conference on Computer Vision and Pattern Recognition", "label": "Conference on Computer Vision and Pattern Recognition", "shape": "dot", "size": 25, "title": "Conference on Computer Vision and Pattern Recognition"}, {"color": "#66CCFF", "id": "Viewpoint invariant pedestrian recognition", "label": "Viewpoint invariant pedestrian recognition", "shape": "dot", "size": 25, "title": "Viewpoint invariant pedestrian recognition"}, {"color": "#66CCFF", "id": "Gray \u0026 Tao (2008)", "label": "Gray \u0026 Tao (2008)", "shape": "dot", "size": 25, "title": "Gray \u0026 Tao (2008)"}, {"color": "#66CCFF", "id": "Gretton et al. (2006)", "label": "Gretton et al. (2006)", "shape": "dot", "size": 25, "title": "Gretton et al. (2006)"}, {"color": "#66CCFF", "id": "Neural Information Processing Systems", "label": "Neural Information Processing Systems", "shape": "dot", "size": 25, "title": "Neural Information Processing Systems"}, {"color": "#66CCFF", "id": "Hinton et al. (2006)", "label": "Hinton et al. (2006)", "shape": "dot", "size": 25, "title": "Hinton et al. (2006)"}, {"color": "#66CCFF", "id": "Neural Computation", "label": "Neural Computation", "shape": "dot", "size": 25, "title": "Neural Computation"}, {"color": "#66CCFF", "id": "Huang et al. (2012)", "label": "Huang et al. (2012)", "shape": "dot", "size": 25, "title": "Huang et al. (2012)"}, {"color": "#66CCFF", "id": "School of Electrical and Electronic Engineering", "label": "School of Electrical and Electronic Engineering", "shape": "dot", "size": 25, "title": "School of Electrical and Electronic Engineering"}, {"color": "#66CCFF", "id": "jhu007@e.ntu.edu.sg", "label": "jhu007@e.ntu.edu.sg", "shape": "dot", "size": 25, "title": "jhu007@e.ntu.edu.sg"}, {"color": "#66CCFF", "id": "Nanyang Technological University", "label": "Nanyang Technological University", "shape": "dot", "size": 25, "title": "Nanyang Technological University"}, {"color": "#66CCFF", "id": "Singapore", "label": "Singapore", "shape": "dot", "size": 25, "title": "Singapore"}, {"color": "#66CCFF", "id": "jiwen.lu@adsc.com.sg", "label": "jiwen.lu@adsc.com.sg", "shape": "dot", "size": 25, "title": "jiwen.lu@adsc.com.sg"}, {"color": "#66CCFF", "id": "Yap-Peng Tan", "label": "Yap-Peng Tan", "shape": "dot", "size": 25, "title": "Yap-Peng Tan"}, {"color": "#66CCFF", "id": "eyptan@ntu.edu.sg", "label": "eyptan@ntu.edu.sg", "shape": "dot", "size": 25, "title": "eyptan@ntu.edu.sg"}, {"color": "#66CCFF", "id": "Takuya Narihira", "label": "Takuya Narihira", "shape": "dot", "size": 25, "title": "Takuya Narihira"}, {"color": "#66CCFF", "id": "Learning Lightness from Human Judgement", "label": "Learning Lightness from Human Judgement", "shape": "dot", "size": 25, "title": "Learning Lightness from Human Judgement"}, {"color": "#66CCFF", "id": "Michael Maire", "label": "Michael Maire", "shape": "dot", "size": 25, "title": "Michael Maire"}, {"color": "#66CCFF", "id": "Stella X. Yu", "label": "Stella X. Yu", "shape": "dot", "size": 25, "title": "Stella X. Yu"}, {"color": "#66CCFF", "id": "Narihira_Learning_Lightness_From_2015_CVPR_paper.pdf", "label": "Narihira_Learning_Lightness_From_2015_CVPR_paper.pdf", "shape": "dot", "size": 25, "title": "Narihira_Learning_Lightness_From_2015_CVPR_paper.pdf"}, {"color": "#66CCFF", "id": "Narihira_Learning_Lightness_From_2015_CVPR_paper", "label": "Narihira_Learning_Lightness_From_2015_CVPR_paper", "shape": "dot", "size": 25, "title": "Narihira_Learning_Lightness_From_2015_CVPR_paper"}, {"color": "#66CCFF", "id": "inferring lightness", "label": "inferring lightness", "shape": "dot", "size": 25, "title": "inferring lightness"}, {"color": "#66CCFF", "id": "perceived re\ufb02ectance", "label": "perceived re\ufb02ectance", "shape": "dot", "size": 25, "title": "perceived re\ufb02ectance"}, {"color": "#66CCFF", "id": "classic methods", "label": "classic methods", "shape": "dot", "size": 25, "title": "classic methods"}, {"color": "#66CCFF", "id": "intrinsic image decomposition", "label": "intrinsic image decomposition", "shape": "dot", "size": 25, "title": "intrinsic image decomposition"}, {"color": "#66CCFF", "id": "re\ufb02ectance and shading components", "label": "re\ufb02ectance and shading components", "shape": "dot", "size": 25, "title": "re\ufb02ectance and shading components"}, {"color": "#66CCFF", "id": "lightness differences between pixels", "label": "lightness differences between pixels", "shape": "dot", "size": 25, "title": "lightness differences between pixels"}, {"color": "#66CCFF", "id": "patch representations", "label": "patch representations", "shape": "dot", "size": 25, "title": "patch representations"}, {"color": "#66CCFF", "id": "deep networks", "label": "deep networks", "shape": "dot", "size": 25, "title": "deep networks"}, {"color": "#66CCFF", "id": "Intrinsic Images in the Wild dataset", "label": "Intrinsic Images in the Wild dataset", "shape": "dot", "size": 25, "title": "Intrinsic Images in the Wild dataset"}, {"color": "#66CCFF", "id": "local lightness model", "label": "local lightness model", "shape": "dot", "size": 25, "title": "local lightness model"}, {"color": "#66CCFF", "id": "on-par with global lightness model", "label": "on-par with global lightness model", "shape": "dot", "size": 25, "title": "on-par with global lightness model"}, {"color": "#66CCFF", "id": "global lightness model", "label": "global lightness model", "shape": "dot", "size": 25, "title": "global lightness model"}, {"color": "#66CCFF", "id": "shading/re\ufb02ectance priors", "label": "shading/re\ufb02ectance priors", "shape": "dot", "size": 25, "title": "shading/re\ufb02ectance priors"}, {"color": "#66CCFF", "id": "on-par performance", "label": "on-par performance", "shape": "dot", "size": 25, "title": "on-par performance"}, {"color": "#66CCFF", "id": "state-of-the-art global lightness model", "label": "state-of-the-art global lightness model", "shape": "dot", "size": 25, "title": "state-of-the-art global lightness model"}, {"color": "#66CCFF", "id": "shading/reflectance priors", "label": "shading/reflectance priors", "shape": "dot", "size": 25, "title": "shading/reflectance priors"}, {"color": "#66CCFF", "id": "dense conditional random field formulation", "label": "dense conditional random field formulation", "shape": "dot", "size": 25, "title": "dense conditional random field formulation"}, {"color": "#66CCFF", "id": "simultaneous reasoning", "label": "simultaneous reasoning", "shape": "dot", "size": 25, "title": "simultaneous reasoning"}, {"color": "#66CCFF", "id": "pairs of pixels", "label": "pairs of pixels", "shape": "dot", "size": 25, "title": "pairs of pixels"}, {"color": "#66CCFF", "id": "multiple priors", "label": "multiple priors", "shape": "dot", "size": 25, "title": "multiple priors"}, {"color": "#66CCFF", "id": "ld dataset", "label": "ld dataset", "shape": "dot", "size": 25, "title": "ld dataset"}, {"color": "#66CCFF", "id": "lightness perception", "label": "lightness perception", "shape": "dot", "size": 25, "title": "lightness perception"}, {"color": "#66CCFF", "id": "cognitive neurosciences", "label": "cognitive neurosciences", "shape": "dot", "size": 25, "title": "cognitive neurosciences"}, {"color": "#66CCFF", "id": "Lightness perception and lightness illusions", "label": "Lightness perception and lightness illusions", "shape": "dot", "size": 25, "title": "Lightness perception and lightness illusions"}, {"color": "#66CCFF", "id": "foundational work", "label": "foundational work", "shape": "dot", "size": 25, "title": "foundational work"}, {"color": "#66CCFF", "id": "H. G. Barrow", "label": "H. G. Barrow", "shape": "dot", "size": 25, "title": "H. G. Barrow"}, {"color": "#66CCFF", "id": "Recovering intrinsic scene characteristics from images", "label": "Recovering intrinsic scene characteristics from images", "shape": "dot", "size": 25, "title": "Recovering intrinsic scene characteristics from images"}, {"color": "#66CCFF", "id": "intrinsic image algorithms", "label": "intrinsic image algorithms", "shape": "dot", "size": 25, "title": "intrinsic image algorithms"}, {"color": "#66CCFF", "id": "relative reflectance", "label": "relative reflectance", "shape": "dot", "size": 25, "title": "relative reflectance"}, {"color": "#66CCFF", "id": "human judgment data", "label": "human judgment data", "shape": "dot", "size": 25, "title": "human judgment data"}, {"color": "#66CCFF", "id": "Computer Vision Systems", "label": "Computer Vision Systems", "shape": "dot", "size": 25, "title": "Computer Vision Systems"}, {"color": "#66CCFF", "id": "scene characteristics from images", "label": "scene characteristics from images", "shape": "dot", "size": 25, "title": "scene characteristics from images"}, {"color": "#66CCFF", "id": "Retinex theory", "label": "Retinex theory", "shape": "dot", "size": 25, "title": "Retinex theory"}, {"color": "#66CCFF", "id": "E. H. Land and J. J. McCann", "label": "E. H. Land and J. J. McCann", "shape": "dot", "size": 25, "title": "E. H. Land and J. J. McCann"}, {"color": "#66CCFF", "id": "brightness perception", "label": "brightness perception", "shape": "dot", "size": 25, "title": "brightness perception"}, {"color": "#66CCFF", "id": "determining lightness from an image", "label": "determining lightness from an image", "shape": "dot", "size": 25, "title": "determining lightness from an image"}, {"color": "#66CCFF", "id": "M. Tappen, W. Freeman, and E. Adelson", "label": "M. Tappen, W. Freeman, and E. Adelson", "shape": "dot", "size": 25, "title": "M. Tappen, W. Freeman, and E. Adelson"}, {"color": "#66CCFF", "id": "intrinsic image recovery", "label": "intrinsic image recovery", "shape": "dot", "size": 25, "title": "intrinsic image recovery"}, {"color": "#66CCFF", "id": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 2005", "label": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 2005", "shape": "dot", "size": 25, "title": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 2005"}, {"color": "#66CCFF", "id": "R. Grosse", "label": "R. Grosse", "shape": "dot", "size": 25, "title": "R. Grosse"}, {"color": "#66CCFF", "id": "Ground truth dataset", "label": "Ground truth dataset", "shape": "dot", "size": 25, "title": "Ground truth dataset"}, {"color": "#66CCFF", "id": "International Conference on Computer Vision, 2009", "label": "International Conference on Computer Vision, 2009", "shape": "dot", "size": 25, "title": "International Conference on Computer Vision, 2009"}, {"color": "#66CCFF", "id": "Y. Tang", "label": "Y. Tang", "shape": "dot", "size": 25, "title": "Y. Tang"}, {"color": "#66CCFF", "id": "Deep Lambertian networks", "label": "Deep Lambertian networks", "shape": "dot", "size": 25, "title": "Deep Lambertian networks"}, {"color": "#66CCFF", "id": "Lambertian reflectance models", "label": "Lambertian reflectance models", "shape": "dot", "size": 25, "title": "Lambertian reflectance models"}, {"color": "#66CCFF", "id": "International Conference on Machine Learning, 2012", "label": "International Conference on Machine Learning, 2012", "shape": "dot", "size": 25, "title": "International Conference on Machine Learning, 2012"}, {"color": "#66CCFF", "id": "Lambertian networks", "label": "Lambertian networks", "shape": "dot", "size": 25, "title": "Lambertian networks"}, {"color": "#66CCFF", "id": "deep learning approaches", "label": "deep learning approaches", "shape": "dot", "size": 25, "title": "deep learning approaches"}, {"color": "#66CCFF", "id": "large dataset", "label": "large dataset", "shape": "dot", "size": 25, "title": "large dataset"}, {"color": "#66CCFF", "id": "training computer vision models", "label": "training computer vision models", "shape": "dot", "size": 25, "title": "training computer vision models"}, {"color": "#66CCFF", "id": "deep convolutional neural network", "label": "deep convolutional neural network", "shape": "dot", "size": 25, "title": "deep convolutional neural network"}, {"color": "#66CCFF", "id": "breakthrough performance", "label": "breakthrough performance", "shape": "dot", "size": 25, "title": "breakthrough performance"}, {"color": "#66CCFF", "id": "research in intrinsic image decomposition", "label": "research in intrinsic image decomposition", "shape": "dot", "size": 25, "title": "research in intrinsic image decomposition"}, {"color": "#66CCFF", "id": "Imaginet classification with deep convolutional neural networks", "label": "Imaginet classification with deep convolutional neural networks", "shape": "dot", "size": 25, "title": "Imaginet classification with deep convolutional neural networks"}, {"color": "#66CCFF", "id": "subsequent research", "label": "subsequent research", "shape": "dot", "size": 25, "title": "subsequent research"}, {"color": "#66CCFF", "id": "deep learning framework", "label": "deep learning framework", "shape": "dot", "size": 25, "title": "deep learning framework"}, {"color": "#66CCFF", "id": "Takaya Narihira", "label": "Takaya Narihira", "shape": "dot", "size": 25, "title": "Takaya Narihira"}, {"color": "#66CCFF", "id": "UC Berkeley", "label": "UC Berkeley", "shape": "dot", "size": 25, "title": "UC Berkeley"}, {"color": "#66CCFF", "id": "ICSI", "label": "ICSI", "shape": "dot", "size": 25, "title": "ICSI"}, {"color": "#66CCFF", "id": "Sony Corp.", "label": "Sony Corp.", "shape": "dot", "size": 25, "title": "Sony Corp."}, {"color": "#66CCFF", "id": "TTI Chicago", "label": "TTI Chicago", "shape": "dot", "size": 25, "title": "TTI Chicago"}, {"color": "#66CCFF", "id": "Hierarchical-PEP Model", "label": "Hierarchical-PEP Model", "shape": "dot", "size": 25, "title": "Hierarchical-PEP Model"}, {"color": "#66CCFF", "id": "face recognition", "label": "face recognition", "shape": "dot", "size": 25, "title": "face recognition"}, {"color": "#66CCFF", "id": "Pose variation", "label": "Pose variation", "shape": "dot", "size": 25, "title": "Pose variation"}, {"color": "#66CCFF", "id": "Hierarchical-PEP model", "label": "Hierarchical-PEP model", "shape": "dot", "size": 25, "title": "Hierarchical-PEP model"}, {"color": "#66CCFF", "id": "Probabilistic Elastic Part (PEP) model", "label": "Probabilistic Elastic Part (PEP) model", "shape": "dot", "size": 25, "title": "Probabilistic Elastic Part (PEP) model"}, {"color": "#66CCFF", "id": "Deep Hierarchical Architectures", "label": "Deep Hierarchical Architectures", "shape": "dot", "size": 25, "title": "Deep Hierarchical Architectures"}, {"color": "#66CCFF", "id": "Face Image", "label": "Face Image", "shape": "dot", "size": 25, "title": "Face Image"}, {"color": "#66CCFF", "id": "Face Parts", "label": "Face Parts", "shape": "dot", "size": 25, "title": "Face Parts"}, {"color": "#66CCFF", "id": "Detail Level", "label": "Detail Level", "shape": "dot", "size": 25, "title": "Detail Level"}, {"color": "#66CCFF", "id": "Face Part Representations", "label": "Face Part Representations", "shape": "dot", "size": 25, "title": "Face Part Representations"}, {"color": "#66CCFF", "id": "Layer", "label": "Layer", "shape": "dot", "size": 25, "title": "Layer"}, {"color": "#66CCFF", "id": "Dimensionality", "label": "Dimensionality", "shape": "dot", "size": 25, "title": "Dimensionality"}, {"color": "#66CCFF", "id": "Face Representation", "label": "Face Representation", "shape": "dot", "size": 25, "title": "Face Representation"}, {"color": "#66CCFF", "id": "Invariant", "label": "Invariant", "shape": "dot", "size": 25, "title": "Invariant"}, {"color": "#66CCFF", "id": "Fine-grained Structures", "label": "Fine-grained Structures", "shape": "dot", "size": 25, "title": "Fine-grained Structures"}, {"color": "#66CCFF", "id": "Supervised Information", "label": "Supervised Information", "shape": "dot", "size": 25, "title": "Supervised Information"}, {"color": "#66CCFF", "id": "LFW", "label": "LFW", "shape": "dot", "size": 25, "title": "LFW"}, {"color": "#66CCFF", "id": "YouTube Faces", "label": "YouTube Faces", "shape": "dot", "size": 25, "title": "YouTube Faces"}, {"color": "#66CCFF", "id": "PaSC", "label": "PaSC", "shape": "dot", "size": 25, "title": "PaSC"}, {"color": "#66CCFF", "id": "face parts", "label": "face parts", "shape": "dot", "size": 25, "title": "face parts"}, {"color": "#66CCFF", "id": "supervised information", "label": "supervised information", "shape": "dot", "size": 25, "title": "supervised information"}, {"color": "#66CCFF", "id": "face recognition challenge", "label": "face recognition challenge", "shape": "dot", "size": 25, "title": "face recognition challenge"}, {"color": "#66CCFF", "id": "PEP (Probabilistic Elastic Part) Model", "label": "PEP (Probabilistic Elastic Part) Model", "shape": "dot", "size": 25, "title": "PEP (Probabilistic Elastic Part) Model"}, {"color": "#66CCFF", "id": "Ahonen, T.", "label": "Ahonen, T.", "shape": "dot", "size": 25, "title": "Ahonen, T."}, {"color": "#66CCFF", "id": "Face recognition with local binary patterns", "label": "Face recognition with local binary patterns", "shape": "dot", "size": 25, "title": "Face recognition with local binary patterns"}, {"color": "#66CCFF", "id": "Grauman, K.", "label": "Grauman, K.", "shape": "dot", "size": 25, "title": "Grauman, K."}, {"color": "#66CCFF", "id": "The pyramid match kernel", "label": "The pyramid match kernel", "shape": "dot", "size": 25, "title": "The pyramid match kernel"}, {"color": "#66CCFF", "id": "Hu, J.", "label": "Hu, J.", "shape": "dot", "size": 25, "title": "Hu, J."}, {"color": "#66CCFF", "id": "Discriminative deep metric learning", "label": "Discriminative deep metric learning", "shape": "dot", "size": 25, "title": "Discriminative deep metric learning"}, {"color": "#66CCFF", "id": "Eigenfaces", "label": "Eigenfaces", "shape": "dot", "size": 25, "title": "Eigenfaces"}, {"color": "#66CCFF", "id": "Fisherfaces", "label": "Fisherfaces", "shape": "dot", "size": 25, "title": "Fisherfaces"}, {"color": "#66CCFF", "id": "linear projection", "label": "linear projection", "shape": "dot", "size": 25, "title": "linear projection"}, {"color": "#66CCFF", "id": "Unsupervised joint alignment", "label": "Unsupervised joint alignment", "shape": "dot", "size": 25, "title": "Unsupervised joint alignment"}, {"color": "#66CCFF", "id": "complex images", "label": "complex images", "shape": "dot", "size": 25, "title": "complex images"}, {"color": "#66CCFF", "id": "Labeled faces in the wild", "label": "Labeled faces in the wild", "shape": "dot", "size": 25, "title": "Labeled faces in the wild"}, {"color": "#66CCFF", "id": "reporting procedures", "label": "reporting procedures", "shape": "dot", "size": 25, "title": "reporting procedures"}, {"color": "#66CCFF", "id": "Large margin multi-metric learning", "label": "Large margin multi-metric learning", "shape": "dot", "size": 25, "title": "Large margin multi-metric learning"}, {"color": "#66CCFF", "id": "kinship verification", "label": "kinship verification", "shape": "dot", "size": 25, "title": "kinship verification"}, {"color": "#66CCFF", "id": "Asian Conference on Computer Vision (ACCV)", "label": "Asian Conference on Computer Vision (ACCV)", "shape": "dot", "size": 25, "title": "Asian Conference on Computer Vision (ACCV)"}, {"color": "#66CCFF", "id": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition", "label": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition", "shape": "dot", "size": 25, "title": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"}, {"color": "#66CCFF", "id": "publication", "label": "publication", "shape": "dot", "size": 25, "title": "publication"}, {"color": "#66CCFF", "id": "practical transfer learning algorithm", "label": "practical transfer learning algorithm", "shape": "dot", "size": 25, "title": "practical transfer learning algorithm"}, {"color": "#66CCFF", "id": "Lei, Z.", "label": "Lei, Z.", "shape": "dot", "size": 25, "title": "Lei, Z."}, {"color": "#66CCFF", "id": "discriminant face descriptor", "label": "discriminant face descriptor", "shape": "dot", "size": 25, "title": "discriminant face descriptor"}, {"color": "#66CCFF", "id": "Simonyan, K.", "label": "Simonyan, K.", "shape": "dot", "size": 25, "title": "Simonyan, K."}, {"color": "#66CCFF", "id": "Deep fisher networks", "label": "Deep fisher networks", "shape": "dot", "size": 25, "title": "Deep fisher networks"}, {"color": "#66CCFF", "id": "Yu Kong", "label": "Yu Kong", "shape": "dot", "size": 25, "title": "Yu Kong"}, {"color": "#66CCFF", "id": "Bilinear Heterogeneous Information Machine", "label": "Bilinear Heterogeneous Information Machine", "shape": "dot", "size": 25, "title": "Bilinear Heterogeneous Information Machine"}, {"color": "#66CCFF", "id": "Yun Fu", "label": "Yun Fu", "shape": "dot", "size": 25, "title": "Yun Fu"}, {"color": "#66CCFF", "id": "{ghua}@steverns.edu", "label": "{ghua}@steverns.edu", "shape": "dot", "size": 25, "title": "{ghua}@steverns.edu"}, {"color": "#66CCFF", "id": "Kong_Bilinear_Heterogeneous_Information_2015_CVPR_paper.pdf", "label": "Kong_Bilinear_Heterogeneous_Information_2015_CVPR_paper.pdf", "shape": "dot", "size": 25, "title": "Kong_Bilinear_Heterogeneous_Information_2015_CVPR_paper.pdf"}, {"color": "#66CCFF", "id": "RGB-D Action Recognition", "label": "RGB-D Action Recognition", "shape": "dot", "size": 25, "title": "RGB-D Action Recognition"}, {"color": "#66CCFF", "id": "depth features", "label": "depth features", "shape": "dot", "size": 25, "title": "depth features"}, {"color": "#66CCFF", "id": "RGB visual features", "label": "RGB visual features", "shape": "dot", "size": 25, "title": "RGB visual features"}, {"color": "#66CCFF", "id": "compressed", "label": "compressed", "shape": "dot", "size": 25, "title": "compressed"}, {"color": "#66CCFF", "id": "projected", "label": "projected", "shape": "dot", "size": 25, "title": "projected"}, {"color": "#66CCFF", "id": "space", "label": "space", "shape": "dot", "size": 25, "title": "space"}, {"color": "#66CCFF", "id": "learned", "label": "learned", "shape": "dot", "size": 25, "title": "learned"}, {"color": "#66CCFF", "id": "knowledge", "label": "knowledge", "shape": "dot", "size": 25, "title": "knowledge"}, {"color": "#66CCFF", "id": "shared", "label": "shared", "shape": "dot", "size": 25, "title": "shared"}, {"color": "#66CCFF", "id": "low-rank", "label": "low-rank", "shape": "dot", "size": 25, "title": "low-rank"}, {"color": "#66CCFF", "id": "generalization power", "label": "generalization power", "shape": "dot", "size": 25, "title": "generalization power"}, {"color": "#66CCFF", "id": "RGB-D action datasets", "label": "RGB-D action datasets", "shape": "dot", "size": 25, "title": "RGB-D action datasets"}, {"color": "#66CCFF", "id": "low-rank classifier", "label": "low-rank classifier", "shape": "dot", "size": 25, "title": "low-rank classifier"}, {"color": "#66CCFF", "id": "promising results", "label": "promising results", "shape": "dot", "size": 25, "title": "promising results"}, {"color": "#66CCFF", "id": "RGB data are missing", "label": "RGB data are missing", "shape": "dot", "size": 25, "title": "RGB data are missing"}, {"color": "#66CCFF", "id": "depth data are missing", "label": "depth data are missing", "shape": "dot", "size": 25, "title": "depth data are missing"}, {"color": "#66CCFF", "id": "eter", "label": "eter", "shape": "dot", "size": 25, "title": "eter"}, {"color": "#66CCFF", "id": "missing RGB data", "label": "missing RGB data", "shape": "dot", "size": 25, "title": "missing RGB data"}, {"color": "#66CCFF", "id": "missing depth data", "label": "missing depth data", "shape": "dot", "size": 25, "title": "missing depth data"}, {"color": "#66CCFF", "id": "Argyriou et al. (2008)", "label": "Argyriou et al. (2008)", "shape": "dot", "size": 25, "title": "Argyriou et al. (2008)"}, {"color": "#66CCFF", "id": "feature learning", "label": "feature learning", "shape": "dot", "size": 25, "title": "feature learning"}, {"color": "#66CCFF", "id": "Bo et al. (2011)", "label": "Bo et al. (2011)", "shape": "dot", "size": 25, "title": "Bo et al. (2011)"}, {"color": "#66CCFF", "id": "kernel descriptors", "label": "kernel descriptors", "shape": "dot", "size": 25, "title": "kernel descriptors"}, {"color": "#66CCFF", "id": "Do and Artieres (2009)", "label": "Do and Artieres (2009)", "shape": "dot", "size": 25, "title": "Do and Artieres (2009)"}, {"color": "#66CCFF", "id": "HMMs", "label": "HMMs", "shape": "dot", "size": 25, "title": "HMMs"}, {"color": "#66CCFF", "id": "Artieres et al.", "label": "Artieres et al.", "shape": "dot", "size": 25, "title": "Artieres et al."}, {"color": "#66CCFF", "id": "Hidden Markov Models", "label": "Hidden Markov Models", "shape": "dot", "size": 25, "title": "Hidden Markov Models"}, {"color": "#66CCFF", "id": "Had\ufb01eld and Bowden", "label": "Had\ufb01eld and Bowden", "shape": "dot", "size": 25, "title": "Had\ufb01eld and Bowden"}, {"color": "#66CCFF", "id": "3D natural scenes", "label": "3D natural scenes", "shape": "dot", "size": 25, "title": "3D natural scenes"}, {"color": "#66CCFF", "id": "Ji et al.", "label": "Ji et al.", "shape": "dot", "size": 25, "title": "Ji et al."}, {"color": "#66CCFF", "id": "3D Convolutional Neural Networks", "label": "3D Convolutional Neural Networks", "shape": "dot", "size": 25, "title": "3D Convolutional Neural Networks"}, {"color": "#66CCFF", "id": "depth cameras", "label": "depth cameras", "shape": "dot", "size": 25, "title": "depth cameras"}, {"color": "#66CCFF", "id": "Kobayashi", "label": "Kobayashi", "shape": "dot", "size": 25, "title": "Kobayashi"}, {"color": "#66CCFF", "id": "low-rank bilinear classification", "label": "low-rank bilinear classification", "shape": "dot", "size": 25, "title": "low-rank bilinear classification"}, {"color": "#66CCFF", "id": "modeling complex interactions", "label": "modeling complex interactions", "shape": "dot", "size": 25, "title": "modeling complex interactions"}, {"color": "#66CCFF", "id": "spatio-temporal depth cuboid similarity feature", "label": "spatio-temporal depth cuboid similarity feature", "shape": "dot", "size": 25, "title": "spatio-temporal depth cuboid similarity feature"}, {"color": "#66CCFF", "id": "activity recognition", "label": "activity recognition", "shape": "dot", "size": 25, "title": "activity recognition"}, {"color": "#66CCFF", "id": "information bottleneck method", "label": "information bottleneck method", "shape": "dot", "size": 25, "title": "information bottleneck method"}, {"color": "#66CCFF", "id": "feature selection", "label": "feature selection", "shape": "dot", "size": 25, "title": "feature selection"}, {"color": "#66CCFF", "id": "representation learning", "label": "representation learning", "shape": "dot", "size": 25, "title": "representation learning"}, {"color": "#66CCFF", "id": "depth camera", "label": "depth camera", "shape": "dot", "size": 25, "title": "depth camera"}, {"color": "#66CCFF", "id": "depth sequences", "label": "depth sequences", "shape": "dot", "size": 25, "title": "depth sequences"}, {"color": "#66CCFF", "id": "HON4D", "label": "HON4D", "shape": "dot", "size": 25, "title": "HON4D"}, {"color": "#66CCFF", "id": "oriented 4D normals", "label": "oriented 4D normals", "shape": "dot", "size": 25, "title": "oriented 4D normals"}, {"color": "#66CCFF", "id": "depth data", "label": "depth data", "shape": "dot", "size": 25, "title": "depth data"}, {"color": "#66CCFF", "id": "information bottlenecks", "label": "information bottlenecks", "shape": "dot", "size": 25, "title": "information bottlenecks"}, {"color": "#66CCFF", "id": "Sebastian Haner", "label": "Sebastian Haner", "shape": "dot", "size": 25, "title": "Sebastian Haner"}, {"color": "#66CCFF", "id": "Absolute Pose for Cameras", "label": "Absolute Pose for Cameras", "shape": "dot", "size": 25, "title": "Absolute Pose for Cameras"}, {"color": "#66CCFF", "id": "Kalle \u02daAstr\u00a8om", "label": "Kalle \u02daAstr\u00a8om", "shape": "dot", "size": 25, "title": "Kalle \u02daAstr\u00a8om"}, {"color": "#66CCFF", "id": "refractive interfaces", "label": "refractive interfaces", "shape": "dot", "size": 25, "title": "refractive interfaces"}, {"color": "#66CCFF", "id": "Yukong", "label": "Yukong", "shape": "dot", "size": 25, "title": "Yukong"}, {"color": "#66CCFF", "id": "yukong@ece.neu.edu", "label": "yukong@ece.neu.edu", "shape": "dot", "size": 25, "title": "yukong@ece.neu.edu"}, {"color": "#66CCFF", "id": "yunfu@ece.neu.edu", "label": "yunfu@ece.neu.edu", "shape": "dot", "size": 25, "title": "yunfu@ece.neu.edu"}, {"color": "#66CCFF", "id": "absolute pose of a perspective camera", "label": "absolute pose of a perspective camera", "shape": "dot", "size": 25, "title": "absolute pose of a perspective camera"}, {"color": "#66CCFF", "id": "perspective camera", "label": "perspective camera", "shape": "dot", "size": 25, "title": "perspective camera"}, {"color": "#66CCFF", "id": "scene", "label": "scene", "shape": "dot", "size": 25, "title": "scene"}, {"color": "#66CCFF", "id": "refractive plane", "label": "refractive plane", "shape": "dot", "size": 25, "title": "refractive plane"}, {"color": "#66CCFF", "id": "transparent media", "label": "transparent media", "shape": "dot", "size": 25, "title": "transparent media"}, {"color": "#66CCFF", "id": "solvers", "label": "solvers", "shape": "dot", "size": 25, "title": "solvers"}, {"color": "#66CCFF", "id": "2D cases", "label": "2D cases", "shape": "dot", "size": 25, "title": "2D cases"}, {"color": "#66CCFF", "id": "minimal", "label": "minimal", "shape": "dot", "size": 25, "title": "minimal"}, {"color": "#66CCFF", "id": "Snell\u2019s law", "label": "Snell\u2019s law", "shape": "dot", "size": 25, "title": "Snell\u2019s law"}, {"color": "#66CCFF", "id": "false solutions", "label": "false solutions", "shape": "dot", "size": 25, "title": "false solutions"}, {"color": "#66CCFF", "id": "complexity of problem", "label": "complexity of problem", "shape": "dot", "size": 25, "title": "complexity of problem"}, {"color": "#66CCFF", "id": "synthetic data", "label": "synthetic data", "shape": "dot", "size": 25, "title": "synthetic data"}, {"color": "#66CCFF", "id": "real data", "label": "real data", "shape": "dot", "size": 25, "title": "real data"}, {"color": "#66CCFF", "id": "pose estimates", "label": "pose estimates", "shape": "dot", "size": 25, "title": "pose estimates"}, {"color": "#66CCFF", "id": "explicitly modelling refraction", "label": "explicitly modelling refraction", "shape": "dot", "size": 25, "title": "explicitly modelling refraction"}, {"color": "#66CCFF", "id": "Absolute Pose Estimation", "label": "Absolute Pose Estimation", "shape": "dot", "size": 25, "title": "Absolute Pose Estimation"}, {"color": "#66CCFF", "id": "Refractive Interfaces Modelling", "label": "Refractive Interfaces Modelling", "shape": "dot", "size": 25, "title": "Refractive Interfaces Modelling"}, {"color": "#66CCFF", "id": "Refractive Interfaces", "label": "Refractive Interfaces", "shape": "dot", "size": 25, "title": "Refractive Interfaces"}, {"color": "#66CCFF", "id": "Snell\u0027s Law", "label": "Snell\u0027s Law", "shape": "dot", "size": 25, "title": "Snell\u0027s Law"}, {"color": "#66CCFF", "id": "Structure-and-Motion", "label": "Structure-and-Motion", "shape": "dot", "size": 25, "title": "Structure-and-Motion"}, {"color": "#66CCFF", "id": "Agrawal et al. (2012)", "label": "Agrawal et al. (2012)", "shape": "dot", "size": 25, "title": "Agrawal et al. (2012)"}, {"color": "#66CCFF", "id": "Multi-layer Flat Refractive Geometry Theory", "label": "Multi-layer Flat Refractive Geometry Theory", "shape": "dot", "size": 25, "title": "Multi-layer Flat Refractive Geometry Theory"}, {"color": "#66CCFF", "id": "Byr\u00a8od et al. (2009)", "label": "Byr\u00a8od et al. (2009)", "shape": "dot", "size": 25, "title": "Byr\u00a8od et al. (2009)"}, {"color": "#66CCFF", "id": "Polynomial Equation Solving", "label": "Polynomial Equation Solving", "shape": "dot", "size": 25, "title": "Polynomial Equation Solving"}, {"color": "#66CCFF", "id": "Multi-layer Flat Refractive Geometry", "label": "Multi-layer Flat Refractive Geometry", "shape": "dot", "size": 25, "title": "Multi-layer Flat Refractive Geometry"}, {"color": "#66CCFF", "id": "Pose Estimation Accuracy", "label": "Pose Estimation Accuracy", "shape": "dot", "size": 25, "title": "Pose Estimation Accuracy"}, {"color": "#66CCFF", "id": "Refraction", "label": "Refraction", "shape": "dot", "size": 25, "title": "Refraction"}, {"color": "#66CCFF", "id": "polynomial equation solving", "label": "polynomial equation solving", "shape": "dot", "size": 25, "title": "polynomial equation solving"}, {"color": "#66CCFF", "id": "Ideals, Varieties, and Algorithms", "label": "Ideals, Varieties, and Algorithms", "shape": "dot", "size": 25, "title": "Ideals, Varieties, and Algorithms"}, {"color": "#66CCFF", "id": "computational algebraic geometry", "label": "computational algebraic geometry", "shape": "dot", "size": 25, "title": "computational algebraic geometry"}, {"color": "#66CCFF", "id": "commutative algebra", "label": "commutative algebra", "shape": "dot", "size": 25, "title": "commutative algebra"}, {"color": "#66CCFF", "id": "\u02daAstr\u00a8om, Kuang, \u0026 Ask", "label": "\u02daAstr\u00a8om, Kuang, \u0026 Ask", "shape": "dot", "size": 25, "title": "\u02daAstr\u00a8om, Kuang, \u0026 Ask"}, {"color": "#66CCFF", "id": "polynomial equation solving optimization", "label": "polynomial equation solving optimization", "shape": "dot", "size": 25, "title": "polynomial equation solving optimization"}, {"color": "#66CCFF", "id": "p-fold symmetries", "label": "p-fold symmetries", "shape": "dot", "size": 25, "title": "p-fold symmetries"}, {"color": "#66CCFF", "id": "Chari \u0026 Sturm", "label": "Chari \u0026 Sturm", "shape": "dot", "size": 25, "title": "Chari \u0026 Sturm"}, {"color": "#66CCFF", "id": "multi-view geometry", "label": "multi-view geometry", "shape": "dot", "size": 25, "title": "multi-view geometry"}, {"color": "#66CCFF", "id": "Chari, V.", "label": "Chari, V.", "shape": "dot", "size": 25, "title": "Chari, V."}, {"color": "#66CCFF", "id": "Multi-view geometry of the refractive plane", "label": "Multi-view geometry of the refractive plane", "shape": "dot", "size": 25, "title": "Multi-view geometry of the refractive plane"}, {"color": "#66CCFF", "id": "Sturm, P. F.", "label": "Sturm, P. F.", "shape": "dot", "size": 25, "title": "Sturm, P. F."}, {"color": "#66CCFF", "id": "British Machine Vision Conference", "label": "British Machine Vision Conference", "shape": "dot", "size": 25, "title": "British Machine Vision Conference"}, {"color": "#66CCFF", "id": "Fitzgibbon, A. W.", "label": "Fitzgibbon, A. W.", "shape": "dot", "size": 25, "title": "Fitzgibbon, A. W."}, {"color": "#66CCFF", "id": "Simultaneous linear estimation", "label": "Simultaneous linear estimation", "shape": "dot", "size": 25, "title": "Simultaneous linear estimation"}, {"color": "#66CCFF", "id": "geometric estimation", "label": "geometric estimation", "shape": "dot", "size": 25, "title": "geometric estimation"}, {"color": "#66CCFF", "id": "lens distortion", "label": "lens distortion", "shape": "dot", "size": 25, "title": "lens distortion"}, {"color": "#66CCFF", "id": "Kuang, Y.", "label": "Kuang, Y.", "shape": "dot", "size": 25, "title": "Kuang, Y."}, {"color": "#66CCFF", "id": "Numerically stable optimization", "label": "Numerically stable optimization", "shape": "dot", "size": 25, "title": "Numerically stable optimization"}, {"color": "#66CCFF", "id": "polynomial solvers", "label": "polynomial solvers", "shape": "dot", "size": 25, "title": "polynomial solvers"}, {"color": "#66CCFF", "id": "chmid", "label": "chmid", "shape": "dot", "size": 25, "title": "chmid"}, {"color": "#66CCFF", "id": "European Conference on ComputerVision", "label": "European Conference on ComputerVision", "shape": "dot", "size": 25, "title": "European Conference on ComputerVision"}, {"color": "#66CCFF", "id": "Lecture Notes in Computer Science", "label": "Lecture Notes in Computer Science", "shape": "dot", "size": 25, "title": "Lecture Notes in Computer Science"}, {"color": "#66CCFF", "id": "polynomial solver optimization", "label": "polynomial solver optimization", "shape": "dot", "size": 25, "title": "polynomial solver optimization"}, {"color": "#66CCFF", "id": "Nist\u00e9r", "label": "Nist\u00e9r", "shape": "dot", "size": 25, "title": "Nist\u00e9r"}, {"color": "#66CCFF", "id": "generalized 3-point pose problem", "label": "generalized 3-point pose problem", "shape": "dot", "size": 25, "title": "generalized 3-point pose problem"}, {"color": "#66CCFF", "id": "Stew\u00e9nius", "label": "Stew\u00e9nius", "shape": "dot", "size": 25, "title": "Stew\u00e9nius"}, {"color": "#66CCFF", "id": "generalized relative pose problems", "label": "generalized relative pose problems", "shape": "dot", "size": 25, "title": "generalized relative pose problems"}, {"color": "#66CCFF", "id": "Kukelova", "label": "Kukelova", "shape": "dot", "size": 25, "title": "Kukelova"}, {"color": "#66CCFF", "id": "polynomial eigenvalue solutions", "label": "polynomial eigenvalue solutions", "shape": "dot", "size": 25, "title": "polynomial eigenvalue solutions"}, {"color": "#66CCFF", "id": "minimal problems", "label": "minimal problems", "shape": "dot", "size": 25, "title": "minimal problems"}, {"color": "#66CCFF", "id": "Centre for Mathematical Sciences", "label": "Centre for Mathematical Sciences", "shape": "dot", "size": 25, "title": "Centre for Mathematical Sciences"}, {"color": "#66CCFF", "id": "Lund University", "label": "Lund University", "shape": "dot", "size": 25, "title": "Lund University"}, {"color": "#66CCFF", "id": "Workshop on Omnidirectional Vision", "label": "Workshop on Omnidirectional Vision", "shape": "dot", "size": 25, "title": "Workshop on Omnidirectional Vision"}, {"color": "#66CCFF", "id": "Centre for Mathematical Sciences, Lund University", "label": "Centre for Mathematical Sciences, Lund University", "shape": "dot", "size": 25, "title": "Centre for Mathematical Sciences, Lund University"}, {"color": "#66CCFF", "id": "haner@maths.lth.se", "label": "haner@maths.lth.se", "shape": "dot", "size": 25, "title": "haner@maths.lth.se"}, {"color": "#66CCFF", "id": "kalle@maths.lth.se", "label": "kalle@maths.lth.se", "shape": "dot", "size": 25, "title": "kalle@maths.lth.se"}, {"color": "#66CCFF", "id": "R6P", "label": "R6P", "shape": "dot", "size": 25, "title": "R6P"}, {"color": "#66CCFF", "id": "Albl_R6P_-_Rolling_2015_CVPR_paper", "label": "Albl_R6P_-_Rolling_2015_CVPR_paper", "shape": "dot", "size": 25, "title": "Albl_R6P_-_Rolling_2015_CVPR_paper"}, {"color": "#66CCFF", "id": "Cenek Albl", "label": "Cenek Albl", "shape": "dot", "size": 25, "title": "Cenek Albl"}, {"color": "#66CCFF", "id": "Zuana Kukelova", "label": "Zuana Kukelova", "shape": "dot", "size": 25, "title": "Zuana Kukelova"}, {"color": "#66CCFF", "id": "Albi_R6P_-_Rolling_2015_CVPR_paper", "label": "Albi_R6P_-_Rolling_2015_CVPR_paper", "shape": "dot", "size": 25, "title": "Albi_R6P_-_Rolling_2015_CVPR_paper"}, {"color": "#66CCFF", "id": "Tomas Pajdla", "label": "Tomas Pajdla", "shape": "dot", "size": 25, "title": "Tomas Pajdla"}, {"color": "#66CCFF", "id": "polynomial solutions", "label": "polynomial solutions", "shape": "dot", "size": 25, "title": "polynomial solutions"}, {"color": "#66CCFF", "id": "absolute pose problem", "label": "absolute pose problem", "shape": "dot", "size": 25, "title": "absolute pose problem"}, {"color": "#66CCFF", "id": "rolling shutter", "label": "rolling shutter", "shape": "dot", "size": 25, "title": "rolling shutter"}, {"color": "#66CCFF", "id": "digital cameras", "label": "digital cameras", "shape": "dot", "size": 25, "title": "digital cameras"}, {"color": "#66CCFF", "id": "camera model", "label": "camera model", "shape": "dot", "size": 25, "title": "camera model"}, {"color": "#66CCFF", "id": "polynomial solver", "label": "polynomial solver", "shape": "dot", "size": 25, "title": "polynomial solver"}, {"color": "#66CCFF", "id": "camera orientation", "label": "camera orientation", "shape": "dot", "size": 25, "title": "camera orientation"}, {"color": "#66CCFF", "id": "linear approximation", "label": "linear approximation", "shape": "dot", "size": 25, "title": "linear approximation"}, {"color": "#66CCFF", "id": "identity rotation", "label": "identity rotation", "shape": "dot", "size": 25, "title": "identity rotation"}, {"color": "#66CCFF", "id": "P3P algorithm", "label": "P3P algorithm", "shape": "dot", "size": 25, "title": "P3P algorithm"}, {"color": "#66CCFF", "id": "camera rotation velocity", "label": "camera rotation velocity", "shape": "dot", "size": 25, "title": "camera rotation velocity"}, {"color": "#66CCFF", "id": "30deg/frame", "label": "30deg/frame", "shape": "dot", "size": 25, "title": "30deg/frame"}, {"color": "#66CCFF", "id": "estimate camera orientation", "label": "estimate camera orientation", "shape": "dot", "size": 25, "title": "estimate camera orientation"}, {"color": "#66CCFF", "id": "Ithm", "label": "Ithm", "shape": "dot", "size": 25, "title": "Ithm"}, {"color": "#66CCFF", "id": "6 degrees", "label": "6 degrees", "shape": "dot", "size": 25, "title": "6 degrees"}, {"color": "#66CCFF", "id": "camera rotation matrix", "label": "camera rotation matrix", "shape": "dot", "size": 25, "title": "camera rotation matrix"}, {"color": "#66CCFF", "id": "identity", "label": "identity", "shape": "dot", "size": 25, "title": "identity"}, {"color": "#66CCFF", "id": "camera position", "label": "camera position", "shape": "dot", "size": 25, "title": "camera position"}, {"color": "#66CCFF", "id": "translational velocity", "label": "translational velocity", "shape": "dot", "size": 25, "title": "translational velocity"}, {"color": "#66CCFF", "id": "angular velocity", "label": "angular velocity", "shape": "dot", "size": 25, "title": "angular velocity"}, {"color": "#66CCFF", "id": "2%", "label": "2%", "shape": "dot", "size": 25, "title": "2%"}, {"color": "#66CCFF", "id": "orientation error", "label": "orientation error", "shape": "dot", "size": 25, "title": "orientation error"}, {"color": "#66CCFF", "id": "0.5 degrees", "label": "0.5 degrees", "shape": "dot", "size": 25, "title": "0.5 degrees"}, {"color": "#66CCFF", "id": "number of inliers", "label": "number of inliers", "shape": "dot", "size": 25, "title": "number of inliers"}, {"color": "#66CCFF", "id": "Rolling Shutter Cameras", "label": "Rolling Shutter Cameras", "shape": "dot", "size": 25, "title": "Rolling Shutter Cameras"}, {"color": "#66CCFF", "id": "Absolute Pose Problem", "label": "Absolute Pose Problem", "shape": "dot", "size": 25, "title": "Absolute Pose Problem"}, {"color": "#66CCFF", "id": "Polynomial Solvers", "label": "Polynomial Solvers", "shape": "dot", "size": 25, "title": "Polynomial Solvers"}, {"color": "#66CCFF", "id": "Linearized Camera Models", "label": "Linearized Camera Models", "shape": "dot", "size": 25, "title": "Linearized Camera Models"}, {"color": "#66CCFF", "id": "model fitting", "label": "model fitting", "shape": "dot", "size": 25, "title": "model fitting"}, {"color": "#66CCFF", "id": "computer vision problems", "label": "computer vision problems", "shape": "dot", "size": 25, "title": "computer vision problems"}, {"color": "#66CCFF", "id": "half a degree", "label": "half a degree", "shape": "dot", "size": 25, "title": "half a degree"}, {"color": "#66CCFF", "id": "relative position error", "label": "relative position error", "shape": "dot", "size": 25, "title": "relative position error"}, {"color": "#66CCFF", "id": "RANSAC", "label": "RANSAC", "shape": "dot", "size": 25, "title": "RANSAC"}, {"color": "#66CCFF", "id": "robust model fitting", "label": "robust model fitting", "shape": "dot", "size": 25, "title": "robust model fitting"}, {"color": "#66CCFF", "id": "structure and motion estimation", "label": "structure and motion estimation", "shape": "dot", "size": 25, "title": "structure and motion estimation"}, {"color": "#66CCFF", "id": "Haralick", "label": "Haralick", "shape": "dot", "size": 25, "title": "Haralick"}, {"color": "#66CCFF", "id": "Hedborg", "label": "Hedborg", "shape": "dot", "size": 25, "title": "Hedborg"}, {"color": "#66CCFF", "id": "RANAC", "label": "RANAC", "shape": "dot", "size": 25, "title": "RANAC"}, {"color": "#66CCFF", "id": "rolling shutter video", "label": "rolling shutter video", "shape": "dot", "size": 25, "title": "rolling shutter video"}, {"color": "#66CCFF", "id": "rolling shutter data", "label": "rolling shutter data", "shape": "dot", "size": 25, "title": "rolling shutter data"}, {"color": "#66CCFF", "id": "bundle adjustment", "label": "bundle adjustment", "shape": "dot", "size": 25, "title": "bundle adjustment"}, {"color": "#66CCFF", "id": "C. Jia and B. L. Evans", "label": "C. Jia and B. L. Evans", "shape": "dot", "size": 25, "title": "C. Jia and B. L. Evans"}, {"color": "#66CCFF", "id": "inertial measurements", "label": "inertial measurements", "shape": "dot", "size": 25, "title": "inertial measurements"}, {"color": "#66CCFF", "id": "motion estimation", "label": "motion estimation", "shape": "dot", "size": 25, "title": "motion estimation"}, {"color": "#66CCFF", "id": "ter video recti\ufb01cation", "label": "ter video recti\ufb01cation", "shape": "dot", "size": 25, "title": "ter video recti\ufb01cation"}, {"color": "#66CCFF", "id": "visual SLAM", "label": "visual SLAM", "shape": "dot", "size": 25, "title": "visual SLAM"}, {"color": "#66CCFF", "id": "parallel tracking and mapping", "label": "parallel tracking and mapping", "shape": "dot", "size": 25, "title": "parallel tracking and mapping"}, {"color": "#66CCFF", "id": "ISMAR \u201909", "label": "ISMAR \u201909", "shape": "dot", "size": 25, "title": "ISMAR \u201909"}, {"color": "#66CCFF", "id": "EEE International Symposium on Mixed and augmented Reality", "label": "EEE International Symposium on Mixed and augmented Reality", "shape": "dot", "size": 25, "title": "EEE International Symposium on Mixed and augmented Reality"}, {"color": "#66CCFF", "id": "Z. Kukelova", "label": "Z. Kukelova", "shape": "dot", "size": 25, "title": "Z. Kukelova"}, {"color": "#66CCFF", "id": "Singly-bordered block-diagonal form", "label": "Singly-bordered block-diagonal form", "shape": "dot", "size": 25, "title": "Singly-bordered block-diagonal form"}, {"color": "#66CCFF", "id": "efficient computation", "label": "efficient computation", "shape": "dot", "size": 25, "title": "efficient computation"}, {"color": "#66CCFF", "id": "Automatic generator of minimal problem solvers", "label": "Automatic generator of minimal problem solvers", "shape": "dot", "size": 25, "title": "Automatic generator of minimal problem solvers"}, {"color": "#66CCFF", "id": "efficient solvers", "label": "efficient solvers", "shape": "dot", "size": 25, "title": "efficient solvers"}, {"color": "#66CCFF", "id": "ECCV 2008", "label": "ECCV 2008", "shape": "dot", "size": 25, "title": "ECCV 2008"}, {"color": "#66CCFF", "id": "Computer Vision - ECCV 2008", "label": "Computer Vision - ECCV 2008", "shape": "dot", "size": 25, "title": "Computer Vision - ECCV 2008"}, {"color": "#66CCFF", "id": "Proceedings", "label": "Proceedings", "shape": "dot", "size": 25, "title": "Proceedings"}, {"color": "#66CCFF", "id": "5304", "label": "5304", "shape": "dot", "size": 25, "title": "5304"}, {"color": "#66CCFF", "id": "Cox", "label": "Cox", "shape": "dot", "size": 25, "title": "Cox"}, {"color": "#66CCFF", "id": "Using Algebraic Geometry", "label": "Using Algebraic Geometry", "shape": "dot", "size": 25, "title": "Using Algebraic Geometry"}, {"color": "#66CCFF", "id": "Springer", "label": "Springer", "shape": "dot", "size": 25, "title": "Springer"}, {"color": "#66CCFF", "id": "Czech Technical University in Prague", "label": "Czech Technical University in Prague", "shape": "dot", "size": 25, "title": "Czech Technical University in Prague"}, {"color": "#66CCFF", "id": "Microsoft Research Ltd", "label": "Microsoft Research Ltd", "shape": "dot", "size": 25, "title": "Microsoft Research Ltd"}, {"color": "#66CCFF", "id": "Sridhar", "label": "Sridhar", "shape": "dot", "size": 25, "title": "Sridhar"}, {"color": "#66CCFF", "id": "Fast and Robust Hand Tracking", "label": "Fast and Robust Hand Tracking", "shape": "dot", "size": 25, "title": "Fast and Robust Hand Tracking"}, {"color": "#66CCFF", "id": "Detection-Guided Optimization", "label": "Detection-Guided Optimization", "shape": "dot", "size": 25, "title": "Detection-Guided Optimization"}, {"color": "#66CCFF", "id": "Theobalt", "label": "Theobalt", "shape": "dot", "size": 25, "title": "Theobalt"}, {"color": "#66CCFF", "id": "tracking inaccuracies", "label": "tracking inaccuracies", "shape": "dot", "size": 25, "title": "tracking inaccuracies"}, {"color": "#66CCFF", "id": "detection-guided optimization strategy", "label": "detection-guided optimization strategy", "shape": "dot", "size": 25, "title": "detection-guided optimization strategy"}, {"color": "#66CCFF", "id": "model-based generative tracking", "label": "model-based generative tracking", "shape": "dot", "size": 25, "title": "model-based generative tracking"}, {"color": "#66CCFF", "id": "discriminative hand pose detection", "label": "discriminative hand pose detection", "shape": "dot", "size": 25, "title": "discriminative hand pose detection"}, {"color": "#66CCFF", "id": "high efficiency", "label": "high efficiency", "shape": "dot", "size": 25, "title": "high efficiency"}, {"color": "#66CCFF", "id": "robust performance", "label": "robust performance", "shape": "dot", "size": 25, "title": "robust performance"}, {"color": "#66CCFF", "id": "varying camera-to-scene arrangements", "label": "varying camera-to-scene arrangements", "shape": "dot", "size": 25, "title": "varying camera-to-scene arrangements"}, {"color": "#66CCFF", "id": "mutual failures", "label": "mutual failures", "shape": "dot", "size": 25, "title": "mutual failures"}, {"color": "#66CCFF", "id": "50 fps", "label": "50 fps", "shape": "dot", "size": 25, "title": "50 fps"}, {"color": "#66CCFF", "id": "Baak et al.", "label": "Baak et al.", "shape": "dot", "size": 25, "title": "Baak et al."}, {"color": "#66CCFF", "id": "full body pose reconstruction", "label": "full body pose reconstruction", "shape": "dot", "size": 25, "title": "full body pose reconstruction"}, {"color": "#66CCFF", "id": "Ballan et al.", "label": "Ballan et al.", "shape": "dot", "size": 25, "title": "Ballan et al."}, {"color": "#66CCFF", "id": "motion capture of hands", "label": "motion capture of hands", "shape": "dot", "size": 25, "title": "motion capture of hands"}, {"color": "#66CCFF", "id": "Bhattacharyya", "label": "Bhattacharyya", "shape": "dot", "size": 25, "title": "Bhattacharyya"}, {"color": "#66CCFF", "id": "measure of divergence", "label": "measure of divergence", "shape": "dot", "size": 25, "title": "measure of divergence"}, {"color": "#66CCFF", "id": "Criminisi and Shotton", "label": "Criminisi and Shotton", "shape": "dot", "size": 25, "title": "Criminisi and Shotton"}, {"color": "#66CCFF", "id": "Decision forests", "label": "Decision forests", "shape": "dot", "size": 25, "title": "Decision forests"}, {"color": "#66CCFF", "id": "Srinath Srilhar", "label": "Srinath Srilhar", "shape": "dot", "size": 25, "title": "Srinath Srilhar"}, {"color": "#66CCFF", "id": "A. Criminisi", "label": "A. Criminisi", "shape": "dot", "size": 25, "title": "A. Criminisi"}, {"color": "#66CCFF", "id": "Decision forests for computer vision", "label": "Decision forests for computer vision", "shape": "dot", "size": 25, "title": "Decision forests for computer vision"}, {"color": "#66CCFF", "id": "J. Shotton", "label": "J. Shotton", "shape": "dot", "size": 25, "title": "J. Shotton"}, {"color": "#66CCFF", "id": "S. R. Fanello", "label": "S. R. Fanello", "shape": "dot", "size": 25, "title": "S. R. Fanello"}, {"color": "#66CCFF", "id": "Learning to be a depth camera", "label": "Learning to be a depth camera", "shape": "dot", "size": 25, "title": "Learning to be a depth camera"}, {"color": "#66CCFF", "id": "Efficient regression of general-activity human poses", "label": "Efficient regression of general-activity human poses", "shape": "dot", "size": 25, "title": "Efficient regression of general-activity human poses"}, {"color": "#66CCFF", "id": "H. Hamer", "label": "H. Hamer", "shape": "dot", "size": 25, "title": "H. Hamer"}, {"color": "#66CCFF", "id": "Tracking a hand manipulating an object", "label": "Tracking a hand manipulating an object", "shape": "dot", "size": 25, "title": "Tracking a hand manipulating an object"}, {"color": "#66CCFF", "id": "C. Keskin", "label": "C. Keskin", "shape": "dot", "size": 25, "title": "C. Keskin"}, {"color": "#66CCFF", "id": "Real time hand pose estimation", "label": "Real time hand pose estimation", "shape": "dot", "size": 25, "title": "Real time hand pose estimation"}, {"color": "#66CCFF", "id": "ICCV Workshops", "label": "ICCV Workshops", "shape": "dot", "size": 25, "title": "ICCV Workshops"}, {"color": "#66CCFF", "id": "F. Kirac", "label": "F. Kirac", "shape": "dot", "size": 25, "title": "F. Kirac"}, {"color": "#66CCFF", "id": "Y. Kara", "label": "Y. Kara", "shape": "dot", "size": 25, "title": "Y. Kara"}, {"color": "#66CCFF", "id": "L. Akarun", "label": "L. Akarun", "shape": "dot", "size": 25, "title": "L. Akarun"}, {"color": "#66CCFF", "id": "Srinath Sridhar", "label": "Srinath Sridhar", "shape": "dot", "size": 25, "title": "Srinath Sridhar"}, {"color": "#66CCFF", "id": "Antti Oulasvirta", "label": "Antti Oulasvirta", "shape": "dot", "size": 25, "title": "Antti Oulasvirta"}, {"color": "#66CCFF", "id": "Aalto University", "label": "Aalto University", "shape": "dot", "size": 25, "title": "Aalto University"}, {"color": "#66CCFF", "id": "Fumin Shen", "label": "Fumin Shen", "shape": "dot", "size": 25, "title": "Fumin Shen"}, {"color": "#66CCFF", "id": "Supervised Discrete Hashing", "label": "Supervised Discrete Hashing", "shape": "dot", "size": 25, "title": "Supervised Discrete Hashing"}, {"color": "#66CCFF", "id": "Heng Tao Shen", "label": "Heng Tao Shen", "shape": "dot", "size": 25, "title": "Heng Tao Shen"}, {"color": "#66CCFF", "id": "Supervised Discrete Hanning (SDH)", "label": "Supervised Discrete Hanning (SDH)", "shape": "dot", "size": 25, "title": "Supervised Discrete Hanning (SDH)"}, {"color": "#66CCFF", "id": "hashing framework", "label": "hashing framework", "shape": "dot", "size": 25, "title": "hashing framework"}, {"color": "#66CCFF", "id": "linear classification", "label": "linear classification", "shape": "dot", "size": 25, "title": "linear classification"}, {"color": "#66CCFF", "id": "handling discrete constraints", "label": "handling discrete constraints", "shape": "dot", "size": 25, "title": "handling discrete constraints"}, {"color": "#66CCFF", "id": "NP-hard optimization problems", "label": "NP-hard optimization problems", "shape": "dot", "size": 25, "title": "NP-hard optimization problems"}, {"color": "#66CCFF", "id": "objective", "label": "objective", "shape": "dot", "size": 25, "title": "objective"}, {"color": "#66CCFF", "id": "introducing an auxiliary variable and regularization algorithm", "label": "introducing an auxiliary variable and regularization algorithm", "shape": "dot", "size": 25, "title": "introducing an auxiliary variable and regularization algorithm"}, {"color": "#66CCFF", "id": "cyclic coordinate descent", "label": "cyclic coordinate descent", "shape": "dot", "size": 25, "title": "cyclic coordinate descent"}, {"color": "#66CCFF", "id": "regularization sub-problem", "label": "regularization sub-problem", "shape": "dot", "size": 25, "title": "regularization sub-problem"}, {"color": "#66CCFF", "id": "SDH", "label": "SDH", "shape": "dot", "size": 25, "title": "SDH"}, {"color": "#66CCFF", "id": "high-quality discrete solutions", "label": "high-quality discrete solutions", "shape": "dot", "size": 25, "title": "high-quality discrete solutions"}, {"color": "#66CCFF", "id": "handling of massive datasets", "label": "handling of massive datasets", "shape": "dot", "size": 25, "title": "handling of massive datasets"}, {"color": "#66CCFF", "id": "four large image datasets", "label": "four large image datasets", "shape": "dot", "size": 25, "title": "four large image datasets"}, {"color": "#66CCFF", "id": "Hashing", "label": "Hashing", "shape": "dot", "size": 25, "title": "Hashing"}, {"color": "#66CCFF", "id": "p-stable distributions", "label": "p-stable distributions", "shape": "dot", "size": 25, "title": "p-stable distributions"}, {"color": "#66CCFF", "id": "hashing technique", "label": "hashing technique", "shape": "dot", "size": 25, "title": "hashing technique"}, {"color": "#66CCFF", "id": "bilinear projections", "label": "bilinear projections", "shape": "dot", "size": 25, "title": "bilinear projections"}, {"color": "#66CCFF", "id": "learning binary codes", "label": "learning binary codes", "shape": "dot", "size": 25, "title": "learning binary codes"}, {"color": "#66CCFF", "id": "Belkin, M., \u0026 Niyogi, P.", "label": "Belkin, M., \u0026 Niyogi, P.", "shape": "dot", "size": 25, "title": "Belkin, M., \u0026 Niyogi, P."}, {"color": "#66CCFF", "id": "foundational paper", "label": "foundational paper", "shape": "dot", "size": 25, "title": "foundational paper"}, {"color": "#66CCFF", "id": "Datar, N., et al.", "label": "Datar, N., et al.", "shape": "dot", "size": 25, "title": "Datar, N., et al."}, {"color": "#66CCFF", "id": "Gong, Y., et al.", "label": "Gong, Y., et al.", "shape": "dot", "size": 25, "title": "Gong, Y., et al."}, {"color": "#66CCFF", "id": "Rowley et al. (2013) paper", "label": "Rowley et al. (2013) paper", "shape": "dot", "size": 25, "title": "Rowley et al. (2013) paper"}, {"color": "#66CCFF", "id": "Weiss et al. (2008) paper", "label": "Weiss et al. (2008) paper", "shape": "dot", "size": 25, "title": "Weiss et al. (2008) paper"}, {"color": "#66CCFF", "id": "spectral hashing", "label": "spectral hashing", "shape": "dot", "size": 25, "title": "spectral hashing"}, {"color": "#66CCFF", "id": "contribution to field", "label": "contribution to field", "shape": "dot", "size": 25, "title": "contribution to field"}, {"color": "#66CCFF", "id": "Gong et al. (2013) paper", "label": "Gong et al. (2013) paper", "shape": "dot", "size": 25, "title": "Gong et al. (2013) paper"}, {"color": "#66CCFF", "id": "iterative quantization approach", "label": "iterative quantization approach", "shape": "dot", "size": 25, "title": "iterative quantization approach"}, {"color": "#66CCFF", "id": "procustean approach", "label": "procustean approach", "shape": "dot", "size": 25, "title": "procustean approach"}, {"color": "#66CCFF", "id": "Kulis \u0026 Darrell (2009) paper", "label": "Kulis \u0026 Darrell (2009) paper", "shape": "dot", "size": 25, "title": "Kulis \u0026 Darrell (2009) paper"}, {"color": "#66CCFF", "id": "method for learning to hash", "label": "method for learning to hash", "shape": "dot", "size": 25, "title": "method for learning to hash"}, {"color": "#66CCFF", "id": "binary reconstructive embeddings", "label": "binary reconstructive embeddings", "shape": "dot", "size": 25, "title": "binary reconstructive embeddings"}, {"color": "#66CCFF", "id": "Kulis \u0026 Darrell (2009)", "label": "Kulis \u0026 Darrell (2009)", "shape": "dot", "size": 25, "title": "Kulis \u0026 Darrell (2009)"}, {"color": "#66CCFF", "id": "binary reconstructive embeddings hashing method", "label": "binary reconstructive embeddings hashing method", "shape": "dot", "size": 25, "title": "binary reconstructive embeddings hashing method"}, {"color": "#66CCFF", "id": "Liu, Wang, Kumar, \u0026 Chang (2011)", "label": "Liu, Wang, Kumar, \u0026 Chang (2011)", "shape": "dot", "size": 25, "title": "Liu, Wang, Kumar, \u0026 Chang (2011)"}, {"color": "#66CCFF", "id": "hashing techniques", "label": "hashing techniques", "shape": "dot", "size": 25, "title": "hashing techniques"}, {"color": "#66CCFF", "id": "graph structures", "label": "graph structures", "shape": "dot", "size": 25, "title": "graph structures"}, {"color": "#66CCFF", "id": "Wang, Kumar, \u0026 Chang (2012)", "label": "Wang, Kumar, \u0026 Chang (2012)", "shape": "dot", "size": 25, "title": "Wang, Kumar, \u0026 Chang (2012)"}, {"color": "#66CCFF", "id": "hashing", "label": "hashing", "shape": "dot", "size": 25, "title": "hashing"}, {"color": "#66CCFF", "id": "semi-supervised setting", "label": "semi-supervised setting", "shape": "dot", "size": 25, "title": "semi-supervised setting"}, {"color": "#66CCFF", "id": "Shen \u0026 Hao (2011)", "label": "Shen \u0026 Hao (2011)", "shape": "dot", "size": 25, "title": "Shen \u0026 Hao (2011)"}, {"color": "#66CCFF", "id": "learning and classification", "label": "learning and classification", "shape": "dot", "size": 25, "title": "learning and classification"}, {"color": "#66CCFF", "id": "Norouzi \u0026 Blei (2011)", "label": "Norouzi \u0026 Blei (2011)", "shape": "dot", "size": 25, "title": "Norouzi \u0026 Blei (2011)"}, {"color": "#66CCFF", "id": "minimal loss hashing", "label": "minimal loss hashing", "shape": "dot", "size": 25, "title": "minimal loss hashing"}, {"color": "#66CCFF", "id": "Blei", "label": "Blei", "shape": "dot", "size": 25, "title": "Blei"}, {"color": "#66CCFF", "id": "University of Electronic Science and Technology of China", "label": "University of Electronic Science and Technology of China", "shape": "dot", "size": 25, "title": "University of Electronic Science and Technology of China"}, {"color": "#66CCFF", "id": "IBM Research", "label": "IBM Research", "shape": "dot", "size": 25, "title": "IBM Research"}, {"color": "#66CCFF", "id": "The University of Queensland", "label": "The University of Queensland", "shape": "dot", "size": 25, "title": "The University of Queensland"}, {"color": "#66CCFF", "id": "A Maximum Entropy Feature Descriptor", "label": "A Maximum Entropy Feature Descriptor", "shape": "dot", "size": 25, "title": "A Maximum Entropy Feature Descriptor"}, {"color": "#66CCFF", "id": "Li, Zhifeng", "label": "Li, Zhifeng", "shape": "dot", "size": 25, "title": "Li, Zhifeng"}, {"color": "#66CCFF", "id": "Tao, Dacheng", "label": "Tao, Dacheng", "shape": "dot", "size": 25, "title": "Tao, Dacheng"}, {"color": "#66CCFF", "id": "Liu, Jianzhang", "label": "Liu, Jianzhang", "shape": "dot", "size": 25, "title": "Liu, Jianzhang"}, {"color": "#66CCFF", "id": "Li, Xuelong", "label": "Li, Xuelong", "shape": "dot", "size": 25, "title": "Li, Xuelong"}, {"color": "#66CCFF", "id": "Age Invariant Face Recognition", "label": "Age Invariant Face Recognition", "shape": "dot", "size": 25, "title": "Age Invariant Face Recognition"}, {"color": "#66CCFF", "id": "age invariant face recognition", "label": "age invariant face recognition", "shape": "dot", "size": 25, "title": "age invariant face recognition"}, {"color": "#66CCFF", "id": "maximum entropy feature descriptor", "label": "maximum entropy feature descriptor", "shape": "dot", "size": 25, "title": "maximum entropy feature descriptor"}, {"color": "#66CCFF", "id": "microstructure", "label": "microstructure", "shape": "dot", "size": 25, "title": "microstructure"}, {"color": "#66CCFF", "id": "discrete codes", "label": "discrete codes", "shape": "dot", "size": 25, "title": "discrete codes"}, {"color": "#66CCFF", "id": "sampling", "label": "sampling", "shape": "dot", "size": 25, "title": "sampling"}, {"color": "#66CCFF", "id": "discriminatory information", "label": "discriminatory information", "shape": "dot", "size": 25, "title": "discriminatory information"}, {"color": "#66CCFF", "id": "identity factor analysis", "label": "identity factor analysis", "shape": "dot", "size": 25, "title": "identity factor analysis"}, {"color": "#66CCFF", "id": "probability of same identity", "label": "probability of same identity", "shape": "dot", "size": 25, "title": "probability of same identity"}, {"color": "#66CCFF", "id": "experimentation", "label": "experimentation", "shape": "dot", "size": 25, "title": "experimentation"}, {"color": "#66CCFF", "id": "MORPH dataset", "label": "MORPH dataset", "shape": "dot", "size": 25, "title": "MORPH dataset"}, {"color": "#66CCFF", "id": "FGNET dataset", "label": "FGNET dataset", "shape": "dot", "size": 25, "title": "FGNET dataset"}, {"color": "#66CCFF", "id": "feature descriptor", "label": "feature descriptor", "shape": "dot", "size": 25, "title": "feature descriptor"}, {"color": "#66CCFF", "id": "MORPH", "label": "MORPH", "shape": "dot", "size": 25, "title": "MORPH"}, {"color": "#66CCFF", "id": "generalizability", "label": "generalizability", "shape": "dot", "size": 25, "title": "generalizability"}, {"color": "#66CCFF", "id": "face aging dataset", "label": "face aging dataset", "shape": "dot", "size": 25, "title": "face aging dataset"}, {"color": "#66CCFF", "id": "FGNET", "label": "FGNET", "shape": "dot", "size": 25, "title": "FGNET"}, {"color": "#66CCFF", "id": "Age Invariant Face Recognition (AIFR)", "label": "Age Invariant Face Recognition (AIFR)", "shape": "dot", "size": 25, "title": "Age Invariant Face Recognition (AIFR)"}, {"color": "#66CCFF", "id": "Age Incompliant Face Recognition (AIFR)", "label": "Age Incompliant Face Recognition (AIFR)", "shape": "dot", "size": 25, "title": "Age Incompliant Face Recognition (AIFR)"}, {"color": "#66CCFF", "id": "Maximum Entropy Feature Descriptor (MEFD)", "label": "Maximum Entropy Feature Descriptor (MEFD)", "shape": "dot", "size": 25, "title": "Maximum Entropy Feature Descriptor (MEFD)"}, {"color": "#66CCFF", "id": "Identity Factor Analysis (IFA)", "label": "Identity Factor Analysis (IFA)", "shape": "dot", "size": 25, "title": "Identity Factor Analysis (IFA)"}, {"color": "#66CCFF", "id": "Nonparametric Discriminent Analysis for Face Recognition", "label": "Nonparametric Discriminent Analysis for Face Recognition", "shape": "dot", "size": 25, "title": "Nonparametric Discriminent Analysis for Face Recognition"}, {"color": "#66CCFF", "id": "Wang, Xiaogang", "label": "Wang, Xiaogang", "shape": "dot", "size": 25, "title": "Wang, Xiaogang"}, {"color": "#66CCFF", "id": "A unified framework for subspace face recognition", "label": "A unified framework for subspace face recognition", "shape": "dot", "size": 25, "title": "A unified framework for subspace face recognition"}, {"color": "#66CCFF", "id": "Li, Unsang", "label": "Li, Unsang", "shape": "dot", "size": 25, "title": "Li, Unsang"}, {"color": "#66CCFF", "id": "A discriminative model for age invariant face recognition", "label": "A discriminative model for age invariant face recognition", "shape": "dot", "size": 25, "title": "A discriminative model for age invariant face recognition"}, {"color": "#66CCFF", "id": "IEEE Trans. Pattern Anal. Mach. Intell.", "label": "IEEE Trans. Pattern Anal. Mach. Intell.", "shape": "dot", "size": 25, "title": "IEEE Trans. Pattern Anal. Mach. Intell."}, {"color": "#66CCFF", "id": "IEEE Transactions on Information Forensics and Security", "label": "IEEE Transactions on Information Forensics and Security", "shape": "dot", "size": 25, "title": "IEEE Transactions on Information Forensics and Security"}, {"color": "#66CCFF", "id": "Park, Unsav", "label": "Park, Unsav", "shape": "dot", "size": 25, "title": "Park, Unsav"}, {"color": "#66CCFF", "id": "Belhumeur, Peter N.", "label": "Belhumeur, Peter N.", "shape": "dot", "size": 25, "title": "Belhumeur, Peter N."}, {"color": "#66CCFF", "id": "Gong, D.", "label": "Gong, D.", "shape": "dot", "size": 25, "title": "Gong, D."}, {"color": "#66CCFF", "id": "ICCV 2013", "label": "ICCV 2013", "shape": "dot", "size": 25, "title": "ICCV 2013"}, {"color": "#66CCFF", "id": "Huang, G.B.", "label": "Huang, G.B.", "shape": "dot", "size": 25, "title": "Huang, G.B."}, {"color": "#66CCFF", "id": "Labelled faces in the wild", "label": "Labelled faces in the wild", "shape": "dot", "size": 25, "title": "Labelled faces in the wild"}, {"color": "#66CCFF", "id": "Zhifeng Li", "label": "Zhifeng Li", "shape": "dot", "size": 25, "title": "Zhifeng Li"}, {"color": "#66CCFF", "id": "Yiying Tong", "label": "Yiying Tong", "shape": "dot", "size": 25, "title": "Yiying Tong"}, {"color": "#66CCFF", "id": "face recognition in unconstrained environments", "label": "face recognition in unconstrained environments", "shape": "dot", "size": 25, "title": "face recognition in unconstrained environments"}, {"color": "#66CCFF", "id": "Technical Report 07-49", "label": "Technical Report 07-49", "shape": "dot", "size": 25, "title": "Technical Report 07-49"}, {"color": "#66CCFF", "id": "Nonparametric Discriminant Analysis for Face Recognition", "label": "Nonparametric Discriminant Analysis for Face Recognition", "shape": "dot", "size": 25, "title": "Nonparametric Discriminant Analysis for Face Recognition"}, {"color": "#66CCFF", "id": "Random sampling LDA", "label": "Random sampling LDA", "shape": "dot", "size": 25, "title": "Random sampling LDA"}, {"color": "#66CCFF", "id": "DiHong Gong", "label": "DiHong Gong", "shape": "dot", "size": 25, "title": "DiHong Gong"}, {"color": "#66CCFF", "id": "Shenzhen Key Lab of Computer Vision and Pattern Recognition", "label": "Shenzhen Key Lab of Computer Vision and Pattern Recognition", "shape": "dot", "size": 25, "title": "Shenzhen Key Lab of Computer Vision and Pattern Recognition"}, {"color": "#66CCFF", "id": "Dihong Gong", "label": "Dihong Gong", "shape": "dot", "size": 25, "title": "Dihong Gong"}, {"color": "#66CCFF", "id": "Shenzhen Key Lab of Computer Vision and Pattern Recogniton", "label": "Shenzhen Key Lab of Computer Vision and Pattern Recogniton", "shape": "dot", "size": 25, "title": "Shenzhen Key Lab of Computer Vision and Pattern Recogniton"}, {"color": "#66CCFF", "id": "Dacheng Tao", "label": "Dacheng Tao", "shape": "dot", "size": 25, "title": "Dacheng Tao"}, {"color": "#66CCFF", "id": "University of Technology, Sydney", "label": "University of Technology, Sydney", "shape": "dot", "size": 25, "title": "University of Technology, Sydney"}, {"color": "#66CCFF", "id": "Jianzhuang Liu", "label": "Jianzhuang Liu", "shape": "dot", "size": 25, "title": "Jianzhuang Liu"}, {"color": "#66CCFF", "id": "Dept. of Information Engineering", "label": "Dept. of Information Engineering", "shape": "dot", "size": 25, "title": "Dept. of Information Engineering"}, {"color": "#66CCFF", "id": "Huawei Technologies Co. Ltd.", "label": "Huawei Technologies Co. Ltd.", "shape": "dot", "size": 25, "title": "Huawei Technologies Co. Ltd."}, {"color": "#66CCFF", "id": "Xuelong Li", "label": "Xuelong Li", "shape": "dot", "size": 25, "title": "Xuelong Li"}, {"color": "#66CCFF", "id": "Xi\u0027an Institute of Optics and Precision Mechanics", "label": "Xi\u0027an Institute of Optics and Precision Mechanics", "shape": "dot", "size": 25, "title": "Xi\u0027an Institute of Optics and Precision Mechanics"}, {"color": "#66CCFF", "id": "Sayed Hossein Khatoonabadi", "label": "Sayed Hossein Khatoonabadi", "shape": "dot", "size": 25, "title": "Sayed Hossein Khatoonabadi"}, {"color": "#66CCFF", "id": "Sayed Hosheen Khatoonabadi", "label": "Sayed Hosheen Khatoonabadi", "shape": "dot", "size": 25, "title": "Sayed Hosheen Khatoonabadi"}, {"color": "#66CCFF", "id": "How Many Bits Does It Take for a Stimulus to Be Salient?", "label": "How Many Bits Does It Take for a Stimulus to Be Salient?", "shape": "dot", "size": 25, "title": "How Many Bits Does It Take for a Stimulus to Be Salient?"}, {"color": "#66CCFF", "id": "Nuno Vasconcelos", "label": "Nuno Vasconcelos", "shape": "dot", "size": 25, "title": "Nuno Vasconcelos"}, {"color": "#66CCFF", "id": "Yuifeng Shan", "label": "Yuifeng Shan", "shape": "dot", "size": 25, "title": "Yuifeng Shan"}, {"color": "#66CCFF", "id": "Khatoonabadi_How_Many_Bits_2015_CVPR_paper.pdf", "label": "Khatoonabadi_How_Many_Bits_2015_CVPR_paper.pdf", "shape": "dot", "size": 25, "title": "Khatoonabadi_How_Many_Bits_2015_CVPR_paper.pdf"}, {"color": "#66CCFF", "id": "hanics", "label": "hanics", "shape": "dot", "size": 25, "title": "hanics"}, {"color": "#66CCFF", "id": "xuelong_li@opt.ac.cn", "label": "xuelong_li@opt.ac.cn", "shape": "dot", "size": 25, "title": "xuelong_li@opt.ac.cn"}, {"color": "#66CCFF", "id": "Stimulus", "label": "Stimulus", "shape": "dot", "size": 25, "title": "Stimulus"}, {"color": "#66CCFF", "id": "computational models", "label": "computational models", "shape": "dot", "size": 25, "title": "computational models"}, {"color": "#66CCFF", "id": "early approaches", "label": "early approaches", "shape": "dot", "size": 25, "title": "early approaches"}, {"color": "#66CCFF", "id": "center-surround filters", "label": "center-surround filters", "shape": "dot", "size": 25, "title": "center-surround filters"}, {"color": "#66CCFF", "id": "recent works", "label": "recent works", "shape": "dot", "size": 25, "title": "recent works"}, {"color": "#66CCFF", "id": "general computational principles", "label": "general computational principles", "shape": "dot", "size": 25, "title": "general computational principles"}, {"color": "#66CCFF", "id": "measure of salience", "label": "measure of salience", "shape": "dot", "size": 25, "title": "measure of salience"}, {"color": "#66CCFF", "id": "bits required by video compressor", "label": "bits required by video compressor", "shape": "dot", "size": 25, "title": "bits required by video compressor"}, {"color": "#66CCFF", "id": "predictive power", "label": "predictive power", "shape": "dot", "size": 25, "title": "predictive power"}, {"color": "#66CCFF", "id": "global salience effects", "label": "global salience effects", "shape": "dot", "size": 25, "title": "global salience effects"}, {"color": "#66CCFF", "id": "state-of-the-art accuracy", "label": "state-of-the-art accuracy", "shape": "dot", "size": 25, "title": "state-of-the-art accuracy"}, {"color": "#66CCFF", "id": "probabilistic inference", "label": "probabilistic inference", "shape": "dot", "size": 25, "title": "probabilistic inference"}, {"color": "#66CCFF", "id": "brain", "label": "brain", "shape": "dot", "size": 25, "title": "brain"}, {"color": "#66CCFF", "id": "universal compression device", "label": "universal compression device", "shape": "dot", "size": 25, "title": "universal compression device"}, {"color": "#66CCFF", "id": "Fixation Prediction", "label": "Fixation Prediction", "shape": "dot", "size": 25, "title": "Fixation Prediction"}, {"color": "#66CCFF", "id": "Salience", "label": "Salience", "shape": "dot", "size": 25, "title": "Salience"}, {"color": "#66CCFF", "id": "view of the brain", "label": "view of the brain", "shape": "dot", "size": 25, "title": "view of the brain"}, {"color": "#66CCFF", "id": "Agarwal et al. (2003)", "label": "Agarwal et al. (2003)", "shape": "dot", "size": 25, "title": "Agarwal et al. (2003)"}, {"color": "#66CCFF", "id": "region-of-interest", "label": "region-of-interest", "shape": "dot", "size": 25, "title": "region-of-interest"}, {"color": "#66CCFF", "id": "compressed MPEG domain", "label": "compressed MPEG domain", "shape": "dot", "size": 25, "title": "compressed MPEG domain"}, {"color": "#66CCFF", "id": "Hou and Zhang (2007)", "label": "Hou and Zhang (2007)", "shape": "dot", "size": 25, "title": "Hou and Zhang (2007)"}, {"color": "#66CCFF", "id": "spectral residual approach", "label": "spectral residual approach", "shape": "dot", "size": 25, "title": "spectral residual approach"}, {"color": "#66CCFF", "id": "salience detection method", "label": "salience detection method", "shape": "dot", "size": 25, "title": "salience detection method"}, {"color": "#66CCFF", "id": "Helbing and Molnar (1995)", "label": "Helbing and Molnar (1995)", "shape": "dot", "size": 25, "title": "Helbing and Molnar (1995)"}, {"color": "#66CCFF", "id": "pedestrian dynamics", "label": "pedestrian dynamics", "shape": "dot", "size": 25, "title": "pedestrian dynamics"}, {"color": "#66CCFF", "id": "EE CVPR\u201907", "label": "EE CVPR\u201907", "shape": "dot", "size": 25, "title": "EE CVPR\u201907"}, {"color": "#66CCFF", "id": "Agarwal, G.", "label": "Agarwal, G.", "shape": "dot", "size": 25, "title": "Agarwal, G."}, {"color": "#66CCFF", "id": "Anstis, S. M.", "label": "Anstis, S. M.", "shape": "dot", "size": 25, "title": "Anstis, S. M."}, {"color": "#66CCFF", "id": "perception of apparent movement", "label": "perception of apparent movement", "shape": "dot", "size": 25, "title": "perception of apparent movement"}, {"color": "#66CCFF", "id": "Attneave, F.", "label": "Attneave, F.", "shape": "dot", "size": 25, "title": "Attneave, F."}, {"color": "#66CCFF", "id": "Informational aspects of visual perception", "label": "Informational aspects of visual perception", "shape": "dot", "size": 25, "title": "Informational aspects of visual perception"}, {"color": "#66CCFF", "id": "Barlow, H.", "label": "Barlow, H.", "shape": "dot", "size": 25, "title": "Barlow, H."}, {"color": "#66CCFF", "id": "Cerebral cortex as a model builder", "label": "Cerebral cortex as a model builder", "shape": "dot", "size": 25, "title": "Cerebral cortex as a model builder"}, {"color": "#66CCFF", "id": "Redundancy reduction revisited", "label": "Redundancy reduction revisited", "shape": "dot", "size": 25, "title": "Redundancy reduction revisited"}, {"color": "#66CCFF", "id": "Besag, J.", "label": "Besag, J.", "shape": "dot", "size": 25, "title": "Besag, J."}, {"color": "#66CCFF", "id": "Spatial interaction", "label": "Spatial interaction", "shape": "dot", "size": 25, "title": "Spatial interaction"}, {"color": "#66CCFF", "id": "Simon Fraser University", "label": "Simon Fraser University", "shape": "dot", "size": 25, "title": "Simon Fraser University"}, {"color": "#66CCFF", "id": "University of California, San Diego", "label": "University of California, San Diego", "shape": "dot", "size": 25, "title": "University of California, San Diego"}, {"color": "#66CCFF", "id": "Royal Statistical Society", "label": "Royal Statistical Society", "shape": "dot", "size": 25, "title": "Royal Statistical Society"}, {"color": "#66CCFF", "id": "Series B", "label": "Series B", "shape": "dot", "size": 25, "title": "Series B"}, {"color": "#66CCFF", "id": "36", "label": "36", "shape": "dot", "size": 25, "title": "36"}, {"color": "#66CCFF", "id": "192\u2013236", "label": "192\u2013236", "shape": "dot", "size": 25, "title": "192\u2013236"}, {"color": "#66CCFF", "id": "Ivan V. Bajic", "label": "Ivan V. Bajic", "shape": "dot", "size": 25, "title": "Ivan V. Bajic"}, {"color": "#66CCFF", "id": "Limin Wang", "label": "Limin Wang", "shape": "dot", "size": 25, "title": "Limin Wang"}, {"color": "#66CCFF", "id": "Action Recognition with Trajectory-Pooled Deep-Convolutional Descriptors", "label": "Action Recognition with Trajectory-Pooled Deep-Convolutional Descriptors", "shape": "dot", "size": 25, "title": "Action Recognition with Trajectory-Pooled Deep-Convolutional Descriptors"}, {"color": "#66CCFF", "id": "Yu Qiao", "label": "Yu Qiao", "shape": "dot", "size": 25, "title": "Yu Qiao"}, {"color": "#66CCFF", "id": "Xiaoou Tang", "label": "Xiaoou Tang", "shape": "dot", "size": 25, "title": "Xiaoou Tang"}, {"color": "#66CCFF", "id": "Action Recognized with Trajectory-Pooled Deep-Convolutional Descriptors", "label": "Action Recognized with Trajectory-Pooled Deep-Convolutional Descriptors", "shape": "dot", "size": 25, "title": "Action Recognized with Trajectory-Pooled Deep-Convolutional Descriptors"}, {"color": "#66CCFF", "id": "TDD", "label": "TDD", "shape": "dot", "size": 25, "title": "TDD"}, {"color": "#66CCFF", "id": "video representation", "label": "video representation", "shape": "dot", "size": 25, "title": "video representation"}, {"color": "#66CCFF", "id": "trajectory-constrained pooling", "label": "trajectory-constrained pooling", "shape": "dot", "size": 25, "title": "trajectory-constrained pooling"}, {"color": "#66CCFF", "id": "feature maps", "label": "feature maps", "shape": "dot", "size": 25, "title": "feature maps"}, {"color": "#66CCFF", "id": "normalization methods", "label": "normalization methods", "shape": "dot", "size": 25, "title": "normalization methods"}, {"color": "#66CCFF", "id": "TDDs", "label": "TDDs", "shape": "dot", "size": 25, "title": "TDDs"}, {"color": "#66CCFF", "id": "deep-learned features", "label": "deep-learned features", "shape": "dot", "size": 25, "title": "deep-learned features"}, {"color": "#66CCFF", "id": "HMD-B51", "label": "HMD-B51", "shape": "dot", "size": 25, "title": "HMD-B51"}, {"color": "#66CCFF", "id": "UCF101", "label": "UCF101", "shape": "dot", "size": 25, "title": "UCF101"}, {"color": "#66CCFF", "id": "Human Action Recognition", "label": "Human Action Recognition", "shape": "dot", "size": 25, "title": "Human Action Recognition"}, {"color": "#66CCFF", "id": "Deep Convolutional Descriptors", "label": "Deep Convolutional Descriptors", "shape": "dot", "size": 25, "title": "Deep Convolutional Descriptors"}, {"color": "#66CCFF", "id": "Trajectory-Constrained Pooling", "label": "Trajectory-Constrained Pooling", "shape": "dot", "size": 25, "title": "Trajectory-Constrained Pooling"}, {"color": "#66CCFF", "id": "Multi-view super vector", "label": "Multi-view super vector", "shape": "dot", "size": 25, "title": "Multi-view super vector"}, {"color": "#66CCFF", "id": "Convolutional Nets", "label": "Convolutional Nets", "shape": "dot", "size": 25, "title": "Convolutional Nets"}, {"color": "#66CCFF", "id": "Aggarwal, J. K., \u0026 Ryoo, M. S.", "label": "Aggarwal, J. K., \u0026 Ryoo, M. S.", "shape": "dot", "size": 25, "title": "Aggarwal, J. K., \u0026 Ryoo, M. S."}, {"color": "#66CCFF", "id": "Human activity analysis review", "label": "Human activity analysis review", "shape": "dot", "size": 25, "title": "Human activity analysis review"}, {"color": "#66CCFF", "id": "Bay, H., Tuytelaars, T., \u0026 Van Gool, L. J.", "label": "Bay, H., Tuytelaars, T., \u0026 Van Gool, L. J.", "shape": "dot", "size": 25, "title": "Bay, H., Tuytelaars, T., \u0026 Van Gool, L. J."}, {"color": "#66CCFF", "id": "SURF description", "label": "SURF description", "shape": "dot", "size": 25, "title": "SURF description"}, {"color": "#66CCFF", "id": "Karpathy et al.", "label": "Karpathy et al.", "shape": "dot", "size": 25, "title": "Karpathy et al."}, {"color": "#66CCFF", "id": "HMDB", "label": "HMDB", "shape": "dot", "size": 25, "title": "HMDB"}, {"color": "#66CCFF", "id": "video database", "label": "video database", "shape": "dot", "size": 25, "title": "video database"}, {"color": "#66CCFF", "id": "human motion recognition", "label": "human motion recognition", "shape": "dot", "size": 25, "title": "human motion recognition"}, {"color": "#66CCFF", "id": "Department of Information Engineering", "label": "Department of Information Engineering", "shape": "dot", "size": 25, "title": "Department of Information Engineering"}, {"color": "#66CCFF", "id": "07wanglimin@gmail.com", "label": "07wanglimin@gmail.com", "shape": "dot", "size": 25, "title": "07wanglimin@gmail.com"}, {"color": "#66CCFF", "id": "researcher", "label": "researcher", "shape": "dot", "size": 25, "title": "researcher"}, {"color": "#66CCFF", "id": "T", "label": "T", "shape": "dot", "size": 25, "title": "T"}, {"color": "#66CCFF", "id": "CAS", "label": "CAS", "shape": "dot", "size": 25, "title": "CAS"}, {"color": "#66CCFF", "id": "Jing Shao", "label": "Jing Shao", "shape": "dot", "size": 25, "title": "Jing Shao"}, {"color": "#66CCFF", "id": "Deeply Learned Attributes", "label": "Deeply Learned Attributes", "shape": "dot", "size": 25, "title": "Deeply Learned Attributes"}, {"color": "#66CCFF", "id": "Kai Kang", "label": "Kai Kang", "shape": "dot", "size": 25, "title": "Kai Kang"}, {"color": "#66CCFF", "id": "Chen Change Loy", "label": "Chen Change Loy", "shape": "dot", "size": 25, "title": "Chen Change Loy"}, {"color": "#66CCFF", "id": "Crowded scene understanding", "label": "Crowded scene understanding", "shape": "dot", "size": 25, "title": "Crowded scene understanding"}, {"color": "#66CCFF", "id": "Deep model", "label": "Deep model", "shape": "dot", "size": 25, "title": "Deep model"}, {"color": "#66CCFF", "id": "appearance features", "label": "appearance features", "shape": "dot", "size": 25, "title": "appearance features"}, {"color": "#66CCFF", "id": "motion features", "label": "motion features", "shape": "dot", "size": 25, "title": "motion features"}, {"color": "#66CCFF", "id": "Crowd motion channels", "label": "Crowd motion channels", "shape": "dot", "size": 25, "title": "Crowd motion channels"}, {"color": "#66CCFF", "id": "deep model", "label": "deep model", "shape": "dot", "size": 25, "title": "deep model"}, {"color": "#66CCFF", "id": "generic properties of crowd systems", "label": "generic properties of crowd systems", "shape": "dot", "size": 25, "title": "generic properties of crowd systems"}, {"color": "#66CCFF", "id": "WWW Crowd dataset", "label": "WWW Crowd dataset", "shape": "dot", "size": 25, "title": "WWW Crowd dataset"}, {"color": "#66CCFF", "id": "10,000 videos", "label": "10,000 videos", "shape": "dot", "size": 25, "title": "10,000 videos"}, {"color": "#66CCFF", "id": "8,257 crowded scenes", "label": "8,257 crowded scenes", "shape": "dot", "size": 25, "title": "8,257 crowded scenes"}, {"color": "#66CCFF", "id": "Attribute set", "label": "Attribute set", "shape": "dot", "size": 25, "title": "Attribute set"}, {"color": "#66CCFF", "id": "94 attributes", "label": "94 attributes", "shape": "dot", "size": 25, "title": "94 attributes"}, {"color": "#66CCFF", "id": "Deep models", "label": "Deep models", "shape": "dot", "size": 25, "title": "Deep models"}, {"color": "#66CCFF", "id": "significant performance improvements", "label": "significant performance improvements", "shape": "dot", "size": 25, "title": "significant performance improvements"}, {"color": "#66CCFF", "id": "cross-scene attribute recognition", "label": "cross-scene attribute recognition", "shape": "dot", "size": 25, "title": "cross-scene attribute recognition"}, {"color": "#66CCFF", "id": "deep models", "label": "deep models", "shape": "dot", "size": 25, "title": "deep models"}, {"color": "#66CCFF", "id": "feature-based baselines", "label": "feature-based baselines", "shape": "dot", "size": 25, "title": "feature-based baselines"}, {"color": "#66CCFF", "id": "deeply learned features", "label": "deeply learned features", "shape": "dot", "size": 25, "title": "deeply learned features"}, {"color": "#66CCFF", "id": "multi-task learning", "label": "multi-task learning", "shape": "dot", "size": 25, "title": "multi-task learning"}, {"color": "#66CCFF", "id": "attribute recognition", "label": "attribute recognition", "shape": "dot", "size": 25, "title": "attribute recognition"}, {"color": "#66CCFF", "id": "cross-scene", "label": "cross-scene", "shape": "dot", "size": 25, "title": "cross-scene"}, {"color": "#66CCFF", "id": "baselines", "label": "baselines", "shape": "dot", "size": 25, "title": "baselines"}, {"color": "#66CCFF", "id": "feature-based", "label": "feature-based", "shape": "dot", "size": 25, "title": "feature-based"}, {"color": "#66CCFF", "id": "Deep learning models", "label": "Deep learning models", "shape": "dot", "size": 25, "title": "Deep learning models"}, {"color": "#66CCFF", "id": "crowd motion channels", "label": "crowd motion channels", "shape": "dot", "size": 25, "title": "crowd motion channels"}, {"color": "#66CCFF", "id": "Ali and Shah (2007)", "label": "Ali and Shah (2007)", "shape": "dot", "size": 25, "title": "Ali and Shah (2007)"}, {"color": "#66CCFF", "id": "crowd flow segmentation", "label": "crowd flow segmentation", "shape": "dot", "size": 25, "title": "crowd flow segmentation"}, {"color": "#66CCFF", "id": "Ali and Shah (2008)", "label": "Ali and Shah (2008)", "shape": "dot", "size": 25, "title": "Ali and Shah (2008)"}, {"color": "#66CCFF", "id": "tracking in high density crowd scenes", "label": "tracking in high density crowd scenes", "shape": "dot", "size": 25, "title": "tracking in high density crowd scenes"}, {"color": "#66CCFF", "id": "Andrade, Blunsden, and Fisher (2006)", "label": "Andrade, Blunsden, and Fisher (2006)", "shape": "dot", "size": 25, "title": "Andrade, Blunsden, and Fisher (2006)"}, {"color": "#66CCFF", "id": "event detection in crowd scenes", "label": "event detection in crowd scenes", "shape": "dot", "size": 25, "title": "event detection in crowd scenes"}, {"color": "#66CCFF", "id": "Chan and Vasconcelos (2008)", "label": "Chan and Vasconcelos (2008)", "shape": "dot", "size": 25, "title": "Chan and Vasconcelos (2008)"}, {"color": "#66CCFF", "id": "video segmentation", "label": "video segmentation", "shape": "dot", "size": 25, "title": "video segmentation"}, {"color": "#66CCFF", "id": "crowd-related features", "label": "crowd-related features", "shape": "dot", "size": 25, "title": "crowd-related features"}, {"color": "#66CCFF", "id": "oring", "label": "oring", "shape": "dot", "size": 25, "title": "oring"}, {"color": "#66CCFF", "id": "CVPR 2008", "label": "CVPR 2008", "shape": "dot", "size": 25, "title": "CVPR 2008"}, {"color": "#66CCFF", "id": "Modeling, clustering, and segmenting video", "label": "Modeling, clustering, and segmenting video", "shape": "dot", "size": 25, "title": "Modeling, clustering, and segmenting video"}, {"color": "#66CCFF", "id": "Vasconcelos, N.", "label": "Vasconcelos, N.", "shape": "dot", "size": 25, "title": "Vasconcelos, N."}, {"color": "#66CCFF", "id": "Dalal, N.", "label": "Dalal, N.", "shape": "dot", "size": 25, "title": "Dalal, N."}, {"color": "#66CCFF", "id": "Triggers, B.", "label": "Triggers, B.", "shape": "dot", "size": 25, "title": "Triggers, B."}, {"color": "#66CCFF", "id": "Farhad, A.", "label": "Farhad, A.", "shape": "dot", "size": 25, "title": "Farhad, A."}, {"color": "#66CCFF", "id": "Describing objects by their attributes", "label": "Describing objects by their attributes", "shape": "dot", "size": 25, "title": "Describing objects by their attributes"}, {"color": "#66CCFF", "id": "Hospedales, T.", "label": "Hospedales, T.", "shape": "dot", "size": 25, "title": "Hospedales, T."}, {"color": "#66CCFF", "id": "A markov clustering topic model", "label": "A markov clustering topic model", "shape": "dot", "size": 25, "title": "A markov clustering topic model"}, {"color": "#66CCFF", "id": "Kang, K.", "label": "Kang, K.", "shape": "dot", "size": 25, "title": "Kang, K."}, {"color": "#66CCFF", "id": "Fully convolutional neural networks", "label": "Fully convolutional neural networks", "shape": "dot", "size": 25, "title": "Fully convolutional neural networks"}, {"color": "#66CCFF", "id": "Wang, X.", "label": "Wang, X.", "shape": "dot", "size": 25, "title": "Wang, X."}, {"color": "#66CCFF", "id": "eye tracking data", "label": "eye tracking data", "shape": "dot", "size": 25, "title": "eye tracking data"}, {"color": "#66CCFF", "id": "dominant visual tracks", "label": "dominant visual tracks", "shape": "dot", "size": 25, "title": "dominant visual tracks"}, {"color": "#66CCFF", "id": "object search algorithm", "label": "object search algorithm", "shape": "dot", "size": 25, "title": "object search algorithm"}, {"color": "#66CCFF", "id": "spatio-temporal mixed graph", "label": "spatio-temporal mixed graph", "shape": "dot", "size": 25, "title": "spatio-temporal mixed graph"}, {"color": "#66CCFF", "id": "binary linear integer programming", "label": "binary linear integer programming", "shape": "dot", "size": 25, "title": "binary linear integer programming"}, {"color": "#66CCFF", "id": "object boundaries", "label": "object boundaries", "shape": "dot", "size": 25, "title": "object boundaries"}, {"color": "#66CCFF", "id": "grabcut segmentation", "label": "grabcut segmentation", "shape": "dot", "size": 25, "title": "grabcut segmentation"}, {"color": "#66CCFF", "id": "eye tracking prior", "label": "eye tracking prior", "shape": "dot", "size": 25, "title": "eye tracking prior"}, {"color": "#66CCFF", "id": "Intriligator \u0026 Cavanagh", "label": "Intriligator \u0026 Cavanagh", "shape": "dot", "size": 25, "title": "Intriligator \u0026 Cavanagh"}, {"color": "#66CCFF", "id": "Cognitive psychology article", "label": "Cognitive psychology article", "shape": "dot", "size": 25, "title": "Cognitive psychology article"}, {"color": "#66CCFF", "id": "Itti, Koch, \u0026 Niebur", "label": "Itti, Koch, \u0026 Niebur", "shape": "dot", "size": 25, "title": "Itti, Koch, \u0026 Niebur"}, {"color": "#66CCFF", "id": "salience-based visual attention model", "label": "salience-based visual attention model", "shape": "dot", "size": 25, "title": "salience-based visual attention model"}, {"color": "#66CCFF", "id": "rapid scene analysis", "label": "rapid scene analysis", "shape": "dot", "size": 25, "title": "rapid scene analysis"}, {"color": "#66CCFF", "id": "Judd, Ehinger, Durand, \u0026 Torralba", "label": "Judd, Ehinger, Durand, \u0026 Torralba", "shape": "dot", "size": 25, "title": "Judd, Ehinger, Durand, \u0026 Torralba"}, {"color": "#66CCFF", "id": "human gaze prediction", "label": "human gaze prediction", "shape": "dot", "size": 25, "title": "human gaze prediction"}, {"color": "#66CCFF", "id": "Object Extraction", "label": "Object Extraction", "shape": "dot", "size": 25, "title": "Object Extraction"}, {"color": "#66CCFF", "id": "Binary Linear Integer Programming", "label": "Binary Linear Integer Programming", "shape": "dot", "size": 25, "title": "Binary Linear Integer Programming"}, {"color": "#66CCFF", "id": "Video Segmentation", "label": "Video Segmentation", "shape": "dot", "size": 25, "title": "Video Segmentation"}, {"color": "#66CCFF", "id": "scene analysis", "label": "scene analysis", "shape": "dot", "size": 25, "title": "scene analysis"}, {"color": "#66CCFF", "id": "Judd et al.", "label": "Judd et al.", "shape": "dot", "size": 25, "title": "Judd et al."}, {"color": "#66CCFF", "id": "Learning to predict where humans look", "label": "Learning to predict where humans look", "shape": "dot", "size": 25, "title": "Learning to predict where humans look"}, {"color": "#66CCFF", "id": "Computer Vision, 2009 IEEE 12th international conference on", "label": "Computer Vision, 2009 IEEE 12th international conference on", "shape": "dot", "size": 25, "title": "Computer Vision, 2009 IEEE 12th international conference on"}, {"color": "#66CCFF", "id": "Karthikeyan et al. (2012)", "label": "Karthikeyan et al. (2012)", "shape": "dot", "size": 25, "title": "Karthikeyan et al. (2012)"}, {"color": "#66CCFF", "id": "Uni\ufb01ed probabilistic framework", "label": "Uni\ufb01ed probabilistic framework", "shape": "dot", "size": 25, "title": "Uni\ufb01ed probabilistic framework"}, {"color": "#66CCFF", "id": "Borji \u0026 Itti", "label": "Borji \u0026 Itti", "shape": "dot", "size": 25, "title": "Borji \u0026 Itti"}, {"color": "#66CCFF", "id": "Borji, Sihite, \u0026 Itti", "label": "Borji, Sihite, \u0026 Itti", "shape": "dot", "size": 25, "title": "Borji, Sihite, \u0026 Itti"}, {"color": "#66CCFF", "id": "Salient object detection: A benchmark", "label": "Salient object detection: A benchmark", "shape": "dot", "size": 25, "title": "Salient object detection: A benchmark"}, {"color": "#66CCFF", "id": "Computer Vision\u2013ECCV 2012", "label": "Computer Vision\u2013ECCV 2012", "shape": "dot", "size": 25, "title": "Computer Vision\u2013ECCV 2012"}, {"color": "#66CCFF", "id": "Karthikeyan et al. (2013)", "label": "Karthikeyan et al. (2013)", "shape": "dot", "size": 25, "title": "Karthikeyan et al. (2013)"}, {"color": "#66CCFF", "id": "Learning top-down scene context", "label": "Learning top-down scene context", "shape": "dot", "size": 25, "title": "Learning top-down scene context"}, {"color": "#66CCFF", "id": "ICIP, IEEE", "label": "ICIP, IEEE", "shape": "dot", "size": 25, "title": "ICIP, IEEE"}, {"color": "#66CCFF", "id": "pages 414\u2013429", "label": "pages 414\u2013429", "shape": "dot", "size": 25, "title": "pages 414\u2013429"}, {"color": "#66CCFF", "id": "Karthikeyan, S.", "label": "Karthikeyan, S.", "shape": "dot", "size": 25, "title": "Karthikeyan, S."}, {"color": "#66CCFF", "id": "University of California Santa Barbara", "label": "University of California Santa Barbara", "shape": "dot", "size": 25, "title": "University of California Santa Barbara"}, {"color": "#66CCFF", "id": "{karthikeyan, thuyen, manj}@ece.ucsb.edu", "label": "{karthikeyan, thuyen, manj}@ece.ucsb.edu", "shape": "dot", "size": 25, "title": "{karthikeyan, thuyen, manj}@ece.ucsb.edu"}, {"color": "#66CCFF", "id": "Thuyen Ngo", "label": "Thuyen Ngo", "shape": "dot", "size": 25, "title": "Thuyen Ngo"}, {"color": "#66CCFF", "id": "Miguel Eckstein", "label": "Miguel Eckstein", "shape": "dot", "size": 25, "title": "Miguel Eckstein"}, {"color": "#66CCFF", "id": "eckstein@psych.ucsb.edu", "label": "eckstein@psych.ucsb.edu", "shape": "dot", "size": 25, "title": "eckstein@psych.ucsb.edu"}, {"color": "#66CCFF", "id": "B.S. Manjunath", "label": "B.S. Manjunath", "shape": "dot", "size": 25, "title": "B.S. Manjunath"}, {"color": "#66CCFF", "id": "Felzenszwalb, P.", "label": "Felzenszwalb, P.", "shape": "dot", "size": 25, "title": "Felzenszwalb, P."}, {"color": "#66CCFF", "id": "Eckstein, M. P.", "label": "Eckstein, M. P.", "shape": "dot", "size": 25, "title": "Eckstein, M. P."}, {"color": "#66CCFF", "id": "Visual search: A retrospective", "label": "Visual search: A retrospective", "shape": "dot", "size": 25, "title": "Visual search: A retrospective"}, {"color": "#66CCFF", "id": "Shervin Ardeshir", "label": "Shervin Ardeshir", "shape": "dot", "size": 25, "title": "Shervin Ardeshir"}, {"color": "#66CCFF", "id": "Geo-Semantic Segmentation", "label": "Geo-Semantic Segmentation", "shape": "dot", "size": 25, "title": "Geo-Semantic Segmentation"}, {"color": "#66CCFF", "id": "Ko\ufb01 Malcolm Collins-Sibley", "label": "Ko\ufb01 Malcolm Collins-Sibley", "shape": "dot", "size": 25, "title": "Ko\ufb01 Malcolm Collins-Sibley"}, {"color": "#66CCFF", "id": "Mubarak Shah", "label": "Mubarak Shah", "shape": "dot", "size": 25, "title": "Mubarak Shah"}, {"color": "#66CCFF", "id": "geo-semantic segmentation method", "label": "geo-semantic segmentation method", "shape": "dot", "size": 25, "title": "geo-semantic segmentation method"}, {"color": "#66CCFF", "id": "GIS databases", "label": "GIS databases", "shape": "dot", "size": 25, "title": "GIS databases"}, {"color": "#66CCFF", "id": "GIS projections alignment", "label": "GIS projections alignment", "shape": "dot", "size": 25, "title": "GIS projections alignment"}, {"color": "#66CCFF", "id": "GIS data", "label": "GIS data", "shape": "dot", "size": 25, "title": "GIS data"}, {"color": "#66CCFF", "id": "building locations", "label": "building locations", "shape": "dot", "size": 25, "title": "building locations"}, {"color": "#66CCFF", "id": "street locations", "label": "street locations", "shape": "dot", "size": 25, "title": "street locations"}, {"color": "#66CCFF", "id": "projections", "label": "projections", "shape": "dot", "size": 25, "title": "projections"}, {"color": "#66CCFF", "id": "GPS errors", "label": "GPS errors", "shape": "dot", "size": 25, "title": "GPS errors"}, {"color": "#66CCFF", "id": "camera parameter inaccuracies", "label": "camera parameter inaccuracies", "shape": "dot", "size": 25, "title": "camera parameter inaccuracies"}, {"color": "#66CCFF", "id": "data fusion approach", "label": "data fusion approach", "shape": "dot", "size": 25, "title": "data fusion approach"}, {"color": "#66CCFF", "id": "projections reliability", "label": "projections reliability", "shape": "dot", "size": 25, "title": "projections reliability"}, {"color": "#66CCFF", "id": "super-pixel segmentations", "label": "super-pixel segmentations", "shape": "dot", "size": 25, "title": "super-pixel segmentations"}, {"color": "#66CCFF", "id": "alignment of projections", "label": "alignment of projections", "shape": "dot", "size": 25, "title": "alignment of projections"}, {"color": "#66CCFF", "id": "alignment", "label": "alignment", "shape": "dot", "size": 25, "title": "alignment"}, {"color": "#66CCFF", "id": "random walks", "label": "random walks", "shape": "dot", "size": 25, "title": "random walks"}, {"color": "#66CCFF", "id": "global transformations", "label": "global transformations", "shape": "dot", "size": 25, "title": "global transformations"}, {"color": "#66CCFF", "id": "segmentations", "label": "segmentations", "shape": "dot", "size": 25, "title": "segmentations"}, {"color": "#66CCFF", "id": "semantically segmented images", "label": "semantically segmented images", "shape": "dot", "size": 25, "title": "semantically segmented images"}, {"color": "#66CCFF", "id": "geo-references", "label": "geo-references", "shape": "dot", "size": 25, "title": "geo-references"}, {"color": "#66CCFF", "id": "addresses", "label": "addresses", "shape": "dot", "size": 25, "title": "addresses"}, {"color": "#66CCFF", "id": "geo-locations", "label": "geo-locations", "shape": "dot", "size": 25, "title": "geo-locations"}, {"color": "#66CCFF", "id": "geo-referenced images", "label": "geo-referenced images", "shape": "dot", "size": 25, "title": "geo-referenced images"}, {"color": "#66CCFF", "id": "image processing technique", "label": "image processing technique", "shape": "dot", "size": 25, "title": "image processing technique"}, {"color": "#66CCFF", "id": "Geo-semantic Segmentation", "label": "Geo-semantic Segmentation", "shape": "dot", "size": 25, "title": "Geo-semantic Segmentation"}, {"color": "#66CCFF", "id": "Random Walks", "label": "Random Walks", "shape": "dot", "size": 25, "title": "Random Walks"}, {"color": "#66CCFF", "id": "Global Transformations", "label": "Global Transformations", "shape": "dot", "size": 25, "title": "Global Transformations"}, {"color": "#66CCFF", "id": "Semantically Segmented Images", "label": "Semantically Segmented Images", "shape": "dot", "size": 25, "title": "Semantically Segmented Images"}, {"color": "#66CCFF", "id": "Geo-references", "label": "Geo-references", "shape": "dot", "size": 25, "title": "Geo-references"}, {"color": "#66CCFF", "id": "Addresses", "label": "Addresses", "shape": "dot", "size": 25, "title": "Addresses"}, {"color": "#66CCFF", "id": "GIS Data Integration", "label": "GIS Data Integration", "shape": "dot", "size": 25, "title": "GIS Data Integration"}, {"color": "#66CCFF", "id": "Iterative Data Fusion", "label": "Iterative Data Fusion", "shape": "dot", "size": 25, "title": "Iterative Data Fusion"}, {"color": "#66CCFF", "id": "Image Alignment", "label": "Image Alignment", "shape": "dot", "size": 25, "title": "Image Alignment"}, {"color": "#66CCFF", "id": "P. Zhao et al. [17]", "label": "P. Zhao et al. [17]", "shape": "dot", "size": 25, "title": "P. Zhao et al. [17]"}, {"color": "#66CCFF", "id": "Rectilinear parsing", "label": "Rectilinear parsing", "shape": "dot", "size": 25, "title": "Rectilinear parsing"}, {"color": "#66CCFF", "id": "O. Teboul et al. [10]", "label": "O. Teboul et al. [10]", "shape": "dot", "size": 25, "title": "O. Teboul et al. [10]"}, {"color": "#66CCFF", "id": "Segmentation of building facades", "label": "Segmentation of building facades", "shape": "dot", "size": 25, "title": "Segmentation of building facades"}, {"color": "#66CCFF", "id": "G. J. Brostow et al. [2]", "label": "G. J. Brostow et al. [2]", "shape": "dot", "size": 25, "title": "G. J. Brostow et al. [2]"}, {"color": "#66CCFF", "id": "Segmentation and recognition", "label": "Segmentation and recognition", "shape": "dot", "size": 25, "title": "Segmentation and recognition"}, {"color": "#66CCFF", "id": "EE", "label": "EE", "shape": "dot", "size": 25, "title": "EE"}, {"color": "#66CCFF", "id": "2010", "label": "2010", "shape": "dot", "size": 25, "title": "2010"}, {"color": "#66CCFF", "id": "Brostow", "label": "Brostow", "shape": "dot", "size": 25, "title": "Brostow"}, {"color": "#66CCFF", "id": "Segmentation and recognition using structure from motion point clouds", "label": "Segmentation and recognition using structure from motion point clouds", "shape": "dot", "size": 25, "title": "Segmentation and recognition using structure from motion point clouds"}, {"color": "#66CCFF", "id": "He", "label": "He", "shape": "dot", "size": 25, "title": "He"}, {"color": "#66CCFF", "id": "Multiscale conditional random fields for image labeling", "label": "Multiscale conditional random fields for image labeling", "shape": "dot", "size": 25, "title": "Multiscale conditional random fields for image labeling"}, {"color": "#66CCFF", "id": "CVPR 2004", "label": "CVPR 2004", "shape": "dot", "size": 25, "title": "CVPR 2004"}, {"color": "#66CCFF", "id": "Liu", "label": "Liu", "shape": "dot", "size": 25, "title": "Liu"}, {"color": "#66CCFF", "id": "Entropy rate superpixel segmentation", "label": "Entropy rate superpixel segmentation", "shape": "dot", "size": 25, "title": "Entropy rate superpixel segmentation"}, {"color": "#66CCFF", "id": "M\u00fcller", "label": "M\u00fcller", "shape": "dot", "size": 25, "title": "M\u00fcller"}, {"color": "#66CCFF", "id": "Procedural modeling of buildings", "label": "Procedural modeling of buildings", "shape": "dot", "size": 25, "title": "Procedural modeling of buildings"}, {"color": "#66CCFF", "id": "Musialski", "label": "Musialski", "shape": "dot", "size": 25, "title": "Musialski"}, {"color": "#66CCFF", "id": "Interactive coherence-based fac\u00b8ade modeling", "label": "Interactive coherence-based fac\u00b8ade modeling", "shape": "dot", "size": 25, "title": "Interactive coherence-based fac\u00b8ade modeling"}, {"color": "#66CCFF", "id": "Computer Graphics Forum", "label": "Computer Graphics Forum", "shape": "dot", "size": 25, "title": "Computer Graphics Forum"}, {"color": "#66CCFF", "id": "dings", "label": "dings", "shape": "dot", "size": 25, "title": "dings"}, {"color": "#66CCFF", "id": "Ardeshir", "label": "Ardeshir", "shape": "dot", "size": 25, "title": "Ardeshir"}, {"color": "#66CCFF", "id": "University of Central Florida", "label": "University of Central Florida", "shape": "dot", "size": 25, "title": "University of Central Florida"}, {"color": "#66CCFF", "id": "Gis-assisted object detection", "label": "Gis-assisted object detection", "shape": "dot", "size": 25, "title": "Gis-assisted object detection"}, {"color": "#66CCFF", "id": "Lerma", "label": "Lerma", "shape": "dot", "size": 25, "title": "Lerma"}, {"color": "#66CCFF", "id": "Hoiem", "label": "Hoiem", "shape": "dot", "size": 25, "title": "Hoiem"}, {"color": "#66CCFF", "id": "Automatic photo popup", "label": "Automatic photo popup", "shape": "dot", "size": 25, "title": "Automatic photo popup"}, {"color": "#66CCFF", "id": "Collins-Sibley", "label": "Collins-Sibley", "shape": "dot", "size": 25, "title": "Collins-Sibley"}, {"color": "#66CCFF", "id": "Northeaster University", "label": "Northeaster University", "shape": "dot", "size": 25, "title": "Northeaster University"}, {"color": "#66CCFF", "id": "Shah", "label": "Shah", "shape": "dot", "size": 25, "title": "Shah"}, {"color": "#66CCFF", "id": "Huang", "label": "Huang", "shape": "dot", "size": 25, "title": "Huang"}, {"color": "#66CCFF", "id": "Bayesian Inference", "label": "Bayesian Inference", "shape": "dot", "size": 25, "title": "Bayesian Inference"}, {"color": "#66CCFF", "id": "University of  Central Florida", "label": "University of  Central Florida", "shape": "dot", "size": 25, "title": "University of  Central Florida"}, {"color": "#66CCFF", "id": "shah@crcv.ucf.edu", "label": "shah@crcv.ucf.edu", "shape": "dot", "size": 25, "title": "shah@crcv.ucf.edu"}, {"color": "#66CCFF", "id": "Chao-Tsung Huang", "label": "Chao-Tsung Huang", "shape": "dot", "size": 25, "title": "Chao-Tsung Huang"}, {"color": "#66CCFF", "id": "Bayesian Inference for Neighborhood Filters", "label": "Bayesian Inference for Neighborhood Filters", "shape": "dot", "size": 25, "title": "Bayesian Inference for Neighborhood Filters"}, {"color": "#66CCFF", "id": "Denoising", "label": "Denoising", "shape": "dot", "size": 25, "title": "Denoising"}, {"color": "#66CCFF", "id": "collins-sibley.k@husky.neu.edu", "label": "collins-sibley.k@husky.neu.edu", "shape": "dot", "size": 25, "title": "collins-sibley.k@husky.neu.edu"}, {"color": "#66CCFF", "id": "Huang_Bayesian_Infrenence_for_2015_CVPR_paper.pdf", "label": "Huang_Bayesian_Infrenence_for_2015_CVPR_paper.pdf", "shape": "dot", "size": 25, "title": "Huang_Bayesian_Infrenence_for_2015_CVPR_paper.pdf"}, {"color": "#66CCFF", "id": "Range-weighted neighborhood filters", "label": "Range-weighted neighborhood filters", "shape": "dot", "size": 25, "title": "Range-weighted neighborhood filters"}, {"color": "#66CCFF", "id": "edge-preserving denoising", "label": "edge-preserving denoising", "shape": "dot", "size": 25, "title": "edge-preserving denoising"}, {"color": "#66CCFF", "id": "limited theoretical understanding", "label": "limited theoretical understanding", "shape": "dot", "size": 25, "title": "limited theoretical understanding"}, {"color": "#66CCFF", "id": "unified empirical Bayesian framework", "label": "unified empirical Bayesian framework", "shape": "dot", "size": 25, "title": "unified empirical Bayesian framework"}, {"color": "#66CCFF", "id": "filters", "label": "filters", "shape": "dot", "size": 25, "title": "filters"}, {"color": "#66CCFF", "id": "range variance", "label": "range variance", "shape": "dot", "size": 25, "title": "range variance"}, {"color": "#66CCFF", "id": "neighborhood noise model", "label": "neighborhood noise model", "shape": "dot", "size": 25, "title": "neighborhood noise model"}, {"color": "#66CCFF", "id": "Yaroslavsky, bilateral, and modified non-local means filters", "label": "Yaroslavsky, bilateral, and modified non-local means filters", "shape": "dot", "size": 25, "title": "Yaroslavsky, bilateral, and modified non-local means filters"}, {"color": "#66CCFF", "id": "EM+ algorithm", "label": "EM+ algorithm", "shape": "dot", "size": 25, "title": "EM+ algorithm"}, {"color": "#66CCFF", "id": "noisy images", "label": "noisy images", "shape": "dot", "size": 25, "title": "noisy images"}, {"color": "#66CCFF", "id": "extensible to other range-weighted filters", "label": "extensible to other range-weighted filters", "shape": "dot", "size": 25, "title": "extensible to other range-weighted filters"}, {"color": "#66CCFF", "id": "color-image denoising", "label": "color-image denoising", "shape": "dot", "size": 25, "title": "color-image denoising"}, {"color": "#66CCFF", "id": "model\u0027s effectiveness", "label": "model\u0027s effectiveness", "shape": "dot", "size": 25, "title": "model\u0027s effectiveness"}, {"color": "#66CCFF", "id": "range-weighted algorithms", "label": "range-weighted algorithms", "shape": "dot", "size": 25, "title": "range-weighted algorithms"}, {"color": "#66CCFF", "id": "recursive fitting", "label": "recursive fitting", "shape": "dot", "size": 25, "title": "recursive fitting"}, {"color": "#66CCFF", "id": "accurate estimation", "label": "accurate estimation", "shape": "dot", "size": 25, "title": "accurate estimation"}, {"color": "#66CCFF", "id": "Image Quality", "label": "Image Quality", "shape": "dot", "size": 25, "title": "Image Quality"}, {"color": "#66CCFF", "id": "Paris, S.", "label": "Paris, S.", "shape": "dot", "size": 25, "title": "Paris, S."}, {"color": "#66CCFF", "id": "Bilateral filtering", "label": "Bilateral filtering", "shape": "dot", "size": 25, "title": "Bilateral filtering"}, {"color": "#66CCFF", "id": "Buades, A.", "label": "Buades, A.", "shape": "dot", "size": 25, "title": "Buades, A."}, {"color": "#66CCFF", "id": "image denoising algorithms review", "label": "image denoising algorithms review", "shape": "dot", "size": 25, "title": "image denoising algorithms review"}, {"color": "#66CCFF", "id": "SIAM Journal on Multi-scale Modeling and Simulation", "label": "SIAM Journal on Multi-scale Modeling and Simulation", "shape": "dot", "size": 25, "title": "SIAM Journal on Multi-scale Modeling and Simulation"}, {"color": "#66CCFF", "id": "Chatterjee, P.", "label": "Chatterjee, P.", "shape": "dot", "size": 25, "title": "Chatterjee, P."}, {"color": "#66CCFF", "id": "Patch-based near-optimal image denoising", "label": "Patch-based near-optimal image denoising", "shape": "dot", "size": 25, "title": "Patch-based near-optimal image denoising"}, {"color": "#66CCFF", "id": "Peng, H.", "label": "Peng, H.", "shape": "dot", "size": 25, "title": "Peng, H."}, {"color": "#66CCFF", "id": "Bilateral kernel parameter optimization", "label": "Bilateral kernel parameter optimization", "shape": "dot", "size": 25, "title": "Bilateral kernel parameter optimization"}, {"color": "#66CCFF", "id": "International Conference on Image Processing", "label": "International Conference on Image Processing", "shape": "dot", "size": 25, "title": "International Conference on Image Processing"}, {"color": "#66CCFF", "id": "Multispectral image denoising", "label": "Multispectral image denoising", "shape": "dot", "size": 25, "title": "Multispectral image denoising"}, {"color": "#66CCFF", "id": "vector bilateral filter", "label": "vector bilateral filter", "shape": "dot", "size": 25, "title": "vector bilateral filter"}, {"color": "#66CCFF", "id": "image filtering technique", "label": "image filtering technique", "shape": "dot", "size": 25, "title": "image filtering technique"}, {"color": "#66CCFF", "id": "Vector bilateral filter", "label": "Vector bilateral filter", "shape": "dot", "size": 25, "title": "Vector bilateral filter"}, {"color": "#66CCFF", "id": "Image denoising", "label": "Image denoising", "shape": "dot", "size": 25, "title": "Image denoising"}, {"color": "#66CCFF", "id": "scale mixtures of gausians", "label": "scale mixtures of gausians", "shape": "dot", "size": 25, "title": "scale mixtures of gausians"}, {"color": "#66CCFF", "id": "Bilateral filter", "label": "Bilateral filter", "shape": "dot", "size": 25, "title": "Bilateral filter"}, {"color": "#66CCFF", "id": "Local Estimation", "label": "Local Estimation", "shape": "dot", "size": 25, "title": "Local Estimation"}, {"color": "#66CCFF", "id": "Deep Networks for Saliency Detection", "label": "Deep Networks for Saliency Detection", "shape": "dot", "size": 25, "title": "Deep Networks for Saliency Detection"}, {"color": "#66CCFF", "id": "Global Search", "label": "Global Search", "shape": "dot", "size": 25, "title": "Global Search"}, {"color": "#66CCFF", "id": "Chao-Tsun Huang", "label": "Chao-Tsun Huang", "shape": "dot", "size": 25, "title": "Chao-Tsun Huang"}, {"color": "#66CCFF", "id": "National Tsing Hua University", "label": "National Tsing Hua University", "shape": "dot", "size": 25, "title": "National Tsing Hua University"}, {"color": "#66CCFF", "id": "Lijun Wang", "label": "Lijun Wang", "shape": "dot", "size": 25, "title": "Lijun Wang"}, {"color": "#66CCFF", "id": "Deep Networks for Salience Detection", "label": "Deep Networks for Salience Detection", "shape": "dot", "size": 25, "title": "Deep Networks for Salience Detection"}, {"color": "#66CCFF", "id": "Deep Networks for Salience Detected", "label": "Deep Networks for Salience Detected", "shape": "dot", "size": 25, "title": "Deep Networks for Salience Detected"}, {"color": "#66CCFF", "id": "Wang_Deep_Networks_for_2015_CVPR_paper", "label": "Wang_Deep_Networks_for_2015_CVPR_paper", "shape": "dot", "size": 25, "title": "Wang_Deep_Networks_for_2015_CVPR_paper"}, {"color": "#66CCFF", "id": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Wang_Deep_Networks_for_2015_CVPR_paper.pdf", "label": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Wang_Deep_Networks_for_2015_CVPR_paper.pdf", "shape": "dot", "size": 25, "title": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Wang_Deep_Networks_for_2015_CVPR_paper.pdf"}, {"color": "#66CCFF", "id": "Salience Detection", "label": "Salience Detection", "shape": "dot", "size": 25, "title": "Salience Detection"}, {"color": "#66CCFF", "id": "local estimation", "label": "local estimation", "shape": "dot", "size": 25, "title": "local estimation"}, {"color": "#66CCFF", "id": "global search", "label": "global search", "shape": "dot", "size": 25, "title": "global search"}, {"color": "#66CCFF", "id": "DNN-L", "label": "DNN-L", "shape": "dot", "size": 25, "title": "DNN-L"}, {"color": "#66CCFF", "id": "local patch features", "label": "local patch features", "shape": "dot", "size": 25, "title": "local patch features"}, {"color": "#66CCFF", "id": "salience value", "label": "salience value", "shape": "dot", "size": 25, "title": "salience value"}, {"color": "#66CCFF", "id": "object concepts", "label": "object concepts", "shape": "dot", "size": 25, "title": "object concepts"}, {"color": "#66CCFF", "id": "global contrast", "label": "global contrast", "shape": "dot", "size": 25, "title": "global contrast"}, {"color": "#66CCFF", "id": "DNN-G", "label": "DNN-G", "shape": "dot", "size": 25, "title": "DNN-G"}, {"color": "#66CCFF", "id": "salient score", "label": "salient score", "shape": "dot", "size": 25, "title": "salient score"}, {"color": "#66CCFF", "id": "salient object regions", "label": "salient object regions", "shape": "dot", "size": 25, "title": "salient object regions"}, {"color": "#66CCFF", "id": "weighted sum", "label": "weighted sum", "shape": "dot", "size": 25, "title": "weighted sum"}, {"color": "#66CCFF", "id": "saliency map", "label": "saliency map", "shape": "dot", "size": 25, "title": "saliency map"}, {"color": "#66CCFF", "id": "local contrast", "label": "local contrast", "shape": "dot", "size": 25, "title": "local contrast"}, {"color": "#66CCFF", "id": "shape information", "label": "shape information", "shape": "dot", "size": 25, "title": "shape information"}, {"color": "#66CCFF", "id": "global saliency cues", "label": "global saliency cues", "shape": "dot", "size": 25, "title": "global saliency cues"}, {"color": "#66CCFF", "id": "insight", "label": "insight", "shape": "dot", "size": 25, "title": "insight"}, {"color": "#66CCFF", "id": "object region", "label": "object region", "shape": "dot", "size": 25, "title": "object region"}, {"color": "#66CCFF", "id": "saliency score", "label": "saliency score", "shape": "dot", "size": 25, "title": "saliency score"}, {"color": "#66CCFF", "id": "global features", "label": "global features", "shape": "dot", "size": 25, "title": "global features"}, {"color": "#66CCFF", "id": "Algorithm", "label": "Algorithm", "shape": "dot", "size": 25, "title": "Algorithm"}, {"color": "#66CCFF", "id": "Salient Region Detection", "label": "Salient Region Detection", "shape": "dot", "size": 25, "title": "Salient Region Detection"}, {"color": "#66CCFF", "id": "field", "label": "field", "shape": "dot", "size": 25, "title": "field"}, {"color": "#66CCFF", "id": "R. Achanta et al.", "label": "R. Achanta et al.", "shape": "dot", "size": 25, "title": "R. Achanta et al."}, {"color": "#66CCFF", "id": "J. Carreira and C. Sminchisescu", "label": "J. Carreira and C. Sminchisescu", "shape": "dot", "size": 25, "title": "J. Carreira and C. Sminchisescu"}, {"color": "#66CCFF", "id": "min-cuts", "label": "min-cuts", "shape": "dot", "size": 25, "title": "min-cuts"}, {"color": "#66CCFF", "id": "Constrained parametric min-cuts", "label": "Constrained parametric min-cuts", "shape": "dot", "size": 25, "title": "Constrained parametric min-cuts"}, {"color": "#66CCFF", "id": "Object Candidate Regions", "label": "Object Candidate Regions", "shape": "dot", "size": 25, "title": "Object Candidate Regions"}, {"color": "#66CCFF", "id": "tric min-cuts", "label": "tric min-cuts", "shape": "dot", "size": 25, "title": "tric min-cuts"}, {"color": "#66CCFF", "id": "visual salience", "label": "visual salience", "shape": "dot", "size": 25, "title": "visual salience"}, {"color": "#66CCFF", "id": "Itti, Koch, and Niebur (1998)", "label": "Itti, Koch, and Niebur (1998)", "shape": "dot", "size": 25, "title": "Itti, Koch, and Niebur (1998)"}, {"color": "#66CCFF", "id": "computational model", "label": "computational model", "shape": "dot", "size": 25, "title": "computational model"}, {"color": "#66CCFF", "id": "saliency detection", "label": "saliency detection", "shape": "dot", "size": 25, "title": "saliency detection"}, {"color": "#66CCFF", "id": "absorbing Markov chain", "label": "absorbing Markov chain", "shape": "dot", "size": 25, "title": "absorbing Markov chain"}, {"color": "#66CCFF", "id": "tric min-cuts paper", "label": "tric min-cuts paper", "shape": "dot", "size": 25, "title": "tric min-cuts paper"}, {"color": "#66CCFF", "id": "Markov Chain approach", "label": "Markov Chain approach", "shape": "dot", "size": 25, "title": "Markov Chain approach"}, {"color": "#66CCFF", "id": "Hierarchical approaches", "label": "Hierarchical approaches", "shape": "dot", "size": 25, "title": "Hierarchical approaches"}, {"color": "#66CCFF", "id": "Selective search", "label": "Selective search", "shape": "dot", "size": 25, "title": "Selective search"}, {"color": "#66CCFF", "id": "generating object proposals", "label": "generating object proposals", "shape": "dot", "size": 25, "title": "generating object proposals"}, {"color": "#66CCFF", "id": "preprocessing step", "label": "preprocessing step", "shape": "dot", "size": 25, "title": "preprocessing step"}, {"color": "#66CCFF", "id": "salient object detection pipelines", "label": "salient object detection pipelines", "shape": "dot", "size": 25, "title": "salient object detection pipelines"}, {"color": "#66CCFF", "id": "salient regions", "label": "salient regions", "shape": "dot", "size": 25, "title": "salient regions"}, {"color": "#66CCFF", "id": "simultaneous detection and segmentation", "label": "simultaneous detection and segmentation", "shape": "dot", "size": 25, "title": "simultaneous detection and segmentation"}, {"color": "#66CCFF", "id": "related task", "label": "related task", "shape": "dot", "size": 25, "title": "related task"}, {"color": "#66CCFF", "id": "Bayesian models", "label": "Bayesian models", "shape": "dot", "size": 25, "title": "Bayesian models"}, {"color": "#66CCFF", "id": "visual salience detection", "label": "visual salience detection", "shape": "dot", "size": 25, "title": "visual salience detection"}, {"color": "#66CCFF", "id": "ICC paper", "label": "ICC paper", "shape": "dot", "size": 25, "title": "ICC paper"}, {"color": "#66CCFF", "id": "ECCV paper", "label": "ECCV paper", "shape": "dot", "size": 25, "title": "ECCV paper"}, {"color": "#66CCFF", "id": "ICIP paper", "label": "ICIP paper", "shape": "dot", "size": 25, "title": "ICIP paper"}, {"color": "#66CCFF", "id": "Bayesian model", "label": "Bayesian model", "shape": "dot", "size": 25, "title": "Bayesian model"}, {"color": "#66CCFF", "id": "Abhishek Kar", "label": "Abhishek Kar", "shape": "dot", "size": 25, "title": "Abhishek Kar"}, {"color": "#66CCFF", "id": "Jo\u02dcao Carreira", "label": "Jo\u02dcao Carreira", "shape": "dot", "size": 25, "title": "Jo\u02dcao Carreira"}, {"color": "#66CCFF", "id": "akar@eecs.berkeley.edu", "label": "akar@eecs.berkeley.edu", "shape": "dot", "size": 25, "title": "akar@eecs.berkeley.edu"}, {"color": "#66CCFF", "id": "shubhtuls@eecs.berkeley.edu", "label": "shubhtuls@eecs.berkeley.edu", "shape": "dot", "size": 25, "title": "shubhtuls@eecs.berkeley.edu"}, {"color": "#66CCFF", "id": "carreira@eecs.berkeley.edu", "label": "carreira@eecs.berkeley.edu", "shape": "dot", "size": 25, "title": "carreira@eecs.berkeley.edu"}, {"color": "#66CCFF", "id": "malik@eecs.berkeley.edu", "label": "malik@eecs.berkeley.edu", "shape": "dot", "size": 25, "title": "malik@eecs.berkeley.edu"}, {"color": "#66CCFF", "id": "Category-Speci\ufb01c Object Reconstruction from a Single Image", "label": "Category-Speci\ufb01c Object Reconstruction from a Single Image", "shape": "dot", "size": 25, "title": "Category-Speci\ufb01c Object Reconstruction from a Single Image"}, {"color": "#66CCFF", "id": "Shubham Tulisiani", "label": "Shubham Tulisiani", "shape": "dot", "size": 25, "title": "Shubham Tulisiani"}, {"color": "#66CCFF", "id": "Thorsten Beier", "label": "Thorsten Beier", "shape": "dot", "size": 25, "title": "Thorsten Beier"}, {"color": "#66CCFF", "id": "Fusion Moves for Correlation Clustering", "label": "Fusion Moves for Correlation Clustering", "shape": "dot", "size": 25, "title": "Fusion Moves for Correlation Clustering"}, {"color": "#66CCFF", "id": "Fred A. Hamprecht", "label": "Fred A. Hamprecht", "shape": "dot", "size": 25, "title": "Fred A. Hamprecht"}, {"color": "#66CCFF", "id": "University of Heidelberg", "label": "University of Heidelberg", "shape": "dot", "size": 25, "title": "University of Heidelberg"}, {"color": "#66CCFF", "id": "J\u00f6rg H. Kappes", "label": "J\u00f6rg H. Kappes", "shape": "dot", "size": 25, "title": "J\u00f6rg H. Kappes"}, {"color": "#66CCFF", "id": "Math", "label": "Math", "shape": "dot", "size": 25, "title": "Math"}, {"color": "#66CCFF", "id": "thorsten.beier@iwr.uni-heidelberg.de", "label": "thorsten.beier@iwr.uni-heidelberg.de", "shape": "dot", "size": 25, "title": "thorsten.beier@iwr.uni-heidelberg.de"}, {"color": "#66CCFF", "id": "fred.hamprecht@iwr.uni-heidelberg.de", "label": "fred.hamprecht@iwr.uni-heidelberg.de", "shape": "dot", "size": 25, "title": "fred.hamprecht@iwr.uni-heidelberg.de"}, {"color": "#66CCFF", "id": "kappes@math.uni-heidelberg.de", "label": "kappes@math.uni-heidelberg.de", "shape": "dot", "size": 25, "title": "kappes@math.uni-heidelberg.de"}, {"color": "#66CCFF", "id": "Hossein Rahmani", "label": "Hossein Rahmani", "shape": "dot", "size": 25, "title": "Hossein Rahmani"}, {"color": "#66CCFF", "id": "The University of Western Australia", "label": "The University of Western Australia", "shape": "dot", "size": 25, "title": "The University of Western Australia"}, {"color": "#66CCFF", "id": "Hossein Rahman\u0131", "label": "Hossein Rahman\u0131", "shape": "dot", "size": 25, "title": "Hossein Rahman\u0131"}, {"color": "#66CCFF", "id": "hossein@csse.uwa.edu.au", "label": "hossein@csse.uwa.edu.au", "shape": "dot", "size": 25, "title": "hossein@csse.uwa.edu.au"}, {"color": "#66CCFF", "id": "Ajmal Mian", "label": "Ajmal Mian", "shape": "dot", "size": 25, "title": "Ajmal Mian"}, {"color": "#66CCFF", "id": "The University of Western Canada", "label": "The University of Western Canada", "shape": "dot", "size": 25, "title": "The University of Western Canada"}, {"color": "#66CCFF", "id": "ajmal.mian@uwa.edu.au", "label": "ajmal.mian@uwa.edu.au", "shape": "dot", "size": 25, "title": "ajmal.mian@uwa.edu.au"}, {"color": "#66CCFF", "id": "Learning a Non-linear Knowledge Transfer Model for Cross-View Action Recognition", "label": "Learning a Non-linear Knowledge Transfer Model for Cross-View Action Recognition", "shape": "dot", "size": 25, "title": "Learning a Non-linear Knowledge Transfer Model for Cross-View Action Recognition"}, {"color": "#66CCFF", "id": "Rahmani_Learning_a_Non-Linear_2015_CVPR_paper.pdf", "label": "Rahmani_Learning_a_Non-Linear_2015_CVPR_paper.pdf", "shape": "dot", "size": 25, "title": "Rahmani_Learning_a_Non-Linear_2015_CVPR_paper.pdf"}, {"color": "#66CCFF", "id": "Sparse Kernel Multi-task Learning (SKMTL) models", "label": "Sparse Kernel Multi-task Learning (SKMTL) models", "shape": "dot", "size": 25, "title": "Sparse Kernel Multi-task Learning (SKMTL) models"}, {"color": "#66CCFF", "id": "Convex Multi-task Cluster Learning", "label": "Convex Multi-task Cluster Learning", "shape": "dot", "size": 25, "title": "Convex Multi-task Cluster Learning"}, {"color": "#66CCFF", "id": "SKMTL problem", "label": "SKMTL problem", "shape": "dot", "size": 25, "title": "SKMTL problem"}, {"color": "#66CCFF", "id": "jointly convex", "label": "jointly convex", "shape": "dot", "size": 25, "title": "jointly convex"}, {"color": "#66CCFF", "id": "clustered structures", "label": "clustered structures", "shape": "dot", "size": 25, "title": "clustered structures"}, {"color": "#66CCFF", "id": "tasks", "label": "tasks", "shape": "dot", "size": 25, "title": "tasks"}, {"color": "#66CCFF", "id": "Robotics (Sarcos) dataset", "label": "Robotics (Sarcos) dataset", "shape": "dot", "size": 25, "title": "Robotics (Sarcos) dataset"}, {"color": "#66CCFF", "id": "sparse structure", "label": "sparse structure", "shape": "dot", "size": 25, "title": "sparse structure"}, {"color": "#66CCFF", "id": "settings beyond computer vision", "label": "settings beyond computer vision", "shape": "dot", "size": 25, "title": "settings beyond computer vision"}, {"color": "#66CCFF", "id": "Laplacian Eigenmaps", "label": "Laplacian Eigenmaps", "shape": "dot", "size": 25, "title": "Laplacian Eigenmaps"}, {"color": "#66CCFF", "id": "Sparse Kernel Multi-task Learning", "label": "Sparse Kernel Multi-task Learning", "shape": "dot", "size": 25, "title": "Sparse Kernel Multi-task Learning"}, {"color": "#66CCFF", "id": "Joint Convexity", "label": "Joint Convexity", "shape": "dot", "size": 25, "title": "Joint Convexity"}, {"color": "#66CCFF", "id": "Cluster Multi-task Learning", "label": "Cluster Multi-task Learning", "shape": "dot", "size": 25, "title": "Cluster Multi-task Learning"}, {"color": "#66CCFF", "id": "Robotics", "label": "Robotics", "shape": "dot", "size": 25, "title": "Robotics"}, {"color": "#66CCFF", "id": "Sarcos dataset", "label": "Sarcos dataset", "shape": "dot", "size": 25, "title": "Sarcos dataset"}, {"color": "#66CCFF", "id": "Robust Multiple Homography Estimation", "label": "Robust Multiple Homography Estimation", "shape": "dot", "size": 25, "title": "Robust Multiple Homography Estimation"}, {"color": "#66CCFF", "id": "ill-solved problem", "label": "ill-solved problem", "shape": "dot", "size": 25, "title": "ill-solved problem"}, {"color": "#66CCFF", "id": "Zygmunt L. Szpak", "label": "Zygmunt L. Szpak", "shape": "dot", "size": 25, "title": "Zygmunt L. Szpak"}, {"color": "#66CCFF", "id": "Wojciech Chojnacki", "label": "Wojciech Chojnacki", "shape": "dot", "size": 25, "title": "Wojciech Chojnacki"}, {"color": "#66CCFF", "id": "Szpak_Robust_Multiple_Homography_2015_CVPR_paper.pdf", "label": "Szpak_Robust_Multiple_Homography_2015_CVPR_paper.pdf", "shape": "dot", "size": 25, "title": "Szpak_Robust_Multiple_Homography_2015_CVPR_paper.pdf"}, {"color": "#66CCFF", "id": "multiple homographies estimation", "label": "multiple homographies estimation", "shape": "dot", "size": 25, "title": "multiple homographies estimation"}, {"color": "#66CCFF", "id": "failure to enforce consistency constraints", "label": "failure to enforce consistency constraints", "shape": "dot", "size": 25, "title": "failure to enforce consistency constraints"}, {"color": "#66CCFF", "id": "rigidity", "label": "rigidity", "shape": "dot", "size": 25, "title": "rigidity"}, {"color": "#66CCFF", "id": "consistency constraints", "label": "consistency constraints", "shape": "dot", "size": 25, "title": "consistency constraints"}, {"color": "#66CCFF", "id": "homographies", "label": "homographies", "shape": "dot", "size": 25, "title": "homographies"}, {"color": "#66CCFF", "id": "new constraints", "label": "new constraints", "shape": "dot", "size": 25, "title": "new constraints"}, {"color": "#66CCFF", "id": "epipolar geometries", "label": "epipolar geometries", "shape": "dot", "size": 25, "title": "epipolar geometries"}, {"color": "#66CCFF", "id": "inconsistent", "label": "inconsistent", "shape": "dot", "size": 25, "title": "inconsistent"}, {"color": "#66CCFF", "id": "robust multi-structure estimation methods", "label": "robust multi-structure estimation methods", "shape": "dot", "size": 25, "title": "robust multi-structure estimation methods"}, {"color": "#66CCFF", "id": "enforcing constraints on homography matrices", "label": "enforcing constraints on homography matrices", "shape": "dot", "size": 25, "title": "enforcing constraints on homography matrices"}, {"color": "#66CCFF", "id": "incomplete constraint satisfaction", "label": "incomplete constraint satisfaction", "shape": "dot", "size": 25, "title": "incomplete constraint satisfaction"}, {"color": "#66CCFF", "id": "views", "label": "views", "shape": "dot", "size": 25, "title": "views"}, {"color": "#66CCFF", "id": "multi-structure estimation methods", "label": "multi-structure estimation methods", "shape": "dot", "size": 25, "title": "multi-structure estimation methods"}, {"color": "#66CCFF", "id": "enforcing constraints", "label": "enforcing constraints", "shape": "dot", "size": 25, "title": "enforcing constraints"}, {"color": "#66CCFF", "id": "homography matrices", "label": "homography matrices", "shape": "dot", "size": 25, "title": "homography matrices"}, {"color": "#66CCFF", "id": "new generation", "label": "new generation", "shape": "dot", "size": 25, "title": "new generation"}, {"color": "#66CCFF", "id": "critiques", "label": "critiques", "shape": "dot", "size": 25, "title": "critiques"}, {"color": "#66CCFF", "id": "Robust Multi-Structure Estimation", "label": "Robust Multi-Structure Estimation", "shape": "dot", "size": 25, "title": "Robust Multi-Structure Estimation"}, {"color": "#66CCFF", "id": "constraints on homography matrices", "label": "constraints on homography matrices", "shape": "dot", "size": 25, "title": "constraints on homography matrices"}, {"color": "#66CCFF", "id": "Homography Matrices", "label": "Homography Matrices", "shape": "dot", "size": 25, "title": "Homography Matrices"}, {"color": "#66CCFF", "id": "Projective Geometry", "label": "Projective Geometry", "shape": "dot", "size": 25, "title": "Projective Geometry"}, {"color": "#66CCFF", "id": "Epiopolar Geometry", "label": "Epiopolar Geometry", "shape": "dot", "size": 25, "title": "Epiopolar Geometry"}, {"color": "#66CCFF", "id": "Baker, S., Datta, A., and Kanade, T.", "label": "Baker, S., Datta, A., and Kanade, T.", "shape": "dot", "size": 25, "title": "Baker, S., Datta, A., and Kanade, T."}, {"color": "#66CCFF", "id": "Parameterizing homographies", "label": "Parameterizing homographies", "shape": "dot", "size": 25, "title": "Parameterizing homographies"}, {"color": "#66CCFF", "id": "Bernstein, D. S.", "label": "Bernstein, D. S.", "shape": "dot", "size": 25, "title": "Bernstein, D. S."}, {"color": "#66CCFF", "id": "Matrix Mathematics", "label": "Matrix Mathematics", "shape": "dot", "size": 25, "title": "Matrix Mathematics"}, {"color": "#66CCFF", "id": "Chen, P., and Suter, D.", "label": "Chen, P., and Suter, D.", "shape": "dot", "size": 25, "title": "Chen, P., and Suter, D."}, {"color": "#66CCFF", "id": "Rank constraints for homographies", "label": "Rank constraints for homographies", "shape": "dot", "size": 25, "title": "Rank constraints for homographies"}, {"color": "#66CCFF", "id": "tech. rep. CMU-RI-TR-06-11", "label": "tech. rep. CMU-RI-TR-06-11", "shape": "dot", "size": 25, "title": "tech. rep. CMU-RI-TR-06-11"}, {"color": "#66CCFF", "id": "Chen, P.", "label": "Chen, P.", "shape": "dot", "size": 25, "title": "Chen, P."}, {"color": "#66CCFF", "id": "Rank constraints", "label": "Rank constraints", "shape": "dot", "size": 25, "title": "Rank constraints"}, {"color": "#66CCFF", "id": "Suter, D.", "label": "Suter, D.", "shape": "dot", "size": 25, "title": "Suter, D."}, {"color": "#66CCFF", "id": "Chojnacki, W.", "label": "Chojnacki, W.", "shape": "dot", "size": 25, "title": "Chojnacki, W."}, {"color": "#66CCFF", "id": "Multiple homography estimation", "label": "Multiple homography estimation", "shape": "dot", "size": 25, "title": "Multiple homography estimation"}, {"color": "#66CCFF", "id": "Szpak, Z.", "label": "Szpak, Z.", "shape": "dot", "size": 25, "title": "Szpak, Z."}, {"color": "#66CCFF", "id": "van den Hengel, A.", "label": "van den Hengel, A.", "shape": "dot", "size": 25, "title": "van den Hengel, A."}, {"color": "#66CCFF", "id": "Dimensionality result", "label": "Dimensionality result", "shape": "dot", "size": 25, "title": "Dimensionality result"}, {"color": "#66CCFF", "id": "Fouhey, D. F.", "label": "Fouhey, D. F.", "shape": "dot", "size": 25, "title": "Fouhey, D. F."}, {"color": "#66CCFF", "id": "Multiple plane detection", "label": "Multiple plane detection", "shape": "dot", "size": 25, "title": "Multiple plane detection"}, {"color": "#66CCFF", "id": "Scharstein, D.", "label": "Scharstein, D.", "shape": "dot", "size": 25, "title": "Scharstein, D."}, {"color": "#66CCFF", "id": "Briggs, A. J.", "label": "Briggs, A. J.", "shape": "dot", "size": 25, "title": "Briggs, A. J."}, {"color": "#66CCFF", "id": "J-linkage", "label": "J-linkage", "shape": "dot", "size": 25, "title": "J-linkage"}, {"color": "#66CCFF", "id": "Goldberger, J.", "label": "Goldberger, J.", "shape": "dot", "size": 25, "title": "Goldberger, J."}, {"color": "#66CCFF", "id": "Camera projection matrices", "label": "Camera projection matrices", "shape": "dot", "size": 25, "title": "Camera projection matrices"}, {"color": "#66CCFF", "id": "Goldberger", "label": "Goldberger", "shape": "dot", "size": 25, "title": "Goldberger"}, {"color": "#66CCFF", "id": "Reconstructing camera projection matrices", "label": "Reconstructing camera projection matrices", "shape": "dot", "size": 25, "title": "Reconstructing camera projection matrices"}, {"color": "#66CCFF", "id": "Irving", "label": "Irving", "shape": "dot", "size": 25, "title": "Irving"}, {"color": "#66CCFF", "id": "Integers, Polynomials, and Rings", "label": "Integers, Polynomials, and Rings", "shape": "dot", "size": 25, "title": "Integers, Polynomials, and Rings"}, {"color": "#66CCFF", "id": "Szpak", "label": "Szpak", "shape": "dot", "size": 25, "title": "Szpak"}, {"color": "#66CCFF", "id": "Chojnicki", "label": "Chojnicki", "shape": "dot", "size": 25, "title": "Chojnicki"}, {"color": "#66CCFF", "id": "van den Hengel", "label": "van den Hengel", "shape": "dot", "size": 25, "title": "van den Hengel"}, {"color": "#66CCFF", "id": "Saturation-Preerving Specular Reflection Separation", "label": "Saturation-Preerving Specular Reflection Separation", "shape": "dot", "size": 25, "title": "Saturation-Preerving Specular Reflection Separation"}, {"color": "#66CCFF", "id": "Yuan", "label": "Yuan", "shape": "dot", "size": 25, "title": "Yuan"}, {"color": "#66CCFF", "id": "Zheng", "label": "Zheng", "shape": "dot", "size": 25, "title": "Zheng"}, {"color": "#66CCFF", "id": "Wu", "label": "Wu", "shape": "dot", "size": 25, "title": "Wu"}, {"color": "#66CCFF", "id": "Yuanliu Liu", "label": "Yuanliu Liu", "shape": "dot", "size": 25, "title": "Yuanliu Liu"}, {"color": "#66CCFF", "id": "Saturation-Preerving Specular Reflection Paper", "label": "Saturation-Preerving Specular Reflection Paper", "shape": "dot", "size": 25, "title": "Saturation-Preerving Specular Reflection Paper"}, {"color": "#66CCFF", "id": "Zejian Yuan", "label": "Zejian Yuan", "shape": "dot", "size": 25, "title": "Zejian Yuan"}, {"color": "#66CCFF", "id": "Nanning Zheng", "label": "Nanning Zheng", "shape": "dot", "size": 25, "title": "Nanning Zheng"}, {"color": "#66CCFF", "id": "Yang Wu", "label": "Yang Wu", "shape": "dot", "size": 25, "title": "Yang Wu"}, {"color": "#66CCFF", "id": "Reflection", "label": "Reflection", "shape": "dot", "size": 25, "title": "Reflection"}, {"color": "#66CCFF", "id": "Specular Reflection", "label": "Specular Reflection", "shape": "dot", "size": 25, "title": "Specular Reflection"}, {"color": "#66CCFF", "id": "Saturation-Preserving Specular Reflection Paper", "label": "Saturation-Preserving Specular Reflection Paper", "shape": "dot", "size": 25, "title": "Saturation-Preserving Specular Reflection Paper"}, {"color": "#66CCFF", "id": "Liu_Saturation-Preerving Specular Reflection Paper", "label": "Liu_Saturation-Preerving Specular Reflection Paper", "shape": "dot", "size": 25, "title": "Liu_Saturation-Preerving Specular Reflection Paper"}, {"color": "#66CCFF", "id": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Liu_Saturation-Preerving_Specular_Reflection_2015_CVPR_paper.pdf", "label": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Liu_Saturation-Preerving_Specular_Reflection_2015_CVPR_paper.pdf", "shape": "dot", "size": 25, "title": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Liu_Saturation-Preerving_Specular_Reflection_2015_CVPR_paper.pdf"}, {"color": "#66CCFF", "id": "Specular reflection", "label": "Specular reflection", "shape": "dot", "size": 25, "title": "Specular reflection"}, {"color": "#66CCFF", "id": "saturation of surface colors", "label": "saturation of surface colors", "shape": "dot", "size": 25, "title": "saturation of surface colors"}, {"color": "#66CCFF", "id": "decreased saturation", "label": "decreased saturation", "shape": "dot", "size": 25, "title": "decreased saturation"}, {"color": "#66CCFF", "id": "confusion with other colors", "label": "confusion with other colors", "shape": "dot", "size": 25, "title": "confusion with other colors"}, {"color": "#66CCFF", "id": "Traditional methods", "label": "Traditional methods", "shape": "dot", "size": 25, "title": "Traditional methods"}, {"color": "#66CCFF", "id": "hue-saturation ambiguity", "label": "hue-saturation ambiguity", "shape": "dot", "size": 25, "title": "hue-saturation ambiguity"}, {"color": "#66CCFF", "id": "Specular-free images", "label": "Specular-free images", "shape": "dot", "size": 25, "title": "Specular-free images"}, {"color": "#66CCFF", "id": "oversaturated", "label": "oversaturated", "shape": "dot", "size": 25, "title": "oversaturated"}, {"color": "#66CCFF", "id": "This paper", "label": "This paper", "shape": "dot", "size": 25, "title": "This paper"}, {"color": "#66CCFF", "id": "two-step approach", "label": "two-step approach", "shape": "dot", "size": 25, "title": "two-step approach"}, {"color": "#66CCFF", "id": "over-saturated specular-free image", "label": "over-saturated specular-free image", "shape": "dot", "size": 25, "title": "over-saturated specular-free image"}, {"color": "#66CCFF", "id": "global chromaticity propagation", "label": "global chromaticity propagation", "shape": "dot", "size": 25, "title": "global chromaticity propagation"}, {"color": "#66CCFF", "id": "Saturation", "label": "Saturation", "shape": "dot", "size": 25, "title": "Saturation"}, {"color": "#66CCFF", "id": "piecewise constancy of diffuse chromaticity", "label": "piecewise constancy of diffuse chromaticity", "shape": "dot", "size": 25, "title": "piecewise constancy of diffuse chromaticity"}, {"color": "#66CCFF", "id": "spatial sparsity/smoothness of specular reflection", "label": "spatial sparsity/smoothness of specular reflection", "shape": "dot", "size": 25, "title": "spatial sparsity/smoothness of specular reflection"}, {"color": "#66CCFF", "id": "achieved by increasing", "label": "achieved by increasing", "shape": "dot", "size": 25, "title": "achieved by increasing"}, {"color": "#66CCFF", "id": "linear programming", "label": "linear programming", "shape": "dot", "size": 25, "title": "linear programming"}, {"color": "#66CCFF", "id": "diffuse chromaticity", "label": "diffuse chromaticity", "shape": "dot", "size": 25, "title": "diffuse chromaticity"}, {"color": "#66CCFF", "id": "chromaticity", "label": "chromaticity", "shape": "dot", "size": 25, "title": "chromaticity"}, {"color": "#66CCFF", "id": "specular reflection", "label": "specular reflection", "shape": "dot", "size": 25, "title": "specular reflection"}, {"color": "#66CCFF", "id": "spatial sparsity", "label": "spatial sparsity", "shape": "dot", "size": 25, "title": "spatial sparsity"}, {"color": "#66CCFF", "id": "achromatic component", "label": "achromatic component", "shape": "dot", "size": 25, "title": "achromatic component"}, {"color": "#66CCFF", "id": "ability to separate specular reflection", "label": "ability to separate specular reflection", "shape": "dot", "size": 25, "title": "ability to separate specular reflection"}, {"color": "#66CCFF", "id": "saturation of underlying surface colors", "label": "saturation of underlying surface colors", "shape": "dot", "size": 25, "title": "saturation of underlying surface colors"}, {"color": "#66CCFF", "id": "surface colors", "label": "surface colors", "shape": "dot", "size": 25, "title": "surface colors"}, {"color": "#66CCFF", "id": "saturation", "label": "saturation", "shape": "dot", "size": 25, "title": "saturation"}, {"color": "#66CCFF", "id": "increase achromatic component", "label": "increase achromatic component", "shape": "dot", "size": 25, "title": "increase achromatic component"}, {"color": "#66CCFF", "id": "Diffuse Chromaticity", "label": "Diffuse Chromaticity", "shape": "dot", "size": 25, "title": "Diffuse Chromaticity"}, {"color": "#66CCFF", "id": "Linear Programming", "label": "Linear Programming", "shape": "dot", "size": 25, "title": "Linear Programming"}, {"color": "#66CCFF", "id": "surface color saturation", "label": "surface color saturation", "shape": "dot", "size": 25, "title": "surface color saturation"}, {"color": "#66CCFF", "id": "Shafer, S.", "label": "Shafer, S.", "shape": "dot", "size": 25, "title": "Shafer, S."}, {"color": "#66CCFF", "id": "Artusi, A. et al.", "label": "Artusi, A. et al.", "shape": "dot", "size": 25, "title": "Artusi, A. et al."}, {"color": "#66CCFF", "id": "survey of specular removal methods", "label": "survey of specular removal methods", "shape": "dot", "size": 25, "title": "survey of specular removal methods"}, {"color": "#66CCFF", "id": "Diffuse Reflection", "label": "Diffuse Reflection", "shape": "dot", "size": 25, "title": "Diffuse Reflection"}, {"color": "#66CCFF", "id": "Hue-Saturation Ambiguity", "label": "Hue-Saturation Ambiguity", "shape": "dot", "size": 25, "title": "Hue-Saturation Ambiguity"}, {"color": "#66CCFF", "id": "Chromaticity Propagation", "label": "Chromaticity Propagation", "shape": "dot", "size": 25, "title": "Chromaticity Propagation"}, {"color": "#66CCFF", "id": "reflection component separation", "label": "reflection component separation", "shape": "dot", "size": 25, "title": "reflection component separation"}, {"color": "#66CCFF", "id": "Diffuse and specular interface reflections", "label": "Diffuse and specular interface reflections", "shape": "dot", "size": 25, "title": "Diffuse and specular interface reflections"}, {"color": "#66CCFF", "id": "Gonzalez \u0026 Woods", "label": "Gonzalez \u0026 Woods", "shape": "dot", "size": 25, "title": "Gonzalez \u0026 Woods"}, {"color": "#66CCFF", "id": "Digital Image Processing", "label": "Digital Image Processing", "shape": "dot", "size": 25, "title": "Digital Image Processing"}, {"color": "#66CCFF", "id": "Land \u0026 McCann", "label": "Land \u0026 McCann", "shape": "dot", "size": 25, "title": "Land \u0026 McCann"}, {"color": "#66CCFF", "id": "retinex theory", "label": "retinex theory", "shape": "dot", "size": 25, "title": "retinex theory"}, {"color": "#66CCFF", "id": "Kim et al.", "label": "Kim et al.", "shape": "dot", "size": 25, "title": "Kim et al."}, {"color": "#66CCFF", "id": "dark channel prior", "label": "dark channel prior", "shape": "dot", "size": 25, "title": "dark channel prior"}, {"color": "#66CCFF", "id": "Mallick et al.", "label": "Mallick et al.", "shape": "dot", "size": 25, "title": "Mallick et al."}, {"color": "#66CCFF", "id": "specular surfaces", "label": "specular surfaces", "shape": "dot", "size": 25, "title": "specular surfaces"}, {"color": "#66CCFF", "id": "color information", "label": "color information", "shape": "dot", "size": 25, "title": "color information"}, {"color": "#66CCFF", "id": "specular reflection separation", "label": "specular reflection separation", "shape": "dot", "size": 25, "title": "specular reflection separation"}, {"color": "#66CCFF", "id": "S. P.", "label": "S. P.", "shape": "dot", "size": 25, "title": "S. P."}, {"color": "#66CCFF", "id": "Beyond lambert", "label": "Beyond lambert", "shape": "dot", "size": 25, "title": "Beyond lambert"}, {"color": "#66CCFF", "id": "Zickler", "label": "Zickler", "shape": "dot", "size": 25, "title": "Zickler"}, {"color": "#66CCFF", "id": "T.", "label": "T.", "shape": "dot", "size": 25, "title": "T."}, {"color": "#66CCFF", "id": "P. N. Belhumeur", "label": "P. N. Belhumeur", "shape": "dot", "size": 25, "title": "P. N. Belhumeur"}, {"color": "#66CCFF", "id": "D. J. Kriegman", "label": "D. J. Kriegman", "shape": "dot", "size": 25, "title": "D. J. Kriegman"}, {"color": "#66CCFF", "id": "reconstructing specular surfaces", "label": "reconstructing specular surfaces", "shape": "dot", "size": 25, "title": "reconstructing specular surfaces"}, {"color": "#66CCFF", "id": "P., Zickler, T., Belhumeur, P. N., \u0026 Kriegman, D. J.", "label": "P., Zickler, T., Belhumeur, P. N., \u0026 Kriegman, D. J.", "shape": "dot", "size": 25, "title": "P., Zickler, T., Belhumeur, P. N., \u0026 Kriegman, D. J."}, {"color": "#66CCFF", "id": "Lin, S., \u0026 Shum, H.-Y.", "label": "Lin, S., \u0026 Shum, H.-Y.", "shape": "dot", "size": 25, "title": "Lin, S., \u0026 Shum, H.-Y."}, {"color": "#66CCFF", "id": "separation of diffuse and specular reflection", "label": "separation of diffuse and specular reflection", "shape": "dot", "size": 25, "title": "separation of diffuse and specular reflection"}, {"color": "#66CCFF", "id": "Tan, R. T., Nishino, K., \u0026 Ikeuchi, K.", "label": "Tan, R. T., Nishino, K., \u0026 Ikeuchi, K.", "shape": "dot", "size": 25, "title": "Tan, R. T., Nishino, K., \u0026 Ikeuchi, K."}, {"color": "#66CCFF", "id": "Mallick, S. P., Zickler, T., Kriegman, D. J., \u0026 Belhumeur, P. N.", "label": "Mallick, S. P., Zickler, T., Kriegman, D. J., \u0026 Belhumeur, P. N.", "shape": "dot", "size": 25, "title": "Mallick, S. P., Zickler, T., Kriegman, D. J., \u0026 Belhumeur, P. N."}, {"color": "#66CCFF", "id": "PDE approach", "label": "PDE approach", "shape": "dot", "size": 25, "title": "PDE approach"}, {"color": "#66CCFF", "id": "diffuse reflection", "label": "diffuse reflection", "shape": "dot", "size": 25, "title": "diffuse reflection"}, {"color": "#66CCFF", "id": "specular removal", "label": "specular removal", "shape": "dot", "size": 25, "title": "specular removal"}, {"color": "#66CCFF", "id": "Institute of Artificial AI and Robotics", "label": "Institute of Artificial AI and Robotics", "shape": "dot", "size": 25, "title": "Institute of Artificial AI and Robotics"}, {"color": "#66CCFF", "id": "Institute of Artificial Intelligence and Robotics", "label": "Institute of Artificial Intelligence and Robotics", "shape": "dot", "size": 25, "title": "Institute of Artificial Intelligence and Robotics"}, {"color": "#66CCFF", "id": "Nara Institute of Science and Technology", "label": "Nara Institute of Science and Technology", "shape": "dot", "size": 25, "title": "Nara Institute of Science and Technology"}, {"color": "#66CCFF", "id": "Wuyuan Xie", "label": "Wuyuan Xie", "shape": "dot", "size": 25, "title": "Wuyuan Xie"}, {"color": "#66CCFF", "id": "Photometric Stereo with Near Point Lighting", "label": "Photometric Stereo with Near Point Lighting", "shape": "dot", "size": 25, "title": "Photometric Stereo with Near Point Lighting"}, {"color": "#66CCFF", "id": "Chengkai Dai", "label": "Chengkai Dai", "shape": "dot", "size": 25, "title": "Chengkai Dai"}, {"color": "#66CCFF", "id": "Charlie C. L. Wang", "label": "Charlie C. L. Wang", "shape": "dot", "size": 25, "title": "Charlie C. L. Wang"}, {"color": "#66CCFF", "id": "Xie_Photometric_Stereo_With_2015_CVPR_paper.pdf", "label": "Xie_Photometric_Stereo_With_2015_CVPR_paper.pdf", "shape": "dot", "size": 25, "title": "Xie_Photometric_Stereo_With_2015_CVPR_paper.pdf"}, {"color": "#66CCFF", "id": "photometric stereo", "label": "photometric stereo", "shape": "dot", "size": 25, "title": "photometric stereo"}, {"color": "#66CCFF", "id": "near point lighting", "label": "near point lighting", "shape": "dot", "size": 25, "title": "near point lighting"}, {"color": "#66CCFF", "id": "nonlinear relationship", "label": "nonlinear relationship", "shape": "dot", "size": 25, "title": "nonlinear relationship"}, {"color": "#66CCFF", "id": "local surface normals", "label": "local surface normals", "shape": "dot", "size": 25, "title": "local surface normals"}, {"color": "#66CCFF", "id": "light source positions", "label": "light source positions", "shape": "dot", "size": 25, "title": "light source positions"}, {"color": "#66CCFF", "id": "mesh deformation approach", "label": "mesh deformation approach", "shape": "dot", "size": 25, "title": "mesh deformation approach"}, {"color": "#66CCFF", "id": "facet position", "label": "facet position", "shape": "dot", "size": 25, "title": "facet position"}, {"color": "#66CCFF", "id": "facet orientation", "label": "facet orientation", "shape": "dot", "size": 25, "title": "facet orientation"}, {"color": "#66CCFF", "id": "local projection", "label": "local projection", "shape": "dot", "size": 25, "title": "local projection"}, {"color": "#66CCFF", "id": "global blending", "label": "global blending", "shape": "dot", "size": 25, "title": "global blending"}, {"color": "#66CCFF", "id": "accurate surface shape estimation", "label": "accurate surface shape estimation", "shape": "dot", "size": 25, "title": "accurate surface shape estimation"}, {"color": "#66CCFF", "id": "robustness to light source position errors", "label": "robustness to light source position errors", "shape": "dot", "size": 25, "title": "robustness to light source position errors"}, {"color": "#66CCFF", "id": "Photometric Stereo", "label": "Photometric Stereo", "shape": "dot", "size": 25, "title": "Photometric Stereo"}, {"color": "#66CCFF", "id": "Nonlinear Optimization", "label": "Nonlinear Optimization", "shape": "dot", "size": 25, "title": "Nonlinear Optimization"}, {"color": "#66CCFF", "id": "S. Barsky", "label": "S. Barsky", "shape": "dot", "size": 25, "title": "S. Barsky"}, {"color": "#66CCFF", "id": "4-source photometric stereo technique", "label": "4-source photometric stereo technique", "shape": "dot", "size": 25, "title": "4-source photometric stereo technique"}, {"color": "#66CCFF", "id": "highlights and shadows", "label": "highlights and shadows", "shape": "dot", "size": 25, "title": "highlights and shadows"}, {"color": "#66CCFF", "id": "D. Nehab", "label": "D. Nehab", "shape": "dot", "size": 25, "title": "D. Nehab"}, {"color": "#66CCFF", "id": "Efficiently combining positions and normals", "label": "Efficiently combining positions and normals", "shape": "dot", "size": 25, "title": "Efficiently combining positions and normals"}, {"color": "#66CCFF", "id": "precise 3d geometry", "label": "precise 3d geometry", "shape": "dot", "size": 25, "title": "precise 3d geometry"}, {"color": "#66CCFF", "id": "A. Hertzmann", "label": "A. Hertzmann", "shape": "dot", "size": 25, "title": "A. Hertzmann"}, {"color": "#66CCFF", "id": "Example-based photometric stereo", "label": "Example-based photometric stereo", "shape": "dot", "size": 25, "title": "Example-based photometric stereo"}, {"color": "#66CCFF", "id": "shape", "label": "shape", "shape": "dot", "size": 25, "title": "shape"}, {"color": "#66CCFF", "id": "Shape and spatially-ranging brdfs", "label": "Shape and spatially-ranging brdfs", "shape": "dot", "size": 25, "title": "Shape and spatially-ranging brdfs"}, {"color": "#66CCFF", "id": "Mesh Deformation", "label": "Mesh Deformation", "shape": "dot", "size": 25, "title": "Mesh Deformation"}, {"color": "#66CCFF", "id": "Near Point Lighting", "label": "Near Point Lighting", "shape": "dot", "size": 25, "title": "Near Point Lighting"}, {"color": "#66CCFF", "id": "Photometric stereo", "label": "Photometric stereo", "shape": "dot", "size": 25, "title": "Photometric stereo"}, {"color": "#66CCFF", "id": "Shape", "label": "Shape", "shape": "dot", "size": 25, "title": "Shape"}, {"color": "#66CCFF", "id": "Proceedings of the Fifth Eurographics Symposium on Geometry Processing", "label": "Proceedings of the Fifth Eurographics Symposium on Geometry Processing", "shape": "dot", "size": 25, "title": "Proceedings of the Fifth Eurographics Symposium on Geometry Processing"}, {"color": "#66CCFF", "id": "Computer Vision Workshops (ICCV Workshops)", "label": "Computer Vision Workshops (ICCV Workshops)", "shape": "dot", "size": 25, "title": "Computer Vision Workshops (ICCV Workshops)"}, {"color": "#66CCFF", "id": "Surface orientation", "label": "Surface orientation", "shape": "dot", "size": 25, "title": "Surface orientation"}, {"color": "#66CCFF", "id": "photometric method", "label": "photometric method", "shape": "dot", "size": 25, "title": "photometric method"}, {"color": "#66CCFF", "id": "Surface modeling", "label": "Surface modeling", "shape": "dot", "size": 25, "title": "Surface modeling"}, {"color": "#66CCFF", "id": "as-rigid-as-possible surface modeling", "label": "as-rigid-as-possible surface modeling", "shape": "dot", "size": 25, "title": "as-rigid-as-possible surface modeling"}, {"color": "#66CCFF", "id": "point light sources", "label": "point light sources", "shape": "dot", "size": 25, "title": "point light sources"}, {"color": "#66CCFF", "id": "Discrete geometry", "label": "Discrete geometry", "shape": "dot", "size": 25, "title": "Discrete geometry"}, {"color": "#66CCFF", "id": "BRDF", "label": "BRDF", "shape": "dot", "size": 25, "title": "BRDF"}, {"color": "#66CCFF", "id": "urographics Symposium on Geometry Processing", "label": "urographics Symposium on Geometry Processing", "shape": "dot", "size": 25, "title": "urographics Symposium on Geometry Processing"}, {"color": "#66CCFF", "id": "2007", "label": "2007", "shape": "dot", "size": 25, "title": "2007"}, {"color": "#66CCFF", "id": "S. Bouaziz", "label": "S. Bouaziz", "shape": "dot", "size": 25, "title": "S. Bouaziz"}, {"color": "#66CCFF", "id": "deep hashing (DH) approach", "label": "deep hashing (DH) approach", "shape": "dot", "size": 25, "title": "deep hashing (DH) approach"}, {"color": "#66CCFF", "id": "existing binary codes learning methods", "label": "existing binary codes learning methods", "shape": "dot", "size": 25, "title": "existing binary codes learning methods"}, {"color": "#66CCFF", "id": "single linear projection", "label": "single linear projection", "shape": "dot", "size": 25, "title": "single linear projection"}, {"color": "#66CCFF", "id": "deep neural network", "label": "deep neural network", "shape": "dot", "size": 25, "title": "deep neural network"}, {"color": "#66CCFF", "id": "hierarchical non-linear transformations", "label": "hierarchical non-linear transformations", "shape": "dot", "size": 25, "title": "hierarchical non-linear transformations"}, {"color": "#66CCFF", "id": "nonlinear relationship of samples", "label": "nonlinear relationship of samples", "shape": "dot", "size": 25, "title": "nonlinear relationship of samples"}, {"color": "#66CCFF", "id": "loss minimization", "label": "loss minimization", "shape": "dot", "size": 25, "title": "loss minimization"}, {"color": "#66CCFF", "id": "real-valued feature descriptor", "label": "real-valued feature descriptor", "shape": "dot", "size": 25, "title": "real-valued feature descriptor"}, {"color": "#66CCFF", "id": "each bit", "label": "each bit", "shape": "dot", "size": 25, "title": "each bit"}, {"color": "#66CCFF", "id": "different bits", "label": "different bits", "shape": "dot", "size": 25, "title": "different bits"}, {"color": "#66CCFF", "id": "independent", "label": "independent", "shape": "dot", "size": 25, "title": "independent"}, {"color": "#66CCFF", "id": "nary vector", "label": "nary vector", "shape": "dot", "size": 25, "title": "nary vector"}, {"color": "#66CCFF", "id": "each other", "label": "each other", "shape": "dot", "size": 25, "title": "each other"}, {"color": "#66CCFF", "id": "DH", "label": "DH", "shape": "dot", "size": 25, "title": "DH"}, {"color": "#66CCFF", "id": "discriminative term", "label": "discriminative term", "shape": "dot", "size": 25, "title": "discriminative term"}, {"color": "#66CCFF", "id": "learned binary codes", "label": "learned binary codes", "shape": "dot", "size": 25, "title": "learned binary codes"}, {"color": "#66CCFF", "id": "discriminative power", "label": "discriminative power", "shape": "dot", "size": 25, "title": "discriminative power"}, {"color": "#66CCFF", "id": "state-of-the-arts", "label": "state-of-the-arts", "shape": "dot", "size": 25, "title": "state-of-the-arts"}, {"color": "#66CCFF", "id": "Deep Hashing", "label": "Deep Hashing", "shape": "dot", "size": 25, "title": "Deep Hashing"}, {"color": "#66CCFF", "id": "Binary Codes Learning", "label": "Binary Codes Learning", "shape": "dot", "size": 25, "title": "Binary Codes Learning"}, {"color": "#66CCFF", "id": "maximize inter-class variations", "label": "maximize inter-class variations", "shape": "dot", "size": 25, "title": "maximize inter-class variations"}, {"color": "#66CCFF", "id": "minimize intra-class variations", "label": "minimize intra-class variations", "shape": "dot", "size": 25, "title": "minimize intra-class variations"}, {"color": "#66CCFF", "id": "Large-Scale Visual Search", "label": "Large-Scale Visual Search", "shape": "dot", "size": 25, "title": "Large-Scale Visual Search"}, {"color": "#66CCFF", "id": "Hashing Functions", "label": "Hashing Functions", "shape": "dot", "size": 25, "title": "Hashing Functions"}, {"color": "#66CCFF", "id": "Deep HHashing", "label": "Deep HHashing", "shape": "dot", "size": 25, "title": "Deep HHashing"}, {"color": "#66CCFF", "id": "Andoni \u0026 Indyk (2006)", "label": "Andoni \u0026 Indyk (2006)", "shape": "dot", "size": 25, "title": "Andoni \u0026 Indyk (2006)"}, {"color": "#66CCFF", "id": "Near-optimal Hashing Algorithms", "label": "Near-optimal Hashing Algorithms", "shape": "dot", "size": 25, "title": "Near-optimal Hashing Algorithms"}, {"color": "#66CCFF", "id": "Gong et al. (2012)", "label": "Gong et al. (2012)", "shape": "dot", "size": 25, "title": "Gong et al. (2012)"}, {"color": "#66CCFF", "id": "Angular Quantization-based Binary Codes", "label": "Angular Quantization-based Binary Codes", "shape": "dot", "size": 25, "title": "Angular Quantization-based Binary Codes"}, {"color": "#66CCFF", "id": "Hinton \u0026 Salakhutdinov (2006)", "label": "Hinton \u0026 Salakhutdinov (2006)", "shape": "dot", "size": 25, "title": "Hinton \u0026 Salakhutdinov (2006)"}, {"color": "#66CCFF", "id": "Reducing Data Dimensionality", "label": "Reducing Data Dimensionality", "shape": "dot", "size": 25, "title": "Reducing Data Dimensionality"}, {"color": "#66CCFF", "id": "Approximate Nearest Neighbor Search", "label": "Approximate Nearest Neighbor Search", "shape": "dot", "size": 25, "title": "Approximate Nearest Neighbor Search"}, {"color": "#66CCFF", "id": "Fast Similarity Search", "label": "Fast Similarity Search", "shape": "dot", "size": 25, "title": "Fast Similarity Search"}, {"color": "#66CCFF", "id": "Neural Networks", "label": "Neural Networks", "shape": "dot", "size": 25, "title": "Neural Networks"}, {"color": "#66CCFF", "id": "data dimensionality", "label": "data dimensionality", "shape": "dot", "size": 25, "title": "data dimensionality"}, {"color": "#66CCFF", "id": "Tiny Images", "label": "Tiny Images", "shape": "dot", "size": 25, "title": "Tiny Images"}, {"color": "#66CCFF", "id": "nonparametric object recognition", "label": "nonparametric object recognition", "shape": "dot", "size": 25, "title": "nonparametric object recognition"}, {"color": "#66CCFF", "id": "Hash Bit Selection", "label": "Hash Bit Selection", "shape": "dot", "size": 25, "title": "Hash Bit Selection"}, {"color": "#66CCFF", "id": "unified solution", "label": "unified solution", "shape": "dot", "size": 25, "title": "unified solution"}, {"color": "#66CCFF", "id": "Shift-invariant kernels", "label": "Shift-invariant kernels", "shape": "dot", "size": 25, "title": "Shift-invariant kernels"}, {"color": "#66CCFF", "id": "locality-sensitive binary codes", "label": "locality-sensitive binary codes", "shape": "dot", "size": 25, "title": "locality-sensitive binary codes"}, {"color": "#66CCFF", "id": "Minimal Loss Hashing", "label": "Minimal Loss Hashing", "shape": "dot", "size": 25, "title": "Minimal Loss Hashing"}, {"color": "#66CCFF", "id": "Science", "label": "Science", "shape": "dot", "size": 25, "title": "Science"}, {"color": "#66CCFF", "id": "Torralba, A.", "label": "Torralba, A.", "shape": "dot", "size": 25, "title": "Torralba, A."}, {"color": "#66CCFF", "id": "80 million tiny images", "label": "80 million tiny images", "shape": "dot", "size": 25, "title": "80 million tiny images"}, {"color": "#66CCFF", "id": "*PAM*I", "label": "*PAM*I", "shape": "dot", "size": 25, "title": "*PAM*I"}, {"color": "#66CCFF", "id": "Wang, J.", "label": "Wang, J.", "shape": "dot", "size": 25, "title": "Wang, J."}, {"color": "#66CCFF", "id": "Venice Erin Liong", "label": "Venice Erin Liong", "shape": "dot", "size": 25, "title": "Venice Erin Liong"}, {"color": "#66CCFF", "id": "Gang Wang", "label": "Gang Wang", "shape": "dot", "size": 25, "title": "Gang Wang"}, {"color": "#66CCFF", "id": "Department of ECE", "label": "Department of ECE", "shape": "dot", "size": 25, "title": "Department of ECE"}, {"color": "#66CCFF", "id": "Jie Zhou", "label": "Jie Zhou", "shape": "dot", "size": 25, "title": "Jie Zhou"}, {"color": "#66CCFF", "id": "Department of Automation", "label": "Department of Automation", "shape": "dot", "size": 25, "title": "Department of Automation"}, {"color": "#66CCFF", "id": "Dongyoon Han", "label": "Dongyoon Han", "shape": "dot", "size": 25, "title": "Dongyoon Han"}, {"color": "#66CCFF", "id": "Unsupervised Simultaneous Orthogonal Basis Clustering Feature Selection", "label": "Unsupervised Simultaneous Orthogonal Basis Clustering Feature Selection", "shape": "dot", "size": 25, "title": "Unsupervised Simultaneous Orthogonal Basis Clustering Feature Selection"}, {"color": "#66CCFF", "id": "jzhou@tsinghua.edu.cn", "label": "jzhou@tsinghua.edu.cn", "shape": "dot", "size": 25, "title": "jzhou@tsinghua.edu.cn"}, {"color": "#66CCFF", "id": "Han_Unsupervised_Simultaneous_Orthogonal_2015_CVPR_paper", "label": "Han_Unsupervised_Simultaneous_Orthogonal_2015_CVPR_paper", "shape": "dot", "size": 25, "title": "Han_Unsupervised_Simultaneous_Orthogonal_2015_CVPR_paper"}, {"color": "#66CCFF", "id": "Han_Unpublished_Simultaneous_Orthogonal_2015_CVPR_paper", "label": "Han_Unpublished_Simultaneous_Orthogonal_2015_CVPR_paper", "shape": "dot", "size": 25, "title": "Han_Unpublished_Simultaneous_Orthogonal_2015_CVPR_paper"}, {"color": "#66CCFF", "id": "feature selection methods", "label": "feature selection methods", "shape": "dot", "size": 25, "title": "feature selection methods"}, {"color": "#66CCFF", "id": "supervised and unsupervised feature selection methods", "label": "supervised and unsupervised feature selection methods", "shape": "dot", "size": 25, "title": "supervised and unsupervised feature selection methods"}, {"color": "#66CCFF", "id": "SOCFS", "label": "SOCFS", "shape": "dot", "size": 25, "title": "SOCFS"}, {"color": "#66CCFF", "id": "unsupervised feature selection method", "label": "unsupervised feature selection method", "shape": "dot", "size": 25, "title": "unsupervised feature selection method"}, {"color": "#66CCFF", "id": "feature selection on unlabeled data", "label": "feature selection on unlabeled data", "shape": "dot", "size": 25, "title": "feature selection on unlabeled data"}, {"color": "#66CCFF", "id": "regularized regression-based formulation", "label": "regularized regression-based formulation", "shape": "dot", "size": 25, "title": "regularized regression-based formulation"}, {"color": "#66CCFF", "id": "target matrix", "label": "target matrix", "shape": "dot", "size": 25, "title": "target matrix"}, {"color": "#66CCFF", "id": "latent cluster centers", "label": "latent cluster centers", "shape": "dot", "size": 25, "title": "latent cluster centers"}, {"color": "#66CCFF", "id": "projection matrix", "label": "projection matrix", "shape": "dot", "size": 25, "title": "projection matrix"}, {"color": "#66CCFF", "id": "discriminative features", "label": "discriminative features", "shape": "dot", "size": 25, "title": "discriminative features"}, {"color": "#66CCFF", "id": "real world datasets", "label": "real world datasets", "shape": "dot", "size": 25, "title": "real world datasets"}, {"color": "#66CCFF", "id": "Nie et al.", "label": "Nie et al.", "shape": "dot", "size": 25, "title": "Nie et al."}, {"color": "#66CCFF", "id": "feature selection via joint l2,1-norms minimization", "label": "feature selection via joint l2,1-norms minimization", "shape": "dot", "size": 25, "title": "feature selection via joint l2,1-norms minimization"}, {"color": "#66CCFF", "id": "Nene et al.", "label": "Nene et al.", "shape": "dot", "size": 25, "title": "Nene et al."}, {"color": "#66CCFF", "id": "Columbia object image library (coil-20)", "label": "Columbia object image library (coil-20)", "shape": "dot", "size": 25, "title": "Columbia object image library (coil-20)"}, {"color": "#66CCFF", "id": "CCUCS-005-96", "label": "CCUCS-005-96", "shape": "dot", "size": 25, "title": "CCUCS-005-96"}, {"color": "#66CCFF", "id": "Yang et al.", "label": "Yang et al.", "shape": "dot", "size": 25, "title": "Yang et al."}, {"color": "#66CCFF", "id": "l2,1-norm regularized discriminative feature selection", "label": "l2,1-norm regularized discriminative feature selection", "shape": "dot", "size": 25, "title": "l2,1-norm regularized discriminative feature selection"}, {"color": "#66CCFF", "id": "Qian and Zhai", "label": "Qian and Zhai", "shape": "dot", "size": 25, "title": "Qian and Zhai"}, {"color": "#66CCFF", "id": "Robust unsupervised feature selection", "label": "Robust unsupervised feature selection", "shape": "dot", "size": 25, "title": "Robust unsupervised feature selection"}, {"color": "#66CCFF", "id": "IJCAI", "label": "IJCAI", "shape": "dot", "size": 25, "title": "IJCAI"}, {"color": "#66CCFF", "id": "Sch\u00a8onemann", "label": "Sch\u00a8onemann", "shape": "dot", "size": 25, "title": "Sch\u00a8onemann"}, {"color": "#66CCFF", "id": "generalized solution of the orthogonal Procustes problem", "label": "generalized solution of the orthogonal Procustes problem", "shape": "dot", "size": 25, "title": "generalized solution of the orthogonal Procustes problem"}, {"color": "#66CCFF", "id": "Zhao and Liu", "label": "Zhao and Liu", "shape": "dot", "size": 25, "title": "Zhao and Liu"}, {"color": "#66CCFF", "id": "Spectral feature selection", "label": "Spectral feature selection", "shape": "dot", "size": 25, "title": "Spectral feature selection"}, {"color": "#66CCFF", "id": "Procrustes problem", "label": "Procrustes problem", "shape": "dot", "size": 25, "title": "Procrustes problem"}, {"color": "#66CCFF", "id": "Psychometrika", "label": "Psychometrika", "shape": "dot", "size": 25, "title": "Psychometrika"}, {"color": "#66CCFF", "id": "Zhao, Z.", "label": "Zhao, Z.", "shape": "dot", "size": 25, "title": "Zhao, Z."}, {"color": "#66CCFF", "id": "Samaria, F. S.", "label": "Samaria, F. S.", "shape": "dot", "size": 25, "title": "Samaria, F. S."}, {"color": "#66CCFF", "id": "stochastic model", "label": "stochastic model", "shape": "dot", "size": 25, "title": "stochastic model"}, {"color": "#66CCFF", "id": "Jianming Zhang", "label": "Jianming Zhang", "shape": "dot", "size": 25, "title": "Jianming Zhang"}, {"color": "#66CCFF", "id": "Salient Object Subitizing", "label": "Salient Object Subitizing", "shape": "dot", "size": 25, "title": "Salient Object Subitizing"}, {"color": "#66CCFF", "id": "Shugao Ma", "label": "Shugao Ma", "shape": "dot", "size": 25, "title": "Shugao Ma"}, {"color": "#66CCFF", "id": "Mehrnooush Sameki", "label": "Mehrnooush Sameki", "shape": "dot", "size": 25, "title": "Mehrnooush Sameki"}, {"color": "#66CCFF", "id": "Stan Sclaroff", "label": "Stan Sclaroff", "shape": "dot", "size": 25, "title": "Stan Sclaroff"}, {"color": "#66CCFF", "id": "Margrit Betke", "label": "Margrit Betke", "shape": "dot", "size": 25, "title": "Margrit Betke"}, {"color": "#66CCFF", "id": "Radom\u00edr M\u011bch", "label": "Radom\u00edr M\u011bch", "shape": "dot", "size": 25, "title": "Radom\u00edr M\u011bch"}, {"color": "#66CCFF", "id": "People", "label": "People", "shape": "dot", "size": 25, "title": "People"}, {"color": "#66CCFF", "id": "subitizing", "label": "subitizing", "shape": "dot", "size": 25, "title": "subitizing"}, {"color": "#66CCFF", "id": "Salient Object Subitizing (SOS)", "label": "Salient Object Subitizing (SOS)", "shape": "dot", "size": 25, "title": "Salient Object Subitizing (SOS)"}, {"color": "#66CCFF", "id": "predict existence and number", "label": "predict existence and number", "shape": "dot", "size": 25, "title": "predict existence and number"}, {"color": "#66CCFF", "id": "annotated through crowdsourcing", "label": "annotated through crowdsourcing", "shape": "dot", "size": 25, "title": "annotated through crowdsourcing"}, {"color": "#66CCFF", "id": "high accuracy", "label": "high accuracy", "shape": "dot", "size": 25, "title": "high accuracy"}, {"color": "#66CCFF", "id": "salient objects", "label": "salient objects", "shape": "dot", "size": 25, "title": "salient objects"}, {"color": "#66CCFF", "id": "number of objects", "label": "number of objects", "shape": "dot", "size": 25, "title": "number of objects"}, {"color": "#66CCFF", "id": "object proposal applications", "label": "object proposal applications", "shape": "dot", "size": 25, "title": "object proposal applications"}, {"color": "#66CCFF", "id": "Crowd Sourcing", "label": "Crowd Sourcing", "shape": "dot", "size": 25, "title": "Crowd Sourcing"}, {"color": "#66CCFF", "id": "Holistic Image Analysis", "label": "Holistic Image Analysis", "shape": "dot", "size": 25, "title": "Holistic Image Analysis"}, {"color": "#66CCFF", "id": "Computer Vision Applications", "label": "Computer Vision Applications", "shape": "dot", "size": 25, "title": "Computer Vision Applications"}, {"color": "#66CCFF", "id": "robot vision", "label": "robot vision", "shape": "dot", "size": 25, "title": "robot vision"}, {"color": "#66CCFF", "id": "Boston University", "label": "Boston University", "shape": "dot", "size": 25, "title": "Boston University"}, {"color": "#66CCFF", "id": "Mehrnoosh Sameki", "label": "Mehrnoosh Sameki", "shape": "dot", "size": 25, "title": "Mehrnoosh Sameki"}, {"color": "#66CCFF", "id": "Radom\u00b4\u0131r M\u02d8ech", "label": "Radom\u00b4\u0131r M\u02d8ech", "shape": "dot", "size": 25, "title": "Radom\u00b4\u0131r M\u02d8ech"}, {"color": "#66CCFF", "id": "Discriminaitve Shape from Shading", "label": "Discriminaitve Shape from Shading", "shape": "dot", "size": 25, "title": "Discriminaitve Shape from Shading"}, {"color": "#66CCFF", "id": "Richter_Discriminaitve_Shape_From_2015_CVPR_paper.pdf", "label": "Richter_Discriminaitve_Shape_From_2015_CVPR_paper.pdf", "shape": "dot", "size": 25, "title": "Richter_Discriminaitve_Shape_From_2015_CVPR_paper.pdf"}, {"color": "#66CCFF", "id": "Estimating surface normals", "label": "Estimating surface normals", "shape": "dot", "size": 25, "title": "Estimating surface normals"}, {"color": "#66CCFF", "id": "challenging problem", "label": "challenging problem", "shape": "dot", "size": 25, "title": "challenging problem"}, {"color": "#66CCFF", "id": "under-constrained problem", "label": "under-constrained problem", "shape": "dot", "size": 25, "title": "under-constrained problem"}, {"color": "#66CCFF", "id": "Simplifying assumptions", "label": "Simplifying assumptions", "shape": "dot", "size": 25, "title": "Simplifying assumptions"}, {"color": "#66CCFF", "id": "directional lighting", "label": "directional lighting", "shape": "dot", "size": 25, "title": "directional lighting"}, {"color": "#66CCFF", "id": "known reflectance maps", "label": "known reflectance maps", "shape": "dot", "size": 25, "title": "known reflectance maps"}, {"color": "#66CCFF", "id": "regression forests", "label": "regression forests", "shape": "dot", "size": 25, "title": "regression forests"}, {"color": "#66CCFF", "id": "Von Mises-Fisher distributions", "label": "Von Mises-Fisher distributions", "shape": "dot", "size": 25, "title": "Von Mises-Fisher distributions"}, {"color": "#66CCFF", "id": "spatial features", "label": "spatial features", "shape": "dot", "size": 25, "title": "spatial features"}, {"color": "#66CCFF", "id": "textons", "label": "textons", "shape": "dot", "size": 25, "title": "textons"}, {"color": "#66CCFF", "id": "novel silhouette features", "label": "novel silhouette features", "shape": "dot", "size": 25, "title": "novel silhouette features"}, {"color": "#66CCFF", "id": "generalization", "label": "generalization", "shape": "dot", "size": 25, "title": "generalization"}, {"color": "#66CCFF", "id": "uncalibrated illumination", "label": "uncalibrated illumination", "shape": "dot", "size": 25, "title": "uncalibrated illumination"}, {"color": "#66CCFF", "id": "pixel-independent prediction", "label": "pixel-independent prediction", "shape": "dot", "size": 25, "title": "pixel-independent prediction"}, {"color": "#66CCFF", "id": "efficient estimation", "label": "efficient estimation", "shape": "dot", "size": 25, "title": "efficient estimation"}, {"color": "#66CCFF", "id": "research area", "label": "research area", "shape": "dot", "size": 25, "title": "research area"}, {"color": "#66CCFF", "id": "Discrimiative Learning", "label": "Discrimiative Learning", "shape": "dot", "size": 25, "title": "Discrimiative Learning"}, {"color": "#66CCFF", "id": "J. T. Barron", "label": "J. T. Barron", "shape": "dot", "size": 25, "title": "J. T. Barron"}, {"color": "#66CCFF", "id": "Color constancy, intrinsic images, and shape estimation", "label": "Color constancy, intrinsic images, and shape estimation", "shape": "dot", "size": 25, "title": "Color constancy, intrinsic images, and shape estimation"}, {"color": "#66CCFF", "id": "Shape, albedo, and illumination from a single image", "label": "Shape, albedo, and illumination from a single image", "shape": "dot", "size": 25, "title": "Shape, albedo, and illumination from a single image"}, {"color": "#66CCFF", "id": "J. Ben-Arie", "label": "J. Ben-Arie", "shape": "dot", "size": 25, "title": "J. Ben-Arie"}, {"color": "#66CCFF", "id": "A neural network approach", "label": "A neural network approach", "shape": "dot", "size": 25, "title": "A neural network approach"}, {"color": "#66CCFF", "id": "L. Breiman", "label": "L. Breiman", "shape": "dot", "size": 25, "title": "L. Breiman"}, {"color": "#66CCFF", "id": "ShapeCollage", "label": "ShapeCollage", "shape": "dot", "size": 25, "title": "ShapeCollage"}, {"color": "#66CCFF", "id": "example-based methods", "label": "example-based methods", "shape": "dot", "size": 25, "title": "example-based methods"}, {"color": "#66CCFF", "id": "Image-to-geometry registration", "label": "Image-to-geometry registration", "shape": "dot", "size": 25, "title": "Image-to-geometry registration"}, {"color": "#66CCFF", "id": "Modeling data", "label": "Modeling data", "shape": "dot", "size": 25, "title": "Modeling data"}, {"color": "#66CCFF", "id": "directional distributions", "label": "directional distributions", "shape": "dot", "size": 25, "title": "directional distributions"}, {"color": "#66CCFF", "id": "Dispersion", "label": "Dispersion", "shape": "dot", "size": 25, "title": "Dispersion"}, {"color": "#66CCFF", "id": "P. Roy. Soc. Lond. B", "label": "P. Roy. Soc. Lond. B", "shape": "dot", "size": 25, "title": "P. Roy. Soc. Lond. B"}, {"color": "#66CCFF", "id": "Floating scale reconstruction", "label": "Floating scale reconstruction", "shape": "dot", "size": 25, "title": "Floating scale reconstruction"}, {"color": "#66CCFF", "id": "SIGGRAPH", "label": "SIGGRAPH", "shape": "dot", "size": 25, "title": "SIGGRAPH"}, {"color": "#66CCFF", "id": "baseline evaluations", "label": "baseline evaluations", "shape": "dot", "size": 25, "title": "baseline evaluations"}, {"color": "#66CCFF", "id": "occlusion-aware shape interpretation", "label": "occlusion-aware shape interpretation", "shape": "dot", "size": 25, "title": "occlusion-aware shape interpretation"}, {"color": "#66CCFF", "id": "Adelson", "label": "Adelson", "shape": "dot", "size": 25, "title": "Adelson"}, {"color": "#66CCFF", "id": "W. T. Freeman", "label": "W. T. Freeman", "shape": "dot", "size": 25, "title": "W. T. Freeman"}, {"color": "#66CCFF", "id": "Jean-Dominique FAVREAU", "label": "Jean-Dominique FAVREAU", "shape": "dot", "size": 25, "title": "Jean-Dominique FAVREAU"}, {"color": "#66CCFF", "id": "Line Drawing Interpretation", "label": "Line Drawing Interpretation", "shape": "dot", "size": 25, "title": "Line Drawing Interpretation"}, {"color": "#66CCFF", "id": "Adrien Bousseau", "label": "Adrien Bousseau", "shape": "dot", "size": 25, "title": "Adrien Bousseau"}, {"color": "#66CCFF", "id": "interpreting line drawings", "label": "interpreting line drawings", "shape": "dot", "size": 25, "title": "interpreting line drawings"}, {"color": "#66CCFF", "id": "line drawings", "label": "line drawings", "shape": "dot", "size": 25, "title": "line drawings"}, {"color": "#66CCFF", "id": "imaginary objects", "label": "imaginary objects", "shape": "dot", "size": 25, "title": "imaginary objects"}, {"color": "#66CCFF", "id": "photographs", "label": "photographs", "shape": "dot", "size": 25, "title": "photographs"}, {"color": "#66CCFF", "id": "computer vision algorithms", "label": "computer vision algorithms", "shape": "dot", "size": 25, "title": "computer vision algorithms"}, {"color": "#66CCFF", "id": "limited support", "label": "limited support", "shape": "dot", "size": 25, "title": "limited support"}, {"color": "#66CCFF", "id": "multi-view stereo algorithms", "label": "multi-view stereo algorithms", "shape": "dot", "size": 25, "title": "multi-view stereo algorithms"}, {"color": "#66CCFF", "id": "real-world scenes", "label": "real-world scenes", "shape": "dot", "size": 25, "title": "real-world scenes"}, {"color": "#66CCFF", "id": "line-drawing interpretation algorithms", "label": "line-drawing interpretation algorithms", "shape": "dot", "size": 25, "title": "line-drawing interpretation algorithms"}, {"color": "#66CCFF", "id": "contextual awareness", "label": "contextual awareness", "shape": "dot", "size": 25, "title": "contextual awareness"}, {"color": "#66CCFF", "id": "strengths", "label": "strengths", "shape": "dot", "size": 25, "title": "strengths"}, {"color": "#66CCFF", "id": "dominant orientations", "label": "dominant orientations", "shape": "dot", "size": 25, "title": "dominant orientations"}, {"color": "#66CCFF", "id": "interpretation", "label": "interpretation", "shape": "dot", "size": 25, "title": "interpretation"}, {"color": "#66CCFF", "id": "polygon", "label": "polygon", "shape": "dot", "size": 25, "title": "polygon"}, {"color": "#66CCFF", "id": "orientation", "label": "orientation", "shape": "dot", "size": 25, "title": "orientation"}, {"color": "#66CCFF", "id": "creation", "label": "creation", "shape": "dot", "size": 25, "title": "creation"}, {"color": "#66CCFF", "id": "new structures", "label": "new structures", "shape": "dot", "size": 25, "title": "new structures"}, {"color": "#66CCFF", "id": "real world", "label": "real world", "shape": "dot", "size": 25, "title": "real world"}, {"color": "#66CCFF", "id": "furniture design", "label": "furniture design", "shape": "dot", "size": 25, "title": "furniture design"}, {"color": "#66CCFF", "id": "archaeology", "label": "archaeology", "shape": "dot", "size": 25, "title": "archaeology"}, {"color": "#66CCFF", "id": "new orientation", "label": "new orientation", "shape": "dot", "size": 25, "title": "new orientation"}, {"color": "#66CCFF", "id": "creation of new structures", "label": "creation of new structures", "shape": "dot", "size": 25, "title": "creation of new structures"}, {"color": "#66CCFF", "id": "unknown orientation", "label": "unknown orientation", "shape": "dot", "size": 25, "title": "unknown orientation"}, {"color": "#66CCFF", "id": "application domain", "label": "application domain", "shape": "dot", "size": 25, "title": "application domain"}, {"color": "#66CCFF", "id": "Computer-Aided Design", "label": "Computer-Aided Design", "shape": "dot", "size": 25, "title": "Computer-Aided Design"}, {"color": "#66CCFF", "id": "Computer-Aided Design application", "label": "Computer-Aided Design application", "shape": "dot", "size": 25, "title": "Computer-Aided Design application"}, {"color": "#66CCFF", "id": "Furniture Design", "label": "Furniture Design", "shape": "dot", "size": 25, "title": "Furniture Design"}, {"color": "#66CCFF", "id": "O-snap", "label": "O-snap", "shape": "dot", "size": 25, "title": "O-snap"}, {"color": "#66CCFF", "id": "optimization-based snapping method", "label": "optimization-based snapping method", "shape": "dot", "size": 25, "title": "optimization-based snapping method"}, {"color": "#66CCFF", "id": "M. Arikan", "label": "M. Arikan", "shape": "dot", "size": 25, "title": "M. Arikan"}, {"color": "#66CCFF", "id": "3D Scene Understanding", "label": "3D Scene Understanding", "shape": "dot", "size": 25, "title": "3D Scene Understanding"}, {"color": "#66CCFF", "id": "H. Barrow", "label": "H. Barrow", "shape": "dot", "size": 25, "title": "H. Barrow"}, {"color": "#66CCFF", "id": "Y. Boykov", "label": "Y. Boykov", "shape": "dot", "size": 25, "title": "Y. Boykov"}, {"color": "#66CCFF", "id": "energy minimization algorithms", "label": "energy minimization algorithms", "shape": "dot", "size": 25, "title": "energy minimization algorithms"}, {"color": "#66CCFF", "id": "A.-L. Chauve", "label": "A.-L. Chauve", "shape": "dot", "size": 25, "title": "A.-L. Chauve"}, {"color": "#66CCFF", "id": "3D reconstruction methods", "label": "3D reconstruction methods", "shape": "dot", "size": 25, "title": "3D reconstruction methods"}, {"color": "#66CCFF", "id": "Multi-View Stereo Reconstruction", "label": "Multi-View Stereo Reconstruction", "shape": "dot", "size": 25, "title": "Multi-View Stereo Reconstruction"}, {"color": "#66CCFF", "id": "minimization in vision", "label": "minimization in vision", "shape": "dot", "size": 25, "title": "minimization in vision"}, {"color": "#66CCFF", "id": "Furukawa et al.", "label": "Furukawa et al.", "shape": "dot", "size": 25, "title": "Furukawa et al."}, {"color": "#66CCFF", "id": "Manhattan-world stereo", "label": "Manhattan-world stereo", "shape": "dot", "size": 25, "title": "Manhattan-world stereo"}, {"color": "#66CCFF", "id": "Architectural modeling", "label": "Architectural modeling", "shape": "dot", "size": 25, "title": "Architectural modeling"}, {"color": "#66CCFF", "id": "Debevec et al.", "label": "Debevec et al.", "shape": "dot", "size": 25, "title": "Debevec et al."}, {"color": "#66CCFF", "id": "Wiley-ISTE", "label": "Wiley-ISTE", "shape": "dot", "size": 25, "title": "Wiley-ISTE"}, {"color": "#66CCFF", "id": "Stochastic geometry for image analysis", "label": "Stochastic geometry for image analysis", "shape": "dot", "size": 25, "title": "Stochastic geometry for image analysis"}, {"color": "#66CCFF", "id": "INRIA Sophia-Antippolis", "label": "INRIA Sophia-Antippolis", "shape": "dot", "size": 25, "title": "INRIA Sophia-Antippolis"}, {"color": "#66CCFF", "id": "Florent LAFARGE", "label": "Florent LAFARGE", "shape": "dot", "size": 25, "title": "Florent LAFARGE"}, {"color": "#66CCFF", "id": "INRIA Sophia-Antipollis", "label": "INRIA Sophia-Antipollis", "shape": "dot", "size": 25, "title": "INRIA Sophia-Antipollis"}, {"color": "#66CCFF", "id": "Bis Publishers", "label": "Bis Publishers", "shape": "dot", "size": 25, "title": "Bis Publishers"}, {"color": "#66CCFF", "id": " Sketching: The Basics", "label": " Sketching: The Basics", "shape": "dot", "size": 25, "title": " Sketching: The Basics"}, {"color": "#66CCFF", "id": "jean-dominuque.favreau@inria.fr", "label": "jean-dominuque.favreau@inria.fr", "shape": "dot", "size": 25, "title": "jean-dominuque.favreau@inria.fr"}, {"color": "#66CCFF", "id": "INRIA Sophia-Antipolis", "label": "INRIA Sophia-Antipolis", "shape": "dot", "size": 25, "title": "INRIA Sophia-Antipolis"}, {"color": "#66CCFF", "id": "INRIAL Sophia-Antipolis", "label": "INRIAL Sophia-Antipolis", "shape": "dot", "size": 25, "title": "INRIAL Sophia-Antipolis"}, {"color": "#66CCFF", "id": "Olga Russakovsky", "label": "Olga Russakovsky", "shape": "dot", "size": 25, "title": "Olga Russakovsky"}, {"color": "#66CCFF", "id": "Best of both worlds", "label": "Best of both worlds", "shape": "dot", "size": 25, "title": "Best of both worlds"}, {"color": "#66CCFF", "id": "Li-Jia Li", "label": "Li-Jia Li", "shape": "dot", "size": 25, "title": "Li-Jia Li"}, {"color": "#66CCFF", "id": "Li Fei-Fei", "label": "Li Fei-Fei", "shape": "dot", "size": 25, "title": "Li Fei-Fei"}, {"color": "#66CCFF", "id": "Russakovsky_Best_of_Both_2015_CVPR_paper.pdf", "label": "Russakovsky_Best_of_Both_2015_CVPR_paper.pdf", "shape": "dot", "size": 25, "title": "Russakovsky_Best_of_Both_2015_CVPR_paper.pdf"}, {"color": "#66CCFF", "id": "jean-dominique.favreau@inria.fr", "label": "jean-dominique.favreau@inria.fr", "shape": "dot", "size": 25, "title": "jean-dominique.favreau@inria.fr"}, {"color": "#66CCFF", "id": "jean-dominique.favreau", "label": "jean-dominique.favreau", "shape": "dot", "size": 25, "title": "jean-dominique.favreau"}, {"color": "#66CCFF", "id": "florent.lafarge@inria.fr", "label": "florent.lafarge@inria.fr", "shape": "dot", "size": 25, "title": "florent.lafarge@inria.fr"}, {"color": "#66CCFF", "id": "adrien.bousseau@inria.fr", "label": "adrien.bousseau@inria.fr", "shape": "dot", "size": 25, "title": "adrien.bousseau@inria.fr"}, {"color": "#66CCFF", "id": "localizing every object in an image", "label": "localizing every object in an image", "shape": "dot", "size": 25, "title": "localizing every object in an image"}, {"color": "#66CCFF", "id": "manual annotation", "label": "manual annotation", "shape": "dot", "size": 25, "title": "manual annotation"}, {"color": "#66CCFF", "id": "quite expensive", "label": "quite expensive", "shape": "dot", "size": 25, "title": "quite expensive"}, {"color": "#66CCFF", "id": "crowd engineering innovations", "label": "crowd engineering innovations", "shape": "dot", "size": 25, "title": "crowd engineering innovations"}, {"color": "#66CCFF", "id": "automatic object detectors", "label": "automatic object detectors", "shape": "dot", "size": 25, "title": "automatic object detectors"}, {"color": "#66CCFF", "id": "at most a few objects per image", "label": "at most a few objects per image", "shape": "dot", "size": 25, "title": "at most a few objects per image"}, {"color": "#66CCFF", "id": "object detection advancements", "label": "object detection advancements", "shape": "dot", "size": 25, "title": "object detection advancements"}, {"color": "#66CCFF", "id": "crowd engineering", "label": "crowd engineering", "shape": "dot", "size": 25, "title": "crowd engineering"}, {"color": "#66CCFF", "id": "image to annotate", "label": "image to annotate", "shape": "dot", "size": 25, "title": "image to annotate"}, {"color": "#66CCFF", "id": "annotation constraints", "label": "annotation constraints", "shape": "dot", "size": 25, "title": "annotation constraints"}, {"color": "#66CCFF", "id": "object annotations", "label": "object annotations", "shape": "dot", "size": 25, "title": "object annotations"}, {"color": "#66CCFF", "id": "human feedback", "label": "human feedback", "shape": "dot", "size": 25, "title": "human feedback"}, {"color": "#66CCFF", "id": "computer vision models", "label": "computer vision models", "shape": "dot", "size": 25, "title": "computer vision models"}, {"color": "#66CCFF", "id": "human input", "label": "human input", "shape": "dot", "size": 25, "title": "human input"}, {"color": "#66CCFF", "id": "Markov Decision Process", "label": "Markov Decision Process", "shape": "dot", "size": 25, "title": "Markov Decision Process"}, {"color": "#66CCFF", "id": "ILSVRC2014 dataset", "label": "ILSVRC2014 dataset", "shape": "dot", "size": 25, "title": "ILSVRC2014 dataset"}, {"color": "#66CCFF", "id": "feedback", "label": "feedback", "shape": "dot", "size": 25, "title": "feedback"}, {"color": "#66CCFF", "id": "human-in-the-loop labeling approach", "label": "human-in-the-loop labeling approach", "shape": "dot", "size": 25, "title": "human-in-the-loop labeling approach"}, {"color": "#66CCFF", "id": "ILSVRC2014 object detection dataset", "label": "ILSVRC2014 object detection dataset", "shape": "dot", "size": 25, "title": "ILSVRC2014 object detection dataset"}, {"color": "#66CCFF", "id": "Stanford University", "label": "Stanford University", "shape": "dot", "size": 25, "title": "Stanford University"}, {"color": "#66CCFF", "id": "Snapchat", "label": "Snapchat", "shape": "dot", "size": 25, "title": "Snapchat"}, {"color": "#66CCFF", "id": "Visesh Chari", "label": "Visesh Chari", "shape": "dot", "size": 25, "title": "Visesh Chari"}, {"color": "#66CCFF", "id": "On Pairwise Costs for Network Flow Multi-Object Tracking", "label": "On Pairwise Costs for Network Flow Multi-Object Tracking", "shape": "dot", "size": 25, "title": "On Pairwise Costs for Network Flow Multi-Object Tracking"}, {"color": "#66CCFF", "id": "Ivan Laptev", "label": "Ivan Laptev", "shape": "dot", "size": 25, "title": "Ivan Laptev"}, {"color": "#66CCFF", "id": "Josef Sivic", "label": "Josef Sivic", "shape": "dot", "size": 25, "title": "Josef Sivic"}, {"color": "#66CCFF", "id": "Chari_On_Pairwise_Costs_2015_CVPR_supplemental", "label": "Chari_On_Pairwise_Costs_2015_CVPR_supplemental", "shape": "dot", "size": 25, "title": "Chari_On_Pairwise_Costs_2015_CVPR_supplemental"}, {"color": "#66CCFF", "id": "Multi-object Tracking", "label": "Multi-object Tracking", "shape": "dot", "size": 25, "title": "Multi-object Tracking"}, {"color": "#66CCFF", "id": "Network Flow Optimization", "label": "Network Flow Optimization", "shape": "dot", "size": 25, "title": "Network Flow Optimization"}, {"color": "#66CCFF", "id": "dependencies among tracks", "label": "dependencies among tracks", "shape": "dot", "size": 25, "title": "dependencies among tracks"}, {"color": "#66CCFF", "id": "Pairwise Costs", "label": "Pairwise Costs", "shape": "dot", "size": 25, "title": "Pairwise Costs"}, {"color": "#66CCFF", "id": "object detector failures", "label": "object detector failures", "shape": "dot", "size": 25, "title": "object detector failures"}, {"color": "#66CCFF", "id": "min-cost network flow framework", "label": "min-cost network flow framework", "shape": "dot", "size": 25, "title": "min-cost network flow framework"}, {"color": "#66CCFF", "id": "Convex Relaxation", "label": "Convex Relaxation", "shape": "dot", "size": 25, "title": "Convex Relaxation"}, {"color": "#66CCFF", "id": "efficient rounding heuristic", "label": "efficient rounding heuristic", "shape": "dot", "size": 25, "title": "efficient rounding heuristic"}, {"color": "#66CCFF", "id": "pairwise costs", "label": "pairwise costs", "shape": "dot", "size": 25, "title": "pairwise costs"}, {"color": "#66CCFF", "id": "real-world video sequences", "label": "real-world video sequences", "shape": "dot", "size": 25, "title": "real-world video sequences"}, {"color": "#66CCFF", "id": "recent tracking methods", "label": "recent tracking methods", "shape": "dot", "size": 25, "title": "recent tracking methods"}, {"color": "#66CCFF", "id": "INRIA", "label": "INRIA", "shape": "dot", "size": 25, "title": "INRIA"}, {"color": "#66CCFF", "id": "Ecole Normale Sup\u00b4erieure", "label": "Ecole Normale Sup\u00b4erieure", "shape": "dot", "size": 25, "title": "Ecole Normale Sup\u00b4erieure"}, {"color": "#66CCFF", "id": "Tracking-by-Detection", "label": "Tracking-by-Detection", "shape": "dot", "size": 25, "title": "Tracking-by-Detection"}, {"color": "#66CCFF", "id": "TILDE", "label": "TILDE", "shape": "dot", "size": 25, "title": "TILDE"}, {"color": "#66CCFF", "id": "Temporally Invariant Learned Detector", "label": "Temporally Invariant Learned Detector", "shape": "dot", "size": 25, "title": "Temporally Invariant Learned Detector"}, {"color": "#66CCFF", "id": "Yannick Verdie", "label": "Yannick Verdie", "shape": "dot", "size": 25, "title": "Yannick Verdie"}, {"color": "#66CCFF", "id": "Kwang Moo Yi", "label": "Kwang Moo Yi", "shape": "dot", "size": 25, "title": "Kwang Moo Yi"}, {"color": "#66CCFF", "id": "Pascal Fua", "label": "Pascal Fua", "shape": "dot", "size": 25, "title": "Pascal Fua"}, {"color": "#66CCFF", "id": "Vincent Lepetit", "label": "Vincent Lepetit", "shape": "dot", "size": 25, "title": "Vincent Lepetit"}, {"color": "#66CCFF", "id": "Simon Lacoste-Julien", "label": "Simon Lacoste-Julien", "shape": "dot", "size": 25, "title": "Simon Lacoste-Julien"}, {"color": "#66CCFF", "id": "INRIO", "label": "INRIO", "shape": "dot", "size": 25, "title": "INRIO"}, {"color": "#66CCFF", "id": "Cortes, C.", "label": "Cortes, C.", "shape": "dot", "size": 25, "title": "Cortes, C."}, {"color": "#66CCFF", "id": "Support-Vector Networks", "label": "Support-Vector Networks", "shape": "dot", "size": 25, "title": "Support-Vector Networks"}, {"color": "#66CCFF", "id": "Vapnik, V.", "label": "Vapnik, V.", "shape": "dot", "size": 25, "title": "Vapnik, V."}, {"color": "#66CCFF", "id": "Harris, C.", "label": "Harris, C.", "shape": "dot", "size": 25, "title": "Harris, C."}, {"color": "#66CCFF", "id": "Combined Corner and Edge Detector", "label": "Combined Corner and Edge Detector", "shape": "dot", "size": 25, "title": "Combined Corner and Edge Detector"}, {"color": "#66CCFF", "id": "Stephens, M.", "label": "Stephens, M.", "shape": "dot", "size": 25, "title": "Stephens, M."}, {"color": "#66CCFF", "id": "C.", "label": "C.", "shape": "dot", "size": 25, "title": "C."}, {"color": "#66CCFF", "id": "Corner Detector", "label": "Corner Detector", "shape": "dot", "size": 25, "title": "Corner Detector"}, {"color": "#66CCFF", "id": "Bay, H.", "label": "Bay, H.", "shape": "dot", "size": 25, "title": "Bay, H."}, {"color": "#66CCFF", "id": "Hinging Hyperplanes", "label": "Hinging Hyperplanes", "shape": "dot", "size": 25, "title": "Hinging Hyperplanes"}, {"color": "#66CCFF", "id": "IEEE Transactions on Information Theory", "label": "IEEE Transactions on Information Theory", "shape": "dot", "size": 25, "title": "IEEE Transactions on Information Theory"}, {"color": "#66CCFF", "id": "Gradient-Based Learning", "label": "Gradient-Based Learning", "shape": "dot", "size": 25, "title": "Gradient-Based Learning"}, {"color": "#66CCFF", "id": "Document Recognition", "label": "Document Recognition", "shape": "dot", "size": 25, "title": "Document Recognition"}, {"color": "#66CCFF", "id": "Af\ufb01ne Region Detectors", "label": "Af\ufb01ne Region Detectors", "shape": "dot", "size": 25, "title": "Af\ufb01ne Region Detectors"}, {"color": "#66CCFF", "id": "Mikolajczyk, K.", "label": "Mikolajczyk, K.", "shape": "dot", "size": 25, "title": "Mikolajczyk, K."}, {"color": "#66CCFF", "id": "A Comparison of Af\ufb01ne Region Detectors", "label": "A Comparison of Af\ufb01ne Region Detectors", "shape": "dot", "size": 25, "title": "A Comparison of Af\ufb01ne Region Detectors"}, {"color": "#66CCFF", "id": "rs", "label": "rs", "shape": "dot", "size": 25, "title": "rs"}, {"color": "#66CCFF", "id": "Zisserma", "label": "Zisserma", "shape": "dot", "size": 25, "title": "Zisserma"}, {"color": "#66CCFF", "id": "Mata", "label": "Mata", "shape": "dot", "size": 25, "title": "Mata"}, {"color": "#66CCFF", "id": "Schaffalitzky", "label": "Schaffalitzky", "shape": "dot", "size": 25, "title": "Schaffalitzky"}, {"color": "#66CCFF", "id": "Kadir", "label": "Kadir", "shape": "dot", "size": 25, "title": "Kadir"}, {"color": "#66CCFF", "id": "Van Gool", "label": "Van Gool", "shape": "dot", "size": 25, "title": "Van Gool"}, {"color": "#66CCFF", "id": "A Review of Af\ufb01ne Region Detectors", "label": "A Review of Af\ufb01ne Region Detectors", "shape": "dot", "size": 25, "title": "A Review of Af\ufb01ne Region Detectors"}, {"color": "#66CCFF", "id": "Dollar", "label": "Dollar", "shape": "dot", "size": 25, "title": "Dollar"}, {"color": "#66CCFF", "id": "Supervised Learning of Edges and Object Boundaries", "label": "Supervised Learning of Edges and Object Boundaries", "shape": "dot", "size": 25, "title": "Supervised Learning of Edges and Object Boundaries"}, {"color": "#66CCFF", "id": "Tu", "label": "Tu", "shape": "dot", "size": 25, "title": "Tu"}, {"color": "#66CCFF", "id": "Belongie", "label": "Belongie", "shape": "dot", "size": 25, "title": "Belongie"}, {"color": "#66CCFF", "id": "Rosten", "label": "Rosten", "shape": "dot", "size": 25, "title": "Rosten"}, {"color": "#66CCFF", "id": "Machine Learning for High-Speed Corner Detection", "label": "Machine Learning for High-Speed Corner Detection", "shape": "dot", "size": 25, "title": "Machine Learning for High-Speed Corner Detection"}, {"color": "#66CCFF", "id": "Drummond", "label": "Drummond", "shape": "dot", "size": 25, "title": "Drummond"}, {"color": "#66CCFF", "id": "Lowe", "label": "Lowe", "shape": "dot", "size": 25, "title": "Lowe"}, {"color": "#66CCFF", "id": "Distinctive Image Features from Scale-Invariant Keypoints", "label": "Distinctive Image Features from Scale-Invariant Keypoints", "shape": "dot", "size": 25, "title": "Distinctive Image Features from Scale-Invariant Keypoints"}, {"color": "#66CCFF", "id": "Fan", "label": "Fan", "shape": "dot", "size": 25, "title": "Fan"}, {"color": "#66CCFF", "id": "LIBLINEAR", "label": "LIBLINEAR", "shape": "dot", "size": 25, "title": "LIBLINEAR"}, {"color": "#66CCFF", "id": "Chang", "label": "Chang", "shape": "dot", "size": 25, "title": "Chang"}, {"color": "#66CCFF", "id": "Hsieh", "label": "Hsieh", "shape": "dot", "size": 25, "title": "Hsieh"}, {"color": "#66CCFF", "id": "Lin", "label": "Lin", "shape": "dot", "size": 25, "title": "Lin"}, {"color": "#66CCFF", "id": "Verdie", "label": "Verdie", "shape": "dot", "size": 25, "title": "Verdie"}, {"color": "#66CCFF", "id": "EPFL", "label": "EPFL", "shape": "dot", "size": 25, "title": "EPFL"}, {"color": "#66CCFF", "id": "Yi", "label": "Yi", "shape": "dot", "size": 25, "title": "Yi"}, {"color": "#66CCFF", "id": "Computer Vision Laboratory, EPFL", "label": "Computer Vision Laboratory, EPFL", "shape": "dot", "size": 25, "title": "Computer Vision Laboratory, EPFL"}, {"color": "#66CCFF", "id": "Vincent Le Petit", "label": "Vincent Le Petit", "shape": "dot", "size": 25, "title": "Vincent Le Petit"}, {"color": "#66CCFF", "id": "JOTS", "label": "JOTS", "shape": "dot", "size": 25, "title": "JOTS"}, {"color": "#66CCFF", "id": "Joint Online Tracking and Segmentation", "label": "Joint Online Tracking and Segmentation", "shape": "dot", "size": 25, "title": "Joint Online Tracking and Segmentation"}, {"color": "#66CCFF", "id": "Longyin Wen", "label": "Longyin Wen", "shape": "dot", "size": 25, "title": "Longyin Wen"}, {"color": "#66CCFF", "id": "Dawei Du", "label": "Dawei Du", "shape": "dot", "size": 25, "title": "Dawei Du"}, {"color": "#66CCFF", "id": "JETS", "label": "JETS", "shape": "dot", "size": 25, "title": "JETS"}, {"color": "#66CCFF", "id": "Zhen Lei", "label": "Zhen Lei", "shape": "dot", "size": 25, "title": "Zhen Lei"}, {"color": "#66CCFF", "id": "Stan Z. Li", "label": "Stan Z. Li", "shape": "dot", "size": 25, "title": "Stan Z. Li"}, {"color": "#66CCFF", "id": "Wen_JOTS_Joint_Online_2015_CVPR_paper.pdf", "label": "Wen_JOTS_Joint_Online_2015_CVPR_paper.pdf", "shape": "dot", "size": 25, "title": "Wen_JOTS_Joint_Online_2015_CVPR_paper.pdf"}, {"color": "#66CCFF", "id": "Video Segmentation task", "label": "Video Segmentation task", "shape": "dot", "size": 25, "title": "Video Segmentation task"}, {"color": "#66CCFF", "id": "Multi-part tracking", "label": "Multi-part tracking", "shape": "dot", "size": 25, "title": "Multi-part tracking"}, {"color": "#66CCFF", "id": "Energy Function Optimization", "label": "Energy Function Optimization", "shape": "dot", "size": 25, "title": "Energy Function Optimization"}, {"color": "#66CCFF", "id": "Tracking and Segmentation stages", "label": "Tracking and Segmentation stages", "shape": "dot", "size": 25, "title": "Tracking and Segmentation stages"}, {"color": "#66CCFF", "id": "RANSA-style approach", "label": "RANSA-style approach", "shape": "dot", "size": 25, "title": "RANSA-style approach"}, {"color": "#66CCFF", "id": "SegTrack database", "label": "SegTrack database", "shape": "dot", "size": 25, "title": "SegTrack database"}, {"color": "#66CCFF", "id": "SegTrack v2 database", "label": "SegTrack v2 database", "shape": "dot", "size": 25, "title": "SegTrack v2 database"}, {"color": "#66CCFF", "id": "Multi-part Models", "label": "Multi-part Models", "shape": "dot", "size": 25, "title": "Multi-part Models"}, {"color": "#66CCFF", "id": "Tracking and Segmentation", "label": "Tracking and Segmentation", "shape": "dot", "size": 25, "title": "Tracking and Segmentation"}, {"color": "#66CCFF", "id": "video analysis", "label": "video analysis", "shape": "dot", "size": 25, "title": "video analysis"}, {"color": "#66CCFF", "id": "multi-target tracking", "label": "multi-target tracking", "shape": "dot", "size": 25, "title": "multi-target tracking"}, {"color": "#66CCFF", "id": "topological constraints", "label": "topological constraints", "shape": "dot", "size": 25, "title": "topological constraints"}, {"color": "#66CCFF", "id": "deformable objects", "label": "deformable objects", "shape": "dot", "size": 25, "title": "deformable objects"}, {"color": "#66CCFF", "id": "occluded objects", "label": "occluded objects", "shape": "dot", "size": 25, "title": "occluded objects"}, {"color": "#66CCFF", "id": "dynamic graph", "label": "dynamic graph", "shape": "dot", "size": 25, "title": "dynamic graph"}, {"color": "#66CCFF", "id": "Vasconcelos", "label": "Vasconcelos", "shape": "dot", "size": 25, "title": "Vasconcelos"}, {"color": "#66CCFF", "id": "NLPR, Institute of Automation, Chinese Academy of Sciences", "label": "NLPR, Institute of Automation, Chinese Academy of Sciences", "shape": "dot", "size": 25, "title": "NLPR, Institute of Automation, Chinese Academy of Sciences"}, {"color": "#66CCFF", "id": "S. Z. Li", "label": "S. Z. Li", "shape": "dot", "size": 25, "title": "S. Z. Li"}, {"color": "#66CCFF", "id": "SCCE, University of Chinese Academy of Sciences", "label": "SCCE, University of Chinese Academy of Sciences", "shape": "dot", "size": 25, "title": "SCCE, University of Chinese Academy of Sciences"}, {"color": "#66CCFF", "id": "tracking deformable and occluded objects", "label": "tracking deformable and occluded objects", "shape": "dot", "size": 25, "title": "tracking deformable and occluded objects"}, {"color": "#66CCFF", "id": "Delong", "label": "Delong", "shape": "dot", "size": 25, "title": "Delong"}, {"color": "#66CCFF", "id": "optimization method", "label": "optimization method", "shape": "dot", "size": 25, "title": "optimization method"}, {"color": "#66CCFF", "id": "NLPR", "label": "NLPR", "shape": "dot", "size": 25, "title": "NLPR"}, {"color": "#66CCFF", "id": "Philipp Kr\u00e4henbuhl", "label": "Philipp Kr\u00e4henbuhl", "shape": "dot", "size": 25, "title": "Philipp Kr\u00e4henbuhl"}, {"color": "#66CCFF", "id": "Learning to Propose Objects", "label": "Learning to Propose Objects", "shape": "dot", "size": 25, "title": "Learning to Propose Objects"}, {"color": "#66CCFF", "id": "Vladlen Koltun", "label": "Vladlen Koltun", "shape": "dot", "size": 25, "title": "Vladlen Koltun"}, {"color": "#66CCFF", "id": "Krahenbuhl_Learning_to_Propos_2015_CVPR_paper.pdf", "label": "Krahenbuhl_Learning_to_Propos_2015_CVPR_paper.pdf", "shape": "dot", "size": 25, "title": "Krahenbuhl_Learning_to_Propos_2015_CVPR_paper.pdf"}, {"color": "#66CCFF", "id": "zlei", "label": "zlei", "shape": "dot", "size": 25, "title": "zlei"}, {"color": "#66CCFF", "id": "highly accurate bottom-up object segmentation", "label": "highly accurate bottom-up object segmentation", "shape": "dot", "size": 25, "title": "highly accurate bottom-up object segmentation"}, {"color": "#66CCFF", "id": "set of regions", "label": "set of regions", "shape": "dot", "size": 25, "title": "set of regions"}, {"color": "#66CCFF", "id": "candidate objects", "label": "candidate objects", "shape": "dot", "size": 25, "title": "candidate objects"}, {"color": "#66CCFF", "id": "ensemble of figure-ground segmentation models", "label": "ensemble of figure-ground segmentation models", "shape": "dot", "size": 25, "title": "ensemble of figure-ground segmentation models"}, {"color": "#66CCFF", "id": "ensemble", "label": "ensemble", "shape": "dot", "size": 25, "title": "ensemble"}, {"color": "#66CCFF", "id": "jointly", "label": "jointly", "shape": "dot", "size": 25, "title": "jointly"}, {"color": "#66CCFF", "id": "ensemble training", "label": "ensemble training", "shape": "dot", "size": 25, "title": "ensemble training"}, {"color": "#66CCFF", "id": "sequence of uncapacitated facility location problems", "label": "sequence of uncapacitated facility location problems", "shape": "dot", "size": 25, "title": "sequence of uncapacitated facility location problems"}, {"color": "#66CCFF", "id": "procedure", "label": "procedure", "shape": "dot", "size": 25, "title": "procedure"}, {"color": "#66CCFF", "id": "size of the ensemble", "label": "size of the ensemble", "shape": "dot", "size": 25, "title": "size of the ensemble"}, {"color": "#66CCFF", "id": "composition", "label": "composition", "shape": "dot", "size": 25, "title": "composition"}, {"color": "#66CCFF", "id": "ensembles", "label": "ensembles", "shape": "dot", "size": 25, "title": "ensembles"}, {"color": "#66CCFF", "id": "elementary image features", "label": "elementary image features", "shape": "dot", "size": 25, "title": "elementary image features"}, {"color": "#66CCFF", "id": "rapid image analysis", "label": "rapid image analysis", "shape": "dot", "size": 25, "title": "rapid image analysis"}, {"color": "#66CCFF", "id": "presented approach", "label": "presented approach", "shape": "dot", "size": 25, "title": "presented approach"}, {"color": "#66CCFF", "id": "prior object proposal algorithms", "label": "prior object proposal algorithms", "shape": "dot", "size": 25, "title": "prior object proposal algorithms"}, {"color": "#66CCFF", "id": "lowest running time", "label": "lowest running time", "shape": "dot", "size": 25, "title": "lowest running time"}, {"color": "#66CCFF", "id": "trained ensembles", "label": "trained ensembles", "shape": "dot", "size": 25, "title": "trained ensembles"}, {"color": "#66CCFF", "id": "bottom-up segmentation model", "label": "bottom-up segmentation model", "shape": "dot", "size": 25, "title": "bottom-up segmentation model"}, {"color": "#66CCFF", "id": "bottom-up segmentation", "label": "bottom-up segmentation", "shape": "dot", "size": 25, "title": "bottom-up segmentation"}, {"color": "#66CCFF", "id": "parameters", "label": "parameters", "shape": "dot", "size": 25, "title": "parameters"}, {"color": "#66CCFF", "id": "generally applicable model", "label": "generally applicable model", "shape": "dot", "size": 25, "title": "generally applicable model"}, {"color": "#66CCFF", "id": "Ensemble Methods", "label": "Ensemble Methods", "shape": "dot", "size": 25, "title": "Ensemble Methods"}, {"color": "#66CCFF", "id": "running time", "label": "running time", "shape": "dot", "size": 25, "title": "running time"}, {"color": "#66CCFF", "id": "Arbel\u00e1ez et al. (2012)", "label": "Arbel\u00e1ez et al. (2012)", "shape": "dot", "size": 25, "title": "Arbel\u00e1ez et al. (2012)"}, {"color": "#66CCFF", "id": "Semantic segmentation using regions and parts", "label": "Semantic segmentation using regions and parts", "shape": "dot", "size": 25, "title": "Semantic segmentation using regions and parts"}, {"color": "#66CCFF", "id": "Microsoft COCO: Common objects in context", "label": "Microsoft COCO: Common objects in context", "shape": "dot", "size": 25, "title": "Microsoft COCO: Common objects in context"}, {"color": "#66CCFF", "id": "Carreira et al.", "label": "Carreira et al.", "shape": "dot", "size": 25, "title": "Carreira et al."}, {"color": "#66CCFF", "id": "Free-form region description with second-order pooling", "label": "Free-form region description with second-order pooling", "shape": "dot", "size": 25, "title": "Free-form region description with second-order pooling"}, {"color": "#66CCFF", "id": "Microsoft COCO", "label": "Microsoft COCO", "shape": "dot", "size": 25, "title": "Microsoft COCO"}, {"color": "#66CCFF", "id": "common objects", "label": "common objects", "shape": "dot", "size": 25, "title": "common objects"}, {"color": "#66CCFF", "id": "Objectness", "label": "Objectness", "shape": "dot", "size": 25, "title": "Objectness"}, {"color": "#66CCFF", "id": "Structured forests", "label": "Structured forests", "shape": "dot", "size": 25, "title": "Structured forests"}, {"color": "#66CCFF", "id": "BING", "label": "BING", "shape": "dot", "size": 25, "title": "BING"}, {"color": "#66CCFF", "id": "Geometry of cuts and metrics", "label": "Geometry of cuts and metrics", "shape": "dot", "size": 25, "title": "Geometry of cuts and metrics"}, {"color": "#66CCFF", "id": "Aravindh Mahendran", "label": "Aravindh Mahendran", "shape": "dot", "size": 25, "title": "Aravindh Mahendran"}, {"color": "#66CCFF", "id": "Understanding Deep Image Representations", "label": "Understanding Deep Image Representations", "shape": "dot", "size": 25, "title": "Understanding Deep Image Representations"}, {"color": "#66CCFF", "id": "Andrea Vedaldi", "label": "Andrea Vedaldi", "shape": "dot", "size": 25, "title": "Andrea Vedaldi"}, {"color": "#66CCFF", "id": "graph cuts", "label": "graph cuts", "shape": "dot", "size": 25, "title": "graph cuts"}, {"color": "#66CCFF", "id": "Understanding Deep Image Representations by Inverting Them", "label": "Understanding Deep Image Representations by Inverting Them", "shape": "dot", "size": 25, "title": "Understanding Deep Image Representations by Inverting Them"}, {"color": "#66CCFF", "id": "Understanding Deep Image Presentations by Inverting Them", "label": "Understanding Deep Image Presentations by Inverting Them", "shape": "dot", "size": 25, "title": "Understanding Deep Image Presentations by Inverting Them"}, {"color": "#66CCFF", "id": "Mahendran_Understanding_Deep_Image_2015_CVPR_paper.pdf", "label": "Mahendran_Understanding_Deep_Image_2015_CVPR_paper.pdf", "shape": "dot", "size": 25, "title": "Mahendran_Understanding_Deep_Image_2015_CVPR_paper.pdf"}, {"color": "#66CCFF", "id": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle", "label": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle", "shape": "dot", "size": 25, "title": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle"}, {"color": "#66CCFF", "id": "Intel Labs", "label": "Intel Labs", "shape": "dot", "size": 25, "title": "Intel Labs"}, {"color": "#66CCFF", "id": "Deep Image Representations", "label": "Deep Image Representations", "shape": "dot", "size": 25, "title": "Deep Image Representations"}, {"color": "#66CCFF", "id": "Image Representations", "label": "Image Representations", "shape": "dot", "size": 25, "title": "Image Representations"}, {"color": "#66CCFF", "id": "geometric and photometric invariance", "label": "geometric and photometric invariance", "shape": "dot", "size": 25, "title": "geometric and photometric invariance"}, {"color": "#66CCFF", "id": "image representation", "label": "image representation", "shape": "dot", "size": 25, "title": "image representation"}, {"color": "#66CCFF", "id": "image processing tasks", "label": "image processing tasks", "shape": "dot", "size": 25, "title": "image processing tasks"}, {"color": "#66CCFF", "id": "Bishop", "label": "Bishop", "shape": "dot", "size": 25, "title": "Bishop"}, {"color": "#66CCFF", "id": "Neural Networks for Pattern Recognition", "label": "Neural Networks for Pattern Recognition", "shape": "dot", "size": 25, "title": "Neural Networks for Pattern Recognition"}, {"color": "#66CCFF", "id": "key component", "label": "key component", "shape": "dot", "size": 25, "title": "key component"}, {"color": "#66CCFF", "id": "deformable part models", "label": "deformable part models", "shape": "dot", "size": 25, "title": "deformable part models"}, {"color": "#66CCFF", "id": "3D textons", "label": "3D textons", "shape": "dot", "size": 25, "title": "3D textons"}, {"color": "#66CCFF", "id": "SIFT detector", "label": "SIFT detector", "shape": "dot", "size": 25, "title": "SIFT detector"}, {"color": "#66CCFF", "id": "open-source implementation", "label": "open-source implementation", "shape": "dot", "size": 25, "title": "open-source implementation"}, {"color": "#66CCFF", "id": "distinctive image features", "label": "distinctive image features", "shape": "dot", "size": 25, "title": "distinctive image features"}, {"color": "#66CCFF", "id": "Zeiler \u0026 Fergus", "label": "Zeiler \u0026 Fergus", "shape": "dot", "size": 25, "title": "Zeiler \u0026 Fergus"}, {"color": "#66CCFF", "id": "visualizing convolutional networks", "label": "visualizing convolutional networks", "shape": "dot", "size": 25, "title": "visualizing convolutional networks"}, {"color": "#66CCFF", "id": "Hinton \u0026 Salakhutdinov", "label": "Hinton \u0026 Salakhutdinov", "shape": "dot", "size": 25, "title": "Hinton \u0026 Salakhutdinov"}, {"color": "#66CCFF", "id": "Wang et al.", "label": "Wang et al.", "shape": "dot", "size": 25, "title": "Wang et al."}, {"color": "#66CCFF", "id": "locality-constrained linear coding", "label": "locality-constrained linear coding", "shape": "dot", "size": 25, "title": "locality-constrained linear coding"}, {"color": "#66CCFF", "id": "Arvindh Mahendran", "label": "Arvindh Mahendran", "shape": "dot", "size": 25, "title": "Arvindh Mahendran"}, {"color": "#66CCFF", "id": "University of Oxford", "label": "University of Oxford", "shape": "dot", "size": 25, "title": "University of Oxford"}, {"color": "#66CCFF", "id": "Yan Xia", "label": "Yan Xia", "shape": "dot", "size": 25, "title": "Yan Xia"}, {"color": "#66CCFF", "id": "Sparse Projections for High-Dimensional Binary Codes", "label": "Sparse Projections for High-Dimensional Binary Codes", "shape": "dot", "size": 25, "title": "Sparse Projections for High-Dimensional Binary Codes"}, {"color": "#66CCFF", "id": "Pushmeet Kohli", "label": "Pushmeet Kohli", "shape": "dot", "size": 25, "title": "Pushmeet Kohli"}, {"color": "#66CCFF", "id": "Sparse Projections", "label": "Sparse Projections", "shape": "dot", "size": 25, "title": "Sparse Projections"}, {"color": "#66CCFF", "id": "problem of learning long binary codes", "label": "problem of learning long binary codes", "shape": "dot", "size": 25, "title": "problem of learning long binary codes"}, {"color": "#66CCFF", "id": "lack of effective regularizer", "label": "lack of effective regularizer", "shape": "dot", "size": 25, "title": "lack of effective regularizer"}, {"color": "#66CCFF", "id": "high computational cost", "label": "high computational cost", "shape": "dot", "size": 25, "title": "high computational cost"}, {"color": "#66CCFF", "id": "sparsity encouraging regularizer", "label": "sparsity encouraging regularizer", "shape": "dot", "size": 25, "title": "sparsity encouraging regularizer"}, {"color": "#66CCFF", "id": "number of parameters", "label": "number of parameters", "shape": "dot", "size": 25, "title": "number of parameters"}, {"color": "#66CCFF", "id": "overfitting", "label": "overfitting", "shape": "dot", "size": 25, "title": "overfitting"}, {"color": "#66CCFF", "id": "sparse nature", "label": "sparse nature", "shape": "dot", "size": 25, "title": "sparse nature"}, {"color": "#66CCFF", "id": "sparse projection matrix", "label": "sparse projection matrix", "shape": "dot", "size": 25, "title": "sparse projection matrix"}, {"color": "#66CCFF", "id": "reduction in computational cost", "label": "reduction in computational cost", "shape": "dot", "size": 25, "title": "reduction in computational cost"}, {"color": "#66CCFF", "id": "better accuracy", "label": "better accuracy", "shape": "dot", "size": 25, "title": "better accuracy"}, {"color": "#66CCFF", "id": "dense projections", "label": "dense projections", "shape": "dot", "size": 25, "title": "dense projections"}, {"color": "#66CCFF", "id": "rix", "label": "rix", "shape": "dot", "size": 25, "title": "rix"}, {"color": "#66CCFF", "id": "ITQ", "label": "ITQ", "shape": "dot", "size": 25, "title": "ITQ"}, {"color": "#66CCFF", "id": "other methods", "label": "other methods", "shape": "dot", "size": 25, "title": "other methods"}, {"color": "#66CCFF", "id": "high-dimensional binary encoding", "label": "high-dimensional binary encoding", "shape": "dot", "size": 25, "title": "high-dimensional binary encoding"}, {"color": "#66CCFF", "id": "Other methods", "label": "Other methods", "shape": "dot", "size": 25, "title": "Other methods"}, {"color": "#66CCFF", "id": "High-Dimensional Binary Encoding", "label": "High-Dimensional Binary Encoding", "shape": "dot", "size": 25, "title": "High-Dimensional Binary Encoding"}, {"color": "#66CCFF", "id": "Agrawal et al. (2014)", "label": "Agrawal et al. (2014)", "shape": "dot", "size": 25, "title": "Agrawal et al. (2014)"}, {"color": "#66CCFF", "id": "Foundational context", "label": "Foundational context", "shape": "dot", "size": 25, "title": "Foundational context"}, {"color": "#66CCFF", "id": "Object Recognition", "label": "Object Recognition", "shape": "dot", "size": 25, "title": "Object Recognition"}, {"color": "#66CCFF", "id": "Fan et al. (2008)", "label": "Fan et al. (2008)", "shape": "dot", "size": 25, "title": "Fan et al. (2008)"}, {"color": "#66CCFF", "id": "Liblinear", "label": "Liblinear", "shape": "dot", "size": 25, "title": "Liblinear"}, {"color": "#66CCFF", "id": "binary code learning", "label": "binary code learning", "shape": "dot", "size": 25, "title": "binary code learning"}, {"color": "#66CCFF", "id": "efficient similarity search", "label": "efficient similarity search", "shape": "dot", "size": 25, "title": "efficient similarity search"}, {"color": "#66CCFF", "id": "approximate nearest neighbor search", "label": "approximate nearest neighbor search", "shape": "dot", "size": 25, "title": "approximate nearest neighbor search"}, {"color": "#66CCFF", "id": "many applications", "label": "many applications", "shape": "dot", "size": 25, "title": "many applications"}, {"color": "#66CCFF", "id": "sparse approximation techniques", "label": "sparse approximation techniques", "shape": "dot", "size": 25, "title": "sparse approximation techniques"}, {"color": "#66CCFF", "id": "product quantization", "label": "product quantization", "shape": "dot", "size": 25, "title": "product quantization"}, {"color": "#66CCFF", "id": "atomic decomposition", "label": "atomic decomposition", "shape": "dot", "size": 25, "title": "atomic decomposition"}, {"color": "#66CCFF", "id": "basis pursuit", "label": "basis pursuit", "shape": "dot", "size": 25, "title": "basis pursuit"}, {"color": "#66CCFF", "id": "hashing algorithms", "label": "hashing algorithms", "shape": "dot", "size": 25, "title": "hashing algorithms"}, {"color": "#66CCFF", "id": "ear-optimal hashing algorithms", "label": "ear-optimal hashing algorithms", "shape": "dot", "size": 25, "title": "ear-optimal hashing algorithms"}, {"color": "#66CCFF", "id": "locality-sensitive hashing", "label": "locality-sensitive hashing", "shape": "dot", "size": 25, "title": "locality-sensitive hashing"}, {"color": "#66CCFF", "id": "Procrustes analysis", "label": "Procrustes analysis", "shape": "dot", "size": 25, "title": "Procrustes analysis"}, {"color": "#66CCFF", "id": "finding optimal transformation", "label": "finding optimal transformation", "shape": "dot", "size": 25, "title": "finding optimal transformation"}, {"color": "#66CCFF", "id": "FOCS", "label": "FOCS", "shape": "dot", "size": 25, "title": "FOCS"}, {"color": "#66CCFF", "id": "Symposium on Computational Geometry", "label": "Symposium on Computational Geometry", "shape": "dot", "size": 25, "title": "Symposium on Computational Geometry"}, {"color": "#66CCFF", "id": "Procrustes problems", "label": "Procrustes problems", "shape": "dot", "size": 25, "title": "Procrustes problems"}, {"color": "#66CCFF", "id": "Oxford University Press", "label": "Oxford University Press", "shape": "dot", "size": 25, "title": "Oxford University Press"}, {"color": "#66CCFF", "id": "University of Science and Technology of China", "label": "University of Science and Technology of China", "shape": "dot", "size": 25, "title": "University of Science and Technology of China"}, {"color": "#66CCFF", "id": "Huazhu Fu", "label": "Huazhu Fu", "shape": "dot", "size": 25, "title": "Huazhu Fu"}, {"color": "#66CCFF", "id": "Dong Xu", "label": "Dong Xu", "shape": "dot", "size": 25, "title": "Dong Xu"}, {"color": "#66CCFF", "id": "Stephen Lin", "label": "Stephen Lin", "shape": "dot", "size": 25, "title": "Stephen Lin"}, {"color": "#66CCFF", "id": "Jiang Liu", "label": "Jiang Liu", "shape": "dot", "size": 25, "title": "Jiang Liu"}, {"color": "#66CCFF", "id": "Object-based RGBD Image Co-segmentation with Mutex Constraint", "label": "Object-based RGBD Image Co-segmentation with Mutex Constraint", "shape": "dot", "size": 25, "title": "Object-based RGBD Image Co-segmentation with Mutex Constraint"}, {"color": "#66CCFF", "id": "Object-based RGBD Image Co-segmentation with Mutux Constraint", "label": "Object-based RGBD Image Co-segmentation with Mutux Constraint", "shape": "dot", "size": 25, "title": "Object-based RGBD Image Co-segmentation with Mutux Constraint"}, {"color": "#66CCFF", "id": "depth channel", "label": "depth channel", "shape": "dot", "size": 25, "title": "depth channel"}, {"color": "#66CCFF", "id": "identification of similar foreground objects", "label": "identification of similar foreground objects", "shape": "dot", "size": 25, "title": "identification of similar foreground objects"}, {"color": "#66CCFF", "id": "detection of object-like regions", "label": "detection of object-like regions", "shape": "dot", "size": 25, "title": "detection of object-like regions"}, {"color": "#66CCFF", "id": "depth-based local features", "label": "depth-based local features", "shape": "dot", "size": 25, "title": "depth-based local features"}, {"color": "#66CCFF", "id": "co-segmentation", "label": "co-segmentation", "shape": "dot", "size": 25, "title": "co-segmentation"}, {"color": "#66CCFF", "id": "fully-connected graph structure", "label": "fully-connected graph structure", "shape": "dot", "size": 25, "title": "fully-connected graph structure"}, {"color": "#66CCFF", "id": "graph structure", "label": "graph structure", "shape": "dot", "size": 25, "title": "graph structure"}, {"color": "#66CCFF", "id": "mutex constraints", "label": "mutex constraints", "shape": "dot", "size": 25, "title": "mutex constraints"}, {"color": "#66CCFF", "id": "improper solutions", "label": "improper solutions", "shape": "dot", "size": 25, "title": "improper solutions"}, {"color": "#66CCFF", "id": "object-based RGBD co-segmentation", "label": "object-based RGBD co-segmentation", "shape": "dot", "size": 25, "title": "object-based RGBD co-segmentation"}, {"color": "#66CCFF", "id": "related methods", "label": "related methods", "shape": "dot", "size": 25, "title": "related methods"}, {"color": "#66CCFF", "id": "RGBD co-segmentation", "label": "RGBD co-segmentation", "shape": "dot", "size": 25, "title": "RGBD co-segmentation"}, {"color": "#66CCFF", "id": "related techniques", "label": "related techniques", "shape": "dot", "size": 25, "title": "related techniques"}, {"color": "#66CCFF", "id": "comparable performance", "label": "comparable performance", "shape": "dot", "size": 25, "title": "comparable performance"}, {"color": "#66CCFF", "id": "RGB co-segmentation techniques", "label": "RGB co-segmentation techniques", "shape": "dot", "size": 25, "title": "RGB co-segmentation techniques"}, {"color": "#66CCFF", "id": "depth maps", "label": "depth maps", "shape": "dot", "size": 25, "title": "depth maps"}, {"color": "#66CCFF", "id": "RGB images", "label": "RGB images", "shape": "dot", "size": 25, "title": "RGB images"}, {"color": "#66CCFF", "id": "segmentation accuracy", "label": "segmentation accuracy", "shape": "dot", "size": 25, "title": "segmentation accuracy"}, {"color": "#66CCFF", "id": "Depth maps", "label": "Depth maps", "shape": "dot", "size": 25, "title": "Depth maps"}, {"color": "#66CCFF", "id": "Object-Based Methods", "label": "Object-Based Methods", "shape": "dot", "size": 25, "title": "Object-Based Methods"}, {"color": "#66CCFF", "id": "Mutual Exclusion Constraints", "label": "Mutual Exclusion Constraints", "shape": "dot", "size": 25, "title": "Mutual Exclusion Constraints"}, {"color": "#66CCFF", "id": "Co-Saliency Maps", "label": "Co-Saliency Maps", "shape": "dot", "size": 25, "title": "Co-Saliency Maps"}, {"color": "#66CCFF", "id": "Graph Formulation", "label": "Graph Formulation", "shape": "dot", "size": 25, "title": "Graph Formulation"}, {"color": "#66CCFF", "id": "Multi-Plane Block-Coordinate Frank-Wilfe Algorithm", "label": "Multi-Plane Block-Coordinate Frank-Wilfe Algorithm", "shape": "dot", "size": 25, "title": "Multi-Plane Block-Coordinate Frank-Wilfe Algorithm"}, {"color": "#66CCFF", "id": "Structural SVMs", "label": "Structural SVMs", "shape": "dot", "size": 25, "title": "Structural SVMs"}, {"color": "#66CCFF", "id": "max-Oracle", "label": "max-Oracle", "shape": "dot", "size": 25, "title": "max-Oracle"}, {"color": "#66CCFF", "id": "Neel Shah", "label": "Neel Shah", "shape": "dot", "size": 25, "title": "Neel Shah"}, {"color": "#66CCFF", "id": "Vladimir Kolmogorov", "label": "Vladimir Kolmogorov", "shape": "dot", "size": 25, "title": "Vladimir Kolmogorov"}, {"color": "#66CCFF", "id": "Chris H. Lampert", "label": "Chris H. Lampert", "shape": "dot", "size": 25, "title": "Chris H. Lampert"}, {"color": "#66CCFF", "id": "Structural Support Vector Machines", "label": "Structural Support Vector Machines", "shape": "dot", "size": 25, "title": "Structural Support Vector Machines"}, {"color": "#66CCFF", "id": "structured computer vision tasks", "label": "structured computer vision tasks", "shape": "dot", "size": 25, "title": "structured computer vision tasks"}, {"color": "#66CCFF", "id": "structured prediction subroutine", "label": "structured prediction subroutine", "shape": "dot", "size": 25, "title": "structured prediction subroutine"}, {"color": "#66CCFF", "id": "Frank-Wolfe algorithm", "label": "Frank-Wolfe algorithm", "shape": "dot", "size": 25, "title": "Frank-Wolfe algorithm"}, {"color": "#66CCFF", "id": "training SSVMs", "label": "training SSVMs", "shape": "dot", "size": 25, "title": "training SSVMs"}, {"color": "#66CCFF", "id": "caching mechanism", "label": "caching mechanism", "shape": "dot", "size": 25, "title": "caching mechanism"}, {"color": "#66CCFF", "id": "geometrically motivated criterion", "label": "geometrically motivated criterion", "shape": "dot", "size": 25, "title": "geometrically motivated criterion"}, {"color": "#66CCFF", "id": "criterion", "label": "criterion", "shape": "dot", "size": 25, "title": "criterion"}, {"color": "#66CCFF", "id": "call max-oracle", "label": "call max-oracle", "shape": "dot", "size": 25, "title": "call max-oracle"}, {"color": "#66CCFF", "id": "faster convergence", "label": "faster convergence", "shape": "dot", "size": 25, "title": "faster convergence"}, {"color": "#66CCFF", "id": "total runtime", "label": "total runtime", "shape": "dot", "size": 25, "title": "total runtime"}, {"color": "#66CCFF", "id": "max-oracle", "label": "max-oracle", "shape": "dot", "size": 25, "title": "max-oracle"}, {"color": "#66CCFF", "id": "bottleneck", "label": "bottleneck", "shape": "dot", "size": 25, "title": "bottleneck"}, {"color": "#66CCFF", "id": "block-coordinate Frank-Wolfe (BCFW) algorithm", "label": "block-coordinate Frank-Wolfe (BCFW) algorithm", "shape": "dot", "size": 25, "title": "block-coordinate Frank-Wolfe (BCFW) algorithm"}, {"color": "#66CCFF", "id": "Max-Oracle", "label": "Max-Oracle", "shape": "dot", "size": 25, "title": "Max-Oracle"}, {"color": "#66CCFF", "id": "Frank-Wolfe Algorithm", "label": "Frank-Wolfe Algorithm", "shape": "dot", "size": 25, "title": "Frank-Wolfe Algorithm"}, {"color": "#66CCFF", "id": "Block-Coordinate Methods", "label": "Block-Coordinate Methods", "shape": "dot", "size": 25, "title": "Block-Coordinate Methods"}, {"color": "#66CCFF", "id": "IST Austria", "label": "IST Austria", "shape": "dot", "size": 25, "title": "IST Austria"}, {"color": "#66CCFF", "id": "IST Australia", "label": "IST Australia", "shape": "dot", "size": 25, "title": "IST Australia"}, {"color": "#66CCFF", "id": "Beier_Fusion_Moves_for_2015_CVPR_supplemental.pdf", "label": "Beier_Fusion_Moves_for_2015_CVPR_supplemental.pdf", "shape": "dot", "size": 25, "title": "Beier_Fusion_Moves_for_2015_CVPR_supplemental.pdf"}, {"color": "#66CCFF", "id": "Beier_Fusion_Moves_for_2015_CVPR_supplemental", "label": "Beier_Fusion_Moves_for_2015_CVPR_supplemental", "shape": "dot", "size": 25, "title": "Beier_Fusion_Moves_for_2015_CVPR_supplemental"}, {"color": "#66CCFF", "id": "PIVOT-BOEM", "label": "PIVOT-BOEM", "shape": "dot", "size": 25, "title": "PIVOT-BOEM"}, {"color": "#66CCFF", "id": "Correlation Clustering", "label": "Correlation Clustering", "shape": "dot", "size": 25, "title": "Correlation Clustering"}, {"color": "#66CCFF", "id": "HC", "label": "HC", "shape": "dot", "size": 25, "title": "HC"}, {"color": "#66CCFF", "id": "CGC", "label": "CGC", "shape": "dot", "size": 25, "title": "CGC"}, {"color": "#66CCFF", "id": "Fusion Moves", "label": "Fusion Moves", "shape": "dot", "size": 25, "title": "Fusion Moves"}, {"color": "#66CCFF", "id": "Experimental Analysis", "label": "Experimental Analysis", "shape": "dot", "size": 25, "title": "Experimental Analysis"}, {"color": "#66CCFF", "id": "Runtime", "label": "Runtime", "shape": "dot", "size": 25, "title": "Runtime"}, {"color": "#66CCFF", "id": "Solution Quality", "label": "Solution Quality", "shape": "dot", "size": 25, "title": "Solution Quality"}, {"color": "#66CCFF", "id": "Anytime Algorithms", "label": "Anytime Algorithms", "shape": "dot", "size": 25, "title": "Anytime Algorithms"}, {"color": "#66CCFF", "id": "Anytime Behavior", "label": "Anytime Behavior", "shape": "dot", "size": 25, "title": "Anytime Behavior"}, {"color": "#66CCFF", "id": "Progressive Improvement", "label": "Progressive Improvement", "shape": "dot", "size": 25, "title": "Progressive Improvement"}, {"color": "#66CCFF", "id": "Dataset Performance", "label": "Dataset Performance", "shape": "dot", "size": 25, "title": "Dataset Performance"}, {"color": "#66CCFF", "id": "Instances", "label": "Instances", "shape": "dot", "size": 25, "title": "Instances"}, {"color": "#66CCFF", "id": "University of Heidelberg (Iwr)", "label": "University of Heidelberg (Iwr)", "shape": "dot", "size": 25, "title": "University of Heidelberg (Iwr)"}, {"color": "#66CCFF", "id": "Mohammadreza Mostajabi", "label": "Mohammadreza Mostajabi", "shape": "dot", "size": 25, "title": "Mohammadreza Mostajabi"}, {"color": "#66CCFF", "id": "Feedforward Semantic Segmentation", "label": "Feedforward Semantic Segmentation", "shape": "dot", "size": 25, "title": "Feedforward Semantic Segmentation"}, {"color": "#66CCFF", "id": "Payman Yadollahpour", "label": "Payman Yadollahpour", "shape": "dot", "size": 25, "title": "Payman Yadollahpour"}, {"color": "#66CCFF", "id": "Gregory Shakhnarovich", "label": "Gregory Shakhnarovich", "shape": "dot", "size": 25, "title": "Gregory Shakhnarovich"}, {"color": "#66CCFF", "id": "University of Heidelberg (Department of Mathematics)", "label": "University of Heidelberg (Department of Mathematics)", "shape": "dot", "size": 25, "title": "University of Heidelberg (Department of Mathematics)"}, {"color": "#66CCFF", "id": "Mostajabi_Feedforward_Semantic_Segmentation_2015_CVPR_paper", "label": "Mostajabi_Feedforward_Semantic_Segmentation_2015_CVPR_paper", "shape": "dot", "size": 25, "title": "Mostajabi_Feedforward_Semantic_Segmentation_2015_CVPR_paper"}, {"color": "#66CCFF", "id": "Mostajabi_Feedforwad_Semantic_Segmentation_2015_CVPR_paper", "label": "Mostajabi_Feedforwad_Semantic_Segmentation_2015_CVPR_paper", "shape": "dot", "size": 25, "title": "Mostajabi_Feedforwad_Semantic_Segmentation_2015_CVPR_paper"}, {"color": "#66CCFF", "id": "feed-forward architecture", "label": "feed-forward architecture", "shape": "dot", "size": 25, "title": "feed-forward architecture"}, {"color": "#66CCFF", "id": "image elements", "label": "image elements", "shape": "dot", "size": 25, "title": "image elements"}, {"color": "#66CCFF", "id": "rich feature representations", "label": "rich feature representations", "shape": "dot", "size": 25, "title": "rich feature representations"}, {"color": "#66CCFF", "id": "nested regions", "label": "nested regions", "shape": "dot", "size": 25, "title": "nested regions"}, {"color": "#66CCFF", "id": "zoom-out", "label": "zoom-out", "shape": "dot", "size": 25, "title": "zoom-out"}, {"color": "#66CCFF", "id": "superpixel", "label": "superpixel", "shape": "dot", "size": 25, "title": "superpixel"}, {"color": "#66CCFF", "id": "statistical structure", "label": "statistical structure", "shape": "dot", "size": 25, "title": "statistical structure"}, {"color": "#66CCFF", "id": "feedforward multilayer network", "label": "feedforward multilayer network", "shape": "dot", "size": 25, "title": "feedforward multilayer network"}, {"color": "#66CCFF", "id": "69.6% average accuracy", "label": "69.6% average accuracy", "shape": "dot", "size": 25, "title": "69.6% average accuracy"}, {"color": "#66CCFF", "id": "PAS-CAL VOC 2012 test set", "label": "PAS-CAL VOC 2012 test set", "shape": "dot", "size": 25, "title": "PAS-CAL VOC 2012 test set"}, {"color": "#66CCFF", "id": "Regions", "label": "Regions", "shape": "dot", "size": 25, "title": "Regions"}, {"color": "#66CCFF", "id": "Parts", "label": "Parts", "shape": "dot", "size": 25, "title": "Parts"}, {"color": "#66CCFF", "id": "Hypercolumns for object segmentation and fine-grained localization", "label": "Hypercolumns for object segmentation and fine-grained localization", "shape": "dot", "size": 25, "title": "Hypercolumns for object segmentation and fine-grained localization"}, {"color": "#66CCFF", "id": "Chen et al.", "label": "Chen et al.", "shape": "dot", "size": 25, "title": "Chen et al."}, {"color": "#66CCFF", "id": "Semantic image segmentation with deep convolutional nets and fully connected crfs", "label": "Semantic image segmentation with deep convolutional nets and fully connected crfs", "shape": "dot", "size": 25, "title": "Semantic image segmentation with deep convolutional nets and fully connected crfs"}, {"color": "#66CCFF", "id": "Long et al.", "label": "Long et al.", "shape": "dot", "size": 25, "title": "Long et al."}, {"color": "#66CCFF", "id": "Fully convolutional networks for semantic segmentation", "label": "Fully convolutional networks for semantic segmentation", "shape": "dot", "size": 25, "title": "Fully convolutional networks for semantic segmentation"}, {"color": "#66CCFF", "id": "Associative hierarchical CRFs for object class image segmentation", "label": "Associative hierarchical CRFs for object class image segmentation", "shape": "dot", "size": 25, "title": "Associative hierarchical CRFs for object class image segmentation"}, {"color": "#66CCFF", "id": "Carreira and Sminchisescu", "label": "Carreira and Sminchisescu", "shape": "dot", "size": 25, "title": "Carreira and Sminchisescu"}, {"color": "#66CCFF", "id": "CPMC: Automatic object segmentation using constrained parametric min-cuts", "label": "CPMC: Automatic object segmentation using constrained parametric min-cuts", "shape": "dot", "size": 25, "title": "CPMC: Automatic object segmentation using constrained parametric min-cuts"}, {"color": "#66CCFF", "id": "CPMC", "label": "CPMC", "shape": "dot", "size": 25, "title": "CPMC"}, {"color": "#66CCFF", "id": "Carreira, J.", "label": "Carreira, J.", "shape": "dot", "size": 25, "title": "Carreira, J."}, {"color": "#66CCFF", "id": "Sminchisescu, C.", "label": "Sminchisescu, C.", "shape": "dot", "size": 25, "title": "Sminchisescu, C."}, {"color": "#66CCFF", "id": "Very deep convolutional networks", "label": "Very deep convolutional networks", "shape": "dot", "size": 25, "title": "Very deep convolutional networks"}, {"color": "#66CCFF", "id": "Zisserma, A.", "label": "Zisserma, A.", "shape": "dot", "size": 25, "title": "Zisserma, A."}, {"color": "#66CCFF", "id": "Toyota Technological Institute at Chicago", "label": "Toyota Technological Institute at Chicago", "shape": "dot", "size": 25, "title": "Toyota Technological Institute at Chicago"}, {"color": "#66CCFF", "id": "Fr\u00b4edo Durand", "label": "Fr\u00b4edo Durand", "shape": "dot", "size": 25, "title": "Fr\u00b4edo Durand"}, {"color": "#66CCFF", "id": "Reflection Removal using Ghosting Cues", "label": "Reflection Removal using Ghosting Cues", "shape": "dot", "size": 25, "title": "Reflection Removal using Ghosting Cues"}, {"color": "#66CCFF", "id": "YiChang Shih", "label": "YiChang Shih", "shape": "dot", "size": 25, "title": "YiChang Shih"}, {"color": "#66CCFF", "id": "Dilip Krishnan", "label": "Dilip Krishnan", "shape": "dot", "size": 25, "title": "Dilip Krishnan"}, {"color": "#66CCFF", "id": "William T. Freeman", "label": "William T. Freeman", "shape": "dot", "size": 25, "title": "William T. Freeman"}, {"color": "#66CCFF", "id": "Ghosting Cues", "label": "Ghosting Cues", "shape": "dot", "size": 25, "title": "Ghosting Cues"}, {"color": "#66CCFF", "id": "Ghosting Cunes", "label": "Ghosting Cunes", "shape": "dot", "size": 25, "title": "Ghosting Cunes"}, {"color": "#66CCFF", "id": "Shih_Reflection_Removal_Using_2015_CVPR_paper", "label": "Shih_Reflection_Removal_Using_2015_CVPR_paper", "shape": "dot", "size": 25, "title": "Shih_Reflection_Removal_Using_2015_CVPR_paper"}, {"color": "#66CCFF", "id": "reflection removal", "label": "reflection removal", "shape": "dot", "size": 25, "title": "reflection removal"}, {"color": "#66CCFF", "id": "desired scene", "label": "desired scene", "shape": "dot", "size": 25, "title": "desired scene"}, {"color": "#66CCFF", "id": "undesired reflections", "label": "undesired reflections", "shape": "dot", "size": 25, "title": "undesired reflections"}, {"color": "#66CCFF", "id": "layer separation", "label": "layer separation", "shape": "dot", "size": 25, "title": "layer separation"}, {"color": "#66CCFF", "id": "ill-posed problem", "label": "ill-posed problem", "shape": "dot", "size": 25, "title": "ill-posed problem"}, {"color": "#66CCFF", "id": "ghosting cues", "label": "ghosting cues", "shape": "dot", "size": 25, "title": "ghosting cues"}, {"color": "#66CCFF", "id": "asymmetry", "label": "asymmetry", "shape": "dot", "size": 25, "title": "asymmetry"}, {"color": "#66CCFF", "id": "barely perceptible", "label": "barely perceptible", "shape": "dot", "size": 25, "title": "barely perceptible"}, {"color": "#66CCFF", "id": "ghosted reflection", "label": "ghosted reflection", "shape": "dot", "size": 25, "title": "ghosted reflection"}, {"color": "#66CCFF", "id": "double-impulse convolution kernel", "label": "double-impulse convolution kernel", "shape": "dot", "size": 25, "title": "double-impulse convolution kernel"}, {"color": "#66CCFF", "id": "Gaussian Mixture Model", "label": "Gaussian Mixture Model", "shape": "dot", "size": 25, "title": "Gaussian Mixture Model"}, {"color": "#66CCFF", "id": "single input image", "label": "single input image", "shape": "dot", "size": 25, "title": "single input image"}, {"color": "#66CCFF", "id": "shifted double reflections", "label": "shifted double reflections", "shape": "dot", "size": 25, "title": "shifted double reflections"}, {"color": "#66CCFF", "id": "synthetic inputs", "label": "synthetic inputs", "shape": "dot", "size": 25, "title": "synthetic inputs"}, {"color": "#66CCFF", "id": "real-world inputs", "label": "real-world inputs", "shape": "dot", "size": 25, "title": "real-world inputs"}, {"color": "#66CCFF", "id": "ghosted reflection components", "label": "ghosted reflection components", "shape": "dot", "size": 25, "title": "ghosted reflection components"}, {"color": "#66CCFF", "id": "relative attenuation", "label": "relative attenuation", "shape": "dot", "size": 25, "title": "relative attenuation"}, {"color": "#66CCFF", "id": "Reflection Removal", "label": "Reflection Removal", "shape": "dot", "size": 25, "title": "Reflection Removal"}, {"color": "#66CCFF", "id": "MIT CSAIL", "label": "MIT CSAIL", "shape": "dot", "size": 25, "title": "MIT CSAIL"}, {"color": "#66CCFF", "id": "Google Research", "label": "Google Research", "shape": "dot", "size": 25, "title": "Google Research"}, {"color": "#66CCFF", "id": "Soonmin Hwang", "label": "Soonmin Hwang", "shape": "dot", "size": 25, "title": "Soonmin Hwang"}, {"color": "#66CCFF", "id": "Multispectral Pedestrian Detection", "label": "Multispectral Pedestrian Detection", "shape": "dot", "size": 25, "title": "Multispectral Pedestrian Detection"}, {"color": "#66CCFF", "id": "Benchmark Dataset", "label": "Benchmark Dataset", "shape": "dot", "size": 25, "title": "Benchmark Dataset"}, {"color": "#66CCFF", "id": "pedestrian datasets", "label": "pedestrian datasets", "shape": "dot", "size": 25, "title": "pedestrian datasets"}, {"color": "#66CCFF", "id": "color channel", "label": "color channel", "shape": "dot", "size": 25, "title": "color channel"}, {"color": "#66CCFF", "id": "thermal channel", "label": "thermal channel", "shape": "dot", "size": 25, "title": "thermal channel"}, {"color": "#66CCFF", "id": "multispectral pedestrian dataset", "label": "multispectral pedestrian dataset", "shape": "dot", "size": 25, "title": "multispectral pedestrian dataset"}, {"color": "#66CCFF", "id": "color-thermal image pairs", "label": "color-thermal image pairs", "shape": "dot", "size": 25, "title": "color-thermal image pairs"}, {"color": "#66CCFF", "id": "previous color-based datasets", "label": "previous color-based datasets", "shape": "dot", "size": 25, "title": "previous color-based datasets"}, {"color": "#66CCFF", "id": "dense annotations", "label": "dense annotations", "shape": "dot", "size": 25, "title": "dense annotations"}, {"color": "#66CCFF", "id": "multispectral ACF", "label": "multispectral ACF", "shape": "dot", "size": 25, "title": "multispectral ACF"}, {"color": "#66CCFF", "id": "aggregated channel features (ACF)", "label": "aggregated channel features (ACF)", "shape": "dot", "size": 25, "title": "aggregated channel features (ACF)"}, {"color": "#66CCFF", "id": "average miss rate of ACF", "label": "average miss rate of ACF", "shape": "dot", "size": 25, "title": "average miss rate of ACF"}, {"color": "#66CCFF", "id": "spectral ACF", "label": "spectral ACF", "shape": "dot", "size": 25, "title": "spectral ACF"}, {"color": "#66CCFF", "id": "aggregated channel features", "label": "aggregated channel features", "shape": "dot", "size": 25, "title": "aggregated channel features"}, {"color": "#66CCFF", "id": "Multispectral ACF", "label": "Multispectral ACF", "shape": "dot", "size": 25, "title": "Multispectral ACF"}, {"color": "#66CCFF", "id": "average miss rate", "label": "average miss rate", "shape": "dot", "size": 25, "title": "average miss rate"}, {"color": "#66CCFF", "id": "15%", "label": "15%", "shape": "dot", "size": 25, "title": "15%"}, {"color": "#66CCFF", "id": "image feature", "label": "image feature", "shape": "dot", "size": 25, "title": "image feature"}, {"color": "#66CCFF", "id": "image type", "label": "image type", "shape": "dot", "size": 25, "title": "image type"}, {"color": "#66CCFF", "id": "image analysis task", "label": "image analysis task", "shape": "dot", "size": 25, "title": "image analysis task"}, {"color": "#66CCFF", "id": "miss rate", "label": "miss rate", "shape": "dot", "size": 25, "title": "miss rate"}, {"color": "#66CCFF", "id": "breakthrough in pedestrian detection", "label": "breakthrough in pedestrian detection", "shape": "dot", "size": 25, "title": "breakthrough in pedestrian detection"}, {"color": "#66CCFF", "id": "RGBD-Fusion", "label": "RGBD-Fusion", "shape": "dot", "size": 25, "title": "RGBD-Fusion"}, {"color": "#66CCFF", "id": "real-time", "label": "real-time", "shape": "dot", "size": 25, "title": "real-time"}, {"color": "#66CCFF", "id": "high precision", "label": "high precision", "shape": "dot", "size": 25, "title": "high precision"}, {"color": "#66CCFF", "id": "Jaesik Park", "label": "Jaesik Park", "shape": "dot", "size": 25, "title": "Jaesik Park"}, {"color": "#66CCFF", "id": "Namil Kim", "label": "Namil Kim", "shape": "dot", "size": 25, "title": "Namil Kim"}, {"color": "#66CCFF", "id": "Roy Or", "label": "Roy Or", "shape": "dot", "size": 25, "title": "Roy Or"}, {"color": "#66CCFF", "id": "RGB-D scanners", "label": "RGB-D scanners", "shape": "dot", "size": 25, "title": "RGB-D scanners"}, {"color": "#66CCFF", "id": "subtle details", "label": "subtle details", "shape": "dot", "size": 25, "title": "subtle details"}, {"color": "#66CCFF", "id": "lighting model", "label": "lighting model", "shape": "dot", "size": 25, "title": "lighting model"}, {"color": "#66CCFF", "id": "natural scene illumination", "label": "natural scene illumination", "shape": "dot", "size": 25, "title": "natural scene illumination"}, {"color": "#66CCFF", "id": "shape from shading-like technique", "label": "shape from shading-like technique", "shape": "dot", "size": 25, "title": "shape from shading-like technique"}, {"color": "#66CCFF", "id": "visual fidelity", "label": "visual fidelity", "shape": "dot", "size": 25, "title": "visual fidelity"}, {"color": "#66CCFF", "id": "detailed geometry", "label": "detailed geometry", "shape": "dot", "size": 25, "title": "detailed geometry"}, {"color": "#66CCFF", "id": "four orders of magnitude faster", "label": "four orders of magnitude faster", "shape": "dot", "size": 25, "title": "four orders of magnitude faster"}, {"color": "#66CCFF", "id": "evidence", "label": "evidence", "shape": "dot", "size": 25, "title": "evidence"}, {"color": "#66CCFF", "id": "improvement in depth", "label": "improvement in depth", "shape": "dot", "size": 25, "title": "improvement in depth"}, {"color": "#66CCFF", "id": "Depth map enhancement", "label": "Depth map enhancement", "shape": "dot", "size": 25, "title": "Depth map enhancement"}, {"color": "#66CCFF", "id": "Shape from shading", "label": "Shape from shading", "shape": "dot", "size": 25, "title": "Shape from shading"}, {"color": "#66CCFF", "id": "Lighting models", "label": "Lighting models", "shape": "dot", "size": 25, "title": "Lighting models"}, {"color": "#66CCFF", "id": "Real-time processing", "label": "Real-time processing", "shape": "dot", "size": 25, "title": "Real-time processing"}, {"color": "#66CCFF", "id": "Lambertian reflectance", "label": "Lambertian reflectance", "shape": "dot", "size": 25, "title": "Lambertian reflectance"}, {"color": "#66CCFF", "id": "Computer Vision, Graphics, and Image Processing", "label": "Computer Vision, Graphics, and Image Processing", "shape": "dot", "size": 25, "title": "Computer Vision, Graphics, and Image Processing"}, {"color": "#66CCFF", "id": "Bayesian nonparametric intrinsic image decomposition", "label": "Bayesian nonparametric intrinsic image decomposition", "shape": "dot", "size": 25, "title": "Bayesian nonparametric intrinsic image decomposition"}, {"color": "#66CCFF", "id": "Variable-source shading analysis", "label": "Variable-source shading analysis", "shape": "dot", "size": 25, "title": "Variable-source shading analysis"}, {"color": "#66CCFF", "id": "Grosse, R.", "label": "Grosse, R.", "shape": "dot", "size": 25, "title": "Grosse, R."}, {"color": "#66CCFF", "id": "Ground-truth dataset", "label": "Ground-truth dataset", "shape": "dot", "size": 25, "title": "Ground-truth dataset"}, {"color": "#66CCFF", "id": "Han, Y.", "label": "Han, Y.", "shape": "dot", "size": 25, "title": "Han, Y."}, {"color": "#66CCFF", "id": "High quality shape", "label": "High quality shape", "shape": "dot", "size": 25, "title": "High quality shape"}, {"color": "#66CCFF", "id": "Horn, B. K.", "label": "Horn, B. K.", "shape": "dot", "size": 25, "title": "Horn, B. K."}, {"color": "#66CCFF", "id": "PhD thesis", "label": "PhD thesis", "shape": "dot", "size": 25, "title": "PhD thesis"}, {"color": "#66CCFF", "id": "The variational approach", "label": "The variational approach", "shape": "dot", "size": 25, "title": "The variational approach"}, {"color": "#66CCFF", "id": "RGB-D image", "label": "RGB-D image", "shape": "dot", "size": 25, "title": "RGB-D image"}, {"color": "#66CCFF", "id": "Horn \u0026 Brooks", "label": "Horn \u0026 Brooks", "shape": "dot", "size": 25, "title": "Horn \u0026 Brooks"}, {"color": "#66CCFF", "id": "The variational approach to shape from shading", "label": "The variational approach to shape from shading", "shape": "dot", "size": 25, "title": "The variational approach to shape from shading"}, {"color": "#66CCFF", "id": "Johnson \u0026 Adelison", "label": "Johnson \u0026 Adelison", "shape": "dot", "size": 25, "title": "Johnson \u0026 Adelison"}, {"color": "#66CCFF", "id": "Technion, Israel Institute of Technology", "label": "Technion, Israel Institute of Technology", "shape": "dot", "size": 25, "title": "Technion, Israel Institute of Technology"}, {"color": "#66CCFF", "id": "Guy Rosman", "label": "Guy Rosman", "shape": "dot", "size": 25, "title": "Guy Rosman"}, {"color": "#66CCFF", "id": "Computer Science and Artificial Intelligence Lab, MIT", "label": "Computer Science and Artificial Intelligence Lab, MIT", "shape": "dot", "size": 25, "title": "Computer Science and Artificial Intelligence Lab, MIT"}, {"color": "#66CCFF", "id": "Aaron Wetzler", "label": "Aaron Wetzler", "shape": "dot", "size": 25, "title": "Aaron Wetzler"}, {"color": "#66CCFF", "id": "Ron Kimmel", "label": "Ron Kimmel", "shape": "dot", "size": 25, "title": "Ron Kimmel"}, {"color": "#66CCFF", "id": "Alfred M. Bruckstein", "label": "Alfred M. Bruckstein", "shape": "dot", "size": 25, "title": "Alfred M. Bruckstein"}, {"color": "#66CCFF", "id": "Xiangyu Zhu", "label": "Xiangyu Zhu", "shape": "dot", "size": 25, "title": "Xiangyu Zhu"}, {"color": "#66CCFF", "id": "High-Fidelity Pose and Expression Normalization for Face Recognition in the Wild", "label": "High-Fidelity Pose and Expression Normalization for Face Recognition in the Wild", "shape": "dot", "size": 25, "title": "High-Fidelity Pose and Expression Normalization for Face Recognition in the Wild"}, {"color": "#66CCFF", "id": "Junnie Yan", "label": "Junnie Yan", "shape": "dot", "size": 25, "title": "Junnie Yan"}, {"color": "#66CCFF", "id": "High-Fidelity Pose and Expression Normalization for FaceRecognition in the Wild", "label": "High-Fidelity Pose and Expression Normalization for FaceRecognition in the Wild", "shape": "dot", "size": 25, "title": "High-Fidelity Pose and Expression Normalization for FaceRecognition in the Wild"}, {"color": "#66CCFF", "id": "Junjie Yan", "label": "Junjie Yan", "shape": "dot", "size": 25, "title": "Junjie Yan"}, {"color": "#66CCFF", "id": "Dong Yi", "label": "Dong Yi", "shape": "dot", "size": 25, "title": "Dong Yi"}, {"color": "#66CCFF", "id": "freddy@cs.technion.ac.il", "label": "freddy@cs.technion.ac.il", "shape": "dot", "size": 25, "title": "freddy@cs.technion.ac.il"}, {"color": "#66CCFF", "id": "Technion - Israel Institute of Technology", "label": "Technion - Israel Institute of Technology", "shape": "dot", "size": 25, "title": "Technion - Israel Institute of Technology"}, {"color": "#66CCFF", "id": "Zhu_High-Fidelity_Pose_and_2015_CVPR_paper.pdf", "label": "Zhu_High-Fidelity_Pose_and_2015_CVPR_paper.pdf", "shape": "dot", "size": 25, "title": "Zhu_High-Fidelity_Pose_and_2015_CVPR_paper.pdf"}, {"color": "#66CCFF", "id": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/", "label": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/", "shape": "dot", "size": 25, "title": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/"}, {"color": "#66CCFF", "id": "face recognition performance", "label": "face recognition performance", "shape": "dot", "size": 25, "title": "face recognition performance"}, {"color": "#66CCFF", "id": "pose variations", "label": "pose variations", "shape": "dot", "size": 25, "title": "pose variations"}, {"color": "#66CCFF", "id": "HPEN method", "label": "HPEN method", "shape": "dot", "size": 25, "title": "HPEN method"}, {"color": "#66CCFF", "id": "3D Morphable Model", "label": "3D Morphable Model", "shape": "dot", "size": 25, "title": "3D Morphable Model"}, {"color": "#66CCFF", "id": "face images", "label": "face images", "shape": "dot", "size": 25, "title": "face images"}, {"color": "#66CCFF", "id": "frontal pose", "label": "frontal pose", "shape": "dot", "size": 25, "title": "frontal pose"}, {"color": "#66CCFF", "id": "neutral expression", "label": "neutral expression", "shape": "dot", "size": 25, "title": "neutral expression"}, {"color": "#66CCFF", "id": "landmark marching", "label": "landmark marching", "shape": "dot", "size": 25, "title": "landmark marching"}, {"color": "#66CCFF", "id": "3DMM fitting", "label": "3DMM fitting", "shape": "dot", "size": 25, "title": "3DMM fitting"}, {"color": "#66CCFF", "id": "3D meshing", "label": "3D meshing", "shape": "dot", "size": 25, "title": "3D meshing"}, {"color": "#66CCFF", "id": "Poisson Editing", "label": "Poisson Editing", "shape": "dot", "size": 25, "title": "Poisson Editing"}, {"color": "#66CCFF", "id": "inpainting", "label": "inpainting", "shape": "dot", "size": 25, "title": "inpainting"}, {"color": "#66CCFF", "id": "Multi-PIE", "label": "Multi-PIE", "shape": "dot", "size": 25, "title": "Multi-PIE"}, {"color": "#66CCFF", "id": "3D Morphable Models", "label": "3D Morphable Models", "shape": "dot", "size": 25, "title": "3D Morphable Models"}, {"color": "#66CCFF", "id": "Amberg, B.", "label": "Amberg, B.", "shape": "dot", "size": 25, "title": "Amberg, B."}, {"color": "#66CCFF", "id": "Optimal step non-rigid icp algorithms for surface registration", "label": "Optimal step non-rigid icp algorithms for surface registration", "shape": "dot", "size": 25, "title": "Optimal step non-rigid icp algorithms for surface registration"}, {"color": "#66CCFF", "id": "Romdhani, S.", "label": "Romdhani, S.", "shape": "dot", "size": 25, "title": "Romdhani, S."}, {"color": "#66CCFF", "id": "Vetter, T.", "label": "Vetter, T.", "shape": "dot", "size": 25, "title": "Vetter, T."}, {"color": "#66CCFF", "id": "Chai, X.", "label": "Chai, X.", "shape": "dot", "size": 25, "title": "Chai, X."}, {"color": "#66CCFF", "id": "Locally linear regression for pose-invariant face recognition", "label": "Locally linear regression for pose-invariant face recognition", "shape": "dot", "size": 25, "title": "Locally linear regression for pose-invariant face recognition"}, {"color": "#66CCFF", "id": "Shan, S.", "label": "Shan, S.", "shape": "dot", "size": 25, "title": "Shan, S."}, {"color": "#66CCFF", "id": "Gao, W.", "label": "Gao, W.", "shape": "dot", "size": 25, "title": "Gao, W."}, {"color": "#66CCFF", "id": "Locably linear regression for pose-invariant face recognition", "label": "Locably linear regression for pose-invariant face recognition", "shape": "dot", "size": 25, "title": "Locably linear regression for pose-invariant face recognition"}, {"color": "#66CCFF", "id": "Arashloo, S. R.", "label": "Arashloo, S. R.", "shape": "dot", "size": 25, "title": "Arashloo, S. R."}, {"color": "#66CCFF", "id": "Pose-invariant face matching using MRF energy minimization framework", "label": "Pose-invariant face matching using MRF energy minimization framework", "shape": "dot", "size": 25, "title": "Pose-invariant face matching using MRF energy minimization framework"}, {"color": "#66CCFF", "id": "Kittler, J.", "label": "Kittler, J.", "shape": "dot", "size": 25, "title": "Kittler, J."}, {"color": "#66CCFF", "id": "Chan, C. H.", "label": "Chan, C. H.", "shape": "dot", "size": 25, "title": "Chan, C. H."}, {"color": "#66CCFF", "id": "Multisculse local phase quantization for robust component-based face recognition", "label": "Multisculse local phase quantization for robust component-based face recognition", "shape": "dot", "size": 25, "title": "Multisculse local phase quantization for robust component-based face recognition"}, {"color": "#66CCFF", "id": "Tahir, M. A.", "label": "Tahir, M. A.", "shape": "dot", "size": 25, "title": "Tahir, M. A."}, {"color": "#66CCFF", "id": "local phase quantization", "label": "local phase quantization", "shape": "dot", "size": 25, "title": "local phase quantization"}, {"color": "#66CCFF", "id": "robust component-based face recognition", "label": "robust component-based face recognition", "shape": "dot", "size": 25, "title": "robust component-based face recognition"}, {"color": "#66CCFF", "id": "component-based face recognition", "label": "component-based face recognition", "shape": "dot", "size": 25, "title": "component-based face recognition"}, {"color": "#66CCFF", "id": "kernel fusion", "label": "kernel fusion", "shape": "dot", "size": 25, "title": "kernel fusion"}, {"color": "#66CCFF", "id": "multiple descriptors", "label": "multiple descriptors", "shape": "dot", "size": 25, "title": "multiple descriptors"}, {"color": "#66CCFF", "id": "Chen, D. (2012)", "label": "Chen, D. (2012)", "shape": "dot", "size": 25, "title": "Chen, D. (2012)"}, {"color": "#66CCFF", "id": "joint formulation", "label": "joint formulation", "shape": "dot", "size": 25, "title": "joint formulation"}, {"color": "#66CCFF", "id": "high-dimensional feature", "label": "high-dimensional feature", "shape": "dot", "size": 25, "title": "high-dimensional feature"}, {"color": "#66CCFF", "id": "efficient compression", "label": "efficient compression", "shape": "dot", "size": 25, "title": "efficient compression"}, {"color": "#66CCFF", "id": "Asthana, A. (2013)", "label": "Asthana, A. (2013)", "shape": "dot", "size": 25, "title": "Asthana, A. (2013)"}, {"color": "#66CCFF", "id": "discriminative response map fitting", "label": "discriminative response map fitting", "shape": "dot", "size": 25, "title": "discriminative response map fitting"}, {"color": "#66CCFF", "id": "constrained local models", "label": "constrained local models", "shape": "dot", "size": 25, "title": "constrained local models"}, {"color": "#66CCFF", "id": "2013 IEEE Conference on Computer Vision and Pattern Recognition", "label": "2013 IEEE Conference on Computer Vision and Pattern Recognition", "shape": "dot", "size": 25, "title": "2013 IEEE Conference on Computer Vision and Pattern Recognition"}, {"color": "#66CCFF", "id": "high-dimensional feature compression", "label": "high-dimensional feature compression", "shape": "dot", "size": 25, "title": "high-dimensional feature compression"}, {"color": "#66CCFF", "id": "Cheng, S.", "label": "Cheng, S.", "shape": "dot", "size": 25, "title": "Cheng, S."}, {"color": "#66CCFF", "id": "Robust discriminative response map fitting", "label": "Robust discriminative response map fitting", "shape": "dot", "size": 25, "title": "Robust discriminative response map fitting"}, {"color": "#66CCFF", "id": "Barkan, O.", "label": "Barkan, O.", "shape": "dot", "size": 25, "title": "Barkan, O."}, {"color": "#66CCFF", "id": "Fast high dimensional vector multiplication face recognition", "label": "Fast high dimensional vector multiplication face recognition", "shape": "dot", "size": 25, "title": "Fast high dimensional vector multiplication face recognition"}, {"color": "#66CCFF", "id": "Weill, J.", "label": "Weill, J.", "shape": "dot", "size": 25, "title": "Weill, J."}, {"color": "#66CCFF", "id": "Wolf, L.", "label": "Wolf, L.", "shape": "dot", "size": 25, "title": "Wolf, L."}, {"color": "#66CCFF", "id": "Aronowitz, H.", "label": "Aronowitz, H.", "shape": "dot", "size": 25, "title": "Aronowitz, H."}, {"color": "#66CCFF", "id": "Center for Biometrics and Security Research", "label": "Center for Biometrics and Security Research", "shape": "dot", "size": 25, "title": "Center for Biometrics and Security Research"}, {"color": "#66CCFF", "id": "National Laboratory of Pattern Recognition", "label": "National Laboratory of Pattern Recognition", "shape": "dot", "size": 25, "title": "National Laboratory of Pattern Recognition"}, {"color": "#66CCFF", "id": "Center for Biomatrics and Security Research", "label": "Center for Biomatrics and Security Research", "shape": "dot", "size": 25, "title": "Center for Biomatrics and Security Research"}, {"color": "#66CCFF", "id": "jjyan@nlpr.ia.ac.cn", "label": "jjyan@nlpr.ia.ac.cn", "shape": "dot", "size": 25, "title": "jjyan@nlpr.ia.ac.cn"}, {"color": "#66CCFF", "id": "szli@nlpr.ia.ac.cn", "label": "szli@nlpr.ia.ac.cn", "shape": "dot", "size": 25, "title": "szli@nlpr.ia.ac.cn"}, {"color": "#66CCFF", "id": "Parsing Occluded People", "label": "Parsing Occluded People", "shape": "dot", "size": 25, "title": "Parsing Occluded People"}, {"color": "#66CCFF", "id": "Xianjie Chen", "label": "Xianjie Chen", "shape": "dot", "size": 25, "title": "Xianjie Chen"}, {"color": "#66CCFF", "id": "Alan Yuille", "label": "Alan Yuille", "shape": "dot", "size": 25, "title": "Alan Yuille"}, {"color": "#66CCFF", "id": "Chen Xianjie", "label": "Chen Xianjie", "shape": "dot", "size": 25, "title": "Chen Xianjie"}, {"color": "#66CCFF", "id": "Alan Yuilie", "label": "Alan Yuilie", "shape": "dot", "size": 25, "title": "Alan Yuilie"}, {"color": "#66CCFF", "id": "parsing humans", "label": "parsing humans", "shape": "dot", "size": 25, "title": "parsing humans"}, {"color": "#66CCFF", "id": "graphical model", "label": "graphical model", "shape": "dot", "size": 25, "title": "graphical model"}, {"color": "#66CCFF", "id": "tree structure", "label": "tree structure", "shape": "dot", "size": 25, "title": "tree structure"}, {"color": "#66CCFF", "id": "connected subtree", "label": "connected subtree", "shape": "dot", "size": 25, "title": "connected subtree"}, {"color": "#66CCFF", "id": "flexible composition", "label": "flexible composition", "shape": "dot", "size": 25, "title": "flexible composition"}, {"color": "#66CCFF", "id": "inference", "label": "inference", "shape": "dot", "size": 25, "title": "inference"}, {"color": "#66CCFF", "id": "search over models", "label": "search over models", "shape": "dot", "size": 25, "title": "search over models"}, {"color": "#66CCFF", "id": "part sharing", "label": "part sharing", "shape": "dot", "size": 25, "title": "part sharing"}, {"color": "#66CCFF", "id": "computations", "label": "computations", "shape": "dot", "size": 25, "title": "computations"}, {"color": "#66CCFF", "id": "twice as many", "label": "twice as many", "shape": "dot", "size": 25, "title": "twice as many"}, {"color": "#66CCFF", "id": "We Are Family", "label": "We Are Family", "shape": "dot", "size": 25, "title": "We Are Family"}, {"color": "#66CCFF", "id": "searching", "label": "searching", "shape": "dot", "size": 25, "title": "searching"}, {"color": "#66CCFF", "id": "entire object", "label": "entire object", "shape": "dot", "size": 25, "title": "entire object"}, {"color": "#66CCFF", "id": "Stickmen dataset", "label": "Stickmen dataset", "shape": "dot", "size": 25, "title": "Stickmen dataset"}, {"color": "#66CCFF", "id": "standard benchmarked dataset", "label": "standard benchmarked dataset", "shape": "dot", "size": 25, "title": "standard benchmarked dataset"}, {"color": "#66CCFF", "id": "alternative algorithms", "label": "alternative algorithms", "shape": "dot", "size": 25, "title": "alternative algorithms"}, {"color": "#66CCFF", "id": "best", "label": "best", "shape": "dot", "size": 25, "title": "best"}, {"color": "#66CCFF", "id": "modeling", "label": "modeling", "shape": "dot", "size": 25, "title": "modeling"}, {"color": "#66CCFF", "id": "occlusion", "label": "occlusion", "shape": "dot", "size": 25, "title": "occlusion"}, {"color": "#66CCFF", "id": "Graphical models", "label": "Graphical models", "shape": "dot", "size": 25, "title": "Graphical models"}, {"color": "#66CCFF", "id": "Sutskever, I.", "label": "Sutskever, I.", "shape": "dot", "size": 25, "title": "Sutskever, I."}, {"color": "#66CCFF", "id": "Hinton, G. E.", "label": "Hinton, G. E.", "shape": "dot", "size": 25, "title": "Hinton, G. E."}, {"color": "#66CCFF", "id": "Yuilie, A.", "label": "Yuilie, A.", "shape": "dot", "size": 25, "title": "Yuilie, A."}, {"color": "#66CCFF", "id": "Huttenlocher, D. P.", "label": "Huttenlocher, D. P.", "shape": "dot", "size": 25, "title": "Huttenlocher, D. P."}, {"color": "#66CCFF", "id": "pictorial structures", "label": "pictorial structures", "shape": "dot", "size": 25, "title": "pictorial structures"}, {"color": "#66CCFF", "id": "grammar models", "label": "grammar models", "shape": "dot", "size": 25, "title": "grammar models"}, {"color": "#66CCFF", "id": "Ferrari, V.", "label": "Ferrari, V.", "shape": "dot", "size": 25, "title": "Ferrari, V."}, {"color": "#66CCFF", "id": "Marin-Jimenez, M.", "label": "Marin-Jimenez, M.", "shape": "dot", "size": 25, "title": "Marin-Jimenez, M."}, {"color": "#66CCFF", "id": "human pose estimation", "label": "human pose estimation", "shape": "dot", "size": 25, "title": "human pose estimation"}, {"color": "#66CCFF", "id": "progressive search space reduction", "label": "progressive search space reduction", "shape": "dot", "size": 25, "title": "progressive search space reduction"}, {"color": "#66CCFF", "id": "support-vector networks", "label": "support-vector networks", "shape": "dot", "size": 25, "title": "support-vector networks"}, {"color": "#66CCFF", "id": "Cortes", "label": "Cortes", "shape": "dot", "size": 25, "title": "Cortes"}, {"color": "#66CCFF", "id": "Support-vector networks", "label": "Support-vector networks", "shape": "dot", "size": 25, "title": "Support-vector networks"}, {"color": "#66CCFF", "id": "Dalal", "label": "Dalal", "shape": "dot", "size": 25, "title": "Dalal"}, {"color": "#66CCFF", "id": "Triggs", "label": "Triggs", "shape": "dot", "size": 25, "title": "Triggs"}, {"color": "#66CCFF", "id": "Sapp", "label": "Sapp", "shape": "dot", "size": 25, "title": "Sapp"}, {"color": "#66CCFF", "id": "Adaptive pose priors", "label": "Adaptive pose priors", "shape": "dot", "size": 25, "title": "Adaptive pose priors"}, {"color": "#66CCFF", "id": "Jordan", "label": "Jordan", "shape": "dot", "size": 25, "title": "Jordan"}, {"color": "#66CCFF", "id": "Taskar", "label": "Taskar", "shape": "dot", "size": 25, "title": "Taskar"}, {"color": "#66CCFF", "id": "University of California, Los Angeles", "label": "University of California, Los Angeles", "shape": "dot", "size": 25, "title": "University of California, Los Angeles"}, {"color": "#66CCFF", "id": "Yuille", "label": "Yuille", "shape": "dot", "size": 25, "title": "Yuille"}, {"color": "#66CCFF", "id": "Alabort-i-Medina", "label": "Alabort-i-Medina", "shape": "dot", "size": 25, "title": "Alabort-i-Medina"}, {"color": "#66CCFF", "id": "Unifying Holistic and Parts-Based Deformable Model Fitting", "label": "Unifying Holistic and Parts-Based Deformable Model Fitting", "shape": "dot", "size": 25, "title": "Unifying Holistic and Parts-Based Deformable Model Fitting"}, {"color": "#66CCFF", "id": "deformable models", "label": "deformable models", "shape": "dot", "size": 25, "title": "deformable models"}, {"color": "#66CCFF", "id": "degrees of freedom", "label": "degrees of freedom", "shape": "dot", "size": 25, "title": "degrees of freedom"}, {"color": "#66CCFF", "id": "unified approach", "label": "unified approach", "shape": "dot", "size": 25, "title": "unified approach"}, {"color": "#66CCFF", "id": "Face Alignment", "label": "Face Alignment", "shape": "dot", "size": 25, "title": "Face Alignment"}, {"color": "#66CCFF", "id": "Holistic Deformable Models", "label": "Holistic Deformable Models", "shape": "dot", "size": 25, "title": "Holistic Deformable Models"}, {"color": "#66CCFF", "id": "Parts-Based Deformable Models", "label": "Parts-Based Deformable Models", "shape": "dot", "size": 25, "title": "Parts-Based Deformable Models"}, {"color": "#66CCFF", "id": "Deformable Models", "label": "Deformable Models", "shape": "dot", "size": 25, "title": "Deformable Models"}, {"color": "#66CCFF", "id": "Active Appearance Models", "label": "Active Appearance Models", "shape": "dot", "size": 25, "title": "Active Appearance Models"}, {"color": "#66CCFF", "id": "Active Shape Models", "label": "Active Shape Models", "shape": "dot", "size": 25, "title": "Active Shape Models"}, {"color": "#66CCFF", "id": "Lucas-Kanade Method", "label": "Lucas-Kanade Method", "shape": "dot", "size": 25, "title": "Lucas-Kanade Method"}, {"color": "#66CCFF", "id": "T. F. Cootes, G. J. Edwards, and C. J. Taylor (2001)", "label": "T. F. Cootes, G. J. Edwards, and C. J. Taylor (2001)", "shape": "dot", "size": 25, "title": "T. F. Cootes, G. J. Edwards, and C. J. Taylor (2001)"}, {"color": "#66CCFF", "id": "Local Scale-Invariant Features", "label": "Local Scale-Invariant Features", "shape": "dot", "size": 25, "title": "Local Scale-Invariant Features"}, {"color": "#66CCFF", "id": "T. F. Cootes, C. J. Taylor, D. H. Cooper, and J. Graham (1995)", "label": "T. F. Cootes, C. J. Taylor, D. H. Cooper, and J. Graham (1995)", "shape": "dot", "size": 25, "title": "T. F. Cootes, C. J. Taylor, D. H. Cooper, and J. Graham (1995)"}, {"color": "#66CCFF", "id": "Bayesian Active Appearance Models", "label": "Bayesian Active Appearance Models", "shape": "dot", "size": 25, "title": "Bayesian Active Appearance Models"}, {"color": "#66CCFF", "id": "J. Alabort-i-Medina and S. Zafeiriou (2014)", "label": "J. Alabort-i-Medina and S. Zafeiriou (2014)", "shape": "dot", "size": 25, "title": "J. Alabort-i-Medina and S. Zafeiriou (2014)"}, {"color": "#66CCFF", "id": "Conference on Computer Vision and Pattern Recognition (CVPR)", "label": "Conference on Computer Vision and Pattern Recognition (CVPR)", "shape": "dot", "size": 25, "title": "Conference on Computer Vision and Pattern Recognition (CVPR)"}, {"color": "#66CCFF", "id": "2014", "label": "2014", "shape": "dot", "size": 25, "title": "2014"}, {"color": "#66CCFF", "id": "Lucas-Kanade", "label": "Lucas-Kanade", "shape": "dot", "size": 25, "title": "Lucas-Kanade"}, {"color": "#66CCFF", "id": "International Journal of Computer Vision (IJCR)", "label": "International Journal of Computer Vision (IJCR)", "shape": "dot", "size": 25, "title": "International Journal of Computer Vision (IJCR)"}, {"color": "#66CCFF", "id": "X. Cao", "label": "X. Cao", "shape": "dot", "size": 25, "title": "X. Cao"}, {"color": "#66CCFF", "id": "Face alignment by explicit shape regression", "label": "Face alignment by explicit shape regression", "shape": "dot", "size": 25, "title": "Face alignment by explicit shape regression"}, {"color": "#66CCFF", "id": "G. Papandreou", "label": "G. Papandreou", "shape": "dot", "size": 25, "title": "G. Papandreou"}, {"color": "#66CCFF", "id": "Adaptive and constrained algorithms", "label": "Adaptive and constrained algorithms", "shape": "dot", "size": 25, "title": "Adaptive and constrained algorithms"}, {"color": "#66CCFF", "id": "A. Asthana", "label": "A. Asthana", "shape": "dot", "size": 25, "title": "A. Asthana"}, {"color": "#66CCFF", "id": "unifying framework", "label": "unifying framework", "shape": "dot", "size": 25, "title": "unifying framework"}, {"color": "#66CCFF", "id": "inverse compositional active appearance model fitting", "label": "inverse compositional active appearance model fitting", "shape": "dot", "size": 25, "title": "inverse compositional active appearance model fitting"}, {"color": "#66CCFF", "id": "S. Zafeiriou", "label": "S. Zafeiriou", "shape": "dot", "size": 25, "title": "S. Zafeiriou"}, {"color": "#66CCFF", "id": "Conference on Computer Vision and Pattern Reduction (CVPR)", "label": "Conference on Computer Vision and Pattern Reduction (CVPR)", "shape": "dot", "size": 25, "title": "Conference on Computer Vision and Pattern Reduction (CVPR)"}, {"color": "#66CCFF", "id": "J. Sragih", "label": "J. Sragih", "shape": "dot", "size": 25, "title": "J. Sragih"}, {"color": "#66CCFF", "id": "Joan Alabort-i-Medina", "label": "Joan Alabort-i-Medina", "shape": "dot", "size": 25, "title": "Joan Alabort-i-Medina"}, {"color": "#66CCFF", "id": "Imperial College London", "label": "Imperial College London", "shape": "dot", "size": 25, "title": "Imperial College London"}, {"color": "#66CCFF", "id": "ja310@imperial.ac.uk", "label": "ja310@imperial.ac.uk", "shape": "dot", "size": 25, "title": "ja310@imperial.ac.uk"}, {"color": "#66CCFF", "id": "Stefanos Zafeiriou", "label": "Stefanos Zafeiriou", "shape": "dot", "size": 25, "title": "Stefanos Zafeiriou"}, {"color": "#66CCFF", "id": "Department of Computing", "label": "Department of Computing", "shape": "dot", "size": 25, "title": "Department of Computing"}, {"color": "#66CCFF", "id": "s.zafeiriou@imperial.ac.uk", "label": "s.zafeiriou@imperial.ac.uk", "shape": "dot", "size": 25, "title": "s.zafeiriou@imperial.ac.uk"}, {"color": "#66CCFF", "id": "Jia Xu", "label": "Jia Xu", "shape": "dot", "size": 25, "title": "Jia Xu"}, {"color": "#66CCFF", "id": "Gaze-Enabled Egocentric Video Summarization", "label": "Gaze-Enabled Egocentric Video Summarization", "shape": "dot", "size": 25, "title": "Gaze-Enabled Egocentric Video Summarization"}, {"color": "#66CCFF", "id": "Lopamudra Mukherjee", "label": "Lopamudra Mukherjee", "shape": "dot", "size": 25, "title": "Lopamudra Mukherjee"}, {"color": "#66CCFF", "id": "Yin Li", "label": "Yin Li", "shape": "dot", "size": 25, "title": "Yin Li"}, {"color": "#66CCFF", "id": "Jamieson Warner", "label": "Jamieson Warner", "shape": "dot", "size": 25, "title": "Jamieson Warner"}, {"color": "#66CCFF", "id": "James M. Rehg", "label": "James M. Rehg", "shape": "dot", "size": 25, "title": "James M. Rehg"}, {"color": "#66CCFF", "id": "Vikas Singh", "label": "Vikas Singh", "shape": "dot", "size": 25, "title": "Vikas Singh"}, {"color": "#66CCFF", "id": "increase in egocentric videos", "label": "increase in egocentric videos", "shape": "dot", "size": 25, "title": "increase in egocentric videos"}, {"color": "#66CCFF", "id": "egocentric videos", "label": "egocentric videos", "shape": "dot", "size": 25, "title": "egocentric videos"}, {"color": "#66CCFF", "id": "compact representation", "label": "compact representation", "shape": "dot", "size": 25, "title": "compact representation"}, {"color": "#66CCFF", "id": "egocentric video summarization", "label": "egocentric video summarization", "shape": "dot", "size": 25, "title": "egocentric video summarization"}, {"color": "#66CCFF", "id": "unique challenges", "label": "unique challenges", "shape": "dot", "size": 25, "title": "unique challenges"}, {"color": "#66CCFF", "id": "gaze tracking information", "label": "gaze tracking information", "shape": "dot", "size": 25, "title": "gaze tracking information"}, {"color": "#66CCFF", "id": "summarization", "label": "summarization", "shape": "dot", "size": 25, "title": "summarization"}, {"color": "#66CCFF", "id": "frame comparison", "label": "frame comparison", "shape": "dot", "size": 25, "title": "frame comparison"}, {"color": "#66CCFF", "id": "summarization model", "label": "summarization model", "shape": "dot", "size": 25, "title": "summarization model"}, {"color": "#66CCFF", "id": "submodular function maximization", "label": "submodular function maximization", "shape": "dot", "size": 25, "title": "submodular function maximization"}, {"color": "#66CCFF", "id": "gaze-enabled egocentric video dataset", "label": "gaze-enabled egocentric video dataset", "shape": "dot", "size": 25, "title": "gaze-enabled egocentric video dataset"}, {"color": "#66CCFF", "id": "personalized summaries", "label": "personalized summaries", "shape": "dot", "size": 25, "title": "personalized summaries"}, {"color": "#66CCFF", "id": "Egocentric Video Summarization", "label": "Egocentric Video Summarization", "shape": "dot", "size": 25, "title": "Egocentric Video Summarization"}, {"color": "#66CCFF", "id": "Submodular Function Maximization", "label": "Submodular Function Maximization", "shape": "dot", "size": 25, "title": "Submodular Function Maximization"}, {"color": "#66CCFF", "id": "Multilinear Relaxation", "label": "Multilinear Relaxation", "shape": "dot", "size": 25, "title": "Multilinear Relaxation"}, {"color": "#66CCFF", "id": "Personalized Summarization", "label": "Personalized Summarization", "shape": "dot", "size": 25, "title": "Personalized Summarization"}, {"color": "#66CCFF", "id": "Almeida et al.", "label": "Almeida et al.", "shape": "dot", "size": 25, "title": "Almeida et al."}, {"color": "#66CCFF", "id": "VISON", "label": "VISON", "shape": "dot", "size": 25, "title": "VISON"}, {"color": "#66CCFF", "id": "Online Applications", "label": "Online Applications", "shape": "dot", "size": 25, "title": "Online Applications"}, {"color": "#66CCFF", "id": "Submodular Maximization", "label": "Submodular Maximization", "shape": "dot", "size": 25, "title": "Submodular Maximization"}, {"color": "#66CCFF", "id": "Partition Matroid", "label": "Partition Matroid", "shape": "dot", "size": 25, "title": "Partition Matroid"}, {"color": "#66CCFF", "id": "Filmus \u0026 Ward", "label": "Filmus \u0026 Ward", "shape": "dot", "size": 25, "title": "Filmus \u0026 Ward"}, {"color": "#66CCFF", "id": "Combinatorial Algorithm", "label": "Combinatorial Algorithm", "shape": "dot", "size": 25, "title": "Combinatorial Algorithm"}, {"color": "#66CCFF", "id": "Fujishige", "label": "Fujishige", "shape": "dot", "size": 25, "title": "Fujishige"}, {"color": "#66CCFF", "id": "Submodular Functions and Optimization", "label": "Submodular Functions and Optimization", "shape": "dot", "size": 25, "title": "Submodular Functions and Optimization"}, {"color": "#66CCFF", "id": "Gaze Tracking", "label": "Gaze Tracking", "shape": "dot", "size": 25, "title": "Gaze Tracking"}, {"color": "#66CCFF", "id": "Wearable Cameras", "label": "Wearable Cameras", "shape": "dot", "size": 25, "title": "Wearable Cameras"}, {"color": "#66CCFF", "id": "Ward", "label": "Ward", "shape": "dot", "size": 25, "title": "Ward"}, {"color": "#66CCFF", "id": "Submodular functions and optimization", "label": "Submodular functions and optimization", "shape": "dot", "size": 25, "title": "Submodular functions and optimization"}, {"color": "#66CCFF", "id": "feature hierarchies", "label": "feature hierarchies", "shape": "dot", "size": 25, "title": "feature hierarchies"}, {"color": "#66CCFF", "id": "sequential subset selection", "label": "sequential subset selection", "shape": "dot", "size": 25, "title": "sequential subset selection"}, {"color": "#66CCFF", "id": "Iyer", "label": "Iyer", "shape": "dot", "size": 25, "title": "Iyer"}, {"color": "#66CCFF", "id": "Krause", "label": "Krause", "shape": "dot", "size": 25, "title": "Krause"}, {"color": "#66CCFF", "id": "information gathering", "label": "information gathering", "shape": "dot", "size": 25, "title": "information gathering"}, {"color": "#66CCFF", "id": "Xu", "label": "Xu", "shape": "dot", "size": 25, "title": "Xu"}, {"color": "#66CCFF", "id": "University of Wisconsin-Madison", "label": "University of Wisconsin-Madison", "shape": "dot", "size": 25, "title": "University of Wisconsin-Madison"}, {"color": "#66CCFF", "id": "submodular maximization", "label": "submodular maximization", "shape": "dot", "size": 25, "title": "submodular maximization"}, {"color": "#66CCFF", "id": "University of Wisconsin-Whitewater", "label": "University of Wisconsin-Whitewater", "shape": "dot", "size": 25, "title": "University of Wisconsin-Whitewater"}, {"color": "#66CCFF", "id": "Andreas Geiger", "label": "Andreas Geiger", "shape": "dot", "size": 25, "title": "Andreas Geiger"}, {"color": "#66CCFF", "id": "Object Scene Flow for Autonomous Vehicles", "label": "Object Scene Flow for Autonomous Vehicles", "shape": "dot", "size": 25, "title": "Object Scene Flow for Autonomous Vehicles"}, {"color": "#66CCFF", "id": "Moritz Menze", "label": "Moritz Menze", "shape": "dot", "size": 25, "title": "Moritz Menze"}, {"color": "#66CCFF", "id": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Menze_Object_Scene_Flow_2015_CVPR_supplemental.pdf", "label": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Menze_Object_Scene_Flow_2015_CVPR_supplemental.pdf", "shape": "dot", "size": 25, "title": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Menze_Object_Scene_Flow_2015_CVPR_supplemental.pdf"}, {"color": "#66CCFF", "id": "Menze_Object_Scene_Flow_2015_CVPR_supplemental", "label": "Menze_Object_Scene_Flow_2015_CVPR_supplemental", "shape": "dot", "size": 25, "title": "Menze_Object_Scene_Flow_2015_CVPR_supplemental"}, {"color": "#66CCFF", "id": "supplementary document", "label": "supplementary document", "shape": "dot", "size": 25, "title": "supplementary document"}, {"color": "#66CCFF", "id": "additional descriptions", "label": "additional descriptions", "shape": "dot", "size": 25, "title": "additional descriptions"}, {"color": "#66CCFF", "id": "visualizations", "label": "visualizations", "shape": "dot", "size": 25, "title": "visualizations"}, {"color": "#66CCFF", "id": "scene flow ground truth", "label": "scene flow ground truth", "shape": "dot", "size": 25, "title": "scene flow ground truth"}, {"color": "#66CCFF", "id": "model parameters", "label": "model parameters", "shape": "dot", "size": 25, "title": "model parameters"}, {"color": "#66CCFF", "id": "model sensitivity", "label": "model sensitivity", "shape": "dot", "size": 25, "title": "model sensitivity"}, {"color": "#66CCFF", "id": "small loss in performance", "label": "small loss in performance", "shape": "dot", "size": 25, "title": "small loss in performance"}, {"color": "#66CCFF", "id": "runtime", "label": "runtime", "shape": "dot", "size": 25, "title": "runtime"}, {"color": "#66CCFF", "id": "stereo approaches", "label": "stereo approaches", "shape": "dot", "size": 25, "title": "stereo approaches"}, {"color": "#66CCFF", "id": "optical flow approaches", "label": "optical flow approaches", "shape": "dot", "size": 25, "title": "optical flow approaches"}, {"color": "#66CCFF", "id": "scene flow approaches", "label": "scene flow approaches", "shape": "dot", "size": 25, "title": "scene flow approaches"}, {"color": "#66CCFF", "id": "qualitative results", "label": "qualitative results", "shape": "dot", "size": 25, "title": "qualitative results"}, {"color": "#66CCFF", "id": "state-of-the-art stereo", "label": "state-of-the-art stereo", "shape": "dot", "size": 25, "title": "state-of-the-art stereo"}, {"color": "#66CCFF", "id": "optical flow", "label": "optical flow", "shape": "dot", "size": 25, "title": "optical flow"}, {"color": "#66CCFF", "id": "scene flow", "label": "scene flow", "shape": "dot", "size": 25, "title": "scene flow"}, {"color": "#66CCFF", "id": "KITTI stereo", "label": "KITTI stereo", "shape": "dot", "size": 25, "title": "KITTI stereo"}, {"color": "#66CCFF", "id": "stereo", "label": "stereo", "shape": "dot", "size": 25, "title": "stereo"}, {"color": "#66CCFF", "id": "scene flow dataset", "label": "scene flow dataset", "shape": "dot", "size": 25, "title": "scene flow dataset"}, {"color": "#66CCFF", "id": "novel", "label": "novel", "shape": "dot", "size": 25, "title": "novel"}, {"color": "#66CCFF", "id": "sphere sequence", "label": "sphere sequence", "shape": "dot", "size": 25, "title": "sphere sequence"}, {"color": "#66CCFF", "id": "Optical Flow", "label": "Optical Flow", "shape": "dot", "size": 25, "title": "Optical Flow"}, {"color": "#66CCFF", "id": "Scene Flow", "label": "Scene Flow", "shape": "dot", "size": 25, "title": "Scene Flow"}, {"color": "#66CCFF", "id": "Brox, T. \u0026 Malik, J.", "label": "Brox, T. \u0026 Malik, J.", "shape": "dot", "size": 25, "title": "Brox, T. \u0026 Malik, J."}, {"color": "#66CCFF", "id": "Large Displacement Optical Flow", "label": "Large Displacement Optical Flow", "shape": "dot", "size": 25, "title": "Large Displacement Optical Flow"}, {"color": "#66CCFF", "id": "Variational Motion Estimation", "label": "Variational Motion Estimation", "shape": "dot", "size": 25, "title": "Variational Motion Estimation"}, {"color": "#66CCFF", "id": "Hirschmueller, H.", "label": "Hirschmueller, H.", "shape": "dot", "size": 25, "title": "Hirschmueller, H."}, {"color": "#66CCFF", "id": "Stereo Processing", "label": "Stereo Processing", "shape": "dot", "size": 25, "title": "Stereo Processing"}, {"color": "#66CCFF", "id": "Scene Flow Datasets", "label": "Scene Flow Datasets", "shape": "dot", "size": 25, "title": "Scene Flow Datasets"}, {"color": "#66CCFF", "id": "Quantitative Results", "label": "Quantitative Results", "shape": "dot", "size": 25, "title": "Quantitative Results"}, {"color": "#66CCFF", "id": "Qualitative Results", "label": "Qualitative Results", "shape": "dot", "size": 25, "title": "Qualitative Results"}, {"color": "#66CCFF", "id": "Stereo processing", "label": "Stereo processing", "shape": "dot", "size": 25, "title": "Stereo processing"}, {"color": "#66CCFF", "id": "Scene flow estimation", "label": "Scene flow estimation", "shape": "dot", "size": 25, "title": "Scene flow estimation"}, {"color": "#66CCFF", "id": "growing correspondence seeds", "label": "growing correspondence seeds", "shape": "dot", "size": 25, "title": "growing correspondence seeds"}, {"color": "#66CCFF", "id": "piecewise rigid scene flow", "label": "piecewise rigid scene flow", "shape": "dot", "size": 25, "title": "piecewise rigid scene flow"}, {"color": "#66CCFF", "id": "variational method", "label": "variational method", "shape": "dot", "size": 25, "title": "variational method"}, {"color": "#66CCFF", "id": "Cech et al. (2011)", "label": "Cech et al. (2011)", "shape": "dot", "size": 25, "title": "Cech et al. (2011)"}, {"color": "#66CCFF", "id": "Vogel et al. (2013)", "label": "Vogel et al. (2013)", "shape": "dot", "size": 25, "title": "Vogel et al. (2013)"}, {"color": "#66CCFF", "id": "Huguet \u0026 Devernay (2007)", "label": "Huguet \u0026 Devernay (2007)", "shape": "dot", "size": 25, "title": "Huguet \u0026 Devernay (2007)"}, {"color": "#66CCFF", "id": "Semiglobal matching", "label": "Semiglobal matching", "shape": "dot", "size": 25, "title": "Semiglobal matching"}, {"color": "#66CCFF", "id": "Mutual information", "label": "Mutual information", "shape": "dot", "size": 25, "title": "Mutual information"}, {"color": "#66CCFF", "id": "scene flow estimation", "label": "scene flow estimation", "shape": "dot", "size": 25, "title": "scene flow estimation"}, {"color": "#66CCFF", "id": "Sun, Roth, \u0026 Black (2013)", "label": "Sun, Roth, \u0026 Black (2013)", "shape": "dot", "size": 25, "title": "Sun, Roth, \u0026 Black (2013)"}, {"color": "#66CCFF", "id": "optical flow estimation", "label": "optical flow estimation", "shape": "dot", "size": 25, "title": "optical flow estimation"}, {"color": "#66CCFF", "id": "Hornacek, Fitzgibbon, \u0026 Rother (2014)", "label": "Hornacek, Fitzgibbon, \u0026 Rother (2014)", "shape": "dot", "size": 25, "title": "Hornacek, Fitzgibbon, \u0026 Rother (2014)"}, {"color": "#66CCFF", "id": "SphereFlow", "label": "SphereFlow", "shape": "dot", "size": 25, "title": "SphereFlow"}, {"color": "#66CCFF", "id": "Valgaerts et al. (2010)", "label": "Valgaerts et al. (2010)", "shape": "dot", "size": 25, "title": "Valgaerts et al. (2010)"}, {"color": "#66CCFF", "id": "motion and geometry estimation", "label": "motion and geometry estimation", "shape": "dot", "size": 25, "title": "motion and geometry estimation"}, {"color": "#66CCFF", "id": "stereo sequences", "label": "stereo sequences", "shape": "dot", "size": 25, "title": "stereo sequences"}, {"color": "#66CCFF", "id": "ICVV", "label": "ICVV", "shape": "dot", "size": 25, "title": "ICVV"}, {"color": "#66CCFF", "id": "Kert et al. (2010)", "label": "Kert et al. (2010)", "shape": "dot", "size": 25, "title": "Kert et al. (2010)"}, {"color": "#66CCFF", "id": "Wedel et al. (2008)", "label": "Wedel et al. (2008)", "shape": "dot", "size": 25, "title": "Wedel et al. (2008)"}, {"color": "#66CCFF", "id": "scene flow computation", "label": "scene flow computation", "shape": "dot", "size": 25, "title": "scene flow computation"}, {"color": "#66CCFF", "id": "Geiger et al. (2011)", "label": "Geiger et al. (2011)", "shape": "dot", "size": 25, "title": "Geiger et al. (2011)"}, {"color": "#66CCFF", "id": "3D reconstruction techniques", "label": "3D reconstruction techniques", "shape": "dot", "size": 25, "title": "3D reconstruction techniques"}, {"color": "#66CCFF", "id": "MPI Tubingen", "label": "MPI Tubingen", "shape": "dot", "size": 25, "title": "MPI Tubingen"}, {"color": "#66CCFF", "id": "Menz", "label": "Menz", "shape": "dot", "size": 25, "title": "Menz"}, {"color": "#66CCFF", "id": "Leibniz Universit\u00a8at Hannover", "label": "Leibniz Universit\u00a8at Hannover", "shape": "dot", "size": 25, "title": "Leibniz Universit\u00a8at Hannover"}, {"color": "#66CCFF", "id": "Tal Hassner", "label": "Tal Hassner", "shape": "dot", "size": 25, "title": "Tal Hassner"}, {"color": "#66CCFF", "id": "Effective Face Frontalization", "label": "Effective Face Frontalization", "shape": "dot", "size": 25, "title": "Effective Face Frontalization"}, {"color": "#66CCFF", "id": "Shai Harel", "label": "Shai Harel", "shape": "dot", "size": 25, "title": "Shai Harel"}, {"color": "#66CCFF", "id": "Eran Paz", "label": "Eran Paz", "shape": "dot", "size": 25, "title": "Eran Paz"}, {"color": "#66CCFF", "id": "Roee Enbar", "label": "Roee Enbar", "shape": "dot", "size": 25, "title": "Roee Enbar"}, {"color": "#66CCFF", "id": "Hassner_Effective_Face_Frontalization_2015_CVPR_paper.pdf", "label": "Hassner_Effective_Face_Frontalization_2015_CVPR_paper.pdf", "shape": "dot", "size": 25, "title": "Hassner_Effective_Face_Frontalization_2015_CVPR_paper.pdf"}, {"color": "#66CCFF", "id": "face recognition systems", "label": "face recognition systems", "shape": "dot", "size": 25, "title": "face recognition systems"}, {"color": "#66CCFF", "id": "unconstrained images", "label": "unconstrained images", "shape": "dot", "size": 25, "title": "unconstrained images"}, {"color": "#66CCFF", "id": "varying poses", "label": "varying poses", "shape": "dot", "size": 25, "title": "varying poses"}, {"color": "#66CCFF", "id": "varying expressions", "label": "varying expressions", "shape": "dot", "size": 25, "title": "varying expressions"}, {"color": "#66CCFF", "id": "varying lighting", "label": "varying lighting", "shape": "dot", "size": 25, "title": "varying lighting"}, {"color": "#66CCFF", "id": "Fronalization", "label": "Fronalization", "shape": "dot", "size": 25, "title": "Fronalization"}, {"color": "#66CCFF", "id": "problem of varying poses", "label": "problem of varying poses", "shape": "dot", "size": 25, "title": "problem of varying poses"}, {"color": "#66CCFF", "id": "problem of varying lighting", "label": "problem of varying lighting", "shape": "dot", "size": 25, "title": "problem of varying lighting"}, {"color": "#66CCFF", "id": "previous methods", "label": "previous methods", "shape": "dot", "size": 25, "title": "previous methods"}, {"color": "#66CCFF", "id": "estimating 3D facial shapes", "label": "estimating 3D facial shapes", "shape": "dot", "size": 25, "title": "estimating 3D facial shapes"}, {"color": "#66CCFF", "id": "this paper", "label": "this paper", "shape": "dot", "size": 25, "title": "this paper"}, {"color": "#66CCFF", "id": "simpler approach", "label": "simpler approach", "shape": "dot", "size": 25, "title": "simpler approach"}, {"color": "#66CCFF", "id": "single 3D surface", "label": "single 3D surface", "shape": "dot", "size": 25, "title": "single 3D surface"}, {"color": "#66CCFF", "id": "frontal views", "label": "frontal views", "shape": "dot", "size": 25, "title": "frontal views"}, {"color": "#66CCFF", "id": "aesthetically pleasing", "label": "aesthetically pleasing", "shape": "dot", "size": 25, "title": "aesthetically pleasing"}, {"color": "#66CCFF", "id": "gender estimation", "label": "gender estimation", "shape": "dot", "size": 25, "title": "gender estimation"}, {"color": "#66CCFF", "id": "Face frontalization", "label": "Face frontalization", "shape": "dot", "size": 25, "title": "Face frontalization"}, {"color": "#66CCFF", "id": "aesthetically pleasing frontal views", "label": "aesthetically pleasing frontal views", "shape": "dot", "size": 25, "title": "aesthetically pleasing frontal views"}, {"color": "#66CCFF", "id": "AI systems", "label": "AI systems", "shape": "dot", "size": 25, "title": "AI systems"}, {"color": "#66CCFF", "id": "factual question answering", "label": "factual question answering", "shape": "dot", "size": 25, "title": "factual question answering"}, {"color": "#66CCFF", "id": "common sense reasoning", "label": "common sense reasoning", "shape": "dot", "size": 25, "title": "common sense reasoning"}, {"color": "#66CCFF", "id": "visual common sense", "label": "visual common sense", "shape": "dot", "size": 25, "title": "visual common sense"}, {"color": "#66CCFF", "id": "semantic knowledge", "label": "semantic knowledge", "shape": "dot", "size": 25, "title": "semantic knowledge"}, {"color": "#66CCFF", "id": "visual cues", "label": "visual cues", "shape": "dot", "size": 25, "title": "visual cues"}, {"color": "#66CCFF", "id": "textual cues", "label": "textual cues", "shape": "dot", "size": 25, "title": "textual cues"}, {"color": "#66CCFF", "id": "benchmarks", "label": "benchmarks", "shape": "dot", "size": 25, "title": "benchmarks"}, {"color": "#66CCFF", "id": "progress", "label": "progress", "shape": "dot", "size": 25, "title": "progress"}, {"color": "#66CCFF", "id": "Xiao Lin", "label": "Xiao Lin", "shape": "dot", "size": 25, "title": "Xiao Lin"}, {"color": "#66CCFF", "id": "Visual Common Sense", "label": "Visual Common Sense", "shape": "dot", "size": 25, "title": "Visual Common Sense"}, {"color": "#66CCFF", "id": "AI Reasoning", "label": "AI Reasoning", "shape": "dot", "size": 25, "title": "AI Reasoning"}, {"color": "#66CCFF", "id": "Visual Paraphasing", "label": "Visual Paraphasing", "shape": "dot", "size": 25, "title": "Visual Paraphasing"}, {"color": "#66CCFF", "id": "linxiao@vt.edu", "label": "linxiao@vt.edu", "shape": "dot", "size": 25, "title": "linxiao@vt.edu"}, {"color": "#66CCFF", "id": "Hsu_Robust_Image_Alignment_2015_CVPR_paper.pdf", "label": "Hsu_Robust_Image_Alignment_2015_CVPR_paper.pdf", "shape": "dot", "size": 25, "title": "Hsu_Robust_Image_Alignment_2015_CVPR_paper.pdf"}, {"color": "#66CCFF", "id": "image alignment", "label": "image alignment", "shape": "dot", "size": 25, "title": "image alignment"}, {"color": "#66CCFF", "id": "complex system", "label": "complex system", "shape": "dot", "size": 25, "title": "complex system"}, {"color": "#66CCFF", "id": "process", "label": "process", "shape": "dot", "size": 25, "title": "process"}, {"color": "#66CCFF", "id": "data analysis", "label": "data analysis", "shape": "dot", "size": 25, "title": "data analysis"}, {"color": "#66CCFF", "id": "solutions", "label": "solutions", "shape": "dot", "size": 25, "title": "solutions"}, {"color": "#66CCFF", "id": "quantitative", "label": "quantitative", "shape": "dot", "size": 25, "title": "quantitative"}, {"color": "#66CCFF", "id": "relationships", "label": "relationships", "shape": "dot", "size": 25, "title": "relationships"}, {"color": "#66CCFF", "id": "dependencies", "label": "dependencies", "shape": "dot", "size": 25, "title": "dependencies"}, {"color": "#66CCFF", "id": "Data Analysis", "label": "Data Analysis", "shape": "dot", "size": 25, "title": "Data Analysis"}, {"color": "#66CCFF", "id": "Rock_Completing_3D_Object_2015_CVPR_paper.pdf", "label": "Rock_Completing_3D_Object_2015_CVPR_paper.pdf", "shape": "dot", "size": 25, "title": "Rock_Completing_3D_Object_2015_CVPR_paper.pdf"}, {"color": "#66CCFF", "id": "Optimization", "label": "Optimization", "shape": "dot", "size": 25, "title": "Optimization"}, {"color": "#66CCFF", "id": "System Dynamics", "label": "System Dynamics", "shape": "dot", "size": 25, "title": "System Dynamics"}, {"color": "#66CCFF", "id": "Parameter Estimation", "label": "Parameter Estimation", "shape": "dot", "size": 25, "title": "Parameter Estimation"}, {"color": "#66CCFF", "id": "Relationship Modeling", "label": "Relationship Modeling", "shape": "dot", "size": 25, "title": "Relationship Modeling"}, {"color": "#66CCFF", "id": "Rock_Compleeting_3D_Object_2015_CVPR_paper.pdf", "label": "Rock_Compleeting_3D_Object_2015_CVPR_paper.pdf", "shape": "dot", "size": 25, "title": "Rock_Compleeting_3D_Object_2015_CVPR_paper.pdf"}, {"color": "#66CCFF", "id": "Jason Rock", "label": "Jason Rock", "shape": "dot", "size": 25, "title": "Jason Rock"}, {"color": "#66CCFF", "id": "Justin Thorsten", "label": "Justin Thorsten", "shape": "dot", "size": 25, "title": "Justin Thorsten"}, {"color": "#66CCFF", "id": "JunYoung Gwak", "label": "JunYoung Gwak", "shape": "dot", "size": 25, "title": "JunYoung Gwak"}, {"color": "#66CCFF", "id": "Daeyun Shin", "label": "Daeyun Shin", "shape": "dot", "size": 25, "title": "Daeyun Shin"}, {"color": "#66CCFF", "id": "problem of recovering a complete 3D model", "label": "problem of recovering a complete 3D model", "shape": "dot", "size": 25, "title": "problem of recovering a complete 3D model"}, {"color": "#66CCFF", "id": "user interaction", "label": "user interaction", "shape": "dot", "size": 25, "title": "user interaction"}, {"color": "#66CCFF", "id": "similar 3D models", "label": "similar 3D models", "shape": "dot", "size": 25, "title": "similar 3D models"}, {"color": "#66CCFF", "id": "symmetries", "label": "symmetries", "shape": "dot", "size": 25, "title": "symmetries"}, {"color": "#66CCFF", "id": "reconstruct a 3D model automatically", "label": "reconstruct a 3D model automatically", "shape": "dot", "size": 25, "title": "reconstruct a 3D model automatically"}, {"color": "#66CCFF", "id": "viewpoint-based shape matching", "label": "viewpoint-based shape matching", "shape": "dot", "size": 25, "title": "viewpoint-based shape matching"}, {"color": "#66CCFF", "id": "3D deformation", "label": "3D deformation", "shape": "dot", "size": 25, "title": "3D deformation"}, {"color": "#66CCFF", "id": "3D mesh analysis", "label": "3D mesh analysis", "shape": "dot", "size": 25, "title": "3D mesh analysis"}, {"color": "#66CCFF", "id": "3D model synthesis", "label": "3D model synthesis", "shape": "dot", "size": 25, "title": "3D model synthesis"}, {"color": "#66CCFF", "id": "3D Shape Reconstruction", "label": "3D Shape Reconstruction", "shape": "dot", "size": 25, "title": "3D Shape Reconstruction"}, {"color": "#66CCFF", "id": "Tanmay Gupta", "label": "Tanmay Gupta", "shape": "dot", "size": 25, "title": "Tanmay Gupta"}, {"color": "#66CCFF", "id": "View-Based Matching", "label": "View-Based Matching", "shape": "dot", "size": 25, "title": "View-Based Matching"}, {"color": "#66CCFF", "id": "Shape Completion", "label": "Shape Completion", "shape": "dot", "size": 25, "title": "Shape Completion"}, {"color": "#66CCFF", "id": "3D Model Synthesis", "label": "3D Model Synthesis", "shape": "dot", "size": 25, "title": "3D Model Synthesis"}, {"color": "#66CCFF", "id": "Symmetry Transfer", "label": "Symmetry Transfer", "shape": "dot", "size": 25, "title": "Symmetry Transfer"}, {"color": "#66CCFF", "id": "Liu_Data-Driven_Sparsity-Based_Restoration_2015_CVPR_paper.pdf", "label": "Liu_Data-Driven_Sparsity-Based_Restoration_2015_CVPR_paper.pdf", "shape": "dot", "size": 25, "title": "Liu_Data-Driven_Sparsity-Based_Restoration_2015_CVPR_paper.pdf"}, {"color": "#66CCFF", "id": "Rui Caseiro", "label": "Rui Caseiro", "shape": "dot", "size": 25, "title": "Rui Caseiro"}, {"color": "#66CCFF", "id": "Beyond the Shortest Path", "label": "Beyond the Shortest Path", "shape": "dot", "size": 25, "title": "Beyond the Shortest Path"}, {"color": "#66CCFF", "id": "Pedro Martins", "label": "Pedro Martins", "shape": "dot", "size": 25, "title": "Pedro Martins"}, {"color": "#66CCFF", "id": "Jorge Batista", "label": "Jorge Batista", "shape": "dot", "size": 25, "title": "Jorge Batista"}, {"color": "#66CCFF", "id": "domain adaptation", "label": "domain adaptation", "shape": "dot", "size": 25, "title": "domain adaptation"}, {"color": "#66CCFF", "id": "spline flow", "label": "spline flow", "shape": "dot", "size": 25, "title": "spline flow"}, {"color": "#66CCFF", "id": "improve_performance", "label": "improve_performance", "shape": "dot", "size": 25, "title": "improve_performance"}, {"color": "#66CCFF", "id": "domain adaptation paradigm", "label": "domain adaptation paradigm", "shape": "dot", "size": 25, "title": "domain adaptation paradigm"}, {"color": "#66CCFF", "id": "shortest path", "label": "shortest path", "shape": "dot", "size": 25, "title": "shortest path"}, {"color": "#66CCFF", "id": "geodesic curve", "label": "geodesic curve", "shape": "dot", "size": 25, "title": "geodesic curve"}, {"color": "#66CCFF", "id": "modeling complex domain shifts", "label": "modeling complex domain shifts", "shape": "dot", "size": 25, "title": "modeling complex domain shifts"}, {"color": "#66CCFF", "id": "use of multiple datasets", "label": "use of multiple datasets", "shape": "dot", "size": 25, "title": "use of multiple datasets"}, {"color": "#66CCFF", "id": "novel approach", "label": "novel approach", "shape": "dot", "size": 25, "title": "novel approach"}, {"color": "#66CCFF", "id": "spline curves", "label": "spline curves", "shape": "dot", "size": 25, "title": "spline curves"}, {"color": "#66CCFF", "id": "rolling maps", "label": "rolling maps", "shape": "dot", "size": 25, "title": "rolling maps"}, {"color": "#66CCFF", "id": "integration of multiple source domains", "label": "integration of multiple source domains", "shape": "dot", "size": 25, "title": "integration of multiple source domains"}, {"color": "#66CCFF", "id": "domain shifts", "label": "domain shifts", "shape": "dot", "size": 25, "title": "domain shifts"}, {"color": "#66CCFF", "id": "improved performance", "label": "improved performance", "shape": "dot", "size": 25, "title": "improved performance"}, {"color": "#66CCFF", "id": "Domain Adaptation", "label": "Domain Adaptation", "shape": "dot", "size": 25, "title": "Domain Adaptation"}, {"color": "#66CCFF", "id": "Subspace Representation", "label": "Subspace Representation", "shape": "dot", "size": 25, "title": "Subspace Representation"}, {"color": "#66CCFF", "id": "Baktas et al. (2013)", "label": "Baktas et al. (2013)", "shape": "dot", "size": 25, "title": "Baktas et al. (2013)"}, {"color": "#66CCFF", "id": "Domain Invariant Projection", "label": "Domain Invariant Projection", "shape": "dot", "size": 25, "title": "Domain Invariant Projection"}, {"color": "#66CCFF", "id": "Gopalan et al. (2013)", "label": "Gopalan et al. (2013)", "shape": "dot", "size": 25, "title": "Gopalan et al. (2013)"}, {"color": "#66CCFF", "id": "location recognition", "label": "location recognition", "shape": "dot", "size": 25, "title": "location recognition"}, {"color": "#66CCFF", "id": "Gopalan et al. (2011)", "label": "Gopalan et al. (2011)", "shape": "dot", "size": 25, "title": "Gopalan et al. (2011)"}, {"color": "#66CCFF", "id": "Unsupervised approach", "label": "Unsupervised approach", "shape": "dot", "size": 25, "title": "Unsupervised approach"}, {"color": "#66CCFF", "id": "Carreira et al. (2012)", "label": "Carreira et al. (2012)", "shape": "dot", "size": 25, "title": "Carreira et al. (2012)"}, {"color": "#66CCFF", "id": "Caseiro et al. (2010)", "label": "Caseiro et al. (2010)", "shape": "dot", "size": 25, "title": "Caseiro et al. (2010)"}, {"color": "#66CCFF", "id": "cast shadows", "label": "cast shadows", "shape": "dot", "size": 25, "title": "cast shadows"}, {"color": "#66CCFF", "id": "Gopalan et al. (2014)", "label": "Gopalan et al. (2014)", "shape": "dot", "size": 25, "title": "Gopalan et al. (2014)"}, {"color": "#66CCFF", "id": "intermediate data representations", "label": "intermediate data representations", "shape": "dot", "size": 25, "title": "intermediate data representations"}, {"color": "#66CCFF", "id": "Spline Flow", "label": "Spline Flow", "shape": "dot", "size": 25, "title": "Spline Flow"}, {"color": "#66CCFF", "id": "Grasmannn Manifold", "label": "Grasmannn Manifold", "shape": "dot", "size": 25, "title": "Grasmannn Manifold"}, {"color": "#66CCFF", "id": "Rolling Maps", "label": "Rolling Maps", "shape": "dot", "size": 25, "title": "Rolling Maps"}, {"color": "#66CCFF", "id": "Gopalan", "label": "Gopalan", "shape": "dot", "size": 25, "title": "Gopalan"}, {"color": "#66CCFF", "id": "Unsupervised adaptation across domain shifts by generating intermediate data representations", "label": "Unsupervised adaptation across domain shifts by generating intermediate data representations", "shape": "dot", "size": 25, "title": "Unsupervised adaptation across domain shifts by generating intermediate data representations"}, {"color": "#66CCFF", "id": "Li", "label": "Li", "shape": "dot", "size": 25, "title": "Li"}, {"color": "#66CCFF", "id": "Griffin", "label": "Griffin", "shape": "dot", "size": 25, "title": "Griffin"}, {"color": "#66CCFF", "id": "Caltech-256 object category dataset", "label": "Caltech-256 object category dataset", "shape": "dot", "size": 25, "title": "Caltech-256 object category dataset"}, {"color": "#66CCFF", "id": "Holub", "label": "Holub", "shape": "dot", "size": 25, "title": "Holub"}, {"color": "#66CCFF", "id": "Cal tech-256 object category dataset", "label": "Cal tech-256 object category dataset", "shape": "dot", "size": 25, "title": "Cal tech-256 object category dataset"}, {"color": "#66CCFF", "id": "Caseiro", "label": "Caseiro", "shape": "dot", "size": 25, "title": "Caseiro"}, {"color": "#66CCFF", "id": "non-parametric riemannian framework", "label": "non-parametric riemannian framework", "shape": "dot", "size": 25, "title": "non-parametric riemannian framework"}, {"color": "#66CCFF", "id": "Henriques", "label": "Henriques", "shape": "dot", "size": 25, "title": "Henriques"}, {"color": "#66CCFF", "id": "Martins", "label": "Martins", "shape": "dot", "size": 25, "title": "Martins"}, {"color": "#66CCFF", "id": "Batista", "label": "Batista", "shape": "dot", "size": 25, "title": "Batista"}, {"color": "#66CCFF", "id": "Hays", "label": "Hays", "shape": "dot", "size": 25, "title": "Hays"}, {"color": "#66CCFF", "id": "Im2gps", "label": "Im2gps", "shape": "dot", "size": 25, "title": "Im2gps"}, {"color": "#66CCFF", "id": "Efroos", "label": "Efroos", "shape": "dot", "size": 25, "title": "Efroos"}, {"color": "#66CCFF", "id": "Pan", "label": "Pan", "shape": "dot", "size": 25, "title": "Pan"}, {"color": "#66CCFF", "id": "A survey on transfer learning", "label": "A survey on transfer learning", "shape": "dot", "size": 25, "title": "A survey on transfer learning"}, {"color": "#66CCFF", "id": "Institute of Systems and Robotics - University of Coimbra", "label": "Institute of Systems and Robotics - University of Coimbra", "shape": "dot", "size": 25, "title": "Institute of Systems and Robotics - University of Coimbra"}, {"color": "#66CCFF", "id": "Jo\u00e3o F. Henriques", "label": "Jo\u00e3o F. Henriques", "shape": "dot", "size": 25, "title": "Jo\u00e3o F. Henriques"}, {"color": "#66CCFF", "id": "Institute of Sistemas and Robotics - University of Coimbra", "label": "Institute of Sistemas and Robotics - University of Coimbra", "shape": "dot", "size": 25, "title": "Institute of Sistemas and Robotics - University of Coimbra"}, {"color": "#66CCFF", "id": "Sparse Composite Quantization", "label": "Sparse Composite Quantization", "shape": "dot", "size": 25, "title": "Sparse Composite Quantization"}, {"color": "#66CCFF", "id": "Zhang_Sparse_Composite_Quantization_2015_CVPR_paper", "label": "Zhang_Sparse_Composite_Quantization_2015_CVPR_paper", "shape": "dot", "size": 25, "title": "Zhang_Sparse_Composite_Quantization_2015_CVPR_paper"}, {"color": "#66CCFF", "id": "quantization techniques", "label": "quantization techniques", "shape": "dot", "size": 25, "title": "quantization techniques"}, {"color": "#66CCFF", "id": "sparse composite quantization", "label": "sparse composite quantization", "shape": "dot", "size": 25, "title": "sparse composite quantization"}, {"color": "#66CCFF", "id": "competitive search accuracy", "label": "competitive search accuracy", "shape": "dot", "size": 25, "title": "competitive search accuracy"}, {"color": "#66CCFF", "id": "Cartesian k-means", "label": "Cartesian k-means", "shape": "dot", "size": 25, "title": "Cartesian k-means"}, {"color": "#66CCFF", "id": "composite quantization", "label": "composite quantization", "shape": "dot", "size": 25, "title": "composite quantization"}, {"color": "#66CCFF", "id": "distance table computation", "label": "distance table computation", "shape": "dot", "size": 25, "title": "distance table computation"}, {"color": "#66CCFF", "id": "sparse dictionaries", "label": "sparse dictionaries", "shape": "dot", "size": 25, "title": "sparse dictionaries"}, {"color": "#66CCFF", "id": "distance evaluation", "label": "distance evaluation", "shape": "dot", "size": 25, "title": "distance evaluation"}, {"color": "#66CCFF", "id": "distance table computation time", "label": "distance table computation time", "shape": "dot", "size": 25, "title": "distance table computation time"}, {"color": "#66CCFF", "id": "large-scale datasets", "label": "large-scale datasets", "shape": "dot", "size": 25, "title": "large-scale datasets"}, {"color": "#66CCFF", "id": "SIFTs", "label": "SIFTs", "shape": "dot", "size": 25, "title": "SIFTs"}, {"color": "#66CCFF", "id": "time", "label": "time", "shape": "dot", "size": 25, "title": "time"}, {"color": "#66CCFF", "id": "1M", "label": "1M", "shape": "dot", "size": 25, "title": "1M"}, {"color": "#66CCFF", "id": "1B", "label": "1B", "shape": "dot", "size": 25, "title": "1B"}, {"color": "#66CCFF", "id": "comparable", "label": "comparable", "shape": "dot", "size": 25, "title": "comparable"}, {"color": "#66CCFF", "id": "search times", "label": "search times", "shape": "dot", "size": 25, "title": "search times"}, {"color": "#66CCFF", "id": "faster", "label": "faster", "shape": "dot", "size": 25, "title": "faster"}, {"color": "#66CCFF", "id": "SIFTS", "label": "SIFTS", "shape": "dot", "size": 25, "title": "SIFTS"}, {"color": "#66CCFF", "id": "ANN", "label": "ANN", "shape": "dot", "size": 25, "title": "ANN"}, {"color": "#66CCFF", "id": "nearest neighbor search", "label": "nearest neighbor search", "shape": "dot", "size": 25, "title": "nearest neighbor search"}, {"color": "#66CCFF", "id": "Product Quantization", "label": "Product Quantization", "shape": "dot", "size": 25, "title": "Product Quantization"}, {"color": "#66CCFF", "id": "for nearest neighbor search", "label": "for nearest neighbor search", "shape": "dot", "size": 25, "title": "for nearest neighbor search"}, {"color": "#66CCFF", "id": "Babenko and Lempitsky (2012)", "label": "Babenko and Lempitsky (2012)", "shape": "dot", "size": 25, "title": "Babenko and Lempitsky (2012)"}, {"color": "#66CCFF", "id": "inverted multi-index", "label": "inverted multi-index", "shape": "dot", "size": 25, "title": "inverted multi-index"}, {"color": "#66CCFF", "id": "Babenko and Lempitsky (2014)", "label": "Babenko and Lempitsky (2014)", "shape": "dot", "size": 25, "title": "Babenko and Lempitsky (2014)"}, {"color": "#66CCFF", "id": "bilayer product quantization", "label": "bilayer product quantization", "shape": "dot", "size": 25, "title": "bilayer product quantization"}, {"color": "#66CCFF", "id": "billion-scale approximate nearest neighbors", "label": "billion-scale approximate nearest neighbors", "shape": "dot", "size": 25, "title": "billion-scale approximate nearest neighbors"}, {"color": "#66CCFF", "id": "High-Dimensional Data", "label": "High-Dimensional Data", "shape": "dot", "size": 25, "title": "High-Dimensional Data"}, {"color": "#66CCFF", "id": "approximate nearest neighbors", "label": "approximate nearest neighbors", "shape": "dot", "size": 25, "title": "approximate nearest neighbors"}, {"color": "#66CCFF", "id": "variation", "label": "variation", "shape": "dot", "size": 25, "title": "variation"}, {"color": "#66CCFF", "id": "vocabulary trees", "label": "vocabulary trees", "shape": "dot", "size": 25, "title": "vocabulary trees"}, {"color": "#66CCFF", "id": "Hamming embedding", "label": "Hamming embedding", "shape": "dot", "size": 25, "title": "Hamming embedding"}, {"color": "#66CCFF", "id": "large scale image search", "label": "large scale image search", "shape": "dot", "size": 25, "title": "large scale image search"}, {"color": "#66CCFF", "id": "semi-supervised hashing", "label": "semi-supervised hashing", "shape": "dot", "size": 25, "title": "semi-supervised hashing"}, {"color": "#66CCFF", "id": "large-scale search", "label": "large-scale search", "shape": "dot", "size": 25, "title": "large-scale search"}, {"color": "#66CCFF", "id": "large-scale applications", "label": "large-scale applications", "shape": "dot", "size": 25, "title": "large-scale applications"}, {"color": "#66CCFF", "id": "CVPR (2013)", "label": "CVPR (2013)", "shape": "dot", "size": 25, "title": "CVPR (2013)"}, {"color": "#66CCFF", "id": "Re-ranking strategies", "label": "Re-ranking strategies", "shape": "dot", "size": 25, "title": "Re-ranking strategies"}, {"color": "#66CCFF", "id": "Graph-based search methods", "label": "Graph-based search methods", "shape": "dot", "size": 25, "title": "Graph-based search methods"}, {"color": "#66CCFF", "id": "search", "label": "search", "shape": "dot", "size": 25, "title": "search"}, {"color": "#66CCFF", "id": "Angular quantization", "label": "Angular quantization", "shape": "dot", "size": 25, "title": "Angular quantization"}, {"color": "#66CCFF", "id": "Guo-Jun Qi", "label": "Guo-Jun Qi", "shape": "dot", "size": 25, "title": "Guo-Jun Qi"}, {"color": "#66CCFF", "id": "Jinhui Tang", "label": "Jinhui Tang", "shape": "dot", "size": 25, "title": "Jinhui Tang"}, {"color": "#66CCFF", "id": "Unknown", "label": "Unknown", "shape": "dot", "size": 25, "title": "Unknown"}, {"color": "#66CCFF", "id": "Ting Zhang", "label": "Ting Zhang", "shape": "dot", "size": 25, "title": "Ting Zhang"}, {"color": "#66CCFF", "id": "Nanjing University of Science and Technology", "label": "Nanjing University of Science and Technology", "shape": "dot", "size": 25, "title": "Nanjing University of Science and Technology"}, {"color": "#66CCFF", "id": "Jingding Wang", "label": "Jingding Wang", "shape": "dot", "size": 25, "title": "Jingding Wang"}, {"color": "#66CCFF", "id": "Ioannis Gkioulekalas", "label": "Ioannis Gkioulekalas", "shape": "dot", "size": 25, "title": "Ioannis Gkioulekalas"}, {"color": "#66CCFF", "id": "On the Appearance of Translueceny Edges", "label": "On the Appearance of Translueceny Edges", "shape": "dot", "size": 25, "title": "On the Appearance of Translueceny Edges"}, {"color": "#66CCFF", "id": "On the Appearence of Translueceny Edges", "label": "On the Appearence of Translueceny Edges", "shape": "dot", "size": 25, "title": "On the Appearence of Translueceny Edges"}, {"color": "#66CCFF", "id": "Gkiooulekas_On_the_Appearance_2015_CVPR_paper.pdf", "label": "Gkiooulekas_On_the_Appearance_2015_CVPR_paper.pdf", "shape": "dot", "size": 25, "title": "Gkiooulekas_On_the_Appearance_2015_CVPR_paper.pdf"}, {"color": "#66CCFF", "id": "Edges in images", "label": "Edges in images", "shape": "dot", "size": 25, "title": "Edges in images"}, {"color": "#66CCFF", "id": "Edges in opaque objects", "label": "Edges in opaque objects", "shape": "dot", "size": 25, "title": "Edges in opaque objects"}, {"color": "#66CCFF", "id": "Edges", "label": "Edges", "shape": "dot", "size": 25, "title": "Edges"}, {"color": "#66CCFF", "id": "Discontinuity in surface orientation", "label": "Discontinuity in surface orientation", "shape": "dot", "size": 25, "title": "Discontinuity in surface orientation"}, {"color": "#66CCFF", "id": "Authors", "label": "Authors", "shape": "dot", "size": 25, "title": "Authors"}, {"color": "#66CCFF", "id": "Edge patterns", "label": "Edge patterns", "shape": "dot", "size": 25, "title": "Edge patterns"}, {"color": "#66CCFF", "id": "Simulations", "label": "Simulations", "shape": "dot", "size": 25, "title": "Simulations"}, {"color": "#66CCFF", "id": "Scattering parameters", "label": "Scattering parameters", "shape": "dot", "size": 25, "title": "Scattering parameters"}, {"color": "#66CCFF", "id": "Material Metamers", "label": "Material Metamers", "shape": "dot", "size": 25, "title": "Material Metamers"}, {"color": "#66CCFF", "id": "Visual Inference tasks", "label": "Visual Inference tasks", "shape": "dot", "size": 25, "title": "Visual Inference tasks"}, {"color": "#66CCFF", "id": "Shape estimation", "label": "Shape estimation", "shape": "dot", "size": 25, "title": "Shape estimation"}, {"color": "#66CCFF", "id": "Material estimation", "label": "Material estimation", "shape": "dot", "size": 25, "title": "Material estimation"}, {"color": "#66CCFF", "id": "Light Transport", "label": "Light Transport", "shape": "dot", "size": 25, "title": "Light Transport"}, {"color": "#66CCFF", "id": "Wave Propagation", "label": "Wave Propagation", "shape": "dot", "size": 25, "title": "Wave Propagation"}, {"color": "#66CCFF", "id": "Ishimaru (1978)", "label": "Ishimaru (1978)", "shape": "dot", "size": 25, "title": "Ishimaru (1978)"}, {"color": "#66CCFF", "id": "Stereo Reconstruction", "label": "Stereo Reconstruction", "shape": "dot", "size": 25, "title": "Stereo Reconstruction"}, {"color": "#66CCFF", "id": "Human Perception", "label": "Human Perception", "shape": "dot", "size": 25, "title": "Human Perception"}, {"color": "#66CCFF", "id": "Adelson (2001)", "label": "Adelson (2001)", "shape": "dot", "size": 25, "title": "Adelson (2001)"}, {"color": "#66CCFF", "id": "Machine Vision", "label": "Machine Vision", "shape": "dot", "size": 25, "title": "Machine Vision"}, {"color": "#66CCFF", "id": "Jensen et al. (2001)", "label": "Jensen et al. (2001)", "shape": "dot", "size": 25, "title": "Jensen et al. (2001)"}, {"color": "#66CCFF", "id": "Rendering", "label": "Rendering", "shape": "dot", "size": 25, "title": "Rendering"}, {"color": "#66CCFF", "id": "Reconstruction", "label": "Reconstruction", "shape": "dot", "size": 25, "title": "Reconstruction"}, {"color": "#66CCFF", "id": "Translucent Objects", "label": "Translucent Objects", "shape": "dot", "size": 25, "title": "Translucent Objects"}, {"color": "#66CCFF", "id": "Material Estimation", "label": "Material Estimation", "shape": "dot", "size": 25, "title": "Material Estimation"}, {"color": "#66CCFF", "id": "Material Metameters", "label": "Material Metameters", "shape": "dot", "size": 25, "title": "Material Metameters"}, {"color": "#66CCFF", "id": "light transport complexities", "label": "light transport complexities", "shape": "dot", "size": 25, "title": "light transport complexities"}, {"color": "#66CCFF", "id": "light transport", "label": "light transport", "shape": "dot", "size": 25, "title": "light transport"}, {"color": "#66CCFF", "id": "translucient materials", "label": "translucient materials", "shape": "dot", "size": 25, "title": "translucient materials"}, {"color": "#66CCFF", "id": "rendering", "label": "rendering", "shape": "dot", "size": 25, "title": "rendering"}, {"color": "#66CCFF", "id": "accurate light transport", "label": "accurate light transport", "shape": "dot", "size": 25, "title": "accurate light transport"}, {"color": "#66CCFF", "id": "artifacts", "label": "artifacts", "shape": "dot", "size": 25, "title": "artifacts"}, {"color": "#66CCFF", "id": "translucent materials", "label": "translucent materials", "shape": "dot", "size": 25, "title": "translucent materials"}, {"color": "#66CCFF", "id": "translucent appearance", "label": "translucent appearance", "shape": "dot", "size": 25, "title": "translucent appearance"}, {"color": "#66CCFF", "id": "phase functions", "label": "phase functions", "shape": "dot", "size": 25, "title": "phase functions"}, {"color": "#66CCFF", "id": "reconstruction", "label": "reconstruction", "shape": "dot", "size": 25, "title": "reconstruction"}, {"color": "#66CCFF", "id": "accurate rendering", "label": "accurate rendering", "shape": "dot", "size": 25, "title": "accurate rendering"}, {"color": "#66CCFF", "id": "translucency", "label": "translucency", "shape": "dot", "size": 25, "title": "translucency"}, {"color": "#66CCFF", "id": "phase function", "label": "phase function", "shape": "dot", "size": 25, "title": "phase function"}, {"color": "#66CCFF", "id": "photon diffusion", "label": "photon diffusion", "shape": "dot", "size": 25, "title": "photon diffusion"}, {"color": "#66CCFF", "id": "Open-surfaces catalog", "label": "Open-surfaces catalog", "shape": "dot", "size": 25, "title": "Open-surfaces catalog"}, {"color": "#66CCFF", "id": "surface appearances", "label": "surface appearances", "shape": "dot", "size": 25, "title": "surface appearances"}, {"color": "#66CCFF", "id": "evaluation", "label": "evaluation", "shape": "dot", "size": 25, "title": "evaluation"}, {"color": "#66CCFF", "id": "ACM Transactions on Graphics", "label": "ACM Transactions on Graphics", "shape": "dot", "size": 25, "title": "ACM Transactions on Graphics"}, {"color": "#66CCFF", "id": "research on phase function", "label": "research on phase function", "shape": "dot", "size": 25, "title": "research on phase function"}, {"color": "#66CCFF", "id": "Journal of Vision", "label": "Journal of Vision", "shape": "dot", "size": 25, "title": "Journal of Vision"}, {"color": "#66CCFF", "id": "research on translucency", "label": "research on translucency", "shape": "dot", "size": 25, "title": "research on translucency"}, {"color": "#66CCFF", "id": "ACM SIGGRAPH", "label": "ACM SIGGRAPH", "shape": "dot", "size": 25, "title": "ACM SIGGRAPH"}, {"color": "#66CCFF", "id": "research on rendering techniques", "label": "research on rendering techniques", "shape": "dot", "size": 25, "title": "research on rendering techniques"}, {"color": "#66CCFF", "id": "Bala. Open-surfaces", "label": "Bala. Open-surfaces", "shape": "dot", "size": 25, "title": "Bala. Open-surfaces"}, {"color": "#66CCFF", "id": "dataset of surface appearances", "label": "dataset of surface appearances", "shape": "dot", "size": 25, "title": "dataset of surface appearances"}, {"color": "#66CCFF", "id": "Gkiouslekas et al.", "label": "Gkiouslekas et al.", "shape": "dot", "size": 25, "title": "Gkiouslekas et al."}, {"color": "#66CCFF", "id": "phase function in translucent appearance", "label": "phase function in translucent appearance", "shape": "dot", "size": 25, "title": "phase function in translucent appearance"}, {"color": "#66CCFF", "id": "materials in context database", "label": "materials in context database", "shape": "dot", "size": 25, "title": "materials in context database"}, {"color": "#66CCFF", "id": "translucent objects", "label": "translucent objects", "shape": "dot", "size": 25, "title": "translucent objects"}, {"color": "#66CCFF", "id": "Bell et al.", "label": "Bell et al.", "shape": "dot", "size": 25, "title": "Bell et al."}, {"color": "#66CCFF", "id": "Mingkui Tan", "label": "Mingkui Tan", "shape": "dot", "size": 25, "title": "Mingkui Tan"}, {"color": "#66CCFF", "id": "Learning graph structure...", "label": "Learning graph structure...", "shape": "dot", "size": 25, "title": "Learning graph structure..."}, {"color": "#66CCFF", "id": "Qinfeng Shi", "label": "Qinfeng Shi", "shape": "dot", "size": 25, "title": "Qinfeng Shi"}, {"color": "#66CCFF", "id": "Fuyuan Hu", "label": "Fuyuan Hu", "shape": "dot", "size": 25, "title": "Fuyuan Hu"}, {"color": "#66CCFF", "id": "Zhen Zhang", "label": "Zhen Zhang", "shape": "dot", "size": 25, "title": "Zhen Zhang"}, {"color": "#66CCFF", "id": "Tan_Learning_Graph_Structure_2015_CVPR_paper.pdf", "label": "Tan_Learning_Graph_Structure_2015_CVPR_paper.pdf", "shape": "dot", "size": 25, "title": "Tan_Learning_Graph_Structure_2015_CVPR_paper.pdf"}, {"color": "#66CCFF", "id": "Multi-label image classification", "label": "Multi-label image classification", "shape": "dot", "size": 25, "title": "Multi-label image classification"}, {"color": "#66CCFF", "id": "Classification performance", "label": "Classification performance", "shape": "dot", "size": 25, "title": "Classification performance"}, {"color": "#66CCFF", "id": "Probabilistic Graphical Models", "label": "Probabilistic Graphical Models", "shape": "dot", "size": 25, "title": "Probabilistic Graphical Models"}, {"color": "#66CCFF", "id": "Label dependency", "label": "Label dependency", "shape": "dot", "size": 25, "title": "Label dependency"}, {"color": "#66CCFF", "id": "Graphical model structure", "label": "Graphical model structure", "shape": "dot", "size": 25, "title": "Graphical model structure"}, {"color": "#66CCFF", "id": "Heuristic methods", "label": "Heuristic methods", "shape": "dot", "size": 25, "title": "Heuristic methods"}, {"color": "#66CCFF", "id": "Limited information", "label": "Limited information", "shape": "dot", "size": 25, "title": "Limited information"}, {"color": "#66CCFF", "id": "Input features", "label": "Input features", "shape": "dot", "size": 25, "title": "Input features"}, {"color": "#66CCFF", "id": "Labels", "label": "Labels", "shape": "dot", "size": 25, "title": "Labels"}, {"color": "#66CCFF", "id": "Problem", "label": "Problem", "shape": "dot", "size": 25, "title": "Problem"}, {"color": "#66CCFF", "id": "Max-margin framework", "label": "Max-margin framework", "shape": "dot", "size": 25, "title": "Max-margin framework"}, {"color": "#66CCFF", "id": "Convex programming problem", "label": "Convex programming problem", "shape": "dot", "size": 25, "title": "Convex programming problem"}, {"color": "#66CCFF", "id": "Procedure", "label": "Procedure", "shape": "dot", "size": 25, "title": "Procedure"}, {"color": "#66CCFF", "id": "Set of cliques", "label": "Set of cliques", "shape": "dot", "size": 25, "title": "Set of cliques"}, {"color": "#66CCFF", "id": "Strong theoretical properties", "label": "Strong theoretical properties", "shape": "dot", "size": 25, "title": "Strong theoretical properties"}, {"color": "#66CCFF", "id": "Performance", "label": "Performance", "shape": "dot", "size": 25, "title": "Performance"}, {"color": "#66CCFF", "id": "set of cliques", "label": "set of cliques", "shape": "dot", "size": 25, "title": "set of cliques"}, {"color": "#66CCFF", "id": "theoretical properties", "label": "theoretical properties", "shape": "dot", "size": 25, "title": "theoretical properties"}, {"color": "#66CCFF", "id": "synthetic data sets", "label": "synthetic data sets", "shape": "dot", "size": 25, "title": "synthetic data sets"}, {"color": "#66CCFF", "id": "cliques", "label": "cliques", "shape": "dot", "size": 25, "title": "cliques"}, {"color": "#66CCFF", "id": "real-world data sets", "label": "real-world data sets", "shape": "dot", "size": 25, "title": "real-world data sets"}, {"color": "#66CCFF", "id": "performance improvement", "label": "performance improvement", "shape": "dot", "size": 25, "title": "performance improvement"}, {"color": "#66CCFF", "id": "Probablistic Graphical Models", "label": "Probablistic Graphical Models", "shape": "dot", "size": 25, "title": "Probablistic Graphical Models"}, {"color": "#66CCFF", "id": "Graph structure learning", "label": "Graph structure learning", "shape": "dot", "size": 25, "title": "Graph structure learning"}, {"color": "#66CCFF", "id": "Clique generation", "label": "Clique generation", "shape": "dot", "size": 25, "title": "Clique generation"}, {"color": "#66CCFF", "id": "learning", "label": "learning", "shape": "dot", "size": 25, "title": "learning"}, {"color": "#66CCFF", "id": "Boutell et al. (2004)", "label": "Boutell et al. (2004)", "shape": "dot", "size": 25, "title": "Boutell et al. (2004)"}, {"color": "#66CCFF", "id": "Bradley \u0026 Guestrin (2010)", "label": "Bradley \u0026 Guestrin (2010)", "shape": "dot", "size": 25, "title": "Bradley \u0026 Guestrin (2010)"}, {"color": "#66CCFF", "id": "tree conditional random fields", "label": "tree conditional random fields", "shape": "dot", "size": 25, "title": "tree conditional random fields"}, {"color": "#66CCFF", "id": "Bucak et al. (2009)", "label": "Bucak et al. (2009)", "shape": "dot", "size": 25, "title": "Bucak et al. (2009)"}, {"color": "#66CCFF", "id": "multi-label ranking", "label": "multi-label ranking", "shape": "dot", "size": 25, "title": "multi-label ranking"}, {"color": "#66CCFF", "id": "R.", "label": "R.", "shape": "dot", "size": 25, "title": "R."}, {"color": "#66CCFF", "id": "Efficient multi-label ranking", "label": "Efficient multi-label ranking", "shape": "dot", "size": 25, "title": "Efficient multi-label ranking"}, {"color": "#66CCFF", "id": "Jain, A. K.", "label": "Jain, A. K.", "shape": "dot", "size": 25, "title": "Jain, A. K."}, {"color": "#66CCFF", "id": "Cai, X.", "label": "Cai, X.", "shape": "dot", "size": 25, "title": "Cai, X."}, {"color": "#66CCFF", "id": "Graph structured sparsity model", "label": "Graph structured sparsity model", "shape": "dot", "size": 25, "title": "Graph structured sparsity model"}, {"color": "#66CCFF", "id": "Chow, C.", "label": "Chow, C.", "shape": "dot", "size": 25, "title": "Chow, C."}, {"color": "#66CCFF", "id": "Approximating discrete probability distributions", "label": "Approximating discrete probability distributions", "shape": "dot", "size": 25, "title": "Approximating discrete probability distributions"}, {"color": "#66CCFF", "id": "Liu, C.", "label": "Liu, C.", "shape": "dot", "size": 25, "title": "Liu, C."}, {"color": "#66CCFF", "id": "Dembczy\u0144ski, K.", "label": "Dembczy\u0144ski, K.", "shape": "dot", "size": 25, "title": "Dembczy\u0144ski, K."}, {"color": "#66CCFF", "id": "Label dependence and loss minimization", "label": "Label dependence and loss minimization", "shape": "dot", "size": 25, "title": "Label dependence and loss minimization"}, {"color": "#66CCFF", "id": "Waegeman, W.", "label": "Waegeman, W.", "shape": "dot", "size": 25, "title": "Waegeman, W."}, {"color": "#66CCFF", "id": "Cheng, W.", "label": "Cheng, W.", "shape": "dot", "size": 25, "title": "Cheng, W."}, {"color": "#66CCFF", "id": "H\u00fcllermeier, E.", "label": "H\u00fcllermeier, E.", "shape": "dot", "size": 25, "title": "H\u00fcllermeier, E."}, {"color": "#66CCFF", "id": "Dembczynski, K.", "label": "Dembczynski, K.", "shape": "dot", "size": 25, "title": "Dembczynski, K."}, {"color": "#66CCFF", "id": "Analysis of chaining", "label": "Analysis of chaining", "shape": "dot", "size": 25, "title": "Analysis of chaining"}, {"color": "#66CCFF", "id": "European Conference on Artificial Intelligence", "label": "European Conference on Artificial Intelligence", "shape": "dot", "size": 25, "title": "European Conference on Artificial Intelligence"}, {"color": "#66CCFF", "id": "Dembczynski", "label": "Dembczynski", "shape": "dot", "size": 25, "title": "Dembczynski"}, {"color": "#66CCFF", "id": "analysis of chaining", "label": "analysis of chaining", "shape": "dot", "size": 25, "title": "analysis of chaining"}, {"color": "#66CCFF", "id": "Learning higher-order graph structure", "label": "Learning higher-order graph structure", "shape": "dot", "size": 25, "title": "Learning higher-order graph structure"}, {"color": "#66CCFF", "id": "Everingham", "label": "Everingham", "shape": "dot", "size": 25, "title": "Everingham"}, {"color": "#66CCFF", "id": "PASUAL Visual Object Classes Challenge 2012", "label": "PASUAL Visual Object Classes Challenge 2012", "shape": "dot", "size": 25, "title": "PASUAL Visual Object Classes Challenge 2012"}, {"color": "#66CCFF", "id": "Bolei Zhou", "label": "Bolei Zhou", "shape": "dot", "size": 25, "title": "Bolei Zhou"}, {"color": "#66CCFF", "id": "ConceptLearner", "label": "ConceptLearner", "shape": "dot", "size": 25, "title": "ConceptLearner"}, {"color": "#66CCFF", "id": "Vignesh Jagadeesh", "label": "Vignesh Jagadeesh", "shape": "dot", "size": 25, "title": "Vignesh Jagadeesh"}, {"color": "#66CCFF", "id": "Zhou_ConceptLearner_Discovering_Visual_2015_CVPR_paper.pdf", "label": "Zhou_ConceptLearner_Discovering_Visual_2015_CVPR_paper.pdf", "shape": "dot", "size": 25, "title": "Zhou_ConceptLearner_Discovering_Visual_2015_CVPR_paper.pdf"}, {"color": "#66CCFF", "id": "Charles Sturt University", "label": "Charles Sturt University", "shape": "dot", "size": 25, "title": "Charles Sturt University"}, {"color": "#66CCFF", "id": "Junbin Gao", "label": "Junbin Gao", "shape": "dot", "size": 25, "title": "Junbin Gao"}, {"color": "#66CCFF", "id": "computer vision recognition systems", "label": "computer vision recognition systems", "shape": "dot", "size": 25, "title": "computer vision recognition systems"}, {"color": "#66CCFF", "id": "visual knowledge", "label": "visual knowledge", "shape": "dot", "size": 25, "title": "visual knowledge"}, {"color": "#66CCFF", "id": "fully labeled data", "label": "fully labeled data", "shape": "dot", "size": 25, "title": "fully labeled data"}, {"color": "#66CCFF", "id": "expensive", "label": "expensive", "shape": "dot", "size": 25, "title": "expensive"}, {"color": "#66CCFF", "id": "scalable approach", "label": "scalable approach", "shape": "dot", "size": 25, "title": "scalable approach"}, {"color": "#66CCFF", "id": "visual concept detectors", "label": "visual concept detectors", "shape": "dot", "size": 25, "title": "visual concept detectors"}, {"color": "#66CCFF", "id": "automatically", "label": "automatically", "shape": "dot", "size": 25, "title": "automatically"}, {"color": "#66CCFF", "id": "image region-level detection", "label": "image region-level detection", "shape": "dot", "size": 25, "title": "image region-level detection"}, {"color": "#66CCFF", "id": "learned concepts", "label": "learned concepts", "shape": "dot", "size": 25, "title": "learned concepts"}, {"color": "#66CCFF", "id": "scene recognition", "label": "scene recognition", "shape": "dot", "size": 25, "title": "scene recognition"}, {"color": "#66CCFF", "id": "promising performance", "label": "promising performance", "shape": "dot", "size": 25, "title": "promising performance"}, {"color": "#66CCFF", "id": "fully supervised methods", "label": "fully supervised methods", "shape": "dot", "size": 25, "title": "fully supervised methods"}, {"color": "#66CCFF", "id": "weakly supervised methods", "label": "weakly supervised methods", "shape": "dot", "size": 25, "title": "weakly supervised methods"}, {"color": "#66CCFF", "id": "domain-specific supervision", "label": "domain-specific supervision", "shape": "dot", "size": 25, "title": "domain-specific supervision"}, {"color": "#66CCFF", "id": "automatic attribute discovery", "label": "automatic attribute discovery", "shape": "dot", "size": 25, "title": "automatic attribute discovery"}, {"color": "#66CCFF", "id": "noisy web data", "label": "noisy web data", "shape": "dot", "size": 25, "title": "noisy web data"}, {"color": "#66CCFF", "id": "pictures", "label": "pictures", "shape": "dot", "size": 25, "title": "pictures"}, {"color": "#66CCFF", "id": "Im2text", "label": "Im2text", "shape": "dot", "size": 25, "title": "Im2text"}, {"color": "#66CCFF", "id": "captioned photographs", "label": "captioned photographs", "shape": "dot", "size": 25, "title": "captioned photographs"}, {"color": "#66CCFF", "id": "object detectors", "label": "object detectors", "shape": "dot", "size": 25, "title": "object detectors"}, {"color": "#66CCFF", "id": "hierarchical images", "label": "hierarchical images", "shape": "dot", "size": 25, "title": "hierarchical images"}, {"color": "#66CCFF", "id": "Deng et al.", "label": "Deng et al.", "shape": "dot", "size": 25, "title": "Deng et al."}, {"color": "#66CCFF", "id": "Places database", "label": "Places database", "shape": "dot", "size": 25, "title": "Places database"}, {"color": "#66CCFF", "id": "Zhou et al.", "label": "Zhou et al.", "shape": "dot", "size": 25, "title": "Zhou et al."}, {"color": "#66CCFF", "id": "Piction", "label": "Piction", "shape": "dot", "size": 25, "title": "Piction"}, {"color": "#66CCFF", "id": "human faces", "label": "human faces", "shape": "dot", "size": 25, "title": "human faces"}, {"color": "#66CCFF", "id": "Srihari", "label": "Srihari", "shape": "dot", "size": 25, "title": "Srihari"}, {"color": "#66CCFF", "id": "Divvala et al.", "label": "Divvala et al.", "shape": "dot", "size": 25, "title": "Divvala et al."}, {"color": "#66CCFF", "id": "webly-supervised visual concept learning", "label": "webly-supervised visual concept learning", "shape": "dot", "size": 25, "title": "webly-supervised visual concept learning"}, {"color": "#66CCFF", "id": "Tang et al.", "label": "Tang et al.", "shape": "dot", "size": 25, "title": "Tang et al."}, {"color": "#66CCFF", "id": "machine learning research", "label": "machine learning research", "shape": "dot", "size": 25, "title": "machine learning research"}, {"color": "#66CCFF", "id": "AAAI", "label": "AAAI", "shape": "dot", "size": 25, "title": "AAAI"}, {"color": "#66CCFF", "id": "AAAI Press", "label": "AAAI Press", "shape": "dot", "size": 25, "title": "AAAI Press"}, {"color": "#66CCFF", "id": "The MIT Press", "label": "The MIT Press", "shape": "dot", "size": 25, "title": "The MIT Press"}, {"color": "#66CCFF", "id": "Divvala, S. K.", "label": "Divvala, S. K.", "shape": "dot", "size": 25, "title": "Divvala, S. K."}, {"color": "#66CCFF", "id": "MIT", "label": "MIT", "shape": "dot", "size": 25, "title": "MIT"}, {"color": "#66CCFF", "id": "eBay Research Labs", "label": "eBay Research Labs", "shape": "dot", "size": 25, "title": "eBay Research Labs"}, {"color": "#66CCFF", "id": "Robinson Piramuthu", "label": "Robinson Piramuthu", "shape": "dot", "size": 25, "title": "Robinson Piramuthu"}, {"color": "#66CCFF", "id": "Yumin Suh", "label": "Yumin Suh", "shape": "dot", "size": 25, "title": "Yumin Suh"}, {"color": "#66CCFF", "id": "Subgraph Matching using Compactness Prior", "label": "Subgraph Matching using Compactness Prior", "shape": "dot", "size": 25, "title": "Subgraph Matching using Compactness Prior"}, {"color": "#66CCFF", "id": "Kamil Adamczewski", "label": "Kamil Adamczewski", "shape": "dot", "size": 25, "title": "Kamil Adamczewski"}, {"color": "#66CCFF", "id": "Suh_Subgraph_Matching_Using_2015_CVPR_paper.pdf", "label": "Suh_Subgraph_Matching_Using_2015_CVPR_paper.pdf", "shape": "dot", "size": 25, "title": "Suh_Subgraph_Matching_Using_2015_CVPR_paper.pdf"}, {"color": "#66CCFF", "id": "Feature correspondence", "label": "Feature correspondence", "shape": "dot", "size": 25, "title": "Feature correspondence"}, {"color": "#66CCFF", "id": "computer vision applications", "label": "computer vision applications", "shape": "dot", "size": 25, "title": "computer vision applications"}, {"color": "#66CCFF", "id": "Graph matching", "label": "Graph matching", "shape": "dot", "size": 25, "title": "Graph matching"}, {"color": "#66CCFF", "id": "Graph matching algorithms", "label": "Graph matching algorithms", "shape": "dot", "size": 25, "title": "Graph matching algorithms"}, {"color": "#66CCFF", "id": "precision", "label": "precision", "shape": "dot", "size": 25, "title": "precision"}, {"color": "#66CCFF", "id": "Solutions", "label": "Solutions", "shape": "dot", "size": 25, "title": "Solutions"}, {"color": "#66CCFF", "id": "subgraph matching formulation", "label": "subgraph matching formulation", "shape": "dot", "size": 25, "title": "subgraph matching formulation"}, {"color": "#66CCFF", "id": "Subgraph matching formulation", "label": "Subgraph matching formulation", "shape": "dot", "size": 25, "title": "Subgraph matching formulation"}, {"color": "#66CCFF", "id": "compactness prior", "label": "compactness prior", "shape": "dot", "size": 25, "title": "compactness prior"}, {"color": "#66CCFF", "id": "Meta-algorithm", "label": "Meta-algorithm", "shape": "dot", "size": 25, "title": "Meta-algorithm"}, {"color": "#66CCFF", "id": "Markov chain Monte Carlo", "label": "Markov chain Monte Carlo", "shape": "dot", "size": 25, "title": "Markov chain Monte Carlo"}, {"color": "#66CCFF", "id": "Formulation and algorithm", "label": "Formulation and algorithm", "shape": "dot", "size": 25, "title": "Formulation and algorithm"}, {"color": "#66CCFF", "id": "baseline performance", "label": "baseline performance", "shape": "dot", "size": 25, "title": "baseline performance"}, {"color": "#66CCFF", "id": "improvement", "label": "improvement", "shape": "dot", "size": 25, "title": "improvement"}, {"color": "#66CCFF", "id": "Cho, M.", "label": "Cho, M.", "shape": "dot", "size": 25, "title": "Cho, M."}, {"color": "#66CCFF", "id": "Reweighted random walks", "label": "Reweighted random walks", "shape": "dot", "size": 25, "title": "Reweighted random walks"}, {"color": "#66CCFF", "id": "Proceedings of the IEEE International Conference on Computer Vision", "label": "Proceedings of the IEEE International Conference on Computer Vision", "shape": "dot", "size": 25, "title": "Proceedings of the IEEE International Conference on Computer Vision"}, {"color": "#66CCFF", "id": "2009 IEEE 12th International Conference on Computer Vision", "label": "2009 IEEE 12th International Conference on Computer Vision", "shape": "dot", "size": 25, "title": "2009 IEEE 12th International Conference on Computer Vision"}, {"color": "#66CCFF", "id": "Computer Vision\u2013ECCV 2010", "label": "Computer Vision\u2013ECCV 2010", "shape": "dot", "size": 25, "title": "Computer Vision\u2013ECCV 2010"}, {"color": "#66CCFF", "id": "Progressive graph matching", "label": "Progressive graph matching", "shape": "dot", "size": 25, "title": "Progressive graph matching"}, {"color": "#66CCFF", "id": "Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on", "label": "Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on", "shape": "dot", "size": 25, "title": "Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on"}, {"color": "#66CCFF", "id": "Alahari, K.", "label": "Alahari, K.", "shape": "dot", "size": 25, "title": "Alahari, K."}, {"color": "#66CCFF", "id": "Ponce, J.", "label": "Ponce, J.", "shape": "dot", "size": 25, "title": "Ponce, J."}, {"color": "#66CCFF", "id": "Lee, Kyoung Mu", "label": "Lee, Kyoung Mu", "shape": "dot", "size": 25, "title": "Lee, Kyoung Mu"}, {"color": "#66CCFF", "id": "Seoul National University", "label": "Seoul National University", "shape": "dot", "size": 25, "title": "Seoul National University"}, {"color": "#66CCFF", "id": "Adamczewski, Kamil", "label": "Adamczewski, Kamil", "shape": "dot", "size": 25, "title": "Adamczewski, Kamil"}, {"color": "#66CCFF", "id": "Suh, Yumin", "label": "Suh, Yumin", "shape": "dot", "size": 25, "title": "Suh, Yumin"}, {"color": "#66CCFF", "id": "Duchenne, O.", "label": "Duchenne, O.", "shape": "dot", "size": 25, "title": "Duchenne, O."}, {"color": "#66CCFF", "id": "tensor-based algorithm", "label": "tensor-based algorithm", "shape": "dot", "size": 25, "title": "tensor-based algorithm"}, {"color": "#66CCFF", "id": "Gilks, W. R.", "label": "Gilks, W. R.", "shape": "dot", "size": 25, "title": "Gilks, W. R."}, {"color": "#66CCFF", "id": "Markov chain monte carlo", "label": "Markov chain monte carlo", "shape": "dot", "size": 25, "title": "Markov chain monte carlo"}, {"color": "#66CCFF", "id": "max-pooling strategy", "label": "max-pooling strategy", "shape": "dot", "size": 25, "title": "max-pooling strategy"}, {"color": "#66CCFF", "id": "Cour, T.", "label": "Cour, T.", "shape": "dot", "size": 25, "title": "Cour, T."}, {"color": "#66CCFF", "id": "balanced graph matching", "label": "balanced graph matching", "shape": "dot", "size": 25, "title": "balanced graph matching"}, {"color": "#66CCFF", "id": "progressive graph matching", "label": "progressive graph matching", "shape": "dot", "size": 25, "title": "progressive graph matching"}, {"color": "#66CCFF", "id": "Guancong Zhang", "label": "Guancong Zhang", "shape": "dot", "size": 25, "title": "Guancong Zhang"}, {"color": "#66CCFF", "id": "Good Features to Track for Visual SLAM", "label": "Good Features to Track for Visual SLAM", "shape": "dot", "size": 25, "title": "Good Features to Track for Visual SLAM"}, {"color": "#66CCFF", "id": "Patricio A. Vela", "label": "Patricio A. Vela", "shape": "dot", "size": 25, "title": "Patricio A. Vela"}, {"color": "#66CCFF", "id": "Zhang_Good_Features_to_2015_CVPR_paper.pdf", "label": "Zhang_Good_Features_to_2015_CVPR_paper.pdf", "shape": "dot", "size": 25, "title": "Zhang_Good_Features_to_2015_CVPR_paper.pdf"}, {"color": "#66CCFF", "id": "measured features", "label": "measured features", "shape": "dot", "size": 25, "title": "measured features"}, {"color": "#66CCFF", "id": "accurate localization", "label": "accurate localization", "shape": "dot", "size": 25, "title": "accurate localization"}, {"color": "#66CCFF", "id": "method for selecting features", "label": "method for selecting features", "shape": "dot", "size": 25, "title": "method for selecting features"}, {"color": "#66CCFF", "id": "observability of SLAM", "label": "observability of SLAM", "shape": "dot", "size": 25, "title": "observability of SLAM"}, {"color": "#66CCFF", "id": "existing SLAM systems", "label": "existing SLAM systems", "shape": "dot", "size": 25, "title": "existing SLAM systems"}, {"color": "#66CCFF", "id": "estimation utility", "label": "estimation utility", "shape": "dot", "size": 25, "title": "estimation utility"}, {"color": "#66CCFF", "id": "observability indices", "label": "observability indices", "shape": "dot", "size": 25, "title": "observability indices"}, {"color": "#66CCFF", "id": "incremental singular value decomposition (SVD)", "label": "incremental singular value decomposition (SVD)", "shape": "dot", "size": 25, "title": "incremental singular value decomposition (SVD)"}, {"color": "#66CCFF", "id": "greedy selection", "label": "greedy selection", "shape": "dot", "size": 25, "title": "greedy selection"}, {"color": "#66CCFF", "id": "SLAM", "label": "SLAM", "shape": "dot", "size": 25, "title": "SLAM"}, {"color": "#66CCFF", "id": "SfM", "label": "SfM", "shape": "dot", "size": 25, "title": "SfM"}, {"color": "#66CCFF", "id": "localization accuracy", "label": "localization accuracy", "shape": "dot", "size": 25, "title": "localization accuracy"}, {"color": "#66CCFF", "id": "incremental singular value decomposition", "label": "incremental singular value decomposition", "shape": "dot", "size": 25, "title": "incremental singular value decomposition"}, {"color": "#66CCFF", "id": "submodular", "label": "submodular", "shape": "dot", "size": 25, "title": "submodular"}, {"color": "#66CCFF", "id": "synthetic experiments", "label": "synthetic experiments", "shape": "dot", "size": 25, "title": "synthetic experiments"}, {"color": "#66CCFF", "id": "improved localization accuracy", "label": "improved localization accuracy", "shape": "dot", "size": 25, "title": "improved localization accuracy"}, {"color": "#66CCFF", "id": "SLAM experiments", "label": "SLAM experiments", "shape": "dot", "size": 25, "title": "SLAM experiments"}, {"color": "#66CCFF", "id": "improved data association", "label": "improved data association", "shape": "dot", "size": 25, "title": "improved data association"}, {"color": "#66CCFF", "id": "temporal observability indices", "label": "temporal observability indices", "shape": "dot", "size": 25, "title": "temporal observability indices"}, {"color": "#66CCFF", "id": "near-optimal", "label": "near-optimal", "shape": "dot", "size": 25, "title": "near-optimal"}, {"color": "#66CCFF", "id": "Visual SLAM", "label": "Visual SLAM", "shape": "dot", "size": 25, "title": "Visual SLAM"}, {"color": "#66CCFF", "id": "Data Association", "label": "Data Association", "shape": "dot", "size": 25, "title": "Data Association"}, {"color": "#66CCFF", "id": "Incremental SVD", "label": "Incremental SVD", "shape": "dot", "size": 25, "title": "Incremental SVD"}, {"color": "#66CCFF", "id": "Observability Analysis", "label": "Observability Analysis", "shape": "dot", "size": 25, "title": "Observability Analysis"}, {"color": "#66CCFF", "id": "map_building", "label": "map_building", "shape": "dot", "size": 25, "title": "map_building"}, {"color": "#66CCFF", "id": "MonoSLAM", "label": "MonoSLAM", "shape": "dot", "size": 25, "title": "MonoSLAM"}, {"color": "#66CCFF", "id": "real-time SLAM", "label": "real-time SLAM", "shape": "dot", "size": 25, "title": "real-time SLAM"}, {"color": "#66CCFF", "id": "LSD-SLAM", "label": "LSD-SLAM", "shape": "dot", "size": 25, "title": "LSD-SLAM"}, {"color": "#66CCFF", "id": "direct monocular SLAM", "label": "direct monocular SLAM", "shape": "dot", "size": 25, "title": "direct monocular SLAM"}, {"color": "#66CCFF", "id": "Covariance recovery", "label": "Covariance recovery", "shape": "dot", "size": 25, "title": "Covariance recovery"}, {"color": "#66CCFF", "id": "data association", "label": "data association", "shape": "dot", "size": 25, "title": "data association"}, {"color": "#66CCFF", "id": "Feature Selection", "label": "Feature Selection", "shape": "dot", "size": 25, "title": "Feature Selection"}, {"color": "#66CCFF", "id": "Andrade-Cetto and Sanfeliu", "label": "Andrade-Cetto and Sanfeliu", "shape": "dot", "size": 25, "title": "Andrade-Cetto and Sanfeliu"}, {"color": "#66CCFF", "id": "partial observability", "label": "partial observability", "shape": "dot", "size": 25, "title": "partial observability"}, {"color": "#66CCFF", "id": "Kaess and Dellaert", "label": "Kaess and Dellaert", "shape": "dot", "size": 25, "title": "Kaess and Dellaert"}, {"color": "#66CCFF", "id": "covariance recovery", "label": "covariance recovery", "shape": "dot", "size": 25, "title": "covariance recovery"}, {"color": "#66CCFF", "id": "Davison et al.", "label": "Davison et al.", "shape": "dot", "size": 25, "title": "Davison et al."}, {"color": "#66CCFF", "id": "Engel et al.", "label": "Engel et al.", "shape": "dot", "size": 25, "title": "Engel et al."}, {"color": "#66CCFF", "id": "Slam_Algorithm", "label": "Slam_Algorithm", "shape": "dot", "size": 25, "title": "Slam_Algorithm"}, {"color": "#66CCFF", "id": "iSAM2", "label": "iSAM2", "shape": "dot", "size": 25, "title": "iSAM2"}, {"color": "#66CCFF", "id": "Bayes Tree", "label": "Bayes Tree", "shape": "dot", "size": 25, "title": "Bayes Tree"}, {"color": "#66CCFF", "id": "Incremental Smoothing", "label": "Incremental Smoothing", "shape": "dot", "size": 25, "title": "Incremental Smoothing"}, {"color": "#66CCFF", "id": "Active search", "label": "Active search", "shape": "dot", "size": 25, "title": "Active search"}, {"color": "#66CCFF", "id": "Live dense reconstruction", "label": "Live dense reconstruction", "shape": "dot", "size": 25, "title": "Live dense reconstruction"}, {"color": "#66CCFF", "id": "Single moving camera", "label": "Single moving camera", "shape": "dot", "size": 25, "title": "Single moving camera"}, {"color": "#66CCFF", "id": "Machine Intelligence", "label": "Machine Intelligence", "shape": "dot", "size": 25, "title": "Machine Intelligence"}, {"color": "#66CCFF", "id": "Slam", "label": "Slam", "shape": "dot", "size": 25, "title": "Slam"}, {"color": "#66CCFF", "id": "Zheng Ma", "label": "Zheng Ma", "shape": "dot", "size": 25, "title": "Zheng Ma"}, {"color": "#66CCFF", "id": "School of ECR", "label": "School of ECR", "shape": "dot", "size": 25, "title": "School of ECR"}, {"color": "#66CCFF", "id": "Lei Yu", "label": "Lei Yu", "shape": "dot", "size": 25, "title": "Lei Yu"}, {"color": "#66CCFF", "id": "Antoni B. Chan", "label": "Antoni B. Chan", "shape": "dot", "size": 25, "title": "Antoni B. Chan"}, {"color": "#66CCFF", "id": "Georgia Tech", "label": "Georgia Tech", "shape": "dot", "size": 25, "title": "Georgia Tech"}, {"color": "#66CCFF", "id": "School of ECE", "label": "School of ECE", "shape": "dot", "size": 25, "title": "School of ECE"}, {"color": "#66CCFF", "id": "zhanggc@gatech.edu", "label": "zhanggc@gatech.edu", "shape": "dot", "size": 25, "title": "zhanggc@gatech.edu"}, {"color": "#66CCFF", "id": "pvela@gatech.edu", "label": "pvela@gatech.edu", "shape": "dot", "size": 25, "title": "pvela@gatech.edu"}, {"color": "#66CCFF", "id": "Ma_Small_Instance_Detection_2015_CVPR_paper", "label": "Ma_Small_Instance_Detection_2015_CVPR_paper", "shape": "dot", "size": 25, "title": "Ma_Small_Instance_Detection_2015_CVPR_paper"}, {"color": "#66CCFF", "id": "Small Instance Detection", "label": "Small Instance Detection", "shape": "dot", "size": 25, "title": "Small Instance Detection"}, {"color": "#66CCFF", "id": "Integer Programming", "label": "Integer Programming", "shape": "dot", "size": 25, "title": "Integer Programming"}, {"color": "#66CCFF", "id": "Object Density Maps", "label": "Object Density Maps", "shape": "dot", "size": 25, "title": "Object Density Maps"}, {"color": "#66CCFF", "id": "partially-occluded small instances", "label": "partially-occluded small instances", "shape": "dot", "size": 25, "title": "partially-occluded small instances"}, {"color": "#66CCFF", "id": "pedestrians", "label": "pedestrians", "shape": "dot", "size": 25, "title": "pedestrians"}, {"color": "#66CCFF", "id": "cells", "label": "cells", "shape": "dot", "size": 25, "title": "cells"}, {"color": "#66CCFF", "id": "partially-occluding small instances", "label": "partially-occluding small instances", "shape": "dot", "size": 25, "title": "partially-occluding small instances"}, {"color": "#66CCFF", "id": "2D integer programming", "label": "2D integer programming", "shape": "dot", "size": 25, "title": "2D integer programming"}, {"color": "#66CCFF", "id": "recover object instance locations", "label": "recover object instance locations", "shape": "dot", "size": 25, "title": "recover object instance locations"}, {"color": "#66CCFF", "id": "ROI counts", "label": "ROI counts", "shape": "dot", "size": 25, "title": "ROI counts"}, {"color": "#66CCFF", "id": "density map", "label": "density map", "shape": "dot", "size": 25, "title": "density map"}, {"color": "#66CCFF", "id": "local density map", "label": "local density map", "shape": "dot", "size": 25, "title": "local density map"}, {"color": "#66CCFF", "id": "fluorescence microscopy cell images", "label": "fluorescence microscopy cell images", "shape": "dot", "size": 25, "title": "fluorescence microscopy cell images"}, {"color": "#66CCFF", "id": "UCSD pedestrians", "label": "UCSD pedestrians", "shape": "dot", "size": 25, "title": "UCSD pedestrians"}, {"color": "#66CCFF", "id": "small animals", "label": "small animals", "shape": "dot", "size": 25, "title": "small animals"}, {"color": "#66CCFF", "id": "insects", "label": "insects", "shape": "dot", "size": 25, "title": "insects"}, {"color": "#66CCFF", "id": "object instances", "label": "object instances", "shape": "dot", "size": 25, "title": "object instances"}, {"color": "#66CCFF", "id": "HOG features", "label": "HOG features", "shape": "dot", "size": 25, "title": "HOG features"}, {"color": "#66CCFF", "id": "Bayesian regression", "label": "Bayesian regression", "shape": "dot", "size": 25, "title": "Bayesian regression"}, {"color": "#66CCFF", "id": "Crowd counting", "label": "Crowd counting", "shape": "dot", "size": 25, "title": "Crowd counting"}, {"color": "#66CCFF", "id": "multiple local features", "label": "multiple local features", "shape": "dot", "size": 25, "title": "multiple local features"}, {"color": "#66CCFF", "id": "IEEE Conf. Computer Vision and Pattern Recognition", "label": "IEEE Conf. Computer Vision and Pattern Recognition", "shape": "dot", "size": 25, "title": "IEEE Conf. Computer Vision and Pattern Recognition"}, {"color": "#66CCFF", "id": "IEEE Trans. on Image Processing", "label": "IEEE Trans. on Image Processing", "shape": "dot", "size": 25, "title": "IEEE Trans. on Image Processing"}, {"color": "#66CCFF", "id": "Learning to count objects in images", "label": "Learning to count objects in images", "shape": "dot", "size": 25, "title": "Learning to count objects in images"}, {"color": "#66CCFF", "id": "Digital Image Computing: Techniques and Applications", "label": "Digital Image Computing: Techniques and Applications", "shape": "dot", "size": 25, "title": "Digital Image Computing: Techniques and Applications"}, {"color": "#66CCFF", "id": "Crowd counting using multiple local features", "label": "Crowd counting using multiple local features", "shape": "dot", "size": 25, "title": "Crowd counting using multiple local features"}, {"color": "#66CCFF", "id": "Human detection", "label": "Human detection", "shape": "dot", "size": 25, "title": "Human detection"}, {"color": "#66CCFF", "id": "Sridharan (2009)", "label": "Sridharan (2009)", "shape": "dot", "size": 25, "title": "Sridharan (2009)"}, {"color": "#66CCFF", "id": "Lowe (2004)", "label": "Lowe (2004)", "shape": "dot", "size": 25, "title": "Lowe (2004)"}, {"color": "#66CCFF", "id": "feature detection", "label": "feature detection", "shape": "dot", "size": 25, "title": "feature detection"}, {"color": "#66CCFF", "id": "feature matching", "label": "feature matching", "shape": "dot", "size": 25, "title": "feature matching"}, {"color": "#66CCFF", "id": "Chan (2008)", "label": "Chan (2008)", "shape": "dot", "size": 25, "title": "Chan (2008)"}, {"color": "#66CCFF", "id": "privacy concerns", "label": "privacy concerns", "shape": "dot", "size": 25, "title": "privacy concerns"}, {"color": "#66CCFF", "id": "crowd monitoring", "label": "crowd monitoring", "shape": "dot", "size": 25, "title": "crowd monitoring"}, {"color": "#66CCFF", "id": "Zhao (2003)", "label": "Zhao (2003)", "shape": "dot", "size": 25, "title": "Zhao (2003)"}, {"color": "#66CCFF", "id": "Bayesian segmentation", "label": "Bayesian segmentation", "shape": "dot", "size": 25, "title": "Bayesian segmentation"}, {"color": "#66CCFF", "id": "crowded scenes", "label": "crowded scenes", "shape": "dot", "size": 25, "title": "crowded scenes"}, {"color": "#66CCFF", "id": "R. ia", "label": "R. ia", "shape": "dot", "size": 25, "title": "R. ia"}, {"color": "#66CCFF", "id": "Lemtipsky, V.", "label": "Lemtipsky, V.", "shape": "dot", "size": 25, "title": "Lemtipsky, V."}, {"color": "#66CCFF", "id": "PASCAL VOC challenge", "label": "PASCAL VOC challenge", "shape": "dot", "size": 25, "title": "PASCAL VOC challenge"}, {"color": "#66CCFF", "id": "PASCAL VOC challenge description", "label": "PASCAL VOC challenge description", "shape": "dot", "size": 25, "title": "PASCAL VOC challenge description"}, {"color": "#66CCFF", "id": "City University of Hong Kong", "label": "City University of Hong Kong", "shape": "dot", "size": 25, "title": "City University of Hong Kong"}, {"color": "#66CCFF", "id": "Mohammad Rastegari", "label": "Mohammad Rastegari", "shape": "dot", "size": 25, "title": "Mohammad Rastegari"}, {"color": "#66CCFF", "id": "Computationally Bound Retrieval", "label": "Computationally Bound Retrieval", "shape": "dot", "size": 25, "title": "Computationally Bound Retrieval"}, {"color": "#66CCFF", "id": "Computationally Bounded Retrieval", "label": "Computationally Bounded Retrieval", "shape": "dot", "size": 25, "title": "Computationally Bounded Retrieval"}, {"color": "#66CCFF", "id": "Cem Keskin", "label": "Cem Keskin", "shape": "dot", "size": 25, "title": "Cem Keskin"}, {"color": "#66CCFF", "id": "Shahram Izadi", "label": "Shahram Izadi", "shape": "dot", "size": 25, "title": "Shahram Izadi"}, {"color": "#66CCFF", "id": "Rastegari_Computationally_Bounded_Retrieval_2015_CVPR_paper.pdf", "label": "Rastegari_Computationally_Bounded_Retrieval_2015_CVPR_paper.pdf", "shape": "dot", "size": 25, "title": "Rastegari_Computationally_Bounded_Retrieval_2015_CVPR_paper.pdf"}, {"color": "#66CCFF", "id": "Rastegari_Computationality_Bounded_Retrieval_2015_CVPR_paper", "label": "Rastegari_Computationality_Bounded_Retrieval_2015_CVPR_paper", "shape": "dot", "size": 25, "title": "Rastegari_Computationality_Bounded_Retrieval_2015_CVPR_paper"}, {"color": "#66CCFF", "id": "large image databases", "label": "large image databases", "shape": "dot", "size": 25, "title": "large image databases"}, {"color": "#66CCFF", "id": "efficient retrieval challenging", "label": "efficient retrieval challenging", "shape": "dot", "size": 25, "title": "efficient retrieval challenging"}, {"color": "#66CCFF", "id": "high dimensional data", "label": "high dimensional data", "shape": "dot", "size": 25, "title": "high dimensional data"}, {"color": "#66CCFF", "id": "retrieval challenge", "label": "retrieval challenge", "shape": "dot", "size": 25, "title": "retrieval challenge"}, {"color": "#66CCFF", "id": "hashing methods", "label": "hashing methods", "shape": "dot", "size": 25, "title": "hashing methods"}, {"color": "#66CCFF", "id": "accuracy for speed", "label": "accuracy for speed", "shape": "dot", "size": 25, "title": "accuracy for speed"}, {"color": "#66CCFF", "id": "speed of image retrieval", "label": "speed of image retrieval", "shape": "dot", "size": 25, "title": "speed of image retrieval"}, {"color": "#66CCFF", "id": "computationally bounded sparse projections", "label": "computationally bounded sparse projections", "shape": "dot", "size": 25, "title": "computationally bounded sparse projections"}, {"color": "#66CCFF", "id": "orthogonality constraint", "label": "orthogonality constraint", "shape": "dot", "size": 25, "title": "orthogonality constraint"}, {"color": "#66CCFF", "id": "bit correlation", "label": "bit correlation", "shape": "dot", "size": 25, "title": "bit correlation"}, {"color": "#66CCFF", "id": "iterative scheme", "label": "iterative scheme", "shape": "dot", "size": 25, "title": "iterative scheme"}, {"color": "#66CCFF", "id": "speed-up of up to a factor of 100", "label": "speed-up of up to a factor of 100", "shape": "dot", "size": 25, "title": "speed-up of up to a factor of 100"}, {"color": "#66CCFF", "id": "ImageNET", "label": "ImageNET", "shape": "dot", "size": 25, "title": "ImageNET"}, {"color": "#66CCFF", "id": "GIST1M", "label": "GIST1M", "shape": "dot", "size": 25, "title": "GIST1M"}, {"color": "#66CCFF", "id": "SUN-attribute", "label": "SUN-attribute", "shape": "dot", "size": 25, "title": "SUN-attribute"}, {"color": "#66CCFF", "id": "fast and efficient projections", "label": "fast and efficient projections", "shape": "dot", "size": 25, "title": "fast and efficient projections"}, {"color": "#66CCFF", "id": "factor of 100", "label": "factor of 100", "shape": "dot", "size": 25, "title": "factor of 100"}, {"color": "#66CCFF", "id": "Near-optimal Hasing Algorithms", "label": "Near-optimal Hasing Algorithms", "shape": "dot", "size": 25, "title": "Near-optimal Hasing Algorithms"}, {"color": "#66CCFF", "id": "Approximate Nearest Neighbor", "label": "Approximate Nearest Neighbor", "shape": "dot", "size": 25, "title": "Approximate Nearest Neighbor"}, {"color": "#66CCFF", "id": "High Dimensions", "label": "High Dimensions", "shape": "dot", "size": 25, "title": "High Dimensions"}, {"color": "#66CCFF", "id": "Datar et al. (2004)", "label": "Datar et al. (2004)", "shape": "dot", "size": 25, "title": "Datar et al. (2004)"}, {"color": "#66CCFF", "id": "Locality-Sensitive Hashing Scheme", "label": "Locality-Sensitive Hashing Scheme", "shape": "dot", "size": 25, "title": "Locality-Sensitive Hashing Scheme"}, {"color": "#66CCFF", "id": "P-stable Distributions", "label": "P-stable Distributions", "shape": "dot", "size": 25, "title": "P-stable Distributions"}, {"color": "#66CCFF", "id": "Methods", "label": "Methods", "shape": "dot", "size": 25, "title": "Methods"}, {"color": "#66CCFF", "id": "Speed-up", "label": "Speed-up", "shape": "dot", "size": 25, "title": "Speed-up"}, {"color": "#66CCFF", "id": "Computer Vision and Pattern Recognition, 2009", "label": "Computer Vision and Pattern Recognition, 2009", "shape": "dot", "size": 25, "title": "Computer Vision and Pattern Recognition, 2009"}, {"color": "#66CCFF", "id": "Gong et al. (2013)", "label": "Gong et al. (2013)", "shape": "dot", "size": 25, "title": "Gong et al. (2013)"}, {"color": "#66CCFF", "id": "Computer Vision and Pattern Recognition (CVPR), 2013", "label": "Computer Vision and Pattern Recognition (CVPR), 2013", "shape": "dot", "size": 25, "title": "Computer Vision and Pattern Recognition (CVPR), 2013"}, {"color": "#66CCFF", "id": "Gong \u0026 Lazebnik (2011)", "label": "Gong \u0026 Lazebnik (2011)", "shape": "dot", "size": 25, "title": "Gong \u0026 Lazebnik (2011)"}, {"color": "#66CCFF", "id": "2011 IEEE Conference on Computer Vision and Pattern Recognition", "label": "2011 IEEE Conference on Computer Vision and Pattern Recognition", "shape": "dot", "size": 25, "title": "2011 IEEE Conference on Computer Vision and Pattern Recognition"}, {"color": "#66CCFF", "id": "J\u00e9goeu et al. (2009)", "label": "J\u00e9goeu et al. (2009)", "shape": "dot", "size": 25, "title": "J\u00e9goeu et al. (2009)"}, {"color": "#66CCFF", "id": "Searching with quantization", "label": "Searching with quantization", "shape": "dot", "size": 25, "title": "Searching with quantization"}, {"color": "#66CCFF", "id": "short codes", "label": "short codes", "shape": "dot", "size": 25, "title": "short codes"}, {"color": "#66CCFF", "id": "iterative quantization", "label": "iterative quantization", "shape": "dot", "size": 25, "title": "iterative quantization"}, {"color": "#66CCFF", "id": "J\u00e9gou et al (2009)", "label": "J\u00e9gou et al (2009)", "shape": "dot", "size": 25, "title": "J\u00e9gou et al (2009)"}, {"color": "#66CCFF", "id": "Krizhevskya et al (2012)", "label": "Krizhevskya et al (2012)", "shape": "dot", "size": 25, "title": "Krizhevskya et al (2012)"}, {"color": "#66CCFF", "id": "Majia et al (2013)", "label": "Majia et al (2013)", "shape": "dot", "size": 25, "title": "Majia et al (2013)"}, {"color": "#66CCFF", "id": "efficient classification", "label": "efficient classification", "shape": "dot", "size": 25, "title": "efficient classification"}, {"color": "#66CCFF", "id": "Norouzia et al (2011)", "label": "Norouzia et al (2011)", "shape": "dot", "size": 25, "title": "Norouzia et al (2011)"}, {"color": "#66CCFF", "id": "New Insights into Laplacian Similarity Search", "label": "New Insights into Laplacian Similarity Search", "shape": "dot", "size": 25, "title": "New Insights into Laplacian Similarity Search"}, {"color": "#66CCFF", "id": "New Insights into Laplacian Similarity Research", "label": "New Insights into Laplacian Similarity Research", "shape": "dot", "size": 25, "title": "New Insights into Laplacian Similarity Research"}, {"color": "#66CCFF", "id": "Wu_New_Insights_Into_2015_CVPR_paper.pdf", "label": "Wu_New_Insights_Into_2015_CVPR_paper.pdf", "shape": "dot", "size": 25, "title": "Wu_New_Insights_Into_2015_CVPR_paper.pdf"}, {"color": "#66CCFF", "id": "Graph-based computer vision applications", "label": "Graph-based computer vision applications", "shape": "dot", "size": 25, "title": "Graph-based computer vision applications"}, {"color": "#66CCFF", "id": "similarity metrics", "label": "similarity metrics", "shape": "dot", "size": 25, "title": "similarity metrics"}, {"color": "#66CCFF", "id": "pairwise similarity", "label": "pairwise similarity", "shape": "dot", "size": 25, "title": "pairwise similarity"}, {"color": "#66CCFF", "id": "vertices", "label": "vertices", "shape": "dot", "size": 25, "title": "vertices"}, {"color": "#66CCFF", "id": "(L + \u03b1\u039b)\u22121", "label": "(L + \u03b1\u039b)\u22121", "shape": "dot", "size": 25, "title": "(L + \u03b1\u039b)\u22121"}, {"color": "#66CCFF", "id": "graph Laplacian", "label": "graph Laplacian", "shape": "dot", "size": 25, "title": "graph Laplacian"}, {"color": "#66CCFF", "id": "positive diagonal matrix", "label": "positive diagonal matrix", "shape": "dot", "size": 25, "title": "positive diagonal matrix"}, {"color": "#66CCFF", "id": "regularizer", "label": "regularizer", "shape": "dot", "size": 25, "title": "regularizer"}, {"color": "#66CCFF", "id": "graph topology", "label": "graph topology", "shape": "dot", "size": 25, "title": "graph topology"}, {"color": "#66CCFF", "id": "cluster density", "label": "cluster density", "shape": "dot", "size": 25, "title": "cluster density"}, {"color": "#66CCFF", "id": "choices", "label": "choices", "shape": "dot", "size": 25, "title": "choices"}, {"color": "#66CCFF", "id": "complementary behaviors", "label": "complementary behaviors", "shape": "dot", "size": 25, "title": "complementary behaviors"}, {"color": "#66CCFF", "id": "analysis of impact", "label": "analysis of impact", "shape": "dot", "size": 25, "title": "analysis of impact"}, {"color": "#66CCFF", "id": "\u039b", "label": "\u039b", "shape": "dot", "size": 25, "title": "\u039b"}, {"color": "#66CCFF", "id": "Paper (1999)", "label": "Paper (1999)", "shape": "dot", "size": 25, "title": "Paper (1999)"}, {"color": "#66CCFF", "id": "Pagerank Citation Ranking", "label": "Pagerank Citation Ranking", "shape": "dot", "size": 25, "title": "Pagerank Citation Ranking"}, {"color": "#66CCFF", "id": "bring order to web", "label": "bring order to web", "shape": "dot", "size": 25, "title": "bring order to web"}, {"color": "#66CCFF", "id": "Chung (1997)", "label": "Chung (1997)", "shape": "dot", "size": 25, "title": "Chung (1997)"}, {"color": "#66CCFF", "id": "Spectral Graph Theory", "label": "Spectral Graph Theory", "shape": "dot", "size": 25, "title": "Spectral Graph Theory"}, {"color": "#66CCFF", "id": "Graph Topology", "label": "Graph Topology", "shape": "dot", "size": 25, "title": "Graph Topology"}, {"color": "#66CCFF", "id": "Andersen et al. (2006)", "label": "Andersen et al. (2006)", "shape": "dot", "size": 25, "title": "Andersen et al. (2006)"}, {"color": "#66CCFF", "id": "Pagerank Vectors", "label": "Pagerank Vectors", "shape": "dot", "size": 25, "title": "Pagerank Vectors"}, {"color": "#66CCFF", "id": "Local Graph Partitioning", "label": "Local Graph Partitioning", "shape": "dot", "size": 25, "title": "Local Graph Partitioning"}, {"color": "#66CCFF", "id": "Belkin \u0026 Niyogi (2001)", "label": "Belkin \u0026 Niyogi (2001)", "shape": "dot", "size": 25, "title": "Belkin \u0026 Niyogi (2001)"}, {"color": "#66CCFF", "id": "Shi \u0026 Malik (2000)", "label": "Shi \u0026 Malik (2000)", "shape": "dot", "size": 25, "title": "Shi \u0026 Malik (2000)"}, {"color": "#66CCFF", "id": "Laplacianfaces", "label": "Laplacianfaces", "shape": "dot", "size": 25, "title": "Laplacianfaces"}, {"color": "#66CCFF", "id": "Random walks", "label": "Random walks", "shape": "dot", "size": 25, "title": "Random walks"}, {"color": "#66CCFF", "id": "Wu, X.-M.", "label": "Wu, X.-M.", "shape": "dot", "size": 25, "title": "Wu, X.-M."}, {"color": "#66CCFF", "id": "Electrical Engineering", "label": "Electrical Engineering", "shape": "dot", "size": 25, "title": "Electrical Engineering"}, {"color": "#66CCFF", "id": "graph-based learning", "label": "graph-based learning", "shape": "dot", "size": 25, "title": "graph-based learning"}, {"color": "#66CCFF", "id": "harmonic structure", "label": "harmonic structure", "shape": "dot", "size": 25, "title": "harmonic structure"}, {"color": "#66CCFF", "id": "Chang, S.-F.", "label": "Chang, S.-F.", "shape": "dot", "size": 25, "title": "Chang, S.-F."}, {"color": "#66CCFF", "id": "Wenguan Wang", "label": "Wenguan Wang", "shape": "dot", "size": 25, "title": "Wenguan Wang"}, {"color": "#66CCFF", "id": "Saliency-Aware Geodesic Video Object Segmentation", "label": "Saliency-Aware Geodesic Video Object Segmentation", "shape": "dot", "size": 25, "title": "Saliency-Aware Geodesic Video Object Segmentation"}, {"color": "#66CCFF", "id": "Jianbing Shen", "label": "Jianbing Shen", "shape": "dot", "size": 25, "title": "Jianbing Shen"}, {"color": "#66CCFF", "id": "Fatih Porikli", "label": "Fatih Porikli", "shape": "dot", "size": 25, "title": "Fatih Porikli"}, {"color": "#66CCFF", "id": "Saliency-Aware Geosesic Video Object Segmentation", "label": "Saliency-Aware Geosesic Video Object Segmentation", "shape": "dot", "size": 25, "title": "Saliency-Aware Geosesic Video Object Segmentation"}, {"color": "#66CCFF", "id": "unsupervised method", "label": "unsupervised method", "shape": "dot", "size": 25, "title": "unsupervised method"}, {"color": "#66CCFF", "id": "geodesic distance", "label": "geodesic distance", "shape": "dot", "size": 25, "title": "geodesic distance"}, {"color": "#66CCFF", "id": "salience as prior", "label": "salience as prior", "shape": "dot", "size": 25, "title": "salience as prior"}, {"color": "#66CCFF", "id": "spatial edges", "label": "spatial edges", "shape": "dot", "size": 25, "title": "spatial edges"}, {"color": "#66CCFF", "id": "temporal motion boundaries", "label": "temporal motion boundaries", "shape": "dot", "size": 25, "title": "temporal motion boundaries"}, {"color": "#66CCFF", "id": "spatiotemporal salience maps", "label": "spatiotemporal salience maps", "shape": "dot", "size": 25, "title": "spatiotemporal salience maps"}, {"color": "#66CCFF", "id": "global appearance models", "label": "global appearance models", "shape": "dot", "size": 25, "title": "global appearance models"}, {"color": "#66CCFF", "id": "dynamic location models", "label": "dynamic location models", "shape": "dot", "size": 25, "title": "dynamic location models"}, {"color": "#66CCFF", "id": "elements within energy minimization framework", "label": "elements within energy minimization framework", "shape": "dot", "size": 25, "title": "elements within energy minimization framework"}, {"color": "#66CCFF", "id": "superiority over existing algorithms", "label": "superiority over existing algorithms", "shape": "dot", "size": 25, "title": "superiority over existing algorithms"}, {"color": "#66CCFF", "id": "Video Object Segmentation", "label": "Video Object Segmentation", "shape": "dot", "size": 25, "title": "Video Object Segmentation"}, {"color": "#66CCFF", "id": "spatially and temporally coherent object segmentation", "label": "spatially and temporally coherent object segmentation", "shape": "dot", "size": 25, "title": "spatially and temporally coherent object segmentation"}, {"color": "#66CCFF", "id": "Geos image segmentation", "label": "Geos image segmentation", "shape": "dot", "size": 25, "title": "Geos image segmentation"}, {"color": "#66CCFF", "id": "ECCV, 2008", "label": "ECCV, 2008", "shape": "dot", "size": 25, "title": "ECCV, 2008"}, {"color": "#66CCFF", "id": "CVPR, 2012", "label": "CVPR, 2012", "shape": "dot", "size": 25, "title": "CVPR, 2012"}, {"color": "#66CCFF", "id": "state-of-the-art superpixel methods", "label": "state-of-the-art superpixel methods", "shape": "dot", "size": 25, "title": "state-of-the-art superpixel methods"}, {"color": "#66CCFF", "id": "IEEE TPAM, 2012", "label": "IEEE TPAM, 2012", "shape": "dot", "size": 25, "title": "IEEE TPAM, 2012"}, {"color": "#66CCFF", "id": "Motion Boundaries", "label": "Motion Boundaries", "shape": "dot", "size": 25, "title": "Motion Boundaries"}, {"color": "#66CCFF", "id": "geodesic image segmentation approach", "label": "geodesic image segmentation approach", "shape": "dot", "size": 25, "title": "geodesic image segmentation approach"}, {"color": "#66CCFF", "id": "Geos", "label": "Geos", "shape": "dot", "size": 25, "title": "Geos"}, {"color": "#66CCFF", "id": "geospatial image segmentation approach", "label": "geospatial image segmentation approach", "shape": "dot", "size": 25, "title": "geospatial image segmentation approach"}, {"color": "#66CCFF", "id": "Geodesic graph cut", "label": "Geodesic graph cut", "shape": "dot", "size": 25, "title": "Geodesic graph cut"}, {"color": "#66CCFF", "id": "interactive image segmentation", "label": "interactive image segmentation", "shape": "dot", "size": 25, "title": "interactive image segmentation"}, {"color": "#66CCFF", "id": "Grabcut", "label": "Grabcut", "shape": "dot", "size": 25, "title": "Grabcut"}, {"color": "#66CCFF", "id": "interactive foreground extraction", "label": "interactive foreground extraction", "shape": "dot", "size": 25, "title": "interactive foreground extraction"}, {"color": "#66CCFF", "id": "iterated graph cuts", "label": "iterated graph cuts", "shape": "dot", "size": 25, "title": "iterated graph cuts"}, {"color": "#66CCFF", "id": "Object segmentation", "label": "Object segmentation", "shape": "dot", "size": 25, "title": "Object segmentation"}, {"color": "#66CCFF", "id": "trajectory analysis", "label": "trajectory analysis", "shape": "dot", "size": 25, "title": "trajectory analysis"}, {"color": "#66CCFF", "id": "automatic object segmentation", "label": "automatic object segmentation", "shape": "dot", "size": 25, "title": "automatic object segmentation"}, {"color": "#66CCFF", "id": "min-cut approach", "label": "min-cut approach", "shape": "dot", "size": 25, "title": "min-cut approach"}, {"color": "#66CCFF", "id": "constrained parametric min-cuts", "label": "constrained parametric min-cuts", "shape": "dot", "size": 25, "title": "constrained parametric min-cuts"}, {"color": "#66CCFF", "id": "Video object segmentation", "label": "Video object segmentation", "shape": "dot", "size": 25, "title": "Video object segmentation"}, {"color": "#66CCFF", "id": "W. Brendel and S. Todorovic", "label": "W. Brendel and S. Todorovic", "shape": "dot", "size": 25, "title": "W. Brendel and S. Todorovic"}, {"color": "#66CCFF", "id": "Geodesic image and video editing", "label": "Geodesic image and video editing", "shape": "dot", "size": 25, "title": "Geodesic image and video editing"}, {"color": "#66CCFF", "id": "geodesic methods", "label": "geodesic methods", "shape": "dot", "size": 25, "title": "geodesic methods"}, {"color": "#66CCFF", "id": "J. Carreira", "label": "J. Carreira", "shape": "dot", "size": 25, "title": "J. Carreira"}, {"color": "#66CCFF", "id": "W. Brendel", "label": "W. Brendel", "shape": "dot", "size": 25, "title": "W. Brendel"}, {"color": "#66CCFF", "id": "Video object segmentation by tracking regions", "label": "Video object segmentation by tracking regions", "shape": "dot", "size": 25, "title": "Video object segmentation by tracking regions"}, {"color": "#66CCFF", "id": "D. Tsai", "label": "D. Tsai", "shape": "dot", "size": 25, "title": "D. Tsai"}, {"color": "#66CCFF", "id": "Motion coherent tracking using multi-label mrf optimization", "label": "Motion coherent tracking using multi-label mrf optimization", "shape": "dot", "size": 25, "title": "Motion coherent tracking using multi-label mrf optimization"}, {"color": "#66CCFF", "id": "NICTA Australia", "label": "NICTA Australia", "shape": "dot", "size": 25, "title": "NICTA Australia"}, {"color": "#66CCFF", "id": "Tali Dekel", "label": "Tali Dekel", "shape": "dot", "size": 25, "title": "Tali Dekel"}, {"color": "#66CCFF", "id": "Best-Buddies Similarity", "label": "Best-Buddies Similarity", "shape": "dot", "size": 25, "title": "Best-Buddies Similarity"}, {"color": "#66CCFF", "id": "template matching", "label": "template matching", "shape": "dot", "size": 25, "title": "template matching"}, {"color": "#66CCFF", "id": "unconstrained environments", "label": "unconstrained environments", "shape": "dot", "size": 25, "title": "unconstrained environments"}, {"color": "#66CCFF", "id": "Best-Buddies Similarity (BBS)", "label": "Best-Buddies Similarity (BBS)", "shape": "dot", "size": 25, "title": "Best-Buddies Similarity (BBS)"}, {"color": "#66CCFF", "id": "Best-Buddies Similarity (BSS)", "label": "Best-Buddies Similarity (BSS)", "shape": "dot", "size": 25, "title": "Best-Buddies Similarity (BSS)"}, {"color": "#66CCFF", "id": "parameter-free", "label": "parameter-free", "shape": "dot", "size": 25, "title": "parameter-free"}, {"color": "#66CCFF", "id": "counting Best-Buddie Pairs (BBPs)", "label": "counting Best-Buddie Pairs (BBPs)", "shape": "dot", "size": 25, "title": "counting Best-Buddie Pairs (BBPs)"}, {"color": "#66CCFF", "id": "Best-Buddie Pairs (BBPs)", "label": "Best-Buddie Pairs (BBPs)", "shape": "dot", "size": 25, "title": "Best-Buddie Pairs (BBPs)"}, {"color": "#66CCFF", "id": "pairs of points", "label": "pairs of points", "shape": "dot", "size": 25, "title": "pairs of points"}, {"color": "#66CCFF", "id": "geometric deformations", "label": "geometric deformations", "shape": "dot", "size": 25, "title": "geometric deformations"}, {"color": "#66CCFF", "id": "background clutter", "label": "background clutter", "shape": "dot", "size": 25, "title": "background clutter"}, {"color": "#66CCFF", "id": "BBS", "label": "BBS", "shape": "dot", "size": 25, "title": "BBS"}, {"color": "#66CCFF", "id": "non-rigid object tracking", "label": "non-rigid object tracking", "shape": "dot", "size": 25, "title": "non-rigid object tracking"}, {"color": "#66CCFF", "id": "Comaniciu, D. et al.", "label": "Comaniciu, D. et al.", "shape": "dot", "size": 25, "title": "Comaniciu, D. et al."}, {"color": "#66CCFF", "id": "mean shift tracking", "label": "mean shift tracking", "shape": "dot", "size": 25, "title": "mean shift tracking"}, {"color": "#66CCFF", "id": "Rubner, Y. et al.", "label": "Rubner, Y. et al.", "shape": "dot", "size": 25, "title": "Rubner, Y. et al."}, {"color": "#66CCFF", "id": "Earth Mover\u0027s Distance", "label": "Earth Mover\u0027s Distance", "shape": "dot", "size": 25, "title": "Earth Mover\u0027s Distance"}, {"color": "#66CCFF", "id": "metric for image retrieval", "label": "metric for image retrieval", "shape": "dot", "size": 25, "title": "metric for image retrieval"}, {"color": "#66CCFF", "id": "consistent success", "label": "consistent success", "shape": "dot", "size": 25, "title": "consistent success"}, {"color": "#66CCFF", "id": "Similarity Measures", "label": "Similarity Measures", "shape": "dot", "size": 25, "title": "Similarity Measures"}, {"color": "#66CCFF", "id": "Outlier Robustness", "label": "Outlier Robustness", "shape": "dot", "size": 25, "title": "Outlier Robustness"}, {"color": "#66CCFF", "id": "Geometric Deformations", "label": "Geometric Deformations", "shape": "dot", "size": 25, "title": "Geometric Deformations"}, {"color": "#66CCFF", "id": "image comparison", "label": "image comparison", "shape": "dot", "size": 25, "title": "image comparison"}, {"color": "#66CCFF", "id": "Rubner et al. (2000)", "label": "Rubner et al. (2000)", "shape": "dot", "size": 25, "title": "Rubner et al. (2000)"}, {"color": "#66CCFF", "id": "Pele et al. (2008)", "label": "Pele et al. (2008)", "shape": "dot", "size": 25, "title": "Pele et al. (2008)"}, {"color": "#66CCFF", "id": "robust pattern matching", "label": "robust pattern matching", "shape": "dot", "size": 25, "title": "robust pattern matching"}, {"color": "#66CCFF", "id": "Simaov et al. (2008)", "label": "Simaov et al. (2008)", "shape": "dot", "size": 25, "title": "Simaov et al. (2008)"}, {"color": "#66CCFF", "id": "summarizing visual data", "label": "summarizing visual data", "shape": "dot", "size": 25, "title": "summarizing visual data"}, {"color": "#66CCFF", "id": "similarity measures", "label": "similarity measures", "shape": "dot", "size": 25, "title": "similarity measures"}, {"color": "#66CCFF", "id": "Hel-Or et al. (2014)", "label": "Hel-Or et al. (2014)", "shape": "dot", "size": 25, "title": "Hel-Or et al. (2014)"}, {"color": "#66CCFF", "id": "photometric invariant template matching", "label": "photometric invariant template matching", "shape": "dot", "size": 25, "title": "photometric invariant template matching"}, {"color": "#66CCFF", "id": "Tian et al. (2012)", "label": "Tian et al. (2012)", "shape": "dot", "size": 25, "title": "Tian et al. (2012)"}, {"color": "#66CCFF", "id": "estimating nonrigid image distortions", "label": "estimating nonrigid image distortions", "shape": "dot", "size": 25, "title": "estimating nonrigid image distortions"}, {"color": "#66CCFF", "id": "matching technique", "label": "matching technique", "shape": "dot", "size": 25, "title": "matching technique"}, {"color": "#66CCFF", "id": "nonrigid image distortions", "label": "nonrigid image distortions", "shape": "dot", "size": 25, "title": "nonrigid image distortions"}, {"color": "#66CCFF", "id": "image estimation", "label": "image estimation", "shape": "dot", "size": 25, "title": "image estimation"}, {"color": "#66CCFF", "id": "Tian \u0026 Narasimhan (2012)", "label": "Tian \u0026 Narasimhan (2012)", "shape": "dot", "size": 25, "title": "Tian \u0026 Narasimhan (2012)"}, {"color": "#66CCFF", "id": "Korman et al. (2013)", "label": "Korman et al. (2013)", "shape": "dot", "size": 25, "title": "Korman et al. (2013)"}, {"color": "#66CCFF", "id": "fast affine template matching algorithm", "label": "fast affine template matching algorithm", "shape": "dot", "size": 25, "title": "fast affine template matching algorithm"}, {"color": "#66CCFF", "id": "Wu et al. (2013)", "label": "Wu et al. (2013)", "shape": "dot", "size": 25, "title": "Wu et al. (2013)"}, {"color": "#66CCFF", "id": "online object tracking benchmark", "label": "online object tracking benchmark", "shape": "dot", "size": 25, "title": "online object tracking benchmark"}, {"color": "#66CCFF", "id": "Olson (2002)", "label": "Olson (2002)", "shape": "dot", "size": 25, "title": "Olson (2002)"}, {"color": "#66CCFF", "id": "maximum-likelihood image matching", "label": "maximum-likelihood image matching", "shape": "dot", "size": 25, "title": "maximum-likelihood image matching"}, {"color": "#66CCFF", "id": "Michael Rubinstein", "label": "Michael Rubinstein", "shape": "dot", "size": 25, "title": "Michael Rubinstein"}, {"color": "#66CCFF", "id": "Shai Avidan", "label": "Shai Avidan", "shape": "dot", "size": 25, "title": "Shai Avidan"}, {"color": "#66CCFF", "id": "Tel Aviv University", "label": "Tel Aviv University", "shape": "dot", "size": 25, "title": "Tel Aviv University"}, {"color": "#66CCFF", "id": "Nianyi Li", "label": "Nianyi Li", "shape": "dot", "size": 25, "title": "Nianyi Li"}, {"color": "#66CCFF", "id": "A Weighted Sparse Coding Framework for Saliency Detection", "label": "A Weighted Sparse Coding Framework for Saliency Detection", "shape": "dot", "size": 25, "title": "A Weighted Sparse Coding Framework for Saliency Detection"}, {"color": "#66CCFF", "id": "Bilin Sun", "label": "Bilin Sun", "shape": "dot", "size": 25, "title": "Bilin Sun"}, {"color": "#66CCFF", "id": "Jingyi", "label": "Jingyi", "shape": "dot", "size": 25, "title": "Jingyi"}, {"color": "#66CCFF", "id": "A Weighted Sparse Coding Framework", "label": "A Weighted Sparse Coding Framework", "shape": "dot", "size": 25, "title": "A Weighted Sparse Coding Framework"}, {"color": "#66CCFF", "id": "Jingyi Yu", "label": "Jingyi Yu", "shape": "dot", "size": 25, "title": "Jingyi Yu"}, {"color": "#66CCFF", "id": "Saliency Detection", "label": "Saliency Detection", "shape": "dot", "size": 25, "title": "Saliency Detection"}, {"color": "#66CCFF", "id": "avidan@eng.tau.ac.il", "label": "avidan@eng.tau.ac.il", "shape": "dot", "size": 25, "title": "avidan@eng.tau.ac.il"}, {"color": "#66CCFF", "id": "billf@mit.edu", "label": "billf@mit.edu", "shape": "dot", "size": 25, "title": "billf@mit.edu"}, {"color": "#66CCFF", "id": "Li_A_Weighted_Sparse_2015_CVPR_paper", "label": "Li_A_Weighted_Sparse_2015_CVPR_paper", "shape": "dot", "size": 25, "title": "Li_A_Weighted_Sparse_2015_CVPR_paper"}, {"color": "#66CCFF", "id": "pdf", "label": "pdf", "shape": "dot", "size": 25, "title": "pdf"}, {"color": "#66CCFF", "id": "salience detection", "label": "salience detection", "shape": "dot", "size": 25, "title": "salience detection"}, {"color": "#66CCFF", "id": "high-dimensional datasets", "label": "high-dimensional datasets", "shape": "dot", "size": 25, "title": "high-dimensional datasets"}, {"color": "#66CCFF", "id": "solution frameworks", "label": "solution frameworks", "shape": "dot", "size": 25, "title": "solution frameworks"}, {"color": "#66CCFF", "id": "uni\ufb01ed saliency detection framework", "label": "uni\ufb01ed saliency detection framework", "shape": "dot", "size": 25, "title": "uni\ufb01ed saliency detection framework"}, {"color": "#66CCFF", "id": "heterogenous types of input data", "label": "heterogenous types of input data", "shape": "dot", "size": 25, "title": "heterogenous types of input data"}, {"color": "#66CCFF", "id": "dictionaries", "label": "dictionaries", "shape": "dot", "size": 25, "title": "dictionaries"}, {"color": "#66CCFF", "id": "data-speci\ufb01c features", "label": "data-speci\ufb01c features", "shape": "dot", "size": 25, "title": "data-speci\ufb01c features"}, {"color": "#66CCFF", "id": "primitive saliency dictionary", "label": "primitive saliency dictionary", "shape": "dot", "size": 25, "title": "primitive saliency dictionary"}, {"color": "#66CCFF", "id": "dictionary", "label": "dictionary", "shape": "dot", "size": 25, "title": "dictionary"}, {"color": "#66CCFF", "id": "state-of-the-art solution", "label": "state-of-the-art solution", "shape": "dot", "size": 25, "title": "state-of-the-art solution"}, {"color": "#66CCFF", "id": "2D, 3D and 4D", "label": "2D, 3D and 4D", "shape": "dot", "size": 25, "title": "2D, 3D and 4D"}, {"color": "#66CCFF", "id": "Sparse Coding", "label": "Sparse Coding", "shape": "dot", "size": 25, "title": "Sparse Coding"}, {"color": "#66CCFF", "id": "Liu et al. (2011)", "label": "Liu et al. (2011)", "shape": "dot", "size": 25, "title": "Liu et al. (2011)"}, {"color": "#66CCFF", "id": "Achanta et al. (2012)", "label": "Achanta et al. (2012)", "shape": "dot", "size": 25, "title": "Achanta et al. (2012)"}, {"color": "#66CCFF", "id": "Borji \u0026 Itti (2012)", "label": "Borji \u0026 Itti (2012)", "shape": "dot", "size": 25, "title": "Borji \u0026 Itti (2012)"}, {"color": "#66CCFF", "id": "Patch Rarities", "label": "Patch Rarities", "shape": "dot", "size": 25, "title": "Patch Rarities"}, {"color": "#66CCFF", "id": "Borji, Sihite, \u0026 Itti (2012)", "label": "Borji, Sihite, \u0026 Itti (2012)", "shape": "dot", "size": 25, "title": "Borji, Sihite, \u0026 Itti (2012)"}, {"color": "#66CCFF", "id": "Salient Object Detection Benchmark", "label": "Salient Object Detection Benchmark", "shape": "dot", "size": 25, "title": "Salient Object Detection Benchmark"}, {"color": "#66CCFF", "id": "2D, 3D and 4D data", "label": "2D, 3D and 4D data", "shape": "dot", "size": 25, "title": "2D, 3D and 4D data"}, {"color": "#66CCFF", "id": "Reynolds \u0026 Desimone", "label": "Reynolds \u0026 Desimone", "shape": "dot", "size": 25, "title": "Reynolds \u0026 Desimone"}, {"color": "#66CCFF", "id": "V4", "label": "V4", "shape": "dot", "size": 25, "title": "V4"}, {"color": "#66CCFF", "id": "attention", "label": "attention", "shape": "dot", "size": 25, "title": "attention"}, {"color": "#66CCFF", "id": "Nothdurft", "label": "Nothdurft", "shape": "dot", "size": 25, "title": "Nothdurft"}, {"color": "#66CCFF", "id": "additivity across dimensions", "label": "additivity across dimensions", "shape": "dot", "size": 25, "title": "additivity across dimensions"}, {"color": "#66CCFF", "id": "Perazzi et al.", "label": "Perazzi et al.", "shape": "dot", "size": 25, "title": "Perazzi et al."}, {"color": "#66CCFF", "id": "salience filters", "label": "salience filters", "shape": "dot", "size": 25, "title": "salience filters"}, {"color": "#66CCFF", "id": "contrast based filtering", "label": "contrast based filtering", "shape": "dot", "size": 25, "title": "contrast based filtering"}, {"color": "#66CCFF", "id": "Cheng et al.", "label": "Cheng et al.", "shape": "dot", "size": 25, "title": "Cheng et al."}, {"color": "#66CCFF", "id": "Borji et al.", "label": "Borji et al.", "shape": "dot", "size": 25, "title": "Borji et al."}, {"color": "#66CCFF", "id": "cal and global patch rarities", "label": "cal and global patch rarities", "shape": "dot", "size": 25, "title": "cal and global patch rarities"}, {"color": "#66CCFF", "id": "Movahedi \u0026 Elder", "label": "Movahedi \u0026 Elder", "shape": "dot", "size": 25, "title": "Movahedi \u0026 Elder"}, {"color": "#66CCFF", "id": "salience from feature contrast", "label": "salience from feature contrast", "shape": "dot", "size": 25, "title": "salience from feature contrast"}, {"color": "#66CCFF", "id": "Neuron", "label": "Neuron", "shape": "dot", "size": 25, "title": "Neuron"}, {"color": "#66CCFF", "id": "Reynolds \u0026 Desimnone", "label": "Reynolds \u0026 Desimnone", "shape": "dot", "size": 25, "title": "Reynolds \u0026 Desimnone"}, {"color": "#66CCFF", "id": "Itti \u0026 Koch", "label": "Itti \u0026 Koch", "shape": "dot", "size": 25, "title": "Itti \u0026 Koch"}, {"color": "#66CCFF", "id": "Nature Reviews Neuroscience", "label": "Nature Reviews Neuroscience", "shape": "dot", "size": 25, "title": "Nature Reviews Neuroscience"}, {"color": "#66CCFF", "id": "Reynolds", "label": "Reynolds", "shape": "dot", "size": 25, "title": "Reynolds"}, {"color": "#66CCFF", "id": "University of Delaware", "label": "University of Delaware", "shape": "dot", "size": 25, "title": "University of Delaware"}, {"color": "#66CCFF", "id": "Souvenir", "label": "Souvenir", "shape": "dot", "size": 25, "title": "Souvenir"}, {"color": "#66CCFF", "id": "Robust Regression on Image Manifolds", "label": "Robust Regression on Image Manifolds", "shape": "dot", "size": 25, "title": "Robust Regression on Image Manifolds"}, {"color": "#66CCFF", "id": "nianyi@eecis.udel.edu", "label": "nianyi@eecis.udel.edu", "shape": "dot", "size": 25, "title": "nianyi@eecis.udel.edu"}, {"color": "#66CCFF", "id": "Sun", "label": "Sun", "shape": "dot", "size": 25, "title": "Sun"}, {"color": "#66CCFF", "id": "sunbilin@eecis.udel.edu", "label": "sunbilin@eecis.udel.edu", "shape": "dot", "size": 25, "title": "sunbilin@eecis.udel.edu"}, {"color": "#66CCFF", "id": "Yu", "label": "Yu", "shape": "dot", "size": 25, "title": "Yu"}, {"color": "#66CCFF", "id": "yu@eecis.udel.edu", "label": "yu@eecis.udel.edu", "shape": "dot", "size": 25, "title": "yu@eecis.udel.edu"}, {"color": "#66CCFF", "id": "robust regression method", "label": "robust regression method", "shape": "dot", "size": 25, "title": "robust regression method"}, {"color": "#66CCFF", "id": "non-parametric", "label": "non-parametric", "shape": "dot", "size": 25, "title": "non-parametric"}, {"color": "#66CCFF", "id": "mis-labeled examples", "label": "mis-labeled examples", "shape": "dot", "size": 25, "title": "mis-labeled examples"}, {"color": "#66CCFF", "id": "ordered labels", "label": "ordered labels", "shape": "dot", "size": 25, "title": "ordered labels"}, {"color": "#66CCFF", "id": "superior denois-ing accuracy", "label": "superior denois-ing accuracy", "shape": "dot", "size": 25, "title": "superior denois-ing accuracy"}, {"color": "#66CCFF", "id": "label corruption levels", "label": "label corruption levels", "shape": "dot", "size": 25, "title": "label corruption levels"}, {"color": "#66CCFF", "id": "80%", "label": "80%", "shape": "dot", "size": 25, "title": "80%"}, {"color": "#66CCFF", "id": "image labels", "label": "image labels", "shape": "dot", "size": 25, "title": "image labels"}, {"color": "#66CCFF", "id": "associated images", "label": "associated images", "shape": "dot", "size": 25, "title": "associated images"}, {"color": "#66CCFF", "id": "Ordered Labels", "label": "Ordered Labels", "shape": "dot", "size": 25, "title": "Ordered Labels"}, {"color": "#66CCFF", "id": "Ordinal Data", "label": "Ordinal Data", "shape": "dot", "size": 25, "title": "Ordinal Data"}, {"color": "#66CCFF", "id": "Label Denoising", "label": "Label Denoising", "shape": "dot", "size": 25, "title": "Label Denoising"}, {"color": "#66CCFF", "id": "accuracy of image labels", "label": "accuracy of image labels", "shape": "dot", "size": 25, "title": "accuracy of image labels"}, {"color": "#66CCFF", "id": "locally linear embedding technique", "label": "locally linear embedding technique", "shape": "dot", "size": 25, "title": "locally linear embedding technique"}, {"color": "#66CCFF", "id": "gigantic image collections", "label": "gigantic image collections", "shape": "dot", "size": 25, "title": "gigantic image collections"}, {"color": "#66CCFF", "id": "Building Rome in a day", "label": "Building Rome in a day", "shape": "dot", "size": 25, "title": "Building Rome in a day"}, {"color": "#66CCFF", "id": "IEEE International Conference on Computer Visions", "label": "IEEE International Conference on Computer Visions", "shape": "dot", "size": 25, "title": "IEEE International Conference on Computer Visions"}, {"color": "#66CCFF", "id": "high-dimensional data", "label": "high-dimensional data", "shape": "dot", "size": 25, "title": "high-dimensional data"}, {"color": "#66CCFF", "id": "R. C. Bolles", "label": "R. C. Bolles", "shape": "dot", "size": 25, "title": "R. C. Bolles"}, {"color": "#66CCFF", "id": "C.-C. Chang", "label": "C.-C. Chang", "shape": "dot", "size": 25, "title": "C.-C. Chang"}, {"color": "#66CCFF", "id": "C.-J. Lin", "label": "C.-J. Lin", "shape": "dot", "size": 25, "title": "C.-J. Lin"}, {"color": "#66CCFF", "id": "USAC", "label": "USAC", "shape": "dot", "size": 25, "title": "USAC"}, {"color": "#66CCFF", "id": "R. Raguram", "label": "R. Raguram", "shape": "dot", "size": 25, "title": "R. Raguram"}, {"color": "#66CCFF", "id": "l-curve", "label": "l-curve", "shape": "dot", "size": 25, "title": "l-curve"}, {"color": "#66CCFF", "id": "Analysis of discrete ill-posed problems", "label": "Analysis of discrete ill-posed problems", "shape": "dot", "size": 25, "title": "Analysis of discrete ill-posed problems"}, {"color": "#66CCFF", "id": "P. C. Hansen", "label": "P. C. Hansen", "shape": "dot", "size": 25, "title": "P. C. Hansen"}, {"color": "#66CCFF", "id": "Internet photo collections", "label": "Internet photo collections", "shape": "dot", "size": 25, "title": "Internet photo collections"}, {"color": "#66CCFF", "id": "Modeling the world", "label": "Modeling the world", "shape": "dot", "size": 25, "title": "Modeling the world"}, {"color": "#66CCFF", "id": "N. Snavely", "label": "N. Snavely", "shape": "dot", "size": 25, "title": "N. Snavely"}, {"color": "#66CCFF", "id": "SIAM review", "label": "SIAM review", "shape": "dot", "size": 25, "title": "SIAM review"}, {"color": "#66CCFF", "id": "Kai Han", "label": "Kai Han", "shape": "dot", "size": 25, "title": "Kai Han"}, {"color": "#66CCFF", "id": "A Fixed Viewpoint Approach", "label": "A Fixed Viewpoint Approach", "shape": "dot", "size": 25, "title": "A Fixed Viewpoint Approach"}, {"color": "#66CCFF", "id": "Kwan-Yee K. Wong", "label": "Kwan-Yee K. Wong", "shape": "dot", "size": 25, "title": "Kwan-Yee K. Wong"}, {"color": "#66CCFF", "id": "Miaomiao Liu", "label": "Miaomiao Liu", "shape": "dot", "size": 25, "title": "Miaomiao Liu"}, {"color": "#66CCFF", "id": "A Fixed View Point Approach", "label": "A Fixed View Point Approach", "shape": "dot", "size": 25, "title": "A Fixed View Point Approach"}, {"color": "#66CCFF", "id": "surface shape reconstruction problem", "label": "surface shape reconstruction problem", "shape": "dot", "size": 25, "title": "surface shape reconstruction problem"}, {"color": "#66CCFF", "id": "transparent object reconstruction", "label": "transparent object reconstruction", "shape": "dot", "size": 25, "title": "transparent object reconstruction"}, {"color": "#66CCFF", "id": "light path triangulation", "label": "light path triangulation", "shape": "dot", "size": 25, "title": "light path triangulation"}, {"color": "#66CCFF", "id": "unknown refractive indices", "label": "unknown refractive indices", "shape": "dot", "size": 25, "title": "unknown refractive indices"}, {"color": "#66CCFF", "id": "complex transparent objects", "label": "complex transparent objects", "shape": "dot", "size": 25, "title": "complex transparent objects"}, {"color": "#66CCFF", "id": "M. Ben-Ezra and S. K. Nayr", "label": "M. Ben-Ezra and S. K. Nayr", "shape": "dot", "size": 25, "title": "M. Ben-Ezra and S. K. Nayr"}, {"color": "#66CCFF", "id": "transparency analysis", "label": "transparency analysis", "shape": "dot", "size": 25, "title": "transparency analysis"}, {"color": "#66CCFF", "id": "G. Eren et al.", "label": "G. Eren et al.", "shape": "dot", "size": 25, "title": "G. Eren et al."}, {"color": "#66CCFF", "id": "shape estimation", "label": "shape estimation", "shape": "dot", "size": 25, "title": "shape estimation"}, {"color": "#66CCFF", "id": "local surface heating", "label": "local surface heating", "shape": "dot", "size": 25, "title": "local surface heating"}, {"color": "#66CCFF", "id": "refractive photo-light-path", "label": "refractive photo-light-path", "shape": "dot", "size": 25, "title": "refractive photo-light-path"}, {"color": "#66CCFF", "id": "feasibility", "label": "feasibility", "shape": "dot", "size": 25, "title": "feasibility"}, {"color": "#66CCFF", "id": "Fischler and Bolles", "label": "Fischler and Bolles", "shape": "dot", "size": 25, "title": "Fischler and Bolles"}, {"color": "#66CCFF", "id": "Hata et al.", "label": "Hata et al.", "shape": "dot", "size": 25, "title": "Hata et al."}, {"color": "#66CCFF", "id": "genetic algorithm", "label": "genetic algorithm", "shape": "dot", "size": 25, "title": "genetic algorithm"}, {"color": "#66CCFF", "id": "shape extraction", "label": "shape extraction", "shape": "dot", "size": 25, "title": "shape extraction"}, {"color": "#66CCFF", "id": "Ihrke et al. (2005)", "label": "Ihrke et al. (2005)", "shape": "dot", "size": 25, "title": "Ihrke et al. (2005)"}, {"color": "#66CCFF", "id": "geometry reconstruction", "label": "geometry reconstruction", "shape": "dot", "size": 25, "title": "geometry reconstruction"}, {"color": "#66CCFF", "id": "dynamic environments", "label": "dynamic environments", "shape": "dot", "size": 25, "title": "dynamic environments"}, {"color": "#66CCFF", "id": "Ihrke et al. (2008)", "label": "Ihrke et al. (2008)", "shape": "dot", "size": 25, "title": "Ihrke et al. (2008)"}, {"color": "#66CCFF", "id": "Transparent objects", "label": "Transparent objects", "shape": "dot", "size": 25, "title": "Transparent objects"}, {"color": "#66CCFF", "id": "The University of Hokkaido", "label": "The University of Hokkaido", "shape": "dot", "size": 25, "title": "The University of Hokkaido"}, {"color": "#66CCFF", "id": "CECS, ANU", "label": "CECS, ANU", "shape": "dot", "size": 25, "title": "CECS, ANU"}, {"color": "#66CCFF", "id": "Benjamin Allain", "label": "Benjamin Allain", "shape": "dot", "size": 25, "title": "Benjamin Allain"}, {"color": "#66CCFF", "id": "An Efficient Volumetric Framework for Shape Tracking", "label": "An Efficient Volumetric Framework for Shape Tracking", "shape": "dot", "size": 25, "title": "An Efficient Volumetric Framework for Shape Tracking"}, {"color": "#66CCFF", "id": "Jean-S\u00e9batian Franco", "label": "Jean-S\u00e9batian Franco", "shape": "dot", "size": 25, "title": "Jean-S\u00e9batian Franco"}, {"color": "#66CCFF", "id": "Edmond Boyer", "label": "Edmond Boyer", "shape": "dot", "size": 25, "title": "Edmond Boyer"}, {"color": "#66CCFF", "id": "Allain_An_Efficient_Volumetric_2015_CVPR_paper.pdf", "label": "Allain_An_Efficient_Volumetric_2015_CVPR_paper.pdf", "shape": "dot", "size": 25, "title": "Allain_An_Efficient_Volumetric_2015_CVPR_paper.pdf"}, {"color": "#66CCFF", "id": "3D shape motion recovery", "label": "3D shape motion recovery", "shape": "dot", "size": 25, "title": "3D shape motion recovery"}, {"color": "#66CCFF", "id": "surface-based strategies", "label": "surface-based strategies", "shape": "dot", "size": 25, "title": "surface-based strategies"}, {"color": "#66CCFF", "id": "observations define several feasible surfaces", "label": "observations define several feasible surfaces", "shape": "dot", "size": 25, "title": "observations define several feasible surfaces"}, {"color": "#66CCFF", "id": "this work", "label": "this work", "shape": "dot", "size": 25, "title": "this work"}, {"color": "#66CCFF", "id": "volumetric shape parametrization", "label": "volumetric shape parametrization", "shape": "dot", "size": 25, "title": "volumetric shape parametrization"}, {"color": "#66CCFF", "id": "Centroidal Voronoi Tesselations (CVT)", "label": "Centroidal Voronoi Tesselations (CVT)", "shape": "dot", "size": 25, "title": "Centroidal Voronoi Tesselations (CVT)"}, {"color": "#66CCFF", "id": "volumetric deformation model", "label": "volumetric deformation model", "shape": "dot", "size": 25, "title": "volumetric deformation model"}, {"color": "#66CCFF", "id": "hybrid multi-camera and marker-based capture dataset", "label": "hybrid multi-camera and marker-based capture dataset", "shape": "dot", "size": 25, "title": "hybrid multi-camera and marker-based capture dataset"}, {"color": "#66CCFF", "id": "improved precision and robustness", "label": "improved precision and robustness", "shape": "dot", "size": 25, "title": "improved precision and robustness"}, {"color": "#66CCFF", "id": "volumetric shape tracking", "label": "volumetric shape tracking", "shape": "dot", "size": 25, "title": "volumetric shape tracking"}, {"color": "#66CCFF", "id": "Volumetric Shape Tracking", "label": "Volumetric Shape Tracking", "shape": "dot", "size": 25, "title": "Volumetric Shape Tracking"}, {"color": "#66CCFF", "id": "Centroidal Voronoi Tesselations", "label": "Centroidal Voronoi Tesselations", "shape": "dot", "size": 25, "title": "Centroidal Voronoi Tesselations"}, {"color": "#66CCFF", "id": "Dynamic Shape Capture", "label": "Dynamic Shape Capture", "shape": "dot", "size": 25, "title": "Dynamic Shape Capture"}, {"color": "#66CCFF", "id": "Motion Estimation", "label": "Motion Estimation", "shape": "dot", "size": 25, "title": "Motion Estimation"}, {"color": "#66CCFF", "id": "Surface-based Methods", "label": "Surface-based Methods", "shape": "dot", "size": 25, "title": "Surface-based Methods"}, {"color": "#66CCFF", "id": "Volume-based Methods", "label": "Volume-based Methods", "shape": "dot", "size": 25, "title": "Volume-based Methods"}, {"color": "#66CCFF", "id": "Alexa et al. (2000)", "label": "Alexa et al. (2000)", "shape": "dot", "size": 25, "title": "Alexa et al. (2000)"}, {"color": "#66CCFF", "id": "As-rigid-as-possible shape interpolation", "label": "As-rigid-as-possible shape interpolation", "shape": "dot", "size": 25, "title": "As-rigid-as-possible shape interpolation"}, {"color": "#66CCFF", "id": "Allain et al. (2014)", "label": "Allain et al. (2014)", "shape": "dot", "size": 25, "title": "Allain et al. (2014)"}, {"color": "#66CCFF", "id": "On mean pose and variability of 3d deformable models", "label": "On mean pose and variability of 3d deformable models", "shape": "dot", "size": 25, "title": "On mean pose and variability of 3d deformable models"}, {"color": "#66CCFF", "id": "Ballan \u0026 Cortelazzo (2008)", "label": "Ballan \u0026 Cortelazzo (2008)", "shape": "dot", "size": 25, "title": "Ballan \u0026 Cortelazzo (2008)"}, {"color": "#66CCFF", "id": "Marker-less motion capture of skinned models", "label": "Marker-less motion capture of skinned models", "shape": "dot", "size": 25, "title": "Marker-less motion capture of skinned models"}, {"color": "#66CCFF", "id": "Bishop (2006)", "label": "Bishop (2006)", "shape": "dot", "size": 25, "title": "Bishop (2006)"}, {"color": "#66CCFF", "id": "Pattern Recognition and Machine Learning", "label": "Pattern Recognition and Machine Learning", "shape": "dot", "size": 25, "title": "Pattern Recognition and Machine Learning"}, {"color": "#66CCFF", "id": "Botsu et al. (2007)", "label": "Botsu et al. (2007)", "shape": "dot", "size": 25, "title": "Botsu et al. (2007)"}, {"color": "#66CCFF", "id": "Adaptive space deformations based on rigid cells", "label": "Adaptive space deformations based on rigid cells", "shape": "dot", "size": 25, "title": "Adaptive space deformations based on rigid cells"}, {"color": "#66CCFF", "id": "Botsu, M.", "label": "Botsu, M.", "shape": "dot", "size": 25, "title": "Botsu, M."}, {"color": "#66CCFF", "id": "Cagniart, C.", "label": "Cagniart, C.", "shape": "dot", "size": 25, "title": "Cagniart, C."}, {"color": "#66CCFF", "id": "Free-form mesh tracking: a patch-based approach", "label": "Free-form mesh tracking: a patch-based approach", "shape": "dot", "size": 25, "title": "Free-form mesh tracking: a patch-based approach"}, {"color": "#66CCFF", "id": "Probabilistic deformable surface tracking from multiple videos", "label": "Probabilistic deformable surface tracking from multiple videos", "shape": "dot", "size": 25, "title": "Probabilistic deformable surface tracking from multiple videos"}, {"color": "#66CCFF", "id": "de Aguiar, E.", "label": "de Aguiar, E.", "shape": "dot", "size": 25, "title": "de Aguiar, E."}, {"color": "#66CCFF", "id": "Performance capture from sparse multi-view video", "label": "Performance capture from sparse multi-view video", "shape": "dot", "size": 25, "title": "Performance capture from sparse multi-view video"}, {"color": "#66CCFF", "id": "de Aguliar, E.", "label": "de Aguliar, E.", "shape": "dot", "size": 25, "title": "de Aguliar, E."}, {"color": "#66CCFF", "id": "Marker-less deformable mesh tracking for human shape and motion capture", "label": "Marker-less deformable mesh tracking for human shape and motion capture", "shape": "dot", "size": 25, "title": "Marker-less deformable mesh tracking for human shape and motion capture"}, {"color": "#66CCFF", "id": "Inria Grenoble Rh\u02c6one-Alpes - LJK", "label": "Inria Grenoble Rh\u02c6one-Alpes - LJK", "shape": "dot", "size": 25, "title": "Inria Grenoble Rh\u02c6one-Alpes - LJK"}, {"color": "#66CCFF", "id": "Comput. Graph. Forum", "label": "Comput. Graph. Forum", "shape": "dot", "size": 25, "title": "Comput. Graph. Forum"}, {"color": "#66CCFF", "id": "Inria Grenoble Rh\u02c6one- Alpes", "label": "Inria Grenoble Rh\u02c6one- Alpes", "shape": "dot", "size": 25, "title": "Inria Grenoble Rh\u02c6one- Alpes"}, {"color": "#66CCFF", "id": "LJK", "label": "LJK", "shape": "dot", "size": 25, "title": "LJK"}, {"color": "#66CCFF", "id": "Benjamin Allaine", "label": "Benjamin Allaine", "shape": "dot", "size": 25, "title": "Benjamin Allaine"}, {"color": "#66CCFF", "id": "firstname.lastname@inria.fr", "label": "firstname.lastname@inria.fr", "shape": "dot", "size": 25, "title": "firstname.lastname@inria.fr"}, {"color": "#66CCFF", "id": "Maximum likelihood", "label": "Maximum likelihood", "shape": "dot", "size": 25, "title": "Maximum likelihood"}, {"color": "#66CCFF", "id": "em algorithm", "label": "em algorithm", "shape": "dot", "size": 25, "title": "em algorithm"}, {"color": "#66CCFF", "id": "Maximum likelihood from incomplete data", "label": "Maximum likelihood from incomplete data", "shape": "dot", "size": 25, "title": "Maximum likelihood from incomplete data"}, {"color": "#66CCFF", "id": "Journal of the Royal Statistical Society, series B", "label": "Journal of the Royal Statistical Society, series B", "shape": "dot", "size": 25, "title": "Journal of the Royal Statistical Society, series B"}, {"color": "#66CCFF", "id": "Maximum likelihood from incomplete data via the em algorithm", "label": "Maximum likelihood from incomplete data via the em algorithm", "shape": "dot", "size": 25, "title": "Maximum likelihood from incomplete data via the em algorithm"}, {"color": "#66CCFF", "id": "Jean-S\u00e9bastien Franco", "label": "Jean-S\u00e9bastien Franco", "shape": "dot", "size": 25, "title": "Jean-S\u00e9bastien Franco"}, {"color": "#66CCFF", "id": "Inria Grenoble Rh\u02c6one-Alpes - LHK", "label": "Inria Grenoble Rh\u02c6one-Alpes - LHK", "shape": "dot", "size": 25, "title": "Inria Grenoble Rh\u02c6one-Alpes - LHK"}, {"color": "#66CCFF", "id": "Yi-Hsuan Tsai", "label": "Yi-Hsuan Tsai", "shape": "dot", "size": 25, "title": "Yi-Hsuan Tsai"}, {"color": "#66CCFF", "id": "Adaptive Region Pooling for Object Detection", "label": "Adaptive Region Pooling for Object Detection", "shape": "dot", "size": 25, "title": "Adaptive Region Pooling for Object Detection"}, {"color": "#66CCFF", "id": "Onur C. Hamsici", "label": "Onur C. Hamsici", "shape": "dot", "size": 25, "title": "Onur C. Hamsici"}, {"color": "#66CCFF", "id": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Tsai_Adaptive_Region_Pooling_2015_CVPR_supplemental.pdf", "label": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Tsai_Adaptive_Region_Pooling_2015_CVPR_supplemental.pdf", "shape": "dot", "size": 25, "title": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Tsai_Adaptive_Region_Pooling_2015_CVPR_supplemental.pdf"}, {"color": "#66CCFF", "id": "Tsai_Adaptive_Region_Pooling_2015_CVPR_supplemental", "label": "Tsai_Adaptive_Region_Pooling_2015_CVPR_supplemental", "shape": "dot", "size": 25, "title": "Tsai_Adaptive_Region_Pooling_2015_CVPR_supplemental"}, {"color": "#66CCFF", "id": "Adaptive Region Pooling", "label": "Adaptive Region Pooling", "shape": "dot", "size": 25, "title": "Adaptive Region Pooling"}, {"color": "#66CCFF", "id": "discriminative object parts", "label": "discriminative object parts", "shape": "dot", "size": 25, "title": "discriminative object parts"}, {"color": "#66CCFF", "id": "Adaptive Region Processing", "label": "Adaptive Region Processing", "shape": "dot", "size": 25, "title": "Adaptive Region Processing"}, {"color": "#66CCFF", "id": "improving object detection", "label": "improving object detection", "shape": "dot", "size": 25, "title": "improving object detection"}, {"color": "#66CCFF", "id": "representative parts", "label": "representative parts", "shape": "dot", "size": 25, "title": "representative parts"}, {"color": "#66CCFF", "id": "detected objects", "label": "detected objects", "shape": "dot", "size": 25, "title": "detected objects"}, {"color": "#66CCFF", "id": "challenging conditions", "label": "challenging conditions", "shape": "dot", "size": 25, "title": "challenging conditions"}, {"color": "#66CCFF", "id": "effectiveness of ARP", "label": "effectiveness of ARP", "shape": "dot", "size": 25, "title": "effectiveness of ARP"}, {"color": "#66CCFF", "id": "ARP", "label": "ARP", "shape": "dot", "size": 25, "title": "ARP"}, {"color": "#66CCFF", "id": "ESVM", "label": "ESVM", "shape": "dot", "size": 25, "title": "ESVM"}, {"color": "#66CCFF", "id": "Ensemble of exemplar-svms", "label": "Ensemble of exemplar-svms", "shape": "dot", "size": 25, "title": "Ensemble of exemplar-svms"}, {"color": "#66CCFF", "id": "Keypoint Transfer", "label": "Keypoint Transfer", "shape": "dot", "size": 25, "title": "Keypoint Transfer"}, {"color": "#66CCFF", "id": "Object Parts Discovery", "label": "Object Parts Discovery", "shape": "dot", "size": 25, "title": "Object Parts Discovery"}, {"color": "#66CCFF", "id": "Long-term Recurrent Convolutional Networks", "label": "Long-term Recurrent Convolutional Networks", "shape": "dot", "size": 25, "title": "Long-term Recurrent Convolutional Networks"}, {"color": "#66CCFF", "id": "Visual Description", "label": "Visual Description", "shape": "dot", "size": 25, "title": "Visual Description"}, {"color": "#66CCFF", "id": "Jeff Donahue", "label": "Jeff Donahue", "shape": "dot", "size": 25, "title": "Jeff Donahue"}, {"color": "#66CCFF", "id": "Lisa Anne Hendricks", "label": "Lisa Anne Hendricks", "shape": "dot", "size": 25, "title": "Lisa Anne Hendricks"}, {"color": "#66CCFF", "id": "UC Merced", "label": "UC Merced", "shape": "dot", "size": 25, "title": "UC Merced"}, {"color": "#66CCFF", "id": "Qualcomm Research", "label": "Qualcomm Research", "shape": "dot", "size": 25, "title": "Qualcomm Research"}, {"color": "#66CCFF", "id": "Sergio Guadarrama", "label": "Sergio Guadarrama", "shape": "dot", "size": 25, "title": "Sergio Guadarrama"}, {"color": "#66CCFF", "id": "Donahue_Long-Term_Recurrent_Convolutional_2015_CVPR_paper", "label": "Donahue_Long-Term_Recurrent_Convolutional_2015_CVPR_paper", "shape": "dot", "size": 25, "title": "Donahue_Long-Term_Recurrent_Convolutional_2015_CVPR_paper"}, {"color": "#66CCFF", "id": "Marcus Rohrbach", "label": "Marcus Rohrbach", "shape": "dot", "size": 25, "title": "Marcus Rohrbach"}, {"color": "#66CCFF", "id": "Subhashini Venugopalan", "label": "Subhashini Venugopalan", "shape": "dot", "size": 25, "title": "Subhashini Venugopalan"}, {"color": "#66CCFF", "id": "Kate Saenko", "label": "Kate Saenko", "shape": "dot", "size": 25, "title": "Kate Saenko"}, {"color": "#66CCFF", "id": "Trevor Darrell", "label": "Trevor Darrell", "shape": "dot", "size": 25, "title": "Trevor Darrell"}, {"color": "#66CCFF", "id": "Visual Features", "label": "Visual Features", "shape": "dot", "size": 25, "title": "Visual Features"}, {"color": "#66CCFF", "id": "Predictions", "label": "Predictions", "shape": "dot", "size": 25, "title": "Predictions"}, {"color": "#66CCFF", "id": "Visual Input", "label": "Visual Input", "shape": "dot", "size": 25, "title": "Visual Input"}, {"color": "#66CCFF", "id": "Models", "label": "Models", "shape": "dot", "size": 25, "title": "Models"}, {"color": "#66CCFF", "id": "recurrent", "label": "recurrent", "shape": "dot", "size": 25, "title": "recurrent"}, {"color": "#66CCFF", "id": "temporally deep", "label": "temporally deep", "shape": "dot", "size": 25, "title": "temporally deep"}, {"color": "#66CCFF", "id": "recurrent convolutional", "label": "recurrent convolutional", "shape": "dot", "size": 25, "title": "recurrent convolutional"}, {"color": "#66CCFF", "id": "large-scale visual learning", "label": "large-scale visual learning", "shape": "dot", "size": 25, "title": "large-scale visual learning"}, {"color": "#66CCFF", "id": "video recognition tasks", "label": "video recognition tasks", "shape": "dot", "size": 25, "title": "video recognition tasks"}, {"color": "#66CCFF", "id": "image description", "label": "image description", "shape": "dot", "size": 25, "title": "image description"}, {"color": "#66CCFF", "id": "video narration challenges", "label": "video narration challenges", "shape": "dot", "size": 25, "title": "video narration challenges"}, {"color": "#66CCFF", "id": "recurrent convolutional models", "label": "recurrent convolutional models", "shape": "dot", "size": 25, "title": "recurrent convolutional models"}, {"color": "#66CCFF", "id": "doubly deep", "label": "doubly deep", "shape": "dot", "size": 25, "title": "doubly deep"}, {"color": "#66CCFF", "id": "compositional", "label": "compositional", "shape": "dot", "size": 25, "title": "compositional"}, {"color": "#66CCFF", "id": "complex target concepts", "label": "complex target concepts", "shape": "dot", "size": 25, "title": "complex target concepts"}, {"color": "#66CCFF", "id": "limited training data", "label": "limited training data", "shape": "dot", "size": 25, "title": "limited training data"}, {"color": "#66CCFF", "id": "network state updates", "label": "network state updates", "shape": "dot", "size": 25, "title": "network state updates"}, {"color": "#66CCFF", "id": "long-term dependencies", "label": "long-term dependencies", "shape": "dot", "size": 25, "title": "long-term dependencies"}, {"color": "#66CCFF", "id": "long-term RNN models", "label": "long-term RNN models", "shape": "dot", "size": 25, "title": "long-term RNN models"}, {"color": "#66CCFF", "id": "variable length outputs", "label": "variable length outputs", "shape": "dot", "size": 25, "title": "variable length outputs"}, {"color": "#66CCFF", "id": "complex temporal dynamics", "label": "complex temporal dynamics", "shape": "dot", "size": 25, "title": "complex temporal dynamics"}, {"color": "#66CCFF", "id": "backpropagation", "label": "backpropagation", "shape": "dot", "size": 25, "title": "backpropagation"}, {"color": "#66CCFF", "id": "recurrent long-term models", "label": "recurrent long-term models", "shape": "dot", "size": 25, "title": "recurrent long-term models"}, {"color": "#66CCFF", "id": "visual convnet models", "label": "visual convnet models", "shape": "dot", "size": 25, "title": "visual convnet models"}, {"color": "#66CCFF", "id": "temporal dynamics", "label": "temporal dynamics", "shape": "dot", "size": 25, "title": "temporal dynamics"}, {"color": "#66CCFF", "id": "convolutional perceptual representations", "label": "convolutional perceptual representations", "shape": "dot", "size": 25, "title": "convolutional perceptual representations"}, {"color": "#66CCFF", "id": "distinct advantages", "label": "distinct advantages", "shape": "dot", "size": 25, "title": "distinct advantages"}, {"color": "#66CCFF", "id": "Recurrent Convolutional Networks (LRCNs)", "label": "Recurrent Convolutional Networks (LRCNs)", "shape": "dot", "size": 25, "title": "Recurrent Convolutional Networks (LRCNs)"}, {"color": "#66CCFF", "id": "Video Recognition", "label": "Video Recognition", "shape": "dot", "size": 25, "title": "Video Recognition"}, {"color": "#66CCFF", "id": "Long-Term Dependencies", "label": "Long-Term Dependencies", "shape": "dot", "size": 25, "title": "Long-Term Dependencies"}, {"color": "#66CCFF", "id": "Sequence Learning", "label": "Sequence Learning", "shape": "dot", "size": 25, "title": "Sequence Learning"}, {"color": "#66CCFF", "id": "state-of-the-art models", "label": "state-of-the-art models", "shape": "dot", "size": 25, "title": "state-of-the-art models"}, {"color": "#66CCFF", "id": "separately optimized", "label": "separately optimized", "shape": "dot", "size": 25, "title": "separately optimized"}, {"color": "#66CCFF", "id": "perceptual representations", "label": "perceptual representations", "shape": "dot", "size": 25, "title": "perceptual representations"}, {"color": "#66CCFF", "id": "Action Classification", "label": "Action Classification", "shape": "dot", "size": 25, "title": "Action Classification"}, {"color": "#66CCFF", "id": "Long short-term memory recurrent neural networks", "label": "Long short-term memory recurrent neural networks", "shape": "dot", "size": 25, "title": "Long short-term memory recurrent neural networks"}, {"color": "#66CCFF", "id": "Multimodal neural language models", "label": "Multimodal neural language models", "shape": "dot", "size": 25, "title": "Multimodal neural language models"}, {"color": "#66CCFF", "id": "visual-semantic embeddings", "label": "visual-semantic embeddings", "shape": "dot", "size": 25, "title": "visual-semantic embeddings"}, {"color": "#66CCFF", "id": "Unifying visual-semantic embeddings", "label": "Unifying visual-semantic embeddings", "shape": "dot", "size": 25, "title": "Unifying visual-semantic embeddings"}, {"color": "#66CCFF", "id": "Video in sentences out", "label": "Video in sentences out", "shape": "dot", "size": 25, "title": "Video in sentences out"}, {"color": "#66CCFF", "id": "soccer videos", "label": "soccer videos", "shape": "dot", "size": 25, "title": "soccer videos"}, {"color": "#66CCFF", "id": "UAI", "label": "UAI", "shape": "dot", "size": 25, "title": "UAI"}, {"color": "#66CCFF", "id": "High accuracy optical flow estimation", "label": "High accuracy optical flow estimation", "shape": "dot", "size": 25, "title": "High accuracy optical flow estimation"}, {"color": "#66CCFF", "id": "3D convolutional neural networks", "label": "3D convolutional neural networks", "shape": "dot", "size": 25, "title": "3D convolutional neural networks"}, {"color": "#66CCFF", "id": "Generating sequences", "label": "Generating sequences", "shape": "dot", "size": 25, "title": "Generating sequences"}, {"color": "#66CCFF", "id": "arXiv preprint arXiv:1308.0850", "label": "arXiv preprint arXiv:1308.0850", "shape": "dot", "size": 25, "title": "arXiv preprint arXiv:1308.0850"}, {"color": "#66CCFF", "id": "Recurrent neural networks", "label": "Recurrent neural networks", "shape": "dot", "size": 25, "title": "Recurrent neural networks"}, {"color": "#66CCFF", "id": "generating sequences", "label": "generating sequences", "shape": "dot", "size": 25, "title": "generating sequences"}, {"color": "#66CCFF", "id": "J. Deng", "label": "J. Deng", "shape": "dot", "size": 25, "title": "J. Deng"}, {"color": "#66CCFF", "id": "W. Dong", "label": "W. Dong", "shape": "dot", "size": 25, "title": "W. Dong"}, {"color": "#66CCFF", "id": "R. Socher", "label": "R. Socher", "shape": "dot", "size": 25, "title": "R. Socher"}, {"color": "#66CCFF", "id": "L.-J. Li", "label": "L.-J. Li", "shape": "dot", "size": 25, "title": "L.-J. Li"}, {"color": "#66CCFF", "id": "K. Li", "label": "K. Li", "shape": "dot", "size": 25, "title": "K. Li"}, {"color": "#66CCFF", "id": "L. Fei-Fei", "label": "L. Fei-Fei", "shape": "dot", "size": 25, "title": "L. Fei-Fei"}, {"color": "#66CCFF", "id": "A. Frome", "label": "A. Frome", "shape": "dot", "size": 25, "title": "A. Frome"}, {"color": "#66CCFF", "id": "UT Austin", "label": "UT Austin", "shape": "dot", "size": 25, "title": "UT Austin"}, {"color": "#66CCFF", "id": "UMass Lowell", "label": "UMass Lowell", "shape": "dot", "size": 25, "title": "UMass Lowell"}, {"color": "#66CCFF", "id": "ubhashini Venugopalan", "label": "ubhashini Venugopalan", "shape": "dot", "size": 25, "title": "ubhashini Venugopalan"}, {"color": "#66CCFF", "id": "SOM", "label": "SOM", "shape": "dot", "size": 25, "title": "SOM"}, {"color": "#66CCFF", "id": "Semantic Obviousness Metric", "label": "Semantic Obviousness Metric", "shape": "dot", "size": 25, "title": "Semantic Obviousness Metric"}, {"color": "#66CCFF", "id": "Image Quality Assessment", "label": "Image Quality Assessment", "shape": "dot", "size": 25, "title": "Image Quality Assessment"}, {"color": "#66CCFF", "id": "Peng Zhang", "label": "Peng Zhang", "shape": "dot", "size": 25, "title": "Peng Zhang"}, {"color": "#66CCFF", "id": "Wengang Zhou", "label": "Wengang Zhou", "shape": "dot", "size": 25, "title": "Wengang Zhou"}, {"color": "#66CCFF", "id": "Lei Wu", "label": "Lei Wu", "shape": "dot", "size": 25, "title": "Lei Wu"}, {"color": "#66CCFF", "id": "Houqiang Li", "label": "Houqiang Li", "shape": "dot", "size": 25, "title": "Houqiang Li"}, {"color": "#66CCFF", "id": "Zhang_SOM_Semantic_Obviousness_2015_CVPR_paper.pdf", "label": "Zhang_SOM_Semantic_Obviousness_2015_CVPR_paper.pdf", "shape": "dot", "size": 25, "title": "Zhang_SOM_Semantic_Obviousness_2015_CVPR_paper.pdf"}, {"color": "#66CCFF", "id": "Image quality assessment (IQA)", "label": "Image quality assessment (IQA)", "shape": "dot", "size": 25, "title": "Image quality assessment (IQA)"}, {"color": "#66CCFF", "id": "objectively estimate human perception", "label": "objectively estimate human perception", "shape": "dot", "size": 25, "title": "objectively estimate human perception"}, {"color": "#66CCFF", "id": "new no-referece (NR) image quality assessment (IQA) framework", "label": "new no-referece (NR) image quality assessment (IQA) framework", "shape": "dot", "size": 25, "title": "new no-referece (NR) image quality assessment (IQA) framework"}, {"color": "#66CCFF", "id": "no-referece (NR) image quality assessment (IQA) framework", "label": "no-referece (NR) image quality assessment (IQA) framework", "shape": "dot", "size": 25, "title": "no-referece (NR) image quality assessment (IQA) framework"}, {"color": "#66CCFF", "id": "semantic obviousness", "label": "semantic obviousness", "shape": "dot", "size": 25, "title": "semantic obviousness"}, {"color": "#66CCFF", "id": "semantic-level factors", "label": "semantic-level factors", "shape": "dot", "size": 25, "title": "semantic-level factors"}, {"color": "#66CCFF", "id": "human perception of image quality", "label": "human perception of image quality", "shape": "dot", "size": 25, "title": "human perception of image quality"}, {"color": "#66CCFF", "id": "local characteristics", "label": "local characteristics", "shape": "dot", "size": 25, "title": "local characteristics"}, {"color": "#66CCFF", "id": "LIVE dataset", "label": "LIVE dataset", "shape": "dot", "size": 25, "title": "LIVE dataset"}, {"color": "#66CCFF", "id": "comparable results", "label": "comparable results", "shape": "dot", "size": 25, "title": "comparable results"}, {"color": "#66CCFF", "id": "existing NR-IQA algorithms", "label": "existing NR-IQA algorithms", "shape": "dot", "size": 25, "title": "existing NR-IQA algorithms"}, {"color": "#66CCFF", "id": "state-of-the-art full-referece IQA (FR-IQA) methods", "label": "state-of-the-art full-referece IQA (FR-IQA) methods", "shape": "dot", "size": 25, "title": "state-of-the-art full-referece IQA (FR-IQA) methods"}, {"color": "#66CCFF", "id": "generalization ability", "label": "generalization ability", "shape": "dot", "size": 25, "title": "generalization ability"}, {"color": "#66CCFF", "id": "NR-IQA algorithms", "label": "NR-IQA algorithms", "shape": "dot", "size": 25, "title": "NR-IQA algorithms"}, {"color": "#66CCFF", "id": "IQA algorithms", "label": "IQA algorithms", "shape": "dot", "size": 25, "title": "IQA algorithms"}, {"color": "#66CCFF", "id": "full-referece IQA (FR-IQA) methods", "label": "full-referece IQA (FR-IQA) methods", "shape": "dot", "size": 25, "title": "full-referece IQA (FR-IQA) methods"}, {"color": "#66CCFF", "id": "IQA methods", "label": "IQA methods", "shape": "dot", "size": 25, "title": "IQA methods"}, {"color": "#66CCFF", "id": "Image Quality Assessment (IQA)", "label": "Image Quality Assessment (IQA)", "shape": "dot", "size": 25, "title": "Image Quality Assessment (IQA)"}, {"color": "#66CCFF", "id": "assessment method", "label": "assessment method", "shape": "dot", "size": 25, "title": "assessment method"}, {"color": "#66CCFF", "id": "No-Reference Image Quality Assessment (NR-IQA)", "label": "No-Reference Image Quality Assessment (NR-IQA)", "shape": "dot", "size": 25, "title": "No-Reference Image Quality Assessment (NR-IQA)"}, {"color": "#66CCFF", "id": "IQA method", "label": "IQA method", "shape": "dot", "size": 25, "title": "IQA method"}, {"color": "#66CCFF", "id": "existing algorithms", "label": "existing algorithms", "shape": "dot", "size": 25, "title": "existing algorithms"}, {"color": "#66CCFF", "id": "NR-IQA", "label": "NR-IQA", "shape": "dot", "size": 25, "title": "NR-IQA"}, {"color": "#66CCFF", "id": "pzhangoo@mail.ustc.edu.cn", "label": "pzhangoo@mail.ustc.edu.cn", "shape": "dot", "size": 25, "title": "pzhangoo@mail.ustc.edu.cn"}, {"color": "#66CCFF", "id": "zhwg@ustc.edu.cn", "label": "zhwg@ustc.edu.cn", "shape": "dot", "size": 25, "title": "zhwg@ustc.edu.cn"}, {"color": "#66CCFF", "id": "wuleibig@gmail.com", "label": "wuleibig@gmail.com", "shape": "dot", "size": 25, "title": "wuleibig@gmail.com"}, {"color": "#66CCFF", "id": "lihq@ustc.edu.cn", "label": "lihq@ustc.edu.cn", "shape": "dot", "size": 25, "title": "lihq@ustc.edu.cn"}, {"color": "#66CCFF", "id": "Damien Teney", "label": "Damien Teney", "shape": "dot", "size": 25, "title": "Damien Teney"}, {"color": "#66CCFF", "id": "Learning Similarity Metrics", "label": "Learning Similarity Metrics", "shape": "dot", "size": 25, "title": "Learning Similarity Metrics"}, {"color": "#66CCFF", "id": "Matthew Brown", "label": "Matthew Brown", "shape": "dot", "size": 25, "title": "Matthew Brown"}, {"color": "#66CCFF", "id": "Dynamic Scene Segmentation", "label": "Dynamic Scene Segmentation", "shape": "dot", "size": 25, "title": "Dynamic Scene Segmentation"}, {"color": "#66CCFF", "id": "dynamic textures", "label": "dynamic textures", "shape": "dot", "size": 25, "title": "dynamic textures"}, {"color": "#66CCFF", "id": "complex patterns", "label": "complex patterns", "shape": "dot", "size": 25, "title": "complex patterns"}, {"color": "#66CCFF", "id": "spatiotemporal filters", "label": "spatiotemporal filters", "shape": "dot", "size": 25, "title": "spatiotemporal filters"}, {"color": "#66CCFF", "id": "metric-learning framework", "label": "metric-learning framework", "shape": "dot", "size": 25, "title": "metric-learning framework"}, {"color": "#66CCFF", "id": "hierarchical", "label": "hierarchical", "shape": "dot", "size": 25, "title": "hierarchical"}, {"color": "#66CCFF", "id": "graph-based", "label": "graph-based", "shape": "dot", "size": 25, "title": "graph-based"}, {"color": "#66CCFF", "id": "applicability to object segmentation", "label": "applicability to object segmentation", "shape": "dot", "size": 25, "title": "applicability to object segmentation"}, {"color": "#66CCFF", "id": "object and motion segmentation", "label": "object and motion segmentation", "shape": "dot", "size": 25, "title": "object and motion segmentation"}, {"color": "#66CCFF", "id": "general object and motion segmentation", "label": "general object and motion segmentation", "shape": "dot", "size": 25, "title": "general object and motion segmentation"}, {"color": "#66CCFF", "id": "unsupervised segmentation", "label": "unsupervised segmentation", "shape": "dot", "size": 25, "title": "unsupervised segmentation"}, {"color": "#66CCFF", "id": "best task-specific approaches", "label": "best task-specific approaches", "shape": "dot", "size": 25, "title": "best task-specific approaches"}, {"color": "#66CCFF", "id": "object identification", "label": "object identification", "shape": "dot", "size": 25, "title": "object identification"}, {"color": "#66CCFF", "id": "motion analysis", "label": "motion analysis", "shape": "dot", "size": 25, "title": "motion analysis"}, {"color": "#66CCFF", "id": "task-specific approaches", "label": "task-specific approaches", "shape": "dot", "size": 25, "title": "task-specific approaches"}, {"color": "#66CCFF", "id": "Dynamic textures", "label": "Dynamic textures", "shape": "dot", "size": 25, "title": "Dynamic textures"}, {"color": "#66CCFF", "id": "Spatio-temporal filters", "label": "Spatio-temporal filters", "shape": "dot", "size": 25, "title": "Spatio-temporal filters"}, {"color": "#66CCFF", "id": "Metric learning", "label": "Metric learning", "shape": "dot", "size": 25, "title": "Metric learning"}, {"color": "#66CCFF", "id": "Graph-based segmentation", "label": "Graph-based segmentation", "shape": "dot", "size": 25, "title": "Graph-based segmentation"}, {"color": "#66CCFF", "id": "Alpert et al. (2007)", "label": "Alpert et al. (2007)", "shape": "dot", "size": 25, "title": "Alpert et al. (2007)"}, {"color": "#66CCFF", "id": "Brox \u0026 Malik (2010)", "label": "Brox \u0026 Malik (2010)", "shape": "dot", "size": 25, "title": "Brox \u0026 Malik (2010)"}, {"color": "#66CCFF", "id": "Chan \u0026 Vasconcelos (2008)", "label": "Chan \u0026 Vasconcelos (2008)", "shape": "dot", "size": 25, "title": "Chan \u0026 Vasconcelos (2008)"}, {"color": "#66CCFF", "id": "Chan \u0026 Vasconcelos (2009)", "label": "Chan \u0026 Vasconcelos (2009)", "shape": "dot", "size": 25, "title": "Chan \u0026 Vasconcelos (2009)"}, {"color": "#66CCFF", "id": "textures", "label": "textures", "shape": "dot", "size": 25, "title": "textures"}, {"color": "#66CCFF", "id": "Variational layered dynamic textures", "label": "Variational layered dynamic textures", "shape": "dot", "size": 25, "title": "Variational layered dynamic textures"}, {"color": "#66CCFF", "id": "Corso", "label": "Corso", "shape": "dot", "size": 25, "title": "Corso"}, {"color": "#66CCFF", "id": "CVPR tutorial on video segmentation", "label": "CVPR tutorial on video segmentation", "shape": "dot", "size": 25, "title": "CVPR tutorial on video segmentation"}, {"color": "#66CCFF", "id": "Spacetime texture representation and recognition", "label": "Spacetime texture representation and recognition", "shape": "dot", "size": 25, "title": "Spacetime texture representation and recognition"}, {"color": "#66CCFF", "id": "Dynamic texture detection based on motion analysis", "label": "Dynamic texture detection based on motion analysis", "shape": "dot", "size": 25, "title": "Dynamic texture detection based on motion analysis"}, {"color": "#66CCFF", "id": "spatio-temporal orientation analysis", "label": "spatio-temporal orientation analysis", "shape": "dot", "size": 25, "title": "spatio-temporal orientation analysis"}, {"color": "#66CCFF", "id": "Dynamic texture detection", "label": "Dynamic texture detection", "shape": "dot", "size": 25, "title": "Dynamic texture detection"}, {"color": "#66CCFF", "id": "S.", "label": "S.", "shape": "dot", "size": 25, "title": "S."}, {"color": "#66CCFF", "id": "Wu, Y. N.", "label": "Wu, Y. N.", "shape": "dot", "size": 25, "title": "Wu, Y. N."}, {"color": "#66CCFF", "id": "Fazekas, S.", "label": "Fazekas, S.", "shape": "dot", "size": 25, "title": "Fazekas, S."}, {"color": "#66CCFF", "id": "Amiatz, T.", "label": "Amiatz, T.", "shape": "dot", "size": 25, "title": "Amiatz, T."}, {"color": "#66CCFF", "id": "Chetverikov, D.", "label": "Chetverikov, D.", "shape": "dot", "size": 25, "title": "Chetverikov, D."}, {"color": "#66CCFF", "id": "Feichtenhofer, C.", "label": "Feichtenhofer, C.", "shape": "dot", "size": 25, "title": "Feichtenhofer, C."}, {"color": "#66CCFF", "id": "Bags of spacetime energies", "label": "Bags of spacetime energies", "shape": "dot", "size": 25, "title": "Bags of spacetime energies"}, {"color": "#66CCFF", "id": "Pinz, A.", "label": "Pinz, A.", "shape": "dot", "size": 25, "title": "Pinz, A."}, {"color": "#66CCFF", "id": "Wilides, R.", "label": "Wilides, R.", "shape": "dot", "size": 25, "title": "Wilides, R."}, {"color": "#66CCFF", "id": "Teney, Damien", "label": "Teney, Damien", "shape": "dot", "size": 25, "title": "Teney, Damien"}, {"color": "#66CCFF", "id": "Brown, Matthew", "label": "Brown, Matthew", "shape": "dot", "size": 25, "title": "Brown, Matthew"}, {"color": "#66CCFF", "id": "University of Bath", "label": "University of Bath", "shape": "dot", "size": 25, "title": "University of Bath"}, {"color": "#66CCFF", "id": "Kit, Dimitry", "label": "Kit, Dimitry", "shape": "dot", "size": 25, "title": "Kit, Dimitry"}, {"color": "#66CCFF", "id": "Hall, Peter", "label": "Hall, Peter", "shape": "dot", "size": 25, "title": "Hall, Peter"}, {"color": "#66CCFF", "id": "Li, Yang", "label": "Li, Yang", "shape": "dot", "size": 25, "title": "Li, Yang"}, {"color": "#66CCFF", "id": "Reliable Patch Tracers", "label": "Reliable Patch Tracers", "shape": "dot", "size": 25, "title": "Reliable Patch Tracers"}, {"color": "#66CCFF", "id": "Zhu, Jianke", "label": "Zhu, Jianke", "shape": "dot", "size": 25, "title": "Zhu, Jianke"}, {"color": "#66CCFF", "id": "Hoi, Steven C.H.", "label": "Hoi, Steven C.H.", "shape": "dot", "size": 25, "title": "Hoi, Steven C.H."}, {"color": "#66CCFF", "id": "modern trackers", "label": "modern trackers", "shape": "dot", "size": 25, "title": "modern trackers"}, {"color": "#66CCFF", "id": "tracking results", "label": "tracking results", "shape": "dot", "size": 25, "title": "tracking results"}, {"color": "#66CCFF", "id": "Reliable Patch Tracers (RPT)", "label": "Reliable Patch Tracers (RPT)", "shape": "dot", "size": 25, "title": "Reliable Patch Tracers (RPT)"}, {"color": "#66CCFF", "id": "tracking method", "label": "tracking method", "shape": "dot", "size": 25, "title": "tracking method"}, {"color": "#66CCFF", "id": "identify reliable patches", "label": "identify reliable patches", "shape": "dot", "size": 25, "title": "identify reliable patches"}, {"color": "#66CCFF", "id": "reliable patches", "label": "reliable patches", "shape": "dot", "size": 25, "title": "reliable patches"}, {"color": "#66CCFF", "id": "tracked effectively", "label": "tracked effectively", "shape": "dot", "size": 25, "title": "tracked effectively"}, {"color": "#66CCFF", "id": "tracking reliability metric", "label": "tracking reliability metric", "shape": "dot", "size": 25, "title": "tracking reliability metric"}, {"color": "#66CCFF", "id": "reliability of patch", "label": "reliability of patch", "shape": "dot", "size": 25, "title": "reliability of patch"}, {"color": "#66CCFF", "id": "probability model", "label": "probability model", "shape": "dot", "size": 25, "title": "probability model"}, {"color": "#66CCFF", "id": "distribution of reliable patches", "label": "distribution of reliable patches", "shape": "dot", "size": 25, "title": "distribution of reliable patches"}, {"color": "#66CCFF", "id": "sequential Monte Carlo framework", "label": "sequential Monte Carlo framework", "shape": "dot", "size": 25, "title": "sequential Monte Carlo framework"}, {"color": "#66CCFF", "id": "motion trajectories", "label": "motion trajectories", "shape": "dot", "size": 25, "title": "motion trajectories"}, {"color": "#66CCFF", "id": "reliable patches from background", "label": "reliable patches from background", "shape": "dot", "size": 25, "title": "reliable patches from background"}, {"color": "#66CCFF", "id": "visual object", "label": "visual object", "shape": "dot", "size": 25, "title": "visual object"}, {"color": "#66CCFF", "id": "cluster", "label": "cluster", "shape": "dot", "size": 25, "title": "cluster"}, {"color": "#66CCFF", "id": "Visual Object Tracking", "label": "Visual Object Tracking", "shape": "dot", "size": 25, "title": "Visual Object Tracking"}, {"color": "#66CCFF", "id": "state-of-the-art trackers", "label": "state-of-the-art trackers", "shape": "dot", "size": 25, "title": "state-of-the-art trackers"}, {"color": "#66CCFF", "id": "source code", "label": "source code", "shape": "dot", "size": 25, "title": "source code"}, {"color": "#66CCFF", "id": "public", "label": "public", "shape": "dot", "size": 25, "title": "public"}, {"color": "#66CCFF", "id": "Adam, A., Rivlin, E., \u0026 Shimshoni, I. (2006)", "label": "Adam, A., Rivlin, E., \u0026 Shimshoni, I. (2006)", "shape": "dot", "size": 25, "title": "Adam, A., Rivlin, E., \u0026 Shimshoni, I. (2006)"}, {"color": "#66CCFF", "id": "Robust fragments-based tracking", "label": "Robust fragments-based tracking", "shape": "dot", "size": 25, "title": "Robust fragments-based tracking"}, {"color": "#66CCFF", "id": "Lucas, B. D., \u0026 Kanade, T. (1981)", "label": "Lucas, B. D., \u0026 Kanade, T. (1981)", "shape": "dot", "size": 25, "title": "Lucas, B. D., \u0026 Kanade, T. (1981)"}, {"color": "#66CCFF", "id": "iterative image registration technique", "label": "iterative image registration technique", "shape": "dot", "size": 25, "title": "iterative image registration technique"}, {"color": "#66CCFF", "id": "Poling, B., Lerman, G., \u0026 Szlarm, A. (2014)", "label": "Poling, B., Lerman, G., \u0026 Szlarm, A. (2014)", "shape": "dot", "size": 25, "title": "Poling, B., Lerman, G., \u0026 Szlarm, A. (2014)"}, {"color": "#66CCFF", "id": "Better feature tracking", "label": "Better feature tracking", "shape": "dot", "size": 25, "title": "Better feature tracking"}, {"color": "#66CCFF", "id": "Cai, Z., Wen, L., Yang, J., Lei, Z., \u0026 Li, S. (2012)", "label": "Cai, Z., Wen, L., Yang, J., Lei, Z., \u0026 Li, S. (2012)", "shape": "dot", "size": 25, "title": "Cai, Z., Wen, L., Yang, J., Lei, Z., \u0026 Li, S. (2012)"}, {"color": "#66CCFF", "id": "Structured visual tracking", "label": "Structured visual tracking", "shape": "dot", "size": 25, "title": "Structured visual tracking"}, {"color": "#66CCFF", "id": "integral histogram", "label": "integral histogram", "shape": "dot", "size": 25, "title": "integral histogram"}, {"color": "#66CCFF", "id": "Sequential Monte Carlo Methods in Practice", "label": "Sequential Monte Carlo Methods in Practice", "shape": "dot", "size": 25, "title": "Sequential Monte Carlo Methods in Practice"}, {"color": "#66CCFF", "id": "Sequential Monte Carlo Framework", "label": "Sequential Monte Carlo Framework", "shape": "dot", "size": 25, "title": "Sequential Monte Carlo Framework"}, {"color": "#66CCFF", "id": "Szlarm", "label": "Szlarm", "shape": "dot", "size": 25, "title": "Szlarm"}, {"color": "#66CCFF", "id": "Cai et al.", "label": "Cai et al.", "shape": "dot", "size": 25, "title": "Cai et al."}, {"color": "#66CCFF", "id": "ACCV", "label": "ACCV", "shape": "dot", "size": 25, "title": "ACCV"}, {"color": "#66CCFF", "id": "Mannning et al.", "label": "Mannning et al.", "shape": "dot", "size": 25, "title": "Mannning et al."}, {"color": "#66CCFF", "id": "Introduction to Information Retrieval", "label": "Introduction to Information Retrieval", "shape": "dot", "size": 25, "title": "Introduction to Information Retrieval"}, {"color": "#66CCFF", "id": "Danelljan et al.", "label": "Danelljan et al.", "shape": "dot", "size": 25, "title": "Danelljan et al."}, {"color": "#66CCFF", "id": "Everingham et al.", "label": "Everingham et al.", "shape": "dot", "size": 25, "title": "Everingham et al."}, {"color": "#66CCFF", "id": "The pascal visual object classes(voc) challenge", "label": "The pascal visual object classes(voc) challenge", "shape": "dot", "size": 25, "title": "The pascal visual object classes(voc) challenge"}, {"color": "#66CCFF", "id": "Grundmann et al.", "label": "Grundmann et al.", "shape": "dot", "size": 25, "title": "Grundmann et al."}, {"color": "#66CCFF", "id": "Yang Li", "label": "Yang Li", "shape": "dot", "size": 25, "title": "Yang Li"}, {"color": "#66CCFF", "id": "College of Computer Science", "label": "College of Computer Science", "shape": "dot", "size": 25, "title": "College of Computer Science"}, {"color": "#66CCFF", "id": "liyang89@zju.edu.cn", "label": "liyang89@zju.edu.cn", "shape": "dot", "size": 25, "title": "liyang89@zju.edu.cn"}, {"color": "#66CCFF", "id": "Jianke Zhu", "label": "Jianke Zhu", "shape": "dot", "size": 25, "title": "Jianke Zhu"}, {"color": "#66CCFF", "id": "V.", "label": "V.", "shape": "dot", "size": 25, "title": "V."}, {"color": "#66CCFF", "id": "Han, M.", "label": "Han, M.", "shape": "dot", "size": 25, "title": "Han, M."}, {"color": "#66CCFF", "id": "Essa, I.", "label": "Essa, I.", "shape": "dot", "size": 25, "title": "Essa, I."}, {"color": "#66CCFF", "id": "SALICON", "label": "SALICON", "shape": "dot", "size": 25, "title": "SALICON"}, {"color": "#66CCFF", "id": "understand and predict visual attention", "label": "understand and predict visual attention", "shape": "dot", "size": 25, "title": "understand and predict visual attention"}, {"color": "#66CCFF", "id": "collecting large-scale human data", "label": "collecting large-scale human data", "shape": "dot", "size": 25, "title": "collecting large-scale human data"}, {"color": "#66CCFF", "id": "mouse-contingent paradigm", "label": "mouse-contingent paradigm", "shape": "dot", "size": 25, "title": "mouse-contingent paradigm"}, {"color": "#66CCFF", "id": "eye tracker", "label": "eye tracker", "shape": "dot", "size": 25, "title": "eye tracker"}, {"color": "#66CCFF", "id": "SALICON dataset", "label": "SALICON dataset", "shape": "dot", "size": 25, "title": "SALICON dataset"}, {"color": "#66CCFF", "id": "human \u0027free-viewing\u0027 data", "label": "human \u0027free-viewing\u0027 data", "shape": "dot", "size": 25, "title": "human \u0027free-viewing\u0027 data"}, {"color": "#66CCFF", "id": "10,000 images", "label": "10,000 images", "shape": "dot", "size": 25, "title": "10,000 images"}, {"color": "#66CCFF", "id": "Microsoft COCO dataset", "label": "Microsoft COCO dataset", "shape": "dot", "size": 25, "title": "Microsoft COCO dataset"}, {"color": "#66CCFF", "id": "ground truth for evaluating salience algorithms", "label": "ground truth for evaluating salience algorithms", "shape": "dot", "size": 25, "title": "ground truth for evaluating salience algorithms"}, {"color": "#66CCFF", "id": "existing annotations", "label": "existing annotations", "shape": "dot", "size": 25, "title": "existing annotations"}, {"color": "#66CCFF", "id": "new possibilities for visual understanding", "label": "new possibilities for visual understanding", "shape": "dot", "size": 25, "title": "new possibilities for visual understanding"}, {"color": "#66CCFF", "id": "ground truth", "label": "ground truth", "shape": "dot", "size": 25, "title": "ground truth"}, {"color": "#66CCFF", "id": "new possibilities", "label": "new possibilities", "shape": "dot", "size": 25, "title": "new possibilities"}, {"color": "#66CCFF", "id": "Ming Jiang", "label": "Ming Jiang", "shape": "dot", "size": 25, "title": "Ming Jiang"}, {"color": "#66CCFF", "id": "Shengshen Huang", "label": "Shengshen Huang", "shape": "dot", "size": 25, "title": "Shengshen Huang"}, {"color": "#66CCFF", "id": "Juanyong Duan", "label": "Juanyong Duan", "shape": "dot", "size": 25, "title": "Juanyong Duan"}, {"color": "#66CCFF", "id": "Qi Zhao", "label": "Qi Zhao", "shape": "dot", "size": 25, "title": "Qi Zhao"}, {"color": "#66CCFF", "id": "Deep LAC", "label": "Deep LAC", "shape": "dot", "size": 25, "title": "Deep LAC"}, {"color": "#66CCFF", "id": "fine-grained recognition", "label": "fine-grained recognition", "shape": "dot", "size": 25, "title": "fine-grained recognition"}, {"color": "#66CCFF", "id": "Di Lin", "label": "Di Lin", "shape": "dot", "size": 25, "title": "Di Lin"}, {"color": "#66CCFF", "id": "Xiaoyong Shen", "label": "Xiaoyong Shen", "shape": "dot", "size": 25, "title": "Xiaoyong Shen"}, {"color": "#66CCFF", "id": "Cewu Lu", "label": "Cewu Lu", "shape": "dot", "size": 25, "title": "Cewu Lu"}, {"color": "#66CCFF", "id": "Fine-grained Recognition", "label": "Fine-grained Recognition", "shape": "dot", "size": 25, "title": "Fine-grained Recognition"}, {"color": "#66CCFF", "id": "Lin_Deep_LAC_Deep_2015_CVPR_paper", "label": "Lin_Deep_LAC_Deep_2015_CVPR_paper", "shape": "dot", "size": 25, "title": "Lin_Deep_LAC_Deep_2015_CVPR_paper"}, {"color": "#66CCFF", "id": "ineering", "label": "ineering", "shape": "dot", "size": 25, "title": "ineering"}, {"color": "#66CCFF", "id": "eleqiz@nus.edu.sg", "label": "eleqiz@nus.edu.sg", "shape": "dot", "size": 25, "title": "eleqiz@nus.edu.sg"}, {"color": "#66CCFF", "id": "Localization", "label": "Localization", "shape": "dot", "size": 25, "title": "Localization"}, {"color": "#66CCFF", "id": "Classification", "label": "Classification", "shape": "dot", "size": 25, "title": "Classification"}, {"color": "#66CCFF", "id": "fine-grained recognition system", "label": "fine-grained recognition system", "shape": "dot", "size": 25, "title": "fine-grained recognition system"}, {"color": "#66CCFF", "id": "part localization", "label": "part localization", "shape": "dot", "size": 25, "title": "part localization"}, {"color": "#66CCFF", "id": "valve linkage function", "label": "valve linkage function", "shape": "dot", "size": 25, "title": "valve linkage function"}, {"color": "#66CCFF", "id": "back-propagation chaining", "label": "back-propagation chaining", "shape": "dot", "size": 25, "title": "back-propagation chaining"}, {"color": "#66CCFF", "id": "deep LAC system", "label": "deep LAC system", "shape": "dot", "size": 25, "title": "deep LAC system"}, {"color": "#66CCFF", "id": "classification errors", "label": "classification errors", "shape": "dot", "size": 25, "title": "classification errors"}, {"color": "#66CCFF", "id": "alignment errors", "label": "alignment errors", "shape": "dot", "size": 25, "title": "alignment errors"}, {"color": "#66CCFF", "id": "update localization", "label": "update localization", "shape": "dot", "size": 25, "title": "update localization"}, {"color": "#66CCFF", "id": "LAC system", "label": "LAC system", "shape": "dot", "size": 25, "title": "LAC system"}, {"color": "#66CCFF", "id": "fine-grained object data", "label": "fine-grained object data", "shape": "dot", "size": 25, "title": "fine-grained object data"}]);
                  edges = new vis.DataSet([{"arrows": "to", "color": "#0077CC", "from": "Saurabh Singh", "label": "is_author_of", "title": "is_author_of", "to": "Learning a Sequential Search for Landmarks", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Saurabh Singh", "label": "affiliated_with", "title": "affiliated_with", "to": "University of Illinois, Urbana-Champaign", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Saurabh Singh", "label": "affiliated_with", "title": "affiliated_with", "to": "University of Indiana", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Learning a Sequential Search for Landmarks", "label": "is_conference_paper", "title": "is_conference_paper", "to": "CVPR", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Learning a Sequential Search for Landmarks", "label": "is_published_in", "title": "is_published_in", "to": "2015", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Derek Hoiem", "label": "is_author_of", "title": "is_author_of", "to": "Learning a Sequential Search for Landmarks", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Derek Hoiem", "label": "affiliated_with", "title": "affiliated_with", "to": "University of Illinois, Urbana-Champaign", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Derek Hoiem", "label": "affiliated_with", "title": "affiliated_with", "to": "University of Indiana", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Derek Hoiem", "label": "is_author_of", "title": "is_author_of", "to": "Rock_Completing_3D_Object_2015_CVPR_paper.pdf", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Derek Hoiem", "label": "affiliation", "title": "affiliation", "to": "University of Illinois at Urbana-Champaign", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "David Forsyth", "label": "is_author_of", "title": "is_author_of", "to": "Learning a Sequential Search for Landmarks", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "David Forsyth", "label": "affiliated_with", "title": "affiliated_with", "to": "University of Illinois, Urbana-Champaign", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "David Forsyth", "label": "affiliated_with", "title": "affiliated_with", "to": "University of Indiana", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "CVPR", "label": "is_conference", "title": "is_conference", "to": "Learning a Sequential Search for Landmarks", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "CVPR", "label": "published", "title": "published", "to": "Benenson, R., Mathias, M., Tuytelaars, T., \u0026 Van Gool, L. (2013)", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "CVPR", "label": "published", "title": "published", "to": "Bayesian color constancy revisited", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "CVPR", "label": "published", "title": "published", "to": "Efficient belief propagation", "width": 7.0}, {"arrows": "to", "color": "#CC7700", "from": "CVPR", "label": "is_conference_of", "title": "is_conference_of", "to": "Computer Vision", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "CVPR", "label": "is_a", "title": "is_a", "to": "conference", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "CVPR", "label": "is_publication_venue_for", "title": "is_publication_venue_for", "to": "reference [3]", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "CVPR", "label": "presents", "title": "presents", "to": "multi-column deep neural networks", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "CVPR", "label": "is_conference_for", "title": "is_conference_for", "to": "computer vision", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "CVPR", "label": "is_conference_for", "title": "is_conference_for", "to": "Image Co-segmentation", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "CVPR", "label": "is_conference_for", "title": "is_conference_for", "to": "Object Detection", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "CVPR", "label": "is_conference_for", "title": "is_conference_for", "to": "Visual semantic search", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "CVPR", "label": "is_conference_for", "title": "is_conference_for", "to": "person re-identi\ufb01cation", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "CVPR", "label": "hosts", "title": "hosts", "to": "Entropy rate superpixel segmentation", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "CVPR", "label": "publishes", "title": "publishes", "to": "tric min-cuts paper", "width": 3.25}, {"arrows": "to", "color": "#0077CC", "from": "CVPR", "label": "hosts", "title": "hosts", "to": "Beyond lambert", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "CVPR", "label": "is_publication_venue", "title": "is_publication_venue", "to": "research paper", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "CVPR", "label": "is_conference_for", "title": "is_conference_for", "to": "Furukawa et al.", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "CVPR", "label": "hosts", "title": "hosts", "to": "constrained parametric min-cuts", "width": 3.25}, {"arrows": "to", "color": "#0077CC", "from": "CVPR", "label": "published", "title": "published", "to": "Variational layered dynamic textures", "width": 3.91}, {"arrows": "to", "color": "#CC7700", "from": "Glaucoma", "label": "is_related_to", "title": "is_related_to", "to": "Learning a Sequential Search for Landmarks", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Glaucoma", "label": "is_analyzed_by", "title": "is_analyzed_by", "to": "Deep Multiple Instance Learning", "width": 3.25}, {"arrows": "to", "color": "#CC7700", "from": "Sequential Search", "label": "is_technique_in", "title": "is_technique_in", "to": "Learning a Sequential Search for Landmarks", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Landmarks", "label": "is_used_in", "title": "is_used_in", "to": "Learning a Sequential Search for Landmarks", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "Ollama", "label": "is_related_to", "title": "is_related_to", "to": "Learning a Sequential Search for Landmarks", "width": 3.25}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "aims_to_find", "title": "aims_to_find", "to": "landmarks", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "uses", "title": "uses", "to": "appearance", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "applied_to", "title": "applied_to", "to": "parsing human body layouts", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "applied_to", "title": "applied_to", "to": "finding landmarks in images of birds", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "learns", "title": "learns", "to": "sequential search", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "method", "label": "represents", "title": "represents", "to": "spatial model", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "displays", "title": "displays", "to": "strong performance", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "addresses", "title": "addresses", "to": "model problems", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "achieves", "title": "achieves", "to": "state-of-the-art results", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "applicable_to", "title": "applicable_to", "to": "contour detection", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "lacks", "title": "lacks", "to": "feature engineering", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "aims_to_achieve", "title": "aims_to_achieve", "to": "subpixel-level accuracy", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "exhibits", "title": "exhibits", "to": "robustness", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "sparsified_in", "title": "sparsified_in", "to": "images", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "separates", "title": "separates", "to": "sparse error tensors", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "addresses", "title": "addresses", "to": "limitations", "width": 3.46}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "outperforms", "title": "outperforms", "to": "traditional methods", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "compared_to", "title": "compared_to", "to": "state-of-the-art methods", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "localizes", "title": "localizes", "to": "ground-level query images", "width": 3.61}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "matches", "title": "matches", "to": "aerial imagery", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "learns", "title": "learns", "to": "feature representation", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "addresses", "title": "addresses", "to": "border ownership assignment", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "utilizes", "title": "utilizes", "to": "Structured Random Forests (SRF)", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "introduces", "title": "introduces", "to": "border ownership structure", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "uses", "title": "uses", "to": "shape descriptors", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "uses", "title": "uses", "to": "spectral properties", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "uses", "title": "uses", "to": "semi-global grouping cues", "width": 3.61}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "evaluated_on", "title": "evaluated_on", "to": "Berkeley Segmentation Dataset (BSDS)", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "evaluated_on", "title": "evaluated_on", "to": "NYU Depth V2 dataset", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "outperforms", "title": "outperforms", "to": "multi-stage approaches", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "achieves", "title": "achieves", "to": "scale invariance", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "is_based_on", "title": "is_based_on", "to": "four simple color features", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "uses", "title": "uses", "to": "regression trees", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "shows", "title": "shows", "to": "effectiveness", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "handles", "title": "handles", "to": "high percentage of outliers", "width": 3.61}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "achieves", "title": "achieves", "to": "16x speedup", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "compared_to", "title": "compared_to", "to": "Deformable Part Model", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "demonstrates_applicability_for", "title": "demonstrates_applicability_for", "to": "parallel computing", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "segments", "title": "segments", "to": "frame", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "adjusts", "title": "adjusts", "to": "enhancement of regions", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "achieves", "title": "achieves", "to": "high fidelity", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "achieves", "title": "achieves", "to": "temporal consistency", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "applied to", "title": "applied to", "to": "texture classification", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "evaluated on", "title": "evaluated on", "to": "KTH-TIPS2 dataset", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "evaluated on", "title": "evaluated on", "to": "FMD dataset", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "evaluated on", "title": "evaluated on", "to": "DTD dataset", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "method", "label": "explores", "title": "explores", "to": "convolutional temporal feature pooling architectures", "width": 3.64}, {"arrows": "to", "color": "#CC7700", "from": "method", "label": "models", "title": "models", "to": "video", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "tested on", "title": "tested on", "to": "datasets", "width": 3.79}, {"arrows": "to", "color": "#CC7700", "from": "method", "label": "handles", "title": "handles", "to": "unseen target crowd scene", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "method", "label": "fine-tunes", "title": "fine-tunes", "to": "trained CNN model", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "performs", "title": "performs", "to": "dense depth optimization", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "removes_need_for", "title": "removes_need_for", "to": "view pairing", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "removes_need_for", "title": "removes_need_for", "to": "stereo depth estimation", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "allows_for", "title": "allows_for", "to": "per-image paralleization", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "is_not_specific_to", "title": "is_not_specific_to", "to": "SfM points", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "leverages", "title": "leverages", "to": "work on depth", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "incorporates", "title": "incorporates", "to": "synthesized right views", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "uses", "title": "uses", "to": "local tangent planes", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "is_composed_of", "title": "is_composed_of", "to": "two steps", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "demonstrates", "title": "demonstrates", "to": "high completion rate", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "achieves", "title": "achieves", "to": "lowest errors", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "demonstrated through", "title": "demonstrated through", "to": "experimental results", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "replaces", "title": "replaces", "to": "geodesic-preserving term", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "estimates", "title": "estimates", "to": "optical flow field", "width": 3.9699999999999998}, {"arrows": "to", "color": "#CC7700", "from": "method", "label": "revitalizes", "title": "revitalizes", "to": "piecewise parametric flow model", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "handles", "title": "handles", "to": "homogeneous motions", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "handles", "title": "handles", "to": "complex motions", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "has_property", "title": "has_property", "to": "equity constraint", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "achieves", "title": "achieves", "to": "top-tier performances", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "evaluated_on", "title": "evaluated_on", "to": "Optical flow benchmarks", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "is", "title": "is", "to": "unsupervised video salieny detection method", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "supports", "title": "supports", "to": "human activity recognition", "width": 3.67}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "supports", "title": "supports", "to": "training of activity detection algorithms", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "evaluated_on", "title": "evaluated_on", "to": "challenging datasets", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "demonstrates", "title": "demonstrates", "to": "favorable performance", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "estimates", "title": "estimates", "to": "3D pose from 2D joint locations", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "uses", "title": "uses", "to": "over-complete dictionary of poses", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "shows", "title": "shows", "to": "good generalization", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "method", "label": "parameterizes", "title": "parameterizes", "to": "body pose", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "estimates", "title": "estimates", "to": "3D pose", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "uses", "title": "uses", "to": "over-completes dictionary of poses", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "avoids", "title": "avoids", "to": "impossible poses", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "compared with", "title": "compared with", "to": "recent work", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "evaluated_on", "title": "evaluated_on", "to": "Mcgill dataset", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "demonstrates", "title": "demonstrates", "to": "state-of-the-art performance", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "utilizes", "title": "utilizes", "to": "optimization algorithm", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "applies", "title": "applies", "to": "visual attention", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "uses", "title": "uses", "to": "deep neural networks", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "relies_on", "title": "relies_on", "to": "additional annotations", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "outperforms", "title": "outperforms", "to": "state-of-the-art image restoration methods", "width": 3.79}, {"arrows": "to", "color": "#CCCCCC", "from": "method", "label": "aims_to", "title": "aims_to", "to": "learning features", "width": 3.94}, {"arrows": "to", "color": "#CCCCCC", "from": "method", "label": "aims_to", "title": "aims_to", "to": "learning similarity metric", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "outperforms", "title": "outperforms", "to": "state of the art", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "is resistant to", "title": "is resistant to", "to": "over-fitting", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "inspired_by", "title": "inspired_by", "to": "linear discriminant embedding", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "establishes", "title": "establishes", "to": "binary tests", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "method", "label": "aims_to", "title": "aims_to", "to": "localize object", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "takes_as_input", "title": "takes_as_input", "to": "collection of images", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "outputs", "title": "outputs", "to": "bounding box", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "applies_to", "title": "applies_to", "to": "videos", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "localizes", "title": "localizes", "to": "object", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "transfers", "title": "transfers", "to": "appearance models", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "targets", "title": "targets", "to": "unseen objects", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "verified_on", "title": "verified_on", "to": "LFW", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "verified_on", "title": "verified_on", "to": "YouTube Faces", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "evaluated_on", "title": "evaluated_on", "to": "RGB-D action datasets", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "shows", "title": "shows", "to": "promising results", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "addresses", "title": "addresses", "to": "tracking inaccuracies", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "segments", "title": "segments", "to": "image", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "projects", "title": "projects", "to": "GIS data", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "presents", "title": "presents", "to": "insight", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "uses", "title": "uses", "to": "min-cuts", "width": 3.73}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "demonstrates", "title": "demonstrates", "to": "ability to separate specular reflection", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "preserves", "title": "preserves", "to": "saturation of underlying surface colors", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "achieves", "title": "achieves", "to": "accurate surface shape estimation", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "demonstrates", "title": "demonstrates", "to": "robustness to light source position errors", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "demonstrates_utility_in", "title": "demonstrates_utility_in", "to": "salient object detection", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "demonstrates_utility_in", "title": "demonstrates_utility_in", "to": "object proposal applications", "width": 3.7}, {"arrows": "to", "color": "#00CC77", "from": "method", "label": "leads_to", "title": "leads_to", "to": "better accuracy", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "is_faster_than", "title": "is_faster_than", "to": "dense projections", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "is_faster_than", "title": "is_faster_than", "to": "other methods", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "improves", "title": "improves", "to": "accuracy", "width": 3.7600000000000002}, {"arrows": "to", "color": "#00CC77", "from": "method", "label": "reduces", "title": "reduces", "to": "computational cost", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "takes_advantage_of", "title": "takes_advantage_of", "to": "depth data", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "handles", "title": "handles", "to": "noisy images", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "utilizes", "title": "utilizes", "to": "depth channel", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "improves", "title": "improves", "to": "detection of object-like regions", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "provides", "title": "provides", "to": "depth-based local features", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "provides", "title": "provides", "to": "comparable performance", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "requires", "title": "requires", "to": "single input image", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "uses", "title": "uses", "to": "Gaussian Mixture Model", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "demonstrates", "title": "demonstrates", "to": "reflection removal", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "enhances", "title": "enhances", "to": "depth maps", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "improves", "title": "improves", "to": "visual fidelity", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "operates", "title": "operates", "to": "four orders of magnitude faster", "width": 3.73}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "exhibits", "title": "exhibits", "to": "small loss in performance", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "limited_by", "title": "limited_by", "to": "runtime", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "compared_to", "title": "compared_to", "to": "stereo approaches", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "compared_to", "title": "compared_to", "to": "optical flow approaches", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "compared_to", "title": "compared_to", "to": "scene flow approaches", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "provides", "title": "provides", "to": "qualitative results", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "compares_to", "title": "compares_to", "to": "state-of-the-art stereo", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "compares_to", "title": "compares_to", "to": "optical flow", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "compares_to", "title": "compares_to", "to": "scene flow", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "evaluates_on", "title": "evaluates_on", "to": "scene flow dataset", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "improves", "title": "improves", "to": "localization", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "has_speed_up", "title": "has_speed_up", "to": "factor of 100", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "builds", "title": "builds", "to": "global appearance models", "width": 3.73}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "establishes", "title": "establishes", "to": "dynamic location models", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "combines", "title": "combines", "to": "elements within energy minimization framework", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "demonstrates", "title": "demonstrates", "to": "superiority over existing algorithms", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "proposes", "title": "proposes", "to": "novel method", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "achieves", "title": "achieves", "to": "superior denois-ing accuracy", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "handles", "title": "handles", "to": "label corruption levels", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "method", "label": "addresses", "title": "addresses", "to": "transparent object reconstruction", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "landmarks", "label": "located_in", "title": "located_in", "to": "images of objects", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "sequential search", "label": "localizes", "title": "localizes", "to": "landmarks", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "landmark addition", "label": "depends_on", "title": "depends_on", "to": "image", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "image", "label": "contains", "title": "contains", "to": "object proposal windows", "width": 3.61}, {"arrows": "to", "color": "#0077CC", "from": "groups", "label": "scored_using", "title": "scored_using", "to": "learned function", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "learned function", "label": "used to", "title": "used to", "to": "expand groups", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "landmark group", "label": "scored using", "title": "scored using", "to": "learned function", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "scoring function", "label": "learned from", "title": "learned from", "to": "data labelled with landmarks", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "scoring function", "label": "derived from", "title": "derived from", "to": "data", "width": 3.7600000000000002}, {"arrows": "to", "color": "#CC7700", "from": "spatial model", "label": "models", "title": "models", "to": "kinematics of landmark groups", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "landmark", "label": "part of", "title": "part of", "to": "landmark group", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "initial landmark", "label": "dependent on", "title": "dependent on", "to": "image", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "data", "label": "is", "title": "is", "to": "compressed", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "data", "label": "is", "title": "is", "to": "projected", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "data", "label": "is", "title": "is", "to": "2D, 3D and 4D", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Method", "label": "represents", "title": "represents", "to": "spatial model", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Method", "label": "displays", "title": "displays", "to": "strong performance", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Method", "label": "demonstrates", "title": "demonstrates", "to": "effectiveness", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Method", "label": "handles", "title": "handles", "to": "outliers", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Method", "label": "applied_to", "title": "applied_to", "to": "Texture Classification", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Method", "label": "evaluated_on", "title": "evaluated_on", "to": "KTH-TIPS2", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Method", "label": "evaluated_on", "title": "evaluated_on", "to": "FMD", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Method", "label": "evaluated_on", "title": "evaluated_on", "to": "DTD", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Method", "label": "outperforms", "title": "outperforms", "to": "State-of-the-art approaches", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Method", "label": "adapts_to", "title": "adapts_to", "to": "unknown reflectance maps", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Method", "label": "reconstructs", "title": "reconstructs", "to": "fine detail", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "Method", "label": "adapts_to", "title": "adapts_to", "to": "Reflectance Maps", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Method", "label": "reconstructs", "title": "reconstructs", "to": "Fine Detail", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Method", "label": "demonstrates", "title": "demonstrates", "to": "high completion rate", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Method", "label": "achieves", "title": "achieves", "to": "lowest errors", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Method", "label": "operates_in", "title": "operates_in", "to": "noisy cases", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Method", "label": "leverages", "title": "leverages", "to": "Deep Features", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Method", "label": "achieves", "title": "achieves", "to": "State-of-the-Art Performance", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Method", "label": "improves", "title": "improves", "to": "generalization power", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Method", "label": "evaluated_on", "title": "evaluated_on", "to": "RGB-D action datasets", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Method", "label": "achieves", "title": "achieves", "to": "state-of-the-art results", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Method", "label": "shows results", "title": "shows results", "to": "missing RGB data", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Method", "label": "shows results", "title": "shows results", "to": "missing depth data", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Method", "label": "more_accurate_than", "title": "more_accurate_than", "to": "Other methods", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Method", "label": "speeds_up", "title": "speeds_up", "to": "High-Dimensional Binary Encoding", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Method", "label": "handles", "title": "handles", "to": "noisy images", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Method", "label": "requires", "title": "requires", "to": "single input image", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Andriluka et al. (2009)", "label": "addresses", "title": "addresses", "to": "People detection", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "Andriluka et al. (2009)", "label": "addresses", "title": "addresses", "to": "articulated pose estimation", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "articulated pose estimation", "label": "is_method_of", "title": "is_method_of", "to": "graphical model", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Barto (1998)", "label": "introduces", "title": "introduces", "to": "Reinforcement learning", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Felzenszwalb and Huttenlocher (2005)", "label": "proposes", "title": "proposes", "to": "Pictorial structures", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "Fergus et al. (2003)", "label": "proposes", "title": "proposes", "to": "Unsupervised scale-invariant learning", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Doll\u00b4ar et al. (2009)", "label": "proposes", "title": "proposes", "to": "Integral channel features", "width": 3.73}, {"arrows": "to", "color": "#0077CC", "from": "Eichner and Ferrari (2012)", "label": "addresses", "title": "addresses", "to": "collective human pose estimation", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Doll\u00e1r, P.", "label": "authored", "title": "authored", "to": "Integral channel features", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Doll\u00e1r, P.", "label": "presented_at", "title": "presented_at", "to": "BMVC", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Eichner, M.", "label": "authored", "title": "authored", "to": "Appearance sharing", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Fei-Fei, L.", "label": "authored", "title": "authored", "to": "One-shot learning", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Fei-Fei, L.", "label": "presented_at", "title": "presented_at", "to": "CVPR", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Felzenszwalb, P. F.", "label": "authored", "title": "authored", "to": "Cascade object detection", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Felzenszwalb, P. F.", "label": "presented_at", "title": "presented_at", "to": "CVPR", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Felzenszwalb, P. F.", "label": "authored", "title": "authored", "to": "Object detection grammar", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Felzenszwalb, P. F.", "label": "writes_paper", "title": "writes_paper", "to": "Object detection with grammar models", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Felzenszwalb, P. F.", "label": "authored", "title": "authored", "to": "Object detection with discriminatively trained part-based models", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Felzenszwalb, P. F.", "label": "co_author_of", "title": "co_author_of", "to": "Huttenlocher, D. P.", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Fergus, R.", "label": "authored", "title": "authored", "to": "Sparse object category model", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Fergus, R.", "label": "presented_at", "title": "presented_at", "to": "CVPR", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Wang, Y.", "label": "authored", "title": "authored", "to": "Multiple tree models", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Wang, Y.", "label": "presented_at", "title": "presented_at", "to": "ECCV", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "BMVC", "label": "is_conference_of", "title": "is_conference_of", "to": "Computer Vision", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "BMVC", "label": "is_a", "title": "is_a", "to": "conference", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "ECCV", "label": "is_publication_venue_for", "title": "is_publication_venue_for", "to": "reference [1]", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "ECCV", "label": "published", "title": "published", "to": "variant co-occurrence local binary pattern", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "ECCV", "label": "is_conference_for", "title": "is_conference_for", "to": "computer vision", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "ECCV", "label": "hosts", "title": "hosts", "to": "geospatial image segmentation approach", "width": 3.25}, {"arrows": "to", "color": "#0077CC", "from": "Gedas Bertasius", "label": "contributed_to", "title": "contributed_to", "to": "DeepEdge", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Gedas Bertasius", "label": "affiliation", "title": "affiliation", "to": "University of Pennsylvania", "width": 3.9699999999999998}, {"arrows": "to", "color": "#CC7700", "from": "DeepEdge", "label": "is_a", "title": "is_a", "to": "Deep Network", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "DeepEdge", "label": "performs", "title": "performs", "to": "Contour Detection", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "DeepEdge", "label": "is_for", "title": "is_for", "to": "Top-Down Contour Detection", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "DeepEdge", "label": "published_in", "title": "published_in", "to": "CVPR", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "DeepEdge", "label": "has_architecture", "title": "has_architecture", "to": "Bifurcated Deep Network", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "DeepEdge", "label": "proposes", "title": "proposes", "to": "novel method", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Lorenzo Torresani", "label": "author_of", "title": "author_of", "to": "DeepEdge", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Lorenzo Torresani", "label": "affiliation", "title": "affiliation", "to": "Dartmouth College", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Contour detection", "label": "relies_on", "title": "relies_on", "to": "low-level features", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "novel method", "label": "improves", "title": "improves", "to": "speed of image retrieval", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "novel method", "label": "learns", "title": "learns", "to": "computationally bounded sparse projections", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "novel method", "label": "adds", "title": "adds", "to": "orthogonality constraint", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "novel method", "label": "is_for", "title": "is_for", "to": "template matching", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "state-of-the-art results", "label": "achieved_in", "title": "achieved_in", "to": "recognition", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "state-of-the-art results", "label": "achieved_in", "title": "achieved_in", "to": "detection", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "state-of-the-art results", "label": "achieved_in", "title": "achieved_in", "to": "retrieval", "width": 3.7600000000000002}, {"arrows": "to", "color": "#CC7700", "from": "contour detection", "label": "related_to", "title": "related_to", "to": "object recognition", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "object recognition", "label": "based on", "title": "based on", "to": "conv-net", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "object recognition", "label": "related_task_of", "title": "related_task_of", "to": "Action Recognition", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "object recognition", "label": "uses", "title": "uses", "to": "pictorial structures", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Arbel\u00e1ez et al. (2011)", "label": "published", "title": "published", "to": "Contour detection and hierarchical image segmentation", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Arbel\u00e1ez et al. (2011)", "label": "contributed_to", "title": "contributed_to", "to": "Hierarchical Image Segmentation", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Lim et al. (2013)", "label": "published", "title": "published", "to": "Sketch tokens", "width": 3.9699999999999998}, {"arrows": "to", "color": "#CC7700", "from": "Sketch tokens", "label": "related_to", "title": "related_to", "to": "contour detection", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Long et al. (2014)", "label": "published", "title": "published", "to": "Fully convolutional networks", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Fully convolutional networks", "label": "used_for", "title": "used_for", "to": "semantic segmentation", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Malik et al. (2001)", "label": "published", "title": "published", "to": "Contour and texture analysis", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Girshick", "label": "author_of", "title": "author_of", "to": "Rich feature hierarchies", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Girshick", "label": "is_affiliated_with", "title": "is_affiliated_with", "to": "Microsoft Research", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Girshick", "label": "authors", "title": "authors", "to": "Rich Feature Hierarchies", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Girshick", "label": "authored", "title": "authored", "to": "feature hierarchies", "width": 3.91}, {"arrows": "to", "color": "#CC7700", "from": "Rich feature hierarchies", "label": "used_for", "title": "used_for", "to": "Object detection", "width": 3.88}, {"arrows": "to", "color": "#CC7700", "from": "Rich feature hierarchies", "label": "used_for", "title": "used_for", "to": "Semantic segmentation", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Rich feature hierarchies", "label": "presented_at", "title": "presented_at", "to": "IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)", "width": 3.73}, {"arrows": "to", "color": "#0077CC", "from": "Rich feature hierarchies", "label": "published_as", "title": "published_as", "to": "arXiv preprint", "width": 3.73}, {"arrows": "to", "color": "#0077CC", "from": "Rich feature hierarchies", "label": "focused_on", "title": "focused_on", "to": "object detection", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Rich feature hierarchies", "label": "focused_on", "title": "focused_on", "to": "semantic segmentation", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Donahue", "label": "author_of", "title": "author_of", "to": "Rich feature hierarchies", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Darrell", "label": "author_of", "title": "author_of", "to": "Rich feature hierarchies", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Hariharan", "label": "authors", "title": "authors", "to": "Hypercolumns", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Hariharan", "label": "is_affiliated_with", "title": "is_affiliated_with", "to": "University of California, Berkeley", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Hypercolumns", "label": "improves", "title": "improves", "to": "Object Segmentation", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "Arbel\u00e1ez", "label": "author_of", "title": "author_of", "to": "Hypercolumns", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Arbel\u00e1ez", "label": "is_affiliated_with", "title": "is_affiliated_with", "to": "Universidad de los Andes", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Arbel\u00e1ez", "label": "authors", "title": "authors", "to": "Semantic Segmentation", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Iandola", "label": "author_of", "title": "author_of", "to": "Densenet", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Jia", "label": "author_of", "title": "author_of", "to": "Caffe", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Jia", "label": "authored", "title": "authored", "to": "Large-scale object classification", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "Caffe", "label": "is_a", "title": "is_a", "to": "Convolutional architecture", "width": 3.7600000000000002}, {"arrows": "to", "color": "#CC7700", "from": "Caffe", "label": "is_architecture_for", "title": "is_architecture_for", "to": "fast feature embedding", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "Caffe", "label": "is_a", "title": "is_a", "to": "deep learning framework", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Caffe", "label": "has_application_in", "title": "has_application_in", "to": "computer vision tasks", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "Caffe", "label": "used_for", "title": "used_for", "to": "intrinsic image decomposition", "width": 3.64}, {"arrows": "to", "color": "#CC7700", "from": "Girshik", "label": "related_to", "title": "related_to", "to": "Object detection", "width": 3.64}, {"arrows": "to", "color": "#CC7700", "from": "Object detection", "label": "requires", "title": "requires", "to": "Rich feature hierarchies", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "Object detection", "label": "utilizes", "title": "utilizes", "to": "part-based models", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Shelhamer et al.", "label": "authored", "title": "authored", "to": "Caffe", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Ren et al.", "label": "authored", "title": "authored", "to": "Scale-invariant contour completion", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Scale-invariant contour completion", "label": "uses", "title": "uses", "to": "Condition random fields", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Jianbo Shi", "label": "affiliation", "title": "affiliation", "to": "University of Pennsylvania", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Wen Wang", "label": "author_of", "title": "author_of", "to": "Discrimi nant Analysis", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Wen Wang", "label": "affiliated_with", "title": "affiliated_with", "to": "Key Laboratory of Intelligent Information Processing", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Wen Wang", "label": "member_of", "title": "member_of", "to": "Institute of Computing Technology", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Wen Wang", "label": "has_email", "title": "has_email", "to": "wen.wang@vipl.ict.ac.cn", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Discrimi nant Analysis", "label": "analyzes", "title": "analyzes", "to": "Gaussian Distributions", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Discrimi nant Analysis", "label": "applied_to", "title": "applied_to", "to": "Face Recognition", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Ruiping Wang", "label": "author_of", "title": "author_of", "to": "Discrimi nant Analysis", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Ruiping Wang", "label": "authored", "title": "authored", "to": "Wang_Discriminant_Analysis_on_2015_CVPR_paper", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Ruiping Wang", "label": "affiliated_with", "title": "affiliated_with", "to": "Key Laboratory of Intelligent Information Processing", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Ruiping Wang", "label": "member_of", "title": "member_of", "to": "Institute of Computing Technology", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Ruiping Wang", "label": "has_email", "title": "has_email", "to": "wangruiping@ict.ac.cn", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Face Recognition", "label": "uses", "title": "uses", "to": "Gaussian Mixture Models", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Face Recognition", "label": "uses", "title": "uses", "to": "Riemannian Manifold", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "Face Recognition", "label": "uses", "title": "uses", "to": "Discriminant Analysis", "width": 3.25}, {"arrows": "to", "color": "#0077CC", "from": "Face Recognition", "label": "uses", "title": "uses", "to": "Kernel Methods", "width": 3.0999999999999996}, {"arrows": "to", "color": "#CC7700", "from": "Face Recognition", "label": "based_on", "title": "based_on", "to": "Image Sets", "width": 3.88}, {"arrows": "to", "color": "#CC7700", "from": "Face Recognition", "label": "has_challenge", "title": "has_challenge", "to": "Pose variation", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "Face Recognition", "label": "employs", "title": "employs", "to": "PEP (Probabilistic Elastic Part) Model", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Face Recognition", "label": "is_evaluated_on", "title": "is_evaluated_on", "to": "LFW", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Face Recognition", "label": "participates_in", "title": "participates_in", "to": "PaSC", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Face Recognition", "label": "achieves", "title": "achieves", "to": "state-of-the-art performance", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Face Recognition", "label": "benefits_from", "title": "benefits_from", "to": "Poisson Editing", "width": 3.64}, {"arrows": "to", "color": "#CC7700", "from": "Face Recognition", "label": "improves_performance_with", "title": "improves_performance_with", "to": "3D Morphable Models", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Zhiwu Huang", "label": "authored", "title": "authored", "to": "Wang_Discribminant_Analysis_on_2015_CVPR_paper", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Zhiwu Huang", "label": "member_of", "title": "member_of", "to": "Institute of Computing Technology", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Zhiwu Huang", "label": "has_email", "title": "has_email", "to": "zhiwu.huang@vipl.ict.ac.cn", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Shiguan Shan", "label": "authored", "title": "authored", "to": "Wang_Discriminant_Analysis_on_2015_CVPR_paper", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Shiguan Shan", "label": "member_of", "title": "member_of", "to": "Institute of Computing Technology", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Shiguan Shan", "label": "has_email", "title": "has_email", "to": "sgshan@ict.ac.cn", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Xilin Chen", "label": "authored", "title": "authored", "to": "Wang_Discriminant_Analysis_on_2015_CVPR_paper", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Xilin Chen", "label": "member_of", "title": "member_of", "to": "Institute of Computing Technology", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Xilin Chen", "label": "has_email", "title": "has_email", "to": "xlchen@ict.ac.cn", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Xilin Chen", "label": "affiliated_with", "title": "affiliated_with", "to": "Key Laboratory of Intelligent Information Processing", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "DARG", "label": "addresses", "title": "addresses", "to": "face recognition problem", "width": 3.91}, {"arrows": "to", "color": "#CC7700", "from": "DARG", "label": "is_a", "title": "is_a", "to": "novel method", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "DARG", "label": "represents", "title": "represents", "to": "image sets", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "DARG", "label": "discriminates", "title": "discriminates", "to": "Gaussian components", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "image sets", "label": "represented_as", "title": "represented_as", "to": "Gaussian Mixture Models", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Gaussian distributions", "label": "lie_on", "title": "lie_on", "to": "Riemannian manifold", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Kernel Discrimiant Analysis", "label": "utilizes", "title": "utilizes", "to": "probabilistic kernels", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "probabilistic kernels", "label": "encode", "title": "encode", "to": "geometry", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "proposed method", "label": "evaluated_on", "title": "evaluated_on", "to": "face recognition databases", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "proposed method", "label": "demonstrates", "title": "demonstrates", "to": "superior performance", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "proposed method", "label": "outperforms", "title": "outperforms", "to": "state-of-the-art algorithms", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "proposed method", "label": "evaluated_on", "title": "evaluated_on", "to": "MultiPIE dataset", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "proposed method", "label": "handles", "title": "handles", "to": "homogeneous motions", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "proposed method", "label": "handles", "title": "handles", "to": "complex motions", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "proposed method", "label": "achieves", "title": "achieves", "to": "top-tier performances", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "proposed method", "label": "performs_better_than", "title": "performs_better_than", "to": "state of the art", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "proposed method", "label": "retrieves", "title": "retrieves", "to": "similar 3D models", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "proposed method", "label": "transfers", "title": "transfers", "to": "symmetries", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "face recognition databases", "label": "are", "title": "are", "to": "challenging", "width": 3.25}, {"arrows": "to", "color": "#0077CC", "from": "superior performance", "label": "compared_to", "title": "compared_to", "to": "state-of-the-art approaches", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "superior performance", "label": "compared_to", "title": "compared_to", "to": "existing methods", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Russian components", "label": "from", "title": "from", "to": "different subjects", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "prior probabilities", "label": "incorporated_in", "title": "incorporated_in", "to": "Russian components", "width": 3.64}, {"arrows": "to", "color": "#CC7700", "from": "Kernel Methods", "label": "is_topic_of", "title": "is_topic_of", "to": "cvpr_papers", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Aranndi et al. (2005)", "label": "published", "title": "published", "to": "Face recognition with image sets using manifold density divergence", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Face recognition with image sets using manifold density divergence", "label": "addresses", "title": "addresses", "to": "Face Recognition", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Amar \u0026 Nagaoka (2000)", "label": "authored", "title": "authored", "to": "Methods of Information Geometry", "width": 3.7600000000000002}, {"arrows": "to", "color": "#CC7700", "from": "Methods of Information Geometry", "label": "describes", "title": "describes", "to": "Information Geometry", "width": 3.46}, {"arrows": "to", "color": "#0077CC", "from": "Chan et al. (2004)", "label": "developed", "title": "developed", "to": "Probabilistic Kernels", "width": 3.64}, {"arrows": "to", "color": "#CC7700", "from": "Probabilistic Kernels", "label": "based_on", "title": "based_on", "to": "Information Divergence", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Probabilistic KernELS", "label": "based on", "title": "based on", "to": "Information Divergence", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Chan, A. B.", "label": "authored", "title": "authored", "to": "Probabilistic Kernels", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Chan, A. B.", "label": "authored", "title": "authored", "to": "Modeling, clustering, and segmenting video", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Moreno, P. J.", "label": "co-authored", "title": "co-authored", "to": "Probabilistic Kernels", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Cevikalp, H.", "label": "authored", "title": "authored", "to": "Face Recognition", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Triggs, B.", "label": "co-authored", "title": "co-authored", "to": "Face Recognition", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "Image Sets Alignment", "label": "for", "title": "for", "to": "Video-based Face Recognition", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Cui, Z.", "label": "authored", "title": "authored", "to": "Image Sets Alignment", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "Grassmann Discriminant Analysis", "label": "provides", "title": "provides", "to": "Subspace-based Learning", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "Hamm, J.", "label": "authored", "title": "authored", "to": "Grassmann Discriminant Analysis", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Lee, D. D.", "label": "co-authored", "title": "co-authored", "to": "Grassmann Discriminant Analysis", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "Sparse Approximated Nearest Points", "label": "used_for", "title": "used_for", "to": "Image Set Classification", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Hu, Y.", "label": "authored", "title": "authored", "to": "Sparse Approximated Nearest Points", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Hu, Y.", "label": "authored", "title": "authored", "to": "Sparse approximated nearest points", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Hu, Y.", "label": "presented_at", "title": "presented_at", "to": "IEEE International Conference on Computer Vision and Pattern Recognized (CVPR)", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Harandi, M. T.", "label": "authored", "title": "authored", "to": "Grasmannian kernels", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Jayasumana, S.", "label": "presented_at", "title": "presented_at", "to": "IEEE Computer Society on Computer Vision and Pattern Recognition (CVPR)", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Kim, M.", "label": "authored", "title": "authored", "to": "Face tracking and recognition", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Kim, M.", "label": "presented_at", "title": "presented_at", "to": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Key Laboratory of Intelligent Information Processing", "label": "is_part_of", "title": "is_part_of", "to": "Chinese Academy of Sciences", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Key Laboratory of Intelligent Information Processing", "label": "located_in", "title": "located_in", "to": "Institute of Computing Technology", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Chinese Academy of Sciences", "label": "is_affiliated_with", "title": "is_affiliated_with", "to": "hanics", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Chinese Academy of Sciences", "label": "located_in", "title": "located_in", "to": "Beijing", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Institute of Computing Technology", "label": "part_of", "title": "part_of", "to": "Chinese Academy of Sciences", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Super-resolution Person Re-identi\ufb01cation", "label": "is_publication_of", "title": "is_publication_of", "to": "Xiao-Yuan Jing", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Super-resolution Person Re-identi\ufb01cation", "label": "is_publication_of", "title": "is_publication_of", "to": "Xiaoke Zhu", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Super-resolution Person Re-identi\ufb01cation", "label": "is_publication_of", "title": "is_publication_of", "to": "Fei Wu", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Super-resolution Person Re-identi\ufb01cation", "label": "is_publication_of", "title": "is_publication_of", "to": "Xinge You", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Super-resolution Person Re-identi\ufb01cation", "label": "is_publication_of", "title": "is_publication_of", "to": "Qinglong Liu", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Super-resolution Person Re-identi\ufb01cation", "label": "is_publication_of", "title": "is_publication_of", "to": "Dong Yue", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Super-resolution Person Re-identi\ufb01cation", "label": "is_publication_of", "title": "is_publication_of", "to": "Ruimin Hu", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Super-resolution Person Re-identi\ufb01cation", "label": "is_publication_of", "title": "is_publication_of", "to": "Baowen Xu", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Super-resolution Person Re-identi\ufb01cation", "label": "presented_at", "title": "presented_at", "to": "CVPR", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Xiao-Yuan Jing", "label": "affiliated_with", "title": "affiliated_with", "to": "State Key Laboratory of Software Engineering", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Xiaoke Zhu", "label": "affiliated_with", "title": "affiliated_with", "to": "State Key Laboratory of Software Engineering", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Fei Wu", "label": "affiliated_with", "title": "affiliated_with", "to": "State Key Laboratory of Software Engineering", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Xinge You", "label": "affiliated_with", "title": "affiliated_with", "to": "Huazhong University of Science and Technology", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Qinglong Liu", "label": "affiliated_with", "title": "affiliated_with", "to": "State Key Laboratory of Software Engineering", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Dong Yue", "label": "affiliated_with", "title": "affiliated_with", "to": "Nanjing University of Posts and Telecommunications", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Ruimin Hu", "label": "affiliated_with", "title": "affiliated_with", "to": "National Engineering Research Center for Multimedia Software", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Baowen Xu", "label": "affiliated_with", "title": "affiliated_with", "to": "State Key Laboratory of Software Engineering", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Baowen Xu", "label": "affiliated_with", "title": "affiliated_with", "to": "School of Computer", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Jing_Super-Resolution_Person_Re-Identification_2015_CVPR_paper.pdf", "label": "is_file_of", "title": "is_file_of", "to": "Super-resolution Person Re-identi\ufb01cation", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Person re-identification", "label": "is_important_in", "title": "is_important_in", "to": "surveillance applications", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Person re-identification", "label": "is_important_in", "title": "is_important_in", "to": "forensics applications", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Person re-identification", "label": "uses", "title": "uses", "to": "semi-coupled dictionaries", "width": 3.67}, {"arrows": "to", "color": "#CC7700", "from": "SLD2L", "label": "has_purpose", "title": "has_purpose", "to": "converting LR probe image features", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "SLD2L", "label": "demonstrates", "title": "demonstrates", "to": "effectiveness", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "effectiveness", "label": "relates_to", "title": "relates_to", "to": "3D shape matching", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "effectiveness", "label": "relates_to", "title": "relates_to", "to": "3D shape retrieval", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "effectiveness", "label": "of", "title": "of", "to": "proposed approach", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "discriminant term", "label": "ensures", "title": "ensures", "to": "converted features are far from different-person HR gallery features", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "low-rank regularization", "label": "characterizes", "title": "characterizes", "to": "intrinsic feature space", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "low-rank regularization", "label": "applied_to", "title": "applied_to", "to": "HR images", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "low-rank regularization", "label": "applied_to", "title": "applied_to", "to": "LR images", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "HR images", "label": "is_type_of", "title": "is_type_of", "to": "image", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "LR images", "label": "is_type_of", "title": "is_type_of", "to": "image", "width": 3.7600000000000002}, {"arrows": "to", "color": "#CC7700", "from": "HR gallery images", "label": "has_feature", "title": "has_feature", "to": "features", "width": 3.25}, {"arrows": "to", "color": "#CC7700", "from": "features", "label": "are", "title": "are", "to": "discriminative for categorization", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "features", "label": "measures", "title": "measures", "to": "semantic obviousness", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "features", "label": "discovers", "title": "discovers", "to": "local characteristics", "width": 3.64}, {"arrows": "to", "color": "#CC7700", "from": "LR probe images", "label": "has_feature", "title": "has_feature", "to": "features", "width": 3.25}, {"arrows": "to", "color": "#0077CC", "from": "public datasets", "label": "demonstrates", "title": "demonstrates", "to": "SLD2L", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "HR gallery", "label": "features", "title": "features", "to": "low-rank regularization", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Super-resolution person re-identification", "label": "is_topic_of", "title": "is_topic_of", "to": "research", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "research", "label": "focuses_on", "title": "focuses_on", "to": "predicting human gaze", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "research", "label": "is in", "title": "is in", "to": "early stage", "width": 3.7600000000000002}, {"arrows": "to", "color": "#CC7700", "from": "research", "label": "is_area_of", "title": "is_area_of", "to": "computer vision", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "Low-rank discriminant dictionary learning", "label": "is_technique", "title": "is_technique", "to": "machine learning", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Semi-coupled dictionaries", "label": "is_method", "title": "is_method", "to": "person re-identification", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "person re-identification", "label": "aims_to", "title": "aims_to", "to": "match pedestrian images", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "person re-identification", "label": "remains", "title": "remains", "to": "challenging", "width": 2.5}, {"arrows": "to", "color": "#0077CC", "from": "person re-identification", "label": "uses", "title": "uses", "to": "Mahalanobis distance", "width": 3.88}, {"arrows": "to", "color": "#CC7700", "from": "person re-identification", "label": "is", "title": "is", "to": "cross-dataset task", "width": 3.64}, {"arrows": "to", "color": "#CC7700", "from": "Feature representation learning", "label": "is_field", "title": "is_field", "to": "machine learning", "width": 3.61}, {"arrows": "to", "color": "#0077CC", "from": "Bak et al. (2010)", "label": "researches", "title": "researches", "to": "Person re-identification", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "Bedagkar-Gala \u0026 Shah (2014)", "label": "surveys", "title": "surveys", "to": "person re-identi\ufb01cation approaches", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "Liu et al. (2014)", "label": "proposes", "title": "proposes", "to": "semi-supervised coupled dictionary learning", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Liu, X.", "label": "authored", "title": "authored", "to": "Semi-supervised coupled dictionary learning for person re-identi\ufb01cation", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Semi-supervised coupled dictionary learning for person re-identi\ufb01cation", "label": "presented_at", "title": "presented_at", "to": "CVPR, IEEE Conference on", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Ma, L.", "label": "authored", "title": "authored", "to": "Sparse representation for face recognition based on discriminative low-rank dictionary learning", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Ma, L.", "label": "authored", "title": "authored", "to": "Person re-identi\ufb01cation over camera networks using multi-task distance metric learning", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Ma, L.", "label": "authored", "title": "authored", "to": "Person re-identi\ufb01cation over camera networks", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Gray, D.", "label": "authored", "title": "authored", "to": "Evaluating appearance models for recognition, reacquisition, and tracking", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Gray, D.", "label": "authored", "title": "authored", "to": "Viewpoint invariant pedestrian recognition with an ensemble of localized features", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Gray, D.", "label": "authored", "title": "authored", "to": "Viewpoint invariant pedestrian recognition", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Evaluating appearance models for recognition, reacquisition, and tracking", "label": "presented_at", "title": "presented_at", "to": "Performance Evaluation of Tracking and Surveillance, IEEE workshop on", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Viewpoint invariant pedestrian recognition with an ensemble of localized features", "label": "presented_at", "title": "presented_at", "to": "ECCV", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Person re-identi\ufb01cation over camera networks using multi-task distance metric learning", "label": "published_in", "title": "published_in", "to": "Image Processing, IEEE Transactions on", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Image Processing, IEEE Transactions on", "label": "publishes", "title": "publishes", "to": "Person re-identi\ufb01cation over camera networks", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Image Processing, IEEE Transactions on", "label": "publishes", "title": "publishes", "to": "Image super-resolution via sparse representation", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Hirzer, M.", "label": "authored", "title": "authored", "to": "Person re-identi\ufb01cation by descriptive and discriminative classification", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Zheng, W.-S.", "label": "authored", "title": "authored", "to": "Reidenti\ufb01cation by relative distance comparison", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "State Key Laboratory of Software Engineering", "label": "affiliated_with", "title": "affiliated_with", "to": "School of Computer", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Image Analysis", "label": "publishes", "title": "publishes", "to": "Person re-identi\ufb01cation by descriptive and discriminative classification", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Pattern Analysis and Machine Intelligence, IEEE Transactions on", "label": "publishes", "title": "publishes", "to": "Reidenti\ufb01cation by relative distance comparison", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "re Engineering", "label": "located_in", "title": "located_in", "to": "School of Computer", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "School of Computer", "label": "part_of", "title": "part_of", "to": "Wuhan University", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Wuhan University", "label": "located_in", "title": "located_in", "to": "China", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "School of Electronic Information and Communications", "label": "part_of", "title": "part_of", "to": "Huazhong University of Science and Technology", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "National Engineering Research Center for Multimedia Software", "label": "part_of", "title": "part_of", "to": "School of Computer", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "Ronan Collobert", "label": "author_of", "title": "author_of", "to": "From Image-level to Pixel-level Labeling", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Ronan Collobert", "label": "affiliated_with", "title": "affiliated_with", "to": "Facebook AI Research", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Ronan Collobert", "label": "email", "title": "email", "to": "ronan@coltobert.com", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Ronan Collobert", "label": "works_at", "title": "works_at", "to": "Menlo Park", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "From Image-level to Pixel-level Labeling", "label": "published_in", "title": "published_in", "to": "CVPR", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Multimedia Software", "label": "developed_by", "title": "developed_by", "to": "School of Computer", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "We", "label": "are_interested_in", "title": "are_interested_in", "to": "object segmentation", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "object segmentation", "label": "requires", "title": "requires", "to": "object class information", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "object segmentation", "label": "related_to", "title": "related_to", "to": "salient object detection", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "object segmentation", "label": "is_part_of", "title": "is_part_of", "to": "object and motion segmentation", "width": 3.64}, {"arrows": "to", "color": "#CC7700", "from": "weakly supervised segmentation task", "label": "fits_framework", "title": "fits_framework", "to": "Multiple Instance Learning (MIL) framework", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "training image", "label": "has", "title": "has", "to": "pixel corresponding to image class label", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "segmentation task", "label": "is_rewritten_as", "title": "is_rewritten_as", "to": "inferring pixels belonging to class of object", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "model", "label": "is_based_on", "title": "is_based_on", "to": "Convolutional Neural Network", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "model", "label": "is_constrained_during", "title": "is_constrained_during", "to": "training", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "model", "label": "discriminates", "title": "discriminates", "to": "pixels", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "model", "label": "discriminates", "title": "discriminates", "to": "right pixels", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "model", "label": "beats", "title": "beats", "to": "state of the art results", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "model", "label": "performs", "title": "performs", "to": "weakly supervised object segmentation task", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "model", "label": "compared_with", "title": "compared_with", "to": "fully-supervised segmentation approaches", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "model", "label": "unifies", "title": "unifies", "to": "existing ZSL algorithms", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "model", "label": "demonstrates", "title": "demonstrates", "to": "performance gains", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "model", "label": "uses", "title": "uses", "to": "patch representations", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "model", "label": "is_benchmarked_on", "title": "is_benchmarked_on", "to": "Intrinsic Images in the Wild dataset", "width": 3.7600000000000002}, {"arrows": "to", "color": "#CC7700", "from": "model", "label": "is", "title": "is", "to": "low-rank", "width": 3.7600000000000002}, {"arrows": "to", "color": "#00CC77", "from": "model", "label": "improves", "title": "improves", "to": "generalization power", "width": 3.67}, {"arrows": "to", "color": "#0077CC", "from": "model", "label": "accounts_for", "title": "accounts_for", "to": "global salience effects", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "model", "label": "achieves", "title": "achieves", "to": "state-of-the-art accuracy", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "model", "label": "fits", "title": "fits", "to": "noisy images", "width": 3.73}, {"arrows": "to", "color": "#0077CC", "from": "model", "label": "constrained_by", "title": "constrained_by", "to": "loss minimization", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "model", "label": "integrates", "title": "integrates", "to": "computer vision models", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "model", "label": "integrates", "title": "integrates", "to": "human input", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "model", "label": "uses", "title": "uses", "to": "Markov Decision Process", "width": 3.7600000000000002}, {"arrows": "to", "color": "#CC7700", "from": "model", "label": "is a model of", "title": "is a model of", "to": "bottom-up segmentation", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "model", "label": "evaluated_on", "title": "evaluated_on", "to": "We Are Family", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "model", "label": "requires", "title": "requires", "to": "computations", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "model", "label": "evaluated_on", "title": "evaluated_on", "to": "Stickmen dataset", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "model", "label": "achieves", "title": "achieves", "to": "performance improvements", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "model", "label": "addresses", "title": "addresses", "to": "light transport complexities", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Convolutional Neural Network", "label": "is_architecture_for", "title": "is_architecture_for", "to": "pose estimation", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "training", "label": "prioritizes", "title": "prioritizes", "to": "pixels important for image classification", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "training", "label": "focuses_on", "title": "focuses_on", "to": "pixels", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "training", "label": "is_expensive_due_to", "title": "is_expensive_due_to", "to": "structured prediction subroutine", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "network-based model", "label": "constrained_during", "title": "constrained_during", "to": "training", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "network-based model", "label": "weights", "title": "weights", "to": "important pixels", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "system", "label": "trained_using", "title": "trained_using", "to": "Imaginet dataset", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "system", "label": "leverages", "title": "leverages", "to": "interest regions", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "system", "label": "requires", "title": "requires", "to": "user\u0027s passive participation", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "system", "label": "calibrates itself", "title": "calibrates itself", "to": "without user participation", "width": 3.91}, {"arrows": "to", "color": "#CC7700", "from": "system", "label": "requires", "title": "requires", "to": "calibration", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "system", "label": "is", "title": "is", "to": "efficient", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "system", "label": "handles", "title": "handles", "to": "noisy data", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "system", "label": "handles", "title": "handles", "to": "missing data", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "system", "label": "handles", "title": "handles", "to": "sudden camera motions", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "system", "label": "requires", "title": "requires", "to": "no training data", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "system", "label": "performs", "title": "performs", "to": "comparably to batch methods", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "system", "label": "improves_over", "title": "improves_over", "to": "sequential methods", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "system", "label": "characterized by", "title": "characterized by", "to": "elegant structure", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "system", "label": "characterized by", "title": "characterized by", "to": "speed", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "system", "label": "performs", "title": "performs", "to": "pixel-level salience computation", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "system", "label": "integrated into", "title": "integrated into", "to": "object proposal generation framework", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "system", "label": "achieves", "title": "achieves", "to": "state-of-the-art performance", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "system", "label": "uses", "title": "uses", "to": "supervised machine learning", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "system", "label": "learns_to", "title": "learns_to", "to": "synthesize images", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "system", "label": "enables", "title": "enables", "to": "redirection of gaze", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "system", "label": "is", "title": "is", "to": "computationally efficient", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "system", "label": "runs_on", "title": "runs_on", "to": "laptop", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "system", "label": "avoids", "title": "avoids", "to": "uncanny valley effect", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "system", "label": "takes as input", "title": "takes as input", "to": "image to annotate", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "system", "label": "takes as input", "title": "takes as input", "to": "annotation constraints", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "system", "label": "produces", "title": "produces", "to": "object annotations", "width": 3.88}, {"arrows": "to", "color": "#CC7700", "from": "system", "label": "has", "title": "has", "to": "relationships", "width": 3.0999999999999996}, {"arrows": "to", "color": "#CC7700", "from": "system", "label": "has", "title": "has", "to": "dependencies", "width": 3.04}, {"arrows": "to", "color": "#0077CC", "from": "segmentation experiments", "label": "performed_on", "title": "performed_on", "to": "Pascal VOC dataset", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Model", "label": "achieves", "title": "achieves", "to": "state of the art results", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Model", "label": "compared_with", "title": "compared_with", "to": "fully-supervised segmentation approaches", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "Model", "label": "is", "title": "is", "to": "Quasi-parametric Model", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Model", "label": "retrieves", "title": "retrieves", "to": "KNN Images", "width": 3.73}, {"arrows": "to", "color": "#0077CC", "from": "Model", "label": "matches", "title": "matches", "to": "Semantic Regions", "width": 3.67}, {"arrows": "to", "color": "#0077CC", "from": "Model", "label": "fuses", "title": "fuses", "to": "Matched Regions", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Model", "label": "refines", "title": "refines", "to": "Result", "width": 3.61}, {"arrows": "to", "color": "#CC7700", "from": "Object Segmentation", "label": "is a", "title": "is a", "to": "task", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "Object Segmentation", "label": "is_related_to", "title": "is_related_to", "to": "Computer Vision", "width": 3.0999999999999996}, {"arrows": "to", "color": "#CC7700", "from": "Object Segmentation", "label": "is_related_to", "title": "is_related_to", "to": "Salient Object Detection", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "Weakly Supervised Segmentation", "label": "is a", "title": "is a", "to": "approach", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "approach", "label": "models", "title": "models", "to": "textured 3D non-rigid models", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "approach", "label": "includes", "title": "includes", "to": "photometric information", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "approach", "label": "uses", "title": "uses", "to": "shape manifold", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "approach", "label": "introduces", "title": "introduces", "to": "new discretization method", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "approach", "label": "demonstrates", "title": "demonstrates", "to": "high performance", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "approach", "label": "is_faster_than", "title": "is_faster_than", "to": "existing learning-based methods", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "approach", "label": "provides", "title": "provides", "to": "best results", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "approach", "label": "handles", "title": "handles", "to": "complex non-uniform motion blur", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "approach", "label": "handles", "title": "handles", "to": "non-uniform motion blur", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "approach", "label": "is", "title": "is", "to": "deblurring model", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "approach", "label": "is useful when", "title": "is useful when", "to": "original enhancement algorithms are unknown", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "approach", "label": "is useful when", "title": "is useful when", "to": "original enhancement algorithms are inaccessible", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "approach", "label": "validates_on", "title": "validates_on", "to": "first-person point-of-view videos", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "approach", "label": "corresponds_to", "title": "corresponds_to", "to": "Cutkosky grasp taxonomy", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "approach", "label": "demonstrates", "title": "demonstrates", "to": "consistent performance gain", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "approach", "label": "uses", "title": "uses", "to": "datasets", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "approach", "label": "focuses_on", "title": "focuses_on", "to": "depth estimation", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "approach", "label": "involves", "title": "involves", "to": "synthesizing intermediate views", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "approach", "label": "enhances", "title": "enhances", "to": "depth perception", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "approach", "label": "demonstrates", "title": "demonstrates", "to": "improvements", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "approach", "label": "has", "title": "has", "to": "lower computational complexity", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "approach", "label": "achieves", "title": "achieves", "to": "state-of-the-art performance", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "approach", "label": "performs_on", "title": "performs_on", "to": "Stanford Background dataset", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "approach", "label": "performs_on", "title": "performs_on", "to": "SIFT Flow datasets", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "approach", "label": "evaluated_on", "title": "evaluated_on", "to": "public datasets", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "approach", "label": "uses", "title": "uses", "to": "semiglobal matching", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "approach", "label": "demonstrated_on", "title": "demonstrated_on", "to": "non-flat manifolds", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "approach", "label": "applied_to", "title": "applied_to", "to": "Euclidean spaces", "width": 3.58}, {"arrows": "to", "color": "#CC7700", "from": "approach", "label": "relies on", "title": "relies on", "to": "global representations", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "approach", "label": "ensures", "title": "ensures", "to": "temporal consistency", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "approach", "label": "ensures", "title": "ensures", "to": "spatial consistency", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "approach", "label": "robust to", "title": "robust to", "to": "noisy data", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "approach", "label": "robust to", "title": "robust to", "to": "missing data", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "approach", "label": "robust to", "title": "robust to", "to": "sudden camera motions", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "approach", "label": "requires", "title": "requires", "to": "training data", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "approach", "label": "utilizes", "title": "utilizes", "to": "encoding method", "width": 3.73}, {"arrows": "to", "color": "#0077CC", "from": "approach", "label": "uses", "title": "uses", "to": "part-based region matching", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "approach", "label": "outperforms", "title": "outperforms", "to": "state of the art", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "approach", "label": "performs", "title": "performs", "to": "evaluations", "width": 3.34}, {"arrows": "to", "color": "#0077CC", "from": "approach", "label": "involves", "title": "involves", "to": "generating motion part candidates", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "approach", "label": "formulates", "title": "formulates", "to": "objective function", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "approach", "label": "avoids", "title": "avoids", "to": "expensive annotations", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "approach", "label": "achieves", "title": "achieves", "to": "significant improvements", "width": 3.73}, {"arrows": "to", "color": "#0077CC", "from": "approach", "label": "demonstrates", "title": "demonstrates", "to": "competitive performance", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "approach", "label": "demonstrates", "title": "demonstrates", "to": "effectiveness", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "approach", "label": "learns", "title": "learns", "to": "lightness differences between pixels", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "approach", "label": "uses", "title": "uses", "to": "depth features", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "approach", "label": "uses", "title": "uses", "to": "RGB visual features", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "approach", "label": "has_property", "title": "has_property", "to": "high efficiency", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "approach", "label": "has_property", "title": "has_property", "to": "robust performance", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "approach", "label": "supports", "title": "supports", "to": "varying camera-to-scene arrangements", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "approach", "label": "minimizes", "title": "minimizes", "to": "mutual failures", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "approach", "label": "achieves", "title": "achieves", "to": "50 fps", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "approach", "label": "demonstrates", "title": "demonstrates", "to": "generalizability", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "approach", "label": "tested_on", "title": "tested_on", "to": "LFW", "width": 3.91}, {"arrows": "to", "color": "#CC7700", "from": "approach", "label": "is", "title": "is", "to": "data fusion approach", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "approach", "label": "evaluates", "title": "evaluates", "to": "projections reliability", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "approach", "label": "fuses", "title": "fuses", "to": "super-pixel segmentations", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "approach", "label": "refines", "title": "refines", "to": "alignment of projections", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "approach", "label": "illustrated_with", "title": "illustrated_with", "to": "Robotics (Sarcos) dataset", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "approach", "label": "incorporates", "title": "incorporates", "to": "spatial features", "width": 3.67}, {"arrows": "to", "color": "#0077CC", "from": "approach", "label": "enables", "title": "enables", "to": "generalization", "width": 3.73}, {"arrows": "to", "color": "#CC7700", "from": "approach", "label": "allows_creation_of", "title": "allows_creation_of", "to": "new structures", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "approach", "label": "demonstrates_flexibility_with", "title": "demonstrates_flexibility_with", "to": "furniture design", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "approach", "label": "demonstrates_flexibility_with", "title": "demonstrates_flexibility_with", "to": "archaeology", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "approach", "label": "validates_on", "title": "validates_on", "to": "ILSVRC2014 dataset", "width": 3.79}, {"arrows": "to", "color": "#CC7700", "from": "approach", "label": "is_a", "title": "is_a", "to": "human-in-the-loop labeling approach", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "approach", "label": "achieves", "title": "achieves", "to": "highly accurate bottom-up object segmentation", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "approach", "label": "generates", "title": "generates", "to": "set of regions", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "approach", "label": "trains", "title": "trains", "to": "ensemble of figure-ground segmentation models", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "approach", "label": "capable of learning", "title": "capable of learning", "to": "model", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "approach", "label": "exploits", "title": "exploits", "to": "statistical structure", "width": 3.46}, {"arrows": "to", "color": "#0077CC", "from": "approach", "label": "deals_with", "title": "deals_with", "to": "parsing humans", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "approach", "label": "evaluated on", "title": "evaluated on", "to": "gaze-enabled egocentric video dataset", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "approach", "label": "incorporates", "title": "incorporates", "to": "visual cues", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "approach", "label": "incorporates", "title": "incorporates", "to": "textual cues", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "approach", "label": "is", "title": "is", "to": "quantitative", "width": 3.46}, {"arrows": "to", "color": "#0077CC", "from": "approach", "label": "exhibits", "title": "exhibits", "to": "theoretical properties", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "approach", "label": "evaluated_on", "title": "evaluated_on", "to": "synthetic data sets", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "approach", "label": "improves_performance", "title": "improves_performance", "to": "state-of-the-art methods", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "approach", "label": "activates", "title": "activates", "to": "set of cliques", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "approach", "label": "evaluated_on", "title": "evaluated_on", "to": "real-world data sets", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "approach", "label": "incorporates", "title": "incorporates", "to": "salience as prior", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "approach", "label": "utilizes", "title": "utilizes", "to": "spatial edges", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "approach", "label": "utilizes", "title": "utilizes", "to": "temporal motion boundaries", "width": 3.79}, {"arrows": "to", "color": "#CC7700", "from": "approach", "label": "presents", "title": "presents", "to": "uni\ufb01ed saliency detection framework", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "approach", "label": "outperforms", "title": "outperforms", "to": "state-of-the-art solution", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "approach", "label": "operates_on", "title": "operates_on", "to": "2D, 3D and 4D data", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "approach", "label": "results_in", "title": "results_in", "to": "image labels", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "approach", "label": "uses", "title": "uses", "to": "light path triangulation", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "approach", "label": "handles", "title": "handles", "to": "unknown refractive indices", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "approach", "label": "suitable_for", "title": "suitable_for", "to": "complex transparent objects", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "approach", "label": "demonstrates", "title": "demonstrates", "to": "feasibility", "width": 3.34}, {"arrows": "to", "color": "#CC7700", "from": "approach", "label": "focuses_on", "title": "focuses_on", "to": "improving object detection", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "approach", "label": "identifies", "title": "identifies", "to": "representative parts", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "approach", "label": "demonstrates", "title": "demonstrates", "to": "superior performance", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "approach", "label": "compares_to", "title": "compares_to", "to": "existing NR-IQA algorithms", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "approach", "label": "achieves", "title": "achieves", "to": "comparable results", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "approach", "label": "exhibits", "title": "exhibits", "to": "generalization ability", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "approach", "label": "demonstrates", "title": "demonstrates", "to": "applicability to object segmentation", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "approach", "label": "applicable_to", "title": "applicable_to", "to": "object and motion segmentation", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "approach", "label": "demonstrates applicability to", "title": "demonstrates applicability to", "to": "general object and motion segmentation", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "approach", "label": "improves_over", "title": "improves_over", "to": "unsupervised segmentation", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "approach", "label": "achieves results comparable to", "title": "achieves results comparable to", "to": "best task-specific approaches", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "approach", "label": "comparable_to", "title": "comparable_to", "to": "task-specific approaches", "width": 3.7600000000000002}, {"arrows": "to", "color": "#CC7700", "from": "Convolutional Neural Networks", "label": "is a", "title": "is a", "to": "network type", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Convolutional Neural Networks", "label": "models", "title": "models", "to": "salience", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "Convolutional Neural Networks", "label": "improves", "title": "improves", "to": "Real-time Performance", "width": 3.64}, {"arrows": "to", "color": "#CC7700", "from": "Convolutional Neural Networks", "label": "contributes_to", "title": "contributes_to", "to": "Bounding Box Calibration", "width": 3.4000000000000004}, {"arrows": "to", "color": "#CC7700", "from": "Convolutional Neural Networks", "label": "is_a", "title": "is_a", "to": "Architecture", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "Multiple Instance Learning", "label": "is a", "title": "is a", "to": "method", "width": 3.64}, {"arrows": "to", "color": "#CC7700", "from": "Image-level Training", "label": "is a", "title": "is a", "to": "training method", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "training method", "label": "applicable_to", "title": "applicable_to", "to": "HMMs", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "training method", "label": "suitable_for", "title": "suitable_for", "to": "Hidden Markov Models", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Arbel\u00e1ez et al. (2009)", "label": "authored", "title": "authored", "to": "Multiscale combinatorial grouping", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Multiscale combinatorial grouping", "label": "presented_in", "title": "presented_in", "to": "CVPR", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Boyd \u0026 Vandenberghe (2004)", "label": "authored", "title": "authored", "to": "Convex optimization", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Convex optimization", "label": "is_publication", "title": "is_publication", "to": "Cambridge University Press", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Bridle (1990)", "label": "authored", "title": "authored", "to": "Probabilistic interpretation of feedforward classification network outputs", "width": 3.82}, {"arrows": "to", "color": "#CC7700", "from": "Probabilistic interpretation", "label": "related_to", "title": "related_to", "to": "Statistical pattern recognition", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "Feedforward classification network outputs", "label": "has_interpretation", "title": "has_interpretation", "to": "Probabilistic interpretation", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Efficient graph-based image segmentation", "label": "published_in", "title": "published_in", "to": "International Journal of Computer Vision (IJCV)", "width": 3.91}, {"arrows": "to", "color": "#CC7700", "from": "Semantic segmentation", "label": "requires", "title": "requires", "to": "Rich feature hierarchies", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "Simultaneous detection and segmentation", "label": "published_in", "title": "published_in", "to": "European Conference on Computer Vision (ECCV)", "width": 3.91}, {"arrows": "to", "color": "#CC7700", "from": "Simultaneous detection and segmentation", "label": "is_task", "title": "is_task", "to": "Fine-grained Localization", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "Graph-based image segmentation", "label": "is_a", "title": "is_a", "to": "Image segmentation", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Image segmentation", "label": "achieved_by", "title": "achieved_by", "to": "probabilistic bottom-up aggregation", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Hariharan et al.", "label": "presented_at", "title": "presented_at", "to": "ECCV", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Hariharan et al.", "label": "published", "title": "published", "to": "Discriminative decorrelation for clustering and classification", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Hariharan et al.", "label": "authored", "title": "authored", "to": "Hypercolumns for object segmentation and fine-grained localization", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Krizhevsky et al.", "label": "presented_at", "title": "presented_at", "to": "NIPS", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Krizhevsky et al.", "label": "developed", "title": "developed", "to": "deep convolutional neural networks", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "NIPS", "label": "published", "title": "published", "to": "Optimal teaching for limited-capacity human learners", "width": 3.0999999999999996}, {"arrows": "to", "color": "#0077CC", "from": "NIPS", "label": "is_publication_venue", "title": "is_publication_venue", "to": "research paper", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "NIPS", "label": "publishes", "title": "publishes", "to": "Angular quantization", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "NIPS", "label": "is_conference_for", "title": "is_conference_for", "to": "machine learning research", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "NIPS", "label": "is_conference", "title": "is_conference", "to": "conference", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "LeCun et al.", "label": "published_in", "title": "published_in", "to": "Proceedings of the IEEE", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Maron \u0026 Lozano-P\u00e9rez", "label": "presented_at", "title": "presented_at", "to": "NIPS", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Pedro O. Pinheiro", "label": "affiliated_with", "title": "affiliated_with", "to": "Idiap Research Institute", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Yeqing Li", "label": "author_of", "title": "author_of", "to": "Deep Sparse Representation", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Yeqing Li", "label": "affiliated_with", "title": "affiliated_with", "to": "University of Texas at Arlington", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Deep Sparse Representation", "label": "presented_at", "title": "presented_at", "to": "CVPR", "width": 3.7600000000000002}, {"arrows": "to", "color": "#CC7700", "from": "Deep Sparse Representation", "label": "is_a", "title": "is_a", "to": "image registration technique", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Chen Chen", "label": "author_of", "title": "author_of", "to": "Deep Sparse Representation", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Chen Chen", "label": "affiliated_with", "title": "affiliated_with", "to": "University of Texas at Arlington", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Fei Yang", "label": "author_of", "title": "author_of", "to": "Deep Sparse Representation", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Junzhou Huang", "label": "author_of", "title": "author_of", "to": "Deep Sparse Representation", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Junzhou Huang", "label": "author_of", "title": "author_of", "to": "Deep Sparse representation", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Junzhou Huang", "label": "affiliated_with", "title": "affiliated_with", "to": "University of Texas at Arlington", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Menlo Park", "label": "located_in", "title": "located_in", "to": "USA", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Li_Deep_Sparse_Representation_2015_CVPR_paper.pdf", "label": "contains", "title": "contains", "to": "Deep Sparse Representation", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "proposes", "title": "proposes", "to": "similarity measure", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "addresses", "title": "addresses", "to": "limitation", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "proposes", "title": "proposes", "to": "new deep architecture", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "authored_by", "title": "authored_by", "to": "Jianping Shi", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "authored_by", "title": "authored_by", "to": "Li Xu", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "introduces", "title": "introduces", "to": "parametrization of the trifocal tensor", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "proposes", "title": "proposes", "to": "Weighted Heat Kernel Signature (W-HKS)", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "addresses", "title": "addresses", "to": "Human Detection", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "proposes", "title": "proposes", "to": "combination models", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "introduces", "title": "introduces", "to": "HOG-III features", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "introduces", "title": "introduces", "to": "weighted-NMS fusion algorithm", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "addresses", "title": "addresses", "to": "fundamental matrix estimation problem", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "addresses", "title": "addresses", "to": "problem of estimating and removing non-uniform motion blur", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "addresses", "title": "addresses", "to": "problem", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "presents", "title": "presents", "to": "framework", "width": 3.9699999999999998}, {"arrows": "to", "color": "#CC7700", "from": "paper", "label": "proposes", "title": "proposes", "to": "sub-categorization model", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "discusses", "title": "discusses", "to": "Edge Radiance Profiles", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "proposes", "title": "proposes", "to": "Interactive Machine Teaching algorithm", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "proposes", "title": "proposes", "to": "unified co-saliency detection framework", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "proposes", "title": "proposes", "to": "approach", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "addresses", "title": "addresses", "to": "disparity refinement", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "investigates", "title": "investigates", "to": "impact of parameters", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "describes", "title": "describes", "to": "depth image enhancement method", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "addresses", "title": "addresses", "to": "pixel-level segmentation", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "uses", "title": "uses", "to": "Long Short Term Memory (LSTM) recurrent neural networks", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "presents", "title": "presents", "to": "algorithm", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "designs", "title": "designs", "to": "pre-training scheme", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "utilizes", "title": "utilizes", "to": "power factorization", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "utilizes", "title": "utilizes", "to": "GPCA", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "influenced_by", "title": "influenced_by", "to": "Sparse representation", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "investigates", "title": "investigates", "to": "CNN architectures", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "addresses", "title": "addresses", "to": "unknown sparsity", "width": 3.82}, {"arrows": "to", "color": "#CC7700", "from": "paper", "label": "discusses", "title": "discusses", "to": "visual attributes", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "appears in", "title": "appears in", "to": "CVPR", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "proposes", "title": "proposes", "to": "semantic class label graph", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "introduces", "title": "introduces", "to": "absorbing Markov chain process", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "introduces", "title": "introduces", "to": "two principled approaches", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "presents", "title": "presents", "to": "supplementary material", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "aims_to_improve", "title": "aims_to_improve", "to": "accuracy", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "compares_against", "title": "compares_against", "to": "KITTI dataset", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "addresses_problem", "title": "addresses_problem", "to": "coding and dictionary learning", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "introduces", "title": "introduces", "to": "geodesic-preserving method", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "proposes", "title": "proposes", "to": "solution", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "investigates", "title": "investigates", "to": "salience model", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "presents", "title": "presents", "to": "method", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "evaluated_on", "title": "evaluated_on", "to": "KITTI benchmark", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "evaluated_on", "title": "evaluated_on", "to": "MPI Sintel benchmark", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "evaluated_on", "title": "evaluated_on", "to": "Middlebury benchmark", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "authored_by", "title": "authored_by", "to": "Suha Kwak", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "authored_by", "title": "authored_by", "to": "Cordelia Schmid", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "title", "title": "title", "to": "Unsupervised Object Discovery", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "year", "title": "year", "to": "2015", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "addresses", "title": "addresses", "to": "generic instance search problem", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "proposes", "title": "proposes", "to": "shape feature learning scheme", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "addresses", "title": "addresses", "to": "challenge", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "demonstrates", "title": "demonstrates", "to": "system\u0027s performance", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "addresses", "title": "addresses", "to": "image restoration", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "proposes", "title": "proposes", "to": "\u21130TV-PADMM", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "introduces", "title": "introduces", "to": "BOLD (Binary Online Learned Descriptor)", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "uses", "title": "uses", "to": "multiple instance learning framework", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "studies_problem", "title": "studies_problem", "to": "absolute pose of a perspective camera", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "addresses_problem", "title": "addresses_problem", "to": "pose estimation", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "addresses", "title": "addresses", "to": "structure and motion estimation", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "addresses", "title": "addresses", "to": "age invariant face recognition", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "proposes", "title": "proposes", "to": "measure of salience", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "proposes", "title": "proposes", "to": "geo-semantic segmentation method", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "introduces", "title": "introduces", "to": "Markov Chain approach", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "applies", "title": "applies", "to": "Hierarchical approaches", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "focuses_on", "title": "focuses_on", "to": "efficient computation", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "focuses_on", "title": "focuses_on", "to": "salient region detection", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "applies", "title": "applies", "to": "visual salience detection", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "investigates", "title": "investigates", "to": "Sparse Kernel Multi-task Learning (SKMTL) models", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "argues", "title": "argues", "to": "multiple homographies estimation", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "addresses", "title": "addresses", "to": "photometric stereo", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "proposes", "title": "proposes", "to": "mesh deformation approach", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "proposes", "title": "proposes", "to": "deep hashing (DH) approach", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "develops", "title": "develops", "to": "deep neural network", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "brings together", "title": "brings together", "to": "object detection advancements", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "brings together", "title": "brings together", "to": "crowd engineering", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "addresses", "title": "addresses", "to": "problem of learning long binary codes", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "overcomes", "title": "overcomes", "to": "lack of effective regularizer", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "overcomes", "title": "overcomes", "to": "high computational cost", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "introduces", "title": "introduces", "to": "sparsity encouraging regularizer", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "presented_by", "title": "presented_by", "to": "Chen Xianjie", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "authored_by", "title": "authored_by", "to": "Alan Yuilie", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "proposes", "title": "proposes", "to": "unified approach", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "addresses", "title": "addresses", "to": "egocentric video summarization", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "discusses", "title": "discusses", "to": "image alignment", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "paper", "label": "focuses on", "title": "focuses on", "to": "complex system", "width": 3.25}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "addresses", "title": "addresses", "to": "problem of recovering a complete 3D model", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "investigates", "title": "investigates", "to": "viewpoint-based shape matching", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "investigates", "title": "investigates", "to": "3D deformation", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "investigates", "title": "investigates", "to": "3D mesh analysis", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "investigates", "title": "investigates", "to": "3D model synthesis", "width": 3.73}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "addresses", "title": "addresses", "to": "limitations", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "describes", "title": "describes", "to": "method for selecting features", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "makes_contribution", "title": "makes_contribution", "to": "analysis of impact", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "introduces", "title": "introduces", "to": "unsupervised method", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "presents", "title": "presents", "to": "robust regression method", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "authored_by", "title": "authored_by", "to": "Hui Wu", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "authored_by", "title": "authored_by", "to": "Richard Souvenir", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "addresses", "title": "addresses", "to": "surface shape reconstruction problem", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "proposes", "title": "proposes", "to": "volumetric deformation model", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "introduces", "title": "introduces", "to": "Adaptive Region Pooling", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "demonstrates", "title": "demonstrates", "to": "effectiveness of ARP", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "proposes", "title": "proposes", "to": "new no-referece (NR) image quality assessment (IQA) framework", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "paper", "label": "addresses", "title": "addresses", "to": "video segmentation", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "similarity measure", "label": "based_on", "title": "based_on", "to": "deep sparse representation", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "images", "label": "sparsified_in", "title": "sparsified_in", "to": "gradient domain", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "images", "label": "sparsified_in", "title": "sparsified_in", "to": "frequency domain", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "images", "label": "collected_from", "title": "collected_from", "to": "4,665 KM2", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "images", "label": "have", "title": "have", "to": "altered gaze direction", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "images", "label": "contain", "title": "contain", "to": "artifacts", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "limitations", "label": "concern", "title": "concern", "to": "spatially-varying intensity distortions", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "state-of-the-art methods", "label": "estimates", "title": "estimates", "to": "ground-truth eye-gaze", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "RASL", "label": "addresses", "title": "addresses", "to": "spatially-varying intensity distortions", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "RASL", "label": "achieves", "title": "achieves", "to": "robustness", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "RASL", "label": "achieves", "title": "achieves", "to": "accuracy", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "RASL", "label": "achieves", "title": "achieves", "to": "efficiency", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "RASL", "label": "utilizes", "title": "utilizes", "to": "sparse decomposition", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "RASL", "label": "utilizes", "title": "utilizes", "to": "low-rank decomposition", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "accuracy", "label": "is_characteristic_of", "title": "is_characteristic_of", "to": "state-of-the-art", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "accuracy", "label": "is", "title": "is", "to": "comparable", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Deformable medical image registration", "label": "is_method_of", "title": "is_method_of", "to": "IEEE Transactions on Medical Imaging", "width": 3.7600000000000002}, {"arrows": "to", "color": "#CC7700", "from": "IEEE Transactions on Medical Imaging", "label": "is_a", "title": "is_a", "to": "journal", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "IEEE Transactions on Medical Imaging", "label": "has_volume", "title": "has_volume", "to": "32", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "IEEE Transactions on Medical Imaging", "label": "has_issue", "title": "has_issue", "to": "7", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Robust principal component analysis", "label": "published_in", "title": "published_in", "to": "Journal of the ACM", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Journal of the ACM", "label": "has_volume", "title": "has_volume", "to": "58", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Journal of the ACM", "label": "has_issue", "title": "has_issue", "to": "3", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "Deep sparse representation", "label": "is_type_of", "title": "is_type_of", "to": "Deep Learning", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "Deep Learning", "label": "surpasses", "title": "surpasses", "to": "Traditional Deep Features", "width": 3.46}, {"arrows": "to", "color": "#0077CC", "from": "Deep Learning", "label": "utilizes", "title": "utilizes", "to": "CNNs", "width": 3.7600000000000002}, {"arrows": "to", "color": "#CC7700", "from": "Deep Learning", "label": "relates to", "title": "relates to", "to": "Convolutional deep belief networks", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Intensity Distortions", "label": "limits", "title": "limits", "to": "existing approaches", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "existing approaches", "label": "rely_on", "title": "rely_on", "to": "predefined weights", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "existing approaches", "label": "suffer_from", "title": "suffer_from", "to": "computational cost", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "existing approaches", "label": "depend_on", "title": "depend_on", "to": "initialization", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "existing approaches", "label": "experience", "title": "experience", "to": "error accumulation", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "existing approaches", "label": "relies on", "title": "relies on", "to": "incomplete constraint satisfaction", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "existing approaches", "label": "require", "title": "require", "to": "user interaction", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "existing approaches", "label": "rely_on", "title": "rely_on", "to": "surface-based strategies", "width": 3.88}, {"arrows": "to", "color": "#CC7700", "from": "journal", "label": "is_a", "title": "is_a", "to": "Journal of the ACM", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "Pattern Recognition", "label": "is_a", "title": "is_a", "to": "journal", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Pattern Recognition", "label": "has_volume", "title": "has_volume", "to": "35", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Pattern Recognition", "label": "has_issue", "title": "has_issue", "to": "2", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Pattern Recognition", "label": "published", "title": "published", "to": "Automatic color constancy algorithm selection and combination", "width": 3.7600000000000002}, {"arrows": "to", "color": "#CC7700", "from": "IEEE Transactions on Geoscience and Remote Sensing", "label": "is_a", "title": "is_a", "to": "journal", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "IEEE Transactions on Geoscience and Remote Sensing", "label": "has_volume", "title": "has_volume", "to": "46", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "IEEE Transactions on Geoscience and Remote Sensing", "label": "published", "title": "published", "to": "Automatic analysis of the difference image", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "IEEE Transactions on Geoscience and Remote Sensing", "label": "published", "title": "published", "to": "A latent analysis of earth surface dynamic evolution", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "IEEE Transactions on Geoscientific and Remote Sensing", "label": "has_issue", "title": "has_issue", "to": "5", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "Medical image analysis", "label": "is_a", "title": "is_a", "to": "journal", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Medical image analysis", "label": "has_volume", "title": "has_volume", "to": "18", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Medical image analysis", "label": "has_issue", "title": "has_issue", "to": "6", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "H", "label": "authored", "title": "authored", "to": "Landmark matching based retinal image alignment", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Maguire", "label": "authored", "title": "authored", "to": "Landmark matching based retinal image alignment", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Brainard", "label": "authored", "title": "authored", "to": "Landmark matching based retinal image alignment", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Tzimiropouulos", "label": "authored", "title": "authored", "to": "Robust FFT-based scale-invariant image registration", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Argyriou", "label": "authored", "title": "authored", "to": "Robust FFT-based scale-invariant image registration", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Zafeiriou", "label": "authored", "title": "authored", "to": "Robust FFT-based scale-invariant image registration", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Zafeiriou", "label": "authored", "title": "authored", "to": "Unifying Holistic and Parts-Based Deformable Model Fitting", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Stathaki", "label": "authored", "title": "authored", "to": "Robust FFT-based scale-invariant image registration", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Viola", "label": "authored", "title": "authored", "to": "Alignment by maximization of mutual information", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Viola", "label": "authored", "title": "authored", "to": "Rapid object detection", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Wells III", "label": "authored", "title": "authored", "to": "Alignment by maximization of mutual information", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Gross", "label": "authored", "title": "authored", "to": "Multi-pie", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Matthews", "label": "authored", "title": "authored", "to": "Multi-pie", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Cohn", "label": "authored", "title": "authored", "to": "Multi-pie", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Kanade", "label": "authored", "title": "authored", "to": "Multi-pie", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Baker", "label": "authored", "title": "authored", "to": "Multi-pie", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Zitova", "label": "authored", "title": "authored", "to": "Image registration methods", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Flusser", "label": "authored", "title": "authored", "to": "Image registration methods", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Tsung-Yi Lin", "label": "author_of", "title": "author_of", "to": "Lin_Learning_Deep_Representations_2015_CVPR_paper", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Tsung-Yi Lin", "label": "affiliated_with", "title": "affiliated_with", "to": "Cornell Tech", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Lin_Learning_Deep_Representations_2015_CVPR_paper", "label": "published_in", "title": "published_in", "to": "Image and vision computing", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Lin_Learning_Deep_Representations_2015_CVPR_paper", "label": "volume", "title": "volume", "to": "21", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Lin_Learning_Deep_Representations_2015_CVPR_paper", "label": "page_range", "title": "page_range", "to": "977\u20131000", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Yin Cui", "label": "author_of", "title": "author_of", "to": "Lin_Learning_Deep_Representations_2015_CVPR_paper", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Yin Cui", "label": "affiliated_with", "title": "affiliated_with", "to": "Cornell Tech", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Serge Belongie", "label": "author_of", "title": "author_of", "to": "Lin_Learning_Deep_Representations_2015_CVPR_paper", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Serge Belongie", "label": "affiliation", "title": "affiliation", "to": "Cornell Tech", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "James Hays", "label": "author_of", "title": "author_of", "to": "Lin_Learning_Deep_Representations_2015_CVPR_paper", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "James Hays", "label": "affiliated_with", "title": "affiliated_with", "to": "Brown University", "width": 3.94}, {"arrows": "to", "color": "#00CC77", "from": "geo-tagged images", "label": "spurs", "title": "spurs", "to": "image-based geolocalization algorithms", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "image-based geolocalization algorithms", "label": "matches", "title": "matches", "to": "ground-level images", "width": 3.64}, {"arrows": "to", "color": "#CC7700", "from": "Where-CNN", "label": "is", "title": "is", "to": "deep learning approach", "width": 3.79}, {"arrows": "to", "color": "#CC7700", "from": "Where-CNN", "label": "inspired_by", "title": "inspired_by", "to": "face verification", "width": 3.67}, {"arrows": "to", "color": "#CC7700", "from": "face verification", "label": "is", "title": "is", "to": "cross-dataset task", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "dataset", "label": "contains", "title": "contains", "to": "78K aligned cross-view image pairs", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "dataset", "label": "includes", "title": "includes", "to": "108 crowd scenes", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "dataset", "label": "contains", "title": "contains", "to": "nearly 200,000 head", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "dataset", "label": "has_size", "title": "has_size", "to": "200,000 head annotations", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "dataset", "label": "evaluates", "title": "evaluates", "to": "cross-scene crowd counting methods", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "dataset", "label": "is", "title": "is", "to": "new", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "dataset", "label": "forms", "title": "forms", "to": "pose-dependent model of joint limits", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "dataset", "label": "is_available_for", "title": "is_available_for", "to": "research purposes", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "dataset", "label": "is", "title": "is", "to": "annotated through crowdsourcing", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "dataset", "label": "serves_as", "title": "serves_as", "to": "ground truth", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "dataset", "label": "complements", "title": "complements", "to": "existing annotations", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "dataset", "label": "offers", "title": "offers", "to": "new possibilities", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "matching views", "label": "are", "title": "are", "to": "close", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "mismatched views", "label": "are", "title": "are", "to": "far apart", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "Geolocalization", "label": "relies_on", "title": "relies_on", "to": "Aerial Imagery", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Cross-View Matching", "label": "uses", "title": "uses", "to": "Feature Representation", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Cross-View Matching", "label": "generalizes_to", "title": "generalizes_to", "to": "Novel Locations", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "Deep Convolutional Neural Networks", "label": "used_in", "title": "used_in", "to": "Image Classification", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "Image Classification", "label": "uses", "title": "uses", "to": "Deep Learning", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Google Street View", "label": "captures", "title": "captures", "to": "World", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Distinctive Image Features", "label": "derived_from", "title": "derived_from", "to": "Scale-Invariant Keypoints", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Deepface", "label": "achieves", "title": "achieves", "to": "Human-Level Performance", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "Learning a Similarity Metric", "label": "applied_to", "title": "applied_to", "to": "Face Verification", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Chopra et al. (2005)", "label": "published_in", "title": "published_in", "to": "CVPR", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Lin et al. (2013)", "label": "published_in", "title": "published_in", "to": "CVPR", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Bansal \u0026 Daniilidis (2014)", "label": "published_in", "title": "published_in", "to": "CVPR", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "van der Maaten \u0026 Hinton (2008)", "label": "published_in", "title": "published_in", "to": "JMLR", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "JMLR", "label": "published", "title": "published", "to": "Matching words and pictures", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Xiao et al. (2010)", "label": "published_in", "title": "published_in", "to": "CVPR", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Felzenszwalb et al. (2010)", "label": "published_in", "title": "published_in", "to": "PAMI", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "PAMI", "label": "is_publication_venue", "title": "is_publication_venue", "to": "research paper", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "PAMI", "label": "is_publication_platform_for", "title": "is_publication_platform_for", "to": "minimization in vision", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Cornell Tech", "label": "affiliated_with", "title": "affiliated_with", "to": "Serge Belongie", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Junho Yim", "label": "author_of", "title": "author_of", "to": "Rotating Your Face Using Multi-task Deep Neural Network", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Heechul Jung", "label": "author_of", "title": "author_of", "to": "Rotating Your face Using Multi-task Deep Neural Network", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Heechul Jung", "label": "member_of", "title": "member_of", "to": "School of Electrical Engineerin, KAIST", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "ByungIn Yoo", "label": "author_of", "title": "author_of", "to": "Rotating Your Face Using Multi-task Deep Neural Network", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "ByungIn Yoo", "label": "member_of", "title": "member_of", "to": "Samsung Advanced Institute of Technology", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Changkyu Choi", "label": "author_of", "title": "author_of", "to": "Rotating Your Face Using Multi-task Deep Neural Network", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Changkyu Choi", "label": "member_of", "title": "member_of", "to": "Samsung Advanced Institute of Technology", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Dusik Park", "label": "author_of", "title": "author_of", "to": "Rotating Your Face Using Multi-task Deep Neural Network", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Dusik Park", "label": "member_of", "title": "member_of", "to": "Samsung Advanced Institute of Technology", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Junmo Kim", "label": "author_of", "title": "author_of", "to": "Rotating Your Face Using Multi-task Deep Neural Network", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Junmo Kim", "label": "member_of", "title": "member_of", "to": "School of Electrical Engineering, KAIST", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Junmo Kim", "label": "affiliated_with", "title": "affiliated_with", "to": "School of Electrical Engineering", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Junmo Kim", "label": "email", "title": "email", "to": "junmo.kim@kaisten.ac.kr", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Junmo Kim", "label": "works_at", "title": "works_at", "to": "KAIST", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "Junmo Kim", "label": "is_author_of", "title": "is_author_of", "to": "Unsupervised Simultaneous Orthogonal Basis Clustering Feature Selection", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Junmo Kim", "label": "affiliated_with", "title": "affiliated_with", "to": "Department of Automation", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Yim_Rotating_Your_Face_2015_CVPR_paper.pdf", "label": "is_a", "title": "is_a", "to": "CVPR paper", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "CVPR paper", "label": "is_a", "title": "is_a", "to": "publication", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Face recognition", "label": "is_problem_of", "title": "is_problem_of", "to": "viewpoint and illumination changes", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Face recognition", "label": "evaluated_by", "title": "evaluated_by", "to": "Feret methodology", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "new deep architecture", "label": "based_on", "title": "based_on", "to": "novel type of multitask learning", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "target-pose face image", "label": "derived_from", "title": "derived_from", "to": "arbitrary pose and illumination image", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "target pose", "label": "controlled_by", "title": "controlled_by", "to": "user\u2019s intention", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "multi-task model", "label": "improves", "title": "improves", "to": "identity preservation", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Controlled Pose Image (CPI)", "label": "used_for", "title": "used_for", "to": "pose-illumination- invariant feature", "width": 3.73}, {"arrows": "to", "color": "#0077CC", "from": "MultiPIE dataset", "label": "is_used_for", "title": "is_used_for", "to": "Face Recognition", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Proposed method", "label": "outperforms", "title": "outperforms", "to": "state-of-the-art algorithms", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Proposed method", "label": "operates_on", "title": "operates_on", "to": "MultiPIE dataset", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Proposed method", "label": "achieves", "title": "achieves", "to": "state-of-the-art detection performance", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Proposed method", "label": "runs_at", "title": "runs_at", "to": "14 FPS", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Proposed method", "label": "runs_at", "title": "runs_at", "to": "100 FPS", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Ramakrishna Vedantam", "label": "author_of", "title": "author_of", "to": "CIDEr", "width": 3.91}, {"arrows": "to", "color": "#CC7700", "from": "CIDEr", "label": "is_a", "title": "is_a", "to": "Image Description Evaluation", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "CIDEr", "label": "presented_at", "title": "presented_at", "to": "CVPR", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "CIDEr", "label": "year", "title": "year", "to": "2015", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "CIDEr", "label": "is_named", "title": "is_named", "to": "CIDEr-D", "width": 3.88}, {"arrows": "to", "color": "#CC7700", "from": "CIDEr", "label": "is_a", "title": "is_a", "to": "Automated Metrics", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "C. Lawrence Zitnick", "label": "author_of", "title": "author_of", "to": "CIDEr", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "C. Lawrence Zitnick", "label": "affiliated_with", "title": "affiliated_with", "to": "Microsoft Research", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Devi Parikh", "label": "author_of", "title": "author_of", "to": "CIDEr", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Devi Parikh", "label": "affiliated_with", "title": "affiliated_with", "to": "Virginia Tech", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "KAIST", "label": "has_department", "title": "has_department", "to": "School of Electrical Engineering", "width": 3.64}, {"arrows": "to", "color": "#CC7700", "from": "Image Description", "label": "is_challenge_in", "title": "is_challenge_in", "to": "Computer Vision", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "Image Description", "label": "is_challenge_in", "title": "is_challenge_in", "to": "Natural Language Processing", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "Image Description", "label": "related_to", "title": "related_to", "to": "Sequence Learning", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Image Description", "label": "utilizes", "title": "utilizes", "to": "perceptual representations", "width": 3.34}, {"arrows": "to", "color": "#CC7700", "from": "Computer Vision", "label": "studies", "title": "studies", "to": "Hand-Object Interaction", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "Computer Vision", "label": "encompasses", "title": "encompasses", "to": "Material Recognition", "width": 3.64}, {"arrows": "to", "color": "#CC7700", "from": "Computer Vision", "label": "includes", "title": "includes", "to": "Salient Object Detection", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "Computer Vision", "label": "is_conference_of", "title": "is_conference_of", "to": "IEEE", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "Natural Language Processing", "label": "is_related_to", "title": "is_related_to", "to": "Computer Vision", "width": 3.7600000000000002}, {"arrows": "to", "color": "#CC7700", "from": "Object Detection", "label": "contributes_to", "title": "contributes_to", "to": "Image Description", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "Object Detection", "label": "field_of_study", "title": "field_of_study", "to": "Computer Vision", "width": 3.25}, {"arrows": "to", "color": "#CC7700", "from": "Object Detection", "label": "influenced_by", "title": "influenced_by", "to": "Convolutional Neural Networks (CNNs)", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Object Detection", "label": "evaluated_on", "title": "evaluated_on", "to": "PASCAL VOC 2007", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Object Detection", "label": "evaluated_on", "title": "evaluated_on", "to": "PASCUAL VOC 2012", "width": 3.88}, {"arrows": "to", "color": "#CC7700", "from": "Attribute Classification", "label": "contributes_to", "title": "contributes_to", "to": "Image Description", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Paradigm", "label": "uses", "title": "uses", "to": "Human Consensus", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Paradigm", "label": "consists_of", "title": "consists_of", "to": "Triplet-based Method", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Paradigm", "label": "consists_of", "title": "consists_of", "to": "Automated Metric", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Paradigm", "label": "consists_of", "title": "consists_of", "to": "Datasets", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Datasets", "label": "includes", "title": "includes", "to": "PASCAL-50S", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Datasets", "label": "includes", "title": "includes", "to": "ABSTRACT-50S", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "PASCAL-50S", "label": "is_dataset", "title": "is_dataset", "to": "dataset", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Metric", "label": "captures", "title": "captures", "to": "Human Judgment", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "nsensus", "label": "is_dataset", "title": "is_dataset", "to": "dataset", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "metric", "label": "captures", "title": "captures", "to": "human judgment", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "metric", "label": "better_than", "title": "better_than", "to": "existing metrics", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "sentences", "label": "generated_by", "title": "generated_by", "to": "various sources", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "CIDEr-D", "label": "is_part_of", "title": "is_part_of", "to": "MS COCO evaluation server", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "MS COCO evaluation server", "label": "enables", "title": "enables", "to": "systematic evaluation", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "image description approaches", "label": "evaluated_by", "title": "evaluated_by", "to": "protocol", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "Microsoft Research", "label": "produces", "title": "produces", "to": "CVPR paper", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Microsoft Research", "label": "employs", "title": "employs", "to": "Cem Keskin", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Microsoft Research", "label": "employs", "title": "employs", "to": "Pushmeet Kohli", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Microsoft Research", "label": "employs", "title": "employs", "to": "Shahram Izadi", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Zhenzhong Lan", "label": "is_author_of", "title": "is_author_of", "to": "Beyond Gaussian Pyramid", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Zhenzhong Lan", "label": "affiliated_with", "title": "affiliated_with", "to": "School of Computer Science, Carnegie Mellon University", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Beyond Gaussian Pyramid", "label": "presented_at", "title": "presented_at", "to": "CVPR", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "Alexander G. Hauptmann", "label": "is_author_of", "title": "is_author_of", "to": "Beyond Gaussian Pyramid", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Alexander G. Hauptmann", "label": "affiliated_with", "title": "affiliated_with", "to": "School of Computer Science, Carnegie Mellon University", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Bhiksha Raj", "label": "is_author_of", "title": "is_author_of", "to": "Beyond Gaussian Pyramid", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Bhiksha Raj", "label": "affiliated_with", "title": "affiliated_with", "to": "School of Computer Science, Carnegie Mellon University", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Bhiksha Raj", "label": "has_email", "title": "has_email", "to": "bhiksha@cs.cmu.edu", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Lan_Beyond_Gausian_Pyramid_2015_CVPR_supplemental", "label": "supports_paper", "title": "supports_paper", "to": "Beyond Gaussian Pyramid: Multi-skip Feature Stacking for Action Recognition", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Lan_Beyond_Gausian_Pyramid_2015_CVPR_supplemental", "label": "provides", "title": "provides", "to": "proof of theorem 1", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Lan_Beyond_Gausian_Pyramid_2015_CVPR_supplemental", "label": "provides", "title": "provides", "to": "proof of theorem 2", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Beyond Gaussian Pyramid: Multi-skip Feature Stacking for Action Recognition", "label": "addresses", "title": "addresses", "to": "Action Recognition", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Beyond Gaussian Pyramid: Multi-skip Feature Stacking for Action Recognition", "label": "uses", "title": "uses", "to": "Matrix Bernstein\u0027s Inequality", "width": 3.64}, {"arrows": "to", "color": "#CC7700", "from": "Action Recognition", "label": "relies_on", "title": "relies_on", "to": "Feature Stacking", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Action Recognition", "label": "uses", "title": "uses", "to": "Trajectory Group Selection", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Action Recognition", "label": "uses", "title": "uses", "to": "Fisher Vector Representation", "width": 3.34}, {"arrows": "to", "color": "#0077CC", "from": "Action Recognition", "label": "improves_with", "title": "improves_with", "to": "Trajectories", "width": 3.61}, {"arrows": "to", "color": "#0077CC", "from": "Ming Lin", "label": "affiliated_with", "title": "affiliated_with", "to": "School of Computer Science, Carnegie Mellon University", "width": 3.88}, {"arrows": "to", "color": "#CC7700", "from": "Feature Stacking", "label": "impacts", "title": "impacts", "to": "Condition Number", "width": 3.25}, {"arrows": "to", "color": "#0077CC", "from": "Alexander G. Hauptman", "label": "has_email", "title": "has_email", "to": "cli@cs.cmu.edu", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Kwang In Kim", "label": "author_of", "title": "author_of", "to": "Kim_Local_High-Order_Regularization_2015_CVPR_paper.pdf", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "James Tompkin", "label": "author_of", "title": "author_of", "to": "Kim_Local_High-Order_Regularization_2015_CVPR_paper.pdf", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Hanspeter Pfister", "label": "author_of", "title": "author_of", "to": "Kim_Local_High-Order_RegularIZATION_2015_CVPR_paper.pdf", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Kim_Local_High-Order_RegularIZATION_2015_CVPR_paper.pdf", "label": "is_paper_about", "title": "is_paper_about", "to": "Local High-order Regularization on Data Manifolds", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Christian Theobalt", "label": "author_of", "title": "author_of", "to": "Kim_Local_High-Order_RegularIZATION_2015_CVPR_paper.pdf", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Christian Theobalt", "label": "affiliated_with", "title": "affiliated_with", "to": "MPI for Informatics", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Christian Theobalt", "label": "affiliated_with", "title": "affiliated_with", "to": "Max Planck Institute for Informatics", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Carnegie Mellon University", "label": "has_school", "title": "has_school", "to": "School of Computer Science", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Carnegie Mellon University", "label": "located_in", "title": "located_in", "to": "Pittsburgh", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "School of Computer Science", "label": "part of", "title": "part of", "to": "Beijing Institute of Technology", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Graph Laplacian Regularizer", "label": "suffers_from", "title": "suffers_from", "to": "degeneracy", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Iterated Graph Laplacian", "label": "addresses", "title": "addresses", "to": "degeneracy", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Iterated Graph Laplacian", "label": "incurs", "title": "incurs", "to": "computational complexity", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Proposed Regularizer", "label": "maintains", "title": "maintains", "to": "sparsity", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "Proposed Regularizer", "label": "based_on", "title": "based_on", "to": "local derivative evaluations", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "Approach", "label": "builds", "title": "builds", "to": "manifold approximation", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Approach", "label": "performs", "title": "performs", "to": "Textured 3D Shape Retrieval", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Approach", "label": "evaluated_on", "title": "evaluated_on", "to": "public datasets", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Approach", "label": "evaluated on", "title": "evaluated on", "to": "SED dataset", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "Approach", "label": "evaluated on", "title": "evaluated on", "to": "HKU-IS dataset", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "Approach", "label": "demonstrates", "title": "demonstrates", "to": "Superior Performance", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Approach", "label": "improves", "title": "improves", "to": "Precision", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Approach", "label": "improves", "title": "improves", "to": "Recall", "width": 3.61}, {"arrows": "to", "color": "#0077CC", "from": "Approach", "label": "improves", "title": "improves", "to": "F-measure", "width": 3.58}, {"arrows": "to", "color": "#0077CC", "from": "Approach", "label": "improves", "title": "improves", "to": "Mean Absolute Error", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Approach", "label": "validated_on", "title": "validated_on", "to": "ILSVRC2014 object detection dataset", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "Approach", "label": "deals_with", "title": "deals_with", "to": "human input", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Approach", "label": "uses", "title": "uses", "to": "Markov Decision Process", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Approach", "label": "uses", "title": "uses", "to": "image representation", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Approach", "label": "considers", "title": "considers", "to": "Input features", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Approach", "label": "considers", "title": "considers", "to": "Labels", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Approach", "label": "exhibits", "title": "exhibits", "to": "Strong theoretical properties", "width": 3.61}, {"arrows": "to", "color": "#0077CC", "from": "Approach", "label": "improves", "title": "improves", "to": "Performance", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "Approach", "label": "exhibits", "title": "exhibits", "to": "theoretical properties", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Approach", "label": "exhibits", "title": "exhibits", "to": "performance improvement", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Experiments", "label": "demonstrate", "title": "demonstrate", "to": "effectiveness", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Experiments", "label": "demonstrate", "title": "demonstrate", "to": "efficiency", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Experiments", "label": "demonstrate", "title": "demonstrate", "to": "performance", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Experiments", "label": "demonstrates", "title": "demonstrates", "to": "convincing performance", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Experiments", "label": "demonstrates", "title": "demonstrates", "to": "effectiveness of approach", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Experiments", "label": "uses", "title": "uses", "to": "RMRC dataset", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Experiments", "label": "conducted_on", "title": "conducted_on", "to": "four large image datasets", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Experiments", "label": "demonstrates", "title": "demonstrates", "to": "superior performance", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Experiments", "label": "demonstrates", "title": "demonstrates", "to": "faster convergence", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Experiments", "label": "evaluates_performance_on", "title": "evaluates_performance_on", "to": "large-scale datasets", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Experiments", "label": "demonstrate", "title": "demonstrate", "to": "improvement", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "Experiments", "label": "demonstrates", "title": "demonstrates", "to": "speed-up of up to a factor of 100", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Experiments", "label": "evaluate_on", "title": "evaluate_on", "to": "ImageNET", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "Experiments", "label": "evaluate_on", "title": "evaluate_on", "to": "GIST1M", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "Experiments", "label": "evaluate_on", "title": "evaluate_on", "to": "SUN-attribute", "width": 3.79}, {"arrows": "to", "color": "#CC7700", "from": "Manifold Approximation", "label": "is", "title": "is", "to": "surrogate geometry", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "Manifold Approximation", "label": "is_related_to", "title": "is_related_to", "to": "Laplacian eigenmaps", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "Graph Laplacian Regularization", "label": "is_method_for", "title": "is_method_for", "to": "Semi-Supervised Learning", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "Semi-Supervised Learning", "label": "uses", "title": "uses", "to": "Graph Laplacian Regularization", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "Laplacian eigenmaps", "label": "is_technique_for", "title": "is_technique_for", "to": "dimensionality reduction", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "High-Order Derivatives", "label": "used_in", "title": "used_in", "to": "Hessian eigenmaps", "width": 3.4000000000000004}, {"arrows": "to", "color": "#CC7700", "from": "Hessian eigenmaps", "label": "is_a", "title": "is_a", "to": "locally linear embedding technique", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Hessian eigenmaps", "label": "addresses", "title": "addresses", "to": "high-dimensional data", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "Hessian eigenMaps", "label": "is_technique_for", "title": "is_technique_for", "to": "locally linear embedding", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Normalized cuts", "label": "used_for", "title": "used_for", "to": "image segmentation", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "image segmentation", "label": "uses", "title": "uses", "to": "lossy data compression", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "Reproducing Kernel Hilbert Space (RKHS)", "label": "is_framework_for", "title": "is_framework_for", "to": "Semi-Supervised Learning", "width": 3.25}, {"arrows": "to", "color": "#CC7700", "from": "Reproducing Kernel Hilbert Space (RKHS)", "label": "is_topic_of", "title": "is_topic_of", "to": "cvpr_papers", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "Spectral Clustering", "label": "uses", "title": "uses", "to": "Graph Laplacian Regularization", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "Spectral Clustering", "label": "tutorial_on", "title": "tutorial_on", "to": "Statistics and Computing", "width": 3.91}, {"arrows": "to", "color": "#CC7700", "from": "Spectral Clustering", "label": "is_technique", "title": "is_technique", "to": "clustering", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Plug-in classifiers", "label": "achieves", "title": "achieves", "to": "fast learning rates", "width": 3.0999999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Supervised Learning", "label": "published_by", "title": "published_by", "to": "MIT Press", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Supervised Learning", "label": "year", "title": "year", "to": "2006", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "Supervised Learning", "label": "relates_to", "title": "relates_to", "to": "learning binary codes", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Normalized Cuts", "label": "applied_to", "title": "applied_to", "to": "Image Segmentation", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "Normalized Cuts", "label": "is_work_in", "title": "is_work_in", "to": "graph-based image segmentation", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Image Segmentation", "label": "achieves", "title": "achieves", "to": "73.1% mean class accuracy", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Real Analysis and Probability", "label": "published_by", "title": "published_by", "to": "Cambridge University Press", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "Graphs", "label": "related_to", "title": "related_to", "to": "Manifolds", "width": 3.82}, {"arrows": "to", "color": "#CC7700", "from": "Graph Laplacians", "label": "exhibits", "title": "exhibits", "to": "Pointwise Consistency", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Jianping Shi", "label": "authored", "title": "authored", "to": "Just Noticeable Defocus Blur Detection and Estimation", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Jianping Shi", "label": "contributed_to", "title": "contributed_to", "to": "CVPR", "width": 3.25}, {"arrows": "to", "color": "#0077CC", "from": "Jianping Shi", "label": "affiliated_with", "title": "affiliated_with", "to": "The Chinese University of Hong Kong", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Just Noticeable Defocus Blur Detection and Estimation", "label": "is_paper_of", "title": "is_paper_of", "to": "CVPR", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Li Xu", "label": "authored", "title": "authored", "to": "Just Noticeable Defocus Blur Detection and Estimation", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Li Xu", "label": "contributed_to", "title": "contributed_to", "to": "CVPR", "width": 3.25}, {"arrows": "to", "color": "#0077CC", "from": "Li Xu", "label": "affiliated_with", "title": "affiliated_with", "to": "Lenovo R\u0026T", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Jiaya Jia", "label": "authored", "title": "authored", "to": "Just Noticeable Defocus Blur Detectio and Estimation", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Jiaya Jia", "label": "contributed_to", "title": "contributed_to", "to": "CVPR", "width": 3.25}, {"arrows": "to", "color": "#0077CC", "from": "Jiaya Jia", "label": "affiliated_with", "title": "affiliated_with", "to": "The Chinese University of Hong Kong", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Jiaya Jia", "label": "author of", "title": "author of", "to": "Deep LAC", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Shi_Just_Noticeable_Defocus_2015_CVPR_paper.pdf", "label": "represents", "title": "represents", "to": "Just Noticeable Defocus Blur Detection and Estimation", "width": 3.7600000000000002}, {"arrows": "to", "color": "#00CC77", "from": "just noticeable blur", "label": "caused_by", "title": "caused_by", "to": "defocus", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "just noticeable blur", "label": "spans", "title": "spans", "to": "small number of pixels", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "slight edge blurriness", "label": "contains", "title": "contains", "to": "informative clues", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "informative clues", "label": "related_to", "title": "related_to", "to": "depth", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "blur descriptors", "label": "based_on", "title": "based_on", "to": "local information", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "blur feature", "label": "uses", "title": "uses", "to": "sparse representation", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "blur feature", "label": "uses", "title": "uses", "to": "image decomposition", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "sparse edge representation", "label": "corresponds_to", "title": "corresponds_to", "to": "blur strength estimation", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "age decomposition", "label": "establishes_correspondence", "title": "establishes_correspondence", "to": "sparse edge representation", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "feature", "label": "manifests", "title": "manifests", "to": "generality", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "feature", "label": "manifests", "title": "manifests", "to": "robustness", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Bernt Schiele", "label": "author_of", "title": "author_of", "to": "Filtered Channel Features for Pedestrian Detection", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Bernt Schiele", "label": "is_author_of", "title": "is_author_of", "to": "Taking a Deeper Look at Pedestrians", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Bernt Schiele", "label": "affiliated_with", "title": "affiliated_with", "to": "Max Planck Institute for Informatics", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Filtered Channel Features for Pedestrian Detection", "label": "is_supplemental_to", "title": "is_supplemental_to", "to": "Zhang_Filtered_Feature_Channels_2015_CVPR_supplemental.pdf", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Shanshan Zhang", "label": "author_of", "title": "author_of", "to": "Filtered Channel Features for Pedestrian Detection", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Shanshan Zhang", "label": "authored", "title": "authored", "to": "Zhang_Filtered_Feature_Channels_2015_CVPR_supplemental", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Rodrigo Benenson", "label": "author_of", "title": "author_of", "to": "Filtered Channel Features for Pedestrian Detection", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Rodrigo Benenson", "label": "authored", "title": "authored", "to": "Zhang_Filtered_Feature_Channels_2015_CVPR_supplemental", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Rodrigo Benenson", "label": "is_author_of", "title": "is_author_of", "to": "Taking a Deeper Look at Pedestrians", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Rodrigo Benenson", "label": "affiliated_with", "title": "affiliated_with", "to": "Max Planck Institute for Informatics", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "Checkerboards4x3 model", "label": "is_model", "title": "is_model", "to": "pedestrian detection model", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "Roerei model", "label": "is_model", "title": "is_model", "to": "weaker model", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "filtered channels", "label": "don\u0027t alter", "title": "don\u0027t alter", "to": "areas of pedestrian deemed informative", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "filtered channels", "label": "enable", "title": "enable", "to": "extraction of discriminative information", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "channel U", "label": "used for", "title": "used for", "to": "face", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "channel L", "label": "used for", "title": "used for", "to": "body", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "gradient magnitude channel", "label": "used for", "title": "used for", "to": "body", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "filter usage distribution", "label": "is_similar_across", "title": "is_similar_across", "to": "filter bank families", "width": 3.7600000000000002}, {"arrows": "to", "color": "#CC7700", "from": "filter bank families", "label": "influences", "title": "influences", "to": "filter usage distribution", "width": 3.25}, {"arrows": "to", "color": "#0077CC", "from": "filter", "label": "used as", "title": "used as", "to": "feature for decision tree split nodes", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "filter", "label": "used_as", "title": "used_as", "to": "decision tree split node feature", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "text", "label": "referenced_in", "title": "referenced_in", "to": "Benenson, R., Mathias, M., Tuytelaars, T., \u0026 Van Gools, L. (2013)", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "Roerei, et al.", "label": "related_to", "title": "related_to", "to": "filter usage", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "ACF", "label": "related_to", "title": "related_to", "to": "filter usage", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "decision tree split nodes", "label": "utilizes", "title": "utilizes", "to": "filter features", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "filter features", "label": "used_in", "title": "used_in", "to": "decision tree", "width": 3.61}, {"arrows": "to", "color": "#CC7700", "from": "spatial feature distribution", "label": "related_to", "title": "related_to", "to": "pedestrian detection", "width": 3.0999999999999996}, {"arrows": "to", "color": "#CC7700", "from": "pedestrian detection", "label": "is_precursor_to", "title": "is_precursor_to", "to": "re-identification", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "pedestrian detection", "label": "is_subject_of", "title": "is_subject_of", "to": "research", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "pedestrian detection", "label": "is_a", "title": "is_a", "to": "image analysis task", "width": 3.25}, {"arrows": "to", "color": "#0077CC", "from": "parametrization of the trifocal tensor", "label": "based on", "title": "based on", "to": "quotient Riemannian manifold", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "parametrization of the trifocal tensor", "label": "is", "title": "is", "to": "almost symmetric", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "parametrization of the trifocal tensor", "label": "utilizes", "title": "utilizes", "to": "preferred camera", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "parametrization of the trifocal tensor", "label": "incorporated into", "title": "incorporated into", "to": "optimization techniques on manifolds", "width": 3.82}, {"arrows": "to", "color": "#CC7700", "from": "Riemannian structure", "label": "provides", "title": "provides", "to": "notion of distance", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "distance between trifocal tensors", "label": "produces", "title": "produces", "to": "meaningful results", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "distance between trifocal tensors", "label": "related to", "title": "related to", "to": "Structure from Motion problem", "width": 3.46}, {"arrows": "to", "color": "#0077CC", "from": "work", "label": "investigates", "title": "investigates", "to": "new formulation of the trifocal tensor", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "work", "label": "addresses", "title": "addresses", "to": "challenge", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "work", "label": "derives", "title": "derives", "to": "angular support", "width": 3.91}, {"arrows": "to", "color": "#CC7700", "from": "work", "label": "highlights", "title": "highlights", "to": "foundational approaches", "width": 3.4000000000000004}, {"arrows": "to", "color": "#CC7700", "from": "work", "label": "highlights", "title": "highlights", "to": "importance", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "work", "label": "builds", "title": "builds", "to": "object appearance model", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "work", "label": "uses", "title": "uses", "to": "familiar objects", "width": 3.88}, {"arrows": "to", "color": "#CC7700", "from": "trifocal tensor", "label": "is a", "title": "is a", "to": "tensor", "width": 3.25}, {"arrows": "to", "color": "#0077CC", "from": "Structure from Motion", "label": "investigates", "title": "investigates", "to": "Trifocal Tensor", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "Structure from Motion", "label": "is_field_of", "title": "is_field_of", "to": "Geometric Computer Vision", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Trifocal Tensor", "label": "measured between", "title": "measured between", "to": "distances", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "Camera Calibration", "label": "related to", "title": "related to", "to": "Geometric Computer Vision", "width": 3.64}, {"arrows": "to", "color": "#CC7700", "from": "Camera Calibration", "label": "related_to", "title": "related_to", "to": "Absolute Pose Estimation", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Optimization Algorithms", "label": "written by", "title": "written by", "to": "Absil, Mahony, and Sepulchre", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Nonlinear Programming", "label": "written by", "title": "written by", "to": "Bertsekas", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Manopt", "label": "developed by", "title": "developed by", "to": "Boumal, Mishra, Absil, and Sepulchre", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "Lines and points", "label": "described in", "title": "described in", "to": "Hartley\u0027s work", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "Multiple View Geometry", "label": "written by", "title": "written by", "to": "Hartley and Zisserman", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Hartley, R. I.", "label": "authored", "title": "authored", "to": "views and the trifocal tensor", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Hartley, R. I.", "label": "authored", "title": "authored", "to": "Projective reconstruction from line correspondences", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Hartley, R. I.", "label": "coauthored", "title": "coauthored", "to": "Multiple View Geometry in Computer Vision", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "views and the trifocal tensor", "label": "published in", "title": "published in", "to": "Int. J. Comput. Vision", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Projective reconstruction from line correspondences", "label": "presented at", "title": "presented at", "to": "IEEE Conf. on Computer Vision and Pattern Recognition", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Papapdoulo, T.", "label": "authored", "title": "authored", "to": "A new characterization of the trifocal tensor", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "A new characterization of the trifocal tensor", "label": "presented at", "title": "presented at", "to": "European Conference on Computer Vision", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "European Conference on Computer Vision", "label": "published_in", "title": "published_in", "to": "Image and Vision Computing", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "European Conference on Computer Vision", "label": "is_conference_of", "title": "is_conference_of", "to": "Computer Vision", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "European Conference on Computer Vision", "label": "published", "title": "published", "to": "Numerically stable optimization", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Kendall, D. G.", "label": "authored", "title": "authored", "to": "Shape Manifolds, Procustean Metrics, and Complex Projective Spaces", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Shape Manifolds, Procustean Metrics, and Complex Projective Spaces", "label": "published in", "title": "published in", "to": "Bulletin of the London Mathematical Society", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "Torr, P.", "label": "authored", "title": "authored", "to": "Robust parameterization and computation of the trifocal tensor", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Torr, P.", "label": "authored", "title": "authored", "to": "Robust parameterization", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Robust parameterization and computation of the trifocal tensor", "label": "published in", "title": "published in", "to": "Image and Vision Computing", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "Multiple View Geometry in Computer Vision", "label": "published by", "title": "published by", "to": "Cambridge University Press", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Weng, J.", "label": "authored", "title": "authored", "to": "Motion and structure", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Grasp Laboratory", "label": "located_in", "title": "located_in", "to": "University of Pennsylvania", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "tron@seas.upenn.edu", "label": "email_address_of", "title": "email_address_of", "to": "Roberto Tron", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "kostas@cis.upenn.edu", "label": "email_address_of", "title": "email_address_of", "to": "Kostas Daniilidis", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Fast 2D Border Ownership Assignment", "label": "authored_by", "title": "authored_by", "to": "Cornelia Ferm\u00fcller", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Fast 2D Border Ownership Assignment", "label": "authored_by", "title": "authored_by", "to": "Ching L. Teo", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Fast 2D Border Ownership Assignment", "label": "authored_by", "title": "authored_by", "to": "Yiannis Aloimonos", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Cornelia Ferm\u00fcller", "label": "affiliated_with", "title": "affiliated_with", "to": "Computer Vision Lab", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Ching L. Teo", "label": "affiliated_with", "title": "affiliated_with", "to": "Computer Vision\u003c0xC2\u003e\u003c0xA0\u003eLab", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Teo_Fast_2D_Border_2015_CVPR_paper.pdf", "label": "is_a", "title": "is_a", "to": "PDF document", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Structured Random Forests (SRF)", "label": "used_for", "title": "used_for", "to": "boundary detection", "width": 3.7600000000000002}, {"arrows": "to", "color": "#CC7700", "from": "Structured Random Forests (SRF)", "label": "related_to", "title": "related_to", "to": "Image Segmentation", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "shape descriptors", "label": "are_type_of", "title": "are_type_of", "to": "HoG-like descriptors", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "shape descriptors", "label": "incorporates", "title": "incorporates", "to": "photometric features", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "spectral properties", "label": "analyzed_with", "title": "analyzed_with", "to": "PCA", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "semi-global grouping cues", "label": "indicate", "title": "indicate", "to": "perceived depth", "width": 3.46}, {"arrows": "to", "color": "#0077CC", "from": "Experimental results", "label": "evaluated_on", "title": "evaluated_on", "to": "Berkeley Segmentation Dataset", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Experimental results", "label": "evaluated_on", "title": "evaluated_on", "to": "NYU Depth V2 dataset", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Experimental results", "label": "show", "title": "show", "to": "Method\u0027s effectiveness", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Experimental results", "label": "demonstrate", "title": "demonstrate", "to": "MoG Regression outperforms subspace clustering methods", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Experimental results", "label": "validate", "title": "validate", "to": "proposed approach", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Experimental results", "label": "validate", "title": "validate", "to": "effectiveness of proposed methods", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Feature Extraction", "label": "employs", "title": "employs", "to": "HoG", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Feature Extraction", "label": "employs", "title": "employs", "to": "PCA", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Cheng et al. (2014)", "label": "developed", "title": "developed", "to": "Bing", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "Dalal \u0026 Triggs (2005)", "label": "introduced", "title": "introduced", "to": "Histograms of oriented gradients", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Dalal \u0026 Triggs (2005)", "label": "introduces", "title": "introduces", "to": "HOG features", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Histograms of oriented gradients", "label": "used_for", "title": "used_for", "to": "human detection", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "Binarized normed gradients", "label": "used_for", "title": "used_for", "to": "objectness estimation", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "objectness estimation", "label": "processed_at", "title": "processed_at", "to": "300fps", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Fast edge detection", "label": "uses", "title": "uses", "to": "structured forests", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Fast feature pyramids", "label": "used_for", "title": "used_for", "to": "object detection", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "object detection", "label": "faces_challenge", "title": "faces_challenge", "to": "3D scenes", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "object detection", "label": "uses", "title": "uses", "to": "grammar models", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Category-independent object proposals", "label": "has_characteristic", "title": "has_characteristic", "to": "diverse ranking", "width": 3.88}, {"arrows": "to", "color": "#CC7700", "from": "Extremely randomized trees", "label": "is_a", "title": "is_a", "to": "machine learning algorithm", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Perceptual organization", "label": "used_for", "title": "used_for", "to": "recognition of indoor scenes", "width": 3.82}, {"arrows": "to", "color": "#CC7700", "from": "Random decision forests", "label": "is_a", "title": "is_a", "to": "machine learning algorithm", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Computer Vision Lab", "label": "located_at", "title": "located_at", "to": "University of Maryland", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "University of Maryland", "label": "located_in", "title": "located_in", "to": "College Park, MD", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Mostafa Abdelrahman", "label": "author_of", "title": "author_of", "to": "Heat Diffusion Over Weighted Manifolds", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Mostafa Abdelrahman", "label": "affiliated_with", "title": "affiliated_with", "to": "Electrical Engineering Department", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Mostafa Abdelrahman", "label": "located_at", "title": "located_at", "to": "Assiut University", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "Heat Diffusion Over Weighted Manifolds", "label": "is_a", "title": "is_a", "to": "descriptor", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Aly Farag", "label": "author_of", "title": "author_of", "to": "Heat Diffusion Over Weighted Manifolds", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Aly Farag", "label": "affiliated_with", "title": "affiliated_with", "to": "CVIP Lab", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Aly Farag", "label": "located_at", "title": "located_at", "to": "University of Louisville", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "David Swanson", "label": "author_of", "title": "author_of", "to": "Heat Diffusion Over Weighted Manifolds", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "David Swanson", "label": "affiliated_with", "title": "affiliated_with", "to": "Department of Mathematics", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "David Swanson", "label": "affiliated_with", "title": "affiliated_with", "to": "University of Louisville", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Moumen T. El-Melegy", "label": "author_of", "title": "author_of", "to": "Heat Diffusion Over Weighted Manifolds", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Moumen T. El-Melegy", "label": "affiliated_with", "title": "affiliated_with", "to": "Electrical Engineering Department", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Moumen T. El-Melegy", "label": "located_at", "title": "located_at", "to": "Assiut University", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "descriptor", "label": "incorporated in", "title": "incorporated in", "to": "Mumford-Shah energy", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "descriptor", "label": "consists_of", "title": "consists_of", "to": "binary strings", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "descriptor", "label": "uses", "title": "uses", "to": "masked Hamming distance calculation", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "Abdelrahman_Heat_Diffusion_Over_2015_CVPR_paper.pdf", "label": "contains", "title": "contains", "to": "Heat Diffusion Over Weighted Manifolds", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "existing descriptors", "label": "focus on", "title": "focus on", "to": "geometric properties", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "existing descriptors", "label": "focus on", "title": "focus on", "to": "topological properties", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "discretization method", "label": "uses", "title": "uses", "to": "finite element approximation", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "weighted heat kernel signature", "label": "encodes", "title": "encodes", "to": "photometric information", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "weighted heat kernel signature", "label": "encodes", "title": "encodes", "to": "geometric information", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "weighted heat kernel signature", "label": "incorporates", "title": "incorporates", "to": "method for scale invariance", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "heat kernel signature", "label": "encodes", "title": "encodes", "to": "photometric information", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "heat kernel signature", "label": "encodes", "title": "encodes", "to": "geometric information", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "challenges", "label": "arise from", "title": "arise from", "to": "pure geometric methods", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "challenges", "label": "arise from", "title": "arise from", "to": "pure photometric methods", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "experimental results", "label": "confirm", "title": "confirm", "to": "approach\u0027s performance", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "experimental results", "label": "demonstrate", "title": "demonstrate", "to": "high fidelity", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "experimental results", "label": "demonstrate", "title": "demonstrate", "to": "temporal consistency", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "experimental results", "label": "validates", "title": "validates", "to": "proposed approach", "width": 3.73}, {"arrows": "to", "color": "#0077CC", "from": "experimental results", "label": "demonstrates", "title": "demonstrates", "to": "effectiveness", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "experimental results", "label": "evaluates", "title": "evaluates", "to": "approach", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "experimental results", "label": "on", "title": "on", "to": "image datasets", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "experimental results", "label": "on", "title": "on", "to": "video datasets", "width": 3.79}, {"arrows": "to", "color": "#CC7700", "from": "textured shape retrieval", "label": "requires", "title": "requires", "to": "approach", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "Challenges", "label": "arise_from", "title": "arise_from", "to": "Pure Geometric Methods", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Challenges", "label": "arise_from", "title": "arise_from", "to": "Photometric Shape Descriptors", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "Weighted Heat Kernel Signature", "label": "related_to", "title": "related_to", "to": "Heat Diffusion on Manifold", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Electrical Engineering Department", "label": "part_of", "title": "part_of", "to": "Assiut University", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Assiut University", "label": "located_in", "title": "located_in", "to": "Assiut", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Si Liu", "label": "author_of", "title": "author_of", "to": "Matching-CNN Meets KNN", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Si Liu", "label": "affiliated_with", "title": "affiliated_with", "to": "SKLOIs", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Xiaodan Liang", "label": "author_of", "title": "author_of", "to": "Matching-CNN Meets KNN", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Xiaodan Liang", "label": "affiliated_with", "title": "affiliated_with", "to": "National University of Singapore", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Luoqi Liu", "label": "author_of", "title": "author_of", "to": "Matching-CNN Meets KNN", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Luoqi Liu", "label": "affiliated_with", "title": "affiliated_with", "to": "National University of Singapore", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Xiaohui Shen", "label": "author_of", "title": "author_of", "to": "Matching-CNN Meets KNN", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Xiaohui Shen", "label": "affiliated_with", "title": "affiliated_with", "to": "Adobe Research", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Xiaohui Shen", "label": "author_of", "title": "author_of", "to": "A Convolutional Neural Network Cascade", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Xiaohui Shen", "label": "authored", "title": "authored", "to": "Salient Object Subitizing", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Jianchao Yang", "label": "author_of", "title": "author_of", "to": "Matching-CNN Meets KNN", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Jianchao Yang", "label": "affiliated_with", "title": "affiliated_with", "to": "Adobe Research", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Changshen Xu", "label": "author_of", "title": "author_of", "to": "Matching-CNN Meets KNN", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Changshen Xu", "label": "affiliated_with", "title": "affiliated_with", "to": "IA", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Liang Lin", "label": "author_of", "title": "author_of", "to": "Matching-CNN Meets KNN", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Liang Lin", "label": "affiliated_with", "title": "affiliated_with", "to": "Sun Yat-sen University", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Xiaochun Cao", "label": "author_of", "title": "author_of", "to": "Matching-CNN Meets KNN", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Xiaochun Cao", "label": "affiliated_with", "title": "affiliated_with", "to": "SKLOIs", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Shuicheng Yan", "label": "author_of", "title": "author_of", "to": "Matching-CNN Meets KNN", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Shuicheng Yan", "label": "affiliated_with", "title": "affiliated_with", "to": "National University of Singapore", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Shuicheng Yan", "label": "is_author_of", "title": "is_author_of", "to": "Motion Part Regularization", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Work", "label": "introduces", "title": "introduces", "to": "Solution", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Solution", "label": "addresses", "title": "addresses", "to": "Human Parsing", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Quasi-parametric Model", "label": "leverages", "title": "leverages", "to": "KNN Framework", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Quasi-parametric Model", "label": "incorporates", "title": "incorporates", "to": "M-CNN", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "M-CNN", "label": "predicts", "title": "predicts", "to": "Matching Confidence", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "M-CNN", "label": "predicts", "title": "predicts", "to": "Displacements", "width": 3.79}, {"arrows": "to", "color": "#CC7700", "from": "M-CNN", "label": "uses", "title": "uses", "to": "superpixel smoothing", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "M-CNN", "label": "fuses", "title": "fuses", "to": "matched regions", "width": 3.88}, {"arrows": "to", "color": "#00CC77", "from": "M-CNN", "label": "improves", "title": "improves", "to": "performance", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Evaluations", "label": "demonstrate", "title": "demonstrate", "to": "Performance Gains", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "performance", "label": "affected_by", "title": "affected_by", "to": "object characteristics", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "performance", "label": "compared_to", "title": "compared_to", "to": "state-of-the-art methods", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "performance", "label": "is", "title": "is", "to": "convincing", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "performance", "label": "compared_to", "title": "compared_to", "to": "existing methods", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "SKLOIs", "label": "is_part_of", "title": "is_part_of", "to": "IIE", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "IIE", "label": "is_affiliated_with", "title": "is_affiliated_with", "to": "Chinese Academy of Sciences", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "National University of Singapore", "label": "hosts", "title": "hosts", "to": "ineering", "width": 3.25}, {"arrows": "to", "color": "#0077CC", "from": "OIS", "label": "is_affiliated_with", "title": "is_affiliated_with", "to": "Chinese Academy of Sciences", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Yunsheng Jiang", "label": "contributed_to", "title": "contributed_to", "to": "Combination Features and Models for Human Detection", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Combination Features and Models for Human Detection", "label": "is_published_in", "title": "is_published_in", "to": "CVPR", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "Combination Features and Models for Human Detection", "label": "concerns", "title": "concerns", "to": "Human Detection", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Jinwen Ma", "label": "is_author_of", "title": "is_author_of", "to": "Combination Features and Models for Human Description", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Jinwen Ma", "label": "contributed_to", "title": "contributed_to", "to": "Combination Features and Models for Human Detection", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Jinwen Ma", "label": "affiliated_with", "title": "affiliated_with", "to": "Peking University", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Jinwen Ma", "label": "works_in", "title": "works_in", "to": "Department of Information Science", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Jiang_Combination_Features_and_2015_CVPR_paper.pdf", "label": "contains", "title": "contains", "to": "Combination Features and Models for Human Detection", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "combination models", "label": "has_feature", "title": "has_feature", "to": "complementary features", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "existing features", "label": "exhibit", "title": "exhibit", "to": "biases", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "biases", "label": "limit", "title": "limit", "to": "performance", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "HOG-III features", "label": "combines", "title": "combines", "to": "color features", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "approaches", "label": "improve", "title": "improve", "to": "detection performance", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "approaches", "label": "maintain", "title": "maintain", "to": "computational efficiency", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "approaches", "label": "are", "title": "are", "to": "flexible", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "experiments", "label": "conducted_on", "title": "conducted_on", "to": "PASCAL VOC datasets", "width": 3.73}, {"arrows": "to", "color": "#0077CC", "from": "experiments", "label": "indicate", "title": "indicate", "to": "more accurate segmentation", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "experiments", "label": "demonstrate", "title": "demonstrate", "to": "effectiveness", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "experiments", "label": "demonstrates", "title": "demonstrates", "to": "reliability", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "experiments", "label": "demonstrate", "title": "demonstrate", "to": "superiority in speed", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "experiments", "label": "demonstrate", "title": "demonstrate", "to": "competitive surface quality", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "experiments", "label": "demonstrate", "title": "demonstrate", "to": "effect of blur", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "experiments", "label": "provide", "title": "provide", "to": "geometric derivations", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "experiments", "label": "performed_on", "title": "performed_on", "to": "PASPAL VOC 2", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "experiments", "label": "evaluates_on", "title": "evaluates_on", "to": "PASPAL VOC 2007", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "experiments", "label": "evaluates_on", "title": "evaluates_on", "to": "KITTI benchmark", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "experiments", "label": "evaluates_on", "title": "evaluates_on", "to": "MPI Sintel benchmark", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "experiments", "label": "evaluates_on", "title": "evaluates_on", "to": "Middlebury benchmark", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "experiments", "label": "demonstrate", "title": "demonstrate", "to": "classification", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "experiments", "label": "demonstrate", "title": "demonstrate", "to": "image annotation", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "experiments", "label": "on", "title": "on", "to": "RMRC dataset", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "experiments", "label": "use", "title": "use", "to": "large-scale datasets", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "Belongie et al. (2001)", "label": "presented_in", "title": "presented_in", "to": "IEEE Int\u0027l Conf. on Computer Vision (IC CV)", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Hubel (1995)", "label": "authored", "title": "authored", "to": "Eye, brain, and vision", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "Eye, brain, and vision", "label": "related_to", "title": "related_to", "to": "human detection", "width": 2.95}, {"arrows": "to", "color": "#0077CC", "from": "Ioffe \u0026 Forsyth (2001)", "label": "presented_in", "title": "presented_in", "to": "IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Dalal (2006)", "label": "authored", "title": "authored", "to": "Finding people in images and videos", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Felzenszwalb et al. (2008)", "label": "presented_in", "title": "presented_in", "to": "IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Felzenszwalb et al. (2008)", "label": "presents", "title": "presents", "to": "deformable part models", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "HOG-III Features", "label": "used_for", "title": "used_for", "to": "human detection", "width": 3.55}, {"arrows": "to", "color": "#00CC77", "from": "Model Fusion", "label": "improves", "title": "improves", "to": "detection performance", "width": 3.4000000000000004}, {"arrows": "to", "color": "#00CC77", "from": "Weighted-NMS", "label": "improves", "title": "improves", "to": "detection performance", "width": 3.25}, {"arrows": "to", "color": "#CC7700", "from": "Matching Shapes", "label": "contributes_to", "title": "contributes_to", "to": "human detection", "width": 3.0999999999999996}, {"arrows": "to", "color": "#0077CC", "from": "elzenszwalb, P. F.", "label": "authored", "title": "authored", "to": "A discriminatively trained, multiscale, deformable part model", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Girshick, R. B.", "label": "authored", "title": "authored", "to": "Rich feature hierarchies", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Girshick, R. B.", "label": "authored", "title": "authored", "to": "Object detection with discriminatively trained part-based models", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Viola, P.", "label": "authored", "title": "authored", "to": "Robust real-time face detection", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Robust real-time face detection", "label": "published_in", "title": "published_in", "to": "International Journal of Computer Vision", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "Object detection grammar", "label": "presented_at", "title": "presented_at", "to": "IEEE Int\u2019l Conf. on Computer Vision (ICCV) Workshops", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Object detection with grammar models", "label": "presented_at", "title": "presented_at", "to": "Advances in Neural Information Processing Systems", "width": 3.67}, {"arrows": "to", "color": "#0077CC", "from": "A discriminatively trained, mult scale, deformable part model", "label": "presented_at", "title": "presented_at", "to": "IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)", "width": 3.7600000000000002}, {"arrows": "to", "color": "#CC7700", "from": "International Journal of Computer Vision", "label": "publishes", "title": "publishes", "to": "appearance models", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "International Journal of Computer Vision", "label": "published", "title": "published", "to": "polynomial equation solving", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "International Journal of Computer Vision", "label": "is_publication", "title": "is_publication", "to": "journal", "width": 3.91}, {"arrows": "to", "color": "#CC7700", "from": "International Journal of Computer Vision", "label": "is_a", "title": "is_a", "to": "publication", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Advances in Neural Information Processing Systems", "label": "publishes", "title": "publishes", "to": "Learning to count objects in images", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Gkioxari, G.", "label": "writes_paper", "title": "writes_paper", "to": "Using k-poselets for detecting people and localizing their keypoints", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Gkioxari, G.", "label": "co-authors_with", "title": "co-authors_with", "to": "Hariharan, B.", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Gkioxari, G.", "label": "co-authors_with", "title": "co-authors_with", "to": "Malik, J.", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Hariharan, B.", "label": "authored", "title": "authored", "to": "Simultaneous detection and segmentation", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "GkioxARI, G.", "label": "co-authors_with", "title": "co-authors_with", "to": "Girshick, R.", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Girshick, R.", "label": "authored", "title": "authored", "to": "Rich feature hierarchies for accurate object detection and semantic segmentation", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Girshick, R.", "label": "authored", "title": "authored", "to": "Rich feature hierarchies", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Girshick, R.", "label": "affiliated_with", "title": "affiliated_with", "to": "CVPR", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "YunshEng Jiang", "label": "affiliated_with", "title": "affiliated_with", "to": "Peking University", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "YunshEng Jiang", "label": "works_in", "title": "works_in", "to": "Department of Information Science", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Cheng", "label": "authors", "title": "authors", "to": "Effective Learning-Based Illuminant Estimation Using Simple Features", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Cheng", "label": "co-authors_with", "title": "co-authors_with", "to": "Price", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Cheng", "label": "co-authors_with", "title": "co-authors_with", "to": "Cohen", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Cheng", "label": "co-authors_with", "title": "co-authors_with", "to": "Michael S. Brown", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Cheng", "label": "authored", "title": "authored", "to": "Bing: Binarized normed gradients", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Cheng", "label": "authored", "title": "authored", "to": "Binarized normed gradients", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Michael S. Brown", "label": "affiliated_with", "title": "affiliated_with", "to": "National University of Singapore", "width": 3.88}, {"arrows": "to", "color": "#CC7700", "from": "Illumination estimation", "label": "is_process_of", "title": "is_process_of", "to": "determining chromaticity", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "Illumination estimation", "label": "relates_to", "title": "relates_to", "to": "white-balancing", "width": 3.7600000000000002}, {"arrows": "to", "color": "#CC7700", "from": "computational color constancy", "label": "is_topic_in", "title": "is_topic_in", "to": "computer vision", "width": 3.88}, {"arrows": "to", "color": "#CC7700", "from": "problem", "label": "is_nature", "title": "is_nature", "to": "ill-posed", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "problem", "label": "is", "title": "is", "to": "nonconvex", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "problem", "label": "solved by", "title": "solved by", "to": "sequence of convex semi-de\ufb01nite programs", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "problem", "label": "is_about", "title": "is_about", "to": "temporally consistent video post-processing", "width": 4.0}, {"arrows": "to", "color": "#CC7700", "from": "problem", "label": "is_type_of", "title": "is_type_of", "to": "discovery problem", "width": 3.7600000000000002}, {"arrows": "to", "color": "#CC7700", "from": "problem", "label": "requires", "title": "requires", "to": "localization", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "problem", "label": "is", "title": "is", "to": "inherently ill-posed", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "problem", "label": "has_application", "title": "has_application", "to": "3D shape motion recovery", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "best results", "label": "reported on", "title": "reported on", "to": "modern color constancy data sets", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "results", "label": "reported_on", "title": "reported_on", "to": "color constancy data sets", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "results", "label": "evaluated on", "title": "evaluated on", "to": "CMU mocap dataset", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "results", "label": "evaluated using", "title": "evaluated using", "to": "manual annotations", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "our approach", "label": "is faster than", "title": "is faster than", "to": "existing learning-based methods", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "our approach", "label": "gives", "title": "gives", "to": "best results", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Forsyth, D. A.", "label": "authored", "title": "authored", "to": "novel algorithm", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Forsyth, D. A.", "label": "authored", "title": "authored", "to": "Variable-source shading analysis", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Bani\u0107, N.", "label": "authored", "title": "authored", "to": "Color dog", "width": 3.88}, {"arrows": "to", "color": "#CC7700", "from": "Color dog", "label": "guides", "title": "guides", "to": "global illumination estimation", "width": 3.79}, {"arrows": "to", "color": "#00CC77", "from": "Color dog", "label": "improves", "title": "improves", "to": "accuracy", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Funt, B.", "label": "authored", "title": "authored", "to": "support vector regression", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "support vector regression", "label": "estimates", "title": "estimates", "to": "illumination chromaticity", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Gao, S.", "label": "authored", "title": "authored", "to": "color constancy", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Gao, S.", "label": "authored", "title": "authored", "to": "Ef\ufb01cient color constancy with local surface re\ufb02ectance statistics", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "color constancy", "label": "uses", "title": "uses", "to": "local surface reflectance statistics", "width": 3.67}, {"arrows": "to", "color": "#CC7700", "from": "color constancy", "label": "related to", "title": "related to", "to": "specular reflection", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "Learning-Based Methods", "label": "is", "title": "is", "to": "slower than", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Color and Imaging Conference", "label": "published", "title": "published", "to": "via support vector regression", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Barnard, K.", "label": "authored", "title": "authored", "to": "A comparison of computational color constancy algorithms", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Barnard, K.", "label": "authored", "title": "authored", "to": "A data set for color research", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "TIP", "label": "published", "title": "published", "to": "A comparison of computational color constancy algorithms", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "TIP", "label": "published", "title": "published", "to": "Improving color constancy using indoor - outdoor image classification", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "TIP", "label": "is_journal", "title": "is_journal", "to": "IEEE Transactions on Image Processing", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Color Research \u0026 Application", "label": "published", "title": "published", "to": "A data set for color research", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "Gehler, P. V.", "label": "authored", "title": "authored", "to": "Bayesian color constancy revisited", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Bianco, S.", "label": "authored", "title": "authored", "to": "Improving color constancy using indoor - outdoor image classification", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Bianco, S.", "label": "is_author_of", "title": "is_author_of", "to": "Automatic color constancy algorithm selection and combination", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "IEEE Transactions on Image Processing", "label": "has_volume", "title": "has_volume", "to": "16", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "IEEE Transactions on Image Processing", "label": "published", "title": "published", "to": "The regularized iteratively reweighted mad method", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "IEEE Transactions on Image Processing", "label": "publishes", "title": "publishes", "to": "Locally linear regression for pose-invariant face recognition", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Botev, Z.", "label": "is_author_of", "title": "is_author_of", "to": "Kernel density estimation via diffusion", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "The Annals of Statistics", "label": "is_journal", "title": "is_journal", "to": "The Annals of Statistics", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Dongliang Cheng", "label": "affiliated_with", "title": "affiliated_with", "to": "National University of Singapore", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Brian Price", "label": "affiliated_with", "title": "affiliated_with", "to": "Adobe Research", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Brian Price", "label": "authored", "title": "authored", "to": "Salient Object Subitizing", "width": 3.73}, {"arrows": "to", "color": "#0077CC", "from": "Scott Cohen", "label": "affiliated_with", "title": "affiliated_with", "to": "Adobe Research", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Hui Wu", "label": "is_author_of", "title": "is_author_of", "to": "Robust Regression on Image Manifolds for Ordered Label Denoising", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Hui Wu", "label": "affiliated_with", "title": "affiliated_with", "to": "University of North Carolina at Charlotte", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Hui Wu", "label": "affiliated_with", "title": "affiliated_with", "to": "University of North Carolin", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Richard Souvenir", "label": "is_author_of", "title": "is_author_of", "to": "Robust Regression on Image Manifold for Ordered Label Denoising", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Richard Souvenir", "label": "affiliated_with", "title": "affiliated_with", "to": "University of North Carolina at Charlotte", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Richard Souvenir", "label": "affiliated_with", "title": "affiliated_with", "to": "University of North Carolin", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Wu_Robust_Regression_on_2015_CVPR_supplemental.pdf", "label": "is_file", "title": "is_file", "to": "supplemental material", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Wu_Robust_Regression_on_2015_CVPR_supplemental", "label": "focuses_on", "title": "focuses_on", "to": "Robust Regression", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Robust Regression", "label": "addresses", "title": "addresses", "to": "image labels", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Wu_Robust_Reservation_on_2015_CVPR_supplemental", "label": "addresses", "title": "addresses", "to": "Ordered Label Denoising", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Figures 1-4", "label": "illustrate", "title": "illustrate", "to": "performance", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "RANSC", "label": "is_method", "title": "is_method", "to": "Robust Regression", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "K-NN", "label": "is_method", "title": "is_method", "to": "Robust Regression", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "RBFN", "label": "is_method", "title": "is_method", "to": "Robust Regression", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "SVR", "label": "is_method", "title": "is_method", "to": "Robust Regression", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "KSPCA", "label": "is_method", "title": "is_method", "to": "Robust Regression", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "H3R", "label": "is_method", "title": "is_method", "to": "Robust Regression", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Statue Data Set", "label": "is_example", "title": "is_example", "to": "Image Manifolds", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Face Pose Estimation", "label": "is_example", "title": "is_example", "to": "Image Manifolds", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Y. Cheng", "label": "affiliated_with", "title": "affiliated_with", "to": "University of North Carolin", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Y. Cheng", "label": "author_of", "title": "author_of", "to": "A Convex Optimization Approach", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Y. Cheng", "label": "affiliation", "title": "affiliation", "to": "Northeastern University", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "J. A. Lopez", "label": "affiliated_with", "title": "affiliated_with", "to": "University of North Carolin", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "J. A. Lopez", "label": "affiliation", "title": "affiliation", "to": "Northeastern University", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "O. Camps", "label": "affiliated_with", "title": "affiliated_with", "to": "University of North Carolin", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "O. Camps", "label": "affiliation", "title": "affiliation", "to": "Northeastern University", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "M. Sznaier", "label": "affiliated_with", "title": "affiliated_with", "to": "University of North Carolin", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "M. Sznaier", "label": "affiliation", "title": "affiliation", "to": "Northeastern University", "width": 3.9699999999999998}, {"arrows": "to", "color": "#CC7700", "from": "Cheng_A_Convex_Optimization_2015_CVPR_paper", "label": "is_a", "title": "is_a", "to": "CVPR paper", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "hwu13@uncc.edu", "label": "email_address_of", "title": "email_address_of", "to": "Hui Wu", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "souvenir@uncc.edu", "label": "email_address_of", "title": "email_address_of", "to": "Richard Souvenir", "width": 3.88}, {"arrows": "to", "color": "#CC7700", "from": "framework", "label": "is", "title": "is", "to": "general nonconvex", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "framework", "label": "accounts for", "title": "accounts for", "to": "rank-2 constraint", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "framework", "label": "accounts for", "title": "accounts for", "to": "noise", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "framework", "label": "is_for", "title": "is_for", "to": "damage detection", "width": 3.91}, {"arrows": "to", "color": "#CC7700", "from": "framework", "label": "is_type_of", "title": "is_type_of", "to": "semi-supervised learning", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "framework", "label": "uses", "title": "uses", "to": "hierarchical shape features", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "framework", "label": "introduces", "title": "introduces", "to": "novel insights", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "framework", "label": "uses", "title": "uses", "to": "convolutional neural network", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "framework", "label": "exploits", "title": "exploits", "to": "visually similar neighbors", "width": 3.79}, {"arrows": "to", "color": "#CC7700", "from": "framework", "label": "explores", "title": "explores", "to": "deep information", "width": 3.7600000000000002}, {"arrows": "to", "color": "#CC7700", "from": "framework", "label": "explores", "title": "explores", "to": "wide information", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "framework", "label": "is_built_on", "title": "is_built_on", "to": "multiple visual features", "width": 3.73}, {"arrows": "to", "color": "#0077CC", "from": "framework", "label": "outperforms", "title": "outperforms", "to": "state-of-the-art methods", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "framework", "label": "learns", "title": "learns", "to": "relationship", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "framework", "label": "extracts", "title": "extracts", "to": "region-keyword pairs", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "framework", "label": "demonstrates", "title": "demonstrates", "to": "performance", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "framework", "label": "utilizes", "title": "utilizes", "to": "labeled 2D samples", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "framework", "label": "uses", "title": "uses", "to": "3D CAD models", "width": 3.67}, {"arrows": "to", "color": "#00CC77", "from": "framework", "label": "overcomes", "title": "overcomes", "to": "lack of 3D training data", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "framework", "label": "combines", "title": "combines", "to": "model-based generative tracking", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "framework", "label": "combines", "title": "combines", "to": "discriminative hand pose detection", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "framework", "label": "confirmed_by", "title": "confirmed_by", "to": "experimentation", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "framework", "label": "has_property", "title": "has_property", "to": "effectiveness", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "framework", "label": "believed to be", "title": "believed to be", "to": "extensible to other range-weighted filters", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "framework", "label": "believed_to_be_extensible_to", "title": "believed_to_be_extensible_to", "to": "range-weighted algorithms", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "framework", "label": "addresses", "title": "addresses", "to": "noisy images", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "framework", "label": "can_recover", "title": "can_recover", "to": "clustered structures", "width": 3.88}, {"arrows": "to", "color": "#CC7700", "from": "framework", "label": "defines", "title": "defines", "to": "solutions", "width": 3.0999999999999996}, {"arrows": "to", "color": "#0077CC", "from": "framework", "label": "addresses", "title": "addresses", "to": "partially-occluded small instances", "width": 3.91}, {"arrows": "to", "color": "#CC7700", "from": "framework", "label": "handles", "title": "handles", "to": "heterogenous types of input data", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "framework", "label": "combines", "title": "combines", "to": "features", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "framework", "label": "evaluates_on", "title": "evaluates_on", "to": "LIVE dataset", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "framework", "label": "demonstrates", "title": "demonstrates", "to": "superior performance", "width": 3.67}, {"arrows": "to", "color": "#0077CC", "from": "framework", "label": "achieves", "title": "achieves", "to": "comparable results", "width": 3.61}, {"arrows": "to", "color": "#CC7700", "from": "algorithm", "label": "is", "title": "is", "to": "extensible", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "algorithm", "label": "handles", "title": "handles", "to": "partially labeled correspondences", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "algorithm", "label": "leverages", "title": "leverages", "to": "co-occurrence information", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "algorithm", "label": "enables", "title": "enables", "to": "computer", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "algorithm", "label": "chooses", "title": "chooses", "to": "labeled images", "width": 3.88}, {"arrows": "to", "color": "#CC7700", "from": "algorithm", "label": "is_type_of", "title": "is_type_of", "to": "Adaptive Algorithms", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "algorithm", "label": "proposes_solution_for", "title": "proposes_solution_for", "to": "global maximization of consensus", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "algorithm", "label": "frames_problem_as", "title": "frames_problem_as", "to": "tree search problem", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "algorithm", "label": "utilizes", "title": "utilizes", "to": "A* search", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "algorithm", "label": "achieves", "title": "achieves", "to": "orders of magnitude faster performance", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "algorithm", "label": "improves_performance_compared_to", "title": "improves_performance_compared_to", "to": "previous exact methods", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "algorithm", "label": "approximates", "title": "approximates", "to": "energy", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "algorithm", "label": "leverages", "title": "leverages", "to": "multi-label graph cut algorithm", "width": 3.82}, {"arrows": "to", "color": "#CC7700", "from": "algorithm", "label": "inspired_by", "title": "inspired_by", "to": "Iterively Reweighted Least Squares (IRLS) algorithm", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "algorithm", "label": "demonstrates_effectiveness_on", "title": "demonstrates_effectiveness_on", "to": "stereo correspondence estimation", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "algorithm", "label": "demonstrates_effectiveness_on", "title": "demonstrates_effectiveness_on", "to": "image inpainting problems", "width": 3.73}, {"arrows": "to", "color": "#0077CC", "from": "algorithm", "label": "outperforms", "title": "outperforms", "to": "graph-cut-based algorithms", "width": 3.67}, {"arrows": "to", "color": "#0077CC", "from": "algorithm", "label": "yields", "title": "yields", "to": "lower energy values", "width": 3.64}, {"arrows": "to", "color": "#CC7700", "from": "algorithm", "label": "is_algorithm_for", "title": "is_algorithm_for", "to": "salient object detection", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "algorithm", "label": "uses", "title": "uses", "to": "detection-guided optimization strategy", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "algorithm", "label": "increases", "title": "increases", "to": "robustness", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "algorithm", "label": "increases", "title": "increases", "to": "speed", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "algorithm", "label": "finds", "title": "finds", "to": "region-of-interest", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "algorithm", "label": "extracts", "title": "extracts", "to": "objects", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "algorithm", "label": "leverages", "title": "leverages", "to": "eye tracking data", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "algorithm", "label": "uses", "title": "uses", "to": "spatio-temporal mixed graph", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "algorithm", "label": "uses", "title": "uses", "to": "binary linear integer programming", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "algorithm", "label": "integrates", "title": "integrates", "to": "local estimation", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "algorithm", "label": "integrates", "title": "integrates", "to": "global search", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "algorithm", "label": "performs", "title": "performs", "to": "state-of-the-art methods", "width": 3.82}, {"arrows": "to", "color": "#CC7700", "from": "algorithm", "label": "combines", "title": "combines", "to": "strengths", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "algorithm", "label": "leverages", "title": "leverages", "to": "dominant orientations", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "algorithm", "label": "guides", "title": "guides", "to": "interpretation", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "algorithm", "label": "assigns", "title": "assigns", "to": "polygon", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "algorithm", "label": "allows for", "title": "allows for", "to": "creation", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "algorithm", "label": "uses", "title": "uses", "to": "geometrically motivated criterion", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "algorithm", "label": "demonstrates", "title": "demonstrates", "to": "faster convergence", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "algorithm", "label": "reduces", "title": "reduces", "to": "total runtime", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "algorithm", "label": "uses", "title": "uses", "to": "block-coordinate Frank-Wolfe (BCFW) algorithm", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "algorithm", "label": "uses", "title": "uses", "to": "Gaussian Mixture Model", "width": 3.79}, {"arrows": "to", "color": "#CC7700", "from": "algorithm", "label": "addresses", "title": "addresses", "to": "submodular maximization", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "algorithm", "label": "addresses", "title": "addresses", "to": "mis-labeled examples", "width": 3.91}, {"arrows": "to", "color": "#CC7700", "from": "optimization", "label": "is", "title": "is", "to": "robust", "width": 3.7600000000000002}, {"arrows": "to", "color": "#CC7700", "from": "Multiple view geometry", "label": "is_foundational_to", "title": "is_foundational_to", "to": "topic", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "Multiple view geometry", "label": "relates_to", "title": "relates_to", "to": "computer vision", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Multiple view geometry", "label": "is_reference_for", "title": "is_reference_for", "to": "geometric relationships", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Hartley, R. \u0026 Zisserman, A.", "label": "authored", "title": "authored", "to": "Multiple view geometry", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Mohan, K. \u0026 Fazel, M.", "label": "authored", "title": "authored", "to": "Iterative reweighted algorithms", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Lasserre, J. B.", "label": "authored", "title": "authored", "to": "Global optimization with polynomials", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "Global optimization with polynomials", "label": "important_for", "title": "important_for", "to": "polynomial optimization methods", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "outliers", "label": "arise_from", "title": "arise_from", "to": "background clutter", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "Fundamental Matrix Estimation", "label": "is_a", "title": "is_a", "to": "topic", "width": 3.4000000000000004}, {"arrows": "to", "color": "#CC7700", "from": "Robust Optimization", "label": "is_a", "title": "is_a", "to": "topic", "width": 3.4000000000000004}, {"arrows": "to", "color": "#CC7700", "from": "Rank-Constrained Optimization", "label": "is_a", "title": "is_a", "to": "topic", "width": 3.4000000000000004}, {"arrows": "to", "color": "#CC7700", "from": "Global optimization", "label": "uses", "title": "uses", "to": "polynomial methods", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "polynomial methods", "label": "relevant to", "title": "relevant to", "to": "optimization methods", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Lasserre", "label": "authored", "title": "authored", "to": "Global optimization with polynomials and the problem of moments", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Bugarin et al.", "label": "compared", "title": "compared", "to": "polynomial global optimization", "width": 3.91}, {"arrows": "to", "color": "#CC7700", "from": "polynomial global optimization", "label": "is an alternative to", "title": "is an alternative to", "to": "eight-point algorithm", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "fundamental matrix estimation", "label": "uses", "title": "uses", "to": "polynomial global optimization", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "fundamental matrix estimation", "label": "uses", "title": "uses", "to": "eight-point algorithm", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "fundamental matrix estimation", "label": "is a type of", "title": "is a type of", "to": "computer vision technique", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "fundamental matrix estimation", "label": "is_topic_of", "title": "is_topic_of", "to": "International journal of computer vision", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "fundamental matrix estimation", "label": "is_topic_of", "title": "is_topic_of", "to": "Computer Vision and Pattern Recognition (CVPR)", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "fundamental matrix estimation", "label": "uses", "title": "uses", "to": "Mlesac", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "fundamental matrix estimation", "label": "is_studied_in", "title": "is_studied_in", "to": "Computer Vision and Image Understanding", "width": 3.79}, {"arrows": "to", "color": "#CC7700", "from": "fundamental matrix estimation", "label": "requires", "title": "requires", "to": "algorithm", "width": 3.64}, {"arrows": "to", "color": "#CC7700", "from": "eight-point algorithm", "label": "is_a", "title": "is_a", "to": "fundamental matrix estimation method", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Torr \u0026 Murray", "label": "studied", "title": "studied", "to": "fundamental matrix estimation methods", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "International journal of computer vision", "label": "publishes", "title": "publishes", "to": "taxonomies", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Computer Vision and Pattern Recognition (CVPR)", "label": "presents", "title": "presents", "to": "comparative study", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Computer Vision and Pattern Recognition (CVPR)", "label": "is_publication_venue_for", "title": "is_publication_venue_for", "to": "Histograms of oriented gradients", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "Mlesac", "label": "is_a", "title": "is_a", "to": "robust estimator", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "SDP relaxations", "label": "is_topic_of", "title": "is_topic_of", "to": "SIAM Journal on Optimization", "width": 3.82}, {"arrows": "to", "color": "#CC7700", "from": "SDP relaxations", "label": "used in", "title": "used in", "to": "polynomial optimization", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Zheng, Y., Sugimoto, S., \u0026 Okutomi, M.", "label": "developed", "title": "developed", "to": "eight-point algorithm", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Lasserre (2006)", "label": "discusses", "title": "discusses", "to": "SDP relaxations", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Sugaya \u0026 Kanatani (2007)", "label": "focuses on", "title": "focuses on", "to": "high-accuracy computation", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Northeastern University", "label": "located_in", "title": "located_in", "to": "Boston", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "RANSA", "label": "applied to", "title": "applied to", "to": "image analysis", "width": 3.4000000000000004}, {"arrows": "to", "color": "#CC7700", "from": "RANSA", "label": "is_technique_for", "title": "is_technique_for", "to": "model fitting", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "RANSA", "label": "is_relevant_to", "title": "is_relevant_to", "to": "computer vision problems", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "RANSA", "label": "improves", "title": "improves", "to": "number of inliers", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "University", "label": "located_in", "title": "located_in", "to": "Boston", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Boston", "label": "is_located_in", "title": "is_located_in", "to": "Massachusetts", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Jian Sun", "label": "is_author_of", "title": "is_author_of", "to": "Learning a Convolutional Neural Network", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Jian Sun", "label": "co-authored", "title": "co-authored", "to": "Convolutional Neural Networks at Constrained Time Cost", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Jian Sun", "label": "is author of", "title": "is author of", "to": "paper", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Jian Sun", "label": "affiliated_with", "title": "affiliated_with", "to": "Microsoft Research", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Jian Sun", "label": "is_author_of", "title": "is_author_of", "to": "A Geodesic-Preserving Method for Image Warping", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Jian Sun", "label": "author_of", "title": "author_of", "to": "Sparse Projections for High-Dimensional Binary Codes", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Learning a Convolutional Neural Network", "label": "is_published_in", "title": "is_published_in", "to": "CVPR", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "Learning a Convolutional Neural Network", "label": "addresses", "title": "addresses", "to": "Non-uniform Motion Blur Removal", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Wenfei Cao", "label": "is_author_of", "title": "is_author_of", "to": "Learning a Convolutional Neural Network", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Jean Ponce", "label": "is_author_of", "title": "is_author_of", "to": "Learning a Convolutional Neural Network", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Jean Ponce", "label": "is_author_of", "title": "is_author_of", "to": "Unsupervised Object Detection and Localization", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Jean Ponce", "label": "is_author_of", "title": "is_author_of", "to": "Unsupervised Object Discovery and Localization in the Wild", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Jean Ponce", "label": "affiliated_with", "title": "affiliated_with", "to": "\u00c9cole Normale Sup\u00e9rieure", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Jean Ponce", "label": "affiliated_with", "title": "affiliated_with", "to": "PSL Research University", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "lopez.jo@husky.neu.edu", "label": "associated_with", "title": "associated_with", "to": "Northeastern University", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "camps@coe.neu.edu", "label": "associated_with", "title": "associated_with", "to": "Northeastern University", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "msznaier@coe.neu.edu", "label": "associated_with", "title": "associated_with", "to": "Northeastern University", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "authors", "label": "propose", "title": "propose", "to": "deep learning approach", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "authors", "label": "propose_to_utilize", "title": "propose_to_utilize", "to": "kernels", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "authors", "label": "introduce", "title": "introduce", "to": "general Riemannian coding framework", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "authors", "label": "solve", "title": "solve", "to": "energy function", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "authors", "label": "uses", "title": "uses", "to": "Gauss-Newton method", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "authors", "label": "propose", "title": "propose", "to": "novel approach", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "CNN", "label": "uses", "title": "uses", "to": "convolutional neural network", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "CNN", "label": "trained_with", "title": "trained_with", "to": "structured loss", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "motion kernels", "label": "extended_by", "title": "extended_by", "to": "image rotations", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "Markov random field model", "label": "used_for", "title": "used_for", "to": "inferring dense non-uniform motion blur field", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "Markov random field model", "label": "enforces", "title": "enforces", "to": "motion smoothness", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Markov random field model", "label": "accounts_for", "title": "accounts_for", "to": "global salience effects", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "Markov random field model", "label": "achieves", "title": "achieves", "to": "state-of-the-art accuracy", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "deblurring model", "label": "removes", "title": "removes", "to": "motion blur", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "deblurring model", "label": "uses", "title": "uses", "to": "patch-level image prior", "width": 3.88}, {"arrows": "to", "color": "#CC7700", "from": "deblurring model", "label": "is_type_of", "title": "is_type_of", "to": "non-uniform model", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "deblurring model", "label": "improves", "title": "improves", "to": "image quality", "width": 3.4000000000000004}, {"arrows": "to", "color": "#CC7700", "from": "motion blur", "label": "is_type_of", "title": "is_type_of", "to": "image artifact", "width": 3.25}, {"arrows": "to", "color": "#CC7700", "from": "patch-level image prior", "label": "is_related_to", "title": "is_related_to", "to": "patch-based image processing", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "ion blur", "label": "removed_by", "title": "removed_by", "to": "deblurring model", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "Object detection systems", "label": "requires", "title": "requires", "to": "large number of classes", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Object detection systems", "label": "based_on", "title": "based_on", "to": "deep convolutional neural network (CNN)", "width": 3.94}, {"arrows": "to", "color": "#00CC77", "from": "extensive convolution operations", "label": "causes", "title": "causes", "to": "long detection times", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "sparse coding methods", "label": "aims_to_reduce", "title": "aims_to_reduce", "to": "computational complexity", "width": 3.7600000000000002}, {"arrows": "to", "color": "#00CC77", "from": "sparse coding methods", "label": "compromises", "title": "compromises", "to": "accuracy", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "Regularized Sparse Coding", "label": "uses", "title": "uses", "to": "filter functionality reconstruction", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Regularized Sparse Coding", "label": "minimizes", "title": "minimizes", "to": "score map error", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "Regularized Sparse Coding", "label": "achieves", "title": "achieves", "to": "16x speedup", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Regularized Sparse Coding", "label": "results_in", "title": "results_in", "to": "0.04 mAP drop", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "Regularized Sparse Coding", "label": "demonstrates_applicability_for", "title": "demonstrates_applicability_for", "to": "parallel computing", "width": 3.73}, {"arrows": "to", "color": "#0077CC", "from": "ILSVIRC 2013", "label": "evaluates", "title": "evaluates", "to": "Regularized Sparse Coding", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Deformable Part Model", "label": "compared_to", "title": "compared_to", "to": "Regularized Sparse Coding", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "parallel computing", "label": "occurs_on", "title": "occurs_on", "to": "GPUs", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "Ting-Hsuan Chao", "label": "affiliated_with", "title": "affiliated_with", "to": "National Taiwan University", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Yen-Liang Lin", "label": "affiliated_with", "title": "affiliated_with", "to": "National Taiwan University", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Yin-Hsi Kuo", "label": "affiliated_with", "title": "affiliated_with", "to": "National Taiwan University", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Winston H. Hsu", "label": "affiliated_with", "title": "affiliated_with", "to": "National Taiwan University", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Xiao-Ming Wu", "label": "affiliated_with", "title": "affiliated_with", "to": "author", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Xiao-Ming Wu", "label": "authored", "title": "authored", "to": "New Insights into Laplacian Similarity Search", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Xiao-Ming Wu", "label": "affiliation", "title": "affiliation", "to": "Department of Electrical Engineering, Columbia University", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Xiao-Ming Wu", "label": "has_email", "title": "has_email", "to": "xmwu@ee.columbia.edu", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Zhenguo Li", "label": "affiliated_with", "title": "affiliated_with", "to": "author", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Zhenguo Li", "label": "affiliation", "title": "affiliation", "to": "Huawei Noah\u2019s Ark Lab, Hong Kong", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Zhenguo Li", "label": "has_email", "title": "has_email", "to": "li.zhenguo@huawei.com", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Zhenguo Li", "label": "authored", "title": "authored", "to": "New Insights into Laplacian Similarity Research", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Shih-Fu Chang", "label": "affiliated_with", "title": "affiliated_with", "to": "author", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Shih-Fu Chang", "label": "has_email", "title": "has_email", "to": "sfchang@ee.columbia.edu", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Shih-Fu Chang", "label": "author_of", "title": "author_of", "to": "Attributes and Categories for Generic Instance Search from One Example", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Shih-Fu Chang", "label": "is_author_of", "title": "is_author_of", "to": "paper", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Shih-Fu Chang", "label": "authored", "title": "authored", "to": "New Insights into Laplacian Similarity Search", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "X.-M. Wu", "label": "affiliated_with", "title": "affiliated_with", "to": "Department of Electrical Engineering, Columbia University", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "X.-M. Wu", "label": "email", "title": "email", "to": "xmwu@ee.columbia.edu", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "X.-M. Wu", "label": "co-authored", "title": "co-authored", "to": "Analyzing the harmonic structure in graph-based learning", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "X.-M. Wu", "label": "co-authored", "title": "co-authored", "to": "New insights into laplacian similarity search", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "X.-M. Wu", "label": "co-authored_with", "title": "co-authored_with", "to": "Zhenguo Li", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "X.-M. Wu", "label": "co-authored_with", "title": "co-authored_with", "to": "Shih-Fu Chang", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "sfchang@ee.columbia.edu", "label": "affiliated_with", "title": "affiliated_with", "to": "Columbia University", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Analyzing the harmonic structure in graph-based learning", "label": "published_in", "title": "published_in", "to": "NIPS", "width": 3.91}, {"arrows": "to", "color": "#CC7700", "from": "Analyzing the harmonic structure in graph-based learning", "label": "deals_with", "title": "deals_with", "to": "harmonic structure", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "Xuan Dong", "label": "author_of", "title": "author_of", "to": "Region-based Temporally Consistent Video Post-processing", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Xuan Dong", "label": "affiliated_with", "title": "affiliated_with", "to": "Tsinghua University", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Region-based Temporally Consistent Video Post-processing", "label": "authored_by", "title": "authored_by", "to": "Boyan Bonev", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Region-based Temporally Consistent Video Post-processing", "label": "authored_by", "title": "authored_by", "to": "Alan L. Yuille", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Region-based Temporally Consistent Video Post-processing", "label": "presented_at", "title": "presented_at", "to": "CVPR", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Region-based Temporally Consistent Video Post-processing", "label": "year", "title": "year", "to": "2015", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Region-based Temporally Consistent Video Post-processing", "label": "file_name", "title": "file_name", "to": "Dong_Region-Based_Temporally_Consistent_2015_CVPR_paper.pdf", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Boyan Bonev", "label": "author_of", "title": "author_of", "to": "Region-based Temporically Consistent Video Post-processing", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Boyan Bonev", "label": "affiliated_with", "title": "affiliated_with", "to": "UC Los Angeles", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Region-based Temporately Consistent Video Post-processing", "label": "authored_by", "title": "authored_by", "to": "Yu Zhu", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Yu Zhu", "label": "author_of", "title": "author_of", "to": "Region-based Temporally Consistent Video Post-processing", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Yu Zhu", "label": "affiliated_with", "title": "affiliated_with", "to": "Northwestern Polytechnical University", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Alan L. Yuille", "label": "author_of", "title": "author_of", "to": "Region-based Temporally Consistent Video Post-processing", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Alan L. Yuille", "label": "affiliated_with", "title": "affiliated_with", "to": "UC Los Angeles", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Columbia University", "label": "has_department", "title": "has_department", "to": "Department of Electrical Engineering", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Department of Electrical Engineering", "label": "is_department_of", "title": "is_department_of", "to": "Columbia University", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "goal", "label": "aims_for", "title": "aims_for", "to": "fidelity", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "goal", "label": "is_to_create", "title": "is_to_create", "to": "semantically segmented images", "width": 3.7600000000000002}, {"arrows": "to", "color": "#CC7700", "from": "goal", "label": "is", "title": "is", "to": "localizing every object in an image", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "goal", "label": "is to", "title": "is to", "to": "reconstruct a 3D model automatically", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "enhancement algorithms", "label": "enforces", "title": "enforces", "to": "spatially consistent prior", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "spatially consistent prior", "label": "relates", "title": "relates", "to": "pixels with same RGB values", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "enhancement of regions", "label": "considers", "title": "considers", "to": "fidelity", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "enhancement of regions", "label": "considers", "title": "considers", "to": "temporal consistency", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "enhancement of regions", "label": "considers", "title": "considers", "to": "spatial consistency", "width": 3.79}, {"arrows": "to", "color": "#CC7700", "from": "frames", "label": "consider", "title": "consider", "to": "fidelity", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Slic superpixels", "label": "compared to", "title": "compared to", "to": "superpixel methods", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Slic superpixels", "label": "is described in", "title": "is described in", "to": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "width": 3.64}, {"arrows": "to", "color": "#CC7700", "from": "Slic superpixels", "label": "is_a", "title": "is_a", "to": "superpixels", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Slic superpixels", "label": "compared_to", "title": "compared_to", "to": "state-of-the-art superpixel methods", "width": 3.46}, {"arrows": "to", "color": "#0077CC", "from": "Slic superpixels", "label": "presented_in", "title": "presented_in", "to": "IEEE TPAM, 2012", "width": 3.34}, {"arrows": "to", "color": "#0077CC", "from": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "label": "is_publication_venue_for", "title": "is_publication_venue_for", "to": "reference [4]", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "label": "is_publication", "title": "is_publication", "to": "journal", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "label": "publishes", "title": "publishes", "to": "Nonparametric Discriminant Analysis for Face Recognition", "width": 7.0}, {"arrows": "to", "color": "#0077CC", "from": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "label": "publishes", "title": "publishes", "to": "Multisculse local phase quantization for robust component-based face recognition", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "label": "publishes", "title": "publishes", "to": "local phase quantization", "width": 3.79}, {"arrows": "to", "color": "#CC7700", "from": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "label": "is_a", "title": "is_a", "to": "publication", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "label": "published", "title": "published", "to": "textures", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "label": "published", "title": "published", "to": "Spacetime texture representation and recognition", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "tone management", "label": "is described in", "title": "is described in", "to": "ACM Trans. on Graph.", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "ACM Trans. on Graph.", "label": "published", "title": "published", "to": "Tonal stabilization of video", "width": 7.0}, {"arrows": "to", "color": "#0077CC", "from": "ACM Trans. on Graph.", "label": "published", "title": "published", "to": "Patch-based high dynamic range video", "width": 7.0}, {"arrows": "to", "color": "#0077CC", "from": "video color grading", "label": "is described in", "title": "is described in", "to": "ACM Trans. on Graph.", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "color transformation", "label": "is described in", "title": "is described in", "to": "IEEE Transactions on Image Processing", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "color transformation", "label": "applied_to", "title": "applied_to", "to": "image", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "Image Enhancement Algorithms", "label": "are", "title": "are", "to": "enhancement algorithms", "width": 3.25}, {"arrows": "to", "color": "#0077CC", "from": "Y. Chang", "label": "authored", "title": "authored", "to": "Example-based color transformation of image and video using basic color categories", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Y. Chang", "label": "authored", "title": "authored", "to": "Example-based color transformation", "width": 7.0}, {"arrows": "to", "color": "#0077CC", "from": "Example-based color transformation of image and video using basic color categories", "label": "published_in", "title": "published_in", "to": "IEEE Transactions on Image Processing", "width": 3.91}, {"arrows": "to", "color": "#CC7700", "from": "Example-based color transformation of image and video using basic color categories", "label": "deals_with", "title": "deals_with", "to": "color transformation", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "S. Saito", "label": "authored", "title": "authored", "to": "Example-based color transformation of image and video using basic color categories", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "S. Saito", "label": "co-authored", "title": "co-authored", "to": "Example-based color transformation", "width": 7.0}, {"arrows": "to", "color": "#0077CC", "from": "M. Nakajima", "label": "authored", "title": "authored", "to": "Example-based color transformation of image and video using basic color categories", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "M. Nakajima", "label": "co-authored", "title": "co-authored", "to": "Example-based color transform", "width": 7.0}, {"arrows": "to", "color": "#0077CC", "from": "article", "label": "has_page_range", "title": "has_page_range", "to": "1\u201311", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "article", "label": "has_year", "title": "has_year", "to": "2013", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Z. Farbman", "label": "authored", "title": "authored", "to": "Tonal stabilization of video", "width": 7.0}, {"arrows": "to", "color": "#0077CC", "from": "D. Lischinski", "label": "co-authored", "title": "co-authored", "to": "Tonal stabilization of video", "width": 7.0}, {"arrows": "to", "color": "#0077CC", "from": "P. F. Felzenszwalb", "label": "authored", "title": "authored", "to": "Efficient belief propagation", "width": 7.0}, {"arrows": "to", "color": "#0077CC", "from": "D. P. Huttenlocher", "label": "co-authored", "title": "co-authored", "to": "Efficient belief propagation", "width": 7.0}, {"arrows": "to", "color": "#0077CC", "from": "M. Grundmann", "label": "authored", "title": "authored", "to": "Post-processing approach", "width": 7.0}, {"arrows": "to", "color": "#0077CC", "from": "Y. Hacohen", "label": "authored", "title": "authored", "to": "Non-rigid dense correspondence", "width": 7.0}, {"arrows": "to", "color": "#0077CC", "from": "N. K. Kalantari", "label": "authored", "title": "authored", "to": "Patch-based high dynamic range video", "width": 7.0}, {"arrows": "to", "color": "#0077CC", "from": "age enhancement", "label": "published_in", "title": "published_in", "to": "ACM Trans. Graph.", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "N. K. Kalantria", "label": "authored", "title": "authored", "to": "Patch-based high dynamic range video", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "S. B. Kang", "label": "authored", "title": "authored", "to": "High dynamic range video", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "High dynamic range video", "label": "published_in", "title": "published_in", "to": "ACM Trans. on Graph.", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Tsinghua University", "label": "located_in", "title": "located_in", "to": "Beijing", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Lionel Gueguen", "label": "authored", "title": "authored", "to": "Large-Scale Damage Detection Using Satellite Imagery", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Large-Scale Damage Detection Using Satellite Imagery", "label": "published_in", "title": "published_in", "to": "CVPR", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Raffay Hamid", "label": "authored", "title": "authored", "to": "Large-Scale Damage Detection Using Satellite Imagery", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Raffay Hamid", "label": "affiliated_with", "title": "affiliated_with", "to": "DigitalGlobe Inc.", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Raffay Hamid", "label": "has_email", "title": "has_email", "to": "mhamid@digitalGlobe.com", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Satellite imagery", "label": "is_used_for", "title": "is_used_for", "to": "assessing damages", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Manual inspection", "label": "is_limited_by", "title": "is_limited_by", "to": "vast amount of data", "width": 3.88}, {"arrows": "to", "color": "#00CC77", "from": "semi-supervised learning", "label": "improves", "title": "improves", "to": "crowd counting", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "study", "label": "uses", "title": "uses", "to": "88 million images", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "study", "label": "analyzes", "title": "analyzes", "to": "sun angle", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "study", "label": "analyzes", "title": "analyzes", "to": "sensor resolution", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "study", "label": "analyzes", "title": "analyzes", "to": "registration differences", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "study", "label": "considers", "title": "considers", "to": "time constraints during offline training", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "4,665 KM2", "label": "is_across", "title": "is_across", "to": "12 locations", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "user study", "label": "demonstrates", "title": "demonstrates", "to": "ten-fold reduction", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "user study", "label": "evaluates", "title": "evaluates", "to": "representation", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "ten-fold reduction", "label": "is_in", "title": "is_in", "to": "human annotation time", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "ten-fold reduction", "label": "impacts", "title": "impacts", "to": "efficiency", "width": 3.64}, {"arrows": "to", "color": "#CC7700", "from": "hierarchical shape features", "label": "is_within", "title": "is_within", "to": "bag-of-visual words setting", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "representation", "label": "compared_to", "title": "compared_to", "to": "five alternatives", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "representation", "label": "offers", "title": "offers", "to": "time efficiency", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "representation", "label": "improves", "title": "improves", "to": "annotation process", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "representation", "label": "is", "title": "is", "to": "core challenge", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "representation", "label": "maintains", "title": "maintains", "to": "rich representation", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "detection accuracy", "label": "compared_to", "title": "compared_to", "to": "manual inspection", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "detection accuracy", "label": "experienced", "title": "experienced", "to": "minimal loss", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "User study", "label": "demonstrates", "title": "demonstrates", "to": "ten-fold reduction in annotation time", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "User study", "label": "results in", "title": "results in", "to": "minimal loss in detection accuracy", "width": 3.88}, {"arrows": "to", "color": "#CC7700", "from": "Damage detection", "label": "utilizes", "title": "utilizes", "to": "Hierarchical shape features", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "Semi-supervised learning", "label": "addresses", "title": "addresses", "to": "Novelty detection", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Semi-supervised learning", "label": "used_in", "title": "used_in", "to": "gigantic image collections", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "Xia et al. (2010)", "label": "publishes", "title": "publishes", "to": "Shape-based invariant texture indexing", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Markou \u0026 Singh (2003)", "label": "reviews", "title": "reviews", "to": "Novelty detection", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Blanchard et al. (2010)", "label": "investigates", "title": "investigates", "to": "Semi-supervised novelty detection", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Bruzone \u0026 Prieto (2000)", "label": "analyzes", "title": "analyzes", "to": "Difference image", "width": 3.82}, {"arrows": "to", "color": "#CC7700", "from": "Difference image", "label": "supports", "title": "supports", "to": "Unsupervised change detection", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "Satellite imagery analysis", "label": "benefits from", "title": "benefits from", "to": "Damage detection", "width": 3.61}, {"arrows": "to", "color": "#0077CC", "from": "Bruzzone", "label": "authored", "title": "authored", "to": "Automatic analysis of the difference image", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Prieto", "label": "authored", "title": "authored", "to": "Automatic analysis of the difference image", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Gerard", "label": "authored", "title": "authored", "to": "A quasi-linear algorithm", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "International Symposium on Mathematical Morphology", "label": "published", "title": "published", "to": "A quasi-linear algorithm", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Monasse", "label": "authored", "title": "authored", "to": "Fast computation of a contrast-invariant image representation", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Guichard", "label": "authored", "title": "authored", "to": "Fast computation of a contrast-invariant image representation", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Nielsen", "label": "authored", "title": "authored", "to": "The regularized iteratively reweighted mad method", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Vaduva", "label": "authored", "title": "authored", "to": "A latent analysis of earth surface dynamic evolution", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "at", "label": "author_of", "title": "author_of", "to": "A latent analysis of earth surface dynamic evolution using change map time series", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Lazarescu", "label": "author_of", "title": "author_of", "to": "A latent analysis of earth surface dynamic evolution using change map time series", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Datcu", "label": "author_of", "title": "author_of", "to": "A latent anisotropy of earth surface dynamic evolution using change map time series", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Gomez-Chova", "label": "author_of", "title": "author_of", "to": "Kernel-based framework for multitemporal and multisource remote sensing data classification and change detection", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Wang", "label": "author_of", "title": "author_of", "to": "Locality-constrained linear coding for image classification", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Wang", "label": "authored", "title": "authored", "to": "Semi-supervised hashing", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Wang", "label": "authored", "title": "authored", "to": "Weakly Supervised Localization", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Wang", "label": "authored", "title": "authored", "to": "LIBLINEAR", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Yang", "label": "author_of", "title": "author_of", "to": "Locality-constrained linear coding for image classification", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Yang", "label": "co-authored", "title": "co-authored", "to": "A survey on transfer learning", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Song", "label": "author_of", "title": "author_of", "to": "Fusing Subcategory Probabilities for Texture Classi\ufb01cation", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Cai", "label": "author_of", "title": "author_of", "to": "Fusing Subcategory Probabilities for Texture Classi\ufb01cation", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Gueguen", "label": "affiliated_with", "title": "affiliated_with", "to": "DigitalGlobe Inc.", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Hamid", "label": "affiliated_with", "title": "affiliated_with", "to": "DigitalGlobe Inc.", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Yang Song", "label": "is_author_of", "title": "is_author_of", "to": "Fusing Subcategory Probabilities for Texture Classification", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Yang Song", "label": "affiliated_with", "title": "affiliated_with", "to": "BMIT Research Group", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Fusing Subcategory Probabilities for Texture Classification", "label": "presented_at", "title": "presented_at", "to": "CVPR", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Fusing Subcategory Probabilities for Texture Classification", "label": "is_located_at", "title": "is_located_at", "to": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Fusing Subcategory Probabilities for Texture Classification", "label": "has_file_name", "title": "has_file_name", "to": "Song_Fusing_Subcategory_Probabilities_2015_CVPR_paper.pdf", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Qing Li", "label": "is_author_of", "title": "is_author_of", "to": "Fusing SubCategory Probabilities for Texture Classification", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Fan Zhang", "label": "is_author_of", "title": "is_author_of", "to": "Fusing Subcategory Probabilities for Texture Classification", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "David Dagan Feng", "label": "is_author_of", "title": "is_author_of", "to": "Fusing Subcategory Probabilities for Texture Classification", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "David Dagan Feng", "label": "affiliated_with", "title": "affiliated_with", "to": "BMIT Research Group", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Heng Huang", "label": "is_author_of", "title": "is_author_of", "to": "Fusing Subcategory Probabilities for Texture Classification", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Heng Huang", "label": "affiliated_with", "title": "affiliated_with", "to": "Department of Computer Science and Engineering", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Texture classification", "label": "remains", "title": "remains", "to": "challenging", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Texture classification", "label": "faces", "title": "faces", "to": "high intra-class variation", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "sub-categorization model", "label": "applied to", "title": "applied to", "to": "texture classification", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "class", "label": "is divided into", "title": "is divided into", "to": "subcategories", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "subcategories", "label": "have", "title": "have", "to": "distinctiveness", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "subcategories", "label": "have", "title": "have", "to": "representativeness", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "subcategory probabilities", "label": "are fused based on", "title": "are fused based on", "to": "contribution levels", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "subcategory probabilities", "label": "are fused based on", "title": "are fused based on", "to": "cluster qualities", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "fused probability", "label": "added to", "title": "added to", "to": "multiclass classification probability", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Texture Classification", "label": "uses", "title": "uses", "to": "Method", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "BMIT Research Group", "label": "located_in", "title": "located_in", "to": "University of Sydney", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "BMIT Research Group", "label": "is_part_of", "title": "is_part_of", "to": "School of IT", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "University of Sydney", "label": "located_in", "title": "located_in", "to": "Australia", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "School of IT", "label": "located_in", "title": "located_in", "to": "University of Sydney", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Department of Computer Science and Engineering", "label": "located_in", "title": "located_in", "to": "University of Texas, Arlington", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Manohar Paluri", "label": "author_of", "title": "author_of", "to": "Beyond Frontal Faces", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Manohar Paluri", "label": "is_author_of", "title": "is_author_of", "to": "Zhang_Beyond_Frontal_Faces_2015_CVPR_paper", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Manohar Paluri", "label": "works_at", "title": "works_at", "to": "Facebook AI Research", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Beyond Frontal Faces", "label": "published_in", "title": "published_in", "to": "CVPR", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Yaniv Taigman", "label": "author_of", "title": "author_of", "to": "Beyond Frontal Faces", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Yaniv Taigman", "label": "is_author_of", "title": "is_author_of", "to": "Zhang_Beyond_Frontal_Faces_2015_CVPR_paper", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Yaniv Taigman", "label": "works_at", "title": "works_at", "to": "Facebook AI Research", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Rob Fergus", "label": "author_of", "title": "author_of", "to": "Beyond Frontal Faces", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Rob Fergus", "label": "is_author_of", "title": "is_author_of", "to": "Zhang_Beyond_Frontal_Faces_2015_CVPR_paper", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Rob Fergus", "label": "works_at", "title": "works_at", "to": "Facebook AI Research", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Lubomir Bourdev", "label": "author_of", "title": "author_of", "to": "Beyond Frontal Faces", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Lubomir Bourdev", "label": "is_author_of", "title": "is_author_of", "to": "Zhang_Beyond_Frontal_Faces_2015_CVPR_paper", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Lubomir Bourdev", "label": "works_at", "title": "works_at", "to": "Facebook AI Research", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Zhang_Beyond_Frontal_Faces_2015_CVPR_paper", "label": "explores_task", "title": "explores_task", "to": "recognizing peoples\u2019 identities", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "PIPA dataset", "label": "facilitates", "title": "facilitates", "to": "recognizing peoples\u2019 identities", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "PIPA dataset", "label": "consists_of", "title": "consists_of", "to": "60000 instances", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "PIPA dataset", "label": "contains", "title": "contains", "to": "2000 individuals", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "PIPER method", "label": "combines", "title": "combines", "to": "face recognizer", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "PIPER method", "label": "combines", "title": "combines", "to": "global recognizer", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "face recognizer", "label": "combined_with", "title": "combined_with", "to": "global recognizer", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "person images", "label": "contains", "title": "contains", "to": "frontal face", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "person recognizers", "label": "trained_by", "title": "trained_by", "to": "deep convolutional networks", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "deep convolutional networks", "label": "aims_to", "title": "aims_to", "to": "discount pose variations", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "PIPER", "label": "improves_performance_on", "title": "improves_performance_on", "to": "DeepFace", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "PIPER", "label": "operates_in", "title": "operates_in", "to": "unconstrained setup", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "DeepFace", "label": "is_regarded_as", "title": "is_regarded_as", "to": "best face recognizers", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "DeepFace", "label": "measured_on", "title": "measured_on", "to": "LFW dataset", "width": 3.7600000000000002}, {"arrows": "to", "color": "#CC7700", "from": "DeepFace", "label": "is_a", "title": "is_a", "to": "face recognizer", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "Pose Invariant Recognition", "label": "is_setting_for", "title": "is_setting_for", "to": "PIPER", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Paul Wohlhart", "label": "works_at", "title": "works_at", "to": "Institute for Computer Vision and Graphics, Graz University of Technology, Austria", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Paul Wohlhart", "label": "contributed_to", "title": "contributed_to", "to": "Learning Descriptors for Object Recognition and 3D Pose Estimation", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "Paul Wohlhart", "label": "affiliated_with", "title": "affiliated_with", "to": "Institute for Computer Vision and Graphics", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Paul Wohlhart", "label": "email", "title": "email", "to": "{wohlhart}@icg.tugraz.at", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Paul Wohlhart", "label": "contributed_to", "title": "contributed_to", "to": "2015 CVPR paper", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Paul Wohlhart", "label": "works_in", "title": "works_in", "to": "Computer Graphics", "width": 3.0999999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Paul Wohlhart", "label": "located_in", "title": "located_in", "to": "Austria", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Learning Descriptors for Object Recognition and 3D Pose Estimation", "label": "is_paper", "title": "is_paper", "to": "cvpr_papers", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "cvpr_papers", "label": "located_in", "title": "located_in", "to": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "cvpr_papers", "label": "located_in", "title": "located_in", "to": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/", "width": 3.25}, {"arrows": "to", "color": "#0077CC", "from": "Institute for Computer Vision and Graphics", "label": "part_of", "title": "part_of", "to": "Graz University of Technology", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "Institute for Computer Vision and Graphics", "label": "research_area", "title": "research_area", "to": "Computer Vision", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Graz University of Technology", "label": "located_in", "title": "located_in", "to": "Austria", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Graz University of Technology", "label": "hosts", "title": "hosts", "to": "Institute for Computer Vision and Graphics", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Graz University of Technology", "label": "has_institute", "title": "has_institute", "to": "Institute for Computer Graphics and Vision", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "2015 CVPR paper", "label": "focuses_on", "title": "focuses_on", "to": "Learning Descriptors", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "2015 CVPR paper", "label": "published_by", "title": "published_by", "to": "IEEE", "width": 2.95}, {"arrows": "to", "color": "#CC7700", "from": "Learning Descriptors", "label": "related_to", "title": "related_to", "to": "Computer Vision", "width": 3.25}, {"arrows": "to", "color": "#0077CC", "from": "IEEE", "label": "published", "title": "published", "to": "A shape-preserving approach to image resizing", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Vincent LePetit", "label": "affiliated_with", "title": "affiliated_with", "to": "Institute for Computer Vision and Graphics", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Vincent LePetit", "label": "located_in", "title": "located_in", "to": "Austria", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Joe Yue-Hei Ng", "label": "author_of", "title": "author_of", "to": "Beyond Short Snippets", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Joe Yue-Hei Ng", "label": "affiliation", "title": "affiliation", "to": "University of Maryland, College Park", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Beyond Short Snippets", "label": "presented_at", "title": "presented_at", "to": "CVPR", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Matthew Hausknecht", "label": "author_of", "title": "author_of", "to": "Beyond Short Snippets", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Matthew Hausknecht", "label": "affiliation", "title": "affiliation", "to": "University of Texas at Austin", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Sudheendra Vijayanarasimhan", "label": "author_of", "title": "author_of", "to": "Beyond Short Snippets", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Sudheendra Vijayanarasimhan", "label": "affiliation", "title": "affiliation", "to": "Google, Inc.", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Oriol Vinyals", "label": "author_of", "title": "author_of", "to": "Beyond Short Snippets", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Oriol Vinyals", "label": "affiliation", "title": "affiliation", "to": "Google, Inc.", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Rajat Monga", "label": "author_of", "title": "author_of", "to": "Beyond Short Snppets", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Rajat Monga", "label": "affiliation", "title": "affiliation", "to": "Google, Inc.", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "George Toderici", "label": "author_of", "title": "author_of", "to": "Beyond Short Snippets", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "George Toderici", "label": "affiliation", "title": "affiliation", "to": "Google, Inc.", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Convolutional neural networks (CNNs)", "label": "applied_for", "title": "applied_for", "to": "image recognition problems", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Convolutional neural networks (CNNs)", "label": "yields", "title": "yields", "to": "state-of-the-art results", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "deep neural network architectures", "label": "combines", "title": "combines", "to": "image information", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "image information", "label": "spans", "title": "spans", "to": "longer time periods", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "video", "label": "models_as", "title": "models_as", "to": "ordered sequence of frames", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "video", "label": "contains", "title": "contains", "to": "novel object", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "video", "label": "contains", "title": "contains", "to": "frames", "width": 3.88}, {"arrows": "to", "color": "#CC7700", "from": "recurrent neural network", "label": "uses", "title": "uses", "to": "Long Short-Term Memory (LSTM) cells", "width": 3.91}, {"arrows": "to", "color": "#CC7700", "from": "LSTM cells", "label": "connected_to", "title": "connected_to", "to": "output of CNN", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "networks", "label": "exhibits", "title": "exhibits", "to": "performance improvements", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "networks", "label": "performs_on", "title": "performs_on", "to": "UCF-101 datasets", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "networks", "label": "captures", "title": "captures", "to": "local contextual information", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "networks", "label": "captures", "title": "captures", "to": "global contextual information", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "networks", "label": "operates_on", "title": "operates_on", "to": "RGB values", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "performance improvements", "label": "over", "title": "over", "to": "alternative algorithms", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "UCF-101 datasets", "label": "has_performance", "title": "has_performance", "to": "88.6%", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "UCF-101 datasets", "label": "compared_to", "title": "compared_to", "to": "88.0%", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "UCF-101 datasets", "label": "has_performance_without", "title": "has_performance_without", "to": "optical flow information", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "UCF-101 datasets", "label": "compared_to", "title": "compared_to", "to": "73.0%", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "UCF-101 datasets", "label": "has_accuracy", "title": "has_accuracy", "to": "82.6%", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Sports 1 million dataset", "label": "has_accuracy", "title": "has_accuracy", "to": "73.1%", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Sports 1 million dataset", "label": "compared_to", "title": "compared_to", "to": "60.9%", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Ioannis Gkiouslekas", "label": "author_of", "title": "author_of", "to": "On the Appearance of Translucnt Edges", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Ioannis Gkiouslekas", "label": "affiliated with", "title": "affiliated with", "to": "Harvard SEAS", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Gkiouslekas", "label": "affiliated_with", "title": "affiliated_with", "to": "Harvard SEAS", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "Harvard SEAS", "label": "is_an_institution_of", "title": "is_an_institution_of", "to": "Higher Education", "width": 3.25}, {"arrows": "to", "color": "#0077CC", "from": "Bruce Walter", "label": "affiliated with", "title": "affiliated with", "to": "Cornell University", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Bruce Walter", "label": "has_email", "title": "has_email", "to": "bruce.walter@cornell.edu", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Bruce Walter", "label": "author_of", "title": "author_of", "to": "On the Appearance of Translueceny Edges", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "Cornell University", "label": "is_an_institution_of", "title": "is_an_institution_of", "to": "Higher Education", "width": 3.25}, {"arrows": "to", "color": "#0077CC", "from": "Cornell University", "label": "has_department", "title": "has_department", "to": "Department of Computer Science", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Edward H. Adelson", "label": "affiliation", "title": "affiliation", "to": "Massachusetts Institute of Technology", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Edward H. Adelson", "label": "has_email", "title": "has_email", "to": "adelson@cail.mit.edu", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Edward H. Adelson", "label": "author_of", "title": "author_of", "to": "On the Appearance of Translueceny Edges", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "Massachusetts Institute of Technology", "label": "is_an_institution_of", "title": "is_an_institution_of", "to": "Higher Education", "width": 3.25}, {"arrows": "to", "color": "#0077CC", "from": "Massachusetts Institute of Technology", "label": "hosts", "title": "hosts", "to": "CSAIL", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Massachusetts Institute of Technology", "label": "hosts", "title": "hosts", "to": "LIDS", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Walter", "label": "authored", "title": "authored", "to": "Microfacet models", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Microfacet models", "label": "addresses", "title": "addresses", "to": "refraction through rough surfaces", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Microfacet models", "label": "presented_at", "title": "presented_at", "to": "EGSR", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Ioannis Gkioleakas", "label": "affiliated_with", "title": "affiliated_with", "to": "Harvard SEAS", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Ioannis Gkioleakas", "label": "has_email", "title": "has_email", "to": "igkio@seas.harvard.edu", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "Higher Education", "label": "provides", "title": "provides", "to": "Academic Degrees", "width": 3.0999999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Ioannis Gkiousleas", "label": "is_affiliated_with", "title": "is_affiliated_with", "to": "Harvard SEAS", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Todd Zickler", "label": "affiliation", "title": "affiliation", "to": "Harvard SEAS", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Todd Zickler", "label": "author_of", "title": "author_of", "to": "On the Appearence of Translueceny Edges", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Kavita Bala", "label": "affiliation", "title": "affiliation", "to": "Cornell University", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Kavita Bala", "label": "is_author_of", "title": "is_author_of", "to": "Material recognition in 2015 CVPR paper", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Kavita Bala", "label": "contributes_to", "title": "contributes_to", "to": "Bell_Material_Recognition_in_2015_CVPR_paper", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Kavita Bala", "label": "affiliated_with", "title": "affiliated_with", "to": "Department of Computer Science, Cornell University", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Kavita Bala", "label": "has_email", "title": "has_email", "to": "kb@cs.cornell.edu", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Kavita Bala", "label": "author_of", "title": "author_of", "to": "On the Appearance of Translueceny Edges", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Naeemullah Khan", "label": "is_author_of", "title": "is_author_of", "to": "Shape-Tailored Local Descriptors", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "Shape-Tailored Local Descriptors", "label": "is_a", "title": "is_a", "to": "CVPR paper", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Shape-Tailored Local Descriptors", "label": "has_application_in", "title": "has_application_in", "to": "Segmentation", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Shape-Tailored Local Descriptors", "label": "has_application_in", "title": "has_application_in", "to": "Tracking", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Marei Algarni", "label": "is_author_of", "title": "is_author_of", "to": "Shape-Tailored Local Descriptors", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Marei Algarni", "label": "affiliated_with", "title": "affiliated_with", "to": "King Abdullah University of Science \u0026 Technology (KAUST)", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Anthony Yezzi", "label": "is_author_of", "title": "is_author_of", "to": "Shape-Tailed Local Descriptors", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Anthony Yezzi", "label": "affiliated_with", "title": "affiliated_with", "to": "Georgia Institute of Technology", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Ganesh Sundaramoorthi", "label": "is_author_of", "title": "is_author_of", "to": "Shape-Tailored Local Descriptors", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Ganesh Sundaramoorthi", "label": "affiliated_with", "title": "affiliated_with", "to": "King Abdullah University of Science \u0026 Technology (KAUST)", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "descriptors", "label": "are", "title": "are", "to": "dense descriptors", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "descriptors", "label": "used for", "title": "used for", "to": "texture segmentation", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "descriptors", "label": "formed from", "title": "formed from", "to": "shape-dependent scale spaces", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "descriptors", "label": "do not", "title": "do not", "to": "aggregate image data across boundary", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "descriptors", "label": "lead_to", "title": "lead_to", "to": "accurate segmentation", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "descriptors", "label": "are", "title": "are", "to": "shape-dependent", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "descriptors", "label": "outperforms", "title": "outperforms", "to": "state-of-the-art", "width": 3.91}, {"arrows": "to", "color": "#CC7700", "from": "descriptors", "label": "are", "title": "are", "to": "non-shape dependent", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "datasets", "label": "contain", "title": "contain", "to": "3D models", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "datasets", "label": "are", "title": "are", "to": "SIFTs", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "accurate segmentation", "label": "improves", "title": "improves", "to": "texture segmentation", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "oriented gradients", "label": "used in", "title": "used in", "to": "existing descriptors", "width": 3.4000000000000004}, {"arrows": "to", "color": "#CC7700", "from": "textured object tracking", "label": "relies_on", "title": "relies_on", "to": "texture segmentation", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Shape-Tailored Descriptors (STLD)", "label": "improves", "title": "improves", "to": "texture segmentation", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Shape-Tailori Descriptors (STLD)", "label": "outperforms", "title": "outperforms", "to": "non-shape dependent descriptors", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Local Descriptors", "label": "used in", "title": "used in", "to": "texture segmentation", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "De-An Huang", "label": "is_author_of", "title": "is_author_of", "to": "How Do We Use Our Hands?", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "De-An Huang", "label": "is_author_of", "title": "is_author_of", "to": "Common Grasps", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "De-An Huang", "label": "affiliated_with", "title": "affiliated_with", "to": "Cnegie Mellon University", "width": 3.9699999999999998}, {"arrows": "to", "color": "#CC7700", "from": "How Do We Use Our Hands?", "label": "discusses", "title": "discusses", "to": "common grasps", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Partial Differential Equations (PDE)", "label": "used in", "title": "used in", "to": "texture segmentation", "width": 3.25}, {"arrows": "to", "color": "#0077CC", "from": "Minghuang Ma", "label": "is_author_of", "title": "is_author_of", "to": "How DoWe Use Our Hands?", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Minghuang Ma", "label": "is_author_of", "title": "is_author_of", "to": "Common Grasps", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Minghuang Ma", "label": "affiliated_with", "title": "affiliated_with", "to": "Cnegie Mellon University", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Minghuang Ma", "label": "affiliated_with", "title": "affiliated_with", "to": "Canezie Mellon University", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Minghuang Ma", "label": "affiliated_with", "title": "affiliated_with", "to": "Carnegie Mellon University", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Wei-Chiu Ma", "label": "is_author_of", "title": "is_author_of", "to": "How Do We Use Our Hands?", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Wei-Chiu Ma", "label": "is_author_of", "title": "is_author_of", "to": "Common Grasps", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Wei-Chiu Ma", "label": "affiliated_with", "title": "affiliated_with", "to": "Cnegie Mellon University", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Wei-Chiu Ma", "label": "affiliated_with", "title": "affiliated_with", "to": "Canezie Mellon University", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Wei-Chiu Ma", "label": "affiliated_with", "title": "affiliated_with", "to": "Carnegie Mellon University", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Kris M. Kitani", "label": "is_author_of", "title": "is_author_of", "to": "How Do We Use Our Hands?", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Kris M. Kitani", "label": "is_author_of", "title": "is_author_of", "to": "Common Graspt", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Kris M. Kitani", "label": "affiliated_with", "title": "affiliated_with", "to": "Canezie Mellon University", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "KAUST", "label": "located_in", "title": "located_in", "to": "Saudi Arabia", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "ganesh.sundaramoorthi@kust.edu.sa", "label": "is_affiliated_with", "title": "is_affiliated_with", "to": "KAUST", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Huang_How_Do_We2015_CVPR_paper.pdf", "label": "is_paper", "title": "is_paper", "to": "How Do We Use Our Hands?", "width": 3.88}, {"arrows": "to", "color": "#CC7700", "from": "computer vision techniques", "label": "can_be_used_to", "title": "can_be_used_to", "to": "advance prehensile analysis", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "prehensile analysis", "label": "is_a", "title": "is_a", "to": "multi-disciplinary field", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "researchers", "label": "analyze", "title": "analyze", "to": "hand-object interaction videos", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "wearable cameras", "label": "can_be_used_to", "title": "can_be_used_to", "to": "automatically discover common modes of human hand use", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "wearable cameras", "label": "led to", "title": "led to", "to": "increase in egocentric videos", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "unsupervised clustering techniques", "label": "can_be_used_to", "title": "can_be_used_to", "to": "automatically discover common modes of human hand use", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "first-person point-of-view camera", "label": "is_used_for", "title": "is_used_for", "to": "observing human hand use", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "common modes of human hand use", "label": "are_discovered_by", "title": "are_discovered_by", "to": "wearable cameras", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "first-person point-of-view videos", "label": "includes", "title": "includes", "to": "choreographed scenarios", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "Grasp Taxonomy", "label": "is_taxonomy_of", "title": "is_taxonomy_of", "to": "hand-object interaction", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Determinantal Point Process (DPP)", "label": "is_used_in", "title": "is_used_in", "to": "approach", "width": 3.25}, {"arrows": "to", "color": "#0077CC", "from": "N. Ailon", "label": "authors", "title": "authors", "to": "Streaming k-means approximation", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "W. Barbakh", "label": "authors", "title": "authors", "to": "Online clustering algorithms", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "A. Fathi", "label": "authors", "title": "authors", "to": "Social interactions: A first-person perspective", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "R. Filipovych", "label": "authors", "title": "authors", "to": "Recognizing primitive interactions", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "J. Case-Smith", "label": "authored", "title": "authored", "to": "Development of hand skills in children", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "Development of hand skills in children", "label": "addresses", "title": "addresses", "to": "hand skills", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "L. Cheng", "label": "coauthored", "title": "coauthored", "to": "Pixel-level hand detection in ego-centric videos", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "American Occupational Therapy Association", "label": "published", "title": "published", "to": "Development of hand skills in children", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "M. Cutkosky", "label": "authored", "title": "authored", "to": "On grasp choice, grasp models", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "H. N. Djidjev", "label": "coauthored", "title": "coauthored", "to": "Computing shortest paths", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "C. Desai", "label": "coauthored", "title": "coauthored", "to": "Discriminaitive models for static human-object interactions", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "Kris M. Kitan", "label": "affiliated_with", "title": "affiliated_with", "to": "Cnegie Mellon University", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Canezie Mellon University", "label": "has_affiliation", "title": "has_affiliation", "to": "Kris M. Kitani", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Canezie Mellon University", "label": "has_affiliation", "title": "has_affiliation", "to": "Wei-Chiu Ma", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Canezie Mellon University", "label": "has_affiliation", "title": "has_affiliation", "to": "Minghuang Ma", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "deanh@andrew.cmu.edu", "label": "associated_with", "title": "associated_with", "to": "Canezie Mellon University", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "deanh@andrew.cmu.edu", "label": "associated_with", "title": "associated_with", "to": "Carnegie Mellon University", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "minghuam@andrew.cmu.edu", "label": "associated_with", "title": "associated_with", "to": "Canezie Mellon University", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "weichium@andrew.cmu.edu", "label": "associated_with", "title": "associated_with", "to": "Canezie Mellon University", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Edward Johns", "label": "is_author_of", "title": "is_author_of", "to": "Becoming the Expert", "width": 3.91}, {"arrows": "to", "color": "#CC7700", "from": "Becoming the Expert", "label": "is_paper_about", "title": "is_paper_about", "to": "Machine Teaching", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Becoming the Expert", "label": "published_in", "title": "published_in", "to": "CVPR", "width": 3.25}, {"arrows": "to", "color": "#0077CC", "from": "Oisin Mac Aodha", "label": "is_author_of", "title": "is_author_of", "to": "Becoming the Efficient", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Gabriel J. Brostow", "label": "is_author_of", "title": "is_author_of", "to": "Becoming the Expert", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Johns_Becoming_the_Expert_2015_CVPR_paper.pdf", "label": "represents", "title": "represents", "to": "Becoming the Expert", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "computer", "label": "teaches", "title": "teaches", "to": "challenging visual concepts", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "teaching strategy", "label": "produces", "title": "produces", "to": "experts", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "challenge", "label": "concerns", "title": "concerns", "to": "annotators", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "challenge", "label": "related_to", "title": "related_to", "to": "stereo matching", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "challenge", "label": "involves", "title": "involves", "to": "ground control points", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "challenge", "label": "of", "title": "of", "to": "interpreting line drawings", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "annotators", "label": "have", "title": "have", "to": "expertise", "width": 3.64}, {"arrows": "to", "color": "#CC7700", "from": "Interactive Machine Teaching", "label": "relates_to", "title": "relates_to", "to": "Human Learning", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "Interactive Machine Teaching", "label": "utilizes", "title": "utilizes", "to": "Adaptive Algorithms", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "Interactive Machine Teaching", "label": "involves", "title": "involves", "to": "Visual Classification", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Bruner", "label": "authored", "title": "authored", "to": "The Process of Education", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "The Process of Education", "label": "provides", "title": "provides", "to": "foundational concepts", "width": 3.91}, {"arrows": "to", "color": "#CC7700", "from": "foundational concepts", "label": "relates_to", "title": "relates_to", "to": "teaching and learning", "width": 3.88}, {"arrows": "to", "color": "#CC7700", "from": "Curriculum learning", "label": "aligns_with", "title": "aligns_with", "to": "teaching strategies", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "Love", "label": "authored", "title": "authored", "to": "Categorization", "width": 3.9699999999999998}, {"arrows": "to", "color": "#CC7700", "from": "Categorization", "label": "is_element_of", "title": "is_element_of", "to": "cognitive neuroscience", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "Categorization", "label": "is_element_of", "title": "is_element_of", "to": "core tasks", "width": 3.79}, {"arrows": "to", "color": "#CC7700", "from": "cognitive neuroscience", "label": "provides_perspective_on", "title": "provides_perspective_on", "to": "Categorization", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "active learning", "label": "is_explored_in", "title": "is_explored_in", "to": "paper", "width": 3.64}, {"arrows": "to", "color": "#CC7700", "from": "active learning", "label": "is_technique", "title": "is_technique", "to": "overview", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "active learning", "label": "is_key", "title": "is_key", "to": "technique", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "Machine teaching", "label": "is_problem_of", "title": "is_problem_of", "to": "machine learning", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "Machine teaching", "label": "is_approach_to", "title": "is_approach_to", "to": "education", "width": 3.4000000000000004}, {"arrows": "to", "color": "#CC7700", "from": "overview", "label": "is_about", "title": "is_about", "to": "transparent object reconstruction", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "technique", "label": "outperforms", "title": "outperforms", "to": "video segmentation", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "technique", "label": "is_based_on", "title": "is_based_on", "to": "CNN", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "technique", "label": "achieves", "title": "achieves", "to": "high accuracy", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "technique", "label": "detects", "title": "detects", "to": "salient objects", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "technique", "label": "predicts", "title": "predicts", "to": "number of objects", "width": 3.88}, {"arrows": "to", "color": "#00CC77", "from": "sampling estimation", "label": "reduces", "title": "reduces", "to": "error", "width": 3.4000000000000004}, {"arrows": "to", "color": "#CC7700", "from": "machine teaching", "label": "is_relevant_to", "title": "is_relevant_to", "to": "approach toward optimal education", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "approach toward optimal education", "label": "incorporates", "title": "incorporates", "to": "machine teaching", "width": 3.61}, {"arrows": "to", "color": "#0077CC", "from": "paper\u0027s design", "label": "addresses", "title": "addresses", "to": "learners with limited cognitive capacity", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "algorithmic teaching", "label": "provides", "title": "provides", "to": "background", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Love \u0026 Patil", "label": "addresses", "title": "addresses", "to": "teaching learners", "width": 3.25}, {"arrows": "to", "color": "#0077CC", "from": "Balbach \u0026 Zeugmann", "label": "contributes_to", "title": "contributes_to", "to": "algorithmic teaching methods", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "Basu \u0026 Christensen", "label": "focuses_on", "title": "focuses_on", "to": "teaching classification boundaries", "width": 3.46}, {"arrows": "to", "color": "#0077CC", "from": "Basu \u0026 Christensen", "label": "related_to", "title": "related_to", "to": "teaching classification tasks", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Language and Automata Theory and Applications", "label": "published", "title": "published", "to": "Recent developments in algorithmic teaching", "width": 3.04}, {"arrows": "to", "color": "#CC7700", "from": "teaching classification tasks", "label": "requires", "title": "requires", "to": "algorithmic teaching methods", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Gigu`ere \u0026 Love", "label": "explores", "title": "explores", "to": "cognitive limitations", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "cognitive limitations", "label": "influences", "title": "influences", "to": "teaching strategies", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Chin", "label": "affiliated_with", "title": "affiliated_with", "to": "University College London", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Eriksson", "label": "affiliated_with", "title": "affiliated_with", "to": "University College London", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Suter", "label": "affiliated_with", "title": "affiliated_with", "to": "University College London", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Chin_Efficient_Globally_Optimal_2015_CVPR_paper.pdf", "label": "is_a", "title": "is_a", "to": "CVPR paper", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Efficient Globally Optimal Consensus Maximisation", "label": "described_in", "title": "described_in", "to": "Chin_Efficient_Globally_Optimal_2015_CVPR_paper.pdf", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CCCCCC", "from": "Maximum Consensus", "label": "is_criterion_for", "title": "is_criterion_for", "to": "Robust Estimation", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "Maximum Consensus", "label": "is_related_to", "title": "is_related_to", "to": "Optimization Problems", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "randomized sample-and-test techniques", "label": "does_not_guarantee", "title": "does_not_guarantee", "to": "optimality", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "globally optimal algorithms", "label": "is_too_slow_compared_to", "title": "is_too_slow_compared_to", "to": "randomized methods", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "tree search problem", "label": "uses", "title": "uses", "to": "LP-type methods", "width": 3.82}, {"arrows": "to", "color": "#CC7700", "from": "A* Search", "label": "is_a", "title": "is_a", "to": "Search Algorithm", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "A* Search", "label": "achieves", "title": "achieves", "to": "globally optimal results", "width": 3.25}, {"arrows": "to", "color": "#CC7700", "from": "Tree Search", "label": "is_a", "title": "is_a", "to": "Search Algorithm", "width": 3.64}, {"arrows": "to", "color": "#CC7700", "from": "LP-type Methods", "label": "addresses", "title": "addresses", "to": "Optimization Problems", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "N. Amenta", "label": "authored", "title": "authored", "to": "Optimal Point Placement for Mesh Smoothing", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "M. Bern", "label": "authored", "title": "authored", "to": "Optimal Point Placement for Mesh Smoothing", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "D. Eppstein", "label": "authored", "title": "authored", "to": "Optimal Point Placement for Mesh Smoothing", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "B. Chazelle", "label": "authored", "title": "authored", "to": "On Linear-Time Deterministic Algorithms", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "J. Matou\u02c7sek", "label": "authored", "title": "authored", "to": "On Linear-Time Deterministic Algorithms", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "Quasiconvex Programming", "label": "is_a", "title": "is_a", "to": "Optimization Technique", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "optimization algorithms", "label": "related_to", "title": "related_to", "to": "core theme", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "optimization algorithms", "label": "used_in", "title": "used_in", "to": "computer vision", "width": 3.25}, {"arrows": "to", "color": "#0077CC", "from": "RANSA algorithm", "label": "introduces", "title": "introduces", "to": "outlier rejection", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "RANSA algorithm", "label": "applies_to", "title": "applies_to", "to": "image analysis", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "RANSA algorithm", "label": "applies_to", "title": "applies_to", "to": "automated cartography", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "multiple view geometry", "label": "is_topic", "title": "is_topic", "to": "central topic", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "multiple view geometry", "label": "is_reference_for", "title": "is_reference_for", "to": "computer vision", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "l\u221e triangulation", "label": "is_method_for", "title": "is_method_for", "to": "outlier handling", "width": 3.67}, {"arrows": "to", "color": "#0077CC", "from": "H. Li", "label": "authored", "title": "authored", "to": "algorithm for l\u221e triangulation", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "algorithm for l\u221e triangulation", "label": "handles", "title": "handles", "to": "outliers", "width": 3.7600000000000002}, {"arrows": "to", "color": "#CC7700", "from": "geometric optimization", "label": "related_to", "title": "related_to", "to": "constraints", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "C. Olsson, O. Enqvist, and F. Kahl", "label": "focused_on", "title": "focused_on", "to": "outlier handling in matching", "width": 3.79}, {"arrows": "to", "color": "#CC7700", "from": "matching", "label": "part_of", "title": "part_of", "to": "registration", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "C. Olsson, A. Eriksson, and F. Kahl", "label": "addressed", "title": "addressed", "to": "optimization techniques", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "optimization techniques", "label": "for", "title": "for", "to": "l\u221e-norm problems", "width": 3.91}, {"arrows": "to", "color": "#CC7700", "from": "optimization techniques", "label": "important_for", "title": "important_for", "to": "efficient computation", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "Tat-Jun Chin", "label": "affiliated_with", "title": "affiliated_with", "to": "School of Computer Science", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Tat-Jun Chin", "label": "is_affiliated_with", "title": "is_affiliated_with", "to": "School of Computer Science, The University of Adelaide", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "University of Adelaide", "label": "contains", "title": "contains", "to": "School of Computer Science", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "University of Adelaide", "label": "has_author", "title": "has_author", "to": "Bolei Zhou", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "University of Adelaide", "label": "has_author", "title": "has_author", "to": "Fuyuan Hu", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "University of Adelaide", "label": "has_author", "title": "has_author", "to": "Zhen Zhang", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "University of Adelaide", "label": "has_author", "title": "has_author", "to": "Anton van den Hengel", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "University of Adelaide", "label": "has_author", "title": "has_author", "to": "Chunhua Shen", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Anders Eriksson", "label": "affiliated_with", "title": "affiliated_with", "to": "School of Computer Science", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Anders Eriksson", "label": "is_affiliated_with", "title": "is_affiliated_with", "to": "School of Electrical Engineering and Computer Science, Queensland University of Technology", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Pulak Purkait", "label": "is_affiliated_with", "title": "is_affiliated_with", "to": "School of Computer Science, The University of Adelaide", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Julian Straub", "label": "is_author_of", "title": "is_author_of", "to": "Small-Variance Nonparametric Clustering on the Hypsphere", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Julian Straub", "label": "affiliated_with", "title": "affiliated_with", "to": "CSAIL", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Julian Straub", "label": "affiliated_with", "title": "affiliated_with", "to": "LIDS", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Small-Variance Nonparametric Clustering on the Hypsphere", "label": "is_located_at", "title": "is_located_at", "to": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Straub_Small-Variance_Nonparametric_Clustering_2015_CVPR_paper.pdf", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Trevor Campbell", "label": "is_author_of", "title": "is_author_of", "to": "Small-Variance Nonparametric Clustering on the Hypsphere", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Trevor Campbell", "label": "affiliated_with", "title": "affiliated_with", "to": "CSAIL", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Trevor Campbell", "label": "affiliated_with", "title": "affiliated_with", "to": "LIDS", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Jonathan P. How", "label": "is_author_of", "title": "is_author_of", "to": "Small-Variance Nonparametric Clustering on the Hypsphere", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Jonathan P. How", "label": "affiliated_with", "title": "affiliated_with", "to": "CSAIL", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Jonathan P. How", "label": "affiliated_with", "title": "affiliated_with", "to": "LIDS", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "John W. Fisher III", "label": "is_author_of", "title": "is_author_of", "to": "Small-Variance Nonparametric Clustering on the Hypsphere", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "John W. Fisher III", "label": "affiliated_with", "title": "affiliated_with", "to": "CSAIL", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "John W. Fisher III", "label": "affiliated_with", "title": "affiliated_with", "to": "LIDS", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "John W. Fisher III", "label": "email", "title": "email", "to": "fisher@csaill.mit.edu", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Straub_Small-Variance_Nonparametric_Clustering_2015_CVPR_paper.pdf", "label": "describes", "title": "describes", "to": "Small-Variance Nonparametric Clustering on the Hypshere", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "surface normals", "label": "reflect", "title": "reflect", "to": "distribution of structural regularities", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "algorithms", "label": "derived from", "title": "derived from", "to": "Bayesian nonparametric von-Mises-Fisher (vMF) mixture distributions", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "algorithms", "label": "respect", "title": "respect", "to": "geometry of directional data", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "algorithms", "label": "demonstrates_performance_on", "title": "demonstrates_performance_on", "to": "synthetic directional data", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "algorithms", "label": "demonstrated on", "title": "demonstrated on", "to": "real 3D surface normals", "width": 3.46}, {"arrows": "to", "color": "#0077CC", "from": "algorithms", "label": "respects", "title": "respects", "to": "geometry", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "algorithms", "label": "demonstrates_performance_on", "title": "demonstrates_performance_on", "to": "3D surface normals", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "algorithms", "label": "generalizes_to", "title": "generalizes_to", "to": "high dimensional directional data", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "DDP-vMF-means", "label": "infers", "title": "infers", "to": "temporally evolving cluster structure", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "DDP-vMF-means", "label": "handles", "title": "handles", "to": "streaming data", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "directional data", "label": "lies_on", "title": "lies_on", "to": "unit sphere", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "DP-vMF-means", "label": "is a", "title": "is a", "to": "Dirichlet process (DP) vMF mixture", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "high dimensional directional data", "label": "includes", "title": "includes", "to": "protein backbone configurations", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "high dimensional directional data", "label": "includes", "title": "includes", "to": "semantic word vectors", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Algorithms", "label": "utilize", "title": "utilize", "to": "RGB-D sensors", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Algorithms", "label": "generalize_to", "title": "generalize_to", "to": "protein backbone configurations", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Algorithms", "label": "generalize_to", "title": "generalize_to", "to": "semantic word vectors", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Algorithms", "label": "measured_by", "title": "measured_by", "to": "Runtime", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Algorithms", "label": "measured_by", "title": "measured_by", "to": "Solution Quality", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Surface Normals", "label": "derived_from", "title": "derived_from", "to": "RGB-D sensors", "width": 3.7600000000000002}, {"arrows": "to", "color": "#CC7700", "from": "Bayesian Nonparametric Clustering", "label": "relates_to", "title": "relates_to", "to": "von-Mises-Fisher Distributions", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "von-Mises-Fisher Distributions", "label": "used_in", "title": "used_in", "to": "Bayesian Nonparametric Clustering", "width": 3.64}, {"arrows": "to", "color": "#CC7700", "from": "Streaming Data Analysis", "label": "related_to", "title": "related_to", "to": "Bayesian Nonparametric Clustering", "width": 3.25}, {"arrows": "to", "color": "#0077CC", "from": "Abramowitz \u0026 Stegun", "label": "provides", "title": "provides", "to": "mathematical background", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Neal", "label": "discusses", "title": "discusses", "to": "Markov chain sampling", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "Markov chain sampling", "label": "applied_to", "title": "applied_to", "to": "DPMMs", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Jiang, Kulis, \u0026 Jordan", "label": "provides", "title": "provides", "to": "theoretical analysis", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "DPMMs", "label": "analyzed_by", "title": "analyzed_by", "to": "Jiang, Kulis, \u0026 Jordan", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "DPMMs", "label": "analyzed by", "title": "analyzed by", "to": "Jiang, K., Kulis, B., and Jordan, M.", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "DPMMs", "label": "related_to", "title": "related_to", "to": "Bayesian nonparametrics", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "DPMMs", "label": "focuses_on", "title": "focuses_on", "to": "spherical data", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Latent Dirichlet Allocation", "label": "is_relevant_to", "title": "is_relevant_to", "to": "spherical topic models", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Latent Dirichlet Allocation", "label": "presented in", "title": "presented in", "to": "JMLR", "width": 3.64}, {"arrows": "to", "color": "#CC7700", "from": "k-means", "label": "connected_to", "title": "connected_to", "to": "Bayesian nonparametrics", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Directional statistics", "label": "treated in", "title": "treated in", "to": "Mardia, K. V. and Jupp, P. E.", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Directional statistics", "label": "is_described_in", "title": "is_described_in", "to": "John Wiley \u0026 Sons", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Bayesian nonparametric methods", "label": "analyzed in", "title": "analyzed in", "to": "Ferguson, T.", "width": 3.88}, {"arrows": "to", "color": "#CC7700", "from": "Bayesian nonparametric methods", "label": "is_foundational_to", "title": "is_foundational_to", "to": "DPMMs", "width": 3.64}, {"arrows": "to", "color": "#CC7700", "from": "Ferguson distributions", "label": "introduced_through", "title": "introduced_through", "to": "p\u00b4olya urn schemes", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Dirichlet processes", "label": "provides_overview_in", "title": "provides_overview_in", "to": "Encyclopedia of Machine Learning", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Bayesian analysis", "label": "analyzes", "title": "analyzes", "to": "nonparametric problems", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "Dingwen Zhang", "label": "author_of", "title": "author_of", "to": "Co-Saliency Detection via Looking Deep and Wide", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Dingwen Zhang", "label": "affiliated_with", "title": "affiliated_with", "to": "Northwestern Polytechnical University", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Co-Saliency Detection via Looking Deep and Wide", "label": "published_as", "title": "published_as", "to": "cvpr_papers/Zhang_Co-Saliency_Detection_via_2015_CVPR_paper.pdf", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Junwei Han", "label": "author_of", "title": "author_of", "to": "Co-Saliency Detection via Looking Deep and Wide", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Junwei Han", "label": "affiliated_with", "title": "affiliated_with", "to": "Northwestern Polytechnical University", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Chao Li", "label": "author_of", "title": "author_of", "to": "Co-Saliency Detection via Looking Deep and Wide", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Chao Li", "label": "affiliated_with", "title": "affiliated_with", "to": "Northwestern Polytechnical University", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Jingdong Wang", "label": "author_of", "title": "author_of", "to": "Co-Saliency Detection via Looking Deep and Wide", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Jingdong Wang", "label": "affiliation", "title": "affiliation", "to": "Microsoft Research", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Jingdong Wang", "label": "email", "title": "email", "to": "jingdw@microsoft.com", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "co-saliency detection", "label": "is_essential_for", "title": "is_essential_for", "to": "video foreground extraction", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "co-saliency detection", "label": "is_essential_for", "title": "is_essential_for", "to": "surveillance", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "co-saliency detection", "label": "is_essential_for", "title": "is_essential_for", "to": "image retrieval", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "co-saliency detection", "label": "is_essential_for", "title": "is_essential_for", "to": "image annotation", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "network", "label": "improves", "title": "improves", "to": "representation of co-salient objects", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "network", "label": "outputs", "title": "outputs", "to": "similarity value", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "network", "label": "achieves_results", "title": "achieves_results", "to": "state of the art", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "network", "label": "fine-tuned_on", "title": "fine-tuned_on", "to": "small target data set", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "network", "label": "trained_on", "title": "trained_on", "to": "large data set", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "neighbors", "label": "suppresses", "title": "suppresses", "to": "common background regions", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "co-salience scores", "label": "calculated_by", "title": "calculated_by", "to": "intra-image contrast", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "co-salience scores", "label": "calculated_by", "title": "calculated_by", "to": "intra-group consistency", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "window-level co-salience scores", "label": "converted_to", "title": "converted_to", "to": "superpixel-level co-salience maps", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "superpixel-level co-salience maps", "label": "generated_by", "title": "generated_by", "to": "foreground region agreement strategy", "width": 3.4000000000000004}, {"arrows": "to", "color": "#CC7700", "from": "Bayesian formulation", "label": "integrates", "title": "integrates", "to": "intra-image contrast", "width": 3.4899999999999998}, {"arrows": "to", "color": "#CC7700", "from": "Bayesian formulation", "label": "integrates", "title": "integrates", "to": "intra-group consistency", "width": 3.4899999999999998}, {"arrows": "to", "color": "#0077CC", "from": "l-level co-saliency maps", "label": "achieves", "title": "achieves", "to": "foreground region agreement", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "proposed approach", "label": "demonstrates", "title": "demonstrates", "to": "consistent performance gain", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "proposed approach", "label": "demonstrates_superiority_over", "title": "demonstrates_superiority_over", "to": "state-of-the-art hashing methods", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "proposed approach", "label": "outperforms", "title": "outperforms", "to": "current state of the art", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "proposed approach", "label": "achieves", "title": "achieves", "to": "robust object discovery", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "proposed approach", "label": "shows superiority over", "title": "shows superiority over", "to": "state-of-the-arts", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "proposed approach", "label": "achieves", "title": "achieves", "to": "state-of-the-art performance", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "proposed approach", "label": "operates on", "title": "operates on", "to": "fluorescence microscopy cell images", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "proposed approach", "label": "operates on", "title": "operates on", "to": "UCSD pedestrians", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "proposed approach", "label": "operates on", "title": "operates on", "to": "small animals", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "proposed approach", "label": "operates on", "title": "operates on", "to": "insects", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "proposed approach", "label": "is_effective", "title": "is_effective", "to": "Visual Object Tracking", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "proposed approach", "label": "compared_to", "title": "compared_to", "to": "state-of-the-art trackers", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "Co-salient object detection", "label": "addresses", "title": "addresses", "to": "multiple images", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Unified approach", "label": "employs", "title": "employs", "to": "low rank matrix recovery", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "iCoseg", "label": "provides", "title": "provides", "to": "intelligent scribble guidance", "width": 3.79}, {"arrows": "to", "color": "#CC7700", "from": "Co-salience detection", "label": "relates to", "title": "relates to", "to": "Convolutional Neural Networks (CNNs)", "width": 3.25}, {"arrows": "to", "color": "#CC7700", "from": "Co-salience detection", "label": "relates to", "title": "relates to", "to": "Image Group Consistency", "width": 3.0999999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Convolutional Neural Networks (CNNs)", "label": "applied_to", "title": "applied_to", "to": "images", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Convolutional Neural Networks (CNNs)", "label": "improves", "title": "improves", "to": "image recognition accuracy", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Convolutional Neural Networks (CNNs)", "label": "increasingly", "title": "increasingly", "to": "complex", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Convolutional Neural Networks (CNNs)", "label": "increasingly", "title": "increasingly", "to": "time-consuming", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "Convolutional Neural Networks (CNNs)", "label": "used_in", "title": "used_in", "to": "Pose Estimation", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "Convolutional Neural Networks (CNNs)", "label": "used_in", "title": "used_in", "to": "Object Detection", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Convolutional Neural Networks (CNNs)", "label": "analyzed_in", "title": "analyzed_in", "to": "Song_Joint_Multi-Feature_Spatial_2015_CVPR_paper", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Convolutional Neural Networks (CNNs)", "label": "used_in", "title": "used_in", "to": "Deep Convolutional Networks", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Convolutional Neural Networks (CNNs)", "label": "used in", "title": "used in", "to": "ImageNet classification", "width": 3.7600000000000002}, {"arrows": "to", "color": "#CC7700", "from": "Convolutional Neural Networks (CNNs)", "label": "uses", "title": "uses", "to": "deep learning", "width": 3.25}, {"arrows": "to", "color": "#CC7700", "from": "Bayesian Formulation", "label": "is used in", "title": "is used in", "to": "salient object detection", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "salient object detection", "label": "uses", "title": "uses", "to": "discriminative regional feature integration", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "Visual Attention", "label": "improves", "title": "improves", "to": "co-salience detection", "width": 3.34}, {"arrows": "to", "color": "#CC7700", "from": "Visual Attention", "label": "related_to", "title": "related_to", "to": "Binary Linear Integer Programming", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "alient object detection", "label": "presented_at", "title": "presented_at", "to": "CVPR (Conference)", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Xie, Y.", "label": "authored", "title": "authored", "to": "Bayesian salience", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Bayesian salience", "label": "utilizes", "title": "utilizes", "to": "low and mid level cues", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Han, J.", "label": "authored", "title": "authored", "to": "object-oriented visual salieny detection framework", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "object-oriented visual salieny detection framework", "label": "based_on", "title": "based_on", "to": "sparse coding representations", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "Rubinstein, M.", "label": "authored", "title": "authored", "to": "joint object discovery", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "joint object discovery", "label": "utilizes", "title": "utilizes", "to": "internet images", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Jiang, H.", "label": "authored", "title": "authored", "to": "salient object segmentation", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "salient object segmentation", "label": "incorporates", "title": "incorporates", "to": "context prior", "width": 3.79}, {"arrows": "to", "color": "#CC7700", "from": "IEEE Trans. Image Process.", "label": "is_journal_of", "title": "is_journal_of", "to": "Image Processing", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Image Processing", "label": "includes_applications", "title": "includes_applications", "to": "Stereo/Inpainting", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "is_author_of", "label": "writes", "title": "writes", "to": "Cong Zhang", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "is_author_of", "label": "writes", "title": "writes", "to": "Hongsheng Li", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "is_author_of", "label": "writes", "title": "writes", "to": "Xiaogang Wang", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "is_author_of", "label": "writes", "title": "writes", "to": "Xiaokang Yang", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Cong Zhang", "label": "author_of", "title": "author_of", "to": "Cross-Scene Crowd Counting", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Cong Zhang", "label": "affiliated_with", "title": "affiliated_with", "to": "Institute of Image Communication and Network Engineering, Shanghai Jiao Tong University", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Hongsheng Li", "label": "affiliated_with", "title": "affiliated_with", "to": "Department of Electronic Engineering, The Chinese University of Hong Kong", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Hongsheng Li", "label": "authored", "title": "authored", "to": "Saliency Detection by Multi-Context Deep Learning", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Hongsheng Li", "label": "affiliated_with", "title": "affiliated_with", "to": "Department of Electronic Engineering", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Hongsheng Li", "label": "email", "title": "email", "to": "hsli@ee.cuhk.edu.hk", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Xiaogang Wang", "label": "author_of", "title": "author_of", "to": "Cross-Scene Crowd Counting", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Xiaogang Wang", "label": "affiliated_with", "title": "affiliated_with", "to": "Department of Electronic Engineering, The Chinese University of Hong Kong", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Xiaogang Wang", "label": "authored", "title": "authored", "to": "Saliency Detection by Multi-Content Deep Learning", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Xiaogang Wang", "label": "affiliated_with", "title": "affiliated_with", "to": "Shenzhen Institutes of Advanced Technology", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Xiaogang Wang", "label": "author_of", "title": "author_of", "to": "Deeply Learned Attributes", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Xiaokang Yang", "label": "author_of", "title": "author_of", "to": "Cross-Scene Crowd Counting", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Xiaokang Yang", "label": "affiliated_with", "title": "affiliated_with", "to": "Institute of Image Communication and Network Engineering, Shanghai Jiao Tong University", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Xiaokang Yang", "label": "affiliated_with", "title": "affiliated_with", "to": "Chinese University of Hong Kong", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Xiaokang Yang", "label": "email", "title": "email", "to": "xgwang@ee.cuhk.edu.hk", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Xiaokang Yang", "label": "email", "title": "email", "to": "xk yang@sjtu.edu.cn", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Xiaokang Yang", "label": "is_author_of", "title": "is_author_of", "to": "Motion Part Regularization", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Automatic salient object segmentation", "label": "presented_at", "title": "presented_at", "to": "BMVC", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Self-Adaptively Weighted Co-Saliency Detection", "label": "published_in", "title": "published_in", "to": "IEEE Trans. Image Process.", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Cross-Scene Crowd Counting", "label": "publication_venue", "title": "publication_venue", "to": "CVPR", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Cross-Scene Crowd Counting", "label": "year", "title": "year", "to": "2015", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "Cross-Scene Crowd Counting", "label": "method", "title": "method", "to": "Deep Convolutional Neural Networks", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Zhang_Cross-Scene_Crowd_Counting_2015_CVPR_paper.pdf", "label": "document_of", "title": "document_of", "to": "Cross-Scene Crowd Counting", "width": 3.91}, {"arrows": "to", "color": "#CC7700", "from": "Cross-scene crowd counting", "label": "is_task", "title": "is_task", "to": "crowd counting", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Cross-scene crowd counting", "label": "requires", "title": "requires", "to": "no laborious data annotation", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "existing crowd counting methods", "label": "experiences", "title": "experiences", "to": "significant performance drop", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "deep convolutional neural network (CNN)", "label": "is_used_for", "title": "is_used_for", "to": "crowd counting", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "deep convolutional neural network (CNN)", "label": "is_trained_with", "title": "is_trained_with", "to": "crowd density", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "deep convolutional neural network (CNN)", "label": "improves", "title": "improves", "to": "object detection benchmarks", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "switchable learning approach", "label": "aims_to_achieve", "title": "aims_to_achieve", "to": "better local optimum", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "data-driven method", "label": "is_used_to", "title": "is_used_to", "to": "fine-tune CNN model", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "CNN model", "label": "is", "title": "is", "to": "trained", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "Crowd Counting", "label": "requires", "title": "requires", "to": "Deep Convolutional Neural Networks (CNNs)", "width": 3.7600000000000002}, {"arrows": "to", "color": "#CC7700", "from": "Crowd Counting", "label": "addresses", "title": "addresses", "to": "object counting", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Crowd Counting", "label": "addressed_by", "title": "addressed_by", "to": "Chen et al. (2013)", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "Crowd Counting", "label": "is_topic", "title": "is_topic", "to": "Computer Vision", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "Deep Convolutional Neural Networks (CNNs)", "label": "used_in", "title": "used_in", "to": "Salient Object Detection", "width": 3.25}, {"arrows": "to", "color": "#0077CC", "from": "Deep Convolutional Neural Networks (CNNs)", "label": "is_used_in", "title": "is_used_in", "to": "Object Detection", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "Deep Convolutional Neural Networks (CNNs)", "label": "related_to", "title": "related_to", "to": "Dimensionality Reduction", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Chen et al. (2013)", "label": "introduces", "title": "introduces", "to": "cumulative attribute space", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Chen et al. (2013)", "label": "evaluates", "title": "evaluates", "to": "cross-scene crowd counting methods", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "cumulative attribute space", "label": "relevant_to", "title": "relevant_to", "to": "crowd density estimation", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Lempitsky \u0026 Zisserman (2010)", "label": "presents", "title": "presents", "to": "object counting", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "Chen et al. (2012)", "label": "focuses_on", "title": "focuses_on", "to": "feature mining", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "feature mining", "label": "applied_to", "title": "applied_to", "to": "localized crowd counting", "width": 3.79}, {"arrows": "to", "color": "#CC7700", "from": "localized crowd counting", "label": "is_aspect_of", "title": "is_aspect_of", "to": "research", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Loy et al. (2012) research", "label": "focuses_on", "title": "focuses_on", "to": "localized crowd counting", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "An et al. (2007) research", "label": "demonstrates", "title": "demonstrates", "to": "kernel ridge regression", "width": 3.7600000000000002}, {"arrows": "to", "color": "#CC7700", "from": "kernel ridge regression", "label": "is_technique", "title": "is_technique", "to": "vision tasks", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Kai et al. (2014) research", "label": "introduces", "title": "introduces", "to": "fully convolutional network", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "fully convolutional network", "label": "is_used_for", "title": "is_used_for", "to": "crowd segmentation", "width": 3.88}, {"arrows": "to", "color": "#CC7700", "from": "fully convolutional network", "label": "is_a", "title": "is_a", "to": "neural network", "width": 3.73}, {"arrows": "to", "color": "#CC7700", "from": "crowd segmentation", "label": "is_field_of", "title": "is_field_of", "to": "computer vision", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Kong et al. (2006) research", "label": "addresses", "title": "addresses", "to": "viewpoint invariance", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "Kong et al. (2006)", "label": "addresses", "title": "addresses", "to": "viewpoint invariance in crowd counting", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "viewpoint invariance in crowd counting", "label": "is_a", "title": "is_a", "to": "practical consideration", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Jing et al. (2015)", "label": "explores", "title": "explores", "to": "deep learning for attribute extraction", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "attribute extraction", "label": "contributes_to", "title": "contributes_to", "to": "crowd scene understanding", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Fiaschi et al. (2012)", "label": "investigates", "title": "investigates", "to": "regression forest for counting", "width": 3.91}, {"arrows": "to", "color": "#CC7700", "from": "regression forest for counting", "label": "is_alternative_to", "title": "is_alternative_to", "to": "neural networks", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "neural networks", "label": "used_for", "title": "used_for", "to": "dimensionality reduction", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Loy et al. (2013)", "label": "explores", "title": "explores", "to": "semi-supervised learning for crowd counting", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Loy et al. (2013)", "label": "explores", "title": "explores", "to": "transfer learning for crowd counting", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "ICPR", "label": "is_a", "title": "is_a", "to": "conference", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "ICCV", "label": "is_a", "title": "is_a", "to": "conference", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "ICCV", "label": "published", "title": "published", "to": "Class-specific material categorisation", "width": 3.91}, {"arrows": "to", "color": "#CC7700", "from": "ICCV", "label": "is_a", "title": "is_a", "to": "Computer Vision Conference", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "ICCV", "label": "published", "title": "published", "to": "1841\u20131848", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "ICCV", "label": "publication_venue", "title": "publication_venue", "to": "Ground truth dataset", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Gong, S.", "label": "authored", "title": "authored", "to": "From semi-supervised to transfer counting of crowds", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Xiang, T.", "label": "authored", "title": "authored", "to": "From semi-supervised to transfer counting of crowds", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Lowe, D. G.", "label": "authored", "title": "authored", "to": "Distinctive image features from scale-invariant keypoints", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Distinctive image features from scale-invariant keypoints", "label": "used_for", "title": "used_for", "to": "feature extraction", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "SIFT features", "label": "introduced_in", "title": "introduced_in", "to": "Distinctive image features from scale-invariant keypoints", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "SIFT features", "label": "is_used_in", "title": "is_used_in", "to": "crowd analysis", "width": 3.7600000000000002}, {"arrows": "to", "color": "#CC7700", "from": "SIFT features", "label": "is_building_block_for", "title": "is_building_block_for", "to": "image representation techniques", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "SIFT features", "label": "is_a", "title": "is_a", "to": "keypoints", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "SIFT features", "label": "described_in", "title": "described_in", "to": "IJCV, 60:91\u2013110, 2004", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "SIFT features", "label": "is_component_of", "title": "is_component_of", "to": "image processing tasks", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "SIFT features", "label": "is a", "title": "is a", "to": "key component", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "SIFT features", "label": "is_a", "title": "is_a", "to": "distinctive image features", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "SIFT features", "label": "used for", "title": "used for", "to": "feature detection", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "SIFT features", "label": "used for", "title": "used for", "to": "feature matching", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Yongzhen Huang", "label": "author_of", "title": "author_of", "to": "Deep SemanticRanking Based Hashing", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Yongzhen Huang", "label": "affiliated_with", "title": "affiliated_with", "to": "Center for Research on Intelligent Perception and Computing", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Liang Wang", "label": "author_of", "title": "author_of", "to": "Deep Semantic Ranking Based Hashing", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Liang Wang", "label": "affiliated_with", "title": "affiliated_with", "to": "Center for Research on Intelligent Perception and Computing", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Deep Semantic Ranking Based Hashing", "label": "is_paper_in", "title": "is_paper_in", "to": "CVPR", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "Deep Semantic Ranking Based Hashing", "label": "addresses", "title": "addresses", "to": "Multi-Label Image Retrieval", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Tieniu Tan", "label": "author_of", "title": "author_of", "to": "Deep Semantic Ranking Based Hashing", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Tieniu Tan", "label": "affiliated_with", "title": "affiliated_with", "to": "Center for Research on Intelligent Perception and Computing", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Institute of Image Communication and Network Engineering", "label": "affiliated_with", "title": "affiliated_with", "to": "Shanghai Jiao Tong University", "width": 3.88}, {"arrows": "to", "color": "#CC7700", "from": "deep hash functions", "label": "overcomes_limitation_of", "title": "overcomes_limitation_of", "to": "hand-crafted features", "width": 3.7600000000000002}, {"arrows": "to", "color": "#00CC77", "from": "deep hash functions", "label": "learns_from", "title": "learns_from", "to": "ranking list", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "ranking list", "label": "encodes", "title": "encodes", "to": "multilevel similarity information", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "multilevel similarity information", "label": "is_encoded_by", "title": "is_encoded_by", "to": "ranking list", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "semantic representation", "label": "is_limited_by", "title": "is_limited_by", "to": "hand-crafted features", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "hash codes", "label": "derived_from", "title": "derived_from", "to": "semantic representation", "width": 3.25}, {"arrows": "to", "color": "#CC7700", "from": "Deep Hash Functions", "label": "guided_by", "title": "guided_by", "to": "Similarity Information", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Proposed Approach", "label": "compared_to", "title": "compared_to", "to": "Hashing Methods", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "ImageNet Classification", "label": "uses", "title": "uses", "to": "Deep Convolutional Neural Networks", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Deep Convolutional Ranking", "label": "addresses", "title": "addresses", "to": "Multi-label Image Annotation", "width": 3.79}, {"arrows": "to", "color": "#CC7700", "from": "Iterative Quantization", "label": "aims_to_learn", "title": "aims_to_learn", "to": "Binary Codes", "width": 3.73}, {"arrows": "to", "color": "#0077CC", "from": "Binary Codes", "label": "used_for", "title": "used_for", "to": "Image Retrieval", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "Image Retrieval", "label": "uses", "title": "uses", "to": "Hashing", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Krizhevsky", "label": "authors", "title": "authors", "to": "ImageNet Classification", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Krizhevsky", "label": "co_authors_with", "title": "co_authors_with", "to": "Hinton", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Krizhevsky", "label": "authored", "title": "authored", "to": "Imaginet classification with deep convolutional neural networks", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Gong", "label": "authored", "title": "authored", "to": "Deep Convolutional Ranking", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Gong", "label": "authored", "title": "authored", "to": "Iterative Quantization", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Gong", "label": "authored", "title": "authored", "to": "Deep convolutional ranking", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Gong", "label": "authored", "title": "authored", "to": "A Maximum Entropy Feature Descriptor", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Gong", "label": "authored", "title": "authored", "to": "sequential subset selection", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Perronnin", "label": "authored", "title": "authored", "to": "Iterative quantization", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "Iterative quantization", "label": "applies", "title": "applies", "to": "binary codes", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "binary codes", "label": "distribute evenly", "title": "distribute evenly", "to": "each bit", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Norouzi", "label": "authored", "title": "authored", "to": "Hamming distance metric learning", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Norouzi", "label": "authored", "title": "authored", "to": "Minimal loss hashing", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Hamming distance metric learning", "label": "presented_in", "title": "presented_in", "to": "Advances in Neural Information Processing Systems", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Semi-supervised hashing", "label": "aims_for", "title": "aims_for", "to": "scalable image retrieval", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Semi-supervised hashing", "label": "is_presented_in", "title": "is_presented_in", "to": "*CVPR*", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Semi-supervised hashing", "label": "explores", "title": "explores", "to": "techniques", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Torralba", "label": "authored", "title": "authored", "to": "Small codes", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Torralba", "label": "authored", "title": "authored", "to": "Learning to Predict Where Humans Look", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Small codes", "label": "used_for", "title": "used_for", "to": "image databases", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Deep convolutional ranking", "label": "addresses", "title": "addresses", "to": "multilabel image annotation", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Deep convolutional ranking", "label": "is_application_of", "title": "is_application_of", "to": "multilable image annotation", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Gong, Y.", "label": "authored", "title": "authored", "to": "Deep convolutional ranking", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Krizhevsky, A.", "label": "authored", "title": "authored", "to": "One weird trick", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Krizhevsky, A.", "label": "authored", "title": "authored", "to": "ImageNet classification with deep convolutional neural networks", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Krizhevsky, A.", "label": "authors", "title": "authors", "to": "ImageNet Classification", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Krizhevsky, A.", "label": "authored", "title": "authored", "to": "ImageNet classification", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Krizhevsky, A.", "label": "co_author_of", "title": "co_author_of", "to": "Sutskever, I.", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Krizhevsky, A.", "label": "co_author_of", "title": "co_author_of", "to": "Hinton, G. E.", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "One weird trick", "label": "describes", "title": "describes", "to": "parallelizing convolutional neural networks", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Minimal loss hashing", "label": "introduced", "title": "introduced", "to": "compact binary codes", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "compact binary codes", "label": "minimizes", "title": "minimizes", "to": "loss", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Lin, G.", "label": "authored", "title": "authored", "to": "Optimizing ranking measures", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Optimizing ranking measures", "label": "focuses_on", "title": "focuses_on", "to": "compact binary code learning", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Fang Zhao", "label": "is_affiliated_with", "title": "is_affiliated_with", "to": "Center for Research on Intelligent Perception and Computing", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Center for Research on Intelligent Perception and Computing", "label": "is_part_of", "title": "is_part_of", "to": "Institute of Automation", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Institute of Automation", "label": "is_part_of", "title": "is_part_of", "to": "Chinese Academy of Sciences", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Institute of Automation", "label": "houses", "title": "houses", "to": "Center for Biometrics and Security Research", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Institute of Automation", "label": "houses", "title": "houses", "to": "National Laboratory of Pattern Recognition", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "David Perra", "label": "is_author_of", "title": "is_author_of", "to": "Adaptive Eye-Camera Calibration", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "David Perra", "label": "affiliated_with", "title": "affiliated_with", "to": "Google Inc.", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Adaptive Eye-Camera Calibration", "label": "is_paper", "title": "is_paper", "to": "cvpr_papers", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Rohit Kumar Gupta", "label": "is_author_of", "title": "is_author_of", "to": "Adaptive Eye-Camera Calibration", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Rohit Kumar Gupta", "label": "affiliated_with", "title": "affiliated_with", "to": "The University of North Carolina at Chapel Hill", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Rohit Kumar Gupta", "label": "email", "title": "email", "to": "rkgupta@cs.unc.edu", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "fang.zhao@nlpr.ia.ac.cn", "label": "affiliated_with", "title": "affiliated_with", "to": "Center for Research on Intelligent Perception and Computing", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Perra_Adaptive_Eye-Camera_Calibration_2015_CVPR_paper.pdf", "label": "is_file", "title": "is_file", "to": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "calibration scheme", "label": "solves_for", "title": "solves_for", "to": "globally optimal model", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "calibration scheme", "label": "addresses", "title": "addresses", "to": "changes in calibration", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "calibration scheme", "label": "calculates", "title": "calculates", "to": "locally optimal eye-device transformation", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "calibration scheme", "label": "computes_from", "title": "computes_from", "to": "local window of previous frames", "width": 3.82}, {"arrows": "to", "color": "#CC7700", "from": "calibration scheme", "label": "is", "title": "is", "to": "continuous", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "calibration scheme", "label": "is", "title": "is", "to": "locally optimal", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "calibration scheme", "label": "outperforms", "title": "outperforms", "to": "state of the art systems", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "calibration scheme", "label": "is less restrictive to", "title": "is less restrictive to", "to": "environment", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "calibration schemes", "label": "performed_on", "title": "performed_on", "to": "per-user basis", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "interest regions", "label": "located within", "title": "located within", "to": "user\u2019s environment", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "eye-device transformation", "label": "is", "title": "is", "to": "locally optimal", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "proposed calibration scheme", "label": "outperforms", "title": "outperforms", "to": "existing state of the art systems", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "proposed calibration scheme", "label": "is", "title": "is", "to": "less restrictive", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "proposed calibration scheme", "label": "is less restrictive to", "title": "is less restrictive to", "to": "environment", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Alnajar et al.", "label": "published", "title": "published", "to": "Calibration-free gaze estimation", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Calibration-free gaze estimation", "label": "uses", "title": "uses", "to": "human gaze patterns", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Chen and Ji", "label": "published", "title": "published", "to": "Probabilistic gaze estimation", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Probabilistic gaze estimation", "label": "avoids", "title": "avoids", "to": "active personal calibration", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Corno et al.", "label": "published", "title": "published", "to": "cost-effective solution", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "cost-effective solution", "label": "is for", "title": "is for", "to": "eye-gaze assistive technology", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "eye-camera calibration", "label": "is related to", "title": "is related to", "to": "gaze tracking", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "F. Corno", "label": "presented_at", "title": "presented_at", "to": "IEEE International Conference on Multimedia and Expo", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "IEEE International Conference on Multimedia and Expo", "label": "hosts", "title": "hosts", "to": "research on assistive technology", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "E. Guestrin", "label": "researched", "title": "researched", "to": "remote gaze estimation", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "remote gaze estimation", "label": "uses", "title": "uses", "to": "pupil center", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "D. Hansen", "label": "surveyed", "title": "surveyed", "to": "models for eyes and gaze", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "J. Harel", "label": "developed", "title": "developed", "to": "graph-based visual saliency", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "J. Harel", "label": "authored", "title": "authored", "to": "Graph-based visual salience", "width": 3.91}, {"arrows": "to", "color": "#CC7700", "from": "graph-based visual saliency", "label": "is_a", "title": "is_a", "to": "neural processing method", "width": 3.25}, {"arrows": "to", "color": "#0077CC", "from": "X. Hou", "label": "researched", "title": "researched", "to": "image signature", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "X. Hou", "label": "published_in", "title": "published_in", "to": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "image signature", "label": "highlights", "title": "highlights", "to": "sparse salient regions", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "IEEE Transactions on Biomedical Engineering", "label": "publishes", "title": "publishes", "to": "research on remote gaze estimation", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "U. Lahiri", "label": "published_in", "title": "published_in", "to": "Neural Systems and Rehabilitation Engineering, IEEE Transactions on", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "U. Lahiri", "label": "published_in", "title": "published_in", "to": "Virtual Rehabilitation (ICVR)", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "U. Lahiri", "label": "developed_system_for", "title": "developed_system_for", "to": "children with autism", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "U. Lahiri", "label": "developed_system_for", "title": "developed_system_for", "to": "social communication", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "R. Kumar", "label": "published_in", "title": "published_in", "to": "Computer Vision and Pattern Recognition", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Jan-Micheal Frahm", "label": "affiliated_with", "title": "affiliated_with", "to": "The University of North Carolian at Chapel Hill", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Jan-Micheal Frahm", "label": "affiliated_with", "title": "affiliated_with", "to": "The University of North Carolina at Chapel Hill", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Jan-Micheal Frahm", "label": "email", "title": "email", "to": "jmf@cs.unc.edu", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Nianuan Jiang", "label": "author_of", "title": "author_of", "to": "Direct Structure Estimation for 3D Reconstruction", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Nianuan Jiang", "label": "affiliated_with", "title": "affiliated_with", "to": "Advanced Digital Sciences Center", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Nianuan Jiang", "label": "affiliated_with", "title": "affiliated_with", "to": "Advanced Digital Sciences Center, Singapore", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Direct Structure Estimation for 3D Reconstruction", "label": "published_in", "title": "published_in", "to": "CVPR", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Wen-Yan Lin", "label": "author_of", "title": "author_of", "to": "Direct Structure Estimated for 3D Reconstruction", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Wen-Yan Lin", "label": "affiliated_with", "title": "affiliated_with", "to": "Advanced Digital Sciences Center", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Wen-Yan Lin", "label": "affiliated_with", "title": "affiliated_with", "to": "Advanced Digital Sciences Center, Singapore", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Minh N. Do", "label": "author_of", "title": "author_of", "to": "Direct Structure Estimation for 3D Reconstruction", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Minh N. Do", "label": "affiliated_with", "title": "affiliated_with", "to": "Advanced Digital Sciences Center", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Minh N. Do", "label": "affiliated_with", "title": "affiliated_with", "to": "University of Illinois at Urbana-Champaign", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Jiangbo Lu", "label": "author_of", "title": "author_of", "to": "Direct Structure Estimation for 3D Reconstruction", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Jiangbo Lu", "label": "affiliated_with", "title": "affiliated_with", "to": "Advanced Digital Sciences Center, Singapore", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Jiang_Direct_Structure_Estimation_2015_CVPR_paper.pdf", "label": "documents", "title": "documents", "to": "Direct Structure Estimation for 3D Reconstruction", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Structure from Motion (SFM)", "label": "requires", "title": "requires", "to": "camera pose estimation", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "Structure from Motion (SFM)", "label": "incorporates", "title": "incorporates", "to": "Euclidean Rigidity", "width": 3.4000000000000004}, {"arrows": "to", "color": "#CC7700", "from": "Structure from Motion (SFM)", "label": "solves", "title": "solves", "to": "scene reconstruction", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "Structure from Motion (SFM)", "label": "relies on", "title": "relies on", "to": "Homography Estimation", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "camera pose estimation", "label": "is_part_of", "title": "is_part_of", "to": "visual SLAM", "width": 3.79}, {"arrows": "to", "color": "#CC7700", "from": "Euclidean Rigidity", "label": "enables", "title": "enables", "to": "scene structure recovery", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "Direct StructureEstimation (DSE)", "label": "combines with", "title": "combines with", "to": "homography estimation", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Direct Structure Estimation (DSE)", "label": "provides", "title": "provides", "to": "formulation for scene structure recovery", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Direct Structure Estimation (DSE)", "label": "works well for", "title": "works well for", "to": "recovering scene structure", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Direct Structure Estimation (DSE)", "label": "works well for", "title": "works well for", "to": "recovering camera poses", "width": 3.7600000000000002}, {"arrows": "to", "color": "#CC7700", "from": "Direct Structure Estimation (DSE)", "label": "is a method in", "title": "is a method in", "to": "Structure from Motion (SFM)", "width": 3.25}, {"arrows": "to", "color": "#0077CC", "from": "scene structure", "label": "is recovered from", "title": "is recovered from", "to": "sideway motion", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "scene structure", "label": "occurs in", "title": "occurs in", "to": "planar or general man-made scenes", "width": 3.64}, {"arrows": "to", "color": "#CC7700", "from": "Camera Pose Estimation", "label": "is part of", "title": "is part of", "to": "Structure from Motion (SFM)", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "Camera Pose Estimation", "label": "is_related_to", "title": "is_related_to", "to": "Non-Rigid Structure from Motion", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "D. Nist\u00e9r", "label": "authored", "title": "authored", "to": "solution to five-point relative pose problem", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "D. G. Aliaga", "label": "authored", "title": "authored", "to": "simplifying reconstruction of 3d models", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "K. S. Arun", "label": "authored", "title": "authored", "to": "least-squares fitting of two 3-d point sets", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "D. Crandall", "label": "authored", "title": "authored", "to": "discrete-continuous optimization", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "D. W. Eggert", "label": "authored", "title": "authored", "to": "comparison of four major algorithms", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "D. W. Eggert", "label": "published", "title": "published", "to": "Estimating 3-d rigid body transformations", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "least-squares fitting", "label": "is a technique in", "title": "is a technique in", "to": "3d point set alignment", "width": 3.0999999999999996}, {"arrows": "to", "color": "#0077CC", "from": "M. A. Fischler", "label": "author of", "title": "author of", "to": "Random sample consensus", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Random sample consensus", "label": "published in", "title": "published in", "to": "Communications of the ACM", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "Random sample consensus", "label": "is_paradigm_for", "title": "is_paradigm_for", "to": "model fitting", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Random sample consensus", "label": "applies_to", "title": "applies_to", "to": "image analysis", "width": 3.61}, {"arrows": "to", "color": "#0077CC", "from": "Advanced Digital Sciences Center", "label": "located_in", "title": "located_in", "to": "Singapore", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Structure from motion", "label": "presented_at", "title": "presented_at", "to": "CVPR", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "R. Hartley", "label": "published", "title": "published", "to": "In defense of the eight-point algorithm", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "H. Isack", "label": "co_authored", "title": "co_authored", "to": "Energy-based geometric multi-model fitting", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "N. Jiang", "label": "co_authored", "title": "co_authored", "to": "A global linear method for camera pose registration", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Stefan Roth", "label": "author_of", "title": "author_of", "to": "Richter_Discriminative_Shape_From_2015_CVPR_supplemental", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Stefan Roth", "label": "affiliated_with", "title": "affiliated_with", "to": "Department of Computer Science, TU Darmstadt", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Stefan Roth", "label": "works_at", "title": "works_at", "to": "TU Darmstadt", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Stefan Roth", "label": "affiliated_with", "title": "affiliated_with", "to": "Adobe Research", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Stefan Roth", "label": "authored", "title": "authored", "to": "Discriminaitve Shape from Shading", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Stefan Roth", "label": "affiliated_with", "title": "affiliated_with", "to": "Department of Computer Science", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Richter_Discriminative_Shape_From_2015_CVPR_supplemental", "label": "publication_venue", "title": "publication_venue", "to": "CVPR", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Richter_Discriminative_Shape_From_2015_CVPR_supplemental", "label": "publication_year", "title": "publication_year", "to": "2015", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "Richter_Discriminative_Shape_From_2015_CVPR_supplemental", "label": "describes", "title": "describes", "to": "Discriminative Shape from Shading", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "Shape from shading method", "label": "is_compared_to", "title": "is_compared_to", "to": "other approaches", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "Shape from shading method", "label": "combines", "title": "combines", "to": "local context", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Shape from shading method", "label": "uses", "title": "uses", "to": "learning framework", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Shape from shading method", "label": "achieves", "title": "achieves", "to": "improved reconstructions", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Shape from shading method", "label": "relies_on", "title": "relies_on", "to": "smooth local context", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Results", "label": "presented_on", "title": "presented_on", "to": "real images", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "Dataset", "label": "is", "title": "is", "to": "ground truth dataset", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Dataset", "label": "contains", "title": "contains", "to": "real objects", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Dataset", "label": "consists_of", "title": "consists_of", "to": "Real Objects", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "Shape from Shading", "label": "related_to", "title": "related_to", "to": "Surface Reconstruction", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "Shape from Shading", "label": "related_to", "title": "related_to", "to": "Illumination Estimation", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "Shape from Shading", "label": "incorporates", "title": "incorporates", "to": "Local and Global Context", "width": 3.4000000000000004}, {"arrows": "to", "color": "#CC7700", "from": "Shape from Shading", "label": "utilizes", "title": "utilizes", "to": "Machine Learning", "width": 3.25}, {"arrows": "to", "color": "#CC7700", "from": "Shape from Shading", "label": "is_topic", "title": "is_topic", "to": "research area", "width": 3.7600000000000002}, {"arrows": "to", "color": "#CC7700", "from": "Surface Reconstruction", "label": "is_topic_of", "title": "is_topic_of", "to": "Zhang_Light_Field_From_2015_CVPR_paper", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Surface Reconstruction", "label": "requires", "title": "requires", "to": "RGB-D Cameras", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Barron, J. T., \u0026 Malik, J.", "label": "contributes_to", "title": "contributes_to", "to": "Color Constancy", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Johnson, M. K., \u0026 Adelson, E. H.", "label": "addresses", "title": "addresses", "to": "Shape Estimation", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Johnson, M. K., \u0026 Adelson, E. H.", "label": "addresses", "title": "addresses", "to": "Natural Illumination", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "Shape Estimation", "label": "addressed in", "title": "addressed in", "to": "Johnson \u0026 Adelson (2011) paper", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Jacobs, D. W.", "label": "appears_in_author_list_for", "title": "appears_in_author_list_for", "to": "reference [4]", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Basri, R.", "label": "appears_in_author_list_for", "title": "appears_in_author_list_for", "to": "reference [4]", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Stephan R. Richter", "label": "affiliated_with", "title": "affiliated_with", "to": "Department of Computer Science, TU Darmstadt", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Stephan R. Richter", "label": "works_at", "title": "works_at", "to": "TU Darmstadt", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Stephan R. Richter", "label": "affiliated_with", "title": "affiliated_with", "to": "Adobe Research", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Stephan R. Richter", "label": "authored", "title": "authored", "to": "Discriminaitve Shape from Shading", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Stephan R. Richter", "label": "affiliated_with", "title": "affiliated_with", "to": "Department of Computer Science", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Department of Computer Science, TU Darmstadt", "label": "is_affiliation_of", "title": "is_affiliation_of", "to": "Stephan R. Richter", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Department of Computer Science, TU Darmstadt", "label": "is_affiliation_of", "title": "is_affiliation_of", "to": "Stefan Roth", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "TU Darmstadt", "label": "hosts_department", "title": "hosts_department", "to": "Department of Computer Science", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Andr\u00e1s B\u00f3dis-Sz\u0151m\u0151ru", "label": "authors", "title": "authors", "to": "Superpixel Meshes", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Superpixel Meshes", "label": "publication_venue", "title": "publication_venue", "to": "CVPR", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Superpixel Meshes", "label": "year", "title": "year", "to": "2015", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "Superpixel Meshes", "label": "describes", "title": "describes", "to": "surface reconstruction", "width": 3.7600000000000002}, {"arrows": "to", "color": "#CC7700", "from": "Superpixel Meshes", "label": "preserves", "title": "preserves", "to": "edges", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Hayko Riemenschneider", "label": "authors", "title": "authors", "to": "Superpixel Meses", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Hayko Riemenschneider", "label": "affiliated_with", "title": "affiliated_with", "to": "ETH Zurich, Computer Vision Lab", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Luc Van Gool", "label": "authors", "title": "authors", "to": "Superpixel Meshes", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Luc Van Gool", "label": "affiliated_with", "title": "affiliated_with", "to": "PSI-VISICS, KU Leuven", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Luc Van Gool", "label": "author_of", "title": "author_of", "to": "Metric imitation by manifold transfer", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Luc Van Gool", "label": "affiliated_with", "title": "affiliated_with", "to": "VISICS, ESAT/PSI, KU Leuven", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Luc Van Gool", "label": "affiliated_with", "title": "affiliated_with", "to": "Computer Vision Lab, ETH Zurich", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Luc Van Gool", "label": "authored", "title": "authored", "to": "Privacy Preserving Optics for Miniature Vision Sensors", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "Bodis-Szomoru_Superpixel_Meshes_for_2015_CVPR_supplemental.pdf", "label": "contains", "title": "contains", "to": "Superpixel Meshes", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Multi-View-Stereo methods", "label": "aim_for", "title": "aim_for", "to": "highest detail", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "surface reconstruction method", "label": "is_based_on", "title": "is_based_on", "to": "image edges", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "surface reconstruction method", "label": "is_constrained_by", "title": "is_constrained_by", "to": "second-order smoothness constraints", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "meshes", "label": "have_quality", "title": "have_quality", "to": "classic MVS surfaces", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "meshes", "label": "are", "title": "are", "to": "edge-aligned", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "meshes", "label": "are", "title": "are", "to": "compact", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "meshes", "label": "aligned_with", "title": "aligned_with", "to": "image gradients", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "dense depth optimization", "label": "occurs_over", "title": "occurs_over", "to": "Ground Control Points", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "SfM points", "label": "used_as", "title": "used_as", "to": "GCPs", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "LiDAR", "label": "used_as", "title": "used_as", "to": "GCPs", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "RGB-D", "label": "can_be_used_as", "title": "can_be_used_as", "to": "GCPs", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Structure-from-Motion (SfM) points", "label": "used_as", "title": "used_as", "to": "GCPs", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "renderings", "label": "are", "title": "are", "to": "lightweight", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "renderings", "label": "are", "title": "are", "to": "per-face flat", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Zhang_Light_Field_From_2015_CVPR_paper", "label": "authored_by", "title": "authored_by", "to": "Didyk et.al", "width": 3.8499999999999996}, {"arrows": "to", "color": "#00CC77", "from": "Zhang_Light_Field_From_2015_CVPR_paper", "label": "improves", "title": "improves", "to": "surface quality", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "Superpixels", "label": "is_topic_of", "title": "is_topic_of", "to": "Zhang_Light_Light_Field_From_2015_CVPR_paper", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "Structure-from-Motion (SfM)", "label": "is_topic_of", "title": "is_topic_of", "to": "Zhang_Light_Field_From_2015_CVPR_paper", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "Mesh Generation", "label": "is_topic_of", "title": "is_topic_of", "to": "Zhang_Light_Field_From_2015_CVPR_paper", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "Edge-Preerving Methods", "label": "is_topic_of", "title": "is_topic_of", "to": "Zhang_Light_Field_From_2015_CVPR_paper", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Andr\u00b4as B\u00b4odis-Szomor\u00b4u", "label": "affiliated_with", "title": "affiliated_with", "to": "ETH Zurich, Computer Vision Lab", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "intermediate views", "label": "uses", "title": "uses", "to": "light field synthesis", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Depth Estimation", "label": "improves", "title": "improves", "to": "existing methods", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "existing methods", "label": "use", "title": "use", "to": "pixel-coordinates", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "existing methods", "label": "has_issue", "title": "has_issue", "to": "failure to enforce consistency constraints", "width": 3.7600000000000002}, {"arrows": "to", "color": "#CC7700", "from": "View Synthesis", "label": "is_a", "title": "is_a", "to": "technique", "width": 3.25}, {"arrows": "to", "color": "#CC7700", "from": "Light Field Reconstruction", "label": "is_a", "title": "is_a", "to": "technique", "width": 3.25}, {"arrows": "to", "color": "#CC7700", "from": "Disparity Refinement", "label": "is_a", "title": "is_a", "to": "technique", "width": 3.25}, {"arrows": "to", "color": "#CC7700", "from": "Iterative View Generation", "label": "is_a", "title": "is_a", "to": "technique", "width": 3.25}, {"arrows": "to", "color": "#0077CC", "from": "Jan Hosang", "label": "is_author_of", "title": "is_author_of", "to": "Taking a Deeper Look at Pedestrians", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Taking a Deeper Look at Pedestrians", "label": "is_paper", "title": "is_paper", "to": "CVPR", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "Mohamed Omran", "label": "is_author_of", "title": "is_author_of", "to": "Taking a Deeper Look at Pedestrians", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Mohamed Omran", "label": "affiliated_with", "title": "affiliated_with", "to": "Max Planck Institute for Informatics", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "CifarNet", "label": "used_in", "title": "used_in", "to": "pedestrian detection", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "AlexNet", "label": "used_in", "title": "used_in", "to": "pedestrian detection", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "AlexNet", "label": "introduced_in", "title": "introduced_in", "to": "Krizhevsky, A., Sutskever, I., and Hinton, G. E.", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "AlexNet", "label": "precedes", "title": "precedes", "to": "re-identification models", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "AlexNet", "label": "is_example_of", "title": "is_example_of", "to": "deep learning", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "AlexNet", "label": "is_a", "title": "is_a", "to": "deep convolutional neural network", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "AlexNet", "label": "achieved", "title": "achieved", "to": "breakthrough performance", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "AlexNet", "label": "influences", "title": "influences", "to": "research in intrinsic image decomposition", "width": 3.64}, {"arrows": "to", "color": "#00CC77", "from": "AlexNet", "label": "influences", "title": "influences", "to": "subsequent research", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "filter size", "label": "affects", "title": "affects", "to": "performance", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "layer width", "label": "affects", "title": "affects", "to": "performance", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "learning rate policies", "label": "affects", "title": "affects", "to": "performance", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "pedestrian heights", "label": "distributed_in", "title": "distributed_in", "to": "datasets", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Calttech dataset", "label": "contains", "title": "contains", "to": "pedestrian heights", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "KITTI dataset", "label": "contains", "title": "contains", "to": "pedestrian heights", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "KITTI dataset", "label": "is", "title": "is", "to": "benchmark", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "KITTI dataset", "label": "used_in", "title": "used_in", "to": "vision", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "KITTI dataset", "label": "used_in", "title": "used_in", "to": "robotics", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "KITTI dataset", "label": "is_benchmark_for", "title": "is_benchmark_for", "to": "vision and robotics", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "transferability", "label": "varies_between", "title": "varies_between", "to": "datasets", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "neural network training", "label": "is_sensitive_to", "title": "is_sensitive_to", "to": "parameter choices", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "parameter optimization", "label": "is_important_for", "title": "is_important_for", "to": "optimal results", "width": 3.7600000000000002}, {"arrows": "to", "color": "#CC7700", "from": "Pedestrian Detection", "label": "is_field_of", "title": "is_field_of", "to": "Neural Network Training", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Pedestrian Detection", "label": "requires", "title": "requires", "to": "Parameter Optimization", "width": 3.8499999999999996}, {"arrows": "to", "color": "#00CC77", "from": "Parameter Optimization", "label": "influences", "title": "influences", "to": "optimal results", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Dataset Analysis", "label": "supports", "title": "supports", "to": "Neural Network Training", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Transfer Learning", "label": "improves", "title": "improves", "to": "Neural Network Training", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Benenson, R.", "label": "affiliated_with", "title": "affiliated_with", "to": "Max Planck Institute for Informatics", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Benenson, R.", "label": "email", "title": "email", "to": "rodrigo.benenson@mpi-inf.mpg.de", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "Max Planck Institute for Informatics", "label": "research_area", "title": "research_area", "to": "informatics", "width": 3.4000000000000004}, {"arrows": "to", "color": "#CC7700", "from": "Max Planck Institute for Informatics", "label": "research_area", "title": "research_area", "to": "computer vision", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Omran, M.", "label": "affiliated_with", "title": "affiliated_with", "to": "Max Planck Institute for Informatics", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Omran, M.", "label": "email", "title": "email", "to": "mohamed.omran@mpi-inf.mpg.de", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Schiele, B.", "label": "affiliated_with", "title": "affiliated_with", "to": "Max Planck Institute for Informatics", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Schiele, B.", "label": "email", "title": "email", "to": "unspecified", "width": 3.0999999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Hosang, J.", "label": "email", "title": "email", "to": "jan.hosang@mpi-inf.mpg.de", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Kiyoshi Matsuo", "label": "contributed_to", "title": "contributed_to", "to": "Depth Image Enhancement Using Local Tangent Plane Approximations", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Kiyoshi Matsuo", "label": "affiliated_with", "title": "affiliated_with", "to": "Hokuyo Automatic Co., LTD.", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Depth Image Enhancement Using Local Tangent Plane Approximations", "label": "presented_at", "title": "presented_at", "to": "CVPR", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Depth Image Enhancement Using Local Tangent Plane Approximations", "label": "publication_date", "title": "publication_date", "to": "2015", "width": 3.25}, {"arrows": "to", "color": "#0077CC", "from": "Yoshimitsu Aoki", "label": "contributed_to", "title": "contributed_to", "to": "Depth Image Enhancement Using Local Tangent Plane Approximations", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Yoshimitsu Aoki", "label": "affiliated_with", "title": "affiliated_with", "to": "Keio University", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "depth image enhancement method", "label": "aims_for", "title": "aims_for", "to": "consumer RGB-D cameras", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "pixel-coordinates", "label": "is_unsuitable_for", "title": "is_unsuitable_for", "to": "handling local geometries", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "two steps", "label": "include", "title": "include", "to": "calculation of local tangents", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "two steps", "label": "include", "title": "include", "to": "surface reconstruction", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "depth image enhancement", "label": "achieved_by", "title": "achieved_by", "to": "local geometries", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "Local Tangent Planes", "label": "related_to", "title": "related_to", "to": "Depth Image Enhancement", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "RGB-D Cameras", "label": "used_for", "title": "used_for", "to": "Surface Reconstruction", "width": 3.64}, {"arrows": "to", "color": "#00CC77", "from": "Noise Reduction", "label": "improves", "title": "improves", "to": "Completion Rate", "width": 3.25}, {"arrows": "to", "color": "#0077CC", "from": "Asus Xtion Pro Live", "label": "is_example_of", "title": "is_example_of", "to": "RGB-D Cameras", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "Kim et al. (2013)", "label": "published", "title": "published", "to": "joint intensity and depth analysis model", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Kim et al. (2014)", "label": "addresses", "title": "addresses", "to": "depth map upsampling", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Lee et al.", "label": "publishes", "title": "publishes", "to": "Journal of Signal Processing Systems", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "depth map upsampling method", "label": "is_robust_to", "title": "is_robust_to", "to": "misalignment of depth and color boundaries", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Kopf et al.", "label": "develops", "title": "develops", "to": "Joint bilateral upsampling", "width": 3.88}, {"arrows": "to", "color": "#CC7700", "from": "Joint bilateral upsampling", "label": "is_a", "title": "is_a", "to": "upsampling method", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Li et al.", "label": "proposes", "title": "proposes", "to": "Joint example-based depth map super-resolution", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "Joint example-based depth map super-resolution", "label": "aims_to_improve", "title": "aims_to_improve", "to": "depth map resolution", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "Lu et al.", "label": "addresses", "title": "addresses", "to": "Depth enhancement", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "Depth enhancement", "label": "uses", "title": "uses", "to": "low-rank matrix completion", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Joint geodesic up-sampling", "label": "targets", "title": "targets", "to": "depth images", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "S. Lu", "label": "authored", "title": "authored", "to": "Depth enhancement via low-rank matrix completion", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "S. Lu", "label": "presented_at", "title": "presented_at", "to": "CVPR 2014", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "D. Scharstein", "label": "authored", "title": "authored", "to": "Learning conditional random fields for stereo", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "J. Papon", "label": "authored", "title": "authored", "to": "Point cloud video object segmentation", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "J. Papon", "label": "presented_at", "title": "presented_at", "to": "IROS 2013", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Sean Bell", "label": "author_of", "title": "author_of", "to": "Material Recognition in the Wild", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Sean Bell", "label": "is_author_of", "title": "is_author_of", "to": "Material Recognition in 2015 CVPR paper", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "Sean Bell", "label": "works_in_field", "title": "works_in_field", "to": "Material Recognition", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Sean Bell", "label": "contributed_to", "title": "contributed_to", "to": "Materials in Context Database", "width": 3.25}, {"arrows": "to", "color": "#0077CC", "from": "Sean Bell", "label": "contributes_to", "title": "contributes_to", "to": "Bell_Material_Recognition_in_2015_CVPR_paper", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Sean Bell", "label": "affiliated_with", "title": "affiliated_with", "to": "Department of Computer Science, Cornell University", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Sean Bell", "label": "has_email", "title": "has_email", "to": "sbell@cs.cornell.edu", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "Material Recognition in the Wild", "label": "uses", "title": "uses", "to": "Materials in Context Database", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "Paul Upchurch", "label": "author_of", "title": "author_of", "to": "Material Recognition in the Wild", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Paul Upchurch", "label": "is_author_of", "title": "is_author_of", "to": "Material Recognition in 2015 CVPR paper", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Paul Upchurch", "label": "contributes_to", "title": "contributes_to", "to": "Bell_Material_Detection_in_2015_CVPR_paper", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Paul Upchurch", "label": "has_email", "title": "has_email", "to": "paulu@cs.cornell.edu", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Material Recognition in 2015 CVPR paper", "label": "published_in", "title": "published_in", "to": "CVPR", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Material Recognition in 2015 CVPR paper", "label": "located_in", "title": "located_in", "to": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "Material Recognition in 2015 CVPR paper", "label": "addresses_topic", "title": "addresses_topic", "to": "Material Recognition", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Noah Snavely", "label": "is_author_of", "title": "is_author_of", "to": "Material Recognition in 2015 CVPR paper", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Noah Snavely", "label": "contributes_to", "title": "contributes_to", "to": "Bell_Material_Recognition_in_2015_CVPR_paper", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Noah Snavely", "label": "affiliated_with", "title": "affiliated_with", "to": "Department of Computer Science, Cornell University", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Noah Snavely", "label": "has_email", "title": "has_email", "to": "snavely@cs.cornell.edu", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Material Recognition", "label": "requires", "title": "requires", "to": "large, well-sampled datasets", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "material recognition", "label": "is_challenging_due_to", "title": "is_challenging_due_to", "to": "rich surface texture", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "material recognition", "label": "is_challenging_due_to", "title": "is_challenging_due_to", "to": "geometry", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "material recognition", "label": "is_challenging_due_to", "title": "is_challenging_due_to", "to": "lighting conditions", "width": 3.64}, {"arrows": "to", "color": "#CC7700", "from": "MINC", "label": "is_a", "title": "is_a", "to": "dataset", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "MINC", "label": "is", "title": "is", "to": "large-scale", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "MINC", "label": "is", "title": "is", "to": "open", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "CNNs", "label": "are_used_for", "title": "are_used_for", "to": "material classification", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "CNNs", "label": "are_used_for", "title": "are_used_for", "to": "material segmentation", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "patch-based classification", "label": "achieves", "title": "achieves", "to": "85.2% mean class accuracy", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "full image segmentation", "label": "achieves", "title": "achieves", "to": "73.1% mean class accuracy", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Dataset Creation", "label": "uses", "title": "uses", "to": "MINC", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "G. Patterson et al.", "label": "authored", "title": "authored", "to": "The SUN Attribute Database", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "The SUN Attribute Database", "label": "facilitates", "title": "facilitates", "to": "Deeper Scene Understanding", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "S. Bell et al.", "label": "authored", "title": "authored", "to": "OpenSurposes", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "OpenSurfaces", "label": "is", "title": "is", "to": "richly annotated catalog", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "X. Qi et al.", "label": "authored", "title": "authored", "to": "Pairwise rotation invariant co-occurrence local binary pattern", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "B. Caputo et al.", "label": "authored", "title": "authored", "to": "Class-speci\ufb01c material categorisation", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "variant co-occurrence local binary pattern", "label": "is_method", "title": "is_method", "to": "image processing", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "ImageNet Large Scale Visual Recognition Challenge", "label": "is_event", "title": "is_event", "to": "visual recognition", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "ImageNet Large Scale Visual Recognition Challenge", "label": "is_benchmark_for", "title": "is_benchmark_for", "to": "Image Classification", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "ACM Transactions on Graphics (TOG)", "label": "published", "title": "published", "to": "Re\ufb02ectence and texture of real-world surfaces", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "LabelMe", "label": "is_database_and_tool_for", "title": "is_database_and_tool_for", "to": "image annotation", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Re\ufb02ectance", "label": "is_property_of", "title": "is_property_of", "to": "real-world surfaces", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "texture", "label": "is_property_of", "title": "is_property_of", "to": "real-world surfaces", "width": 3.79}, {"arrows": "to", "color": "#CC7700", "from": "Describing textures in the wild", "label": "addresses", "title": "addresses", "to": "texture", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "TOG", "label": "is_publication_of", "title": "is_publication_of", "to": "Graphics", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Pascal VOC Challenge", "label": "is_challenge_in", "title": "is_challenge_in", "to": "Visual Object Classes", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Department of Computer Science", "label": "located_in", "title": "located_in", "to": "Canada", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Wonmin Byeon", "label": "is_author_of", "title": "is_author_of", "to": "Scene Labeling with LSTM Recurrent Neural Networks", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Wonmin Byeon", "label": "affiliated_with", "title": "affiliated_with", "to": "German Research Center for Arti\ufb01cial Intelligence (DFKI)", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Scene Labeling with LSTM Recurrent Neural Networks", "label": "is_paper_type", "title": "is_paper_type", "to": "CVPR paper", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Thomas M. Breuel", "label": "is_author_of", "title": "is_author_of", "to": "Scene Labeling with LSTM Recurrent Neural Networks", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Thomas M. Breuel", "label": "affiliated_with", "title": "affiliated_with", "to": "University of Kaiserslautern", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Federico Raue", "label": "is_author_of", "title": "is_author_of", "to": "Scene Labeling with LSTM Recurrent Neural Networks", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Federico Raue", "label": "affiliated_with", "title": "affiliated_with", "to": "German Research Center for Arti\ufb01cial Intelligence (DFKI)", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Byeon_Scene_Labeling_With_2015_CVPR_paper.pdf", "label": "contains", "title": "contains", "to": "Scene Labeling with LSTM Recurrent Neural Networks", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Accurate scene labeling", "label": "is_step_towards", "title": "is_step_towards", "to": "image understanding", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "LSTM recurrent neural networks", "label": "is_a", "title": "is_a", "to": "neural network", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "state-of-the-art performance", "label": "measured_on", "title": "measured_on", "to": "benchmark datasets", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Networks", "label": "captures", "title": "captures", "to": "local contextual information", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Networks", "label": "captures", "title": "captures", "to": "global contextual information", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Networks", "label": "operates_on", "title": "operates_on", "to": "raw RGB values", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Networks", "label": "adapts_well_for", "title": "adapts_well_for", "to": "complex scene images", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Thalaiyasingam Ajanthan", "label": "author_of", "title": "author_of", "to": "Iteratively Reweighted Graph Cut", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Thalaiyasingam Ajanthan", "label": "affiliated with", "title": "affiliated with", "to": "Australian National University", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Iteratively Reweighted Graph Cut", "label": "used_for", "title": "used_for", "to": "Multi-label MRFs", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "energy", "label": "of", "title": "of", "to": "Multi-label Markov Random Fields (MRFs)", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "energy", "label": "takes_into_account", "title": "takes_into_account", "to": "piecewise constant model assumption", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "energy", "label": "takes_into_account", "title": "takes_into_account", "to": "flow field continuity constraint", "width": 3.91}, {"arrows": "to", "color": "#CC7700", "from": "MRFs", "label": "has_property", "title": "has_property", "to": "Non-convex Priors", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Multi-label Markov Random Fields", "label": "outperforms", "title": "outperforms", "to": "graph-cut-based algorithms", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Multi-label Markov Random Fields", "label": "yields", "title": "yields", "to": "lower energy values", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Multi-label Markov Random Fields", "label": "has_optimization_method", "title": "has_optimization_method", "to": "Graph Cut Optimization", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Ishikawa, H.", "label": "authored", "title": "authored", "to": "Exact optimization for Markov random fields with convex priors", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Ishikawa, H.", "label": "is_foundational_work_on", "title": "is_foundational_work_on", "to": "MRF optimization", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Boykov, Y.", "label": "authored", "title": "authored", "to": "Fast approximate energy minimization via graph cuts", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Boykov, Y.", "label": "method_uses", "title": "method_uses", "to": "graph cut methods", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "Boykov, Y.", "label": "co-authored", "title": "co-authored", "to": "Fast approximate energy minimization", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Fast approximate energy minimization via graph cuts", "label": "presented_in", "title": "presented_in", "to": "PAMI", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "Fast approximate energy minimization via graph cuts", "label": "uses", "title": "uses", "to": "graph cuts", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Kolmogorov, V.", "label": "authored", "title": "authored", "to": "Convergent tree-reweighted message passing", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Kolmogorov, V.", "label": "focuses_on", "title": "focuses_on", "to": "reweighted message passing", "width": 3.82}, {"arrows": "to", "color": "#CC7700", "from": "reweighted message passing", "label": "contributes_to", "title": "contributes_to", "to": "energy minimization", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "Iteratively Reweighted Algorithms", "label": "related_to", "title": "related_to", "to": "reweighted message passing", "width": 3.64}, {"arrows": "to", "color": "#CC7700", "from": "energy minimization", "label": "related_to", "title": "related_to", "to": "pattern analysis", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "geometric relationships", "label": "studied_in", "title": "studied_in", "to": "Multiple view geometry", "width": 3.73}, {"arrows": "to", "color": "#0077CC", "from": "energy minimization techniques", "label": "is_analyzed_in", "title": "is_analyzed_in", "to": "comparative study", "width": 3.88}, {"arrows": "to", "color": "#CC7700", "from": "energy minimization techniques", "label": "related to", "title": "related to", "to": "smoothness-based priors", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "comparative study", "label": "published_in", "title": "published_in", "to": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "comparative study", "label": "examines", "title": "examines", "to": "inference techniques", "width": 3.88}, {"arrows": "to", "color": "#CC7700", "from": "Markov random fields", "label": "uses", "title": "uses", "to": "energy minimization methods", "width": 3.82}, {"arrows": "to", "color": "#CC7700", "from": "smoothness-based priors", "label": "associated_with", "title": "associated_with", "to": "Markov random fields", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Pattern Analysis and Machine Intelligence", "label": "published", "title": "published", "to": "ds with smoothness-based priors", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Pattern Analysis and Machine Intelligence", "label": "is_journal_of", "title": "is_journal_of", "to": "Large Displacement Optical Flow", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "ds with smoothness-based priors", "label": "analyzes", "title": "analyzes", "to": "energy minimization techniques", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Boykov et al.", "label": "published", "title": "published", "to": "An experimental comparison", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Boykov et al.", "label": "published", "title": "published", "to": "IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI)", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Boykov et al.", "label": "authored", "title": "authored", "to": "Fast approximate energy minimization via graph cuts", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "An experimental comparison", "label": "compares", "title": "compares", "to": "min-cut/max-flow algorithms", "width": 3.88}, {"arrows": "to", "color": "#CC7700", "from": "min-cut/max-flow algorithms", "label": "used for", "title": "used for", "to": "energy minimization", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Pock et al.", "label": "introduced", "title": "introduced", "to": "convex formulation", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Pock et al.", "label": "presented in", "title": "presented in", "to": "Computer Vision\u2013ECCV 2008", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "convex formulation", "label": "for", "title": "for", "to": "multi-label problems", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "multi-label problems", "label": "addressed_by", "title": "addressed_by", "to": "convex formulation", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Computer Vision\u2013ECCV 2008", "label": "presents", "title": "presents", "to": "convex formulation", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "H. f.", "label": "introduces", "title": "introduces", "to": "convex formulation", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Kappes et al.", "label": "conducts", "title": "conducts", "to": "comparative study", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Scharstein \u0026 Szeliski", "label": "addresses", "title": "addresses", "to": "dense two-frame stereo correspondence algorithms", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "dense two-frame stereo correspondence algorithms", "label": "is_important_for", "title": "is_important_for", "to": "understanding", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "stereo correspondence algorithms", "label": "addressed by", "title": "addressed by", "to": "International journal of computer vision", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Vekler", "label": "authored", "title": "authored", "to": "Multi-label moves for mrfs", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Multi-label moves for mrfs", "label": "uses", "title": "uses", "to": "truncated convex priors", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Mathieu Salzmann", "label": "affiliated_with", "title": "affiliated_with", "to": "Australian National University", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Mathieu Salzmann", "label": "author of", "title": "author of", "to": "Riemannian Coding and Dictionary Learning", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Mathieu Salzmann", "label": "affiliated_with", "title": "affiliated_with", "to": "NICITA", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Hongdong Li", "label": "affiliated with", "title": "affiliated with", "to": "Australian National University", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Hongdong Li", "label": "author_of", "title": "author_of", "to": "Dense, Accurate Optical Flow Estimation", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Hongdong Li", "label": "affiliated_with", "title": "affiliated_with", "to": "Research School of Engineering", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Hongdong Li", "label": "affiliated_with", "title": "affiliated_with", "to": "The Australian National University (ANU)", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Hongdong Li", "label": "affiliated_with", "title": "affiliated_with", "to": "NICTA", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Rui Zhao", "label": "authored", "title": "authored", "to": "Saliency Detection by Multi-Context Deep Learning", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Rui Zhao", "label": "affiliated_with", "title": "affiliated_with", "to": "Shenzhen Institutes of Advanced Technology", "width": 3.91}, {"arrows": "to", "color": "#CC7700", "from": "Saliency Detection by Multi-Context Deep Learning", "label": "uses", "title": "uses", "to": "Multi-Context Deep Learning", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Wanli Ouyang", "label": "authored", "title": "authored", "to": "Saliency Detection by Multi-Context Deep Learning", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Wanli Ouyang", "label": "affiliated_with", "title": "affiliated_with", "to": "Department of Electronic Engineering", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Wanli Ouyang", "label": "email", "title": "email", "to": "wlouyang@ee.cuhk.edu.hk", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Image salience detection", "label": "aims_to", "title": "aims_to", "to": "highlight visually salient regions", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Conventional approaches", "label": "struggle_with", "title": "struggle_with", "to": "salient objects in low-contrast backgrounds", "width": 2.5}, {"arrows": "to", "color": "#0077CC", "from": "multi-context deep learning framework", "label": "employs", "title": "employs", "to": "Convolutional Neural Networks", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "multi-context deep learning framework", "label": "considers", "title": "considers", "to": "global context", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "multi-context deep learning framework", "label": "considers", "title": "considers", "to": "local context", "width": 3.82}, {"arrows": "to", "color": "#CC7700", "from": "salience", "label": "connects_to", "title": "connects_to", "to": "probabilistic inference", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "salience", "label": "is_aware_of", "title": "is_aware_of", "to": "unsupervised method", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "pre-training scheme", "label": "improves", "title": "improves", "to": "performance", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "Task-specific pre-training scheme", "label": "improves", "title": "improves", "to": "performance", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Frequency-tuned salient region detection", "label": "published_in", "title": "published_in", "to": "CVPR", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Frequency-tuned salient region detection", "label": "is_foundational_work_in", "title": "is_foundational_work_in", "to": "Salient Object Detection", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Training products of experts", "label": "published_in", "title": "published_in", "to": "Neural computation", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "Saliency detection", "label": "presented_at", "title": "presented_at", "to": "CVPR", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "Category-independent object-level saliency detection", "label": "presented_at", "title": "presented_at", "to": "ICCV", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Graph-based visual saliency", "label": "presented_at", "title": "presented_at", "to": "NIPS", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "Salient Object Detection", "label": "related_to", "title": "related_to", "to": "Computer Vision", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "Salient Object Detection", "label": "related_to", "title": "related_to", "to": "Multi-Context Modeling", "width": 3.4000000000000004}, {"arrows": "to", "color": "#CC7700", "from": "Salient Object Detection", "label": "uses", "title": "uses", "to": "bootstrap learning", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Salient Object Detection", "label": "presented_at", "title": "presented_at", "to": "CVPR", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Salient Object Detection", "label": "year", "title": "year", "to": "2015", "width": 3.7600000000000002}, {"arrows": "to", "color": "#CC7700", "from": "Salient Object Detection", "label": "uses", "title": "uses", "to": "Bootstrap Learning", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Salient Object Detection", "label": "evaluated_on", "title": "evaluated_on", "to": "challenging datasets", "width": 3.64}, {"arrows": "to", "color": "#CC7700", "from": "Salient Object Detection", "label": "utilizes", "title": "utilizes", "to": "Deep Learning Features", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "Salient Object Detection", "label": "incorporates", "title": "incorporates", "to": "Multiscale Analysis", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "ik", "label": "published_in", "title": "published_in", "to": "arXiv", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "ik", "label": "focuses_on", "title": "focuses_on", "to": "object detection", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "ik", "label": "focuses_on", "title": "focuses_on", "to": "semantic segmentation", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "A. Borji", "label": "authored", "title": "authored", "to": "Boosting bottom-up and top-down visual features", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "A. Borji", "label": "authored", "title": "authored", "to": "visual attention modeling", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "M.-M. Cheng", "label": "developed", "title": "developed", "to": "Global contrast based salient region detection", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Global contrast based salient region detection", "label": "is_foundational_work_in", "title": "is_foundational_work_in", "to": "Salient Object Detection", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Global contrast based salient region detection", "label": "published_in", "title": "published_in", "to": "TRAMI", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "R. Mairon", "label": "authored", "title": "authored", "to": "A closer look at context", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "Shenzhen Institutes of Advanced Technology", "label": "part_of", "title": "part_of", "to": "Chinese Academy of Sciences", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Advanced Technology", "label": "part_of", "title": "part_of", "to": "Chinese Academy of Sciences", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Baohua Li", "label": "author_of", "title": "author_of", "to": "Subspace Clustering by Mixture of Gaussian Regression", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Baohua Li", "label": "affiliation", "title": "affiliation", "to": "Dalian University of Technology", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Subspace Clustering by Mixture of Gaussian Regression", "label": "published_in", "title": "published_in", "to": "CVPR", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Ying Zhang", "label": "author_of", "title": "author_of", "to": "Subspace Clustering by Mixture of Gaussian Regression", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Ying Zhang", "label": "is_affiliated_with", "title": "is_affiliated_with", "to": "Dalian\u003c0xC2\u003e\u003c0xA0\u003eUniversity of Technology", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Ying Zhang", "label": "affiliation", "title": "affiliation", "to": "Dalian University of Technology", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Zhouchen Lin", "label": "author_of", "title": "author_of", "to": "Subclone Clustering by Mixture of Gaussian Regression", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Zhouchen Lin", "label": "is_affiliated_with", "title": "is_affiliated_with", "to": "Dalian University of Technology", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Zhouchen Lin", "label": "is_part_of", "title": "is_part_of", "to": "School of EECS", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Huchuan Lu", "label": "author_of", "title": "author_of", "to": "Subspace Clustering by Mixture of Gaussian Regression", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Huchuan Lu", "label": "affiliated_with", "title": "affiliated_with", "to": "Dalian University of Technology", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Huchuan Lu", "label": "authored", "title": "authored", "to": "Salient Object Detectio", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Huchuan Lu", "label": "is_author_of", "title": "is_author_of", "to": "Deep Networks for Saliency Detection", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Huchuan Lu", "label": "is_author_of", "title": "is_author_of", "to": "Deep Networks for Salience Detected", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "Subspace clustering", "label": "aims_to", "title": "aims_to", "to": "multi-subspace representation", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "Subspace clustering", "label": "operates_in", "title": "operates_in", "to": "high-dimensional space", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "Existing methods", "label": "relies_on", "title": "relies_on", "to": "norms", "width": 3.7600000000000002}, {"arrows": "to", "color": "#CC7700", "from": "MoG Regression", "label": "approaches", "title": "approaches", "to": "subspace clustering", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "MoG Regression", "label": "provides", "title": "provides", "to": "affinity matrix", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "MoG Regression", "label": "improves", "title": "improves", "to": "clustering performance", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "MoG Regression", "label": "models", "title": "models", "to": "noise distributions", "width": 3.7600000000000002}, {"arrows": "to", "color": "#CC7700", "from": "Mixture of Gausians (MoG)", "label": "models", "title": "models", "to": "noise", "width": 3.88}, {"arrows": "to", "color": "#CC7700", "from": "Noise Modeling", "label": "is_approach_in", "title": "is_approach_in", "to": "Subspace clustering", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "Noise Modeling", "label": "is_technique", "title": "is_technique", "to": "clustering", "width": 3.25}, {"arrows": "to", "color": "#0077CC", "from": "Subspace Clustering", "label": "outperforms", "title": "outperforms", "to": "state-of-the-art subspace clustering methods", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "K-plane clustering", "label": "is_technique_for", "title": "is_technique_for", "to": "segmentation", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "K-plane clustering", "label": "introduced_in", "title": "introduced_in", "to": "Journal of Global Optimization", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "segmentation", "label": "results_in", "title": "results_in", "to": "geo-referenced images", "width": 3.64}, {"arrows": "to", "color": "#CC7700", "from": "segmentation", "label": "is", "title": "is", "to": "hierarchical", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "segmentation", "label": "is", "title": "is", "to": "graph-based", "width": 3.64}, {"arrows": "to", "color": "#CC7700", "from": "segmentation", "label": "requires", "title": "requires", "to": "object identification", "width": 3.25}, {"arrows": "to", "color": "#CC7700", "from": "segmentation", "label": "requires", "title": "requires", "to": "motion analysis", "width": 3.25}, {"arrows": "to", "color": "#0077CC", "from": "power factorization", "label": "utilized_for", "title": "utilized_for", "to": "motion segmentation", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "motion segmentation", "label": "covers", "title": "covers", "to": "various types of motion", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "motion segmentation", "label": "provides", "title": "provides", "to": "broad framework", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "motion segmentation", "label": "is_part_of", "title": "is_part_of", "to": "object and motion segmentation", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "GPCA", "label": "utilized_for", "title": "utilized_for", "to": "motion segmentation", "width": 3.88}, {"arrows": "to", "color": "#CC7700", "from": "GPCA", "label": "is", "title": "is", "to": "technique", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "GPCA", "label": "used_in", "title": "used_in", "to": "referenced papers", "width": 3.7600000000000002}, {"arrows": "to", "color": "#CC7700", "from": "Mixture of Gaussian Regression", "label": "is_technique", "title": "is_technique", "to": "clustering", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "Affinity Matrix Construction", "label": "is_part_of", "title": "is_part_of", "to": "clustering", "width": 3.25}, {"arrows": "to", "color": "#CC7700", "from": "Spectral clustering", "label": "is", "title": "is", "to": "technique", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "tutorial", "label": "provides", "title": "provides", "to": "overview", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "tutorial", "label": "focuses_on", "title": "focuses_on", "to": "Spectral clustering", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "Sparse representation", "label": "is", "title": "is", "to": "technique", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Sparse representation", "label": "informs", "title": "informs", "to": "Approaches", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "Sparse representation", "label": "is_technique", "title": "is_technique", "to": "Pattern Analysis", "width": 3.25}, {"arrows": "to", "color": "#0077CC", "from": "Wright et al.", "label": "develops", "title": "develops", "to": "face recognition approach", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "face recognition approach", "label": "uses", "title": "uses", "to": "Sparse representation", "width": 3.46}, {"arrows": "to", "color": "#CCCCCC", "from": "EM algorithm", "label": "facilitates", "title": "facilitates", "to": "Segmentation", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "EM algorithm", "label": "addresses", "title": "addresses", "to": "Convergence properties", "width": 3.7600000000000002}, {"arrows": "to", "color": "#CC7700", "from": "EM algorithm", "label": "is_algorithm", "title": "is_algorithm", "to": "Gaussian mixtures", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "Motion segmentation", "label": "covered_by", "title": "covered_by", "to": "Framework", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "Motion segmentation", "label": "is_process", "title": "is_process", "to": "Computer Vision", "width": 3.0999999999999996}, {"arrows": "to", "color": "#CC7700", "from": "Motion segmentation", "label": "related_to", "title": "related_to", "to": "Optical flow estimation", "width": 3.25}, {"arrows": "to", "color": "#0077CC", "from": "Framework", "label": "covers", "title": "covers", "to": "Types of motion", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Framework", "label": "extracts", "title": "extracts", "to": "region-keyword pairs", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Framework", "label": "improves", "title": "improves", "to": "Image Quality", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "lossy data compression", "label": "is_methodology", "title": "is_methodology", "to": "image segmentation", "width": 3.64}, {"arrows": "to", "color": "#CC7700", "from": "robust PCA", "label": "is_consideration_for", "title": "is_consideration_for", "to": "segmentation", "width": 3.4000000000000004}, {"arrows": "to", "color": "#CC7700", "from": "A Convolutional Neural Network Cascade", "label": "is_a", "title": "is_a", "to": "Face Detection", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "A Convolutional Neural Network Cascade", "label": "publication_venue", "title": "publication_venue", "to": "CVPR paper", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "Face Detection", "label": "uses", "title": "uses", "to": "Convolutional Neural Networks", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Face Detection", "label": "runs_at", "title": "runs_at", "to": "14 FPS", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Face Detection", "label": "runs_at", "title": "runs_at", "to": "100 FPS", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Face Detection", "label": "uses", "title": "uses", "to": "Convolutional Neural Networks (CNNs)", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Haoxiang Li", "label": "author_of", "title": "author_of", "to": "A Convolutional Neural Network Cascade", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Haoxiang Li", "label": "is_affiliated_with", "title": "is_affiliated_with", "to": "Stevens Institute of Technology", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Haoxiang Li", "label": "is_author_of", "title": "is_author_of", "to": "Hierarchical-PEP Model", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Zhe Lin", "label": "author_of", "title": "author_of", "to": "A Convolutional Neural Network Cascade", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Zhe Lin", "label": "affiliated_with", "title": "affiliated_with", "to": "Adobe Research", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Zhe Lin", "label": "authored", "title": "authored", "to": "Salient Object Subitizing", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "Jonathan Brandt", "label": "author_of", "title": "author_of", "to": "A Convolutional Neural Network Cascade", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Gang Hua", "label": "author_of", "title": "author_of", "to": "A Convolutional Neural Network Cascade", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Gang Hua", "label": "affiliated_with", "title": "affiliated_with", "to": "Stevens Institute of Technology", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Gang Hua", "label": "is_author_of", "title": "is_author_of", "to": "Hierarchical-PEP Model", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Gang Hua", "label": "has_email", "title": "has_email", "to": "{ghua}@steverns.edu", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Face detection", "label": "faces", "title": "faces", "to": "large visual variations", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Face detection", "label": "faces", "title": "faces", "to": "large search space", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Advanced models", "label": "addresses", "title": "addresses", "to": "visual variations", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Advanced models", "label": "is", "title": "is", "to": "computationally expensive", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Paper", "label": "proposes", "title": "proposes", "to": "CNN cascade architecture", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Paper", "label": "introduces", "title": "introduces", "to": "Approach", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Paper", "label": "proposes", "title": "proposes", "to": "subgraph matching formulation", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "CNN cascade architecture", "label": "balances", "title": "balances", "to": "conflicting demands", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Cascade", "label": "rejects", "title": "rejects", "to": "background regions", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "CNN-based calibration stage", "label": "improves", "title": "improves", "to": "localization effectiveness", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "100 FPS", "label": "requires", "title": "requires", "to": "GPU", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Cascade Architecture", "label": "optimizes", "title": "optimizes", "to": "Real-time Performance", "width": 3.61}, {"arrows": "to", "color": "#CC7700", "from": "Cascade Architecture", "label": "enhances", "title": "enhances", "to": "Face Detection", "width": 3.7600000000000002}, {"arrows": "to", "color": "#CC7700", "from": "Bounding Box Calibration", "label": "improves", "title": "improves", "to": "Face Detection", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "LeCun", "label": "authored", "title": "authored", "to": "Convolutional networks", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Convolutional networks", "label": "processes", "title": "processes", "to": "speech", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Rowley", "label": "researched", "title": "researched", "to": "Neural network-based face detection", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "Felzenszwalb", "label": "developed", "title": "developed", "to": "part-based models", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "part-based models", "label": "used_for", "title": "used_for", "to": "Object Detection", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "P. F.", "label": "authored", "title": "authored", "to": "Object detection with discriminatatively trained part-based models", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "P. F.", "label": "authored", "title": "authored", "to": "Object detection with discriminatively trained part-based models", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "P. F.", "label": "affiliated_with", "title": "affiliated_with", "to": "IEEE Trans. Pattern Anal. Mach. Intell", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Object detection with discriminatively trained part-based models", "label": "published_in", "title": "published_in", "to": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Jain, V.", "label": "authored", "title": "authored", "to": "Fddb: A benchmark for face detection in unconstrained settings", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Fddb: A benchmark for face detection in unconstrained settings", "label": "is_report_from", "title": "is_report_from", "to": "University of Massachusetts, Amherst", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Rich feature hierarchies for accurate object detection and semantic segmentation", "label": "published_as", "title": "published_as", "to": "arXiv preprint arXiv:1311.2524", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "ImageNet classification with deep convolutional neural networks", "label": "published_in", "title": "published_in", "to": "Advances in neural information processing systems", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Jia, Y.", "label": "authored", "title": "authored", "to": "Caffe: Convolutional architecture for fast feature embedding", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Advances in neural information processing systems", "label": "is_publication", "title": "is_publication", "to": "conference", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "helhamer, E.", "label": "is_author_of", "title": "is_author_of", "to": "Caffe", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Vaillant, R.", "label": "is_author_of", "title": "is_author_of", "to": "object localization approach", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Yang, B.", "label": "is_author_of", "title": "is_author_of", "to": "multi-view face detection method", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Stevens Institute of Technology", "label": "has_author", "title": "has_author", "to": "Haoxiang Li", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Na Tong", "label": "authored", "title": "authored", "to": "Salient Object Detection", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Na Tong", "label": "affiliated_with", "title": "affiliated_with", "to": "Dalian University of Technology", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Xiang Ruan", "label": "authored", "title": "authored", "to": "Salient Object Detection", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Xiang Ruan", "label": "affiliated_with", "title": "affiliated_with", "to": "OMRON Corporation", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Xiang Ruan", "label": "is_author_of", "title": "is_author_of", "to": "Deep Networks for Saliency Detection", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Xiang Ruan", "label": "is_author_of", "title": "is_author_of", "to": "Deep Networks for Salience Detection", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Ming-Hsuan Yang", "label": "authored", "title": "authored", "to": "Salient Object Detection", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Ming-Hsuan Yang", "label": "affiliated_with", "title": "affiliated_with", "to": "University of California at Merced", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Ming-Hsuan Yang", "label": "is_author_of", "title": "is_author_of", "to": "Deep Networks for Saliency Detection", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Ming-Hsuan Yang", "label": "is_author_of", "title": "is_author_of", "to": "Deep Networks for Salience Detection", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Ming-Hsuan Yang", "label": "is_author_of", "title": "is_author_of", "to": "JOTS", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Ming-Hsuan Yang", "label": "author_of", "title": "author_of", "to": "Adaptive Region Pooling for Object Detection", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Ming-Hsuan Yang", "label": "affiliated_with", "title": "affiliated_with", "to": "UC Merced", "width": 3.91}, {"arrows": "to", "color": "#CC7700", "from": "Bootstrap Learning", "label": "is_method_of", "title": "is_method_of", "to": "Object Detection", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Tong_Salient_Object_Detection_2015_CVPR_paper.pdf", "label": "contains", "title": "contains", "to": "Salient Object Detection", "width": 3.91}, {"arrows": "to", "color": "#CC7700", "from": "bootstrap learning algorithm", "label": "exploits", "title": "exploits", "to": "weak models", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "bootstrap learning algorithm", "label": "exploits", "title": "exploits", "to": "strong models", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "bootstrap learning algorithm", "label": "performs_favorably_against", "title": "performs_favorably_against", "to": "state-of-the-art salience detection methods", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "weak salience map", "label": "generated_from", "title": "generated_from", "to": "image priors", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "weak salience map", "label": "generates", "title": "generates", "to": "training samples", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "strong classi\ufb01er", "label": "detects", "title": "detects", "to": "salient pixels", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "strong classi\ufb01er", "label": "based_on", "title": "based_on", "to": "input image", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "multiscale salience maps", "label": "improves", "title": "improves", "to": "detection performance", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "bootstrap learning approach", "label": "results_in", "title": "results_in", "to": "signi\ufb01cant improvement", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "Salieny Detection Methods", "label": "is_state_of_the_art", "title": "is_state_of_the_art", "to": "Computer Vision", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Bootstrap Learning Approach", "label": "is_applicable_to", "title": "is_applicable_to", "to": "Bottom-up Salieny Models", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Li et al. (2014)", "label": "researches", "title": "researches", "to": "Salient Object Segmentation", "width": 3.9699999999999998}, {"arrows": "to", "color": "#CC7700", "from": "Salient Object Segmentation", "label": "is_field_of", "title": "is_field_of", "to": "Computer Vision", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Achanta et al. (2009)", "label": "researches", "title": "researches", "to": "Frequency-tuned Salient Region Detection", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Movahedi \u0026 Elder (2010)", "label": "designs", "title": "designs", "to": "Performance Measures", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Performance Measures", "label": "validates", "title": "validates", "to": "Salient Object Segmentation", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "Achanta et al. (2010)", "label": "researches", "title": "researches", "to": "Slic Superpixels", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Achanta, R.", "label": "authored", "title": "authored", "to": "Slic superpixels", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Ojala, T.", "label": "authored", "title": "authored", "to": "Multiresolution gray-scale texture classification", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Bach, F. R.", "label": "authored", "title": "authored", "to": "Multiple kernel learning", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Multiple kernel learning", "label": "uses", "title": "uses", "to": "SMO algorithm", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "Perazzi, F.", "label": "authored", "title": "authored", "to": "Saliency filters", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Borji, A.", "label": "authored", "title": "authored", "to": "Salient object detection benchmark", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Borji, A.", "label": "authored", "title": "authored", "to": "Salient Object Detection: A Benchmark", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "authored", "label": "Segmenting salient objects", "title": "Segmenting salient objects", "to": "Rahtu, E.", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Salient object segmentation", "label": "uses", "title": "uses", "to": "performance measures", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "superpixels", "label": "classified_by", "title": "classified_by", "to": "feedforward multilayer network", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "superpixels", "label": "builds", "title": "builds", "to": "primitive saliency dictionary", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Local binary patterns", "label": "used_in", "title": "used_in", "to": "texture classification", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "J. Heikkil\u00e4", "label": "authored", "title": "authored", "to": "Segmenting salient objects from images and videos", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Segmenting salient objects from images and videos", "label": "presented_at", "title": "presented_at", "to": "ECCV", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Kaiming He", "label": "co-authored", "title": "co-authored", "to": "Convolutional Neural Networks at Constrained Time Cost", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Kaiming He", "label": "is author of", "title": "is author of", "to": "paper", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Kaiming He", "label": "affiliated_with", "title": "affiliated_with", "to": "Microsoft Research", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Kaiming He", "label": "author", "title": "author", "to": "A Geodesic-Prepreserving Method for Image Warping", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Kaiming He", "label": "is_author_of", "title": "is_author_of", "to": "A Geolesic-Preerving Method for Image Warping", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Kaiming He", "label": "author_of", "title": "author_of", "to": "Sparse Projections for High-Dimensional Binary Codes", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Convolutional Neural Networks at Constained Time Cost", "label": "presented_at", "title": "presented_at", "to": "CVPR", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "University of California at Merced", "label": "located_in", "title": "located_in", "to": "USA", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "architecture", "label": "achieves", "title": "achieves", "to": "competitive accuracy", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "architecture", "label": "is", "title": "is", "to": "20% faster than AlexNet", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "architecture", "label": "designed_for", "title": "designed_for", "to": "person re-identification", "width": 3.91}, {"arrows": "to", "color": "#CC7700", "from": "architecture", "label": "is_example_of", "title": "is_example_of", "to": "application domain", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "architecture", "label": "achieves", "title": "achieves", "to": "69.6% average accuracy", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "architecture", "label": "tested_on", "title": "tested_on", "to": "PAS-CAL VOC 2012 test set", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "accuracy improvements", "label": "influenced_by", "title": "influenced_by", "to": "factors", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "ImageNet dataset", "label": "has", "title": "has", "to": "11.8% top-5 error", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Offline Training", "label": "has_constraint", "title": "has_constraint", "to": "Time Constraints", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "Accuracy Improvements", "label": "influenced_by", "title": "influenced_by", "to": "Factors", "width": 3.8499999999999996}, {"arrows": "to", "color": "#00CC77", "from": "Accuracy Improvements", "label": "requires", "title": "requires", "to": "Limited Time Budget", "width": 3.7600000000000002}, {"arrows": "to", "color": "#CC7700", "from": "Architecture", "label": "is_example_of", "title": "is_example_of", "to": "Computer-Aided Design application", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Architecture", "label": "achieves_accuracy", "title": "achieves_accuracy", "to": "PAS-CAL VOC 2012 test set", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Architecture", "label": "is", "title": "is", "to": "recurrent convolutional", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Architecture", "label": "is", "title": "is", "to": "large-scale visual learning", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "ImageNet Dataset", "label": "is_foundational_for", "title": "is_foundational_for", "to": "Computer Vision Tasks", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "ImageNet Dataset", "label": "is_hierarchical", "title": "is_hierarchical", "to": "Image Database", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Deng et al. (2009)", "label": "introduces", "title": "introduces", "to": "ImageNet Dataset", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Deng et al. (2009)", "label": "creates", "title": "creates", "to": "ImageNet", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "Layer Replacement", "label": "is_optimization_strategy", "title": "is_optimization_strategy", "to": "Architecture Optimization", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "ImageNet", "label": "is_used_for", "title": "is_used_for", "to": "computer vision tasks", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "ImageNet", "label": "is_a", "title": "is_a", "to": "hierarchical image database", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "ImageNet", "label": "is_database_for", "title": "is_database_for", "to": "Hierarchical Image Database", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "ImageNet", "label": "is_competition", "title": "is_competition", "to": "ILSVRC2012", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "ImageNet", "label": "is", "title": "is", "to": "large dataset", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "ImageNet", "label": "used for", "title": "used for", "to": "training computer vision models", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "ImageNet", "label": "is_used_by", "title": "is_used_by", "to": "AlexNet", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "ImageNet", "label": "is_classified_with", "title": "is_classified_with", "to": "deep convolutional neural networks", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "ImageNet", "label": "presented_at", "title": "presented_at", "to": "CVPR", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "ImageNet", "label": "is_database_of", "title": "is_database_of", "to": "hierarchical image", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "very deep convolutional networks", "label": "is_a", "title": "is_a", "to": "development", "width": 3.7600000000000002}, {"arrows": "to", "color": "#CC7700", "from": "very deep convolutional networks", "label": "is_a", "title": "is_a", "to": "network architecture", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "multi-column deep neural networks", "label": "demonstrates", "title": "demonstrates", "to": "power of deep learning", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "multi-column deep neural networks", "label": "is_used_for", "title": "is_used_for", "to": "image classification", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "rich feature hierarchies", "label": "used_for", "title": "used_for", "to": "object detection", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "rich feature hierarchies", "label": "used_for", "title": "used_for", "to": "semantic segmentation", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "deep learning", "label": "uses", "title": "uses", "to": "multi-column deep neural networks", "width": 3.58}, {"arrows": "to", "color": "#CC7700", "from": "deep learning", "label": "encompasses", "title": "encompasses", "to": "AlexNet", "width": 3.88}, {"arrows": "to", "color": "#CC7700", "from": "deep learning", "label": "is_related_to", "title": "is_related_to", "to": "intrinsic image decomposition", "width": 3.25}, {"arrows": "to", "color": "#0077CC", "from": "Malik et al. (2014)", "label": "introduces", "title": "introduces", "to": "rich feature hierarchies", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Zeiler et al. (2014)", "label": "focuses_on", "title": "focuses_on", "to": "convolutional neural networks", "width": 3.91}, {"arrows": "to", "color": "#CC7700", "from": "convolutional neural networks", "label": "important_for", "title": "important_for", "to": "interpretability", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Eigen et al. (2013)", "label": "explores", "title": "explores", "to": "deep architectures", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "deep architectures", "label": "understood_using", "title": "understood_using", "to": "recursive convolutional networks", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "recursive convolutional networks", "label": "used_to_understand", "title": "used_to_understand", "to": "deep architectures", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "Chatfield et al. (2014)", "label": "investigates", "title": "investigates", "to": "convolutional networks", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Chatfield et al. (2014)", "label": "focuses_on", "title": "focuses_on", "to": "details", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "convolutional networks", "label": "used_in", "title": "used_in", "to": "image processing", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "convolutional networks", "label": "uses", "title": "uses", "to": "output of last layer", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Return of the devil in the details", "label": "investigates", "title": "investigates", "to": "convolutional networks", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Overfeat", "label": "is", "title": "is", "to": "integrated recognition, localization, and detection system", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Going deeper with convolutions", "label": "explores", "title": "explores", "to": "convolutions", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Lei Zhang", "label": "is_author_of", "title": "is_author_of", "to": "Reweighted Laplace Prior Based Hyperspectral Compressive Sensing", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Yanning Zhang", "label": "is_author_of", "title": "is_author_of", "to": "Reweighted Laplace Prior Based Hyperspectral Compressive Sensing", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Yanning Zhang", "label": "author_of", "title": "author_of", "to": "Zhang_Reweighted_Laplace_Prior_2015_CVPR_supplemental.pdf", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Chunna Tian", "label": "is_author_of", "title": "is_author_of", "to": "Reweighted Laplace Prior Based Hyperspectral Compressive Sensing", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Chunna Tian", "label": "is_author_of", "title": "is_author_of", "to": "Reweighted Laplace Prior Based Hyperspectra Compressives Sensing", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Chunna Tian", "label": "author_of", "title": "author_of", "to": "Zhang_Reweighted_Lapless_Prior_2015_CVPR_supplemental.pdf", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Fei Li", "label": "is_author_of", "title": "is_author_of", "to": "Reweighted Laplace Prior Based Hyperspectral Compressive Sensing", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Fei Li", "label": "is_author_of", "title": "is_author_of", "to": "Reweighted Laplace Prior Based Hyperspectral Compressives Sensing", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Fei Li", "label": "author_of", "title": "author_of", "to": "Zhang_Reweighted_Laplace_Prior_2015_CVPR_supplemental.pdf", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Wei Wei", "label": "is_author_of", "title": "is_author_of", "to": "Reweighted Laplace Prior Based Hyperspectral Compressive Sensing", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Reweighted Laplace Prior Based Hyperspectral Compressives Sensing", "label": "is_published_in", "title": "is_published_in", "to": "CVPR", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "Reweighted Laplace Prior Based Hyperspectral Compressives Sensing", "label": "deals_with", "title": "deals_with", "to": "Hyperspectral Compressives Sensing", "width": 3.25}, {"arrows": "to", "color": "#CC7700", "from": "Reweighted Laplace Prior Based Hyperspectral Compressives Sensing", "label": "uses", "title": "uses", "to": "Laplace Prior", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Reweighted Laplace Prior Based Hyperspectral Compressives Sensing", "label": "is_supplemental_material_for", "title": "is_supplemental_material_for", "to": "Zhang_Reweighted_Laplace_Prior_2015_CVPR_supplemental.pdf", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Zhang_Reweighted_Laplace_Prior_2015_CVPR_supplemental.pdf", "label": "is_located_at", "title": "is_located_at", "to": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers", "width": 3.0999999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Zhang_Reweighted_Laplace_Prior_2015_CVPR_supplemental.pdf", "label": "details", "title": "details", "to": "hyperspectral compressive sensing method", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "hyperspectral compressive sensing method", "label": "utilizes", "title": "utilizes", "to": "reweighted Laplace prior", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "optimization procedure", "label": "involves", "title": "involves", "to": "matrix algebra manipulations", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "optimization procedure", "label": "involves", "title": "involves", "to": "conjugate functions", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "optimization procedure", "label": "includes", "title": "includes", "to": "sparsity learning over \u03b3", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "optimization procedure", "label": "includes", "title": "includes", "to": "noise estimation over \u03bb", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "conjugate functions", "label": "transforms", "title": "transforms", "to": "non-convex optimization problems", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "Hyberspectral Compressive Sensing", "label": "is_a", "title": "is_a", "to": "approach", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "Reweighted Laplace Prior", "label": "is_a", "title": "is_a", "to": "optimization technique", "width": 3.4000000000000004}, {"arrows": "to", "color": "#CC7700", "from": "Sparsity Learning", "label": "is_a", "title": "is_a", "to": "optimization technique", "width": 3.4000000000000004}, {"arrows": "to", "color": "#CC7700", "from": "Noise Estimation", "label": "is_a", "title": "is_a", "to": "optimization technique", "width": 3.25}, {"arrows": "to", "color": "#0077CC", "from": "Fang_Collaborative_Feature_Learning_2015_CVPR_paper", "label": "is_a", "title": "is_a", "to": "research paper", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Fang_Collaborative_Feature_Learning_2015_CVPR_paper", "label": "published_in", "title": "published_in", "to": "CVPR", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "Paper Abstract", "label": "requires", "title": "requires", "to": "readable text", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "Research", "label": "potentially covers", "title": "potentially covers", "to": "Data Encoding/Decoding", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "Research", "label": "potentially covers", "title": "potentially covers", "to": "Text Corruption/Error Correction", "width": 3.4000000000000004}, {"arrows": "to", "color": "#CC7700", "from": "Research", "label": "potentially covers", "title": "potentially covers", "to": "Pattern Recognition", "width": 3.25}, {"arrows": "to", "color": "#CC7700", "from": "Research", "label": "potentially covers", "title": "potentially covers", "to": "Information Retrieval", "width": 3.0999999999999996}, {"arrows": "to", "color": "#CC7700", "from": "Research", "label": "connects", "title": "connects", "to": "Salience", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Research", "label": "discusses", "title": "discusses", "to": "Material Metamers", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "Research", "label": "has implications for", "title": "has implications for", "to": "Visual Inference tasks", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "Victor Escorcia", "label": "is_author_of", "title": "is_author_of", "to": "On the Relationship between Visual Attributes and Convolutional Networks", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Victor Escorcia", "label": "authored", "title": "authored", "to": "paper", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Victor Escorcia", "label": "author_of", "title": "author_of", "to": "Escorcia_On_the_Relationship_2015_CVPR_paper.pdf", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Victor Escorcia", "label": "affiliated with", "title": "affiliated with", "to": "King Abdullah University of Science and Technology", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Juan Carlos Niebles", "label": "is_author_of", "title": "is_author_of", "to": "On the Relationship between Visual Attributes and Convolutional Networks", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Juan Carlos Niebles", "label": "authored", "title": "authored", "to": "paper", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Juan Carlos Niebles", "label": "author_of", "title": "author_of", "to": "Escorcia_On_the_Relationship_2015_CVPR_paper.pdf", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Juan Carlos Niebles", "label": "affiliated with", "title": "affiliated with", "to": "Universidad del Norte", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Bernard Ghanem", "label": "is_author_of", "title": "is_author_of", "to": "On the Relationship between Visual Attributes and Convolutional Networks", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Bernard Ghanem", "label": "authored", "title": "authored", "to": "paper", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Bernard Ghanem", "label": "author_of", "title": "author_of", "to": "Escorcia_On_the_Relationship_2015_CVPR_paper.pdf", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Bernard Ghanem", "label": "affiliated with", "title": "affiliated with", "to": "King Abdullah University of Science and Technology", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Bernard Ghanem", "label": "is_author_of", "title": "is_author_of", "to": "\u21130TV: A New Method", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Bernard Ghanem", "label": "affiliated_with", "title": "affiliated_with", "to": "King Abdullah University of Science and Technology (KAUST)", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Bernard Ghanem", "label": "email", "title": "email", "to": "bernard.ghanem@kust.edu.sa", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "Convolutional Networks", "label": "related_to", "title": "related_to", "to": "Visual Attributes", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "Visual Attributes", "label": "influence", "title": "influence", "to": "conv-net based object recognition", "width": 3.7600000000000002}, {"arrows": "to", "color": "#00CC77", "from": "visual attributes", "label": "impacts", "title": "impacts", "to": "convolutional networks", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Escorcia_On_the_Relationship_2015_CVPR_paper.pdf", "label": "is_a", "title": "is_a", "to": "CVPR paper", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Escorcia_On_the_Relationship_2015_CVPR_paper.pdf", "label": "located_in", "title": "located_in", "to": "/mnt/DATA/Glucomaa/LLM/Ollama_pdf_handle/cvpr_papers", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "conv-nets", "label": "learns", "title": "learns", "to": "abstract concepts", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "conv-nets", "label": "use", "title": "use", "to": "semantic visual attributes", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "abstract concepts", "label": "are_examples_of", "title": "are_examples_of", "to": "objects in images", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "semantic visual attributes", "label": "impact", "title": "impact", "to": "object description", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Attribute Centric Nodes (ACNs)", "label": "exist_in", "title": "exist_in", "to": "conv-net", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "Attribute Centric Nodes (ACNs)", "label": "encode", "title": "encode", "to": "visual attribute representation", "width": 3.73}, {"arrows": "to", "color": "#0077CC", "from": "Attribute Centric Nodes (ACNs)", "label": "contribute_to", "title": "contribute_to", "to": "discrimination", "width": 3.61}, {"arrows": "to", "color": "#0077CC", "from": "conv-net", "label": "is_trained_to_recognize", "title": "is_trained_to_recognize", "to": "objects", "width": 3.67}, {"arrows": "to", "color": "#CC7700", "from": "conv-net", "label": "performs", "title": "performs", "to": "object recognition", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "visual attribute representation", "label": "encoded by", "title": "encoded by", "to": "conv-net nodes", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "conv-net nodes", "label": "encode", "title": "encode", "to": "information", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "conv-net nodes", "label": "distributed across", "title": "distributed across", "to": "layers", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "conv-net nodes", "label": "are", "title": "are", "to": "sparsely distributed", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "conv-net nodes", "label": "play a role in", "title": "play a role in", "to": "object recognition", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "conv-net nodes", "label": "are", "title": "are", "to": "unevenly distributed", "width": 3.61}, {"arrows": "to", "color": "#CC7700", "from": "information", "label": "pertinent to", "title": "pertinent to", "to": "visual attribute representation and discrimination", "width": 3.73}, {"arrows": "to", "color": "#0077CC", "from": "Zero-Shot Object Recognition", "label": "uses", "title": "uses", "to": "Semantic Manifold Distance", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Zhenyong Fu", "label": "authored", "title": "authored", "to": "Zero-Shot Object Recognition", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Zhenyong Fu", "label": "affiliated_with", "title": "affiliated_with", "to": "Queen Mary, University of London", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Tao Xiang", "label": "authored", "title": "authored", "to": "Zero-Shot Object Recognition", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Tao Xiang", "label": "affiliated_with", "title": "affiliated_with", "to": "Queen Mary, University of London", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Tao Xiang", "label": "affiliated_with", "title": "affiliated_with", "to": "QueenMary, University of London", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Elyor Kodirov", "label": "authored", "title": "authored", "to": "Zero-Shot Object Research", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Elyor Kodirov", "label": "affiliated_with", "title": "affiliated_with", "to": "Queen Mary, University of London", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Shaogang Gong", "label": "authored", "title": "authored", "to": "Zero-Shot Object Research", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Zero-shot learning", "label": "aims_to", "title": "aims_to", "to": "recognise objects", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "Zero-shot learning", "label": "learns", "title": "learns", "to": "knowledge transfer", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "existing works", "label": "measures", "title": "measures", "to": "similarity", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "similarity", "label": "located_in", "title": "located_in", "to": "semantic embedding space", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "semantic embedding space", "label": "uses", "title": "uses", "to": "distance metrics", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "distance metrics", "label": "do_not_consider", "title": "do_not_consider", "to": "intrinsic structure", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "semantic categories", "label": "has", "title": "has", "to": "intrinsic structure", "width": 3.46}, {"arrows": "to", "color": "#0077CC", "from": "absorbing Markov chain process", "label": "computes", "title": "computes", "to": "semantic manifold distance", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "performance gains", "label": "observed_on", "title": "observed_on", "to": "ImageNet", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "performance gains", "label": "observed_on", "title": "observed_on", "to": "AwA datasets", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "proposed model", "label": "unifies", "title": "unifies", "to": "ZSL algorithms", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "proposed model", "label": "demonstrates", "title": "demonstrates", "to": "performance gains", "width": 3.91}, {"arrows": "to", "color": "#CC7700", "from": "semantic manifold", "label": "used_by", "title": "used_by", "to": "AMP", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "AMP", "label": "computes", "title": "computes", "to": "semantic manifold distance", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "Label-embedding", "label": "presented_in", "title": "presented_in", "to": "Akata et al. (2013)", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Label-embedding", "label": "supports", "title": "supports", "to": "attribute-based classification", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Single-example learning", "label": "presented_in", "title": "presented_in", "to": "Bart \u0026 Ullman (2005)", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Cluster kernels", "label": "presented_in", "title": "presented_in", "to": "Chapelle et al. (2002)", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "Zero-shot learning (ZSL)", "label": "relies_on", "title": "relies_on", "to": "knowledge transfer", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "Chapelle", "label": "authored", "title": "authored", "to": "Cluster kernels", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Weston", "label": "authored", "title": "authored", "to": "Cluster kernels", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Sch\u00f6lkopf", "label": "authored", "title": "authored", "to": "Cluster kernels", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Deng", "label": "authored", "title": "authored", "to": "Large-scale object classification", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Deng", "label": "authored", "title": "authored", "to": "ImageNet", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Ding", "label": "authored", "title": "authored", "to": "Large-scale object classification", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Ding", "label": "authored", "title": "authored", "to": "Learning higher-order graph structure", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Ding", "label": "published_in", "title": "published_in", "to": "Advances in Neural Information Processing Systems", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Label relation graphs", "label": "used_in", "title": "used_in", "to": "Large-scale object classification", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Dong", "label": "authored", "title": "authored", "to": "ImageNet", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Frome", "label": "authored", "title": "authored", "to": "Devise", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "Devise", "label": "is_a", "title": "is_a", "to": "embedding model", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Devise", "label": "presented_at", "title": "presented_at", "to": "Advances in Neural Information Processing Systems", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "Devise", "label": "implements", "title": "implements", "to": "visual-semantic embedding", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Transductive multi-view embedding", "label": "presented_at", "title": "presented_at", "to": "ECCV", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "Transductive multi-view embedding", "label": "aims_for", "title": "aims_for", "to": "zero-shot recognition", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "Zero shot recognition with unreliable attributes", "label": "published_as", "title": "published_as", "to": "arXiv preprint", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "ImageNet classification", "label": "presented_at", "title": "presented_at", "to": "NIPS", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "ImageNet classification", "label": "used", "title": "used", "to": "deep convolutional neural networks", "width": 3.79}, {"arrows": "to", "color": "#CC7700", "from": "ImageNet classification", "label": "uses", "title": "uses", "to": "SIFT features", "width": 3.25}, {"arrows": "to", "color": "#0077CC", "from": "Efficient estimation of word representations", "label": "published_as", "title": "published_as", "to": "arXiv preprint", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Sakrapee Paisitkriangkrai", "label": "author_of", "title": "author_of", "to": "Learning to rank in person re-identi\ufb01cation with metric ensembles", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Sakrapee Paisitkriangkrai", "label": "affiliated_with", "title": "affiliated_with", "to": "The University of Adelaide", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Sakrapee Paisitkriangkrai", "label": "affiliated_with", "title": "affiliated_with", "to": "Australian Centre for Robotic Vision", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Learning to rank in person re-identi\ufb01cation with metric ensembles", "label": "is_paper_of", "title": "is_paper_of", "to": "CVPR", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Learning to rank in person re-identi\ufb01cation with metric ensembles", "label": "published_in", "title": "published_in", "to": "2015", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "Learning to rank in person re-identi\ufb01cation with metric ensembles", "label": "file_name", "title": "file_name", "to": "Paisitkriangrai_Learning_to_Rank_2015_CVPR_paper.pdf", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Chunhua Shen", "label": "author_of", "title": "author_of", "to": "Learning to rank in person re-identi\ufb01cation with metric ensembles", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Chunhua Shen", "label": "affiliated_with", "title": "affiliated_with", "to": "The University of Adelaide", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Chunhua Shen", "label": "affiliated_with", "title": "affiliated_with", "to": "Australian Centre for Robotic Vision", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Chunhua Shen", "label": "affiliated_with", "title": "affiliated_with", "to": "The University of Adelaide, Australia", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Chunhua Shen", "label": "author_of", "title": "author_of", "to": "Supervised Discrete Hashing", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Chunhua Shen", "label": "affiliated_with", "title": "affiliated_with", "to": "University of Adelaide", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Chunhua Shen", "label": "author_of", "title": "author_of", "to": "Learning graph structure...", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Anton van den Hengel", "label": "author_of", "title": "author_of", "to": "Learning to rank in person re-identi\ufb01cation with metric ensembles", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Anton van den Hengel", "label": "affiliated_with", "title": "affiliated_with", "to": "The University of Adelaide, Australia", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Anton van den Hengel", "label": "affiliated_with", "title": "affiliated_with", "to": "Australian Centre for Robotic Vision", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Anton van den Hengel", "label": "is_author_of", "title": "is_author_of", "to": "Robust Multiple Homography Estimation", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Anton van den Hengel", "label": "author_of", "title": "author_of", "to": "Learning graph structure...", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Anton van den Hengel", "label": "affiliated_with", "title": "affiliated_with", "to": "University of Adelaide", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "predefined weights", "label": "are", "title": "are", "to": "not adaptable", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "two principled approaches", "label": "optimize", "title": "optimize", "to": "relative distance", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "two principled approaches", "label": "maximize", "title": "maximize", "to": "average rank-k recognition rate", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "RMS methods", "label": "improves", "title": "improves", "to": "rank-1 recognition rates", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Ensemble-based approaches", "label": "are", "title": "are", "to": "flexible", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Ensemble-based approaches", "label": "can be combined with", "title": "can be combined with", "to": "linear metrics", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Ensemble-based approaches", "label": "can be combined with", "title": "can be combined with", "to": "non-linear metrics", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Similarity metric", "label": "introduced in", "title": "introduced in", "to": "Chopra et al. (2005)", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "Person Re-Identification", "label": "is covered in", "title": "is covered in", "to": "Gong et al. (2014)", "width": 3.91}, {"arrows": "to", "color": "#CC7700", "from": "Person Re-Identification", "label": "relies on", "title": "relies on", "to": "Similarity metric", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Person Re-Identification", "label": "described_in", "title": "described_in", "to": "Gong, S., Crisitan, M., Yan, S., and Loy, C. C.", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Gong et al. (2014)", "label": "provides", "title": "provides", "to": "techniques", "width": 3.79}, {"arrows": "to", "color": "#CC7700", "from": "techniques", "label": "adopt", "title": "adopt", "to": "solution frameworks", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Gong, S., Crisitan, M., Yan, S., and Loy, C. C.", "label": "provides", "title": "provides", "to": "techniques", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "support vector method", "label": "relevant_to", "title": "relevant_to", "to": "performance measures", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "support vector method", "label": "authored", "title": "authored", "to": "Joachims, T.", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "re-identification models", "label": "builds_upon", "title": "builds_upon", "to": "AlexNet", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "re-identification systems", "label": "evaluated_using", "title": "evaluated_using", "to": "performance measures", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "Imaginet classification", "label": "utilized", "title": "utilized", "to": "deep convolutional neural networks", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Mahalanobis distance", "label": "is_technique", "title": "is_technique", "to": "metric learning", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Mahalanobis distance", "label": "is_used_in", "title": "is_used_in", "to": "person re-identification", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "metric learning", "label": "is_aspect_of", "title": "is_aspect_of", "to": "scalability", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "distance metric learning", "label": "addresses", "title": "addresses", "to": "computational challenges", "width": 3.55}, {"arrows": "to", "color": "#00CC77", "from": "distance metric learning", "label": "improves", "title": "improves", "to": "scalability", "width": 3.64}, {"arrows": "to", "color": "#00CC77", "from": "computational challenges", "label": "impacts", "title": "impacts", "to": "scalability", "width": 3.25}, {"arrows": "to", "color": "#0077CC", "from": "paper (Felzenszwalb et al., 2010)", "label": "introduces", "title": "introduces", "to": "part-based model", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "paper (Felzenszwalb et al., 2010)", "label": "addresses", "title": "addresses", "to": "object detection", "width": 3.4000000000000004}, {"arrows": "to", "color": "#CC7700", "from": "part-based model", "label": "relevant_to", "title": "relevant_to", "to": "pedestrian detection", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "image representation techniques", "label": "used_in", "title": "used_in", "to": "re-identification", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "shape and appearance information", "label": "is_strategy_for", "title": "is_strategy_for", "to": "robust re-identification", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "kernel methods", "label": "used_in", "title": "used_in", "to": "metric learning", "width": 3.7600000000000002}, {"arrows": "to", "color": "#CC7700", "from": "kernel methods", "label": "operates_on", "title": "operates_on", "to": "geodesic metric spaces", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "kernel methods", "label": "include", "title": "include", "to": "geodesic Laplacian kernels", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Australian Centre for Robotic Vision", "label": "located_in", "title": "located_in", "to": "Australia", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Australian Centre for Robotic Vision", "label": "part_of", "title": "part_of", "to": "University of Adelaide", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Adelaide, Australia", "label": "is_located_in", "title": "is_located_in", "to": "Australia", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "The University of Adelaide, Australia", "label": "located_in", "title": "located_in", "to": "Australia", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Dengxin Dai", "label": "author_of", "title": "author_of", "to": "Metric imitation by manifold transfer", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Dengxin Dai", "label": "affiliated_with", "title": "affiliated_with", "to": "Computer Vision Lab, ETH Zurich", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Metric imitation by manifold transfer", "label": "application_area", "title": "application_area", "to": "efficient vision applications", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Till Kroeger", "label": "author_of", "title": "author_of", "to": "Metric imitation by manifold transfer", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Radu Timofte", "label": "author_of", "title": "author_of", "to": "Metric imitation by manifold transfer", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Radu Timofte", "label": "affiliated_with", "title": "affiliated_with", "to": "Computer Vision Lab, ETH Zurich", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "Metric Imitation", "label": "aims_to", "title": "aims_to", "to": "improve performance", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Metric Imitation", "label": "transfers", "title": "transfers", "to": "manifold structure", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Metric Imitation", "label": "demonstrated_on", "title": "demonstrated_on", "to": "instance-based object retrieval", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Metric Imitation", "label": "demonstrated_on", "title": "demonstrated_on", "to": "image clustering", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Metric Imitation", "label": "demonstrated_on", "title": "demonstrated_on", "to": "category-based image retrieval", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Metric Imitation", "label": "yields", "title": "yields", "to": "better performance", "width": 3.79}, {"arrows": "to", "color": "#CC7700", "from": "vision applications", "label": "benefits_from", "title": "benefits_from", "to": "Metric Imitation", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "GIST features", "label": "used_as", "title": "used_as", "to": "target features", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "SIFT-llc", "label": "used_as", "title": "used_as", "to": "source features", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "object-bank", "label": "used_as", "title": "used_as", "to": "source features", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "CNN features", "label": "used_as", "title": "used_as", "to": "source features", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "CNN features", "label": "is_source_feature", "title": "is_source_feature", "to": "performance", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Metric Imitation (MI)", "label": "yields", "title": "yields", "to": "better performance", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Metric Imitation (MI)", "label": "compared_to", "title": "compared_to", "to": "original target features", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Bosch, A.", "label": "authored", "title": "authored", "to": "Image classification using random forests and ferns", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Chatfield, K.", "label": "authored", "title": "authored", "to": "Return of the devil in the details", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Dai, D.", "label": "authored", "title": "authored", "to": "Ensemble partitioning", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "llc", "label": "is_source_feature", "title": "is_source_feature", "to": "performance", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "Object Retrieval", "label": "relates_to", "title": "relates_to", "to": "Object-bank (OB)", "width": 3.25}, {"arrows": "to", "color": "#CC7700", "from": "Image Clustering", "label": "relates_to", "title": "relates_to", "to": "Ensemble partitioning", "width": 3.0999999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Dana et al.", "label": "authored", "title": "authored", "to": "Reflectance and texture of real-world surfaces", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Reflectance and texture of real-world surfaces", "label": "published in", "title": "published in", "to": "*ACM Trans. Graph.*", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Fei-Fei et al.", "label": "authored", "title": "authored", "to": "Learning generative visual models", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Learning generative visual models", "label": "presented at", "title": "presented at", "to": "Workshop on Generative-Model Based Vision", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Lazebnik et al.", "label": "authored", "title": "authored", "to": "Spatial pyramid matching", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Spatial pyramid matching", "label": "used for", "title": "used for", "to": "recognizing natural scene categories", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Li \u0026 Fei-Fei", "label": "authored", "title": "authored", "to": "What, where and who?", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "What, where and who?", "label": "authored", "title": "authored", "to": "Li, L.-J.", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "What, where and who?", "label": "published", "title": "published", "to": "ICCV", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "Li et al. (2010)", "label": "authored", "title": "authored", "to": "Object bank", "width": 3.91}, {"arrows": "to", "color": "#CC7700", "from": "Object bank", "label": "aims_to", "title": "aims_to", "to": "scene classification", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Object bank", "label": "published", "title": "published", "to": "NIPS", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Li, L.-J. et al. (2010)", "label": "published_in", "title": "published_in", "to": "*NIPS*", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Nist\u00b4er, D. \u0026 Stew\u00b4enius, H. (2006)", "label": "published_in", "title": "published_in", "to": "*CVPR*", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "Vocabulary tree", "label": "enables", "title": "enables", "to": "scalable recognition", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Oliva, A. \u0026 Torralba, A. (2001)", "label": "published_in", "title": "published_in", "to": "*IJCV*", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "Spatial envelope", "label": "represents", "title": "represents", "to": "shape of the scene", "width": 3.67}, {"arrows": "to", "color": "#0077CC", "from": "Tuytelaars, T. et al. (2009)", "label": "published_in", "title": "published_in", "to": "*IJCLP*", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "Object discovery", "label": "is", "title": "is", "to": "unsupervised", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Wang, J. et al. (2010)", "label": "published_in", "title": "published_in", "to": "*CVPR*", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "Locality-constrained linear coding", "label": "applied_to", "title": "applied_to", "to": "image classification", "width": 3.61}, {"arrows": "to", "color": "#0077CC", "from": "Computer Vision Lab, ETH Zurich", "label": "located_in", "title": "located_in", "to": "ETH Zurich", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Francesco Pittaluca", "label": "affiliated_with", "title": "affiliated_with", "to": "Computer Vision Lab, ETH Zurich", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Francesco Pittaluca", "label": "authored", "title": "authored", "to": "Privacy Preserving Optics for Miniature Vision Sensors", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "Sanjeev J. Koppal", "label": "affiliated_with", "title": "affiliated_with", "to": "Computer Vision Lab, ETH Zurich", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Sanjeev J. Koppal", "label": "authored", "title": "authored", "to": "Privacy Preserving Optics for Miniature Vision Sensors", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "Pittaluga_Privacy_Preserving_Optics_2015_CVPR_supplemental.pdf", "label": "is_supplement_to", "title": "is_supplement_to", "to": "Privacy Preserving Optics for Miniature Vision Sensors", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Privacy Preseving Optics for Miniature Vision Sensors", "label": "presented_at", "title": "presented_at", "to": "CVPR", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "supplementary material", "label": "for", "title": "for", "to": "privacy-preserving optics", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "privacy-preserving optics", "label": "designed for", "title": "designed for", "to": "minature vision sensors", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "impact of defocusing optics", "label": "on", "title": "on", "to": "performance of face recognition algorithms", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "angular support", "label": "for", "title": "for", "to": "FLIR One thermal sensor", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "angular support", "label": "for", "title": "for", "to": "Kinect time-of-flight sensor", "width": 3.82}, {"arrows": "to", "color": "#CC7700", "from": "FLIR One thermal sensor", "label": "related_to", "title": "related_to", "to": "Angular support derivation", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "Kinect time-of-flight sensor", "label": "related_to", "title": "related_to", "to": "Angular support derivation", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "effect of blur", "label": "on", "title": "on", "to": "face recognition rates", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "geometric derivations", "label": "for", "title": "for", "to": "determining angular support", "width": 3.79}, {"arrows": "to", "color": "#CC7700", "from": "Privacy-preserving optics", "label": "concerns", "title": "concerns", "to": "Facial images", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "Feret methodology", "label": "is_standard_for", "title": "is_standard_for", "to": "face recognition evaluation", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "CSU face identification evaluation system", "label": "describes", "title": "describes", "to": "Face identification", "width": 3.64}, {"arrows": "to", "color": "#CC7700", "from": "Angular support derivation", "label": "relevant_to", "title": "relevant_to", "to": "Sensor positioning", "width": 3.25}, {"arrows": "to", "color": "#0077CC", "from": "Bolme, D. S. et al.", "label": "authored", "title": "authored", "to": "CSU face identification evaluation system", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Newton, E. et al.", "label": "authored", "title": "authored", "to": "Privacy-preserving techniques", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Phillips, P. J. et al.", "label": "authored", "title": "authored", "to": "Feret evaluation methodology", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "Feret evaluation methodology", "label": "standard_for", "title": "standard_for", "to": "Face recognition evaluation", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Park, Min-Gyu", "label": "is_author_of", "title": "is_author_of", "to": "Leveraging Stereo Matching with Learning-based Con\ufb01dence Measures", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Park, Min-Gyu", "label": "affiliated_with", "title": "affiliated_with", "to": "University of Florida", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Yoon, Kuk-Jin", "label": "is_author_of", "title": "is_author_of", "to": "Leverging Stereo Matching with Learning-based Con\ufb01dence Measures", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Pittaluga, Francesco", "label": "is_author_of", "title": "is_author_of", "to": "P. J. The furet evaluation methodology", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Pittaluga, Francesco", "label": "affiliated_with", "title": "affiliated_with", "to": "University of Florida", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "Random Forests", "label": "is_machine_learning_algorithm", "title": "is_machine_learning_algorithm", "to": "algorithm", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Random Forests", "label": "is_a", "title": "is_a", "to": "machine learning algorithm", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Random Forests", "label": "is_used_in", "title": "is_used_in", "to": "machine learning", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Random Forests", "label": "is_published_in", "title": "is_published_in", "to": "Machine Learning", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Breiman, L.", "label": "authored", "title": "authored", "to": "Random forests", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Random forests", "label": "used for", "title": "used for", "to": "real time 3d face analysis", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Koppal, Sanjeev J.", "label": "affiliated_with", "title": "affiliated_with", "to": "University of Florida", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Breiman", "label": "authored", "title": "authored", "to": "Random Forests", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "stereo confidence metric", "label": "is_a", "title": "is_a", "to": "key aspect of stereo vision", "width": 3.88}, {"arrows": "to", "color": "#CC7700", "from": "stereo confidence metric", "label": "is_related_to", "title": "is_related_to", "to": "stereo vision", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "Egnal", "label": "authored", "title": "authored", "to": "stereo confidence metric", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Hirschm\u00fcller", "label": "presented", "title": "presented", "to": "semiglobal matching", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "semiglobal matching", "label": "uses", "title": "uses", "to": "mutual information", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "semiglobal matching", "label": "is_technique_for", "title": "is_technique_for", "to": "stereo processing", "width": 3.73}, {"arrows": "to", "color": "#CC7700", "from": "stereo vision", "label": "requires", "title": "requires", "to": "confidence measures", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Hu, X.", "label": "authored", "title": "authored", "to": "quantitative evaluation", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "quantitative evaluation", "label": "evaluates", "title": "evaluates", "to": "confidence measures", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "confidence measures", "label": "is_aspect_of", "title": "is_aspect_of", "to": "stereo vision", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "dense matching", "label": "occurs_in", "title": "occurs_in", "to": "complex scenes", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Manduchi, R.", "label": "proposed", "title": "proposed", "to": "distinctiveness maps", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "distinctiveness maps", "label": "is_technique_for", "title": "is_technique_for", "to": "image matching", "width": 3.88}, {"arrows": "to", "color": "#CC7700", "from": "image matching", "label": "utilizes", "title": "utilizes", "to": "distinctiveness maps", "width": 3.64}, {"arrows": "to", "color": "#CC7700", "from": "pose estimation", "label": "is_problem_of", "title": "is_problem_of", "to": "determining viewpoint", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "pose estimation", "label": "is_problem_in", "title": "is_problem_in", "to": "computer vision", "width": 3.7600000000000002}, {"arrows": "to", "color": "#CC7700", "from": "viewpoint", "label": "explains", "title": "explains", "to": "coarse pose", "width": 3.7600000000000002}, {"arrows": "to", "color": "#CC7700", "from": "keypoint prediction", "label": "captures", "title": "captures", "to": "finer details", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "constrained setting", "label": "has_input", "title": "has_input", "to": "bounding boxes", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "detection setting", "label": "is_more_challenging_than", "title": "is_more_challenging_than", "to": "constrained setting", "width": 3.88}, {"arrows": "to", "color": "#00CC77", "from": "viewpoint estimates", "label": "improves", "title": "improves", "to": "keypoint predictions", "width": 3.91}, {"arrows": "to", "color": "#00CC77", "from": "object characteristics", "label": "affects", "title": "affects", "to": "performance", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Tulsiani_Viewpoints_and_Keypoints_2015_CVPR_paper", "label": "authored_by", "title": "authored_by", "to": "Shubham Tulsiani", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Tulsiani_Viewpoints_and_Keypoints_2015_CVPR_paper", "label": "authored_by", "title": "authored_by", "to": "Jitendra Malik", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Shubham Tulsiani", "label": "affiliated_with", "title": "affiliated_with", "to": "University of California, Berkeley", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Shubham Tulsiani", "label": "email", "title": "email", "to": "shubhtuls@eecs.berkeley.edu", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Jitendra Malik", "label": "affiliates_with", "title": "affiliates_with", "to": "University of California, Berkeley", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Jitendra Malik", "label": "email", "title": "email", "to": "malik@eecs.berkeley.edu", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Jitendra Malik", "label": "contributed_to", "title": "contributed_to", "to": "Category-Speci\ufb01c Object Reconstruction from a Single Image", "width": 3.88}, {"arrows": "to", "color": "#CC7700", "from": "effort", "label": "aims_to", "title": "aims_to", "to": "goal", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "future efforts", "label": "guided_by", "title": "guided_by", "to": "analysis", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "analysis", "label": "focuses_on", "title": "focuses_on", "to": "error modes", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "analysis", "label": "examines", "title": "examines", "to": "effect", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "Pose Estimation", "label": "improves_via", "title": "improves_via", "to": "Convolutional Neural Networks (CNNs)", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "Pose Estimation", "label": "requires", "title": "requires", "to": "Keypoint Prediction", "width": 3.7600000000000002}, {"arrows": "to", "color": "#CC7700", "from": "Pose Estimation", "label": "requires", "title": "requires", "to": "Viewpoint Prediction", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Pose Estimation", "label": "affected_by", "title": "affected_by", "to": "Refraction", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "Abed Malti", "label": "author_of", "title": "author_of", "to": "Malti_A_Linear_Least-Squares_2015_CVPR_paper", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Abed Malti", "label": "affiliated_with", "title": "affiliated_with", "to": "Fluminance/INRIA", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Abed Malti", "label": "affiliation", "title": "affiliation", "to": "Fuminance/INRIA", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Adrien Bartoli", "label": "author_of", "title": "author_of", "to": "Maiti_A_Linear_Least-Squares_2015_CVPR_paper", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Adrien Bartoli", "label": "affiliation", "title": "affiliation", "to": "ALCoV/ISIT", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Richard Hartley", "label": "author_of", "title": "author_of", "to": "Malti_A_Linear_Least-Squares_2015_CVPR_paper", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Richard Hartley", "label": "affiliation", "title": "affiliation", "to": "Australian National University", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Richard Hartley", "label": "affiliation", "title": "affiliation", "to": "NICTA", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Shape-from-Template methods", "label": "struggles_with", "title": "struggles_with", "to": "balancing accuracy, speed, and robustness", "width": 3.7600000000000002}, {"arrows": "to", "color": "#CC7700", "from": "non-linear optimization", "label": "is_a", "title": "is_a", "to": "existing approach", "width": 3.64}, {"arrows": "to", "color": "#CC7700", "from": "Kalman filtering", "label": "is_a", "title": "is_a", "to": "existing approach", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "proposed solution", "label": "utilizes", "title": "utilizes", "to": "mechanical constraints", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "proposed solution", "label": "employs", "title": "employs", "to": "finite element methods", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "proposed solution", "label": "allows_for", "title": "allows_for", "to": "accurate reconstruction", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "proposed solution", "label": "offers", "title": "offers", "to": "efficient reconstruction", "width": 3.7600000000000002}, {"arrows": "to", "color": "#CC7700", "from": "proposed solution", "label": "is_a", "title": "is_a", "to": "linear least-squares SfT method", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "finite element methods", "label": "represents", "title": "represents", "to": "surface", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "finite element methods", "label": "represents", "title": "represents", "to": "deformation", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Shape-from-Template (SfT)", "label": "models", "title": "models", "to": "elastic deformations", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Shape-from-Template (SfT)", "label": "uses", "title": "uses", "to": "linear least-squares estimation", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "elastic deformations", "label": "requires", "title": "requires", "to": "accurate reconstruction", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "elastic deformations", "label": "is_modeled_by", "title": "is_modeled_by", "to": "fully linear least-squares SfT method", "width": 3.61}, {"arrows": "to", "color": "#0077CC", "from": "Finite Element Methods (FEM)", "label": "codes", "title": "codes", "to": "non-rigid EKF monocular SLAM", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Finite Element Methods (FEM)", "label": "is_used_in", "title": "is_used_in", "to": "sequential bayesian non-rigid structure from motion", "width": 3.67}, {"arrows": "to", "color": "#0077CC", "from": "Agudo, A.", "label": "authored", "title": "authored", "to": "FEM models to code non-rigid EKF monocular SLAM", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Agudo, A.", "label": "authored", "title": "authored", "to": "Finite element based sequential bayesian non-rigid structure from motion", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Finite element based sequential bayesian non-rigid structure from motion", "label": "is_cited", "title": "is_cited", "to": "frequently", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "FEM models", "label": "is_cited", "title": "is_cited", "to": "frequently", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Agudo et al.", "label": "cited_by", "title": "cited_by", "to": "related work", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "Agudo et al.", "label": "addresses", "title": "addresses", "to": "structure from motion", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "Bartoli et al.", "label": "cited_by", "title": "cited_by", "to": "methodology", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "Bartoli et al.", "label": "addresses", "title": "addresses", "to": "surface reconstruction", "width": 3.34}, {"arrows": "to", "color": "#0077CC", "from": "Salzmann and Urtasun", "label": "related_to", "title": "related_to", "to": "approach", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Moreno-Noguer and Porta", "label": "related_to", "title": "related_to", "to": "approach", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "Moreno-Noguer and Porta", "label": "addresses", "title": "addresses", "to": "shape recovery", "width": 3.16}, {"arrows": "to", "color": "#0077CC", "from": "Finite Element Methods", "label": "described_in", "title": "described_in", "to": "Chaskalovic", "width": 3.88}, {"arrows": "to", "color": "#CC7700", "from": "Salzmann and Urutasun", "label": "addresses", "title": "addresses", "to": "3D reconstruction", "width": 3.25}, {"arrows": "to", "color": "#0077CC", "from": "ape recovery", "label": "related_to", "title": "related_to", "to": "CVPR", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Chaskaloric", "label": "authored", "title": "authored", "to": "Finite Elements Methods for Engineering Sciences", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Finite Elements Methods for Engineering Sciences", "label": "provides", "title": "provides", "to": "background knowledge", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Reconstructing sharply folding surfaces", "label": "presented_at", "title": "presented_at", "to": "CVPR", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Salzmann, M., and Fua, P.", "label": "authored", "title": "authored", "to": "Linear local models for monocular reconstruction", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Linear local models for monocular reconstruction", "label": "published_in", "title": "published_in", "to": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Matthews, I., and Baker, S.", "label": "authored", "title": "authored", "to": "Active appearance models revisited", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "Active appearance models revisited", "label": "related_to", "title": "related_to", "to": "modeling techniques", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "appearance models", "label": "revisited in", "title": "revisited in", "to": "International Journal of Computer Vision", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "appearance models", "label": "of", "title": "of", "to": "familiar objects", "width": 3.91}, {"arrows": "to", "color": "#CC7700", "from": "Riemannian Coding", "label": "uses", "title": "uses", "to": "Kernels", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Mehrtash Harandi", "label": "author of", "title": "author of", "to": "Riemannian Coding and Dictionary Learning", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "covariance descriptors", "label": "lies_on", "title": "lies_on", "to": "Riemannian manifolds", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "Riemannian manifolds", "label": "relevant to", "title": "relevant to", "to": "geodesic Laplacian kernels", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "normalized histograms", "label": "lies_on", "title": "lies_on", "to": "Riemannian manifolds", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "linear subspaces", "label": "lies_on", "title": "lies_on", "to": "Riemannianmanifolds", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "2D shape outlines", "label": "lies_on", "title": "lies_on", "to": "Riemannian manifolds", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "existing solutions", "label": "are", "title": "are", "to": "dedicated to specific manifolds", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "existing solutions", "label": "rely_on", "title": "rely_on", "to": "optimization problems", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "optimization problems", "label": "are", "title": "are", "to": "difficult to solve", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "general Riemannian coding framework", "label": "has_counterpart", "title": "has_counterpart", "to": "kernel-based counterpart", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "kernel-based counterpart", "label": "allows_for", "title": "allows_for", "to": "generalization beyond sparse coding", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "kernel-based counterpart", "label": "related_to", "title": "related_to", "to": "kernel parameters", "width": 3.52}, {"arrows": "to", "color": "#CC7700", "from": "Riemannian coding framework", "label": "has_counterpart", "title": "has_counterpart", "to": "kernel-based counterpart", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Riemannian coding framework", "label": "allows", "title": "allows", "to": "generalization beyond sparse coding", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Riemannian coding framework", "label": "enables", "title": "enables", "to": "learning of kernel parameters", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Riemannian coding framework", "label": "simplifies", "title": "simplifies", "to": "dictionary learning", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "sparse coding", "label": "limited_by", "title": "limited_by", "to": "flat data", "width": 3.46}, {"arrows": "to", "color": "#CC7700", "from": "coding schemes", "label": "require", "title": "require", "to": "efficient solutions", "width": 3.4000000000000004}, {"arrows": "to", "color": "#CC7700", "from": "Riemannian Manifolds", "label": "is_topic_of", "title": "is_topic_of", "to": "cvpr_papers", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "Dictionary Learning", "label": "is_topic_of", "title": "is_topic_of", "to": "cvpr_papers", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "Coding Theory", "label": "is_topic_of", "title": "is_topic_of", "to": "cvpr_papers", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Mehrtas Harandi", "label": "affiliated_with", "title": "affiliated_with", "to": "Australian National University", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Mehrtas Harandi", "label": "affiliated_with", "title": "affiliated_with", "to": "NICITA", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Yuting Zhang", "label": "author_of", "title": "author_of", "to": "Improving Object Detection", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Yuting Zhang", "label": "affiliated_with", "title": "affiliated_with", "to": "Department of Computer Science, Zhejiang University", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Improving Object Detection", "label": "presented_at", "title": "presented_at", "to": "CVPR", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Kihyuk Sohn", "label": "author_of", "title": "author_of", "to": "Improving ObjectDetection", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Kihyuk Sohn", "label": "affiliated_with", "title": "affiliated_with", "to": "Department of Electrical Engineering and Computer Science, University of Michigan", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Ruben Villegas", "label": "author_of", "title": "author_of", "to": "Improving Object Detection", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Ruben Villegas", "label": "affiliated_with", "title": "affiliated_with", "to": "Department of Electrical Engineering and Computer Science, University of Michigan", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Gang Pan", "label": "author_of", "title": "author_of", "to": "Improving Object Detection", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Gang Pan", "label": "affiliated_with", "title": "affiliated_with", "to": "Department of Computer Science, Zhejiang University", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Honglak Lee", "label": "author_of", "title": "author_of", "to": "Improving Object Detection", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Honglak Lee", "label": "affiliated_with", "title": "affiliated_with", "to": "Department of Electrical Engineering and Computer Science, University of Michigan", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "inaccurate localization", "label": "is", "title": "is", "to": "major source of error", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "search algorithm", "label": "based_on", "title": "based_on", "to": "Bayesian optimization", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "search algorithm", "label": "proposes", "title": "proposes", "to": "candidate regions", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "structured loss", "label": "penalizes", "title": "penalizes", "to": "localization inaccuracy", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "methods", "label": "improves", "title": "improves", "to": "detection performance", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "methods", "label": "improves_over", "title": "improves_over", "to": "baseline method", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "methods", "label": "impose", "title": "impose", "to": "prior over human poses", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "methods", "label": "extend to", "title": "extend to", "to": "object search", "width": 3.79}, {"arrows": "to", "color": "#CC7700", "from": "methods", "label": "are", "title": "are", "to": "state-of-the-art", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "loss", "label": "penalizes", "title": "penalizes", "to": "localization inaccuracy", "width": 3.94}, {"arrows": "to", "color": "#00CC77", "from": "proposed methods", "label": "improves", "title": "improves", "to": "detection performance", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "proposed methods", "label": "performs_better_than", "title": "performs_better_than", "to": "baseline method", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "proposed methods", "label": "validates", "title": "validates", "to": "effectiveness", "width": 3.64}, {"arrows": "to", "color": "#CC7700", "from": "two methods", "label": "are", "title": "are", "to": "complementary", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "combined methods", "label": "outperforms", "title": "outperforms", "to": "state-of-the-art", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "Bayesian Optimization", "label": "used_in", "title": "used_in", "to": "Object Detection", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "Structured Prediction (Structured SVM)", "label": "used_in", "title": "used_in", "to": "Object Detection", "width": 3.46}, {"arrows": "to", "color": "#0077CC", "from": "Localization Accuracy", "label": "metric_for", "title": "metric_for", "to": "Object Detection", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Deep Networks", "label": "trained_using", "title": "trained_using", "to": "Greedy Layer-Wise Training", "width": 3.64}, {"arrows": "to", "color": "#CC7700", "from": "Deep Networks", "label": "performs", "title": "performs", "to": "Local Estimation", "width": 3.46}, {"arrows": "to", "color": "#CC7700", "from": "Deep Networks", "label": "performs", "title": "performs", "to": "Global Search", "width": 3.46}, {"arrows": "to", "color": "#0077CC", "from": "Local Binary Patterns", "label": "applied_to", "title": "applied_to", "to": "Face Recognition", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Representation Learning", "label": "is_publication_of", "title": "is_publication_of", "to": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Bengio, Y.", "label": "authors", "title": "authors", "to": "Representation Learning", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Bengio, Y.", "label": "authored", "title": "authored", "to": "Learning deep architectures for AI", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Girschick, R.", "label": "authors", "title": "authors", "to": "Rich Feature Hierarchies", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Rich Feature Hierarchies", "label": "improves", "title": "improves", "to": "Object Detection", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Rich Feature Hierarchies", "label": "improves", "title": "improves", "to": "Semantic Segmentation", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Everingham, M.", "label": "organizes", "title": "organizes", "to": "VOC2007", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Everingham, M.", "label": "authored", "title": "authored", "to": "PASCAL VOC challenge description", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "VOC2007", "label": "is_a", "title": "is_a", "to": "Visual Object Classes Challenge", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "VOC2007", "label": "is_challenge_of", "title": "is_challenge_of", "to": "Pascal Network", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Deng, J.", "label": "authors", "title": "authors", "to": "ImageNet", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Donahue, J.", "label": "authors", "title": "authors", "to": "DeCAF", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "DeCAF", "label": "is_for", "title": "is_for", "to": "Visual Recognition", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "DeCAF", "label": "published_in", "title": "published_in", "to": "CoRR", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "DeCAF", "label": "author", "title": "author", "to": "Donahue, J.", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Erhan", "label": "author", "title": "author", "to": "Erhan, D.", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Erhan", "label": "presented_at", "title": "presented_at", "to": "CVPR", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Dongping Li", "label": "author", "title": "author", "to": "A Geodesic-Prepreserving Method for Image Warping", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Dongping Li", "label": "is_author_of", "title": "is_author_of", "to": "A Geodesic-Preserving Method for Image Warping", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Dongping Li", "label": "affiliated_with", "title": "affiliated_with", "to": "Zhejiang University", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Department of Electrical Engineering and Computer Science", "label": "is_affiliation_of", "title": "is_affiliation_of", "to": "University of Michigan", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "A Geodesic-Preserving Method for Image Warping", "label": "is_publication_of", "title": "is_publication_of", "to": "CVPR", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "A Geodesic-Preserving Method for Image Warping", "label": "addresses", "title": "addresses", "to": "Image Warping", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "Kun Zhou", "label": "is_author_of", "title": "is_author_of", "to": "A Geodesic-Preserving Method for Image Warping", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Kun Zhou", "label": "affiliated_with", "title": "affiliated_with", "to": "Zhejiang University", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "honglak@umich.edu", "label": "is_email_of", "title": "is_email_of", "to": "University of Michigan", "width": 3.25}, {"arrows": "to", "color": "#0077CC", "from": "Li_A_Geodesic-Preserving_Method_2015_CVPR_supplemental.pdf", "label": "is_related_to", "title": "is_related_to", "to": "A Geodesic-Preserving Method for Image Warping", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Image Warping", "label": "achieves", "title": "achieves", "to": "high-quality warped images", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "geodesic-preserving method", "label": "aims to", "title": "aims to", "to": "maintain shape", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "geodesic-preserving method", "label": "minimizes", "title": "minimizes", "to": "distortions", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "core of method", "label": "lies in", "title": "lies in", "to": "preserving geodesic distances", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "preserving geodesic distances", "label": "ensures", "title": "ensures", "to": "local smoothness", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "preserving geodesic distances", "label": "prevents", "title": "prevents", "to": "unwanted artifacts", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "energy function", "label": "incorporates", "title": "incorporates", "to": "shape preservation terms", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "energy function", "label": "incorporates", "title": "incorporates", "to": "boundary preservation terms", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "energy function", "label": "incorporates", "title": "incorporates", "to": "geodesic preservation terms", "width": 3.82}, {"arrows": "to", "color": "#CC7700", "from": "Gauss-Newton method", "label": "is_a", "title": "is_a", "to": "optimization technique", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "Shape and Boundary Preservation", "label": "is_method_of", "title": "is_method_of", "to": "Image Warping", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "Energy Minimization", "label": "is_technique_for", "title": "is_technique_for", "to": "Image Warping", "width": 3.4000000000000004}, {"arrows": "to", "color": "#CC7700", "from": "Gausss-Newton Method", "label": "is_method_for", "title": "is_method_for", "to": "Energy Minimization", "width": 3.25}, {"arrows": "to", "color": "#0077CC", "from": "Zhang, G. et al.", "label": "authored", "title": "authored", "to": "A shape-preserving approach to image resizing", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "A shape-preserivng approach to image resizing", "label": "addresses", "title": "addresses", "to": "Shape and Boundary Preservation", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Rotation matrix R\u03b8,\u03c6", "label": "is_component_of", "title": "is_component_of", "to": "Eqn. (1)", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Shape-preserving term ES(V)", "label": "is_component_of", "title": "is_component_of", "to": "Eqn. (7)", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "shape-preserving approach", "label": "uses", "title": "uses", "to": "Eqn. (7) - Shape-preserving term ES(V)", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Eqn. (7) - Shape-preserving term ES(V)", "label": "is_part_of", "title": "is_part_of", "to": "shape-preserving approach", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "local smoothness preservation", "label": "described_by", "title": "described_by", "to": "Eqn. (4) - Local smoothness preservation EC(V)", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Eqn. (4) - Local smoothness preservation EC(V)", "label": "contributes_to", "title": "contributes_to", "to": "local smoothness preservation", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Eqn. (5) - Combined energy function E(V)", "label": "represents", "title": "represents", "to": "overall energy function", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Q", "label": "relies_on", "title": "relies_on", "to": "orthogonal matrices", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "orthogonal matrices", "label": "is_a", "title": "is_a", "to": "mathematical concept", "width": 3.64}, {"arrows": "to", "color": "#CC7700", "from": "orthogonal matrices", "label": "is_concept_in", "title": "is_concept_in", "to": "mathematics", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "E(V)", "label": "combines", "title": "combines", "to": "various terms", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Antonio Agudo", "label": "affiliated_with", "title": "affiliated_with", "to": "Microsoft Research", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Antonio Agudo", "label": "is_author_of", "title": "is_author_of", "to": "Simultaneous Pose and Non-Rigid Shape with Particle Dynamics", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Antonio Agudo", "label": "affiliated_with", "title": "affiliated_with", "to": "Instituto de Invesigaci\u00b4on en Ingenier\u00b4\u0131a de Arag\u00b4on (I3A)", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Francesc Moreno-Noguer", "label": "affiliated_with", "title": "affiliated_with", "to": "Microsoft Research", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Francesc Moreno-Noguer", "label": "is_author_of", "title": "is_author_of", "to": "Simultaneous Pose and Non-Rigid Shape with Particle Dynamics", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Francesc Moreno-Noguer", "label": "affiliated_with", "title": "affiliated_with", "to": "Institut de Rob`otica i Inform`atica Industrial (CSI-UPC)", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Simultaneous Pose and Non-Rigid Shape", "label": "uses", "title": "uses", "to": "particle dynamics", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "particle dynamics", "label": "used_in", "title": "used_in", "to": "Simultaneous Pose and Non-Rigid Shape", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "geodesic preservation", "label": "is_concept_in", "title": "is_concept_in", "to": "geometry", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Simultaneous Pose and Non-Rigid Shape with Particle Dynamics", "label": "presented_at", "title": "presented_at", "to": "CVPR", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Simultaneous Pose and Non-Rigid Shape with Particle Dynamics", "label": "deals_with", "title": "deals_with", "to": "particle dynamics", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Simultaneous Pose and Non-Rigid Shape with Particle Dynamics", "label": "addresses", "title": "addresses", "to": "non-rigid shape", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Simultaneous Pose and Non-Rigid Shape with Particle Dynamics", "label": "concerns", "title": "concerns", "to": "simultaneous pose", "width": 3.64}, {"arrows": "to", "color": "#CC7700", "from": "Simultaneous Pose and Non-Rigid Shape with Particle Dynamics", "label": "is_a", "title": "is_a", "to": "CVPR paper", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "non-rigid shape", "label": "requires", "title": "requires", "to": "particle dynamics", "width": 3.25}, {"arrows": "to", "color": "#0077CC", "from": "solution", "label": "estimates", "title": "estimates", "to": "camera pose", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "solution", "label": "solves", "title": "solves", "to": "absolute pose problem", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "solution", "label": "computes", "title": "computes", "to": "camera position", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "solution", "label": "computes", "title": "computes", "to": "camera orientation", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "solution", "label": "computes", "title": "computes", "to": "translational velocity", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "solution", "label": "computes", "title": "computes", "to": "angular velocity", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "solution", "label": "improves", "title": "improves", "to": "number of inliers", "width": 3.61}, {"arrows": "to", "color": "#0077CC", "from": "solution", "label": "uses", "title": "uses", "to": "RANSA", "width": 3.58}, {"arrows": "to", "color": "#0077CC", "from": "solution", "label": "uses", "title": "uses", "to": "spatiotemporal filters", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "solution", "label": "uses", "title": "uses", "to": "metric-learning framework", "width": 3.91}, {"arrows": "to", "color": "#CC7700", "from": "object", "label": "modeled as", "title": "modeled as", "to": "ensemble of particles", "width": 3.88}, {"arrows": "to", "color": "#CC7700", "from": "object", "label": "has_relationship", "title": "has_relationship", "to": "annotation proposals", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "object", "label": "is_unseen", "title": "is_unseen", "to": "object", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "particle", "label": "ruled by", "title": "ruled by", "to": "Newton\u2019s second law of motion", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "dynamic model", "label": "incorporated into", "title": "incorporated into", "to": "bundle adjustment framework", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "validation", "label": "occurs_in", "title": "occurs_in", "to": "real video sequences", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "motion", "label": "is", "title": "is", "to": "articulated", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "motion", "label": "is", "title": "is", "to": "non-rigid", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "shapes", "label": "are", "title": "are", "to": "continuous", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "shapes", "label": "are", "title": "are", "to": "discontinuous", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "batch methods", "label": "are", "title": "are", "to": "computationally expensive", "width": 3.73}, {"arrows": "to", "color": "#0077CC", "from": "System", "label": "performs_comparable_to", "title": "performs_comparable_to", "to": "Competing Batch Methods", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "System", "label": "avoids", "title": "avoids", "to": "Uncanny Valley Effect", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "System", "label": "evaluated_with", "title": "evaluated_with", "to": "Qualitative Evaluations", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Non-Rigid Structure from Motion", "label": "is_topic_of", "title": "is_topic_of", "to": "Paper", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "Particle Dynamics", "label": "is_related_to", "title": "is_related_to", "to": "Non-Rigid Structure from Motion", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "Bundle Adjustment", "label": "is_related_to", "title": "is_related_to", "to": "Non-Rigid Structure from Motion", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "Monocular Video Analysis", "label": "is_related_to", "title": "is_related_to", "to": "Non-Rigid Structure from Motion", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Instituto de Invesigaci\u00b4on en Ingenier\u00b4\u0131a de Arag\u00b4on (I3a)", "label": "located_in", "title": "located_in", "to": "Universidad de Zaragoza", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Simone Frintrop", "label": "is_author_of", "title": "is_author_of", "to": "Paper", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Simone Frintrop", "label": "affiliated_with", "title": "affiliated_with", "to": "Rheinische Friedrich-Wilhelms-Universit\u00a8at Bonn", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Simone Frintrop", "label": "affiliated_with", "title": "affiliated_with", "to": "Rheinische Friedrich-Wilhelms-Universit\u00e4t Bonn", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Simone Frintrop", "label": "email", "title": "email", "to": "frintrop@iai.uni-bonn.de", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Thomas Werner", "label": "is_author_of", "title": "is_author_of", "to": "Paper", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Thomas Werner", "label": "affiliated_with", "title": "affiliated_with", "to": "Rheinische Friedrich-Wilhelms-Universit\u00a8at Bonn", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Thomas Werner", "label": "affiliated_with", "title": "affiliated_with", "to": "Rheinische Friedrich-Wilhelms-Universit\u00e4t Bonn", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Germ\u00e1n M. Garc\u00eda", "label": "is_author_of", "title": "is_author_of", "to": "Paper", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Germ\u00e1n M. Garc\u00eda", "label": "affiliated_with", "title": "affiliated_with", "to": "Rheinische Friedrich-Wilhelms-Universit\u00e4t Bonn", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "salience model", "label": "proposed by", "title": "proposed by", "to": "Itti et al.", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "adaptations", "label": "concerns", "title": "concerns", "to": "scale-space structure", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "scale-space structure", "label": "introduces", "title": "introduces", "to": "twin pyramid", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "foundational approaches", "label": "requires", "title": "requires", "to": "adaptation", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "VOCUS2", "label": "is a", "title": "is a", "to": "system", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "ration framework", "label": "produces", "title": "produces", "to": "segment-based salience maps", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "ration framework", "label": "aims_for", "title": "aims_for", "to": "high performance", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "segment-based salience maps", "label": "achieves", "title": "achieves", "to": "state-of-the-art performance", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "segment-based salience maps", "label": "used_in", "title": "used_in", "to": "modern applications", "width": 3.4000000000000004}, {"arrows": "to", "color": "#CC7700", "from": "importance", "label": "related_to", "title": "related_to", "to": "revisiting foundational approaches", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "adaptation", "label": "for", "title": "for", "to": "modern applications", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "t-based salience maps", "label": "achieves", "title": "achieves", "to": "state-of-the-art performance", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "Saliency Models", "label": "includes", "title": "includes", "to": "Itti Model", "width": 3.4000000000000004}, {"arrows": "to", "color": "#CC7700", "from": "Itti Model", "label": "is_related_to", "title": "is_related_to", "to": "Computer Vision", "width": 3.25}, {"arrows": "to", "color": "#0077CC", "from": "A cognitive approach for object discovery", "label": "presented_in", "title": "presented_in", "to": "ICPR", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Discriminant salieny", "label": "presented_in", "title": "presented_in", "to": "TPAMI", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "State-of-the-art in visual attention modeling", "label": "authored_by", "title": "authored_by", "to": "A. Borji", "width": 3.4000000000000004}, {"arrows": "to", "color": "#CC7700", "from": "Scale-space representation", "label": "is_used_in", "title": "is_used_in", "to": "Computer Vision", "width": 3.0999999999999996}, {"arrows": "to", "color": "#0077CC", "from": "asri", "label": "authored", "title": "authored", "to": "Image segmentation", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "L. Itti", "label": "developed", "title": "developed", "to": "salienicy-based visual attention model", "width": 3.9699999999999998}, {"arrows": "to", "color": "#CC7700", "from": "N. D. B. Bruce", "label": "proposed", "title": "proposed", "to": "information theoretic approach", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "L. Hurvich", "label": "proposed", "title": "proposed", "to": "opponent-process theory", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Rheinische Friedrich-Wilhelms-Universit\u00e4t Bonn", "label": "location", "title": "location", "to": "Bonn", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "Jiaolong Yang", "label": "author_of", "title": "author_of", "to": "Dense, Accurate Optical Flow Estimation", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Jiaolong Yang", "label": "affiliated_with", "title": "affiliated_with", "to": "Beijing Lab of Intelligent Information Technology", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Jiaolong Yang", "label": "affiliated_with", "title": "affiliated_with", "to": "Beijing Institute of Technology", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Dense, Accurate Optical Flow Estimation", "label": "publication_venue", "title": "publication_venue", "to": "CVPR", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Dense, Accurate Optical Flow Estimation", "label": "year", "title": "year", "to": "2015", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Yang_Dense_Accurate_Optical_2015_CVPR_paper", "label": "is_a", "title": "is_a", "to": "PDF document", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "multi-model fitting scheme", "label": "achieved_by", "title": "achieved_by", "to": "energy minimization", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Optical flow benchmarks", "label": "include", "title": "include", "to": "KITTI", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Optical flow benchmarks", "label": "include", "title": "include", "to": "MPI Sintel", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "KITTI", "label": "is_benchmark_for", "title": "is_benchmark_for", "to": "Optical Flow", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "KITTI", "label": "is_benchmark_for", "title": "is_benchmark_for", "to": "Scene Flow", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "Optical flow estimation", "label": "uses", "title": "uses", "to": "Piecewise parametric models", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "Optical flow estimation", "label": "uses", "title": "uses", "to": "Energy minimization", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "Homography transformation", "label": "used_in", "title": "used_in", "to": "Optical flow estimation", "width": 3.25}, {"arrows": "to", "color": "#0077CC", "from": "Baker et al. (2011)", "label": "published", "title": "published", "to": "database and evaluation methodology", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Bao et al. (2014)", "label": "published", "title": "published", "to": "Fast edge-preserving patchmatch", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Barnes et al. (2009)", "label": "published", "title": "published", "to": "PatchMatch", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "PatchMatch", "label": "addresses", "title": "addresses", "to": "structural image editing", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "PatchMatch", "label": "presented_in", "title": "presented_in", "to": "ACM Transactions on Graphics (TOG)", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Piecewise image registration", "label": "presented_in", "title": "presented_in", "to": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "Multiway cut", "label": "used_for", "title": "used_for", "to": "stereo and motion", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "Multiway cut", "label": "handles", "title": "handles", "to": "slanted surfaces", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "robust estimation", "label": "addresses", "title": "addresses", "to": "multiple motions", "width": 3.7600000000000002}, {"arrows": "to", "color": "#CC7700", "from": "robust estimation", "label": "produces", "title": "produces", "to": "parametric flow fields", "width": 3.64}, {"arrows": "to", "color": "#CC7700", "from": "robust estimation", "label": "produces", "title": "produces", "to": "piecewise-smooth flow fields", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Black", "label": "coauthored_with", "title": "coauthored_with", "to": "Anandan", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Black, M. J.", "label": "co-authored", "title": "co-authored", "to": "The robust estimation of multiple motions", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Black, M. J.", "label": "co-authored", "title": "co-authored", "to": "Estimating optical flow in segmented images", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Anandan, P.", "label": "co-authored", "title": "co-authored", "to": "The robust estimation of multiple motions", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Jepson, A. D.", "label": "co-authored", "title": "co-authored", "to": "Estimating optical flow in segmented images", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Bleyer, M.", "label": "co-authored", "title": "co-authored", "to": "PatchMatch Stereo", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Rhemann, C.", "label": "co-authored", "title": "co-authored", "to": "PatchMatch Stereo", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Rother, C.", "label": "co-authored", "title": "co-authored", "to": "PatchMatch Stere", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Braux-Zin et al.", "label": "authored", "title": "authored", "to": "A general dense image matching framework", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Beijing Lab of Intelligent Information Technology", "label": "is part of", "title": "is part of", "to": "School of Computer Science", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Thomas Mauthner", "label": "authored", "title": "authored", "to": "Encoding Based Saliency Detection for Videos and Images", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Horst Possegger", "label": "authored", "title": "authored", "to": "Encoding Based Saliency Detection for Videos and Images", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Horst Possegger", "label": "contributed_to", "title": "contributed_to", "to": "Mauthner_Encoding_Based_Saliency_2015_CVPR_paper", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Georg Waltner", "label": "authored", "title": "authored", "to": "Encoding Based Saliency Detection for Videos and Images", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Georg Waltner", "label": "contributed_to", "title": "contributed_to", "to": "Mauthner_Encoding_Based_Salieney_2015_CVPR_paper", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Georg Waltner", "label": "is_affiliated_with", "title": "is_affiliated_with", "to": "Institute for Computer Graphics and Vision", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Georg Waltner", "label": "has_email", "title": "has_email", "to": "waltner@icg.tugraz.at", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Horst Bischof", "label": "authored", "title": "authored", "to": "Encoding Based Saliency Detection for Videos and Images", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Horst Bischof", "label": "contributed_to", "title": "contributed_to", "to": "Mauthne_Encoding_Based_Salieney_2015_CVPR_paper", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Horst Bischof", "label": "has_email", "title": "has_email", "to": "bischof@icg.tugraz.at", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Mauthner_Encoding_Based_Saliency_2015_CVPR_paper", "label": "is_a", "title": "is_a", "to": "CVPR paper", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "eye-traking data", "label": "used_in", "title": "used_in", "to": "predicting human gaze", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "reliance on human gaze", "label": "introduces", "title": "introduces", "to": "bias", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "encoding method", "label": "approximates", "title": "approximates", "to": "joint feature distributions", "width": 3.61}, {"arrows": "to", "color": "#0077CC", "from": "salience computation", "label": "is", "title": "is", "to": "efficient", "width": 3.58}, {"arrows": "to", "color": "#CC7700", "from": "salience computation", "label": "enforces", "title": "enforces", "to": "Gestalt principle of figure-ground segregation", "width": 3.52}, {"arrows": "to", "color": "#CC7700", "from": "re-ground segregation", "label": "is", "title": "is", "to": "method", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Video Salience Detection", "label": "uses", "title": "uses", "to": "Gestalt Principles", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Video Salience Detection", "label": "employs", "title": "employs", "to": "Encoding Methods", "width": 3.4000000000000004}, {"arrows": "to", "color": "#CC7700", "from": "Video Salience Detection", "label": "related_to", "title": "related_to", "to": "Human Activity Recognition", "width": 3.25}, {"arrows": "to", "color": "#0077CC", "from": "Itti, L.", "label": "authored", "title": "authored", "to": "model of salience-based visual attention", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Alexe, B.", "label": "authored", "title": "authored", "to": "What is an object?", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Liu, T.", "label": "authored", "title": "authored", "to": "Learning to Detect A Salient Object", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Johansson, G.", "label": "authored", "title": "authored", "to": "model for analysis of biological motion", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Gorelick, L.", "label": "authored", "title": "authored", "to": "Actions as Space-Time Shapes", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Gorelick", "label": "authored", "title": "authored", "to": "Actions as Space-Time Shapes", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Blank", "label": "authored", "title": "authored", "to": "Actions as Space-Time Shapes", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Shechtman", "label": "authored", "title": "authored", "to": "Actions as Space-Time Shapes", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Irani", "label": "authored", "title": "authored", "to": "Actions as Space-Time Shapes", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Basri", "label": "authored", "title": "authored", "to": "Actions as Space-Time Shapes", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Borji", "label": "authored", "title": "authored", "to": "Salient Object Detection: A Benchmark", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Sihte", "label": "authored", "title": "authored", "to": "Salient Object Determination: A Benchmark", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Itti", "label": "authored", "title": "authored", "to": "Salient Object Detection: A Benchmark", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Guo", "label": "authored", "title": "authored", "to": "Spatio-temporal Saliency detection", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Zhang", "label": "authored", "title": "authored", "to": "Spatio-temporal Saliency detection", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Judd", "label": "authored", "title": "authored", "to": "Learning to Predict Where Humans Look", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Ehinger", "label": "authored", "title": "authored", "to": "Learning to Predict Where Humans Look", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Durand", "label": "authored", "title": "authored", "to": "Learning to Predict Where Humans Look", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Harel", "label": "authored", "title": "authored", "to": "Graph-based Visual Saliency", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Koch", "label": "authored", "title": "authored", "to": "Graph-based Visual Saliency", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Perona", "label": "authored", "title": "authored", "to": "Graph-based Visual Saliency", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Perona", "label": "co-developed", "title": "co-developed", "to": "Caltech-256 object category dataset", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Rahtu", "label": "authored", "title": "authored", "to": "Segmenting Salient Objects from Images and Videos", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Kannala", "label": "authored", "title": "authored", "to": "Segmenting Salient Objects from Images and Videos", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Salo", "label": "authored", "title": "authored", "to": "Segmenting Salient Objects from Images and Videos", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Heikkil\u00a8a", "label": "authored", "title": "authored", "to": "Segmenting Salient Objects from Images and Videos", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Mauthner", "label": "affiliated_with", "title": "affiliated_with", "to": "Institute for Computer Graphics and Vision, Graz University of Technology", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Possegger", "label": "affiliated_with", "title": "affiliated_with", "to": "Institute for Computer Graphics and Vision, Graz University of Technology", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Waltner", "label": "affiliated_with", "title": "affiliated_with", "to": "Institute for Computer Graphics and Vision, Graz University of Technology", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Institute for Computer Graphics and Vision", "label": "is_part_of", "title": "is_part_of", "to": "Graz University of Technology", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Institute for Computer Graphics and Vision", "label": "has_email", "title": "has_email", "to": "possegger@icg.tugraz.at", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Guanbin Li", "label": "is_author_of", "title": "is_author_of", "to": "Visual Saliency Based on Multiscale Deep Features", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Visual Saliency Based on Multiscale Deep Features", "label": "is_publication_of", "title": "is_publication_of", "to": "CVPR", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Yizhou Yu", "label": "is_author_of", "title": "is_author_of", "to": "Visual Saliency Based on Multiscale Deep Features", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Deep Features", "label": "extracted at", "title": "extracted at", "to": "Multiple Scales", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "MDF approach", "label": "generates", "title": "generates", "to": "Salience maps", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "MDF approach", "label": "generates", "title": "generates", "to": "accurate salience maps", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Spectral residual approach", "label": "is_method_for", "title": "is_method_for", "to": "Salience detection", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "Salience detection", "label": "is_field_within", "title": "is_field_within", "to": "Image Processing", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Salient object detection", "label": "employs", "title": "employs", "to": "discriminative regional feature integration", "width": 3.7600000000000002}, {"arrows": "to", "color": "#CC7700", "from": "Salient object detection", "label": "is_field_of", "title": "is_field_of", "to": "computer vision", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Salient object detection", "label": "is_benchmark", "title": "is_benchmark", "to": "ECCV", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Yuan et al. (2013)", "label": "is_foundational_work_in", "title": "is_foundational_work_in", "to": "salient object detection", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Perazzi et al. (2012)", "label": "is_foundational_work_in", "title": "is_foundational_work_in", "to": "salient region detection", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "salient region detection", "label": "is_task_in", "title": "is_task_in", "to": "computer vision", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Wei et al. (2012)", "label": "is_foundational_work_in", "title": "is_foundational_work_in", "to": "salient region detection", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Yan et al. (2013)", "label": "is_foundational_work_in", "title": "is_foundational_work_in", "to": "salient object detection", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "Yang et al. (2013)", "label": "is_foundational_work", "title": "is_foundational_work", "to": "salient object detection", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Yang et al. (2013)", "label": "presented_in", "title": "presented_in", "to": "CVPR", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "Salient region detection", "label": "is_field_of", "title": "is_field_of", "to": "computer vision", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Salience filters", "label": "method_for", "title": "method_for", "to": "salient region detection", "width": 3.4000000000000004}, {"arrows": "to", "color": "#CC7700", "from": "Zhu et al. (2014)", "label": "is_foundational_work", "title": "is_foundational_work", "to": "salient object detection", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Sun et al. (2014)", "label": "focuses_on", "title": "focuses_on", "to": "salient object detection", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Sun et al. (2014)", "label": "uses", "title": "uses", "to": "discriminative manifold-based approach", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Sun et al. (2014)", "label": "presented_in", "title": "presented_in", "to": "CVPR", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Minsu Cho", "label": "is_author_of", "title": "is_author_of", "to": "Unsupervised Object Discovery and Localization", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Minsu Cho", "label": "is_author_of", "title": "is_author_of", "to": "Unsupervised Object Discovery and Localization in the Wild", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Minsu Cho", "label": "affiliated_with", "title": "affiliated_with", "to": "Inria", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Unsupervised Object Discovery and Localization", "label": "uses", "title": "uses", "to": "bottom-up region proposals", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "Suha Kwak", "label": "is_author_of", "title": "is_author_of", "to": "Unsupervised Object Discovery and Localization", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Suha Kwak", "label": "is_author_of", "title": "is_author_of", "to": "Unsupervised Object Discovery and Localization in the Wild", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Suha Kwak", "label": "affiliated_with", "title": "affiliated_with", "to": "Inria", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Cordelia Schmid", "label": "is_author_of", "title": "is_author_of", "to": "Unsupervised Object Detection and Localization", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Cordelia Schmid", "label": "affiliated_with", "title": "affiliated_with", "to": "Inria", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Unsupervised Object Discovery and Localization in the Wild", "label": "is_publication_of", "title": "is_publication_of", "to": "CVPR", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "Unsupervised Object Discovery and Localization in the Wild", "label": "has_topic", "title": "has_topic", "to": "object discovery", "width": 3.7600000000000002}, {"arrows": "to", "color": "#CC7700", "from": "Unsupervised Object Discovery and Localization in the Wild", "label": "has_topic", "title": "has_topic", "to": "object localization", "width": 3.7600000000000002}, {"arrows": "to", "color": "#CC7700", "from": "Unsupervised Object Discovery and Localization in the Wild", "label": "uses_method", "title": "uses_method", "to": "part-based matching", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "object discovery", "label": "occurs in", "title": "occurs in", "to": "mixed-class datasets", "width": 3.7600000000000002}, {"arrows": "to", "color": "#CC7700", "from": "Unsupervised Object Localization", "label": "uses_approach", "title": "uses_approach", "to": "bottom-up region proposals", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "setting", "label": "is", "title": "is", "to": "fully unsupervised", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "region proposals", "label": "form", "title": "form", "to": "candidate bounding boxes", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "correspondence", "label": "evaluated_using", "title": "evaluated_using", "to": "probabilistic Hough transform", "width": 3.73}, {"arrows": "to", "color": "#0077CC", "from": "Hough transform", "label": "considers", "title": "considers", "to": "appearance", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Hough transform", "label": "considers", "title": "considers", "to": "spatial consistency", "width": 3.64}, {"arrows": "to", "color": "#CC7700", "from": "candidate correspondence", "label": "considers", "title": "considers", "to": "appearance", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "candidate correspondence", "label": "considers", "title": "considers", "to": "spatial consistency", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "dominant objects", "label": "are discovered by", "title": "are discovered by", "to": "comparing scores", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "dominant objects", "label": "are localized by", "title": "are localized by", "to": "selecting regions", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "regions", "label": "contain", "title": "contain", "to": "dominant objects", "width": 3.46}, {"arrows": "to", "color": "#0077CC", "from": "regions", "label": "delineate", "title": "delineate", "to": "candidate objects", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "evaluations", "label": "occur on", "title": "occur on", "to": "standard benchmarks", "width": 3.25}, {"arrows": "to", "color": "#0077CC", "from": "rd benchmarks", "label": "demonstrates", "title": "demonstrates", "to": "proposed approach", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Probabiltistic Hough transform", "label": "used for", "title": "used for", "to": "multiple object detection", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Alexe et al.", "label": "authored", "title": "authored", "to": "Measuring the object-ness of image windows", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Ballard", "label": "published", "title": "published", "to": "Generalizing the Hough transform", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Joulin et al.", "label": "published", "title": "published", "to": "Discriminative clustering for image co-segmentation", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Cho et al.", "label": "published", "title": "published", "to": "Learning graphs to match", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Learning graphs to match", "label": "presented_in", "title": "presented_in", "to": "Proceedings of the IEEE International Conference on Computer Vision", "width": 3.7600000000000002}, {"arrows": "to", "color": "#CC7700", "from": "Unsupervised object localization", "label": "related to", "title": "related to", "to": "object discovery", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Image Co-segmentation", "label": "presented_at", "title": "presented_at", "to": "CVPR", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Learning Graphs", "label": "presented_at", "title": "presented_at", "to": "ICCV", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Efficient Image Localization", "label": "presented_at", "title": "presented_at", "to": "ECCV", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Pictoiral Structures", "label": "published_in", "title": "published_in", "to": "IJCV", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "IJCV", "label": "is_publication_platform_for", "title": "is_publication_platform_for", "to": "Architectural modeling", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "IJCV", "label": "published", "title": "published", "to": "Dynamic textures", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "IJCV", "label": "published", "title": "published", "to": "Dynamic texture detection based on motion analysis", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Ijaz Akhter", "label": "contributor_to", "title": "contributor_to", "to": "Pose-Conditioned Joint Angle Limits", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Ijaz Akhter", "label": "affiliation", "title": "affiliation", "to": "Max Planck Institute for Intelligent Systems", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Ijaz Akhter", "label": "email", "title": "email", "to": "ijaz.akhter@tuebingen.mpg.de", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Pose-Conditioned Joint Angle Limits", "label": "presented_at", "title": "presented_at", "to": "CVPR", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Pose-Conditioned Joint Angle Limits", "label": "file_name", "title": "file_name", "to": "Akhter_Pose-Conditioned_Joint_Angle_2015_CVPR_paper.pdf", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Michael J. Black", "label": "author_of", "title": "author_of", "to": "Pose-Conditioning Joint Angle Limits", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Michael J. Black", "label": "contributor_to", "title": "contributor_to", "to": "Pose-Conditioned Joint Angle Limits", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Michael J. Black", "label": "affiliation", "title": "affiliation", "to": "Max Planck Institute for Intelligent Systems", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "3D human pose estimation", "label": "is_central_to", "title": "is_central_to", "to": "analysis of people in images and video", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "joint limits", "label": "vary_with", "title": "vary_with", "to": "pose", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "motion capture dataset", "label": "explores", "title": "explores", "to": "range of human poses", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "3D pose", "label": "derived from", "title": "derived from", "to": "2D joint locations", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "detections", "label": "performed on", "title": "performed on", "to": "Leeds sports pose dataset", "width": 3.46}, {"arrows": "to", "color": "#0077CC", "from": "he-art results", "label": "evaluates", "title": "evaluates", "to": "2D to 3D pose estimation", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "he-art results", "label": "uses", "title": "uses", "to": "CMU mocap dataset", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "superior results", "label": "based_on", "title": "based_on", "to": "manual annotations", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "superior results", "label": "based_on", "title": "based_on", "to": "automatic detections", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "automatic detections", "label": "uses", "title": "uses", "to": "Leeds sports pose dataset", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Andriluka, M. et al. (2010)", "label": "addresses", "title": "addresses", "to": "monocular 3D pose estimation and tracking", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Barr`on, C. \u0026 Kakadiaris, I. (2001)", "label": "addresses", "title": "addresses", "to": "estimating anthropometry and pose", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "BenAbdelkader, C. \u0026 Yacoob, Y. (2008)", "label": "addresses", "title": "addresses", "to": "statistical estimation of human anthropometry", "width": 3.64}, {"arrows": "to", "color": "#CC7700", "from": "Human Pose Reconstruction", "label": "relies_on", "title": "relies_on", "to": "Motion Capture Data", "width": 3.4000000000000004}, {"arrows": "to", "color": "#CC7700", "from": "Prior Models", "label": "inform", "title": "inform", "to": "Human Pose Reconstruction", "width": 3.25}, {"arrows": "to", "color": "#0077CC", "from": "Bourdev \u0026 Malik", "label": "published_in", "title": "published_in", "to": "International Conference on Computer Vision", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "Poselets", "label": "is_a", "title": "is_a", "to": "body part detector", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Poselets", "label": "trained_using", "title": "trained_using", "to": "3D human pose annotations", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Chen, Nie, \u0026 Ji", "label": "published_in", "title": "published_in", "to": "IEEE Transactions on Image Processing", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Guan et al.", "label": "published_in", "title": "published_in", "to": "Int. Conf. on Computer Vision (ICCV)", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Estimating human shape", "label": "requires", "title": "requires", "to": "single image", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Grochow et al.", "label": "published_in", "title": "published_in", "to": "ACM Transactions on Graphics (TOG)", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Grochow et al.", "label": "published", "title": "published", "to": "Style-based inverse kinematics", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "Style-based inverse kinematics", "label": "is_a", "title": "is_a", "to": "method", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Style-based inverse kinematics", "label": "appeared_in", "title": "appeared_in", "to": "ACM Transactions on Graphics (TOG)", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "human anthropometry", "label": "estimated_from", "title": "estimated_from", "to": "single uncalibrated image", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "three-dimensional multivariate model", "label": "appeared_in", "title": "appeared_in", "to": "Clinical Biomechanics", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Herda et al.", "label": "published", "title": "published", "to": "Hierarchical implicit surface joint limits", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Hierarchical implicit surface joint limits", "label": "appeared_in", "title": "appeared_in", "to": "Computer Vision and Image Understanding", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Lin et al.", "label": "published", "title": "published", "to": "sketching interface", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Lin et al.", "label": "authored", "title": "authored", "to": "Microsoft COCO: Common objects in context", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "sketching interface", "label": "appeared_in", "title": "appeared_in", "to": "IEEE Transactions on Visualization and Computer Graphics", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Max Planck Institute for Intelligent Systems", "label": "location", "title": "location", "to": "Tuebingen", "width": 3.25}, {"arrows": "to", "color": "#0077CC", "from": "Ran Tao", "label": "author_of", "title": "author_of", "to": "Attributes and Categories for Generic Instance Search from One Example", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Ran Tao", "label": "is_author_of", "title": "is_author_of", "to": "paper", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Attributes and Categories for Generic Instance Search from One Example", "label": "published_in", "title": "published_in", "to": "CVPR paper", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Arnold W.M. Smeulders", "label": "author_of", "title": "author_of", "to": "Attributes and Categories for Generic Instance Search from One Example", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Arnold W.M. Smeulders", "label": "is_author_of", "title": "is_author_of", "to": "paper", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Tao_Attributes_and_Categories_2015_CVPR_paper", "label": "represents", "title": "represents", "to": "Attributes and Categories for Generic Instance Search from One Example", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "instance search methods", "label": "struggle with", "title": "struggle with", "to": "arbitrary 3D objects", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "arbitrary 3D objects", "label": "include", "title": "include", "to": "shoes", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "category-specific attributes", "label": "propose using", "title": "propose using", "to": "authors", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "category-specific attributes", "label": "handles", "title": "handles", "to": "appearance variations", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "category-level information", "label": "combines_with", "title": "combines_with", "to": "category-specific attributes", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "combination", "label": "outperforms", "title": "outperforms", "to": "approaches relying on low-level features", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "core challenge", "label": "is", "title": "is", "to": "representing query image", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "query image", "label": "requires", "title": "requires", "to": "robust representation", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "robust representation", "label": "is_resistant_to", "title": "is_resistant_to", "to": "appearance variations", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "robust representation", "label": "handles", "title": "handles", "to": "Appearance Variation", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "rich representation", "label": "enables", "title": "enables", "to": "distinction from similar instances", "width": 3.79}, {"arrows": "to", "color": "#CC7700", "from": "distinction", "label": "is_between", "title": "is_between", "to": "similar instances", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Generic Instance Search", "label": "relies_on", "title": "relies_on", "to": "low-level features", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Core Challenge", "label": "involves", "title": "involves", "to": "robust representation", "width": 3.55}, {"arrows": "to", "color": "#00CC77", "from": "Appearance Variation", "label": "impacts", "title": "impacts", "to": "robust representation", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "Attribute Representation", "label": "facilitates", "title": "facilitates", "to": "distinction", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Large Scale Visual Recognition Challenge", "label": "provides", "title": "provides", "to": "dataset", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Attribute Transfer", "label": "enables", "title": "enables", "to": "detecting unseen object classes", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "Multiple queries", "label": "supports", "title": "supports", "to": "large scale specific object retrieval", "width": 3.61}, {"arrows": "to", "color": "#0077CC", "from": "Aranandjelovic", "label": "authored", "title": "authored", "to": "Multiple queries", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Zisserman", "label": "co-authored", "title": "co-authored", "to": "Multiple queries", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Naphade", "label": "authored", "title": "authored", "to": "concept ontology", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "concept ontology", "label": "is_published_in", "title": "is_published_in", "to": "IEEE MultiMedia", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Perdoch", "label": "authored", "title": "authored", "to": "efficient representation", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Farhad", "label": "authored", "title": "authored", "to": "describing objects", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Farhad", "label": "authors", "title": "authors", "to": "CVPR paper", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Farhad", "label": "affiliated_with", "title": "affiliated_with", "to": "ISLA", "width": 3.7600000000000002}, {"arrows": "to", "color": "#CC7700", "from": "describing objects", "label": "focuses_on", "title": "focuses_on", "to": "object attributes", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "Recognition algorithms", "label": "based on", "title": "based on", "to": "convolutional networks", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "last layer output", "label": "acts as", "title": "acts as", "to": "feature representation", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "last layer output", "label": "is", "title": "is", "to": "spatially coarse", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "earlier layers", "label": "are", "title": "are", "to": "precise in localization", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "earlier layers", "label": "lacks", "title": "lacks", "to": "semantics", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "hypercolumn", "label": "is", "title": "is", "to": "vector of activations", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "pixel", "label": "above", "title": "above", "to": "CNN units", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "hypercolumns", "label": "acts as", "title": "acts as", "to": "pixel descriptors", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "hypercolumns", "label": "demonstrates", "title": "demonstrates", "to": "improvements", "width": 3.67}, {"arrows": "to", "color": "#0077CC", "from": "hypercolumns", "label": "improves", "title": "improves", "to": "state-of-the-art results", "width": 3.73}, {"arrows": "to", "color": "#CC7700", "from": "simultaneous detection", "label": "is a", "title": "is a", "to": "localization task", "width": 3.58}, {"arrows": "to", "color": "#CC7700", "from": "keypoint localization", "label": "is a", "title": "is a", "to": "localization task", "width": 3.58}, {"arrows": "to", "color": "#CC7700", "from": "part labeling", "label": "is a", "title": "is a", "to": "localization task", "width": 3.58}, {"arrows": "to", "color": "#CC7700", "from": "Keypoint Localization", "label": "is_task", "title": "is_task", "to": "Fine-grained Localization", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Spatial Pyramid Pooling", "label": "improves", "title": "improves", "to": "Visual Recognition", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Multi-scale Feature Integration", "label": "used_in", "title": "used_in", "to": "Object Segmentation", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Arbel\u00e1ez, P.", "label": "authored", "title": "authored", "to": "Multiscale Combinatorial Grouping", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "He, K.", "label": "authored", "title": "authored", "to": "Spatial Pyramid Pooling", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Barron, J. T.", "label": "authored", "title": "authored", "to": "Volumetric Semantic Segmentation", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "Hypercolumn Representation", "label": "enhances", "title": "enhances", "to": "Object Segmentation", "width": 3.25}, {"arrows": "to", "color": "#0077CC", "from": "Barron et al.", "label": "published_in", "title": "published_in", "to": "ICCV", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Hubel \u0026 Wiesel", "label": "published_in", "title": "published_in", "to": "The Journal of Physiology", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Bo \u0026 Fowlakes", "label": "published_in", "title": "published_in", "to": "CVPR", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Ionescu et al.", "label": "published_in", "title": "published_in", "to": "CVPR", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Jones \u0026 Malik", "label": "published_in", "title": "published_in", "to": "ECCV", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Koenderink \u0026 van Doorn", "label": "published_in", "title": "published_in", "to": "Biological Cybernetics", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Volumetric semantic segmentation", "label": "uses", "title": "uses", "to": "pyramid context features", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Visual cortex", "label": "studied_in", "title": "studied_in", "to": "Hubel \u0026 Wiesel\u0027s work", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "Pedestrian parsing", "label": "uses", "title": "uses", "to": "shape-based methods", "width": 3.4000000000000004}, {"arrows": "to", "color": "#CC7700", "from": "Biological cybernetics", "label": "is_journal", "title": "is_journal", "to": "visual system", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Malik", "label": "is_affiliated_with", "title": "is_affiliated_with", "to": "University of California, Berkeley", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Xie", "label": "is_author_of", "title": "is_author_of", "to": "DeepShape", "width": 3.9699999999999998}, {"arrows": "to", "color": "#CC7700", "from": "DeepShape", "label": "is_a", "title": "is_a", "to": "shape descriptor", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "DeepShape", "label": "supports", "title": "supports", "to": "3D shape matching", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "DeepShape", "label": "supports", "title": "supports", "to": "3D shape retrieval", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "shape descriptor", "label": "used for", "title": "used for", "to": "3D shape matching and retrieval", "width": 3.88}, {"arrows": "to", "color": "#CCCCCC", "from": "shape descriptor", "label": "is_formed_by", "title": "is_formed_by", "to": "multiple discriminative auto-encoders", "width": 3.94}, {"arrows": "to", "color": "#CCCCCC", "from": "shape descriptor", "label": "used_for", "title": "used_for", "to": "3D shape matching", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CCCCCC", "from": "shape descriptor", "label": "used_for", "title": "used_for", "to": "3D shape retrieval", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "3D model", "label": "poses challenge to", "title": "poses challenge to", "to": "3D shape matching and retrieval", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "shape feature learning scheme", "label": "uses", "title": "uses", "to": "multiscale shape distribution", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "auto-encoder", "label": "is", "title": "is", "to": "discriminative deep auto-encoder", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "shape distribution", "label": "is used as input to", "title": "is used as input to", "to": "auto-encoder", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "Fisher discrimination criterion", "label": "is imposed on", "title": "is imposed on", "to": "neurons", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Fisher discrimination criterion", "label": "applied to", "title": "applied to", "to": "hidden layer neurons", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "neurons", "label": "are concatenated to form", "title": "are concatenated to form", "to": "shape descriptor", "width": 3.73}, {"arrows": "to", "color": "#0077CC", "from": "3D models", "label": "have", "title": "have", "to": "geometric variations", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Proposed Method", "label": "used_for", "title": "used_for", "to": "3D shape matching and retrieval", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Proposed Method", "label": "demonstrated_by", "title": "demonstrated_by", "to": "Experimental results", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Agathos et al. (2009)", "label": "addresses", "title": "addresses", "to": "Retrieval of 3D articulated objects", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Assfalg et al. (2007)", "label": "addresses", "title": "addresses", "to": "Content-based retrieval of 3D objects", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Belongie et al. (2000)", "label": "introduces", "title": "introduces", "to": "Shape context", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Belongie et al. (2000)", "label": "developed", "title": "developed", "to": "Shape Context", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Shape context", "label": "used_for", "title": "used_for", "to": "shape matching and object recognition", "width": 3.91}, {"arrows": "to", "color": "#CC7700", "from": "Geometric Feature Learning", "label": "related_to", "title": "related_to", "to": "3D shape matching", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Deep Auto-encoders", "label": "applied_in", "title": "applied_in", "to": "3D shape matching", "width": 3.25}, {"arrows": "to", "color": "#0077CC", "from": "Ric variations", "label": "is_example_of", "title": "is_example_of", "to": "Mcgill dataset", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Ric variations", "label": "is_example_of", "title": "is_example_of", "to": "SHREC\u002710 Shape dataset", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Shape Context", "label": "used for", "title": "used for", "to": "shape matching", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Shape Context", "label": "used for", "title": "used for", "to": "object recognition", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "Deep Architectures", "label": "relevant to", "title": "relevant to", "to": "AI", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Shape Google", "label": "uses", "title": "uses", "to": "geometric words", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Isometry-invariant distances", "label": "computed by", "title": "computed by", "to": "Bronstein et al. (2006)", "width": 3.7600000000000002}, {"arrows": "to", "color": "#CC7700", "from": "Gromov-Hausdorff framework", "label": "supports", "title": "supports", "to": "non-rigid shape matching", "width": 3.79}, {"arrows": "to", "color": "#CC7700", "from": "Diffusion geometry", "label": "contributes to", "title": "contributes to", "to": "topologically-robust matching", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Bronstein et al. (2011)", "label": "introduced", "title": "introduced", "to": "Shape Google", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Mahmoudi, M.", "label": "authored", "title": "authored", "to": "A Gromov-Hausdorff framework", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "A Gromov-Hausdorff framework", "label": "is_used_in", "title": "is_used_in", "to": "shape matching", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "A Gromov-Hausdorff framework", "label": "is_published_in", "title": "is_published_in", "to": "International Journal of Computer Vision", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Chen, D.-Y.", "label": "authored", "title": "authored", "to": "On visual similarity based 3D model retrieval", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "On visual similarity based 3D model retrieval", "label": "addresses", "title": "addresses", "to": "3D model retrieval", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Chen, X.", "label": "authored", "title": "authored", "to": "A benchmark for 3D mesh segmentation", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Chen, X.", "label": "authored", "title": "authored", "to": "Locally linear regression for pose-invariant face recognition", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "A benchmark", "label": "is_for", "title": "is_for", "to": "3D mesh segmentation", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "De Goes, F.", "label": "authored", "title": "authored", "to": "A hierarchical segmentation", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "A hierarchical segmentation", "label": "concerns", "title": "concerns", "to": "articulated bodies", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Jin Xie", "label": "works_at", "title": "works_at", "to": "New York University Abu Dhabi", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Jin Xie", "label": "has_email", "title": "has_email", "to": "jin.xie@nyu.edu", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Yi Fang", "label": "works_at", "title": "works_at", "to": "New York University Abu Dhabi", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Yi Fang", "label": "has_email", "title": "has_email", "to": "yfang@nyu.edu", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Fan Zhu", "label": "is_affiliated_with", "title": "is_affiliated_with", "to": "Department of Electrical and Computer Engineering", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Department of Electrical and ComputerEngineering", "label": "is_located_at", "title": "is_located_at", "to": "New York University Abu Dhabi", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Edward Wong", "label": "is_affiliated_with", "title": "is_affiliated_with", "to": "Polytechnic School of Engineering", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Polytechnic School of Engineering", "label": "is_part_of", "title": "is_part_of", "to": "New York University", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Bingbing Ni", "label": "is_author_of", "title": "is_author_of", "to": "Motion Part Regularization", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Bingbing Ni", "label": "affiliated_with", "title": "affiliated_with", "to": "ADSC Singapore", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Motion Part Regularization", "label": "improves", "title": "improves", "to": "Action Recognition", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Pierre Moulin", "label": "is_author_of", "title": "is_author_of", "to": "Motion Part Regularization", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Pierre Moulin", "label": "affiliated_with", "title": "affiliated_with", "to": "Department of ECE", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "golf", "label": "is_an_action_label", "title": "is_an_action_label", "to": "Action", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "punch", "label": "is_an_action_label", "title": "is_an_action_label", "to": "Action", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Fisher vector", "label": "is_used_in", "title": "is_used_in", "to": "Action Recognition", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "Ni_Motion_Part_Regularization_2015_CVPR_paper", "label": "proposes", "title": "proposes", "to": "Motion Part Regularization framework", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "Motion Part Regularization framework", "label": "aims_to_improve", "title": "aims_to_improve", "to": "action recognition", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Motion Part Regularization framework", "label": "mines", "title": "mines", "to": "dense trajectories", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "action recognition", "label": "implemented_in", "title": "implemented_in", "to": "3D natural scenes", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "action recognition", "label": "uses", "title": "uses", "to": "depth cameras", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "discriminativeness weighted Fisher vector representation", "label": "is_more_discriminative_than", "title": "is_more_discriminative_than", "to": "traditional Fisher vector", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "objective function", "label": "encourages", "title": "encourages", "to": "sparse selection of trajectory groups", "width": 3.73}, {"arrows": "to", "color": "#0077CC", "from": "objective function", "label": "includes", "title": "includes", "to": "action class discriminative term", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "objective function", "label": "includes", "title": "includes", "to": "discriminative term", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "optimization algorithm", "label": "uses", "title": "uses", "to": "auxiliary variables", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Optimization Algorithm", "label": "used_for", "title": "used_for", "to": "Action Recognition", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Motion Part", "label": "has", "title": "has", "to": "Discriminative Weights", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Dense Trajectories", "label": "described_in", "title": "described_in", "to": "Wang et al. (2011)", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Behavior Recognition", "label": "uses", "title": "uses", "to": "Spatio-Temporal Grouping", "width": 3.46}, {"arrows": "to", "color": "#CC7700", "from": "LIBSVM", "label": "is_a", "title": "is_a", "to": "library", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Automatic Annotation", "label": "uses", "title": "uses", "to": "Human Actions", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "O. Duchenne", "label": "authors", "title": "authors", "to": "Automatic annotation of human actions in video", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "I. Laptev", "label": "authors", "title": "authors", "to": "Automatic annotation of human actions in video", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "J. Sivic", "label": "authors", "title": "authors", "to": "Automatic annotation of human actions in video", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "J. Ponce", "label": "authors", "title": "authors", "to": "Automatic annotation of human actions in video", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "H. Wang", "label": "authors", "title": "authors", "to": "Action recognition with improved trajectories", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "C. Schmid", "label": "authors", "title": "authors", "to": "Action recognition with improved trajectories", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "C. Schmid", "label": "co-authored", "title": "co-authored", "to": "Multi-fold mil training", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "P. Felzenszwalb", "label": "authors", "title": "authors", "to": "Object detection with discriminatively trained part based models", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "R. Girshick", "label": "authors", "title": "authors", "to": "Object detection with discriminatively trained part based models", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "R. Girshick", "label": "co_authored", "title": "co_authored", "to": "Efficient regression of general-activity human poses", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "D. McAlleser", "label": "authors", "title": "authors", "to": "Object detection with discriminatively trained part based models", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "D. Ramanan", "label": "authors", "title": "authors", "to": "Object description with discriminatively trained part based models", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "J. Wang", "label": "authors", "title": "authors", "to": "Mining actionlet ensemble for action recognition with depth cameras", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Z. Liu", "label": "authors", "title": "authors", "to": "Mining actionlet ensemble for action recognition with depth cameras", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Y. Wu", "label": "authors", "title": "authors", "to": "Mining actionlet ensemble for action recognition with depth cameras", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "J. Yuan", "label": "authors", "title": "authors", "to": "Mining actionlet ensemble for action recognition with depth cameras", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "M. Jain", "label": "authors", "title": "authors", "to": "Better exploiting motion for better action recognition", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "H. Jegou", "label": "authors", "title": "authors", "to": "Better exploiting motion for better action recognition", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "P. Bouthemy", "label": "authors", "title": "authors", "to": "Better exploiting motion for better action recognition", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Y.-G. Jiang", "label": "authors", "title": "authors", "to": "Trajectory-based modeling of human actions with motion reference points", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Q. Dai", "label": "authors", "title": "authors", "to": "Trajectory-based modeling of human actions with motion reference points", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "X. Xue", "label": "authors", "title": "authors", "to": "Trajectory-based modeling of human actions with motion reference points", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "W. Liu", "label": "authors", "title": "authors", "to": "Trajectory-based modeling of human actions with motion reference points", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "C.-W. Ngo", "label": "authors", "title": "authors", "to": "Trajectory-based modeling of human actions with motion reference points", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Pierre Bounameaux", "label": "affiliated_with", "title": "affiliated_with", "to": "ADSC Singapore", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "gaze correction solutions", "label": "relies_on", "title": "relies_on", "to": "additional hardware", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "pixel replacement operations", "label": "localized_around", "title": "localized_around", "to": "eyes", "width": 3.91}, {"arrows": "to", "color": "#CC7700", "from": "Monocular Gaze Correction", "label": "is_field_of", "title": "is_field_of", "to": "Computer Vision", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "Monocular Gaze Correction", "label": "uses", "title": "uses", "to": "Machine Learning", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Uncanny Valley Effect", "label": "is_effect_of", "title": "is_effect_of", "to": "Localized Pixel Replacement", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Amit", "label": "authored", "title": "authored", "to": "Shape Quantization", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Shape Quantization", "label": "is_published_in", "title": "is_published_in", "to": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "Doll\u00e1r", "label": "authored", "title": "authored", "to": "Structured Forests", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Structured Forests", "label": "facilitates", "title": "facilitates", "to": "Fast Edge Detection", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Fanelli", "label": "authored", "title": "authored", "to": "Random Forests for 3D Face Analysis", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "Random Forests for 3D Face Analysis", "label": "is_published_in", "title": "is_published_in", "to": "International Journal of Computer Vision", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Fanelli, G.", "label": "authored", "title": "authored", "to": "Random forests for real time 3d face analysis", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Gaze correction", "label": "achieved with", "title": "achieved with", "to": "single webcam", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Gall, J.", "label": "authored", "title": "authored", "to": "Class-specific hough forests for object detection", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Hough forests", "label": "used for", "title": "used for", "to": "object detection", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Jones, A.", "label": "authored", "title": "authored", "to": "Achieving eye contact in a one-to-many 3D video teleconferecing system", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "eye contact", "label": "achieved in", "title": "achieved in", "to": "3D video teleconferencing system", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Kazemi, V.", "label": "authored", "title": "authored", "to": "One milliseccond face alignment with an ensemble of regression trees", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Kazemi, V.", "label": "authored", "title": "authored", "to": "One milliseccond face alignment", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Kazemi, V.", "label": "collaborated_with", "title": "collaborated_with", "to": "Sullivan, J.", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "face alignment", "label": "achieved with", "title": "achieved with", "to": "regression trees", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Kuster, C.", "label": "authored", "title": "authored", "to": "Gaze correction", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Ren, S.", "label": "authored", "title": "authored", "to": "Face alignment", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Ren, S.", "label": "collaborated_with", "title": "collaborated_with", "to": "Cao, X.", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Cao, X.", "label": "authored", "title": "authored", "to": "practical transfer learning algorithm", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Kononenko, Daniil", "label": "affiliated_with", "title": "affiliated_with", "to": "Skolkovo Institute of Science and Technology (Skoltech)", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Lempitsky, Victor", "label": "affiliated_with", "title": "affiliated_with", "to": "Skolkovo Institute of Science and Technology (Skeltech)", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Zhao, Kaili", "label": "authored", "title": "authored", "to": "Joint Patch and Multi-label Learning", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "Joint Patch and Multi-label Learning", "label": "focuses_on", "title": "focuses_on", "to": "Facial Action Unit Detection", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Zhao_Joint_Patch_and_2015_CVPR_paper.pdf", "label": "is_publication_of", "title": "is_publication_of", "to": "Joint Patch and Multi-label Learning", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Facial Action Coding System", "label": "is_system_for", "title": "is_system_for", "to": "describing facial movements", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Facial Action Coding System", "label": "is_reference_for", "title": "is_reference_for", "to": "human face", "width": 3.7600000000000002}, {"arrows": "to", "color": "#CC7700", "from": "Facial Action Coding System", "label": "defines", "title": "defines", "to": "facial action units", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "Facial Action Coding System", "label": "is_reference_for", "title": "is_reference_for", "to": "facial action unit detection", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Action Units", "label": "part_of", "title": "part_of", "to": "Facial Action Coding System", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "JPML", "label": "is_instance_of", "title": "is_instance_of", "to": "Multi-label Learning", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "JPML", "label": "achieves", "title": "achieves", "to": "highest average F1 scores", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "JPML", "label": "compared_to", "title": "compared_to", "to": "state-of-the-art methods", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "CK+", "label": "is_a", "title": "is_a", "to": "dataset", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "BP4D", "label": "is_a", "title": "is_a", "to": "dataset", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Machine learning", "label": "addresses", "title": "addresses", "to": "facial expression recognition", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Machine learning", "label": "is_publication_venue_for", "title": "is_publication_venue_for", "to": "Support-vector networks", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "FACIAL Action Coding System (FACS)", "label": "related_to", "title": "related_to", "to": "facial expression recognition", "width": 3.25}, {"arrows": "to", "color": "#CC7700", "from": "Action Unit (AU) Detection", "label": "part_of", "title": "part_of", "to": "FACIAL Action Coding System (FACS)", "width": 3.4000000000000004}, {"arrows": "to", "color": "#CC7700", "from": "Patch Learning", "label": "related_to", "title": "related_to", "to": "JPML", "width": 3.0999999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Alternating direction method of multipliers", "label": "utilized_in", "title": "utilized_in", "to": "optimization techniques", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "Alternating Direction Method of Multipliers", "label": "is_optimization_technique", "title": "is_optimization_technique", "to": "optimization techniques", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "Affective Computing", "label": "is_field_of", "title": "is_field_of", "to": "Machine Learning", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Facial Action Unit Event Detection", "label": "is_topic_of", "title": "is_topic_of", "to": "ICCV", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Facial Action Unit Event Detection", "label": "uses", "title": "uses", "to": "cascade of tasks", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "X. Ding", "label": "is_author_of", "title": "is_author_of", "to": "Facial Action Unit Event Detection", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "P. Ekman", "label": "authored", "title": "authored", "to": "Facial Action Coding System", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "F. De la Torre", "label": "is_author_of", "title": "is_author_of", "to": "Intraface", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "F. De la Torre", "label": "authored", "title": "authored", "to": "Selective transfer machine", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "J. C. Hager", "label": "authored", "title": "authored", "to": "Facial Action Coding System", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Selective transfer machine", "label": "addresses", "title": "addresses", "to": "personalization in facial action unit detection", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "W.-S. Chu", "label": "authored", "title": "authored", "to": "Selective transfer machine", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "J. F. Cohn", "label": "authored", "title": "authored", "to": "Selective transfer machine", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Facing imbalanced data", "label": "deals_with", "title": "deals_with", "to": "imbalanced datasets", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "L. A. Jeni", "label": "authored", "title": "authored", "to": "Facing imbalanced data", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Data-free prior model", "label": "uses", "title": "uses", "to": "data-free approach", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Data-free prior model", "label": "for", "title": "for", "to": "facial action unit recognition", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "Y. Li", "label": "authored", "title": "authored", "to": "Data-free prior model", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Y. Zhao", "label": "affiliated_with", "title": "affiliated_with", "to": "School of Comm. and Info. Engineering", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "School of Comm. and Info. Engineering", "label": "part_of", "title": "part_of", "to": "Beijing University of Posts and Telecom.", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Beijing University of Posts and Telecom.", "label": "located_in", "title": "located_in", "to": "Beijing", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "G. Littlewort", "label": "authored", "title": "authored", "to": "Dynamics of facial expression", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Dynamics of facial expression", "label": "uses", "title": "uses", "to": "AU-cascades", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "AU-cascades", "label": "for", "title": "for", "to": "action unit detection", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Wen-Sheng Chu", "label": "affiliated_with", "title": "affiliated_with", "to": "Robotics Institute", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Robotics Institute", "label": "part_of", "title": "part_of", "to": "Carnegie Mellon University", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Fernando De la Torre", "label": "affiliated_with", "title": "affiliated_with", "to": "Robotics Institute", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Jeffrey F. Cohn", "label": "affiliated_with", "title": "affiliated_with", "to": "Robotic Institute", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Jeffrey F. Cohn", "label": "affiliated_with", "title": "affiliated_with", "to": "Robotics Institute", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Honggang Zhang", "label": "affiliated_with", "title": "affiliated_with", "to": "School of Comm. and Info. Engineering", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Beijing University of Posts and Telecom", "label": "located_in", "title": "located_in", "to": "China", "width": 3.91}, {"arrows": "to", "color": "#CC7700", "from": "TVSum", "label": "summarizes", "title": "summarizes", "to": "Web Videos", "width": 3.9699999999999998}, {"arrows": "to", "color": "#CC7700", "from": "TVSum", "label": "uses", "title": "uses", "to": "Titles", "width": 3.88}, {"arrows": "to", "color": "#CC7700", "from": "TVSum", "label": "is_a", "title": "is_a", "to": "video summarization framework", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "TVSum", "label": "guides", "title": "guides", "to": "title-based image search results", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "TVSum", "label": "produces", "title": "produces", "to": "superior quality summaries", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "TVSum", "label": "compared_to", "title": "compared_to", "to": "existing approaches", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "TVSum", "label": "produces", "title": "produces", "to": "summaries", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "TVSum", "label": "has_quality", "title": "has_quality", "to": "superior", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "Yale Song", "label": "author_of", "title": "author_of", "to": "TVSum", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Yale Song", "label": "works_at", "title": "works_at", "to": "Yahoo Labs", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Yale Song", "label": "has_contact", "title": "has_contact", "to": "yalessong@yahoo-inc.com", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Jordi Vallmitjana", "label": "author_of", "title": "author_of", "to": "TVSum", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Jordi Vallmitjana", "label": "works_at", "title": "works_at", "to": "Yahoo Labs", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Amanda Stent", "label": "author_of", "title": "author_of", "to": "TVSum", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Amanda Stent", "label": "works_at", "title": "works_at", "to": "Yahoo Labs", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Alejandro Jaimes", "label": "author_of", "title": "author_of", "to": "TVSum", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Alejandro Jaimes", "label": "works_at", "title": "works_at", "to": "Yahoo Labs", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Song_TVSum_Summarizing_Web_2015_CVPR_paper.pdf", "label": "is_a", "title": "is_a", "to": "CVPR paper", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "Video summarization", "label": "is_challenging_problem_due_to", "title": "is_challenging_problem_due_to", "to": "need for prior knowledge", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "video titles", "label": "are", "title": "are", "to": "descriptive", "width": 3.91}, {"arrows": "to", "color": "#CC7700", "from": "co-archetypal analysis", "label": "is", "title": "is", "to": "novel technique", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "co-archetypal analysis", "label": "learns", "title": "learns", "to": "visual concepts", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "visual concepts", "label": "are_shared_between", "title": "are_shared_between", "to": "video and images", "width": 3.7600000000000002}, {"arrows": "to", "color": "#CC7700", "from": "TVSum50", "label": "is_a", "title": "is_a", "to": "benchmark dataset", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "TVSum50", "label": "is_a", "title": "is_a", "to": "dataset", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "TVSum50", "label": "introduced_in", "title": "introduced_in", "to": "study", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "image search results", "label": "contains", "title": "contains", "to": "noise and variance", "width": 3.64}, {"arrows": "to", "color": "#CC7700", "from": "Co-Archetypal Analysis", "label": "is_a", "title": "is_a", "to": "analysis method", "width": 3.7600000000000002}, {"arrows": "to", "color": "#CC7700", "from": "Canonical Visual Concepts", "label": "is_a", "title": "is_a", "to": "concept", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "M. Basseville", "label": "authored", "title": "authored", "to": "Detection of abrupt changes", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "A. Beck", "label": "authored", "title": "authored", "to": "shrinkage-thresholding algorithm", "width": 3.73}, {"arrows": "to", "color": "#0077CC", "from": "K. Bleakley", "label": "authored", "title": "authored", "to": "group fused lasso", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Y. Chen", "label": "authored", "title": "authored", "to": "archetypal analysis", "width": 3.67}, {"arrows": "to", "color": "#0077CC", "from": "S. Fidler", "label": "authored", "title": "authored", "to": "sentence is worth a thousand pixels", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "S. Fidler", "label": "co-authored", "title": "co-authored", "to": "A sentence is worth a thousand pixels", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Airal", "label": "co-authored", "title": "co-authored", "to": "Fast and robust archetypal analysis", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Fast and robust archetypal analysis", "label": "published_in", "title": "published_in", "to": "CVPR", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "M. Gygli", "label": "co-authored", "title": "co-authored", "to": "Creating summaries from user videos", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Creating summaries from user videos", "label": "published_in", "title": "published_in", "to": "ECCV", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Y. Jia", "label": "co-authored", "title": "co-authored", "to": "Visual concept learning", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Visual concept learning", "label": "published_in", "title": "published_in", "to": "NIPS", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Y. J. Lee", "label": "co-authored", "title": "co-authored", "to": "Discovering important people and objects", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Y. J. Lee", "label": "authored", "title": "authored", "to": "Object-graphs for context-aware category discovery", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Discovering important people and objects", "label": "published_in", "title": "published_in", "to": "CVPR", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "L. Li", "label": "co-authored", "title": "co-authored", "to": "Video summarization via transferrable structured learning", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Video summarization via transferrable structured learning", "label": "published_in", "title": "published_in", "to": "WWW", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "D. Lin", "label": "co-authored", "title": "co-authored", "to": "Visual semantic search", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Visual semantic search", "label": "published_in", "title": "published_in", "to": "CVPR", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "Visual semantic search", "label": "is_method_for", "title": "is_method_for", "to": "Retrieving videos", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Visual semantic search", "label": "uses", "title": "uses", "to": "complex textual queries", "width": 3.88}, {"arrows": "to", "color": "#CC7700", "from": "Yahoo Labs", "label": "is_organization", "title": "is_organization", "to": "research institution", "width": 3.25}, {"arrows": "to", "color": "#0077CC", "from": "isual semantic search", "label": "is_topic_of", "title": "is_topic_of", "to": "video retrieval", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "isual semantic search", "label": "presented_at", "title": "presented_at", "to": "CVPR", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Tianjun Xiao", "label": "is_author_of", "title": "is_author_of", "to": "The Application of Two-level Attention Models", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Tianjun Xiao", "label": "affiliated_with", "title": "affiliated_with", "to": "Institute of Computer Science and Technologies", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "The Application of Two-level Attention Models", "label": "uses", "title": "uses", "to": "Deep Convolutional Neural Network", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Yichong Xu", "label": "is_author_of", "title": "is_author_of", "to": "The Application of Two-level Attention Models", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Yichong Xu", "label": "affiliated_with", "title": "affiliated_with", "to": "Microsoft Research", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Kuiyuan Yang", "label": "is_author_of", "title": "is_author_of", "to": "The Application of Two-level Attention Models", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Kuiyuan Yang", "label": "affiliated_with", "title": "affiliated_with", "to": "Microsoft Research", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Jiaxing Zhang", "label": "is_author_of", "title": "is_author_of", "to": "The Application of Two-level Attention Models", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Jiaxing Zhang", "label": "affiliated_with", "title": "affiliated_with", "to": "Microsoft Research", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Yuxin Peng", "label": "is_author_of", "title": "is_author_of", "to": "The Application of Two-level Attack Models", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Yuxin Peng", "label": "affiliated_with", "title": "affiliated_with", "to": "Institute of Computer Science and Technology", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Zheng Zhang", "label": "is_author_of", "title": "is_author_of", "to": "The Application of Two-level Attention Models", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Zheng Zhang", "label": "affiliated_with", "title": "affiliated_with", "to": "New York University Shanghai", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Fine-grained classification", "label": "is_challenging_due_to", "title": "is_challenging_due_to", "to": "subtle differences between categories", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "pipeline", "label": "integrates", "title": "integrates", "to": "bottom-up attention", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "pipeline", "label": "integrates", "title": "integrates", "to": "object-level top-down attention", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "pipeline", "label": "integrates", "title": "integrates", "to": "part-level top-down attention", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "Fine-grained image classification", "label": "relies_on", "title": "relies_on", "to": "Visual attention models", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Fine-grained image classification", "label": "uses", "title": "uses", "to": "Deep convolutional neural networks", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Deep convolutional neural networks", "label": "used_for", "title": "used_for", "to": "ImageNet classification", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Weak supervision", "label": "addresses", "title": "addresses", "to": "additional annotations", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Institute of Computer Science and Technology", "label": "located_at", "title": "located_at", "to": "Peking University", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Geodesic Exponential Kernel", "label": "addresses", "title": "addresses", "to": "curvature and linearity conflict", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "Aasa Feragen", "label": "author_of", "title": "author_of", "to": "Geodesic Exponential Kernel", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Aasa Feragen", "label": "is_affiliated_with", "title": "is_affiliated_with", "to": "DIKU, University of Copenhagen", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Fran\u00e7ois Lauze", "label": "author_of", "title": "author_of", "to": "Geodesic Exponential Kernel", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Fran\u00e7ois Lauze", "label": "is_associated_with", "title": "is_associated_with", "to": "Feragen", "width": 3.25}, {"arrows": "to", "color": "#0077CC", "from": "Fran\u00e7ois Lauze", "label": "is_affiliated_with", "title": "is_affiliated_with", "to": "DIKU, University of Copenhagen", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "S\u00f8ren Hauberg", "label": "author_of", "title": "author_of", "to": "Geodesic Exponential Kernel", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "S\u00f8ren Hauberg", "label": "is_affiliated_with", "title": "is_affiliated_with", "to": "DTU Compute", "width": 3.91}, {"arrows": "to", "color": "#CC7700", "from": "geodesic metric spaces", "label": "related to", "title": "related to", "to": "geodesic Laplacian kernels", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "Gaussian kernel", "label": "is_generalized_to", "title": "is_generalized_to", "to": "positive definite kernel", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "positive definite kernel", "label": "requires", "title": "requires", "to": "flat space", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "geodesic Gaussian kernel", "label": "is_positive_definite_if_and_only_if", "title": "is_positive_definite_if_and_only_if", "to": "Riemannian manifold is Euclidean", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "geodesic Laplacian kernel", "label": "retains", "title": "retains", "to": "positive de\ufb01niteness", "width": 3.7600000000000002}, {"arrows": "to", "color": "#CC7700", "from": "geodesic Laplacian kernel", "label": "is applicable to", "title": "is applicable to", "to": "curved spaces", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "geodesic Laplacian kernel", "label": "generalized to", "title": "generalized to", "to": "spheres", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "geodesic Laplacian kernel", "label": "generalized to", "title": "generalized to", "to": "hyperbolic spaces", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "geodesic Laplacian kernel", "label": "is a type of", "title": "is a type of", "to": "kernel", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "curved spaces", "label": "include", "title": "include", "to": "spheres", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "curved spaces", "label": "include", "title": "include", "to": "hyperbolic spaces", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "spaces", "label": "has_property", "title": "has_property", "to": "conditionally negative de\ufb01nite distances", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "theoretical results", "label": "are verified", "title": "are verified", "to": "empirically", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "geodesic Laplacian kernels", "label": "can be generalized to", "title": "can be generalized to", "to": "curved spaces", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "Gaussian kernels", "label": "related to", "title": "related to", "to": "kernel methods", "width": 3.25}, {"arrows": "to", "color": "#CC7700", "from": "Laplacian kernels", "label": "related to", "title": "related to", "to": "kernel methods", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "M. Alamgir and U. von Luxburg", "label": "authored", "title": "authored", "to": "Shortest path distance in random k-nearest neighbor graphs", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "N. Dalal and B. Triggs", "label": "authored", "title": "authored", "to": "Histograms of oriented gradients for human detection", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "S. Amar\u00ed and H. Nagaoka", "label": "authored", "title": "authored", "to": "Methods of information geometry", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Arsigny et al.", "label": "publishes", "title": "publishes", "to": "Fast and simple calculus on tensors", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Fast and simple calculus on tensors", "label": "appears_in", "title": "appears_in", "to": "MICCAI", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Feragen et al.", "label": "publishes", "title": "publishes", "to": "Means in spaces of tree-like shapes", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Feragen et al.", "label": "publishes", "title": "publishes", "to": "Scalable kernels for graphs", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Means in spaces of tree-like shapes", "label": "appears_in", "title": "appears_in", "to": "ICCV", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Scalable kernels for graphs", "label": "appears_in", "title": "appears_in", "to": "NIPS", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Bekka and de la Harple", "label": "publishes", "title": "publishes", "to": "Kazhdan\u2019s Property (T)", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "Kazhdan\u2019s Property (T)", "label": "is_a", "title": "is_a", "to": "mathematical_property", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Kazhdan\u2019s Property (T)", "label": "is_presented_in", "title": "is_presented_in", "to": "New Mathematical Monographs", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Bridson and Hae\ufb02iger", "label": "publishes", "title": "publishes", "to": "Metric spaces of non-positive curvature", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Ganzhao Yuan", "label": "is_author_of", "title": "is_author_of", "to": "\u21130TV: A New Method", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Ganzhao Yuan", "label": "is_author_of", "title": "is_author_of", "to": "paper", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Ganzhao Yuan", "label": "affiliated_with", "title": "affiliated_with", "to": "South China University of Technology (SCUT)", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Ganzhao Yuan", "label": "email", "title": "email", "to": "yuan Ganzhao@gmail.com", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "\u21130TV: A New Method", "label": "addresses", "title": "addresses", "to": "Image Restoration", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Image Restoration", "label": "uses", "title": "uses", "to": "Total Variation", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Image Restoration", "label": "uses", "title": "uses", "to": "PADMM", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Yuan_L0TV_A_New_2015_CVPR_paper", "label": "is_a", "title": "is_a", "to": "CVPR paper", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "image restoration", "label": "affected_by", "title": "affected_by", "to": "Impulse Noise", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "\u21130TV-PADMM", "label": "solves", "title": "solves", "to": "TV-based restoration problem", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "TV-based restoration problem", "label": "uses", "title": "uses", "to": "\u21130-norm data fidelity", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "MPEC", "label": "solved_with", "title": "solved_with", "to": "PADMM", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "PADMM", "label": "is_method_of", "title": "is_method_of", "to": "Alternating Direction Method of Multipliers", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Ejaz Ahmed", "label": "author_of", "title": "author_of", "to": "An Improved Deep Learning Architecture", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Ejaz Ahmed", "label": "affiliated_with", "title": "affiliated_with", "to": "University of Maryland", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "An Improved Deep Learning Architecture", "label": "presented_at", "title": "presented_at", "to": "CVPR", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "Michael Jones", "label": "author_of", "title": "author_of", "to": "An Improved Deep Learning Architecture", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Michael Jones", "label": "affiliated_with", "title": "affiliated_with", "to": "Mitsubishi Electric Research Labs", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Tim K. Marks", "label": "author_of", "title": "author_of", "to": "An Improved Deep Learning Architecture", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Tim K. Marks", "label": "affiliated_with", "title": "affiliated_with", "to": "Mitsubishi Electric Research Labs", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "similarity value", "label": "indicates", "title": "indicates", "to": "same person", "width": 3.91}, {"arrows": "to", "color": "#CCCCCC", "from": "layer", "label": "computes", "title": "computes", "to": "cross-input neighborhood differences", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "cross-input neighborhood differences", "label": "captures", "title": "captures", "to": "local relationships", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "local relationships", "label": "based_on", "title": "based_on", "to": "mid-level features", "width": 3.7600000000000002}, {"arrows": "to", "color": "#CCCCCC", "from": "patch summary features", "label": "computed_by", "title": "computed_by", "to": "layer", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "layer of patch summary features", "label": "computes", "title": "computes", "to": "high-level summary", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "CUHK03", "label": "is", "title": "is", "to": "large data set", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "CUHK01", "label": "is", "title": "is", "to": "medium-sized data set", "width": 3.73}, {"arrows": "to", "color": "#0077CC", "from": "VIPeR", "label": "is", "title": "is", "to": "small data set", "width": 3.8499999999999996}, {"arrows": "to", "color": "#00CC77", "from": "initial training", "label": "improves results", "title": "improves results", "to": "fine-tuning", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Deep Convolutional Architecture", "label": "used_for", "title": "used_for", "to": "person re-identification", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Similarity Metric Learning", "label": "used_for", "title": "used_for", "to": "person re-identification", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Neighborhood Difference Layer", "label": "used_for", "title": "used_for", "to": "person re-identification", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Metric Learning", "label": "used_for", "title": "used_for", "to": "person re-identification", "width": 3.79}, {"arrows": "to", "color": "#CC7700", "from": "Metric Learning", "label": "utilizes", "title": "utilizes", "to": "Distribution Divergence", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "[Li, W., \u0026 Wang, X. (2013)]", "label": "published_in", "title": "published_in", "to": "CVPR", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Li, Z.", "label": "authored", "title": "authored", "to": "Learning locally-adaptive decision functions", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Li, Z.", "label": "authored", "title": "authored", "to": "Analyzing the harmonic structure in graph-based learning", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Bazzani, L.", "label": "authored", "title": "authored", "to": "Multiple-shot person re-identi\ufb01cation", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Multiple-shot person re-identi\ufb01cation", "label": "uses", "title": "uses", "to": "chromatic analyses", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Bottou, L.", "label": "authored", "title": "authored", "to": "Stochastic gradient tricks", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Davis, J. V.", "label": "authored", "title": "authored", "to": "Information-theoretic metric learning", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Farenzena, M.", "label": "authored", "title": "authored", "to": "Person re-identi\ufb01cation by symmetry-driven accumulation", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Person re-identi\ufb01cation", "label": "relies_on", "title": "relies_on", "to": "local features", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "local features", "label": "capture", "title": "capture", "to": "local contrast", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "local features", "label": "capture", "title": "capture", "to": "shape information", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "McAllester, D.", "label": "authored", "title": "authored", "to": "Object detection with discriminatively trained part-based models", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Vassileios Balntas", "label": "authored", "title": "authored", "to": "BOLD", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Vassileios Balntas", "label": "affiliated_with", "title": "affiliated_with", "to": "University of Surrey, UK", "width": 3.9699999999999998}, {"arrows": "to", "color": "#CC7700", "from": "BOLD", "label": "is_a", "title": "is_a", "to": "Binary Online Learned Descriptor", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Lilian Tang", "label": "authored", "title": "authored", "to": "BOLD", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Lilian Tang", "label": "affiliated_with", "title": "affiliated_with", "to": "University of Surrey, UK", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Krystian Mikolajczyk", "label": "authored", "title": "authored", "to": "BOLD", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Krystian Mikolajczyk", "label": "affiliated_with", "title": "affiliated_with", "to": "University of Surrey, UK", "width": 3.9699999999999998}, {"arrows": "to", "color": "#CC7700", "from": "BOLD (Binary Online Learned Descriptor)", "label": "is_a", "title": "is_a", "to": "approach", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "BOLD (Binary Online Learned Descriptor)", "label": "optimizes", "title": "optimizes", "to": "image patch", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "binary strings", "label": "represents", "title": "represents", "to": "test results", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "binary strings", "label": "indicates", "title": "indicates", "to": "subset of robust tests", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "per-patch optimization", "label": "performs_better_than", "title": "performs_better_than", "to": "global optimization", "width": 3.61}, {"arrows": "to", "color": "#0077CC", "from": "Masked Hamming distance", "label": "requires", "title": "requires", "to": "tests", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Per-patch optimization", "label": "benefits_over", "title": "benefits_over", "to": "Global optimization", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "D. G. Lowe", "label": "developed", "title": "developed", "to": "SIFT features", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Local descriptors", "label": "compared_in", "title": "compared_in", "to": "IEEE TPAMI, 27(10):1615\u20131630, 2005", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "SURF descriptor", "label": "introduced_in", "title": "introduced_in", "to": "ECCV, 2006", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "SURF descriptor", "label": "introduced_by", "title": "introduced_by", "to": "G. H. M. Brown and S. Winder", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Local image descriptors", "label": "explores_learning", "title": "explores_learning", "to": "Discriminative learning", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Discriminative learning", "label": "described_in", "title": "described_in", "to": "IEEE TPAMI, 33(1):43\u201357, 2010", "width": 3.73}, {"arrows": "to", "color": "#CC7700", "from": "Binary descriptors", "label": "related_to", "title": "related_to", "to": "Image matching", "width": 3.64}, {"arrows": "to", "color": "#CC7700", "from": "Online descriptor optimization", "label": "improves", "title": "improves", "to": "Image matching", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "G. H. M. Brown and S. Winder", "label": "explores", "title": "explores", "to": "discriminative learning", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "discriminative learning", "label": "used_in", "title": "used_in", "to": "SURF descriptor", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "interest point detection", "label": "addressed_by", "title": "addressed_by", "to": "K. Mikolajczyk and C. Schmid", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "tracking applications", "label": "relevant_to", "title": "relevant_to", "to": "Struck", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Tracking-learning-detection", "label": "discusses", "title": "discusses", "to": "integration", "width": 3.91}, {"arrows": "to", "color": "#CC7700", "from": "Tracking-learning-detection", "label": "integrates", "title": "integrates", "to": "tracking", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "keypoint recognition", "label": "method", "title": "method", "to": "random ferns", "width": 3.88}, {"arrows": "to", "color": "#CC7700", "from": "random ferns", "label": "enhances", "title": "enhances", "to": "keypoint recognition", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "M. Ozuysal", "label": "authored_by", "title": "authored_by", "to": "random ferns", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "M. Ozuysal", "label": "authored", "title": "authored", "to": "Fast keypoint recognition method", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "tracking", "label": "incorporates", "title": "incorporates", "to": "topological constraints", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "tracking", "label": "addresses", "title": "addresses", "to": "deformable objects", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "tracking", "label": "addresses", "title": "addresses", "to": "occluded objects", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "tracking", "label": "uses", "title": "uses", "to": "dynamic graph", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Fast keypoint recognition method", "label": "published_in", "title": "published_in", "to": "IEEE TPAMI", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "IEEE TPAMI", "label": "published", "title": "published", "to": "annotation of pictures", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "ORB", "label": "is_alternative_to", "title": "is_alternative_to", "to": "SIFT", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "ORB", "label": "is_alternative_to", "title": "is_alternative_to", "to": "SURF", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "SURF", "label": "developed by", "title": "developed by", "to": "Bay, H.", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "SURF", "label": "published in", "title": "published in", "to": "Computer Vision and Image Understanding", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "V. L. T. Trzcinski", "label": "authored", "title": "authored", "to": "Boosting Binary Keypoint Descriptors", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Jiajun Wu", "label": "authored", "title": "authored", "to": "Deep Multiple Instance Learning for Image Classi\ufb01cation and Auto-Annotation", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Jiajun Wu", "label": "is_author_of", "title": "is_author_of", "to": "Deep Multiple Instance Learning", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Deep Multiple Instance Learning for Image Classi\ufb01cation and Auto-Annotation", "label": "presented_at", "title": "presented_at", "to": "CVPR", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Deep Multiple Instance Learning", "label": "is_presented_at", "title": "is_presented_at", "to": "CVPR", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "Deep Multiple Instance Learning", "label": "is_a", "title": "is_a", "to": "Learning Algorithm", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Deep Multiple Instance Learning", "label": "addresses", "title": "addresses", "to": "Glaucoma", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "Yinan Yu", "label": "is_author_of", "title": "is_author_of", "to": "Deep Multiple Instance Learning", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Yinan Yu", "label": "affiliated_with", "title": "affiliated_with", "to": "Institute of Deep Learning", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Kai Yu", "label": "is_author_of", "title": "is_author_of", "to": "Deep Multiple instance Learning", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Wu_Deep_Multiple_Instance_2015_CVPR_paper.pdf", "label": "contains", "title": "contains", "to": "Deep Multiple Instance Learning", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Deep learning", "label": "achieved", "title": "achieved", "to": "tremendous improvements", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "object proposals", "label": "regarded as", "title": "regarded as", "to": "instance sets", "width": 3.91}, {"arrows": "to", "color": "#CC7700", "from": "text annotations", "label": "regarded as", "title": "regarded as", "to": "instance sets", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "systems", "label": "exploits", "title": "exploits", "to": "MIL property", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "systems", "label": "uses", "title": "uses", "to": "deep learning strategies", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "region-keyword pairs", "label": "are", "title": "are", "to": "reasonable", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "extraction", "label": "requires", "title": "requires", "to": "little supervision", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Region-keyword pairs", "label": "requires", "title": "requires", "to": "little supervision", "width": 3.7600000000000002}, {"arrows": "to", "color": "#CC7700", "from": "Multiple Instance Learning (MIL)", "label": "is a field of", "title": "is a field of", "to": "Machine Learning", "width": 3.34}, {"arrows": "to", "color": "#0077CC", "from": "Andrews et al.", "label": "studies", "title": "studies", "to": "Support vector machines", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Li \u0026 Wang", "label": "studies", "title": "studies", "to": "computerized annotation", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "computerized annotation", "label": "applied_to", "title": "applied_to", "to": "pictures", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Barnard et al.", "label": "studies", "title": "studies", "to": "Matching words and pictures", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "Image Annotation", "label": "uses", "title": "uses", "to": "Deep Learning", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Barnard", "label": "authored", "title": "authored", "to": "Matching words and pictures", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Li, L.-J.", "label": "authored", "title": "authored", "to": "Object bank", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Chen", "label": "authored", "title": "authored", "to": "Hierarchical matching", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Chen", "label": "affiliated_with", "title": "affiliated_with", "to": "University of California, Los Angeles", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Hierarchical matching", "label": "published", "title": "published", "to": "CVPR", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "Li, Q.", "label": "authored", "title": "authored", "to": "Harvesting mid-level visual concepts", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Harvesting mid-level visual concepts", "label": "published", "title": "published", "to": "CVPR", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Bing: Binarized normed gradients", "label": "published", "title": "published", "to": "CVPR", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Imaginet", "label": "is_database_of", "title": "is_database_of", "to": "hierarchical image", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Imaginet", "label": "is_database_of", "title": "is_database_of", "to": "hierarchical images", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "Imaginet", "label": "is_a", "title": "is_a", "to": "hierarchical image database", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Imaginet", "label": "presented_at", "title": "presented_at", "to": "Computer Vision and Pattern Recognition, 2009", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Rochan", "label": "authored", "title": "authored", "to": "Weakly Supervised Localization", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Weakly Supervised Localization", "label": "presented_at", "title": "presented_at", "to": "CVPR", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Mrigank Rochan", "label": "affiliated_with", "title": "affiliated_with", "to": "Massachusetts Institute of Technology", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Mrigank Rochan", "label": "affiliated_with", "title": "affiliated_with", "to": "University of Manitoba", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Mrigank Rochan", "label": "works_in", "title": "works_in", "to": "Department of Computer Science", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Chang Huang", "label": "affiliated_with", "title": "affiliated_with", "to": "Institute of Deep Learning", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "training images", "label": "requires", "title": "requires", "to": "object bounding boxes", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "weakly labeled data", "label": "is_easier_to_collect_than", "title": "is_easier_to_collect_than", "to": "training images", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "YouTube videos", "label": "provides", "title": "provides", "to": "user-generated tags", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "image search", "label": "enables_collection_of", "title": "enables_collection_of", "to": "weakly labeled images", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "collection of images", "label": "labeled_with", "title": "labeled_with", "to": "object category", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "bounding box", "label": "estimated_using", "title": "estimated_using", "to": "local density map", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "image datasets", "label": "used_for", "title": "used_for", "to": "experimental results", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "video datasets", "label": "used_for", "title": "used_for", "to": "experimental results", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Lampert et al. (2009)", "label": "addresses", "title": "addresses", "to": "unseen object classes", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "K. Grauman", "label": "co-authored", "title": "co-authored", "to": "Object-graphs for context-aware category discovery", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "R. G. Cinbis", "label": "authored", "title": "authored", "to": "Multi-fold mil training", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "J. Verbeek", "label": "co-authored", "title": "co-authored", "to": "Multi-fold mil training", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "T. Mikolov", "label": "authored", "title": "authored", "to": "Distributed representations of words and phrases", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "I. Sutskever", "label": "co-authored", "title": "co-authored", "to": "Distributed representations of words and phrases", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "M. H. Nguyen", "label": "authored", "title": "authored", "to": "Weakly supervised discrimiative localization", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "A. Papazoglou", "label": "authored", "title": "authored", "to": "Fast object segmentation", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Fast object segmentation", "label": "presented_at", "title": "presented_at", "to": "IEEE International Conference on Computer Vision", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "V. Ferrari", "label": "co-authored", "title": "co-authored", "to": "Fast object segmentation", "width": 3.82}, {"arrows": "to", "color": "#CC7700", "from": "Object-graphs", "label": "addresses", "title": "addresses", "to": "context-aware category discovery", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "Distributed representations", "label": "enables", "title": "enables", "to": "compositionality", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "IEEE International Conference on Computer Vision", "label": "presented_at", "title": "presented_at", "to": "Graph structured sparsity model", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "A. Prest", "label": "authored", "title": "authored", "to": "Learning object class detectors", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "M. Rohrbach", "label": "authored", "title": "authored", "to": "Evaluating knowledge transfer", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "M. Rohrbach", "label": "authored", "title": "authored", "to": "What helps where", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Evaluating knowledge transfer", "label": "presented_at", "title": "presented_at", "to": "IEEE Conference on Computer Vision and Pattern Recognition", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "What helps where", "label": "presented_at", "title": "presented_at", "to": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "University of Manitoba", "label": "has_department", "title": "has_department", "to": "Department of Computer Science", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Yang Wang", "label": "affiliated_with", "title": "affiliated_with", "to": "University of Manitoba", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Yang Wang", "label": "works_in", "title": "works_in", "to": "Department of Computer Science", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Mriganka Rochan", "label": "has_email", "title": "has_email", "to": "mrochan@cs.umanitoba.ca", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "rigank Rochan", "label": "affiliated_with", "title": "affiliated_with", "to": "University of Manitoba", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Wei Liu", "label": "contributor_to", "title": "contributor_to", "to": "Towards 3D Object Detection", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Wei Liu", "label": "affiliated_with", "title": "affiliated_with", "to": "Dep. of Cognitive Science", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Wei Liu", "label": "located_in", "title": "located_in", "to": "Xiamen University", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Wei Liu", "label": "author_of", "title": "author_of", "to": "Supervised Discrete Hashing", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Wei Liu", "label": "affiliated_with", "title": "affiliated_with", "to": "IBM Research", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Towards 3D Object Detection", "label": "presented_at", "title": "presented_at", "to": "CVPR", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Rongrong Ji", "label": "contributor_to", "title": "contributor_to", "to": "Towards 3D Object Detection", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Rongrong Ji", "label": "affiliated_with", "title": "affiliated_with", "to": "Dep. of Cognitive Space", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Rongrong Ji", "label": "located_in", "title": "located_in", "to": "Xiamen University", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Shaozi Li", "label": "author_of", "title": "author_of", "to": "Towards 3D Object Detection", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Shaozi Li", "label": "contributor_to", "title": "contributor_to", "to": "Towards 3D ObjectDetection", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Shaozi Li", "label": "affiliated_with", "title": "affiliated_with", "to": "Dep. of Cognitive Science", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "ywang@cs.umanitoba.ca", "label": "contact_email", "title": "contact_email", "to": "Yang Wang", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "accurate detection algorithm", "label": "requires", "title": "requires", "to": "RGB and depth modalities", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "RGB and depth modalities", "label": "are", "title": "are", "to": "correlated", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "cross-modality deep learning framework", "label": "utilizes", "title": "utilizes", "to": "deep Boltzmann Machines", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "cross-modality deep learning framework", "label": "addresses", "title": "addresses", "to": "lack of 3D training data", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "labeled 2D samples", "label": "from", "title": "from", "to": "existing datasets", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "3D CAD models", "label": "are", "title": "are", "to": "models", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "RMRC dataset", "label": "demonstrates", "title": "demonstrates", "to": "effectiveness", "width": 3.82}, {"arrows": "to", "color": "#CC7700", "from": "RMRC dataset", "label": "is", "title": "is", "to": "dataset", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "cross-modality features", "label": "captured from", "title": "captured from", "to": "RGBD data", "width": 3.91}, {"arrows": "to", "color": "#CC7700", "from": "models", "label": "can be", "title": "can be", "to": "compositional", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "models", "label": "optimized_with", "title": "optimized_with", "to": "backpropagation", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "models", "label": "learn", "title": "learn", "to": "temporal dynamics", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "models", "label": "learn", "title": "learn", "to": "convolutional perceptual representations", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "models", "label": "have", "title": "have", "to": "distinct advantages", "width": 3.61}, {"arrows": "to", "color": "#0077CC", "from": "k", "label": "utilizes", "title": "utilizes", "to": "labeled 2D samples", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "k", "label": "utilizes", "title": "utilizes", "to": "3D CAD models", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "k", "label": "addresses", "title": "addresses", "to": "lack of 3D training data", "width": 3.91}, {"arrows": "to", "color": "#CC7700", "from": "Semantic labeling", "label": "is_method_for", "title": "is_method_for", "to": "3d point clouds", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Semantic labeling", "label": "is_described_in", "title": "is_described_in", "to": "Advances in Neural Information Processing Systems", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Learning rich features", "label": "uses", "title": "uses", "to": "RGBD images", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Learning rich features", "label": "is_presented_in", "title": "is_presented_in", "to": "European Conference on Computer Vision", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "ILSVRC2012", "label": "held_in", "title": "held_in", "to": "2012", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Efficient 3d scene labeling", "label": "uses", "title": "uses", "to": "field-s of trees", "width": 3.25}, {"arrows": "to", "color": "#0077CC", "from": "Efficient 3d scene labeling", "label": "presented_at", "title": "presented_at", "to": "International Conference on Computer Vision", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "geNet", "label": "is_competition", "title": "is_competition", "to": "ILSVRC2012", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "O. Kahler", "label": "authored", "title": "authored", "to": "Efficient 3d scene labeling", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "N. Srivastava", "label": "authored", "title": "authored", "to": "Multimodal learning", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "K. Lai", "label": "authored", "title": "authored", "to": "Detection-based object labeling", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Detection-based object labeling", "label": "presented_at", "title": "presented_at", "to": "IEEE International Conference on Robotics and Automation", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "L. Bo", "label": "authored", "title": "authored", "to": "Unsupervised feature learning", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Unsupervised feature learning", "label": "focuses_on", "title": "focuses_on", "to": "rgb-d based object recognition", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "X. Xiong", "label": "authored", "title": "authored", "to": "3-d scene analysis", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "3-d scene analysis", "label": "uses", "title": "uses", "to": "sequenced predictions", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "A. Wang", "label": "authored", "title": "authored", "to": "Multi-modal unsupervised feature learning", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Multi-modal unsupervised feature learning", "label": "focuses_on", "title": "focuses_on", "to": "rgb-d scene labeling", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "Multi-modal unsupervised feature learning", "label": "presented_at", "title": "presented_at", "to": "European Conference on Computer Vision", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Abhishek Sharma", "label": "author_of", "title": "author_of", "to": "Deep Hierarchical Parsing", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Abhishek Sharma", "label": "authored", "title": "authored", "to": "Sharma_Deep_Hierarial_Parsing_2015_CVPR_paper", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Abhishek Sharma", "label": "affiliated_with", "title": "affiliated_with", "to": "Computer Science Department", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Abhishek Sharma", "label": "email", "title": "email", "to": "bhokaal@cs.umd.edu", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Abhishek Sharma", "label": "works_at", "title": "works_at", "to": "University of Maryland", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Deep Hierarchical Parsing", "label": "presented_at", "title": "presented_at", "to": "CVPR", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Oncel Tuzel", "label": "author_of", "title": "author_of", "to": "Deep Hierarchical Parsing", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Oncel Tuzel", "label": "authored", "title": "authored", "to": "Sharma_Deep_Hierarial_Parsing_2015_CVPR_paper", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Oncel Tuzel", "label": "works_at", "title": "works_at", "to": "MERL", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "David W. Jacobs", "label": "author_of", "title": "author_of", "to": "Deep Hierarchical Parsing", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "David W. Jacobs", "label": "authored", "title": "authored", "to": "Sharma_Deep_Hierarial_Parsing_2015_CVPR_paper", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "David W. Jacobs", "label": "works_at", "title": "works_at", "to": "University of Maryland", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "David W. Jacobs", "label": "affiliated_with", "title": "affiliated_with", "to": "Computer Science Department", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Sliding shapes", "label": "presented_at", "title": "presented_at", "to": "European Conference on Computer Vision", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Sharma_Deep_Hierarial_Parsing_2015_CVPR_paper", "label": "proposes", "title": "proposes", "to": "improvements to RCPN", "width": 3.91}, {"arrows": "to", "color": "#CC7700", "from": "RCPN", "label": "is_a", "title": "is_a", "to": "deep feed-forward neural network", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "RCPN", "label": "used_for", "title": "used_for", "to": "semantic segmentation", "width": 3.82}, {"arrows": "to", "color": "#00CC77", "from": "bypass error paths", "label": "hinders", "title": "hinders", "to": "contextual propagation", "width": 3.7600000000000002}, {"arrows": "to", "color": "#CCCCCC", "from": "bypass error paths", "label": "reduces", "title": "reduces", "to": "performance", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "modifications", "label": "incorporates", "title": "incorporates", "to": "classification loss", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "modifications", "label": "utilizes", "title": "utilizes", "to": "tree-style MRF", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "modifications", "label": "achieves", "title": "achieves", "to": "state-of-the-art results", "width": 3.73}, {"arrows": "to", "color": "#0077CC", "from": "classification loss", "label": "is_part_of", "title": "is_part_of", "to": "random parse trees", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "tree-style MRF", "label": "models", "title": "models", "to": "hierarchical dependencies", "width": 3.67}, {"arrows": "to", "color": "#0077CC", "from": "Tree-Style MRF", "label": "models", "title": "models", "to": "hierarchical dependencies", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Modifications", "label": "enhance", "title": "enhance", "to": "performance", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Modifications", "label": "achieves", "title": "achieves", "to": "state-of-the-art results", "width": 3.7600000000000002}, {"arrows": "to", "color": "#CC7700", "from": "Semantic Segmentation", "label": "uses", "title": "uses", "to": "Deep Neural Networks", "width": 3.64}, {"arrows": "to", "color": "#CC7700", "from": "Semantic Segmentation", "label": "is_related_to", "title": "is_related_to", "to": "Image Alignment", "width": 3.0999999999999996}, {"arrows": "to", "color": "#CC7700", "from": "Semantic Segmentation", "label": "uses", "title": "uses", "to": "Regions", "width": 3.7600000000000002}, {"arrows": "to", "color": "#CC7700", "from": "Semantic Segmentation", "label": "uses", "title": "uses", "to": "Parts", "width": 3.7600000000000002}, {"arrows": "to", "color": "#CC7700", "from": "Recursive Context Propagation Network (RCPN)", "label": "related_to", "title": "related_to", "to": "Contextual Propagation", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Socher et al. (2011)", "label": "researches", "title": "researches", "to": "Recursive Neural Networks", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "Farabet et al. (2013)", "label": "researches", "title": "researches", "to": "scene labeling", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "Fergus and Eigen (2012)", "label": "researches", "title": "researches", "to": "image parsing", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Markov Random Fields (MRF)", "label": "used_in", "title": "used_in", "to": "Semantic Segmentation", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Najman", "label": "authored", "title": "authored", "to": "Learning hierarchical features for scene labeling", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Learning hierarchical features for scene labeling", "label": "presented_at", "title": "presented_at", "to": "IEEE TPAM", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "R. Fergus", "label": "authored", "title": "authored", "to": "Nonparametric image parsing", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Nonparametric image parsing", "label": "presented_at", "title": "presented_at", "to": "IEEE CVPR", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "A. Torralba", "label": "authored", "title": "authored", "to": "Context-based vision system", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Context-based vision system", "label": "presented_at", "title": "presented_at", "to": "IEEE CVPR", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "P. H. O. Pinheiro", "label": "authored", "title": "authored", "to": "Recurrent convolutional neural networks", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Recurrent convolutional neural networks", "label": "presented_at", "title": "presented_at", "to": "ICML", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "A. Sharma", "label": "authored", "title": "authored", "to": "Recursive context propagation network", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Recursive context propagation network", "label": "presented_at", "title": "presented_at", "to": "NIPS", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "J. Tighe", "label": "authored", "title": "authored", "to": "Finding things", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "J. Tighe", "label": "authored", "title": "authored", "to": "Superparsing", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Finding things", "label": "presented_at", "title": "presented_at", "to": "IEEE CVPR", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "R. Mottaghi", "label": "authored", "title": "authored", "to": "Analyzing semantic segmentation", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Analyzing semantic segmentation", "label": "presented_at", "title": "presented_at", "to": "IEEE CVPR", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Superparsing", "label": "published_in", "title": "published_in", "to": "Int. J. Comput. Vision", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "IEEE CVPR", "label": "is_conference_of", "title": "is_conference_of", "to": "Computer Vision", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "IEEE CVPR", "label": "publishes", "title": "publishes", "to": "Bell et al.", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "ICML", "label": "is_publication_venue", "title": "is_publication_venue", "to": "research paper", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Junlin Hu", "label": "is_author_of", "title": "is_author_of", "to": "Deep Transfer Metric Learning", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Junlin Hu", "label": "affiliated_with", "title": "affiliated_with", "to": "Computer Science Department", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Junlin Hu", "label": "affiliated_with", "title": "affiliated_with", "to": "School of Electrical and Electronic Engineering", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Junlin Hu", "label": "email", "title": "email", "to": "jhu007@e.ntu.edu.sg", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Deep Transfer Metric Learning", "label": "is_paper_in", "title": "is_paper_in", "to": "IEEE CVPR", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Jiwen Lu", "label": "is_author_of", "title": "is_author_of", "to": "Deep Transfer Metric Learning", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Jiwen Lu", "label": "affiliated_with", "title": "affiliated_with", "to": "Advanced Digital Sciences Center", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Jiwen Lu", "label": "affiliated_with", "title": "affiliated_with", "to": "Nanyang Technological University", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Jiwen Lu", "label": "email", "title": "email", "to": "jiwen.lu@adsc.com.sg", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "metric learning methods", "label": "typically_assume", "title": "typically_assume", "to": "similar scenarios", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "real-world visual recognition applications", "label": "often_unmet", "title": "often_unmet", "to": "similar scenarios", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "DTML method", "label": "addresses", "title": "addresses", "to": "challenge", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "DTML method", "label": "transfers", "title": "transfers", "to": "discriminative knowledge", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "DTML method", "label": "maximizes", "title": "maximizes", "to": "inter-class variations", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "DTML method", "label": "minimizes", "title": "minimizes", "to": "intra-class variations", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "DTML method", "label": "minimizes", "title": "minimizes", "to": "distribution divergence", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "DSTML method", "label": "optimizes", "title": "optimizes", "to": "outputs of hidden and top layers", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "DSTML method", "label": "is_a", "title": "is_a", "to": "deeply supervised transfer metric learning", "width": 3.9699999999999998}, {"arrows": "to", "color": "#CC7700", "from": "DSTML method", "label": "is", "title": "is", "to": "transfer metric learning method", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "DSTML method", "label": "developed_for", "title": "developed_for", "to": "cross-dataset tasks", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "vergence", "label": "exists_between", "title": "exists_between", "to": "source and target domains", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "hidden layers", "label": "part_of", "title": "part_of", "to": "neural network", "width": 3.25}, {"arrows": "to", "color": "#CC7700", "from": "top layers", "label": "part_of", "title": "part_of", "to": "neural network", "width": 3.25}, {"arrows": "to", "color": "#CC7700", "from": "Deep Transfer Metric Learning (DTML)", "label": "is_subfield_of", "title": "is_subfield_of", "to": "Metric Learning", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "Deep Transfer Metric Learning (DTML)", "label": "addresses", "title": "addresses", "to": "Cross-Domain Visual Recognition", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Deep Transfer Metric Learning (DTML)", "label": "employs", "title": "employs", "to": "Deep Neural Networks", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "Cross-Domain Visual Recognition", "label": "benefits_from", "title": "benefits_from", "to": "Deep Neural Networks", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Face Description", "label": "uses", "title": "uses", "to": "Local Binary Patterns", "width": 3.79}, {"arrows": "to", "color": "#CC7700", "from": "Learning Deep Architectures", "label": "contributes_to", "title": "contributes_to", "to": "AI", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Predictive Structures", "label": "learned_from", "title": "learned_from", "to": "Multiple Tasks", "width": 3.58}, {"arrows": "to", "color": "#0077CC", "from": "Journal of Machine Learning Research", "label": "is_publication_of", "title": "is_publication_of", "to": "Machine Learning", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Foundations and Trends in Machine Learning", "label": "is_publication_of", "title": "is_publication_of", "to": "Machine Learning", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "ACM Multimedia", "label": "is_publication_of", "title": "is_publication_of", "to": "ACM", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "ACM Multimedia", "label": "publishes", "title": "publishes", "to": "Graph-based search methods", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Chen, D.", "label": "authored", "title": "authored", "to": "Bayesian face revisited", "width": 3.9699999999999998}, {"arrows": "to", "color": "#CC7700", "from": "Bayesian face revisited", "label": "provides", "title": "provides", "to": "joint formulation", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Duan, L.", "label": "authored", "title": "authored", "to": "Domain transfer SVM", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Conference on Computer Vision and Pattern Recognition", "label": "is_conference_of", "title": "is_conference_of", "to": "Computer Vision", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Gray \u0026 Tao (2008)", "label": "presented_at", "title": "presented_at", "to": "European Conference on Computer Vision", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Gretton et al. (2006)", "label": "presented_at", "title": "presented_at", "to": "Neural Information Processing Systems", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Neural Information Processing Systems", "label": "hosts", "title": "hosts", "to": "Imaginet classification with deep convolutional neural networks", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Hinton et al. (2006)", "label": "presented_at", "title": "presented_at", "to": "Neural Computation", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Huang et al. (2012)", "label": "presented_at", "title": "presented_at", "to": "Conference on Computer Vision and Pattern Recognition", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Nanyang Technological University", "label": "located_in", "title": "located_in", "to": "Singapore", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Yap-Peng Tan", "label": "email", "title": "email", "to": "eyptan@ntu.edu.sg", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Takuya Narihira", "label": "author_of", "title": "author_of", "to": "Learning Lightness from Human Judgement", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Learning Lightness from Human Judgement", "label": "presented_at", "title": "presented_at", "to": "CVPR", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Michael Maire", "label": "author_of", "title": "author_of", "to": "Learning Lightness from Human Judgement", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Michael Maire", "label": "affiliated_with", "title": "affiliated_with", "to": "TTI Chicago", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "Stella X. Yu", "label": "author_of", "title": "author_of", "to": "Learning Lightness from Human Judgement", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Stella X. Yu", "label": "affiliated_with", "title": "affiliated_with", "to": "UC Berkeley", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "Stella X. Yu", "label": "affiliated_with", "title": "affiliated_with", "to": "ICSI", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "Narihira_Learning_Lightness_From_2015_CVPR_paper.pdf", "label": "contains", "title": "contains", "to": "Learning Lightness from Human Judgement", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Narihira_Learning_Lightness_From_2015_CVPR_paper", "label": "addresses", "title": "addresses", "to": "inferring lightness", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "inferring lightness", "label": "is_problem_of", "title": "is_problem_of", "to": "perceived re\ufb02ectance", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "classic methods", "label": "view problem_as", "title": "view problem_as", "to": "intrinsic image decomposition", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "intrinsic image decomposition", "label": "separates", "title": "separates", "to": "re\ufb02ectance and shading components", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "intrinsic image decomposition", "label": "relies_on", "title": "relies_on", "to": "lightness perception", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "patch representations", "label": "are_built_using", "title": "are_built_using", "to": "deep networks", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "deep networks", "label": "exploits", "title": "exploits", "to": "global saliency cues", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "local lightness model", "label": "achieves_performance", "title": "achieves_performance", "to": "on-par with global lightness model", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "local lightness model", "label": "achieves_performance", "title": "achieves_performance", "to": "on-par performance", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "local lightness model", "label": "competes_with", "title": "competes_with", "to": "state-of-the-art global lightness model", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "local lightness model", "label": "is_dataset", "title": "is_dataset", "to": "ld dataset", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "global lightness model", "label": "incorporates", "title": "incorporates", "to": "shading/re\ufb02ectance priors", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "global lightness model", "label": "uses", "title": "uses", "to": "dense conditional random field formulation", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "state-of-the-art global lightness model", "label": "incorporates", "title": "incorporates", "to": "shading/reflectance priors", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "state-of-the-art global lightness model", "label": "uses", "title": "uses", "to": "dense conditional random field formulation", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "state-of-the-art global lightness model", "label": "incorporates", "title": "incorporates", "to": "multiple priors", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "dense conditional random field formulation", "label": "enables", "title": "enables", "to": "simultaneous reasoning", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "simultaneous reasoning", "label": "occurs_between", "title": "occurs_between", "to": "pairs of pixels", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "lightness perception", "label": "is_concept_in", "title": "is_concept_in", "to": "cognitive neurosciences", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "lightness perception", "label": "is_foundation_for", "title": "is_foundation_for", "to": "intrinsic image decomposition", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Lightness perception and lightness illusions", "label": "provides", "title": "provides", "to": "foundational work", "width": 3.91}, {"arrows": "to", "color": "#CC7700", "from": "foundational work", "label": "related_to", "title": "related_to", "to": "feature learning", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "H. G. Barrow", "label": "authored", "title": "authored", "to": "Recovering intrinsic scene characteristics from images", "width": 3.9699999999999998}, {"arrows": "to", "color": "#CC7700", "from": "Recovering intrinsic scene characteristics from images", "label": "lays_groundwork_for", "title": "lays_groundwork_for", "to": "intrinsic image algorithms", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "intrinsic image algorithms", "label": "evaluated_using", "title": "evaluated_using", "to": "baseline evaluations", "width": 3.64}, {"arrows": "to", "color": "#CC7700", "from": "relative reflectance", "label": "is_concept_in", "title": "is_concept_in", "to": "intrinsic image decomposition", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "human judgment data", "label": "is_related_to", "title": "is_related_to", "to": "lightness perception", "width": 3.0999999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Computer Vision Systems", "label": "publishes", "title": "publishes", "to": "scene characteristics from images", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "scene characteristics from images", "label": "lays groundwork for", "title": "lays groundwork for", "to": "intrinsic image algorithms", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Retinex theory", "label": "introduced by", "title": "introduced by", "to": "E. H. Land and J. J. McCann", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "Retinex theory", "label": "explains", "title": "explains", "to": "brightness perception", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Retinex theory", "label": "relevant to", "title": "relevant to", "to": "intrinsic image recovery", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "determining lightness from an image", "label": "is part of", "title": "is part of", "to": "intrinsic image decomposition", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "M. Tappen, W. Freeman, and E. Adelson", "label": "addresses", "title": "addresses", "to": "intrinsic image recovery", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "intrinsic image recovery", "label": "uses", "title": "uses", "to": "single image", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "intrinsic image recovery", "label": "addressed_in", "title": "addressed_in", "to": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 2005", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "R. Grosse", "label": "authored", "title": "authored", "to": "Ground truth dataset", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Ground truth dataset", "label": "evaluated_for", "title": "evaluated_for", "to": "intrinsic image algorithms", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "International Conference on Computer Vision, 2009", "label": "hosts", "title": "hosts", "to": "Ground truth dataset", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Y. Tang", "label": "authored", "title": "authored", "to": "Deep Lambertian networks", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "Deep Lambertian networks", "label": "employs", "title": "employs", "to": "Lambertian reflectance models", "width": 3.88}, {"arrows": "to", "color": "#CC7700", "from": "Deep Lambertian networks", "label": "aims_to", "title": "aims_to", "to": "intrinsic image decomposition", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "Lambertian reflectance models", "label": "used_in", "title": "used_in", "to": "Deep Lambertian networks", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Lambertian reflectance models", "label": "used in", "title": "used in", "to": "deep learning approaches", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "International Conference on Machine Learning, 2012", "label": "hosts", "title": "hosts", "to": "Deep Lambertian networks", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Lambertian networks", "label": "explores", "title": "explores", "to": "deep learning approaches", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "deep learning approaches", "label": "used for", "title": "used for", "to": "intrinsic image decomposition", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Takaya Narihira", "label": "affiliated_with", "title": "affiliated_with", "to": "UC Berkeley", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "Takaya Narihira", "label": "affiliated_with", "title": "affiliated_with", "to": "ICSI", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "Takaya Narihira", "label": "affiliated_with", "title": "affiliated_with", "to": "Sony Corp.", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "ICSI", "label": "is_part_of", "title": "is_part_of", "to": "UC Berkeley", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Hierarchical-PEP Model", "label": "addresses", "title": "addresses", "to": "face recognition", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "face recognition", "label": "requires", "title": "requires", "to": "Labelled faces in the wild", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Hierarchical-PEP model", "label": "addresses", "title": "addresses", "to": "Pose variation", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Hierarchical-PEP model", "label": "is_inspired_by", "title": "is_inspired_by", "to": "Probabilistic Elastic Part (PEP) model", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Hierarchical-PEP model", "label": "is_inspired_by", "title": "is_inspired_by", "to": "Deep Hierarchical Architectures", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Hierarchical-PEP model", "label": "exploits", "title": "exploits", "to": "Fine-grained Structures", "width": 3.73}, {"arrows": "to", "color": "#0077CC", "from": "Hierarchical-PEP model", "label": "is_guided_by", "title": "is_guided_by", "to": "Supervised Information", "width": 3.67}, {"arrows": "to", "color": "#0077CC", "from": "Hierarchical-PEP model", "label": "is_verified_on", "title": "is_verified_on", "to": "LFW", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Hierarchical-PEP model", "label": "is_verified_on", "title": "is_verified_on", "to": "YouTube Faces", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Hierarchical-PEP model", "label": "is_verified_on", "title": "is_verified_on", "to": "PaSC", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Face Image", "label": "is_decomposed_into", "title": "is_decomposed_into", "to": "Face Parts", "width": 3.79}, {"arrows": "to", "color": "#CC7700", "from": "Face Parts", "label": "has_level", "title": "has_level", "to": "Detail Level", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Face Part Representations", "label": "is_stacked_at", "title": "is_stacked_at", "to": "Layer", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Layer", "label": "reduces", "title": "reduces", "to": "Dimensionality", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "Face Representation", "label": "is", "title": "is", "to": "Invariant", "width": 3.61}, {"arrows": "to", "color": "#0077CC", "from": "LFW", "label": "is_a", "title": "is_a", "to": "dataset", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "LFW", "label": "is_used_in", "title": "is_used_in", "to": "experiments", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "face parts", "label": "is_analyzed_by", "title": "is_analyzed_by", "to": "supervised information", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "face recognition challenge", "label": "is", "title": "is", "to": "PaSC", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "PEP (Probabilistic Elastic Part) Model", "label": "is_guided_by", "title": "is_guided_by", "to": "supervised information", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Ahonen, T.", "label": "authored", "title": "authored", "to": "Face recognition with local binary patterns", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Face recognition with local binary patterns", "label": "presented_in", "title": "presented_in", "to": "European Conference on Computer Vision", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Grauman, K.", "label": "authored", "title": "authored", "to": "The pyramid match kernel", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "The pyramid match kernel", "label": "presented_in", "title": "presented_in", "to": "IEEE International Conference on Computer Vision", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Hu, J.", "label": "authored", "title": "authored", "to": "Discriminative deep metric learning", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Hu, J.", "label": "authored", "title": "authored", "to": "Large margin multi-metric learning", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Hu, J.", "label": "affiliated_with", "title": "affiliated_with", "to": "Stevens Institute of Technology", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Discriminative deep metric learning", "label": "used_for", "title": "used_for", "to": "face verification", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "Eigenfaces", "label": "compared_with", "title": "compared_with", "to": "Fisherfaces", "width": 3.79}, {"arrows": "to", "color": "#CC7700", "from": "Fisherfaces", "label": "is_a", "title": "is_a", "to": "linear projection", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "Unsupervised joint alignment", "label": "applied_to", "title": "applied_to", "to": "complex images", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Labeled faces in the wild", "label": "provides", "title": "provides", "to": "reporting procedures", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Large margin multi-metric learning", "label": "used_for", "title": "used_for", "to": "face verification", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Large margin multi-metric learning", "label": "used_for", "title": "used_for", "to": "kinship verification", "width": 3.88}, {"arrows": "to", "color": "#CC7700", "from": "Asian Conference on Computer Vision (ACCV)", "label": "is_a", "title": "is_a", "to": "conference", "width": 3.79}, {"arrows": "to", "color": "#CC7700", "from": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition", "label": "is_a", "title": "is_a", "to": "publication", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Lei, Z.", "label": "authored", "title": "authored", "to": "discriminant face descriptor", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Simonyan, K.", "label": "authored", "title": "authored", "to": "Deep fisher networks", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Simonyan, K.", "label": "authored", "title": "authored", "to": "Very deep convolutional networks", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Yu Kong", "label": "contributed_to", "title": "contributed_to", "to": "Bilinear Heterogeneous Information Machine", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Yu Kong", "label": "is_author_of", "title": "is_author_of", "to": "low-rank bilinear classification", "width": 3.25}, {"arrows": "to", "color": "#0077CC", "from": "Bilinear Heterogeneous Information Machine", "label": "is_paper_title", "title": "is_paper_title", "to": "Kong_Bilinear_Heterogeneous_Information_2015_CVPR_paper.pdf", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Bilinear Heterogeneous Information Machine", "label": "addresses_problem", "title": "addresses_problem", "to": "RGB-D Action Recognition", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Yun Fu", "label": "contributed_to", "title": "contributed_to", "to": "Bilinear Heterogeneous Information Machine", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Yun Fu", "label": "affiliated_with", "title": "affiliated_with", "to": "Northeastern University", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Yun Fu", "label": "email", "title": "email", "to": "yunfu@ece.neu.edu", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Kong_Bilinear_Heterogeneous_Information_2015_CVPR_paper.pdf", "label": "is_published_in", "title": "is_published_in", "to": "CVPR", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "space", "label": "is", "title": "is", "to": "learned", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "knowledge", "label": "is", "title": "is", "to": "shared", "width": 3.61}, {"arrows": "to", "color": "#0077CC", "from": "RGB-D action datasets", "label": "are", "title": "are", "to": "public datasets", "width": 3.55}, {"arrows": "to", "color": "#CCCCCC", "from": "low-rank classifier", "label": "improves", "title": "improves", "to": "generalization power", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "low-rank classifier", "label": "minimizes", "title": "minimizes", "to": "eter", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "promising results", "label": "when", "title": "when", "to": "RGB data are missing", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "promising results", "label": "when", "title": "when", "to": "depth data are missing", "width": 3.7600000000000002}, {"arrows": "to", "color": "#CC7700", "from": "Argyriou et al. (2008)", "label": "provides", "title": "provides", "to": "foundational work", "width": 3.7600000000000002}, {"arrows": "to", "color": "#CC7700", "from": "feature learning", "label": "relevant_to", "title": "relevant_to", "to": "Action Recognition", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "Bo et al. (2011)", "label": "deals_with", "title": "deals_with", "to": "object recognition", "width": 3.73}, {"arrows": "to", "color": "#0077CC", "from": "Bo et al. (2011)", "label": "uses", "title": "uses", "to": "kernel descriptors", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Do and Artieres (2009)", "label": "introduces", "title": "introduces", "to": "training method", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "HMMs", "label": "often_used_in", "title": "often_used_in", "to": "Action Recognition", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Artieres et al.", "label": "presents", "title": "presents", "to": "training method", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Hidden Markov Models", "label": "used_in", "title": "used_in", "to": "action recognition", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Had\ufb01eld and Bowden", "label": "addresses", "title": "addresses", "to": "action recognition", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Ji et al.", "label": "presents", "title": "presents", "to": "3D Convolutional Neural Networks", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "3D Convolutional Neural Networks", "label": "dominant_in", "title": "dominant_in", "to": "action recognition", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "Kobayashi", "label": "introduces", "title": "introduces", "to": "low-rank bilinear classification", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "low-rank bilinear classification", "label": "is_method_for", "title": "is_method_for", "to": "modeling complex interactions", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "spatio-temporal depth cuboid similarity feature", "label": "is_feature_representation_for", "title": "is_feature_representation_for", "to": "activity recognition", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "activity recognition", "label": "requires", "title": "requires", "to": "depth data", "width": 3.46}, {"arrows": "to", "color": "#CC7700", "from": "information bottleneck method", "label": "is_applicable_to", "title": "is_applicable_to", "to": "feature selection", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "information bottleneck method", "label": "is_applicable_to", "title": "is_applicable_to", "to": "representation learning", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "depth camera", "label": "used_for", "title": "used_for", "to": "activity recognition", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "depth sequences", "label": "used_for", "title": "used_for", "to": "action recognition", "width": 3.4000000000000004}, {"arrows": "to", "color": "#CC7700", "from": "HON4D", "label": "is_based_on", "title": "is_based_on", "to": "oriented 4D normals", "width": 3.0999999999999996}, {"arrows": "to", "color": "#CC7700", "from": "information bottlenecks", "label": "is_concept", "title": "is_concept", "to": "feature selection", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "information bottlenecks", "label": "is_concept", "title": "is_concept", "to": "representation learning", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Sebastian Haner", "label": "authored", "title": "authored", "to": "Absolute Pose for Cameras", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Sebastian Haner", "label": "affiliated_with", "title": "affiliated_with", "to": "Centre for Mathematical Sciences", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Sebastian Haner", "label": "affiliated_with", "title": "affiliated_with", "to": "Lund University", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Sebastian Haner", "label": "affiliated_with", "title": "affiliated_with", "to": "Centre for Mathematical Sciences, Lund University", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Sebastian Haner", "label": "has_email", "title": "has_email", "to": "haner@maths.lth.se", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Absolute Pose for Cameras", "label": "presented_at", "title": "presented_at", "to": "CVPR", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "Absolute Pose for Cameras", "label": "addresses", "title": "addresses", "to": "camera pose estimation", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "Absolute Pose for Cameras", "label": "involves", "title": "involves", "to": "refractive interfaces", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "Kalle \u02daAstr\u00a8om", "label": "authored", "title": "authored", "to": "Absolute Pose for Cameras", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Kalle \u02daAstr\u00a8om", "label": "affiliated_with", "title": "affiliated_with", "to": "Workshop on Omnidirectional Vision", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Kalle \u02daAstr\u00a8om", "label": "affiliated_with", "title": "affiliated_with", "to": "Centre for Mathematical Sciences, Lund University", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Kalle \u02daAstr\u00a8om", "label": "has_email", "title": "has_email", "to": "kalle@maths.lth.se", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Yukong", "label": "email", "title": "email", "to": "yukong@ece.neu.edu", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "perspective camera", "label": "observes", "title": "observes", "to": "scene", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "scene", "label": "has_property", "title": "has_property", "to": "rigidity", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "refractive plane", "label": "is_boundary_between", "title": "is_boundary_between", "to": "transparent media", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "solvers", "label": "developed_for", "title": "developed_for", "to": "2D cases", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "solvers", "label": "are", "title": "are", "to": "minimal", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "solvers", "label": "evaluated_on", "title": "evaluated_on", "to": "synthetic data", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "solvers", "label": "evaluated_on", "title": "evaluated_on", "to": "real data", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Snell\u2019s law", "label": "gives_rise_to", "title": "gives_rise_to", "to": "false solutions", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "false solutions", "label": "increases", "title": "increases", "to": "complexity of problem", "width": 3.79}, {"arrows": "to", "color": "#00CC77", "from": "pose estimates", "label": "requires", "title": "requires", "to": "explicitly modelling refraction", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "Absolute Pose Estimation", "label": "requires", "title": "requires", "to": "Refractive Interfaces Modelling", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Refractive Interfaces", "label": "governed_by", "title": "governed_by", "to": "Snell\u0027s Law", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "Structure-and-Motion", "label": "uses", "title": "uses", "to": "Camera Calibration", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "Structure-and-Motion", "label": "benefits_from", "title": "benefits_from", "to": "Polynomial Equation Solving", "width": 3.25}, {"arrows": "to", "color": "#0077CC", "from": "Agrawal et al. (2012)", "label": "introduces", "title": "introduces", "to": "Multi-layer Flat Refractive Geometry Theory", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Byr\u00a8od et al. (2009)", "label": "focuses_on", "title": "focuses_on", "to": "Polynomial Equation Solving", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Polynomial Equation Solving", "label": "applied_to", "title": "applied_to", "to": "Computer Vision", "width": 3.64}, {"arrows": "to", "color": "#00CC77", "from": "Multi-layer Flat Refractive Geometry", "label": "improves", "title": "improves", "to": "Pose Estimation Accuracy", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "polynomial equation solving", "label": "applied_to", "title": "applied_to", "to": "computer vision", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "Ideals, Varieties, and Algorithms", "label": "focuses_on", "title": "focuses_on", "to": "computational algebraic geometry", "width": 3.7600000000000002}, {"arrows": "to", "color": "#CC7700", "from": "Ideals, Varieties, and Algorithms", "label": "focuses_on", "title": "focuses_on", "to": "commutative algebra", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "\u02daAstr\u00a8om, Kuang, \u0026 Ask", "label": "researched", "title": "researched", "to": "polynomial equation solving optimization", "width": 3.88}, {"arrows": "to", "color": "#CC7700", "from": "polynomial equation solving optimization", "label": "related_to", "title": "related_to", "to": "p-fold symmetries", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Chari \u0026 Sturm", "label": "researched", "title": "researched", "to": "multi-view geometry", "width": 3.82}, {"arrows": "to", "color": "#CC7700", "from": "multi-view geometry", "label": "concerns", "title": "concerns", "to": "refractive plane", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Chari, V.", "label": "authored", "title": "authored", "to": "Multi-view geometry of the refractive plane", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Multi-view geometry of the refractive plane", "label": "cited_by", "title": "cited_by", "to": "British Machine Vision Conference", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Sturm, P. F.", "label": "co-authored", "title": "co-authored", "to": "Multi-view geometry of the refractive plane", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Fitzgibbon, A. W.", "label": "authored", "title": "authored", "to": "Simultaneous linear estimation", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Fitzgibbon, A. W.", "label": "presented_at", "title": "presented_at", "to": "Conference on Computer Vision and Pattern Recognition", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Simultaneous linear estimation", "label": "relevant_to", "title": "relevant_to", "to": "geometric estimation", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Simultaneous linear estimation", "label": "addresses", "title": "addresses", "to": "lens distortion", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "Kuang, Y.", "label": "co-authored", "title": "co-authored", "to": "Numerically stable optimization", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Numerically stable optimization", "label": "focuses_on", "title": "focuses_on", "to": "polynomial solvers", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "chmid", "label": "edited", "title": "edited", "to": "European Conference on ComputerVision", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "European Conference on ComputerVision", "label": "is_volume_of", "title": "is_volume_of", "to": "Lecture Notes in Computer Science", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "European Conference on ComputerVision", "label": "focuses_on", "title": "focuses_on", "to": "polynomial solver optimization", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Lecture Notes in Computer Science", "label": "publishes", "title": "publishes", "to": "Computer Vision - ECCV 2008", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Lecture Notes in Computer Science", "label": "has_volume", "title": "has_volume", "to": "5304", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Nist\u00e9r", "label": "addresses", "title": "addresses", "to": "generalized 3-point pose problem", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Stew\u00e9nius", "label": "addresses", "title": "addresses", "to": "generalized relative pose problems", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "Stew\u00e9nius", "label": "related_to", "title": "related_to", "to": "pose estimation", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Kukelova", "label": "focuses_on", "title": "focuses_on", "to": "polynomial eigenvalue solutions", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Kukelova", "label": "focuses_on", "title": "focuses_on", "to": "minimal problems", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "R6P", "label": "is_topic_of", "title": "is_topic_of", "to": "Albl_R6P_-_Rolling_2015_CVPR_paper", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Albl_R6P_-_Rolling_2015_CVPR_paper", "label": "published_in", "title": "published_in", "to": "CVPR", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Albl_R6P_-_Rolling_2015_CVPR_paper", "label": "focuses_on", "title": "focuses_on", "to": "polynomial solutions", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Cenek Albl", "label": "is_author_of", "title": "is_author_of", "to": "Albl_R6P_-_Rolling_2015_CVPR_paper", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Cenek Albl", "label": "affiliated_with", "title": "affiliated_with", "to": "Czech Technical University in Prague", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Zuana Kukelova", "label": "is_author_of", "title": "is_author_of", "to": "Albi_R6P_-_Rolling_2015_CVPR_paper", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Zuana Kukelova", "label": "affiliated_with", "title": "affiliated_with", "to": "Microsoft Research Ltd", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Tomas Pajdla", "label": "is_author_of", "title": "is_author_of", "to": "Albl_R6P_-_Rolling_2015_CVPR_paper", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Tomas Pajdla", "label": "affiliated_with", "title": "affiliated_with", "to": "Czech Technical University in Prague", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "absolute pose problem", "label": "is_problem_in", "title": "is_problem_in", "to": "computer vision", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "rolling shutter", "label": "present_in", "title": "present_in", "to": "digital cameras", "width": 3.91}, {"arrows": "to", "color": "#CC7700", "from": "camera model", "label": "is_type_of", "title": "is_type_of", "to": "model", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "camera model", "label": "is_verified_for", "title": "is_verified_for", "to": "polynomial solver", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "camera orientation", "label": "is_approximated_by", "title": "is_approximated_by", "to": "linear approximation", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "camera orientation", "label": "has_error", "title": "has_error", "to": "6 degrees", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "linear approximation", "label": "is_valid_around", "title": "is_valid_around", "to": "identity rotation", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "P3P algorithm", "label": "estimates", "title": "estimates", "to": "camera orientation", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "P3P algorithm", "label": "can_be_used_to", "title": "can_be_used_to", "to": "estimate camera orientation", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "P3P algorithm", "label": "brings", "title": "brings", "to": "camera rotation matrix", "width": 3.73}, {"arrows": "to", "color": "#0077CC", "from": "camera rotation velocity", "label": "reaches", "title": "reaches", "to": "30deg/frame", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Ithm", "label": "estimates", "title": "estimates", "to": "camera orientation", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "camera rotation matrix", "label": "approaches", "title": "approaches", "to": "identity", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "camera position", "label": "has_error", "title": "has_error", "to": "2%", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "orientation error", "label": "is_less_than", "title": "is_less_than", "to": "0.5 degrees", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "orientation error", "label": "is_less_than", "title": "is_less_than", "to": "half a degree", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "Rolling Shutter Cameras", "label": "deals_with", "title": "deals_with", "to": "Absolute Pose Problem", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "Absolute Pose Problem", "label": "requires", "title": "requires", "to": "Polynomial Solvers", "width": 3.25}, {"arrows": "to", "color": "#CC7700", "from": "Linearized Camera Models", "label": "used_in", "title": "used_in", "to": "Absolute Pose Problem", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "relative position error", "label": "is_less_than", "title": "is_less_than", "to": "2%", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "RANSAC", "label": "is_technique_for", "title": "is_technique_for", "to": "robust model fitting", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "RANSAC", "label": "is_algorithm_for", "title": "is_algorithm_for", "to": "model fitting", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "robust model fitting", "label": "is_relevant_to", "title": "is_relevant_to", "to": "computer vision problems", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "structure and motion estimation", "label": "is_focus_of", "title": "is_focus_of", "to": "paper", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "structure and motion estimation", "label": "is_problem_in", "title": "is_problem_in", "to": "computer vision", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Haralick", "label": "authored_paper", "title": "authored_paper", "to": "pose estimation", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Hedborg", "label": "authored_paper", "title": "authored_paper", "to": "structure and motion estimation", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "RANAC", "label": "is_core_technique", "title": "is_core_technique", "to": "automated cartography", "width": 3.4000000000000004}, {"arrows": "to", "color": "#CC7700", "from": "RANAC", "label": "is_technique_for", "title": "is_technique_for", "to": "robust model fitting", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "RANAC", "label": "is_relevant_to", "title": "is_relevant_to", "to": "computer vision problems", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "rolling shutter video", "label": "has_property", "title": "has_property", "to": "structure and motion estimation", "width": 3.7600000000000002}, {"arrows": "to", "color": "#00CC77", "from": "rolling shutter video", "label": "benefits_from", "title": "benefits_from", "to": "inertial measurements", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "rolling shutter video", "label": "requires", "title": "requires", "to": "motion estimation", "width": 3.55}, {"arrows": "to", "color": "#00CC77", "from": "rolling shutter video", "label": "improves", "title": "improves", "to": "accuracy", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "rolling shutter data", "label": "uses", "title": "uses", "to": "bundle adjustment", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "bundle adjustment", "label": "applies_to", "title": "applies_to", "to": "rolling shutter data", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "C. Jia and B. L. Evans", "label": "combines", "title": "combines", "to": "rolling shutter video", "width": 3.91}, {"arrows": "to", "color": "#00CC77", "from": "inertial measurements", "label": "improves", "title": "improves", "to": "accuracy", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "motion estimation", "label": "relevant_to", "title": "relevant_to", "to": "video analysis", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "ter video recti\ufb01cation", "label": "combines", "title": "combines", "to": "rolling shutter video", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "ter video recti\ufb01cation", "label": "combines", "title": "combines", "to": "inertial measurements", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "visual SLAM", "label": "relevant_to", "title": "relevant_to", "to": "camera pose estimation", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "parallel tracking and mapping", "label": "introduced_in", "title": "introduced_in", "to": "ISMAR \u201909", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "EEE International Symposium on Mixed and augmented Reality", "label": "introduces", "title": "introduces", "to": "visual SLAM", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Z. Kukelova", "label": "authors", "title": "authors", "to": "Singly-bordered block-diagonal form", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Z. Kukelova", "label": "authors", "title": "authors", "to": "Automatic generator of minimal problem solvers", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Singly-bordered block-diagonal form", "label": "addresses", "title": "addresses", "to": "optimization techniques", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "efficient computation", "label": "relates_to", "title": "relates_to", "to": "salient regions", "width": 3.7600000000000002}, {"arrows": "to", "color": "#CC7700", "from": "Automatic generator of minimal problem solvers", "label": "related_to", "title": "related_to", "to": "efficient solvers", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "efficient solvers", "label": "contributes_to", "title": "contributes_to", "to": "efficient computation", "width": 3.25}, {"arrows": "to", "color": "#0077CC", "from": "ECCV 2008", "label": "publishes", "title": "publishes", "to": "Automatic generator of minimal problem solvers", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Proceedings", "label": "part_of", "title": "part_of", "to": "Lecture Notes in Computer Science", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Cox", "label": "authored", "title": "authored", "to": "Using Algebraic Geometry", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Using Algebraic Geometry", "label": "published_by", "title": "published_by", "to": "Springer", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Springer", "label": "published", "title": "published", "to": "Line Drawing Interpretation", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Sridhar", "label": "authored", "title": "authored", "to": "Fast and Robust Hand Tracking", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Sridhar", "label": "collaborated_with", "title": "collaborated_with", "to": "Theobalt", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "Fast and Robust Hand Tracking", "label": "uses", "title": "uses", "to": "Detection-Guided Optimization", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Baak et al.", "label": "authored", "title": "authored", "to": "full body pose reconstruction", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "full body pose reconstruction", "label": "uses", "title": "uses", "to": "depth camera", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Ballan et al.", "label": "authored", "title": "authored", "to": "motion capture of hands", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Bhattacharyya", "label": "authored", "title": "authored", "to": "measure of divergence", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Criminisi and Shotton", "label": "authored", "title": "authored", "to": "Decision forests", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Srinath Srilhar", "label": "affiliated_with", "title": "affiliated_with", "to": "Max Planck Institute for Informatics", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "A. Criminisi", "label": "co_authored", "title": "co_authored", "to": "Decision forests for computer vision", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "A. Criminisi", "label": "co_authored", "title": "co_authored", "to": "Learning to be a depth camera", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "A. Criminisi", "label": "co_authored", "title": "co_authored", "to": "Efficient regression of general-activity human poses", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "A. Criminisi", "label": "authored", "title": "authored", "to": "Geodesic image and video editing", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "J. Shotton", "label": "co_authored", "title": "co_authored", "to": "Decision forests for computer vision", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "J. Shotton", "label": "co_authored", "title": "co_authored", "to": "Learning to be a depth camera", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "J. Shotton", "label": "co_authored", "title": "co_authored", "to": "Efficient regression of general-activity human poses", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "S. R. Fanello", "label": "co_authored", "title": "co_authored", "to": "Learning to be a depth camera", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "H. Hamer", "label": "co_authored", "title": "co_authored", "to": "Tracking a hand manipulating an object", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "C. Keskin", "label": "co_authored", "title": "co_authored", "to": "Real time hand pose estimation", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "C. Keskin", "label": "affiliated_with", "title": "affiliated_with", "to": "ICCV Workshops", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "F. Kirac", "label": "affiliated_with", "title": "affiliated_with", "to": "ICCV Workshops", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Y. Kara", "label": "affiliated_with", "title": "affiliated_with", "to": "ICCV", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "L. Akarun", "label": "affiliated_with", "title": "affiliated_with", "to": "ICCV Workshops", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Srinath Sridhar", "label": "affiliated_with", "title": "affiliated_with", "to": "Max Planck Institute for Informatics", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Antti Oulasvirta", "label": "affiliated_with", "title": "affiliated_with", "to": "Aalto University", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Fumin Shen", "label": "author_of", "title": "author_of", "to": "Supervised Discrete Hashing", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Fumin Shen", "label": "affiliated_with", "title": "affiliated_with", "to": "University of Electronic Science and Technology of China", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Supervised Discrete Hashing", "label": "presented_at", "title": "presented_at", "to": "CVPR", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Heng Tao Shen", "label": "author_of", "title": "author_of", "to": "Supervised Discrete Hashing", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Heng Tao Shen", "label": "affiliated_with", "title": "affiliated_with", "to": "The University of Queensland", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "Supervised Discrete Hanning (SDH)", "label": "is_a", "title": "is_a", "to": "hashing framework", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "Supervised Discrete Hanning (SDH)", "label": "designed_for", "title": "designed_for", "to": "linear classification", "width": 3.8499999999999996}, {"arrows": "to", "color": "#00CC77", "from": "handling discrete constraints", "label": "leads_to", "title": "leads_to", "to": "NP-hard optimization problems", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "objective", "label": "reformulated_by", "title": "reformulated_by", "to": "introducing an auxiliary variable and regularization algorithm", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "cyclic coordinate descent", "label": "solves", "title": "solves", "to": "regularization sub-problem", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "SDH", "label": "achieves", "title": "achieves", "to": "high-quality discrete solutions", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "SDH", "label": "enables", "title": "enables", "to": "handling of massive datasets", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "SDH", "label": "superior_to", "title": "superior_to", "to": "state-of-the-art hashing methods", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "SDH", "label": "extends", "title": "extends", "to": "DH", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "SDH", "label": "includes", "title": "includes", "to": "discriminative term", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Hashing", "label": "enables", "title": "enables", "to": "handling of massive datasets", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Hashing", "label": "applied_to", "title": "applied_to", "to": "image datasets", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "p-stable distributions", "label": "basis_for", "title": "basis_for", "to": "hashing technique", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "bilinear projections", "label": "used_for", "title": "used_for", "to": "learning binary codes", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "bilinear projections", "label": "relate_to", "title": "relate_to", "to": "binary code learning", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "learning binary codes", "label": "uses", "title": "uses", "to": "bilinear projections", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Belkin, M., \u0026 Niyogi, P.", "label": "authored", "title": "authored", "to": "foundational paper", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Datar, N., et al.", "label": "introduced", "title": "introduced", "to": "hashing technique", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Gong, Y., et al.", "label": "explored", "title": "explored", "to": "bilinear projections", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Rowley et al. (2013) paper", "label": "explores", "title": "explores", "to": "learning binary codes", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Weiss et al. (2008) paper", "label": "introduces", "title": "introduces", "to": "spectral hashing", "width": 3.91}, {"arrows": "to", "color": "#CC7700", "from": "spectral hashing", "label": "is_a", "title": "is_a", "to": "contribution to field", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Gong et al. (2013) paper", "label": "presents", "title": "presents", "to": "iterative quantization approach", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "iterative quantization approach", "label": "aims at", "title": "aims at", "to": "learning binary codes", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "iterative quantization approach", "label": "uses", "title": "uses", "to": "procustean approach", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Kulis \u0026 Darrell (2009) paper", "label": "introduces", "title": "introduces", "to": "method for learning to hash", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "method for learning to hash", "label": "uses", "title": "uses", "to": "binary reconstructive embeddings", "width": 3.73}, {"arrows": "to", "color": "#CC7700", "from": "binary reconstructive embeddings", "label": "is_a", "title": "is_a", "to": "method", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Kulis \u0026 Darrell (2009)", "label": "introduces", "title": "introduces", "to": "binary reconstructive embeddings hashing method", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "binary reconstructive embeddings hashing method", "label": "is_a", "title": "is_a", "to": "hashing technique", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Liu, Wang, Kumar, \u0026 Chang (2011)", "label": "explores", "title": "explores", "to": "hashing techniques", "width": 3.88}, {"arrows": "to", "color": "#CC7700", "from": "hashing techniques", "label": "utilizes", "title": "utilizes", "to": "graph structures", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "hashing techniques", "label": "are_used_for", "title": "are_used_for", "to": "efficient similarity search", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Wang, Kumar, \u0026 Chang (2012)", "label": "addresses", "title": "addresses", "to": "hashing", "width": 3.91}, {"arrows": "to", "color": "#CC7700", "from": "hashing", "label": "operates_in", "title": "operates_in", "to": "semi-supervised setting", "width": 3.79}, {"arrows": "to", "color": "#CC7700", "from": "Shen \u0026 Hao (2011)", "label": "relates_to", "title": "relates_to", "to": "learning and classification", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Norouzi \u0026 Blei (2011)", "label": "introduces", "title": "introduces", "to": "minimal loss hashing", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "minimal loss hashing", "label": "creates", "title": "creates", "to": "compact binary codes", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "Blei", "label": "authored", "title": "authored", "to": "Minimal loss hashing", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "A Maximum Entropy Feature Descriptor", "label": "used_for", "title": "used_for", "to": "Age Invariant Face Recognition", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Li, Zhifeng", "label": "authored", "title": "authored", "to": "A Maximum Entropy Feature Descriptor", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Li, Zhifeng", "label": "authored", "title": "authored", "to": "Nonparametric Discriminent Analysis for Face Recognition", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Tao, Dacheng", "label": "authored", "title": "authored", "to": "A Maximum Entropy Feature Descriptor", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Liu, Jianzhang", "label": "authored", "title": "authored", "to": "A Maximum Entropy Feature Descriptor", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Li, Xuelong", "label": "authored", "title": "authored", "to": "A Maximum Entropy Feature Descriptor", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "maximum entropy feature descriptor", "label": "encodes", "title": "encodes", "to": "microstructure", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "maximum entropy feature descriptor", "label": "transforms into", "title": "transforms into", "to": "discrete codes", "width": 3.7600000000000002}, {"arrows": "to", "color": "#CC7700", "from": "maximum entropy feature descriptor", "label": "is", "title": "is", "to": "feature descriptor", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "sampling", "label": "extracts", "title": "extracts", "to": "discriminatory information", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "identity factor analysis", "label": "estimates", "title": "estimates", "to": "probability of same identity", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "experimentation", "label": "uses", "title": "uses", "to": "MORPH dataset", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "experimentation", "label": "uses", "title": "uses", "to": "FGNET dataset", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "experimentation", "label": "uses", "title": "uses", "to": "LFW dataset", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "experimentation", "label": "performed_on", "title": "performed_on", "to": "MORPH", "width": 3.88}, {"arrows": "to", "color": "#CC7700", "from": "MORPH", "label": "is_a", "title": "is_a", "to": "face aging dataset", "width": 3.7600000000000002}, {"arrows": "to", "color": "#CC7700", "from": "FGNET", "label": "is_a", "title": "is_a", "to": "face aging dataset", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Age Invariant Face Recognition (AIFR)", "label": "evaluated_on", "title": "evaluated_on", "to": "MORPH", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Age Invariant Face Recognition (AIFR)", "label": "evaluated_on", "title": "evaluated_on", "to": "LFW dataset", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Age Incompliant Face Recognition (AIFR)", "label": "evaluated_on", "title": "evaluated_on", "to": "FGNET", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "Maximum Entropy Feature Descriptor (MEFD)", "label": "used_in", "title": "used_in", "to": "Age Invariant Face Recognition (AIFR)", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "Identity Factor Analysis (IFA)", "label": "used_in", "title": "used_in", "to": "Age Invariant Face Recognition (AIFR)", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Nonparametric Discriminent Analysis for Face Recognition", "label": "published_in", "title": "published_in", "to": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Wang, Xiaogang", "label": "authored", "title": "authored", "to": "A unified framework for subspace face recognition", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "A unified framework for subspace face recognition", "label": "published_in", "title": "published_in", "to": "IEEE Trans. Pattern Anal. Mach. Intell.", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Li, Unsang", "label": "authored", "title": "authored", "to": "A discriminative model for age invariant face recognition", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Li, Unsang", "label": "published", "title": "published", "to": "IEEE Transactions on Information Forensics and Security", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Li, Unsang", "label": "collaborated_with", "title": "collaborated_with", "to": "Zhifeng Li", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "A discriminative model for age invariant face recognition", "label": "published_in", "title": "published_in", "to": "IEEE Transactions on Information Forensics and Security", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "IEEE Trans. Pattern Anal. Mach. Intell.", "label": "publishes", "title": "publishes", "to": "Semi-supervised hashing", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Park, Unsav", "label": "published", "title": "published", "to": "IEEE Trans. Pattern Anal. Mach. Intell.", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Park, Unsav", "label": "collaborated_with", "title": "collaborated_with", "to": "Yiying Tong", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Belhumeur, Peter N.", "label": "published", "title": "published", "to": "IEEE Trans. Pattern Anal. Mach. Intell.", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Gong, D.", "label": "presented", "title": "presented", "to": "ICCV 2013", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Huang, G.B.", "label": "created", "title": "created", "to": "Labelled faces in the wild", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Labelled faces in the wild", "label": "is_database_for", "title": "is_database_for", "to": "face recognition", "width": 7.0}, {"arrows": "to", "color": "#0077CC", "from": "Labelled faces in the wild", "label": "studies", "title": "studies", "to": "face recognition in unconstrained environments", "width": 7.0}, {"arrows": "to", "color": "#0077CC", "from": "Zhifeng Li", "label": "is_affiliated_with", "title": "is_affiliated_with", "to": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "width": 7.0}, {"arrows": "to", "color": "#0077CC", "from": "Zhifeng Li", "label": "affiliated_with", "title": "affiliated_with", "to": "Shenzhen Key Lab of Computer Vision and Pattern Recogniton", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Technical Report 07-49", "label": "published_by", "title": "published_by", "to": "University of Massachusetts, Amherst", "width": 7.0}, {"arrows": "to", "color": "#0077CC", "from": "Nonparametric Discriminant Analysis for Face Recognition", "label": "addresses", "title": "addresses", "to": "face recognition", "width": 7.0}, {"arrows": "to", "color": "#0077CC", "from": "Random sampling LDA", "label": "used_for", "title": "used_for", "to": "face recognition", "width": 7.0}, {"arrows": "to", "color": "#0077CC", "from": "DiHong Gong", "label": "is_affiliated_with", "title": "is_affiliated_with", "to": "Shenzhen Key Lab of Computer Vision and Pattern Recognition", "width": 7.0}, {"arrows": "to", "color": "#0077CC", "from": "Shenzhen Key Lab of Computer Vision and Pattern Recognition", "label": "part_of", "title": "part_of", "to": "Shenzhen Institutes of Advanced Technology", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Dihong Gong", "label": "affiliated_with", "title": "affiliated_with", "to": "Shenzhen Key Lab of Computer Vision and Pattern Recognition", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Dacheng Tao", "label": "affiliated_with", "title": "affiliated_with", "to": "University of Technology, Sydney", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Jianzhuang Liu", "label": "affiliated_with", "title": "affiliated_with", "to": "Dept. of Information Engineering", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Jianzhuang Liu", "label": "affiliated_with", "title": "affiliated_with", "to": "Huawei Technologies Co. Ltd.", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Xuelong Li", "label": "affiliated_with", "title": "affiliated_with", "to": "Xi\u0027an Institute of Optics and Precision Mechanics", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Xi\u0027an Institute of Optics and Precision Mechanics", "label": "part_of", "title": "part_of", "to": "Chinese Academy of Sciences", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Sayed Hossein Khatoonabadi", "label": "author_of", "title": "author_of", "to": "research paper", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Sayed Hossein Khatoonabadi", "label": "affiliated_with", "title": "affiliated_with", "to": "Simon Fraser University", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Sayed Hosheen Khatoonabadi", "label": "is_author_of", "title": "is_author_of", "to": "How Many Bits Does It Take for a Stimulus to Be Salient?", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "How Many Bits Does It Take for a Stimulus to Be Salient?", "label": "is_paper", "title": "is_paper", "to": "CVPR paper", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Nuno Vasconcelos", "label": "is_author_of", "title": "is_author_of", "to": "How Many Bits Does It Take for a Stimulus to Be Salient?", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Nuno Vasconcelos", "label": "affiliated_with", "title": "affiliated_with", "to": "University of California, San Diego", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Yuifeng Shan", "label": "is_author_of", "title": "is_author_of", "to": "How Many Bits Does It Take for a Stimulus to Be Salient?", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Khatoonabadi_How_Many_Bits_2015_CVPR_paper.pdf", "label": "contains", "title": "contains", "to": "How Many Bits Does It Take for a Stimulus to Be Salient?", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "xuelong_li@opt.ac.cn", "label": "is_email_contact", "title": "is_email_contact", "to": "hanics", "width": 3.25}, {"arrows": "to", "color": "#CC7700", "from": "Stimulus", "label": "has_property", "title": "has_property", "to": "salience", "width": 3.4000000000000004}, {"arrows": "to", "color": "#CC7700", "from": "computational models", "label": "focus_of_research", "title": "focus_of_research", "to": "salience", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "early approaches", "label": "model_salience_as", "title": "model_salience_as", "to": "center-surround filters", "width": 3.88}, {"arrows": "to", "color": "#CC7700", "from": "recent works", "label": "seek", "title": "seek", "to": "general computational principles", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "measure of salience", "label": "based_on", "title": "based_on", "to": "bits required by video compressor", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "measure of salience", "label": "demonstrates", "title": "demonstrates", "to": "predictive power", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "measure of salience", "label": "is_embedded_in", "title": "is_embedded_in", "to": "Markov random field model", "width": 3.91}, {"arrows": "to", "color": "#CC7700", "from": "probabilistic inference", "label": "relates_to", "title": "relates_to", "to": "salience", "width": 3.82}, {"arrows": "to", "color": "#CC7700", "from": "brain", "label": "viewed_as", "title": "viewed_as", "to": "universal compression device", "width": 3.91}, {"arrows": "to", "color": "#CC7700", "from": "universal compression device", "label": "represents", "title": "represents", "to": "brain", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "Fixation Prediction", "label": "achieves", "title": "achieves", "to": "state-of-the-art accuracy", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "view of the brain", "label": "is_a", "title": "is_a", "to": "universal compression device", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Agarwal et al. (2003)", "label": "develops", "title": "develops", "to": "algorithm", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "region-of-interest", "label": "is_in", "title": "is_in", "to": "compressed MPEG domain", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Hou and Zhang (2007)", "label": "proposes", "title": "proposes", "to": "spectral residual approach", "width": 3.82}, {"arrows": "to", "color": "#CC7700", "from": "spectral residual approach", "label": "is_a", "title": "is_a", "to": "salience detection method", "width": 3.73}, {"arrows": "to", "color": "#0077CC", "from": "Helbing and Molnar (1995)", "label": "models", "title": "models", "to": "pedestrian dynamics", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "EE CVPR\u201907", "label": "published_in", "title": "published_in", "to": "IEEE", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Agarwal, G.", "label": "authored", "title": "authored", "to": "algorithm", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Anstis, S. M.", "label": "authored", "title": "authored", "to": "perception of apparent movement", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "Attneave, F.", "label": "authored", "title": "authored", "to": "Informational aspects of visual perception", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Barlow, H.", "label": "authored", "title": "authored", "to": "Cerebral cortex as a model builder", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "Barlow, H.", "label": "authored", "title": "authored", "to": "Redundancy reduction revisited", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Besag, J.", "label": "authored", "title": "authored", "to": "Spatial interaction", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Royal Statistical Society", "label": "publishes", "title": "publishes", "to": "Series B", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Series B", "label": "volume", "title": "volume", "to": "36", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Series B", "label": "issue", "title": "issue", "to": "192\u2013236", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Ivan V. Bajic", "label": "affiliated_with", "title": "affiliated_with", "to": "Simon Fraser University", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Limin Wang", "label": "author_of", "title": "author_of", "to": "Action Recognition with Trajectory-Pooled Deep-Convolutional Descriptors", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Limin Wang", "label": "affiliated_with", "title": "affiliated_with", "to": "Department of Information Engineering", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Limin Wang", "label": "contact_email", "title": "contact_email", "to": "07wanglimin@gmail.com", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Action Recognition with Trajectory-Pooled Deep-Convolutional Descriptors", "label": "published_in", "title": "published_in", "to": "CVPR paper", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Yu Qiao", "label": "author_of", "title": "author_of", "to": "Action Recognition with Trajectory-Pooled Deep-Convolutional Descriptors", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "Yu Qiao", "label": "is", "title": "is", "to": "researcher", "width": 3.25}, {"arrows": "to", "color": "#0077CC", "from": "Yu Qiao", "label": "affiliated_with", "title": "affiliated_with", "to": "Shenzhen Institutes of Advanced Technology", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Yu Qiao", "label": "works_at", "title": "works_at", "to": "CAS", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Xiaoou Tang", "label": "author_of", "title": "author_of", "to": "Action Recognized with Trajectory-Pooled Deep-Convolutional Descriptors", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Xiaoou Tang", "label": "affiliated_with", "title": "affiliated_with", "to": "Department of Information Engineering", "width": 3.7600000000000002}, {"arrows": "to", "color": "#CC7700", "from": "TDD", "label": "is_a", "title": "is_a", "to": "video representation", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "TDD", "label": "combines", "title": "combines", "to": "hand-crafted features", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "TDD", "label": "utilizes", "title": "utilizes", "to": "deep architectures", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "TDD", "label": "employs", "title": "employs", "to": "trajectory-constrained pooling", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "trajectory-constrained pooling", "label": "aggregates", "title": "aggregates", "to": "feature maps", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "feature maps", "label": "are_generated_by", "title": "are_generated_by", "to": "deep architectures", "width": 3.55}, {"arrows": "to", "color": "#00CC77", "from": "normalization methods", "label": "enhance", "title": "enhance", "to": "robustness", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "TDDs", "label": "outperform", "title": "outperform", "to": "hand-crafted features", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "TDDs", "label": "outperform", "title": "outperform", "to": "deep-learned features", "width": 3.79}, {"arrows": "to", "color": "#CC7700", "from": "HMD-B51", "label": "is_a", "title": "is_a", "to": "dataset", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "UCF101", "label": "is_a", "title": "is_a", "to": "dataset", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Human Action Recognition", "label": "achieves", "title": "achieves", "to": "state-of-the-art performance", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "Deep Convolutional Descriptors", "label": "used in", "title": "used in", "to": "Human Action Recognition", "width": 3.7600000000000002}, {"arrows": "to", "color": "#CC7700", "from": "Trajectory-Constrained Pooling", "label": "used in", "title": "used in", "to": "Human Action Recognition", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Multi-view super vector", "label": "used for", "title": "used for", "to": "action recognition", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Convolutional Nets", "label": "delves into", "title": "delves into", "to": "details", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "Aggarwal, J. K., \u0026 Ryoo, M. S.", "label": "authored", "title": "authored", "to": "Human activity analysis review", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Bay, H., Tuytelaars, T., \u0026 Van Gool, L. J.", "label": "authored", "title": "authored", "to": "SURF description", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Karpathy et al.", "label": "used", "title": "used", "to": "convolutional neural networks", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "HMDB", "label": "is", "title": "is", "to": "video database", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "HMDB", "label": "is_database_for", "title": "is_database_for", "to": "human motion recognition", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "T", "label": "authored", "title": "authored", "to": "HMDB", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "T", "label": "published_in", "title": "published_in", "to": "ICCV", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Jing Shao", "label": "author_of", "title": "author_of", "to": "Deeply Learned Attributes", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Jing Shao", "label": "affiliated_with", "title": "affiliated_with", "to": "Department of Electronic Engineering", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Deeply Learned Attributes", "label": "presented_at", "title": "presented_at", "to": "CVPR", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Kai Kang", "label": "author_of", "title": "author_of", "to": "Deeply Learned Attributes", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Kai Kang", "label": "affiliated_with", "title": "affiliated_with", "to": "Department of Electronic Engineering", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Chen Change Loy", "label": "author_of", "title": "author_of", "to": "Deeply Learned Attributes", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "Crowded scene understanding", "label": "is_problem_in", "title": "is_problem_in", "to": "computer vision", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Deep model", "label": "learns", "title": "learns", "to": "appearance features", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Deep model", "label": "learns", "title": "learns", "to": "motion features", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Crowd motion channels", "label": "is_input_of", "title": "is_input_of", "to": "deep model", "width": 3.91}, {"arrows": "to", "color": "#CC7700", "from": "Crowd motion channels", "label": "inspired_by", "title": "inspired_by", "to": "generic properties of crowd systems", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "WWW Crowd dataset", "label": "contains", "title": "contains", "to": "10,000 videos", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "WWW Crowd dataset", "label": "contains", "title": "contains", "to": "8,257 crowded scenes", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "WWW Crowd dataset", "label": "used for", "title": "used for", "to": "cross-scene attribute recognition", "width": 3.0999999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Attribute set", "label": "has_quantity", "title": "has_quantity", "to": "94 attributes", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Deep models", "label": "displays", "title": "displays", "to": "significant performance improvements", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "significant performance improvements", "label": "occurs_in", "title": "occurs_in", "to": "cross-scene attribute recognition", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "deep models", "label": "displays", "title": "displays", "to": "significant performance improvements", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "deep models", "label": "compared_to", "title": "compared_to", "to": "feature-based baselines", "width": 3.8499999999999996}, {"arrows": "to", "color": "#00CC77", "from": "deep models", "label": "improves", "title": "improves", "to": "performance", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "deep models", "label": "uses", "title": "uses", "to": "deeply learned features", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "deeply learned features", "label": "behaves with", "title": "behaves with", "to": "superior performance", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "deeply learned features", "label": "utilized_in", "title": "utilized_in", "to": "multi-task learning", "width": 3.88}, {"arrows": "to", "color": "#CC7700", "from": "attribute recognition", "label": "occurs_in", "title": "occurs_in", "to": "cross-scene", "width": 3.64}, {"arrows": "to", "color": "#CC7700", "from": "attribute recognition", "label": "relies on", "title": "relies on", "to": "crowd-related features", "width": 3.4000000000000004}, {"arrows": "to", "color": "#CC7700", "from": "baselines", "label": "related_to", "title": "related_to", "to": "feature-based", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "Deep learning models", "label": "improves", "title": "improves", "to": "cross-scene attribute recognition", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Deep learning models", "label": "utilizes", "title": "utilizes", "to": "crowd motion channels", "width": 3.25}, {"arrows": "to", "color": "#0077CC", "from": "Ali and Shah (2007)", "label": "addresses", "title": "addresses", "to": "crowd flow segmentation", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Ali and Shah (2008)", "label": "addresses", "title": "addresses", "to": "tracking in high density crowd scenes", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Andrade, Blunsden, and Fisher (2006)", "label": "addresses", "title": "addresses", "to": "event detection in crowd scenes", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Chan and Vasconcelos (2008)", "label": "addresses", "title": "addresses", "to": "video segmentation", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "video segmentation", "label": "uses", "title": "uses", "to": "eye tracking prior", "width": 3.61}, {"arrows": "to", "color": "#CC7700", "from": "video segmentation", "label": "involves", "title": "involves", "to": "dynamic textures", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "oring", "label": "published_in", "title": "published_in", "to": "CVPR 2008", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Vasconcelos, N.", "label": "authored", "title": "authored", "to": "Modeling, clustering, and segmenting video", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Dalal, N.", "label": "authored", "title": "authored", "to": "Histograms of oriented gradients", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Triggers, B.", "label": "authored", "title": "authored", "to": "Histograms of oriented gradients", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Farhad, A.", "label": "authored", "title": "authored", "to": "Describing objects by their attributes", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Hospedales, T.", "label": "authored", "title": "authored", "to": "A markov clustering topic model", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "Kang, K.", "label": "authored", "title": "authored", "to": "Fully convolutional neural networks", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Wang, X.", "label": "authored", "title": "authored", "to": "Fully convolutional neural networks", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "eye tracking data", "label": "identifies", "title": "identifies", "to": "dominant visual tracks", "width": 3.79}, {"arrows": "to", "color": "#00CC77", "from": "eye tracking data", "label": "enhances", "title": "enhances", "to": "Object Extraction", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "dominant visual tracks", "label": "guides", "title": "guides", "to": "object search algorithm", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "object boundaries", "label": "refined_by", "title": "refined_by", "to": "grabcut segmentation", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Intriligator \u0026 Cavanagh", "label": "published", "title": "published", "to": "Cognitive psychology article", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Cognitive psychology article", "label": "studies", "title": "studies", "to": "visual attention", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Itti, Koch, \u0026 Niebur", "label": "developed", "title": "developed", "to": "salience-based visual attention model", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "salience-based visual attention model", "label": "enables", "title": "enables", "to": "rapid scene analysis", "width": 3.82}, {"arrows": "to", "color": "#00CC77", "from": "salience-based visual attention model", "label": "improves", "title": "improves", "to": "scene analysis", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Judd, Ehinger, Durand, \u0026 Torralba", "label": "researched", "title": "researched", "to": "human gaze prediction", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Object Extraction", "label": "benefits_from", "title": "benefits_from", "to": "eye tracking data", "width": 3.7600000000000002}, {"arrows": "to", "color": "#CC7700", "from": "Video Segmentation", "label": "utilizes", "title": "utilizes", "to": "Visual Attention", "width": 3.64}, {"arrows": "to", "color": "#CC7700", "from": "Video Segmentation", "label": "requires", "title": "requires", "to": "Energy Function Optimization", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "Judd et al.", "label": "authored", "title": "authored", "to": "Learning to predict where humans look", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Judd et al.", "label": "published_in", "title": "published_in", "to": "Computer Vision, 2009 IEEE 12th international conference on", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Karthikeyan et al. (2012)", "label": "authored", "title": "authored", "to": "Uni\ufb01ed probabilistic framework", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Borji \u0026 Itti", "label": "authored", "title": "authored", "to": "State-of-the-art in visual attention modeling", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Borji \u0026 Itti", "label": "published_in", "title": "published_in", "to": "Pattern Analysis and Machine Intelligence, IEEE Transactions on", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Borji, Sihite, \u0026 Itti", "label": "authored", "title": "authored", "to": "Salient object detection: A benchmark", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Borji, Sihite, \u0026 Itti", "label": "published_in", "title": "published_in", "to": "Computer Vision\u2013ECCV 2012", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Computer Vision\u2013ECCV 2012", "label": "appears_in", "title": "appears_in", "to": "pages 414\u2013429", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Computer Vision\u2013ECCV 2012", "label": "hosts", "title": "hosts", "to": "Bayesian face revisited", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Karthikeyan et al. (2013)", "label": "authored", "title": "authored", "to": "Learning top-down scene context", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Karthikeyan et al. (2013)", "label": "published_in", "title": "published_in", "to": "ICIP, IEEE", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Karthikeyan, S.", "label": "affiliated_with", "title": "affiliated_with", "to": "University of California Santa Barbara", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Karthikeyan, S.", "label": "email", "title": "email", "to": "{karthikeyan, thuyen, manj}@ece.ucsb.edu", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "{karthikeyan, thuyen, manj}@ece.ucsb.edu", "label": "contact_email", "title": "contact_email", "to": "B.S. Manjunath", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Thuyen Ngo", "label": "email", "title": "email", "to": "{karthikeyan, thuyen, manj}@ece.ucsb.edu", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Thuyen Ngo", "label": "affiliated_with", "title": "affiliated_with", "to": "University of California Santa Barbara", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Miguel Eckstein", "label": "affiliated_with", "title": "affiliated_with", "to": "University of California Santa Barbara", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Miguel Eckstein", "label": "email", "title": "email", "to": "eckstein@psych.ucsb.edu", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "eckstein@psych.ucsb.edu", "label": "contact_email", "title": "contact_email", "to": "Miguel Eckstein", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "B.S. Manjunath", "label": "affiliated_with", "title": "affiliated_with", "to": "University of California Santa Barbara", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Felzenszwalb, P.", "label": "authored", "title": "authored", "to": "A discriminatively trained, multiscale, deformable part model", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Eckstein, M. P.", "label": "authored", "title": "authored", "to": "Visual search: A retrospective", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Shervin Ardeshir", "label": "author_of", "title": "author_of", "to": "Geo-Semantic Segmentation", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Geo-Semantic Segmentation", "label": "published_in", "title": "published_in", "to": "CVPR", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Ko\ufb01 Malcolm Collins-Sibley", "label": "author_of", "title": "author_of", "to": "Geo-Semantic Segmentation", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Mubarak Shah", "label": "author_of", "title": "author_of", "to": "Geo-Semantic Segmentation", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Mubarak Shah", "label": "affiliated_with", "title": "affiliated_with", "to": "University of  Central Florida", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Mubarak Shah", "label": "email", "title": "email", "to": "shah@crcv.ucf.edu", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "geo-semantic segmentation method", "label": "leverages", "title": "leverages", "to": "GIS databases", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "geo-semantic segmentation method", "label": "refines", "title": "refines", "to": "GIS projections alignment", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "GIS data", "label": "includes", "title": "includes", "to": "building locations", "width": 3.46}, {"arrows": "to", "color": "#0077CC", "from": "GIS data", "label": "includes", "title": "includes", "to": "street locations", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "projections", "label": "affected_by", "title": "affected_by", "to": "GPS errors", "width": 3.25}, {"arrows": "to", "color": "#0077CC", "from": "projections", "label": "affected_by", "title": "affected_by", "to": "camera parameter inaccuracies", "width": 3.16}, {"arrows": "to", "color": "#0077CC", "from": "projections", "label": "refined_by", "title": "refined_by", "to": "random walks", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "projections", "label": "refined_by", "title": "refined_by", "to": "global transformations", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "projections", "label": "results in", "title": "results in", "to": "fast and efficient projections", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "alignment", "label": "uses", "title": "uses", "to": "random walks", "width": 3.34}, {"arrows": "to", "color": "#0077CC", "from": "alignment", "label": "uses", "title": "uses", "to": "global transformations", "width": 3.25}, {"arrows": "to", "color": "#CC7700", "from": "random walks", "label": "is_a", "title": "is_a", "to": "optimization technique", "width": 3.25}, {"arrows": "to", "color": "#CC7700", "from": "global transformations", "label": "is_a", "title": "is_a", "to": "image processing technique", "width": 3.25}, {"arrows": "to", "color": "#0077CC", "from": "segmentations", "label": "improves", "title": "improves", "to": "alignment of projections", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "geo-references", "label": "includes", "title": "includes", "to": "addresses", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "geo-references", "label": "includes", "title": "includes", "to": "geo-locations", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Geo-semantic Segmentation", "label": "uses", "title": "uses", "to": "Random Walks", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Geo-semantic Segmentation", "label": "uses", "title": "uses", "to": "Global Transformations", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "Geo-semantic Segmentation", "label": "results_in", "title": "results_in", "to": "Semantically Segmented Images", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "Geo-semantic Segmentation", "label": "relates_to", "title": "relates_to", "to": "GIS Data Integration", "width": 3.25}, {"arrows": "to", "color": "#CC7700", "from": "Geo-semantic Segmentation", "label": "involves", "title": "involves", "to": "Iterative Data Fusion", "width": 3.0999999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Semantically Segmented Images", "label": "has", "title": "has", "to": "Geo-references", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Geo-references", "label": "include", "title": "include", "to": "Addresses", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "P. Zhao et al. [17]", "label": "discusses", "title": "discusses", "to": "Rectilinear parsing", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "O. Teboul et al. [10]", "label": "discusses", "title": "discusses", "to": "Segmentation of building facades", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "G. J. Brostow et al. [2]", "label": "discusses", "title": "discusses", "to": "Segmentation and recognition", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "EE", "label": "published", "title": "published", "to": "2010", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Brostow", "label": "authored", "title": "authored", "to": "Segmentation and recognition using structure from motion point clouds", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "He", "label": "authored", "title": "authored", "to": "Multiscale conditional random fields for image labeling", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "CVPR 2004", "label": "hosts", "title": "hosts", "to": "Multiscale conditional random fields for image labeling", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Liu", "label": "authored", "title": "authored", "to": "Entropy rate superpixel segmentation", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Liu", "label": "authored", "title": "authored", "to": "Saturation-Preerving Specular Reflection Separation", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "M\u00fcller", "label": "authored", "title": "authored", "to": "Procedural modeling of buildings", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Musialski", "label": "authored", "title": "authored", "to": "Interactive coherence-based fac\u00b8ade modeling", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Computer Graphics Forum", "label": "publishes", "title": "publishes", "to": "Interactive coherence-based fac\u00b8ade modeling", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "dings", "label": "published_by", "title": "published_by", "to": "ACM", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Ardeshir", "label": "affiliated_with", "title": "affiliated_with", "to": "University of Central Florida", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Ardeshir", "label": "authored", "title": "authored", "to": "Gis-assisted object detection", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Lerma", "label": "authored", "title": "authored", "to": "Semantic segmentation", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "Hoiem", "label": "authored", "title": "authored", "to": "Automatic photo popup", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "Collins-Sibley", "label": "affiliated_with", "title": "affiliated_with", "to": "Northeaster University", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Shah", "label": "affiliated_with", "title": "affiliated_with", "to": "University of Central Florida", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Huang", "label": "authored", "title": "authored", "to": "Bayesian Inference", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Chao-Tsung Huang", "label": "authored", "title": "authored", "to": "Bayesian Inference for Neighborhood Filters", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Chao-Tsung Huang", "label": "email", "title": "email", "to": "collins-sibley.k@husky.neu.edu", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Bayesian Inference for Neighborhood Filters", "label": "application_in", "title": "application_in", "to": "Denoising", "width": 3.88}, {"arrows": "to", "color": "#CC7700", "from": "Bayesian Inference for Neighborhood Filters", "label": "field_of_study", "title": "field_of_study", "to": "Image Processing", "width": 3.25}, {"arrows": "to", "color": "#CC7700", "from": "Denoising", "label": "is_application_of", "title": "is_application_of", "to": "Image Processing", "width": 3.0999999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Huang_Bayesian_Infrenence_for_2015_CVPR_paper.pdf", "label": "publication_date", "title": "publication_date", "to": "2015", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Huang_Bayesian_Infrenence_for_2015_CVPR_paper.pdf", "label": "conference", "title": "conference", "to": "CVPR", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Range-weighted neighborhood filters", "label": "useful for", "title": "useful for", "to": "edge-preserving denoising", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Range-weighted neighborhood filters", "label": "have", "title": "have", "to": "limited theoretical understanding", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "unified empirical Bayesian framework", "label": "directly infers", "title": "directly infers", "to": "filters", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "unified empirical Bayesian framework", "label": "estimates", "title": "estimates", "to": "range variance", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "range variance", "label": "requires", "title": "requires", "to": "accurate estimation", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "neighborhood noise model", "label": "reasons", "title": "reasons", "to": "Yaroslavsky, bilateral, and modified non-local means filters", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "EM+ algorithm", "label": "estimates", "title": "estimates", "to": "range variance", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "EM+ algorithm", "label": "uses", "title": "uses", "to": "model fitting", "width": 3.64}, {"arrows": "to", "color": "#00CC77", "from": "noisy images", "label": "affects", "title": "affects", "to": "image quality", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "color-image denoising", "label": "demonstrates", "title": "demonstrates", "to": "model\u0027s effectiveness", "width": 3.67}, {"arrows": "to", "color": "#00CC77", "from": "recursive fitting", "label": "improves", "title": "improves", "to": "image quality", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Paris, S.", "label": "authored", "title": "authored", "to": "Bilateral filtering", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "Bilateral filtering", "label": "is_a", "title": "is_a", "to": "image processing technique", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Bilateral filtering", "label": "is_introduced_in", "title": "is_introduced_in", "to": "International Conference on Computer Vision", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Buades, A.", "label": "authored", "title": "authored", "to": "image denoising algorithms review", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "image denoising algorithms review", "label": "published_in", "title": "published_in", "to": "SIAM Journal on Multi-scale Modeling and Simulation", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Chatterjee, P.", "label": "authored", "title": "authored", "to": "Patch-based near-optimal image denoising", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Patch-based near-optimal image denoising", "label": "published_in", "title": "published_in", "to": "IEEE Transactions on Image Processing", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Peng, H.", "label": "authored", "title": "authored", "to": "Bilateral kernel parameter optimization", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Peng, H.", "label": "authored", "title": "authored", "to": "Multispectral image denoising", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Bilateral kernel parameter optimization", "label": "presented_at", "title": "presented_at", "to": "International Conference on Image Processing", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "Multispectral image denoising", "label": "uses", "title": "uses", "to": "vector bilateral filter", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Multispectral image denoising", "label": "is_addressed_in", "title": "is_addressed_in", "to": "IEEE Transactions on Image Processing", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "vector bilateral filter", "label": "is_a", "title": "is_a", "to": "image filtering technique", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Vector bilateral filter", "label": "optimizes", "title": "optimizes", "to": "Multispectral image denoising", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Image denoising", "label": "uses", "title": "uses", "to": "scale mixtures of gausians", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Bilateral filter", "label": "is_improved_in", "title": "is_improved_in", "to": "IEEE Transactions on Image Processing", "width": 3.82}, {"arrows": "to", "color": "#CC7700", "from": "Local Estimation", "label": "is_part_of", "title": "is_part_of", "to": "Deep Networks for Saliency Detection", "width": 3.91}, {"arrows": "to", "color": "#CC7700", "from": "Local Estimation", "label": "is_technique_in", "title": "is_technique_in", "to": "Deep Neural Networks", "width": 3.25}, {"arrows": "to", "color": "#CC7700", "from": "Global Search", "label": "is_part_of", "title": "is_part_of", "to": "Deep Networks for Saliency Detection", "width": 3.91}, {"arrows": "to", "color": "#CC7700", "from": "Global Search", "label": "is_technique_in", "title": "is_technique_in", "to": "Deep Neural Networks", "width": 3.25}, {"arrows": "to", "color": "#0077CC", "from": "Chao-Tsun Huang", "label": "is_affiliated_with", "title": "is_affiliated_with", "to": "National Tsing Hua University", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Lijun Wang", "label": "is_author_of", "title": "is_author_of", "to": "Deep Networks for Saliency Detection", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Lijun Wang", "label": "is_author_of", "title": "is_author_of", "to": "Deep Networks for Salience Detection", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Deep Networks for Salience Detection", "label": "presented_at", "title": "presented_at", "to": "CVPR", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Deep Networks for Salience Detection", "label": "is_paper", "title": "is_paper", "to": "Wang_Deep_Networks_for_2015_CVPR_paper", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Wang_Deep_Networks_for_2015_CVPR_paper", "label": "is_file", "title": "is_file", "to": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Wang_Deep_Networks_for_2015_CVPR_paper.pdf", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "Salience Detection", "label": "uses", "title": "uses", "to": "Deep Networks", "width": 3.64}, {"arrows": "to", "color": "#CC7700", "from": "Salience Detection", "label": "related_to", "title": "related_to", "to": "Sparse Coding", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "Salience Detection", "label": "addresses", "title": "addresses", "to": "Visual Attention", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "local estimation", "label": "uses", "title": "uses", "to": "DNN-L", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "local estimation", "label": "refines", "title": "refines", "to": "object concepts", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "global search", "label": "uses", "title": "uses", "to": "geometric information", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "global search", "label": "uses", "title": "uses", "to": "global contrast", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "DNN-L", "label": "learns", "title": "learns", "to": "local patch features", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "DNN-L", "label": "determines", "title": "determines", "to": "salience value", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "DNN-G", "label": "trained_to_predict", "title": "trained_to_predict", "to": "salient score", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "salient object regions", "label": "generated_by", "title": "generated_by", "to": "weighted sum", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "saliency map", "label": "generated_by", "title": "generated_by", "to": "weighted sum", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "object region", "label": "predicted_by", "title": "predicted_by", "to": "saliency score", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "saliency score", "label": "based_on", "title": "based_on", "to": "global features", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Algorithm", "label": "performs_favorably_against", "title": "performs_favorably_against", "to": "state-of-the-art methods", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Algorithm", "label": "uses", "title": "uses", "to": "Gaussian Mixture Model", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "Salient Region Detection", "label": "is_concept_in", "title": "is_concept_in", "to": "field", "width": 3.64}, {"arrows": "to", "color": "#CC7700", "from": "Salient Region Detection", "label": "precursor_to", "title": "precursor_to", "to": "Video Object Segmentation", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "Salient Region Detection", "label": "described_in", "title": "described_in", "to": "CVPR, 2012", "width": 3.25}, {"arrows": "to", "color": "#0077CC", "from": "R. Achanta et al.", "label": "provides", "title": "provides", "to": "foundational work", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "J. Carreira and C. Sminchisescu", "label": "presents", "title": "presents", "to": "method", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "Constrained parametric min-cuts", "label": "is_work_by", "title": "is_work_by", "to": "J. Carreira and C. Sminchisescu", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Constrained parametric min-cuts", "label": "is_method_of", "title": "is_method_of", "to": "automatic object segmentation", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Constrained parametric min-cuts", "label": "uses", "title": "uses", "to": "min-cut approach", "width": 3.61}, {"arrows": "to", "color": "#0077CC", "from": "Constrained parametric min-cuts", "label": "addresses", "title": "addresses", "to": "object segmentation", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "Object Candidate Regions", "label": "is_part_of", "title": "is_part_of", "to": "Deep Neural Networks", "width": 2.95}, {"arrows": "to", "color": "#0077CC", "from": "tric min-cuts", "label": "used_for", "title": "used_for", "to": "object segmentation", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "visual salience", "label": "established_by", "title": "established_by", "to": "Itti, Koch, and Niebur (1998)", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Itti, Koch, and Niebur (1998)", "label": "presented", "title": "presented", "to": "computational model", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Itti, Koch, and Niebur (1998)", "label": "published_in", "title": "published_in", "to": "PAMI", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "saliency detection", "label": "uses", "title": "uses", "to": "absorbing Markov chain", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "absorbing Markov chain", "label": "is_approach_for", "title": "is_approach_for", "to": "saliency detection", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Markov Chain approach", "label": "is_used_for", "title": "is_used_for", "to": "saliency detection", "width": 3.91}, {"arrows": "to", "color": "#CC7700", "from": "Hierarchical approaches", "label": "are_common_in", "title": "are_common_in", "to": "computer vision", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Selective search", "label": "used_for", "title": "used_for", "to": "generating object proposals", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Selective search", "label": "used_as", "title": "used_as", "to": "preprocessing step", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "salient object detection pipelines", "label": "uses", "title": "uses", "to": "Selective search", "width": 3.79}, {"arrows": "to", "color": "#CC7700", "from": "simultaneous detection and segmentation", "label": "is_a", "title": "is_a", "to": "related task", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "simultaneous detection and segmentation", "label": "is_related_to", "title": "is_related_to", "to": "salient region detection", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "Bayesian models", "label": "are_used_in", "title": "are_used_in", "to": "computer vision", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "ICC paper", "label": "focuses_on", "title": "focuses_on", "to": "efficient computation", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "ECCV paper", "label": "addresses", "title": "addresses", "to": "simultaneous detection and segmentation", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "ICIP paper", "label": "uses", "title": "uses", "to": "Bayesian model", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "Abhishek Kar", "label": "affiliated_with", "title": "affiliated_with", "to": "University of California, Berkeley", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Abhishek Kar", "label": "email", "title": "email", "to": "akar@eecs.berkeley.edu", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Abhishek Kar", "label": "contributed_to", "title": "contributed_to", "to": "Category-Speci\ufb01c Object Reconstruction from a Single Image", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Jo\u02dcao Carreira", "label": "affiliated_with", "title": "affiliated_with", "to": "University of California, Berkeley", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Jo\u02dcao Carreira", "label": "email", "title": "email", "to": "carreira@eecs.berkeley.edu", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Jo\u02dcao Carreira", "label": "contributed_to", "title": "contributed_to", "to": "Category-Speci\ufb01c Object Reconstruction from a Single Image", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Shubham Tulisiani", "label": "contributed_to", "title": "contributed_to", "to": "Category-Speci\ufb01c Object Reconstruction from a Single Image", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Thorsten Beier", "label": "contributed_to", "title": "contributed_to", "to": "Fusion Moves for Correlation Clustering", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Thorsten Beier", "label": "affiliates_with", "title": "affiliates_with", "to": "University of Heidelberg", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Thorsten Beier", "label": "has_email", "title": "has_email", "to": "thorsten.beier@iwr.uni-heidelberg.de", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Thorsten Beier", "label": "associated_with", "title": "associated_with", "to": "Beier_Fusion_Moves_for_2015_CVPR_supplemental", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Thorsten Beier", "label": "works_at", "title": "works_at", "to": "University of Heidelberg (Iwr)", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Fusion Moves for Correlation Clustering", "label": "is_paper_of", "title": "is_paper_of", "to": "CVPR", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Fusion Moves for Correlation Clustering", "label": "is_supplement_for", "title": "is_supplement_for", "to": "Beier_Fusion_Moves_for_2015_CVPR_supplemental.pdf", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Fred A. Hamprecht", "label": "contributed_to", "title": "contributed_to", "to": "Fusion Moves for Correlation Clustering", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Fred A. Hamprecht", "label": "affiliates_with", "title": "affiliates_with", "to": "University of Heidelberg", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Fred A. Hamprecht", "label": "has_email", "title": "has_email", "to": "fred.hamprecht@iwr.uni-heidelberg.de", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Fred A. Hamprecht", "label": "works_at", "title": "works_at", "to": "University of Heidelberg (Iwr)", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "J\u00f6rg H. Kappes", "label": "affiliates_with", "title": "affiliates_with", "to": "University of Heidelberg", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "J\u00f6rg H. Kappes", "label": "affiliates_with", "title": "affiliates_with", "to": "Math", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "J\u00f6rg H. Kappes", "label": "has_email", "title": "has_email", "to": "kappes@math.uni-heidelberg.de", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "J\u00f6rg H. Kappes", "label": "contributed_to", "title": "contributed_to", "to": "Fusion Moves for Correlation Clustering", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "Hossein Rahmani", "label": "affiliated_with", "title": "affiliated_with", "to": "The University of Western Australia", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Hossein Rahmani", "label": "contributed_to", "title": "contributed_to", "to": "Learning a Non-linear Knowledge Transfer Model for Cross-View Action Recognition", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Hossein Rahmani", "label": "is_author", "title": "is_author", "to": "Rahmani_Learning_a_Non-Linear_2015_CVPR_paper.pdf", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Hossein Rahman\u0131", "label": "email", "title": "email", "to": "hossein@csse.uwa.edu.au", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Ajmal Mian", "label": "affiliated_with", "title": "affiliated_with", "to": "The University of Western Canada", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Ajmal Mian", "label": "email", "title": "email", "to": "ajmal.mian@uwa.edu.au", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Ajmal Mian", "label": "contributed_to", "title": "contributed_to", "to": "Learning a Non-linear Knowledge Transfer Model for Cross-View Action Recognition", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Ajmal Mian", "label": "is_author", "title": "is_author", "to": "Rahmani_Learning_a_Non-Linear_2015_CVPR_paper.pdf", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Learning a Non-linear Knowledge Transfer Model for Cross-View Action Recognition", "label": "presented_at", "title": "presented_at", "to": "CVPR", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "Sparse Kernel Multi-task Learning (SKMTL) models", "label": "relates_to", "title": "relates_to", "to": "Convex Multi-task Cluster Learning", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Sparse Kernel Multi-task Learning (SKMTL) models", "label": "uses", "title": "uses", "to": "Laplacian Eigenmaps", "width": 3.25}, {"arrows": "to", "color": "#0077CC", "from": "SKMTL problem", "label": "is", "title": "is", "to": "jointly convex", "width": 3.91}, {"arrows": "to", "color": "#CC7700", "from": "clustered structures", "label": "of", "title": "of", "to": "tasks", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "tasks", "label": "serve as", "title": "serve as", "to": "benchmarks", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "tasks", "label": "evaluating", "title": "evaluating", "to": "progress", "width": 3.82}, {"arrows": "to", "color": "#CC7700", "from": "Robotics (Sarcos) dataset", "label": "is_example_of", "title": "is_example_of", "to": "dataset", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "sparse structure", "label": "has_implications_in", "title": "has_implications_in", "to": "settings beyond computer vision", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Laplacian Eigenmaps", "label": "related_to", "title": "related_to", "to": "Sparse Kernel Multi-task Learning", "width": 3.0999999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Sparse Kernel Multi-task Learning", "label": "related_to", "title": "related_to", "to": "computer vision", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Joint Convexity", "label": "related_to", "title": "related_to", "to": "Sparse Kernel Multi-task Learning", "width": 3.25}, {"arrows": "to", "color": "#0077CC", "from": "Cluster Multi-task Learning", "label": "related_to", "title": "related_to", "to": "Sparse Kernel Multi-task Learning", "width": 3.25}, {"arrows": "to", "color": "#0077CC", "from": "Robotics", "label": "uses", "title": "uses", "to": "Sarcos dataset", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "Robust Multiple Homography Estimation", "label": "is_problem", "title": "is_problem", "to": "ill-solved problem", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Zygmunt L. Szpak", "label": "is_author_of", "title": "is_author_of", "to": "Robust Multiple Homography Estimation", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Wojciech Chojnacki", "label": "is_author_of", "title": "is_author_of", "to": "Robust Multiple Homography Estimation", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Szpak_Robust_Multiple_Homography_2015_CVPR_paper.pdf", "label": "is_publication", "title": "is_publication", "to": "Robust Multiple Homography Estimation", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "multiple homographies estimation", "label": "is", "title": "is", "to": "ill-solved problem", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "rigidity", "label": "implies", "title": "implies", "to": "consistency constraints", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "homographies", "label": "must_satisfy", "title": "must_satisfy", "to": "new constraints", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "homographies", "label": "are_related_to", "title": "are_related_to", "to": "views", "width": 3.25}, {"arrows": "to", "color": "#0077CC", "from": "epipolar geometries", "label": "are", "title": "are", "to": "inconsistent", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "robust multi-structure estimation methods", "label": "requires", "title": "requires", "to": "enforcing constraints on homography matrices", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "multi-structure estimation methods", "label": "is_capable_of", "title": "is_capable_of", "to": "enforcing constraints", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "multi-structure estimation methods", "label": "is", "title": "is", "to": "robust", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "homography matrices", "label": "has_constraint", "title": "has_constraint", "to": "constraints", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "new generation", "label": "includes", "title": "includes", "to": "robust multi-structure estimation methods", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "critiques", "label": "targets", "title": "targets", "to": "existing approaches", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "Robust Multi-Structure Estimation", "label": "enforces", "title": "enforces", "to": "constraints on homography matrices", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Robust Multi-Structure Estimation", "label": "critiques", "title": "critiques", "to": "existing approaches", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "Homography Matrices", "label": "related to", "title": "related to", "to": "Projective Geometry", "width": 3.64}, {"arrows": "to", "color": "#CC7700", "from": "Homography Matrices", "label": "related to", "title": "related to", "to": "Epiopolar Geometry", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Baker, S., Datta, A., and Kanade, T.", "label": "authored", "title": "authored", "to": "Parameterizing homographies", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Parameterizing homographies", "label": "is a", "title": "is a", "to": "tech. rep. CMU-RI-TR-06-11", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Bernstein, D. S.", "label": "authored", "title": "authored", "to": "Matrix Mathematics", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Chen, P., and Suter, D.", "label": "authored", "title": "authored", "to": "Rank constraints for homographies", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Chen, P.", "label": "authors", "title": "authors", "to": "Rank constraints", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Rank constraints", "label": "concerns", "title": "concerns", "to": "homographies", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Suter, D.", "label": "authors", "title": "authors", "to": "Rank constraints", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Chojnacki, W.", "label": "authors", "title": "authors", "to": "Multiple homography estimation", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Chojnacki, W.", "label": "authors", "title": "authors", "to": "Dimensionality result", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Multiple homography estimation", "label": "uses", "title": "uses", "to": "consistency constraints", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Szpak, Z.", "label": "authors", "title": "authors", "to": "Multiple homography estimation", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "van den Hengel, A.", "label": "authors", "title": "authors", "to": "Multiple homography estimation", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "van den Hengel, A.", "label": "authors", "title": "authors", "to": "Dimensionality result", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Dimensionality result", "label": "concerns", "title": "concerns", "to": "homography matrices", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Fouhey, D. F.", "label": "authors", "title": "authors", "to": "Multiple plane detection", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Multiple plane detection", "label": "uses", "title": "uses", "to": "J-linkage", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Scharstein, D.", "label": "authors", "title": "authors", "to": "Multiple plane detection", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Briggs, A. J.", "label": "authors", "title": "authors", "to": "Multiple plane detection", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Goldberger, J.", "label": "authors", "title": "authors", "to": "Camera projection matrices", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Goldberger", "label": "published", "title": "published", "to": "Reconstructing camera projection matrices", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Irving", "label": "authored", "title": "authored", "to": "Integers, Polynomials, and Rings", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Szpak", "label": "affiliated_with", "title": "affiliated_with", "to": "School of Computer Science", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Chojnicki", "label": "affiliated_with", "title": "affiliated_with", "to": "School of Computer Science", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "van den Hengel", "label": "affiliated_with", "title": "affiliated_with", "to": "School of Computer Science", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Saturation-Preerving Specular Reflection Separation", "label": "published_in", "title": "published_in", "to": "CVPR", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Yuan", "label": "authored", "title": "authored", "to": "Saturation-Preerving Specular Reflection Separation", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Zheng", "label": "authored", "title": "authored", "to": "Saturation-Preerving Specular Reflection Separation", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Wu", "label": "authored", "title": "authored", "to": "Saturation-Preerving Specular Reflection Separation", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Wu", "label": "affiliated with", "title": "affiliated with", "to": "University of Delaware", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Wu", "label": "authored", "title": "authored", "to": "Robust Regression on Image Manifolds", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Wu", "label": "email", "title": "email", "to": "nianyi@eecis.udel.edu", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Yuanliu Liu", "label": "authored", "title": "authored", "to": "Saturation-Preerving Specular Reflection Paper", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Yuanliu Liu", "label": "affiliated with", "title": "affiliated with", "to": "Institute of Artificial AI and Robotics", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Yuanliu Liu", "label": "affiliated_with", "title": "affiliated_with", "to": "Institute of Artificial Intelligence and Robotics", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Saturation-Preerving Specular Reflection Paper", "label": "presented_at", "title": "presented_at", "to": "CVPR", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Saturation-Preerving Specular Reflection Paper", "label": "publication_year", "title": "publication_year", "to": "2015", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Zejian Yuan", "label": "authored", "title": "authored", "to": "Saturation-Preerving Specular Reflection Paper", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Zejian Yuan", "label": "associated with", "title": "associated with", "to": "Yuanliu Liu", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Zejian Yuan", "label": "affiliated_with", "title": "affiliated_with", "to": "Institute of Artificial AI and Robotics", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Nanning Zheng", "label": "authored", "title": "authored", "to": "Saturation-Preerving Specular Reflection Paper", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Nanning Zheng", "label": "affiliated_with", "title": "affiliated_with", "to": "Institute of Artificial AI and Robotics", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Yang Wu", "label": "authored", "title": "authored", "to": "Saturation-Preerving Specular Reflection Paper", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Yang Wu", "label": "affiliated_with", "title": "affiliated_with", "to": "Nara Institute of Science and Technology", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "Reflection", "label": "is_topic_of", "title": "is_topic_of", "to": "Saturation-Preerving Specular Reflection Paper", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "Specular Reflection", "label": "is_topic_of", "title": "is_topic_of", "to": "Saturation-Preserving Specular Reflection Paper", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Specular Reflection", "label": "is_separated_by", "title": "is_separated_by", "to": "Linear Programming", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "Specular Reflection", "label": "is_related_to", "title": "is_related_to", "to": "Diffuse Reflection", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Liu_Saturation-Preerving Specular Reflection Paper", "label": "file_path", "title": "file_path", "to": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Liu_Saturation-Preerving_Specular_Reflection_2015_CVPR_paper.pdf", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Specular reflection", "label": "decreases", "title": "decreases", "to": "saturation of surface colors", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "decreased saturation", "label": "leads to", "title": "leads to", "to": "confusion with other colors", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Traditional methods", "label": "suffer from", "title": "suffer from", "to": "hue-saturation ambiguity", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Specular-free images", "label": "are", "title": "are", "to": "oversaturated", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "This paper", "label": "proposes", "title": "proposes", "to": "two-step approach", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "two-step approach", "label": "produces", "title": "produces", "to": "over-saturated specular-free image", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "over-saturated specular-free image", "label": "is produced through", "title": "is produced through", "to": "global chromaticity propagation", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Saturation", "label": "is recovered based on", "title": "is recovered based on", "to": "piecewise constancy of diffuse chromaticity", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "Saturation", "label": "is recovered based on", "title": "is recovered based on", "to": "spatial sparsity/smoothness of specular reflection", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "achieved by increasing", "label": "uses", "title": "uses", "to": "linear programming", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "linear programming", "label": "is_used_to", "title": "is_used_to", "to": "increase achromatic component", "width": 3.67}, {"arrows": "to", "color": "#CC7700", "from": "diffuse chromaticity", "label": "has_property", "title": "has_property", "to": "chromaticity", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "specular reflection", "label": "has_property", "title": "has_property", "to": "spatial sparsity", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "specular reflection", "label": "is_separated_from", "title": "is_separated_from", "to": "surface colors", "width": 3.61}, {"arrows": "to", "color": "#CC7700", "from": "spatial sparsity", "label": "is_property_of", "title": "is_property_of", "to": "specular reflection", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "achromatic component", "label": "used_with", "title": "used_with", "to": "linear programming", "width": 3.64}, {"arrows": "to", "color": "#CC7700", "from": "surface colors", "label": "has_property", "title": "has_property", "to": "saturation", "width": 3.73}, {"arrows": "to", "color": "#CC7700", "from": "Diffuse Chromaticity", "label": "is_component_of", "title": "is_component_of", "to": "Linear Programming", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Linear Programming", "label": "preserves", "title": "preserves", "to": "surface color saturation", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Linear Programming", "label": "used_in", "title": "used_in", "to": "reflection component separation", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Shafer, S.", "label": "authored", "title": "authored", "to": "foundational work", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Artusi, A. et al.", "label": "authored", "title": "authored", "to": "survey of specular removal methods", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Hue-Saturation Ambiguity", "label": "is_addressed_by", "title": "is_addressed_by", "to": "Linear Programming", "width": 3.25}, {"arrows": "to", "color": "#CC7700", "from": "Chromaticity Propagation", "label": "is_related_to", "title": "is_related_to", "to": "Diffuse Chromaticity", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "Diffuse and specular interface reflections", "label": "studied_in", "title": "studied_in", "to": "International Journal of Computer Vision", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Gonzalez \u0026 Woods", "label": "authored", "title": "authored", "to": "Digital Image Processing", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Land \u0026 McCann", "label": "introduced", "title": "introduced", "to": "retinex theory", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "retinex theory", "label": "relevant_to", "title": "relevant_to", "to": "color constancy", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Kim et al.", "label": "utilized", "title": "utilized", "to": "dark channel prior", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "dark channel prior", "label": "technique_for", "title": "technique_for", "to": "specular reflection separation", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "Mallick et al.", "label": "explored", "title": "explored", "to": "specular surfaces", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Mallick et al.", "label": "used", "title": "used", "to": "color information", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "specular surfaces", "label": "reconstructed using", "title": "reconstructed using", "to": "color information", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "S. P.", "label": "authored", "title": "authored", "to": "Beyond lambert", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Beyond lambert", "label": "explores", "title": "explores", "to": "reconstructing specular surfaces", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Zickler", "label": "authored", "title": "authored", "to": "Beyond lambert", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "T.", "label": "authored", "title": "authored", "to": "Beyond lambert", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "P. N. Belhumeur", "label": "authored", "title": "authored", "to": "Beyond lambert", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "D. J. Kriegman", "label": "authored", "title": "authored", "to": "Beyond lambert", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "P., Zickler, T., Belhumeur, P. N., \u0026 Kriegman, D. J.", "label": "explores", "title": "explores", "to": "specular surfaces", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Lin, S., \u0026 Shum, H.-Y.", "label": "addresses", "title": "addresses", "to": "separation of diffuse and specular reflection", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Tan, R. T., Nishino, K., \u0026 Ikeuchi, K.", "label": "addresses", "title": "addresses", "to": "color constancy", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Mallick, S. P., Zickler, T., Kriegman, D. J., \u0026 Belhumeur, P. N.", "label": "presents", "title": "presents", "to": "PDE approach", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "PDE approach", "label": "used for", "title": "used for", "to": "specular removal", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "diffuse reflection", "label": "related to", "title": "related to", "to": "specular reflection", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Wuyuan Xie", "label": "author_of", "title": "author_of", "to": "Photometric Stereo with Near Point Lighting", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Photometric Stereo with Near Point Lighting", "label": "presents", "title": "presents", "to": "PDE approach", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Photometric Stereo with Near Point Lighting", "label": "published_in", "title": "published_in", "to": "CVPR paper", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Chengkai Dai", "label": "author_of", "title": "author_of", "to": "Photometric Stereo with Near Point Lighting", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Charlie C. L. Wang", "label": "author_of", "title": "author_of", "to": "Photometric Stereo with Near Point Lighting", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Xie_Photometric_Stereo_With_2015_CVPR_paper.pdf", "label": "is_a", "title": "is_a", "to": "CVPR paper", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "photometric stereo", "label": "occurs under", "title": "occurs under", "to": "near point lighting", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "near point lighting", "label": "introduces", "title": "introduces", "to": "nonlinear relationship", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "nonlinear relationship", "label": "links", "title": "links", "to": "local surface normals", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "nonlinear relationship", "label": "links", "title": "links", "to": "light source positions", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "mesh deformation approach", "label": "determines", "title": "determines", "to": "facet position", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "mesh deformation approach", "label": "determines", "title": "determines", "to": "facet orientation", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "mesh deformation approach", "label": "decomposes into", "title": "decomposes into", "to": "local projection", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "mesh deformation approach", "label": "decomposes into", "title": "decomposes into", "to": "global blending", "width": 3.79}, {"arrows": "to", "color": "#CC7700", "from": "Photometric Stereo", "label": "requires", "title": "requires", "to": "Nonlinear Optimization", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "S. Barsky", "label": "authored", "title": "authored", "to": "4-source photometric stereo technique", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "4-source photometric stereo technique", "label": "addresses", "title": "addresses", "to": "highlights and shadows", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "D. Nehab", "label": "authored", "title": "authored", "to": "Efficiently combining positions and normals", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Efficiently combining positions and normals", "label": "aims_for", "title": "aims_for", "to": "precise 3d geometry", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "A. Hertzmann", "label": "authored", "title": "authored", "to": "Example-based photometric stereo", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Example-based photometric stereo", "label": "reconstructs", "title": "reconstructs", "to": "shape", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Shape and spatially-ranging brdfs", "label": "derived_from", "title": "derived_from", "to": "photometric stereo", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "Mesh Deformation", "label": "related_to", "title": "related_to", "to": "Surface Reconstruction", "width": 3.25}, {"arrows": "to", "color": "#CC7700", "from": "Near Point Lighting", "label": "influences", "title": "influences", "to": "Photometric Stereo", "width": 3.0999999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Photometric stereo", "label": "is_studied_in", "title": "is_studied_in", "to": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Photometric stereo", "label": "is_studied_in", "title": "is_studied_in", "to": "Computer Vision Workshops (ICCV Workshops)", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Photometric stereo", "label": "uses", "title": "uses", "to": "multiple images", "width": 3.46}, {"arrows": "to", "color": "#0077CC", "from": "Photometric stereo", "label": "uses", "title": "uses", "to": "point light sources", "width": 3.67}, {"arrows": "to", "color": "#0077CC", "from": "Shape", "label": "is_studied_in", "title": "is_studied_in", "to": "Proceedings of the Fifth Eurographics Symposium on Geometry Processing", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Surface orientation", "label": "is_determined_by", "title": "is_determined_by", "to": "photometric method", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "Surface modeling", "label": "is_approached_by", "title": "is_approached_by", "to": "as-rigid-as-possible surface modeling", "width": 3.61}, {"arrows": "to", "color": "#CC7700", "from": "Discrete geometry", "label": "is_shaped_by", "title": "is_shaped_by", "to": "projections", "width": 3.58}, {"arrows": "to", "color": "#0077CC", "from": "BRDF", "label": "is_studied_in", "title": "is_studied_in", "to": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "width": 3.73}, {"arrows": "to", "color": "#0077CC", "from": "urographics Symposium on Geometry Processing", "label": "held in", "title": "held in", "to": "2007", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "S. Bouaziz", "label": "affiliated with", "title": "affiliated with", "to": "urographics Symposium on Geometry Processing", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "deep hashing (DH) approach", "label": "used_for", "title": "used_for", "to": "compact binary codes", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "existing binary codes learning methods", "label": "uses", "title": "uses", "to": "single linear projection", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "deep neural network", "label": "performs", "title": "performs", "to": "hierarchical non-linear transformations", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "deep neural network", "label": "exploits", "title": "exploits", "to": "nonlinear relationship of samples", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "loss minimization", "label": "relates_to", "title": "relates_to", "to": "real-valued feature descriptor", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "different bits", "label": "are", "title": "are", "to": "independent", "width": 3.46}, {"arrows": "to", "color": "#0077CC", "from": "different bits", "label": "are independent", "title": "are independent", "to": "each other", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "nary vector", "label": "is_minimized", "title": "is_minimized", "to": "binary codes", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "discriminative term", "label": "maximizes", "title": "maximizes", "to": "inter-class variations", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "discriminative term", "label": "minimizes", "title": "minimizes", "to": "intra-class variations", "width": 3.88}, {"arrows": "to", "color": "#CC7700", "from": "learned binary codes", "label": "have", "title": "have", "to": "discriminative power", "width": 3.82}, {"arrows": "to", "color": "#CC7700", "from": "Deep Hashing", "label": "is_a", "title": "is_a", "to": "Binary Codes Learning", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "Deep Hashing", "label": "supports", "title": "supports", "to": "Large-Scale Visual Search", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Binary Codes Learning", "label": "aims_to", "title": "aims_to", "to": "maximize inter-class variations", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Binary Codes Learning", "label": "aims_to", "title": "aims_to", "to": "minimize intra-class variations", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "Hashing Functions", "label": "used_in", "title": "used_in", "to": "Deep HHashing", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Andoni \u0026 Indyk (2006)", "label": "proposes", "title": "proposes", "to": "Near-optimal Hashing Algorithms", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "Near-optimal Hashing Algorithms", "label": "enables", "title": "enables", "to": "Approximate Nearest Neighbor Search", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Gong et al. (2012)", "label": "proposes", "title": "proposes", "to": "Angular Quantization-based Binary Codes", "width": 3.82}, {"arrows": "to", "color": "#CC7700", "from": "Angular Quantization-based Binary Codes", "label": "facilitates", "title": "facilitates", "to": "Fast Similarity Search", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Hinton \u0026 Salakhutdinov (2006)", "label": "investigates", "title": "investigates", "to": "Reducing Data Dimensionality", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "Neural Networks", "label": "reduces", "title": "reduces", "to": "data dimensionality", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "Neural Networks", "label": "provides", "title": "provides", "to": "background knowledge", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "Neural Networks", "label": "used_for", "title": "used_for", "to": "Object Recognition", "width": 3.25}, {"arrows": "to", "color": "#0077CC", "from": "Tiny Images", "label": "used_for", "title": "used_for", "to": "nonparametric object recognition", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Hash Bit Selection", "label": "provides", "title": "provides", "to": "unified solution", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Shift-invariant kernels", "label": "generates", "title": "generates", "to": "locality-sensitive binary codes", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Minimal Loss Hashing", "label": "creates", "title": "creates", "to": "compact binary codes", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Science", "label": "is_publication_venue", "title": "is_publication_venue", "to": "research paper", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Torralba, A.", "label": "authored", "title": "authored", "to": "80 million tiny images", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "80 million tiny images", "label": "is_published_in", "title": "is_published_in", "to": "*PAM*I", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Wang, J.", "label": "authored", "title": "authored", "to": "Semi-supervised hashing", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Venice Erin Liong", "label": "affiliated_with", "title": "affiliated_with", "to": "Advanced Digital Sciences Center", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Gang Wang", "label": "affiliated_with", "title": "affiliated_with", "to": "School of Electrical and Electronic Engineering", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Jie Zhou", "label": "affiliated_with", "title": "affiliated_with", "to": "Department of Automation", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Department of Automation", "label": "is_part_of", "title": "is_part_of", "to": "Tsinghua University", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Dongyoon Han", "label": "is_author_of", "title": "is_author_of", "to": "Unsupervised Simultaneous Orthogonal Basis Clustering Feature Selection", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Dongyoon Han", "label": "affiliated_with", "title": "affiliated_with", "to": "Department of Automation", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Dongyoon Han", "label": "email", "title": "email", "to": "jzhou@tsinghua.edu.cn", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "Unsupervised Simultaneous Orthogonal Basis Clustering Feature Selection", "label": "is_topic_of", "title": "is_topic_of", "to": "Han_Unpublished_Simultaneous_Orthogonal_2015_CVPR_paper", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Han_Unsupervised_Simultaneous_Orthogonal_2015_CVPR_paper", "label": "publication_venue", "title": "publication_venue", "to": "CVPR", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Han_Unsupervised_Simultaneous_Orthogonal_2015_CVPR_paper", "label": "year", "title": "year", "to": "2015", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "feature selection methods", "label": "is_characterized_as", "title": "is_characterized_as", "to": "supervised and unsupervised feature selection methods", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "SOCFS", "label": "is_a", "title": "is_a", "to": "unsupervised feature selection method", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "SOCFS", "label": "designed_for", "title": "designed_for", "to": "feature selection on unlabeled data", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "SOCFS", "label": "uses", "title": "uses", "to": "regularized regression-based formulation", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "SOCFS", "label": "achieves", "title": "achieves", "to": "state-of-the-art results", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "SOCFS", "label": "performs_on", "title": "performs_on", "to": "real world datasets", "width": 3.73}, {"arrows": "to", "color": "#0077CC", "from": "regularized regression-based formulation", "label": "uses", "title": "uses", "to": "target matrix", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "target matrix", "label": "captures", "title": "captures", "to": "latent cluster centers", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "target matrix", "label": "guides", "title": "guides", "to": "projection matrix", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "projection matrix", "label": "selects", "title": "selects", "to": "discriminative features", "width": 3.79}, {"arrows": "to", "color": "#CC7700", "from": "projection matrix", "label": "has_property", "title": "has_property", "to": "sparse nature", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Nie et al.", "label": "authored", "title": "authored", "to": "feature selection via joint l2,1-norms minimization", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "feature selection via joint l2,1-norms minimization", "label": "published in", "title": "published in", "to": "NIPS", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Nene et al.", "label": "authored", "title": "authored", "to": "Columbia object image library (coil-20)", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Columbia object image library (coil-20)", "label": "is_technical_report", "title": "is_technical_report", "to": "CCUCS-005-96", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Yang et al.", "label": "authored", "title": "authored", "to": "l2,1-norm regularized discriminative feature selection", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Qian and Zhai", "label": "authored", "title": "authored", "to": "Robust unsupervised feature selection", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Robust unsupervised feature selection", "label": "published in", "title": "published in", "to": "IJCAI", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Sch\u00a8onemann", "label": "authored", "title": "authored", "to": "generalized solution of the orthogonal Procustes problem", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Zhao and Liu", "label": "authored", "title": "authored", "to": "Spectral feature selection", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Spectral feature selection", "label": "published in", "title": "published in", "to": "ICML", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Procrustes problem", "label": "published_in", "title": "published_in", "to": "Psychometrika", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Zhao, Z.", "label": "authored", "title": "authored", "to": "Spectral feature selection", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Samaria, F. S.", "label": "authored", "title": "authored", "to": "stochastic model", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Jianming Zhang", "label": "authored", "title": "authored", "to": "Salient Object Subitizing", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Jianming Zhang", "label": "affiliated_with", "title": "affiliated_with", "to": "Boston University", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Shugao Ma", "label": "authored", "title": "authored", "to": "Salient Object Subitizing", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Shugao Ma", "label": "affiliated_with", "title": "affiliated_with", "to": "Boston University", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Mehrnooush Sameki", "label": "authored", "title": "authored", "to": "Salient Object Subitizing", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Stan Sclaroff", "label": "authored", "title": "authored", "to": "Salient Object Subitizing", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Stan Sclaroff", "label": "affiliated_with", "title": "affiliated_with", "to": "Boston University", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Margrit Betke", "label": "authored", "title": "authored", "to": "Salient Object Subitizing", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "Margrit Betke", "label": "affiliated_with", "title": "affiliated_with", "to": "Boston University", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Radom\u00edr M\u011bch", "label": "authored", "title": "authored", "to": "Salient Object Subitizing", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "People", "label": "can", "title": "can", "to": "subitizing", "width": 3.91}, {"arrows": "to", "color": "#CC7700", "from": "Salient Object Subitizing (SOS)", "label": "aims_to", "title": "aims_to", "to": "predict existence and number", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Salient Object Subitizing (SOS)", "label": "has_application_in", "title": "has_application_in", "to": "salient object detection", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Salient Object Subitizing (SOS)", "label": "utilizes", "title": "utilizes", "to": "Convolutional Neural Networks (CNNs)", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Crowd Sourcing", "label": "used_for", "title": "used_for", "to": "Dataset Creation", "width": 3.7600000000000002}, {"arrows": "to", "color": "#CC7700", "from": "Holistic Image Analysis", "label": "encompasses", "title": "encompasses", "to": "Salient Object Subitizing (SOS)", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Computer Vision Applications", "label": "includes", "title": "includes", "to": "object detection", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Computer Vision Applications", "label": "includes", "title": "includes", "to": "robot vision", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Mehrnoosh Sameki", "label": "affiliated_with", "title": "affiliated_with", "to": "Boston University", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Radom\u00b4\u0131r M\u02d8ech", "label": "affiliated_with", "title": "affiliated_with", "to": "Adobe Research", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Discriminaitve Shape from Shading", "label": "is_paper_title", "title": "is_paper_title", "to": "Richter_Discriminaitve_Shape_From_2015_CVPR_paper.pdf", "width": 3.91}, {"arrows": "to", "color": "#CC7700", "from": "Discriminaitve Shape from Shading", "label": "addresses_topic", "title": "addresses_topic", "to": "Shape from Shading", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Richter_Discriminaitve_Shape_From_2015_CVPR_paper.pdf", "label": "is_publication", "title": "is_publication", "to": "CVPR", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Estimating surface normals", "label": "is_a", "title": "is_a", "to": "challenging problem", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Estimating surface normals", "label": "is", "title": "is", "to": "under-constrained problem", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Simplifying assumptions", "label": "example_includes", "title": "example_includes", "to": "directional lighting", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Simplifying assumptions", "label": "example_includes", "title": "example_includes", "to": "known reflectance maps", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "regression forests", "label": "incorporates", "title": "incorporates", "to": "Von Mises-Fisher distributions", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "spatial features", "label": "example_includes", "title": "example_includes", "to": "textons", "width": 3.58}, {"arrows": "to", "color": "#0077CC", "from": "spatial features", "label": "example_includes", "title": "example_includes", "to": "novel silhouette features", "width": 3.52}, {"arrows": "to", "color": "#0077CC", "from": "generalization", "label": "relates_to", "title": "relates_to", "to": "uncalibrated illumination", "width": 3.61}, {"arrows": "to", "color": "#0077CC", "from": "pixel-independent prediction", "label": "achieves", "title": "achieves", "to": "efficient estimation", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "Discrimiative Learning", "label": "is_topic", "title": "is_topic", "to": "research area", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "J. T. Barron", "label": "authored", "title": "authored", "to": "Color constancy, intrinsic images, and shape estimation", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "J. T. Barron", "label": "authored", "title": "authored", "to": "Shape, albedo, and illumination from a single image", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "J. Ben-Arie", "label": "authored", "title": "authored", "to": "A neural network approach", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "L. Breiman", "label": "authored", "title": "authored", "to": "Random forests", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "ShapeCollage", "label": "interprets", "title": "interprets", "to": "shape", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "ShapeCollage", "label": "uses", "title": "uses", "to": "example-based methods", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "ShapeCollage", "label": "presented_at", "title": "presented_at", "to": "ECCV", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "ShapeCollage", "label": "addresses", "title": "addresses", "to": "occlusion-aware shape interpretation", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Image-to-geometry registration", "label": "method_uses", "title": "method_uses", "to": "mutual information", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Image-to-geometry registration", "label": "exploits", "title": "exploits", "to": "geometric properties", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "Modeling data", "label": "uses", "title": "uses", "to": "directional distributions", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Dispersion", "label": "published_in", "title": "published_in", "to": "P. Roy. Soc. Lond. B", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Floating scale reconstruction", "label": "presented_at", "title": "presented_at", "to": "SIGGRAPH", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "SIGGRAPH", "label": "is_conference_for", "title": "is_conference_for", "to": "Debevec et al.", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Adelson", "label": "authored", "title": "authored", "to": "Ground truth dataset", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "W. T. Freeman", "label": "authored", "title": "authored", "to": "Ground truth dataset", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Jean-Dominique FAVREAU", "label": "authored", "title": "authored", "to": "Line Drawing Interpretation", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Jean-Dominique FAVREAU", "label": "is_affiliated_with", "title": "is_affiliated_with", "to": "INRIA Sophia-Antippolis", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Line Drawing Interpretation", "label": "presented_at", "title": "presented_at", "to": "CVPR", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "Line Drawing Interpretation", "label": "related_to", "title": "related_to", "to": "3D Scene Understanding", "width": 3.25}, {"arrows": "to", "color": "#0077CC", "from": "Adrien Bousseau", "label": "authored", "title": "authored", "to": "Line Drawing Interpretation", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Adrien Bousseau", "label": "is_affiliated_with", "title": "is_affiliated_with", "to": "INRIA Sophia-Antipollis", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Adrien Bousseau", "label": "works_at", "title": "works_at", "to": "INRIAL Sophia-Antipolis", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "line drawings", "label": "of", "title": "of", "to": "imaginary objects", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "line drawings", "label": "drawn over", "title": "drawn over", "to": "photographs", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "photographs", "label": "contains", "title": "contains", "to": "desired scene", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "photographs", "label": "contains", "title": "contains", "to": "undesired reflections", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "computer vision algorithms", "label": "offer", "title": "offer", "to": "limited support", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "multi-view stereo algorithms", "label": "reconstruct", "title": "reconstruct", "to": "real-world scenes", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "line-drawing interpretation algorithms", "label": "lack", "title": "lack", "to": "contextual awareness", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "polygon", "label": "belongs to", "title": "belongs to", "to": "orientation", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "new structures", "label": "present_in", "title": "present_in", "to": "real world", "width": 3.7600000000000002}, {"arrows": "to", "color": "#CC7700", "from": "furniture design", "label": "is_example_of", "title": "is_example_of", "to": "application domain", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "archaeology", "label": "is_example_of", "title": "is_example_of", "to": "application domain", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "new orientation", "label": "allows", "title": "allows", "to": "creation of new structures", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "new orientation", "label": "is", "title": "is", "to": "unknown orientation", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "Computer-Aided Design", "label": "facilitates", "title": "facilitates", "to": "creation of new structures", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "Furniture Design", "label": "is_example_of", "title": "is_example_of", "to": "Computer-Aided Design application", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "O-snap", "label": "is_a", "title": "is_a", "to": "optimization-based snapping method", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "M. Arikan", "label": "author_of", "title": "author_of", "to": "O-snap", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "H. Barrow", "label": "author_of", "title": "author_of", "to": "Line Drawing Interpretation", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Y. Boykov", "label": "author_of", "title": "author_of", "to": "energy minimization algorithms", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "A.-L. Chauve", "label": "author_of", "title": "author_of", "to": "3D reconstruction methods", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "Multi-View Stereo Reconstruction", "label": "uses", "title": "uses", "to": "energy minimization algorithms", "width": 3.0999999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Furukawa et al.", "label": "presented_at", "title": "presented_at", "to": "Manhattan-world stereo", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Wiley-ISTE", "label": "published", "title": "published", "to": "Stochastic geometry for image analysis", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Florent LAFARGE", "label": "is_affiliated_with", "title": "is_affiliated_with", "to": "INRIA Sophia-Antipollis", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Florent LAFARGE", "label": "works_at", "title": "works_at", "to": "INRIA Sophia-Antipolis", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Bis Publishers", "label": "published", "title": "published", "to": " Sketching: The Basics", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "jean-dominuque.favreau@inria.fr", "label": "works_at", "title": "works_at", "to": "INRIA Sophia-Antipolis", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Olga Russakovsky", "label": "is_author_of", "title": "is_author_of", "to": "Best of both worlds", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Olga Russakovsky", "label": "affiliated_with", "title": "affiliated_with", "to": "Stanford University", "width": 3.7600000000000002}, {"arrows": "to", "color": "#CC7700", "from": "Best of both worlds", "label": "is_a", "title": "is_a", "to": "CVPR paper", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Li-Jia Li", "label": "is_author_of", "title": "is_author_of", "to": "Best of both worlds", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Li-Jia Li", "label": "works_at", "title": "works_at", "to": "Snapchat", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Li Fei-Fei", "label": "is_author_of", "title": "is_author_of", "to": "Best of both worlds", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Li Fei-Fei", "label": "affiliated_with", "title": "affiliated_with", "to": "Stanford University", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Russakovsky_Best_of_Both_2015_CVPR_paper.pdf", "label": "is_an_example_of", "title": "is_an_example_of", "to": "CVPR paper", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "jean-dominique.favreau@inria.fr", "label": "is_email_of", "title": "is_email_of", "to": "jean-dominique.favreau", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "florent.lafarge@inria.fr", "label": "is_email_of", "title": "is_email_of", "to": "Florent LAFARGE", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "adrien.bousseau@inria.fr", "label": "is_email_of", "title": "is_email_of", "to": "Adrien Bousseau", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "manual annotation", "label": "is", "title": "is", "to": "quite expensive", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "manual annotation", "label": "influenced by", "title": "influenced by", "to": "crowd engineering innovations", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "automatic object detectors", "label": "detect", "title": "detect", "to": "at most a few objects per image", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "object annotations", "label": "informed_by", "title": "informed_by", "to": "human feedback", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "object annotations", "label": "informed_by", "title": "informed_by", "to": "computer vision", "width": 3.82}, {"arrows": "to", "color": "#CC7700", "from": "computer vision models", "label": "provides", "title": "provides", "to": "object annotations", "width": 3.67}, {"arrows": "to", "color": "#0077CC", "from": "human input", "label": "source_of", "title": "source_of", "to": "feedback", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Visesh Chari", "label": "authored", "title": "authored", "to": "On Pairwise Costs for Network Flow Multi-Object Tracking", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Visesh Chari", "label": "is_affiliated_with", "title": "is_affiliated_with", "to": "INRIA", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "Visesh Chari", "label": "affiliated_with", "title": "affiliated_with", "to": "Ecole Normale Sup\u00b4erieure", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Ivan Laptev", "label": "authored", "title": "authored", "to": "On Pairwise Costs for Network Flow Multi-Object Tracking", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Ivan Laptev", "label": "is_affiliated_with", "title": "is_affiliated_with", "to": "INRIA", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "Josef Sivic", "label": "authored", "title": "authored", "to": "On Pairwise Costs for Network Flow Multi-Object Tracking", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Josef Sivic", "label": "is_affiliated_with", "title": "is_affiliated_with", "to": "INRIA", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "Chari_On_Pairwise_Costs_2015_CVPR_supplemental", "label": "is_supplement_to", "title": "is_supplement_to", "to": "On Pairwise Costs for Network Flow Multi-Object Tracking", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Multi-object Tracking", "label": "uses", "title": "uses", "to": "Network Flow Optimization", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "Multi-object Tracking", "label": "is_field_of", "title": "is_field_of", "to": "Computer Vision", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Network Flow Optimization", "label": "models", "title": "models", "to": "dependencies among tracks", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Pairwise Costs", "label": "addresses", "title": "addresses", "to": "object detector failures", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Pairwise Costs", "label": "introduced_to", "title": "introduced_to", "to": "min-cost network flow framework", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Convex Relaxation", "label": "includes", "title": "includes", "to": "efficient rounding heuristic", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "pairwise costs", "label": "evaluated_in", "title": "evaluated_in", "to": "real-world video sequences", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "pairwise costs", "label": "improves", "title": "improves", "to": "recent tracking methods", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "Tracking-by-Detection", "label": "uses", "title": "uses", "to": "pairwise costs", "width": 3.4000000000000004}, {"arrows": "to", "color": "#CC7700", "from": "Tracking-by-Detection", "label": "is_approach_in", "title": "is_approach_in", "to": "Multi-object Tracking", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "TILDE", "label": "is_method", "title": "is_method", "to": "Temporally Invariant Learned Detector", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Yannick Verdie", "label": "is_author_of", "title": "is_author_of", "to": "TILDE", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Yannick Verdie", "label": "affiliated_with", "title": "affiliated_with", "to": "Computer Vision Laboratory, EPFL", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Kwang Moo Yi", "label": "is_author_of", "title": "is_author_of", "to": "TILDE", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Kwang Moo Yi", "label": "affiliated_with", "title": "affiliated_with", "to": "Computer Vision Laboratory, EPFL", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Pascal Fua", "label": "is_author_of", "title": "is_author_of", "to": "TILDE", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Vincent Lepetit", "label": "is_author_of", "title": "is_author_of", "to": "TILDE", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Simon Lacoste-Julien", "label": "is_affiliated_with", "title": "is_affiliated_with", "to": "INRIO", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "Cortes, C.", "label": "contributed_to", "title": "contributed_to", "to": "Support-Vector Networks", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "Cortes, C.", "label": "co_author_of", "title": "co_author_of", "to": "Vapnik, V.", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Support-Vector Networks", "label": "published in", "title": "published in", "to": "Machine Learning", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Vapnik, V.", "label": "contributed_to", "title": "contributed_to", "to": "Support-Vector Networks", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "Harris, C.", "label": "contributed_to", "title": "contributed_to", "to": "Combined Corner and Edge Detector", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "Stephens, M.", "label": "contributed_to", "title": "contributed_to", "to": "Combined Corner and Edge Detector", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "C.", "label": "authored", "title": "authored", "to": "Support-Vector Networks", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Corner Detector", "label": "developed by", "title": "developed by", "to": "Harris, C.", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Corner Detector", "label": "developed by", "title": "developed by", "to": "Stephens, M.", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Hinging Hyperplanes", "label": "developed by", "title": "developed by", "to": "Breiman, L.", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Hinging Hyperplanes", "label": "published in", "title": "published in", "to": "IEEE Transactions on Information Theory", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Gradient-Based Learning", "label": "applied to", "title": "applied to", "to": "Document Recognition", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Af\ufb01ne Region Detectors", "label": "compared in", "title": "compared in", "to": "International Journal of Computer Vision", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Mikolajczyk, K.", "label": "authored", "title": "authored", "to": "A Comparison of Af\ufb01ne Region Detectors", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "rs", "label": "authored", "title": "authored", "to": "A Comparison of Af\ufb01ne Region Detectors", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Zisserma", "label": "authored", "title": "authored", "to": "A Comparison of Af\ufb01ne Region Detectors", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Mata", "label": "authored", "title": "authored", "to": "A Comparison of Af\ufb01ne Region Detectors", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Schaffalitzky", "label": "authored", "title": "authored", "to": "A Comparison of Af\ufb01ne Region Detectors", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Kadir", "label": "authored", "title": "authored", "to": "A Comparison of Af\ufb01ne Region Detectors", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Van Gool", "label": "authored", "title": "authored", "to": "A Review of Af\ufb01ne Region Detectors", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Dollar", "label": "authored", "title": "authored", "to": "Supervised Learning of Edges and Object Boundaries", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Tu", "label": "authored", "title": "authored", "to": "Supervised Learning of Edges and Object Boundaries", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Belongie", "label": "authored", "title": "authored", "to": "Supervised Learning of Edges and Object Boundaries", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Rosten", "label": "authored", "title": "authored", "to": "Machine Learning for High-Speed Corner Detection", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Drummond", "label": "authored", "title": "authored", "to": "Machine Learning for High-Speed Corner Detection", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Lowe", "label": "authored", "title": "authored", "to": "Distinctive Image Features from Scale-Invariant Keypoints", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Fan", "label": "developed", "title": "developed", "to": "LIBLINEAR", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Fan", "label": "published_in", "title": "published_in", "to": "Journal of Machine Learning Research", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Chang", "label": "authored", "title": "authored", "to": "LIBLINEAR", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Chang", "label": "deals_with", "title": "deals_with", "to": "tracking", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Hsieh", "label": "authored", "title": "authored", "to": "LIBLINEAR", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Lin", "label": "authored", "title": "authored", "to": "LIBLINEAR", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Verdie", "label": "affiliated_with", "title": "affiliated_with", "to": "EPFL", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Yi", "label": "affiliated_with", "title": "affiliated_with", "to": "EPFL", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Vincent Le Petit", "label": "affiliated_with", "title": "affiliated_with", "to": "Institute for Computer Graphics and Vision, Graz University of Technology", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "JOTS", "label": "is_a", "title": "is_a", "to": "Joint Online Tracking and Segmentation", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Joint Online Tracking and Segmentation", "label": "addresses", "title": "addresses", "to": "Video Segmentation task", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Joint Online Tracking and Segmentation", "label": "integrates", "title": "integrates", "to": "Multi-part tracking", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Joint Online Tracking and Segmentation", "label": "integrates", "title": "integrates", "to": "Segmentation", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Joint Online Tracking and Segmentation", "label": "uses", "title": "uses", "to": "Energy Function Optimization", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Joint Online Tracking and Segmentation", "label": "demonstrated_effectiveness_on", "title": "demonstrated_effectiveness_on", "to": "SegTrack database", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Joint Online Tracking and Segmentation", "label": "demonstrated_effectiveness_on", "title": "demonstrated_effectiveness_on", "to": "SegTrack v2 database", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Longyin Wen", "label": "is_author_of", "title": "is_author_of", "to": "JOTS", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Longyin Wen", "label": "affiliated_with", "title": "affiliated_with", "to": "NLPR, Institute of Automation, Chinese Academy of Sciences", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Dawei Du", "label": "is_author_of", "title": "is_author_of", "to": "JETS", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Dawei Du", "label": "affiliated_with", "title": "affiliated_with", "to": "SCCE, University of Chinese Academy of Sciences", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Zhen Lei", "label": "is_author_of", "title": "is_author_of", "to": "JOTS", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Zhen Lei", "label": "affiliated_with", "title": "affiliated_with", "to": "NLPR, Institute of Automation, Chinese Academy of Sciences", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Zhen Lei", "label": "is_author_of", "title": "is_author_of", "to": "High-Fidelity Pose and Expression Normalization for Face Recognition in the Wild", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Zhen Lei", "label": "is_author_of", "title": "is_author_of", "to": "High-Fidelity Pose and Expression Normalization for FaceRecognition in the Wild", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Zhen Lei", "label": "affiliated_with", "title": "affiliated_with", "to": "Center for Biometrics and Security Research", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Zhen Lei", "label": "affiliated_with", "title": "affiliated_with", "to": "National Laboratory of Pattern Recognition", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Stan Z. Li", "label": "is_author_of", "title": "is_author_of", "to": "JOTS", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Stan Z. Li", "label": "affiliated_with", "title": "affiliated_with", "to": "NLPR", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Stan Z. Li", "label": "affiliated_with", "title": "affiliated_with", "to": "Center for Biometrics and Security Research", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Stan Z. Li", "label": "affiliated_with", "title": "affiliated_with", "to": "National Laboratory of Pattern Recognition", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Stan Z. Li", "label": "email", "title": "email", "to": "szli@nlpr.ia.ac.cn", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Wen_JOTS_Joint_Online_2015_CVPR_paper.pdf", "label": "is_publication_of", "title": "is_publication_of", "to": "CVPR", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Tracking and Segmentation stages", "label": "optimized_using", "title": "optimized_using", "to": "RANSA-style approach", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "Multi-part Models", "label": "related_to", "title": "related_to", "to": "Tracking and Segmentation", "width": 3.25}, {"arrows": "to", "color": "#0077CC", "from": "multi-target tracking", "label": "is_task_of", "title": "is_task_of", "to": "video analysis", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "topological constraints", "label": "enhances", "title": "enhances", "to": "tracking", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Vasconcelos", "label": "affiliated_with", "title": "affiliated_with", "to": "NLPR, Institute of Automation, Chinese Academy of Sciences", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Vasconcelos", "label": "addresses", "title": "addresses", "to": "tracking deformable and occluded objects", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "S. Z. Li", "label": "affiliated_with", "title": "affiliated_with", "to": "NLPR, Institute of Automation, Chinese Academy of Sciences", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "Delong", "label": "provides", "title": "provides", "to": "optimization method", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "NLPR", "label": "located_in", "title": "located_in", "to": "Chinese Academy of Sciences", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Philipp Kr\u00e4henbuhl", "label": "authored", "title": "authored", "to": "Learning to Propose Objects", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Learning to Propose Objects", "label": "authored", "title": "authored", "to": "Vladlen Koltun", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Learning to Propose Objects", "label": "is_publication_of", "title": "is_publication_of", "to": "CVPR", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Krahenbuhl_Learning_to_Propos_2015_CVPR_paper.pdf", "label": "represents", "title": "represents", "to": "Learning to Propose Objects", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "zlei", "label": "affiliated_with", "title": "affiliated_with", "to": "NLPR", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "ensemble", "label": "trained", "title": "trained", "to": "jointly", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "ensemble", "label": "operates_on", "title": "operates_on", "to": "elementary image features", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "ensemble", "label": "enables", "title": "enables", "to": "rapid image analysis", "width": 3.88}, {"arrows": "to", "color": "#CC7700", "from": "ensemble", "label": "has", "title": "has", "to": "composition", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "ensemble", "label": "has", "title": "has", "to": "parameters", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "ensemble training", "label": "reduced_to", "title": "reduced_to", "to": "sequence of uncapacitated facility location problems", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "procedure", "label": "optimizes", "title": "optimizes", "to": "size of the ensemble", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "procedure", "label": "optimizes", "title": "optimizes", "to": "composition", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "procedure", "label": "is_scalable", "title": "is_scalable", "to": "set of cliques", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "procedure", "label": "activates", "title": "activates", "to": "cliques", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "ensembles", "label": "operate_on", "title": "operate_on", "to": "elementary image features", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "presented approach", "label": "outperforms", "title": "outperforms", "to": "prior object proposal algorithms", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "presented approach", "label": "has", "title": "has", "to": "lowest running time", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "presented approach", "label": "capable_of_learning", "title": "capable_of_learning", "to": "bottom-up segmentation model", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "presented approach", "label": "learns", "title": "learns", "to": "generally applicable model", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "trained ensembles", "label": "generalize across", "title": "generalize across", "to": "datasets", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "bottom-up segmentation", "label": "is_a", "title": "is_a", "to": "model", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Ensemble Methods", "label": "improves", "title": "improves", "to": "running time", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Arbel\u00e1ez et al. (2012)", "label": "authored", "title": "authored", "to": "Semantic segmentation using regions and parts", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Carreira et al.", "label": "authored", "title": "authored", "to": "Free-form region description with second-order pooling", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Free-form region description with second-order pooling", "label": "published_in", "title": "published_in", "to": "PAMI", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Microsoft COCO", "label": "contains", "title": "contains", "to": "common objects", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Objectness", "label": "is measured by", "title": "is measured by", "to": "Alexe et al.", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "Structured forests", "label": "presented_in", "title": "presented_in", "to": "ICCV", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "BING", "label": "presented_in", "title": "presented_in", "to": "CVPR", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Geometry of cuts and metrics", "label": "published_by", "title": "published_by", "to": "Springer", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "Aravindh Mahendran", "label": "is_author_of", "title": "is_author_of", "to": "Understanding Deep Image Representations", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Aravindh Mahendran", "label": "authored", "title": "authored", "to": "Understanding Deep Image Representations by Inverting Them", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Aravindh Mahendran", "label": "affiliated_with", "title": "affiliated_with", "to": "Intel Labs", "width": 3.25}, {"arrows": "to", "color": "#0077CC", "from": "Andrea Vedaldi", "label": "is_author_of", "title": "is_author_of", "to": "Understanding Deep Image Representations", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Andrea Vedaldi", "label": "authored", "title": "authored", "to": "Understanding Deep Image Representations by Inverting Them", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Understanding Deep Image Representations by Inverting Them", "label": "is_publication_of", "title": "is_publication_of", "to": "CVPR", "width": 3.7600000000000002}, {"arrows": "to", "color": "#CC7700", "from": "Understanding Deep Image Representations by Inverting Them", "label": "concerns", "title": "concerns", "to": "Deep Image Representations", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Understanding Deep Image Presentations by Inverting Them", "label": "year", "title": "year", "to": "2015", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Mahendran_Understanding_Deep_Image_2015_CVPR_paper.pdf", "label": "is_file_of", "title": "is_file_of", "to": "cvpr_papers", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Image Representations", "label": "exhibits", "title": "exhibits", "to": "geometric and photometric invariance", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Bishop", "label": "authored", "title": "authored", "to": "Neural Networks for Pattern Recognition", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "deformable part models", "label": "is_used_in", "title": "is_used_in", "to": "object detection", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "3D textons", "label": "used_in", "title": "used_in", "to": "material recognition", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "SIFT detector", "label": "is_implementation", "title": "is_implementation", "to": "open-source implementation", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "SIFT detector", "label": "is_part_of", "title": "is_part_of", "to": "image processing tasks", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Zeiler \u0026 Fergus", "label": "focuses_on", "title": "focuses_on", "to": "visualizing convolutional networks", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Hinton \u0026 Salakhutdinov", "label": "discusses", "title": "discusses", "to": "dimensionality reduction", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Wang et al.", "label": "introduces", "title": "introduces", "to": "locality-constrained linear coding", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Arvindh Mahendran", "label": "affiliated_with", "title": "affiliated_with", "to": "University of Oxford", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Yan Xia", "label": "author_of", "title": "author_of", "to": "Sparse Projections for High-Dimensional Binary Codes", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Yan Xia", "label": "is_author_of", "title": "is_author_of", "to": "Sparse Projections", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Yan Xia", "label": "affiliated_with", "title": "affiliated_with", "to": "University of Science and Technology of China", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Pushmeet Kohli", "label": "author_of", "title": "author_of", "to": "Sparse Projections for High-Dimensional Binary Codes", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Pushmeet Kohli", "label": "affiliated_with", "title": "affiliated_with", "to": "Microsoft Research", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Pushmeet Kohli", "label": "author_of", "title": "author_of", "to": "Computationally Bounded Retrieval", "width": 3.91}, {"arrows": "to", "color": "#CC7700", "from": "Sparse Projections", "label": "is_topic_of", "title": "is_topic_of", "to": "CVPR paper", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "problem of learning long binary codes", "label": "has_challenge", "title": "has_challenge", "to": "lack of effective regularizer", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "sparsity encouraging regularizer", "label": "reduces", "title": "reduces", "to": "number of parameters", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "sparsity encouraging regularizer", "label": "reduces", "title": "reduces", "to": "overfitting", "width": 3.7600000000000002}, {"arrows": "to", "color": "#00CC77", "from": "sparse projection matrix", "label": "leads_to", "title": "leads_to", "to": "reduction in computational cost", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "dense projections", "label": "includes", "title": "includes", "to": "ITQ", "width": 3.55}, {"arrows": "to", "color": "#00CC77", "from": "rix", "label": "leads_to", "title": "leads_to", "to": "reduction in computational cost", "width": 3.94}, {"arrows": "to", "color": "#00CC77", "from": "other methods", "label": "speeds_up", "title": "speeds_up", "to": "high-dimensional binary encoding", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Agrawal et al. (2014)", "label": "provides", "title": "provides", "to": "Foundational context", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Agrawal et al. (2014)", "label": "focuses_on", "title": "focuses_on", "to": "Neural Networks", "width": 3.4000000000000004}, {"arrows": "to", "color": "#CC7700", "from": "Object Recognition", "label": "uses", "title": "uses", "to": "Local Scale-Invariant Features", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Fan et al. (2008)", "label": "introduces", "title": "introduces", "to": "Liblinear", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "Liblinear", "label": "is_a", "title": "is_a", "to": "library", "width": 3.91}, {"arrows": "to", "color": "#CC7700", "from": "binary code learning", "label": "is_technique_for", "title": "is_technique_for", "to": "efficient similarity search", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "approximate nearest neighbor search", "label": "is_task_in", "title": "is_task_in", "to": "many applications", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "sparse approximation techniques", "label": "can_be_used_for", "title": "can_be_used_for", "to": "feature selection", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "sparse approximation techniques", "label": "can_be_used_for", "title": "can_be_used_for", "to": "dimensionality reduction", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "product quantization", "label": "is_used_for", "title": "is_used_for", "to": "approximate nearest neighbor search", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "atomic decomposition", "label": "uses", "title": "uses", "to": "basis pursuit", "width": 3.64}, {"arrows": "to", "color": "#CC7700", "from": "basis pursuit", "label": "is_method_for", "title": "is_method_for", "to": "atomic decomposition", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "hashing algorithms", "label": "aim_for", "title": "aim_for", "to": "approximate nearest neighbor search", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "ear-optimal hashing algorithms", "label": "used_for", "title": "used_for", "to": "approximate nearest neighbor search", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "ear-optimal hashing algorithms", "label": "presented_in", "title": "presented_in", "to": "FOCS", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "locality-sensitive hashing", "label": "is_technique_for", "title": "is_technique_for", "to": "approximate nearest neighbor search", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "locality-sensitive hashing", "label": "presented_in", "title": "presented_in", "to": "Symposium on Computational Geometry", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "Procrustes analysis", "label": "is_method_for", "title": "is_method_for", "to": "finding optimal transformation", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Procrustes problems", "label": "published_by", "title": "published_by", "to": "Oxford University Press", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Huazhu Fu", "label": "affiliated_with", "title": "affiliated_with", "to": "Microsoft Research", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Dong Xu", "label": "affiliated_with", "title": "affiliated_with", "to": "Microsoft Research", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Stephen Lin", "label": "affiliated_with", "title": "affiliated_with", "to": "Microsoft Research", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Jiang Liu", "label": "affiliated_with", "title": "affiliated_with", "to": "Microsoft Research", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Object-based RGBD Image Co-segmentation with Mutex Constraint", "label": "is_a", "title": "is_a", "to": "CVPR paper", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Object-based RGBD Image Co-segmentation with Mutex Constraint", "label": "authored_by", "title": "authored_by", "to": "Huazhu Fu", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Object-based RGBD Image Co-segmentation with Mutux Constraint", "label": "authored_by", "title": "authored_by", "to": "Dong Xu", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Object-based RGBD Image Co-segmentation with Mutux Constraint", "label": "authored_by", "title": "authored_by", "to": "Stephen Lin", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "depth channel", "label": "enhances", "title": "enhances", "to": "identification of similar foreground objects", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "co-segmentation", "label": "formulated_in", "title": "formulated_in", "to": "fully-connected graph structure", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "graph structure", "label": "includes", "title": "includes", "to": "mutex constraints", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "mutex constraints", "label": "prevents", "title": "prevents", "to": "improper solutions", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "object-based RGBD co-segmentation", "label": "outperforms", "title": "outperforms", "to": "related methods", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "object-based RGBD co-segmentation", "label": "incorporates", "title": "incorporates", "to": "mutex constraints", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "object-based RGBD co-segmentation", "label": "improves", "title": "improves", "to": "segmentation accuracy", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "RGBD co-segmentation", "label": "outperforms", "title": "outperforms", "to": "related techniques", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "comparable performance", "label": "relates_to", "title": "relates_to", "to": "RGB co-segmentation techniques", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "RGB co-segmentation techniques", "label": "utilizes", "title": "utilizes", "to": "depth maps", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "depth maps", "label": "estimated_from", "title": "estimated_from", "to": "RGB images", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Depth maps", "label": "estimated from", "title": "estimated from", "to": "RGB images", "width": 3.88}, {"arrows": "to", "color": "#CC7700", "from": "Object-Based Methods", "label": "is_a", "title": "is_a", "to": "Method", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "Mutual Exclusion Constraints", "label": "is_a", "title": "is_a", "to": "Method", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "Co-Saliency Maps", "label": "is_a", "title": "is_a", "to": "Method", "width": 3.64}, {"arrows": "to", "color": "#CC7700", "from": "Graph Formulation", "label": "is_a", "title": "is_a", "to": "Method", "width": 3.61}, {"arrows": "to", "color": "#0077CC", "from": "Multi-Plane Block-Coordinate Frank-Wilfe Algorithm", "label": "trains", "title": "trains", "to": "Structural SVMs", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Multi-Plane Block-Coordinate Frank-Wilfe Algorithm", "label": "uses", "title": "uses", "to": "max-Oracle", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "Neel Shah", "label": "is_author_of", "title": "is_author_of", "to": "paper", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Neel Shah", "label": "affiliated_with", "title": "affiliated_with", "to": "IST Austria", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Vladimir Kolmogorov", "label": "is_author_of", "title": "is_author_of", "to": "paper", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Vladimir Kolmogorov", "label": "affiliated_with", "title": "affiliated_with", "to": "IST Austria", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Chris H. Lampert", "label": "is_author_of", "title": "is_author_of", "to": "paper", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Chris H. Lampert", "label": "affiliated_with", "title": "affiliated_with", "to": "IST Australia", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Structural Support Vector Machines", "label": "is_effective_for", "title": "is_effective_for", "to": "structured computer vision tasks", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Structural Support Vector Machines", "label": "uses", "title": "uses", "to": "Max-Oracle", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Structural Support Vector Machines", "label": "uses", "title": "uses", "to": "Frank-Wolfe Algorithm", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Structural Support Vector Machines", "label": "uses", "title": "uses", "to": "Block-Coordinate Methods", "width": 3.46}, {"arrows": "to", "color": "#0077CC", "from": "Frank-Wolfe algorithm", "label": "is_designed_for", "title": "is_designed_for", "to": "training SSVMs", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Frank-Wolfe algorithm", "label": "combines_with", "title": "combines_with", "to": "caching mechanism", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "criterion", "label": "decides_whether_to", "title": "decides_whether_to", "to": "call max-oracle", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "max-oracle", "label": "is", "title": "is", "to": "bottleneck", "width": 3.7600000000000002}, {"arrows": "to", "color": "#CC7700", "from": "Max-Oracle", "label": "is", "title": "is", "to": "optimization technique", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Beier_Fusion_Moves_for_2015_CVPR_supplemental", "label": "is_supplemental_material_for", "title": "is_supplemental_material_for", "to": "Fusion Moves for Correlation Clustering", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "PIVOT-BOEM", "label": "is_algorithm", "title": "is_algorithm", "to": "Correlation Clustering", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "HC", "label": "is_algorithm", "title": "is_algorithm", "to": "Correlation Clustering", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "CGC", "label": "is_algorithm", "title": "is_algorithm", "to": "Correlation Clustering", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Fusion Moves", "label": "is_variant_of", "title": "is_variant_of", "to": "Correlation Clustering", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Experimental Analysis", "label": "evaluates", "title": "evaluates", "to": "Algorithms", "width": 3.64}, {"arrows": "to", "color": "#CC7700", "from": "Anytime Algorithms", "label": "exhibits", "title": "exhibits", "to": "Anytime Behavior", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "Anytime Behavior", "label": "describes", "title": "describes", "to": "Progressive Improvement", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "Dataset Performance", "label": "evaluated_on", "title": "evaluated_on", "to": "Instances", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "University of Heidelberg (Iwr)", "label": "affiliation_of", "title": "affiliation_of", "to": "Thorsten Beier", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "University of Heidelberg (Iwr)", "label": "affiliation_of", "title": "affiliation_of", "to": "Fred A. Hamprecht", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Mohammadreza Mostajabi", "label": "is_author_of", "title": "is_author_of", "to": "Feedforward Semantic Segmentation", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Mohammadreza Mostajabi", "label": "affiliated_with", "title": "affiliated_with", "to": "Toyota Technological Institute at Chicago", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Feedforward Semantic Segmentation", "label": "published_in", "title": "published_in", "to": "CVPR", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Payman Yadollahpour", "label": "is_author_of", "title": "is_author_of", "to": "Feedforward Semantic Segmentation", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Payman Yadollahpour", "label": "affiliated_with", "title": "affiliated_with", "to": "Toyota Technological Institute at Chicago", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Gregory Shakhnarovich", "label": "is_author_of", "title": "is_author_of", "to": "Feedforward Semantic Segmentation", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Gregory Shakhnarovich", "label": "authors", "title": "authors", "to": "Mostajabi_Feedforward_Semantic_Segmentation_2015_CVPR_paper", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Gregory Shakhnarovich", "label": "affiliated_with", "title": "affiliated_with", "to": "Toyota Technological Institute at Chicago", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "University of Heidelberg (Department of Mathematics)", "label": "affiliation_of", "title": "affiliation_of", "to": "J\u00f6rg H. Kappes", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Mostajabi_Feedforwad_Semantic_Segmentation_2015_CVPR_paper", "label": "introduces", "title": "introduces", "to": "feed-forward architecture", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "feed-forward architecture", "label": "aims_to", "title": "aims_to", "to": "semantic segmentation", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "image elements", "label": "mapped_to", "title": "mapped_to", "to": "rich feature representations", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "nested regions", "label": "obtained_by", "title": "obtained_by", "to": "zoom-out", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "zoom-out", "label": "starts_from", "title": "starts_from", "to": "superpixel", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "Hypercolumns for object segmentation and fine-grained localization", "label": "is_publication_type", "title": "is_publication_type", "to": "arXiv preprint", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "Hypercolumns for object segmentation and fine-grained localization", "label": "addresses", "title": "addresses", "to": "object segmentation", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Chen et al.", "label": "authored", "title": "authored", "to": "Semantic image segmentation with deep convolutional nets and fully connected crfs", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Semantic image segmentation with deep convolutional nets and fully connected crfs", "label": "is_publication_type", "title": "is_publication_type", "to": "arXiv preprint", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Long et al.", "label": "authored", "title": "authored", "to": "Fully convolutional networks for semantic segmentation", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Fully convolutional networks for semantic segmentation", "label": "is_publication_type", "title": "is_publication_type", "to": "arXiv preprint", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "Fully convolutional networks for semantic segmentation", "label": "addresses", "title": "addresses", "to": "semantic segmentation", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Associative hierarchical CRFs for object class image segmentation", "label": "presented_at", "title": "presented_at", "to": "ICCV", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Carreira and Sminchisescu", "label": "authored", "title": "authored", "to": "CPMC: Automatic object segmentation using constrained parametric min-cuts", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "CPMC: Automatic object segmentation using constrained parametric min-cuts", "label": "published_in", "title": "published_in", "to": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "CPMC", "label": "published_in", "title": "published_in", "to": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Carreira, J.", "label": "authored", "title": "authored", "to": "CPMC", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Sminchisescu, C.", "label": "authored", "title": "authored", "to": "CPMC", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Zisserma, A.", "label": "authored", "title": "authored", "to": "Very deep convolutional networks", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Fr\u00b4edo Durand", "label": "authored", "title": "authored", "to": "Reflection Removal using Ghosting Cues", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Fr\u00b4edo Durand", "label": "is_author_of", "title": "is_author_of", "to": "Ghosting Cues", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Fr\u00b4edo Durand", "label": "affiliated_with", "title": "affiliated_with", "to": "MIT CSAIL", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "YiChang Shih", "label": "authored", "title": "authored", "to": "Reflection Removal using Ghosting Cues", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "YiChang Shih", "label": "is_author_of", "title": "is_author_of", "to": "Ghosting Cues", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "YiChang Shih", "label": "affiliated_with", "title": "affiliated_with", "to": "MIT CSAIL", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Dilip Krishnan", "label": "authored", "title": "authored", "to": "Reflection Removal using Ghosting Cues", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Dilip Krishnan", "label": "is_author_of", "title": "is_author_of", "to": "Ghosting Cues", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Dilip Krishnan", "label": "affiliated_with", "title": "affiliated_with", "to": "Google Research", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "William T. Freeman", "label": "authored", "title": "authored", "to": "Reflection Removal using Ghosting Cues", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "William T. Freeman", "label": "is_author_of", "title": "is_author_of", "to": "Ghosting Cunes", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "William T. Freeman", "label": "affilates_with", "title": "affilates_with", "to": "MIT CSAIL", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "William T. Freeman", "label": "email", "title": "email", "to": "billf@mit.edu", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Ghosting Cues", "label": "is_discussed_in", "title": "is_discussed_in", "to": "Shih_Reflection_Removal_Using_2015_CVPR_paper", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Shih_Reflection_Removal_Using_2015_CVPR_paper", "label": "is_publication", "title": "is_publication", "to": "CVPR paper", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Shih_Reflection_Removal_Using_2015_CVPR_paper", "label": "is_authored_by", "title": "is_authored_by", "to": "YiChang Shih", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Shih_Reflection_Removal_Using_2015_CVPR_paper", "label": "addresses", "title": "addresses", "to": "reflection removal", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "reflection removal", "label": "occurs_on", "title": "occurs_on", "to": "synthetic inputs", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "reflection removal", "label": "occurs_on", "title": "occurs_on", "to": "real-world inputs", "width": 3.79}, {"arrows": "to", "color": "#CC7700", "from": "layer separation", "label": "is", "title": "is", "to": "ill-posed problem", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "ghosting cues", "label": "exploits", "title": "exploits", "to": "asymmetry", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "ghosting cues", "label": "is", "title": "is", "to": "barely perceptible", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "ghosting cues", "label": "arises from", "title": "arises from", "to": "shifted double reflections", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "ghosted reflection", "label": "modeled using", "title": "modeled using", "to": "double-impulse convolution kernel", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "ghosted reflection components", "label": "exhibits", "title": "exhibits", "to": "relative attenuation", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Reflection Removal", "label": "applied_to", "title": "applied_to", "to": "synthetic inputs", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Reflection Removal", "label": "applied_to", "title": "applied_to", "to": "real-world inputs", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Soonmin Hwang", "label": "author_of", "title": "author_of", "to": "Multispectral Pedestrian Detection", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Soonmin Hwang", "label": "is", "title": "is", "to": "author", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Multispectral Pedestrian Detection", "label": "is", "title": "is", "to": "Benchmark Dataset", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "pedestrian datasets", "label": "focus_on", "title": "focus_on", "to": "color channel", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "thermal channel", "label": "is_helpful_for", "title": "is_helpful_for", "to": "detection", "width": 3.88}, {"arrows": "to", "color": "#00CC77", "from": "multispectral pedestrian dataset", "label": "addresses", "title": "addresses", "to": "limitation", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "multispectral pedestrian dataset", "label": "provides", "title": "provides", "to": "color-thermal image pairs", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "multispectral pedestrian dataset", "label": "is_as_large_as", "title": "is_as_large_as", "to": "previous color-based datasets", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "multispectral pedestrian dataset", "label": "provides", "title": "provides", "to": "dense annotations", "width": 3.82}, {"arrows": "to", "color": "#CC7700", "from": "color-thermal image pairs", "label": "is_a", "title": "is_a", "to": "image type", "width": 3.4000000000000004}, {"arrows": "to", "color": "#CC7700", "from": "multispectral ACF", "label": "is_extension_of", "title": "is_extension_of", "to": "aggregated channel features (ACF)", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "multispectral ACF", "label": "handles", "title": "handles", "to": "color-thermal image pairs", "width": 3.8499999999999996}, {"arrows": "to", "color": "#00CC77", "from": "multispectral ACF", "label": "reduces", "title": "reduces", "to": "average miss rate of ACF", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "spectral ACF", "label": "is_extension_of", "title": "is_extension_of", "to": "aggregated channel features", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "spectral ACF", "label": "handles", "title": "handles", "to": "color-thermal image pairs", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "aggregated channel features", "label": "is_a", "title": "is_a", "to": "image feature", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Multispectral ACF", "label": "reduces", "title": "reduces", "to": "average miss rate", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Multispectral ACF", "label": "reduces_by", "title": "reduces_by", "to": "15%", "width": 3.9699999999999998}, {"arrows": "to", "color": "#00CC77", "from": "Multispectral ACF", "label": "improves", "title": "improves", "to": "pedestrian detection", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Multispectral ACF", "label": "reduces", "title": "reduces", "to": "miss rate", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Multispectral ACF", "label": "handles", "title": "handles", "to": "color-thermal image pairs", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Multispectral ACF", "label": "achieves", "title": "achieves", "to": "breakthrough in pedestrian detection", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "RGBD-Fusion", "label": "is", "title": "is", "to": "real-time", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "RGBD-Fusion", "label": "provides", "title": "provides", "to": "high precision", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "Jaesik Park", "label": "is", "title": "is", "to": "author", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Namil Kim", "label": "is", "title": "is", "to": "author", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Roy Or", "label": "is", "title": "is", "to": "author", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Roy Or", "label": "affiliated with", "title": "affiliated with", "to": "Technion, Israel Institute of Technology", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "RGB-D scanners", "label": "has_limitation", "title": "has_limitation", "to": "subtle details", "width": 3.7600000000000002}, {"arrows": "to", "color": "#CC7700", "from": "RGB-D scanners", "label": "utilizes", "title": "utilizes", "to": "Depth map enhancement", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "lighting model", "label": "handles", "title": "handles", "to": "natural scene illumination", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "lighting model", "label": "integrated_into", "title": "integrated_into", "to": "shape from shading-like technique", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "detailed geometry", "label": "calculated_via", "title": "calculated_via", "to": "method", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "evidence", "label": "supports", "title": "supports", "to": "improvement in depth", "width": 3.61}, {"arrows": "to", "color": "#CC7700", "from": "Depth map enhancement", "label": "relies_on", "title": "relies_on", "to": "Shape from shading", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Depth map enhancement", "label": "improves", "title": "improves", "to": "depth", "width": 3.46}, {"arrows": "to", "color": "#CC7700", "from": "Shape from shading", "label": "influenced_by", "title": "influenced_by", "to": "Lighting models", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "Shape from shading", "label": "studied_in", "title": "studied_in", "to": "Computer Vision, Graphics, and Image Processing", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "Shape from shading", "label": "is_a", "title": "is_a", "to": "computer vision technique", "width": 3.7600000000000002}, {"arrows": "to", "color": "#CC7700", "from": "Real-time processing", "label": "requires", "title": "requires", "to": "Depth map enhancement", "width": 3.25}, {"arrows": "to", "color": "#0077CC", "from": "Lambertian reflectance", "label": "described_in", "title": "described_in", "to": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Bayesian nonparametric intrinsic image decomposition", "label": "presented_in", "title": "presented_in", "to": "European Conference on Computer Vision", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Variable-source shading analysis", "label": "published_in", "title": "published_in", "to": "International Journal of Computer Vision", "width": 3.79}, {"arrows": "to", "color": "#CC7700", "from": "Variable-source shading analysis", "label": "is_a", "title": "is_a", "to": "computer vision technique", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Grosse, R.", "label": "authored", "title": "authored", "to": "Ground-truth dataset", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Ground-truth dataset", "label": "used_for", "title": "used_for", "to": "intrinsic image algorithms", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Han, Y.", "label": "authored", "title": "authored", "to": "High quality shape", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "High quality shape", "label": "derived_from", "title": "derived_from", "to": "RGB-D image", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Horn, B. K.", "label": "authored", "title": "authored", "to": "PhD thesis", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Horn, B. K.", "label": "coauthored", "title": "coauthored", "to": "The variational approach", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Horn \u0026 Brooks", "label": "authored", "title": "authored", "to": "The variational approach to shape from shading", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Horn \u0026 Brooks", "label": "published in", "title": "published in", "to": "Computer Vision, Graphics, and Image Processing", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Johnson \u0026 Adelison", "label": "presented at", "title": "presented at", "to": "IEEE Conference on Computer Vision and Pattern Recognition", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Guy Rosman", "label": "affiliated with", "title": "affiliated with", "to": "Computer Science and Artificial Intelligence Lab, MIT", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Aaron Wetzler", "label": "affiliated with", "title": "affiliated with", "to": "Technion, Israel Institute of Technology", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Ron Kimmel", "label": "affiliated with", "title": "affiliated with", "to": "Technion, Israel Institute of Technology", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Alfred M. Bruckstein", "label": "affiliated with", "title": "affiliated with", "to": "Technion, Israel Institute of Technology", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Xiangyu Zhu", "label": "is_author_of", "title": "is_author_of", "to": "High-Fidelity Pose and Expression Normalization for Face Recognition in the Wild", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Xiangyu Zhu", "label": "affiliated_with", "title": "affiliated_with", "to": "Center for Biometrics and Security Research", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Xiangyu Zhu", "label": "affiliated_with", "title": "affiliated_with", "to": "National Laboratory of Pattern Recognition", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "High-Fidelity Pose and Expression Normalization for Face Recognition in the Wild", "label": "is_author_of", "title": "is_author_of", "to": "Stan Z. Li", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "High-Fidelity Pose and Expression Normalization for Face Recognition in the Wild", "label": "is_paper", "title": "is_paper", "to": "cvpr_papers", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Junnie Yan", "label": "is_author_of", "title": "is_author_of", "to": "High-Fidelity Pose and Expression Normalization for Face Recognition in the Wild", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Junjie Yan", "label": "is_author_of", "title": "is_author_of", "to": "High-Fidelity Pose and Expression Normalization for Face Recognition in the Wild", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Junjie Yan", "label": "affiliated_with", "title": "affiliated_with", "to": "Center for Biomatrics and Security Research", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Junjie Yan", "label": "affiliated_with", "title": "affiliated_with", "to": "National Laboratory of Pattern Recognition", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Junjie Yan", "label": "affiliated_with", "title": "affiliated_with", "to": "Center for Biometrics and Security Research", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Junjie Yan", "label": "email", "title": "email", "to": "jjyan@nlpr.ia.ac.cn", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Dong Yi", "label": "is_author_of", "title": "is_author_of", "to": "High-Fidelity Pose and Expression Normalization for Face Recognition in the Wild", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Dong Yi", "label": "affiliated_with", "title": "affiliated_with", "to": "Center for Biometrics and Security Research", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Dong Yi", "label": "affiliated_with", "title": "affiliated_with", "to": "National Laboratory of Pattern Recognition", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "freddy@cs.technion.ac.il", "label": "affiliated_with", "title": "affiliated_with", "to": "Technion - Israel Institute of Technology", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Zhu_High-Fidelity_Pose_and_2015_CVPR_paper.pdf", "label": "is_file_of", "title": "is_file_of", "to": "cvpr_papers", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "face recognition performance", "label": "impacted_by", "title": "impacted_by", "to": "pose variations", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "HPEN method", "label": "addresses", "title": "addresses", "to": "challenge", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "HPEN method", "label": "utilizes", "title": "utilizes", "to": "3D Morphable Model", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "HPEN method", "label": "generates", "title": "generates", "to": "face images", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "HPEN method", "label": "involves", "title": "involves", "to": "landmark marching", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "HPEN method", "label": "involves", "title": "involves", "to": "3DMM fitting", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "HPEN method", "label": "involves", "title": "involves", "to": "3D meshing", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "HPEN method", "label": "involves", "title": "involves", "to": "Poisson Editing", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "face images", "label": "has_pose", "title": "has_pose", "to": "frontal pose", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "face images", "label": "has_expression", "title": "has_expression", "to": "neutral expression", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Poisson Editing", "label": "used_for", "title": "used_for", "to": "inpainting", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Multi-PIE", "label": "is_used_in", "title": "is_used_in", "to": "experiments", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Multi-PIE", "label": "is_a", "title": "is_a", "to": "dataset", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Amberg, B.", "label": "authored", "title": "authored", "to": "Optimal step non-rigid icp algorithms for surface registration", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Romdhani, S.", "label": "authored", "title": "authored", "to": "Optimal step non-rigid icp algorithms for surface registration", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Vetter, T.", "label": "authored", "title": "authored", "to": "Optimal step non-rigid icp algorithms for surface registration", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Chai, X.", "label": "authored", "title": "authored", "to": "Locally linear regression for pose-invariant face recognition", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Shan, S.", "label": "authored", "title": "authored", "to": "Locally linear regression for pose-invariant face recognition", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Gao, W.", "label": "authored", "title": "authored", "to": "Locably linear regression for pose-invariant face recognition", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Arashloo, S. R.", "label": "authored", "title": "authored", "to": "Pose-invariant face matching using MRF energy minimization framework", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Kittler, J.", "label": "authored", "title": "authored", "to": "Pose-invariant face matching using MRF energy minimization framework", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Chan, C. H.", "label": "authored", "title": "authored", "to": "Multisculse local phase quantization for robust component-based face recognition", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Tahir, M. A.", "label": "authored", "title": "authored", "to": "Multisculse local phase quantization for robust component-based face recognition", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "local phase quantization", "label": "is_used_for", "title": "is_used_for", "to": "robust component-based face recognition", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "component-based face recognition", "label": "uses", "title": "uses", "to": "kernel fusion", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "kernel fusion", "label": "combines", "title": "combines", "to": "multiple descriptors", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Chen, D. (2012)", "label": "authored", "title": "authored", "to": "Bayesian face revisited", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "high-dimensional feature", "label": "benefits", "title": "benefits", "to": "efficient compression", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Asthana, A. (2013)", "label": "authored", "title": "authored", "to": "discriminative response map fitting", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "discriminative response map fitting", "label": "uses", "title": "uses", "to": "constrained local models", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "2013 IEEE Conference on Computer Vision and Pattern Recognition", "label": "hosts", "title": "hosts", "to": "high-dimensional feature compression", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Cheng, S.", "label": "authored", "title": "authored", "to": "Robust discriminative response map fitting", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Robust discriminative response map fitting", "label": "utilizes", "title": "utilizes", "to": "constrained local models", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Barkan, O.", "label": "authored", "title": "authored", "to": "Fast high dimensional vector multiplication face recognition", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Weill, J.", "label": "authored", "title": "authored", "to": "Fast high dimensional vector multiplication face recognition", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Wolf, L.", "label": "authored", "title": "authored", "to": "Fast high dimensional vector multiplication face recognition", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Aronowitz, H.", "label": "authored", "title": "authored", "to": "Fast high dimensional vector multiplication face recognition", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Parsing Occluded People", "label": "is_paper", "title": "is_paper", "to": "CVPR paper", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Parsing Occluded People", "label": "located_in", "title": "located_in", "to": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Xianjie Chen", "label": "is_author_of", "title": "is_author_of", "to": "Parsing Occluded People", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Alan Yuille", "label": "is_author_of", "title": "is_author_of", "to": "Parsing Occluded People", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "graphical model", "label": "has", "title": "has", "to": "tree structure", "width": 3.82}, {"arrows": "to", "color": "#CC7700", "from": "connected subtree", "label": "is", "title": "is", "to": "flexible composition", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "inference", "label": "requires", "title": "requires", "to": "search over models", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "inference", "label": "exploits", "title": "exploits", "to": "part sharing", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "computations", "label": "is", "title": "is", "to": "twice as many", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "searching", "label": "searches_for", "title": "searches_for", "to": "entire object", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Stickmen dataset", "label": "is", "title": "is", "to": "standard benchmarked dataset", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "alternative algorithms", "label": "are", "title": "are", "to": "best", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "modeling", "label": "avoids", "title": "avoids", "to": "occlusion", "width": 3.88}, {"arrows": "to", "color": "#CC7700", "from": "Graphical models", "label": "related_to", "title": "related_to", "to": "object detection", "width": 3.25}, {"arrows": "to", "color": "#0077CC", "from": "Yuilie, A.", "label": "co_author_of", "title": "co_author_of", "to": "Chen, X.", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Ferrari, V.", "label": "co_author_of", "title": "co_author_of", "to": "Marin-Jimenez, M.", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "human pose estimation", "label": "uses", "title": "uses", "to": "progressive search space reduction", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "support-vector networks", "label": "is_method_in", "title": "is_method_in", "to": "machine learning", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Cortes", "label": "authored", "title": "authored", "to": "Support-vector networks", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Dalal", "label": "authored", "title": "authored", "to": "Histograms of oriented gradients", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Triggs", "label": "authored", "title": "authored", "to": "Histograms of oriented gradients", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Sapp", "label": "authored", "title": "authored", "to": "Adaptive pose priors", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Jordan", "label": "authored", "title": "authored", "to": "Adaptive pose priors", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Taskar", "label": "authored", "title": "authored", "to": "Adaptive pose priors", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Yuille", "label": "affiliated_with", "title": "affiliated_with", "to": "University of California, Los Angeles", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Alabort-i-Medina", "label": "authored", "title": "authored", "to": "Unifying Holistic and Parts-Based Deformable Model Fitting", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "deformable models", "label": "captures", "title": "captures", "to": "degrees of freedom", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "Face Alignment", "label": "utilizes", "title": "utilizes", "to": "Holistic Deformable Models", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "Face Alignment", "label": "utilizes", "title": "utilizes", "to": "Parts-Based Deformable Models", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "Holistic Deformable Models", "label": "is_type_of", "title": "is_type_of", "to": "Deformable Models", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "Parts-Based Deformable Models", "label": "is_type_of", "title": "is_type_of", "to": "Deformable Models", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "Active Appearance Models", "label": "related_to", "title": "related_to", "to": "Face Alignment", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Active Appearance Models", "label": "described_in", "title": "described_in", "to": "T. F. Cootes, G. J. Edwards, and C. J. Taylor (2001)", "width": 3.7600000000000002}, {"arrows": "to", "color": "#CC7700", "from": "Active Shape Models", "label": "related_to", "title": "related_to", "to": "Face Alignment", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Active Shape Models", "label": "described_in", "title": "described_in", "to": "T. F. Cootes, C. J. Taylor, D. H. Cooper, and J. Graham (1995)", "width": 3.7600000000000002}, {"arrows": "to", "color": "#CC7700", "from": "Lucas-Kanade Method", "label": "related_to", "title": "related_to", "to": "Face Alignment", "width": 3.25}, {"arrows": "to", "color": "#0077CC", "from": "Bayesian Active Appearance Models", "label": "described_in", "title": "described_in", "to": "J. Alabort-i-Medina and S. Zafeiriou (2014)", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Conference on Computer Vision and Pattern Recognition (CVPR)", "label": "held_in", "title": "held_in", "to": "2014", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Lucas-Kanade", "label": "published_in", "title": "published_in", "to": "International Journal of Computer Vision (IJCR)", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "Lucas-Kanade", "label": "is_a", "title": "is_a", "to": "unifying framework", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "X. Cao", "label": "authored", "title": "authored", "to": "Face alignment by explicit shape regression", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Face alignment by explicit shape regression", "label": "presented_at", "title": "presented_at", "to": "Conference on Computer Vision and Pattern Recognition (CVPR)", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "G. Papandreou", "label": "authored", "title": "authored", "to": "Adaptive and constrained algorithms", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Adaptive and constrained algorithms", "label": "used_for", "title": "used_for", "to": "inverse compositional active appearance model fitting", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "A. Asthana", "label": "authored", "title": "authored", "to": "Robust discriminative response map fitting", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "A. Asthana", "label": "affiliated_with", "title": "affiliated_with", "to": "Conference on Computer Vision and Pattern Recognition (CVPR)", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "S. Zafeiriou", "label": "affiliated_with", "title": "affiliated_with", "to": "Conference on Computer Vision and Pattern Reduction (CVPR)", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "J. Sragih", "label": "affiliated_with", "title": "affiliated_with", "to": "Conference on Computer Vision and Pattern Recognition (CVPR)", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Joan Alabort-i-Medina", "label": "located_in", "title": "located_in", "to": "Imperial College London", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Joan Alabort-i-Medina", "label": "email", "title": "email", "to": "ja310@imperial.ac.uk", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Stefanos Zafeiriou", "label": "affiliated_with", "title": "affiliated_with", "to": "Department of Computing", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Stefanos Zafeiriou", "label": "affiliated_with", "title": "affiliated_with", "to": "Imperial College London", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Stefanos Zafeiriou", "label": "email", "title": "email", "to": "s.zafeiriou@imperial.ac.uk", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Jia Xu", "label": "author_of", "title": "author_of", "to": "Gaze-Enabled Egocentric Video Summarization", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Jia Xu", "label": "affiliated_with", "title": "affiliated_with", "to": "University of Wisconsin-Madison", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Lopamudra Mukherjee", "label": "author_of", "title": "author_of", "to": "Gaze-Enabled Egocentric Video Summarization", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Lopamudra Mukherjee", "label": "affiliated_with", "title": "affiliated_with", "to": "University of Wisconsin-Whitewater", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Yin Li", "label": "author_of", "title": "author_of", "to": "Gaze-Enabled Egocentric Video Summarization", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Yin Li", "label": "affiliated_with", "title": "affiliated_with", "to": "Georgia Institute of Technology", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Jamieson Warner", "label": "author_of", "title": "author_of", "to": "Gaze-Enabled Egocentric Video Summarization", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "James M. Rehg", "label": "author_of", "title": "author_of", "to": "Gaze-Enabled Egocentric Video Summarization", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "James M. Rehg", "label": "affiliated_with", "title": "affiliated_with", "to": "Georgia Institute of Technology", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Vikas Singh", "label": "author_of", "title": "author_of", "to": "Gaze-Enabled Egocentric Video Summarization", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "egocentric videos", "label": "necessitate", "title": "necessitate", "to": "compact representation", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "egocentric video summarization", "label": "presents", "title": "presents", "to": "unique challenges", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "gaze tracking information", "label": "improves", "title": "improves", "to": "summarization", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "gaze tracking information", "label": "enables", "title": "enables", "to": "frame comparison", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "summarization", "label": "requires", "title": "requires", "to": "personalized summaries", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "summarization model", "label": "is based on", "title": "is based on", "to": "submodular function maximization", "width": 3.91}, {"arrows": "to", "color": "#CC7700", "from": "Egocentric Video Summarization", "label": "uses", "title": "uses", "to": "Submodular Function Maximization", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Submodular Function Maximization", "label": "solved_via", "title": "solved_via", "to": "Multilinear Relaxation", "width": 3.7600000000000002}, {"arrows": "to", "color": "#CC7700", "from": "Personalized Summarization", "label": "is_a_type_of", "title": "is_a_type_of", "to": "Egocentric Video Summarization", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Almeida et al.", "label": "developed", "title": "developed", "to": "VISON", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "VISON", "label": "is_for", "title": "is_for", "to": "Online Applications", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Submodular Maximization", "label": "constrained_by", "title": "constrained_by", "to": "Partition Matroid", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Filmus \u0026 Ward", "label": "developed", "title": "developed", "to": "Combinatorial Algorithm", "width": 3.67}, {"arrows": "to", "color": "#0077CC", "from": "Fujishige", "label": "authored", "title": "authored", "to": "Submodular Functions and Optimization", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "Fujishige", "label": "authored", "title": "authored", "to": "Submodular functions and optimization", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "Gaze Tracking", "label": "enables", "title": "enables", "to": "Personalized Summarization", "width": 3.4000000000000004}, {"arrows": "to", "color": "#CC7700", "from": "Wearable Cameras", "label": "used_in", "title": "used_in", "to": "Egocentric Video Summarization", "width": 3.25}, {"arrows": "to", "color": "#0077CC", "from": "Ward", "label": "authored", "title": "authored", "to": "algorithm", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "Submodular functions and optimization", "label": "discusses", "title": "discusses", "to": "submodular maximization", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Iyer", "label": "authored", "title": "authored", "to": "optimization", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Krause", "label": "authored", "title": "authored", "to": "information gathering", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Xu", "label": "affiliated_with", "title": "affiliated_with", "to": "University of Wisconsin-Madison", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Andreas Geiger", "label": "author_of", "title": "author_of", "to": "Object Scene Flow for Autonomous Vehicles", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Andreas Geiger", "label": "affiliated with", "title": "affiliated with", "to": "MPI Tubingen", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Object Scene Flow for Autonomous Vehicles", "label": "presented_at", "title": "presented_at", "to": "CVPR", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Object Scene Flow for Autonomous Vehicles", "label": "supplemental_material_location", "title": "supplemental_material_location", "to": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Menze_Object_Scene_Flow_2015_CVPR_supplemental.pdf", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Moritz Menze", "label": "author_of", "title": "author_of", "to": "Object Scene Flow for Autonomous Vehicles", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Menze_Object_Scene_Flow_2015_CVPR_supplemental", "label": "describes", "title": "describes", "to": "Object Scene Flow for Autonomous Vehicles", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "supplementary document", "label": "provides", "title": "provides", "to": "additional descriptions", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "supplementary document", "label": "provides", "title": "provides", "to": "visualizations", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "supplementary document", "label": "provides", "title": "provides", "to": "experiments", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "scene flow ground truth", "label": "generated_from", "title": "generated_from", "to": "3D CAD models", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "model parameters", "label": "contributes_to", "title": "contributes_to", "to": "model sensitivity", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "KITTI stereo", "label": "is_benchmark_for", "title": "is_benchmark_for", "to": "optical flow", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "KITTI stereo", "label": "is_benchmark_for", "title": "is_benchmark_for", "to": "stereo", "width": 3.64}, {"arrows": "to", "color": "#CC7700", "from": "scene flow dataset", "label": "is", "title": "is", "to": "novel", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "sphere sequence", "label": "provides", "title": "provides", "to": "qualitative results", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "Optical Flow", "label": "is_used_in", "title": "is_used_in", "to": "Scene Flow", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Brox, T. \u0026 Malik, J.", "label": "authored", "title": "authored", "to": "Large Displacement Optical Flow", "width": 3.9699999999999998}, {"arrows": "to", "color": "#CC7700", "from": "Large Displacement Optical Flow", "label": "is_method_of", "title": "is_method_of", "to": "Variational Motion Estimation", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Hirschmueller, H.", "label": "authored", "title": "authored", "to": "Stereo Processing", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Scene Flow Datasets", "label": "provides", "title": "provides", "to": "Quantitative Results", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Scene Flow Datasets", "label": "provides", "title": "provides", "to": "Qualitative Results", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Stereo processing", "label": "uses", "title": "uses", "to": "semiglobal matching", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Stereo processing", "label": "uses", "title": "uses", "to": "mutual information", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Scene flow estimation", "label": "method", "title": "method", "to": "growing correspondence seeds", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Scene flow estimation", "label": "approach", "title": "approach", "to": "piecewise rigid scene flow", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Scene flow estimation", "label": "method", "title": "method", "to": "variational method", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Cech et al. (2011)", "label": "addresses", "title": "addresses", "to": "Scene flow estimation", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Vogel et al. (2013)", "label": "addresses", "title": "addresses", "to": "Scene flow estimation", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Huguet \u0026 Devernay (2007)", "label": "addresses", "title": "addresses", "to": "Scene flow estimation", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Huguet \u0026 Devernay (2007)", "label": "is_methodology_reference", "title": "is_methodology_reference", "to": "scene flow estimation", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Huguet \u0026 Devernay (2007)", "label": "presented_at", "title": "presented_at", "to": "ICVV", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "Semiglobal matching", "label": "is_technique_for", "title": "is_technique_for", "to": "Stereo processing", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "Mutual information", "label": "is_technique_for", "title": "is_technique_for", "to": "Stereo processing", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "scene flow estimation", "label": "uses", "title": "uses", "to": "stereo sequences", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Sun, Roth, \u0026 Black (2013)", "label": "provides_analysis", "title": "provides_analysis", "to": "optical flow estimation", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "optical flow estimation", "label": "relates_to", "title": "relates_to", "to": "scene flow estimation", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Hornacek, Fitzgibbon, \u0026 Rother (2014)", "label": "introduces", "title": "introduces", "to": "SphereFlow", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Hornacek, Fitzgibbon, \u0026 Rother (2014)", "label": "presented_at", "title": "presented_at", "to": "CVPR", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "SphereFlow", "label": "estimates", "title": "estimates", "to": "scene flow", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Valgaerts et al. (2010)", "label": "addresses", "title": "addresses", "to": "motion and geometry estimation", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "stereo sequences", "label": "provides_data_for", "title": "provides_data_for", "to": "motion estimation", "width": 3.46}, {"arrows": "to", "color": "#0077CC", "from": "Kert et al. (2010)", "label": "references", "title": "references", "to": "motion and geometry estimation", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Wedel et al. (2008)", "label": "addresses", "title": "addresses", "to": "scene flow computation", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Geiger et al. (2011)", "label": "focuses on", "title": "focuses on", "to": "3D reconstruction techniques", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Menz", "label": "affiliated with", "title": "affiliated with", "to": "Leibniz Universit\u00a8at Hannover", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Tal Hassner", "label": "author of", "title": "author of", "to": "Effective Face Frontalization", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Shai Harel", "label": "author of", "title": "author of", "to": "Effective Face Frontalization", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Eran Paz", "label": "author of", "title": "author of", "to": "Effective Face Frontalization", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Roee Enbar", "label": "author of", "title": "author of", "to": "Effective Face Frontalization", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Hassner_Effective_Face_Frontalization_2015_CVPR_paper.pdf", "label": "contains", "title": "contains", "to": "Effective Face Frontalization", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "face recognition systems", "label": "operates_in", "title": "operates_in", "to": "unconstrained images", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "unconstrained images", "label": "exhibits", "title": "exhibits", "to": "varying poses", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "unconstrained images", "label": "exhibits", "title": "exhibits", "to": "varying expressions", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "unconstrained images", "label": "exhibits", "title": "exhibits", "to": "varying lighting", "width": 3.8499999999999996}, {"arrows": "to", "color": "#00CC77", "from": "Fronalization", "label": "solves", "title": "solves", "to": "problem of varying poses", "width": 3.7600000000000002}, {"arrows": "to", "color": "#00CC77", "from": "Fronalization", "label": "solves", "title": "solves", "to": "problem of varying lighting", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "previous methods", "label": "relies_on", "title": "relies_on", "to": "estimating 3D facial shapes", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "this paper", "label": "explores", "title": "explores", "to": "simpler approach", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "this paper", "label": "proposes leveraging", "title": "proposes leveraging", "to": "visual common sense", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "this paper", "label": "proposes", "title": "proposes", "to": "novel method", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "simpler approach", "label": "uses", "title": "uses", "to": "single 3D surface", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "frontal views", "label": "is", "title": "is", "to": "aesthetically pleasing", "width": 3.64}, {"arrows": "to", "color": "#00CC77", "from": "frontal views", "label": "improves", "title": "improves", "to": "face recognition", "width": 3.7}, {"arrows": "to", "color": "#00CC77", "from": "frontal views", "label": "improves", "title": "improves", "to": "gender estimation", "width": 3.67}, {"arrows": "to", "color": "#0077CC", "from": "Face frontalization", "label": "produces", "title": "produces", "to": "aesthetically pleasing frontal views", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "AI systems", "label": "excels at", "title": "excels at", "to": "factual question answering", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "AI systems", "label": "struggles with", "title": "struggles with", "to": "common sense reasoning", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "visual common sense", "label": "is", "title": "is", "to": "semantic knowledge", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Xiao Lin", "label": "affiliated_with", "title": "affiliated_with", "to": "Virginia Tech", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Xiao Lin", "label": "has_email", "title": "has_email", "to": "linxiao@vt.edu", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "Visual Common Sense", "label": "is_a", "title": "is_a", "to": "AI Reasoning", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "Visual Paraphasing", "label": "is_a", "title": "is_a", "to": "AI Reasoning", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "Hsu_Robust_Image_Alignment_2015_CVPR_paper.pdf", "label": "is_a", "title": "is_a", "to": "CVPR paper", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Hsu_Robust_Image_Alignment_2015_CVPR_paper.pdf", "label": "located_in", "title": "located_in", "to": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "image alignment", "label": "is a", "title": "is a", "to": "process", "width": 3.25}, {"arrows": "to", "color": "#0077CC", "from": "process", "label": "involves", "title": "involves", "to": "data analysis", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "process", "label": "involves", "title": "involves", "to": "optimization", "width": 3.34}, {"arrows": "to", "color": "#0077CC", "from": "Data Analysis", "label": "is_topic_of", "title": "is_topic_of", "to": "Rock_Completing_3D_Object_2015_CVPR_paper.pdf", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "Rock_Completing_3D_Object_2015_CVPR_paper.pdf", "label": "requires", "title": "requires", "to": "Parameter Estimation", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Optimization", "label": "is_topic_of", "title": "is_topic_of", "to": "Rock_Completing_3D_Object_2015_CVPR_paper.pdf", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "System Dynamics", "label": "is_topic_of", "title": "is_topic_of", "to": "Rock_Completing_3D_Object_2015_CVPR_paper.pdf", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Parameter Estimation", "label": "is_topic_of", "title": "is_topic_of", "to": "Rock_Completing_3D_Object_2015_CVPR_paper.pdf", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Relationship Modeling", "label": "is_topic_of", "title": "is_topic_of", "to": "Rock_Compleeting_3D_Object_2015_CVPR_paper.pdf", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Jason Rock", "label": "is_author_of", "title": "is_author_of", "to": "Rock_Completing_3D_Object_2015_CVPR_paper.pdf", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Jason Rock", "label": "affiliation", "title": "affiliation", "to": "University of Illinois at Urbana-Champaign", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Justin Thorsten", "label": "is_author_of", "title": "is_author_of", "to": "Rock_Completing_3D_Object_2015_CVPR_paper.pdf", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "JunYoung Gwak", "label": "is_author_of", "title": "is_author_of", "to": "Rock_Completing_3D_Object_2015_CVPR_paper.pdf", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "JunYoung Gwak", "label": "affiliation", "title": "affiliation", "to": "University of Illinois at Urbana-Champaign", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Daeyun Shin", "label": "is_author_of", "title": "is_author_of", "to": "Rock_Completing_3D_Object_2015_CVPR_paper.pdf", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Daeyun Shin", "label": "affiliation", "title": "affiliation", "to": "University of Illinois at Urbana-Champaign", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "3D Shape Reconstruction", "label": "is a", "title": "is a", "to": "topic", "width": 3.9699999999999998}, {"arrows": "to", "color": "#CC7700", "from": "3D Shape Reconstruction", "label": "includes", "title": "includes", "to": "View-Based Matching", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "3D Shape Reconstruction", "label": "includes", "title": "includes", "to": "Shape Completion", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "3D Shape Reconstruction", "label": "includes", "title": "includes", "to": "3D Model Synthesis", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "3D Shape Reconstruction", "label": "includes", "title": "includes", "to": "Symmetry Transfer", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Tanmay Gupta", "label": "affiliation", "title": "affiliation", "to": "University of Illinois at Urbana-Champaign", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Liu_Data-Driven_Sparsity-Based_Restoration_2015_CVPR_paper.pdf", "label": "is_a", "title": "is_a", "to": "CVPR paper", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Rui Caseiro", "label": "authored", "title": "authored", "to": "Beyond the Shortest Path", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Rui Caseiro", "label": "affiliated_with", "title": "affiliated_with", "to": "Institute of Systems and Robotics - University of Coimbra", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Beyond the Shortest Path", "label": "presented_at", "title": "presented_at", "to": "CVPR", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "Beyond the Shortest Path", "label": "addresses", "title": "addresses", "to": "domain adaptation", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Beyond the Shortest Path", "label": "utilizes", "title": "utilizes", "to": "spline flow", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "Pedro Martins", "label": "authored", "title": "authored", "to": "Beyond the Shortest Path", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Pedro Martins", "label": "is_affiliated_with", "title": "is_affiliated_with", "to": "Institute of Systems and Robotics - University of Coimbra", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Jorge Batista", "label": "authored", "title": "authored", "to": "Beyond the Shortest Path", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Jorge Batista", "label": "affiliated_with", "title": "affiliated_with", "to": "Institute of Systems and Robotics - University of Coimbra", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Jorge Batista", "label": "is_affiliated_with", "title": "is_affiliated_with", "to": "Institute of Sistemas and Robotics - University of Coimbra", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "domain adaptation", "label": "aims_to", "title": "aims_to", "to": "improve_performance", "width": 3.0999999999999996}, {"arrows": "to", "color": "#CC7700", "from": "spline flow", "label": "is_a", "title": "is_a", "to": "method", "width": 3.25}, {"arrows": "to", "color": "#0077CC", "from": "domain adaptation paradigm", "label": "represents", "title": "represents", "to": "source and target domains", "width": 3.88}, {"arrows": "to", "color": "#CC7700", "from": "shortest path", "label": "is", "title": "is", "to": "geodesic curve", "width": 3.9699999999999998}, {"arrows": "to", "color": "#00CC77", "from": "shortest path", "label": "is insufficient", "title": "is insufficient", "to": "modeling complex domain shifts", "width": 3.7600000000000002}, {"arrows": "to", "color": "#00CC77", "from": "shortest path", "label": "restricts", "title": "restricts", "to": "use of multiple datasets", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "novel approach", "label": "utilizes", "title": "utilizes", "to": "spline curves", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "novel approach", "label": "allows for", "title": "allows for", "to": "integration of multiple source domains", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "novel approach", "label": "models", "title": "models", "to": "domain shifts", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "novel approach", "label": "demonstrates", "title": "demonstrates", "to": "improved performance", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "spline curves", "label": "computed via", "title": "computed via", "to": "rolling maps", "width": 3.88}, {"arrows": "to", "color": "#CC7700", "from": "Domain Adaptation", "label": "addresses", "title": "addresses", "to": "domain shifts", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Domain Adaptation", "label": "improves", "title": "improves", "to": "performance", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "Subspace Representation", "label": "used_in", "title": "used_in", "to": "Domain Adaptation", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Baktas et al. (2013)", "label": "proposes", "title": "proposes", "to": "Domain Invariant Projection", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Gopalan et al. (2013)", "label": "focuses_on", "title": "focuses_on", "to": "location recognition", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Gopalan et al. (2011)", "label": "presents", "title": "presents", "to": "Unsupervised approach", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Carreira et al. (2012)", "label": "explores", "title": "explores", "to": "Semantic segmentation", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Caseiro et al. (2010)", "label": "investigates", "title": "investigates", "to": "cast shadows", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "Gopalan et al. (2014)", "label": "uses", "title": "uses", "to": "intermediate data representations", "width": 3.61}, {"arrows": "to", "color": "#CC7700", "from": "Spline Flow", "label": "related_to", "title": "related_to", "to": "Domain Adaptation", "width": 3.25}, {"arrows": "to", "color": "#CC7700", "from": "Grasmannn Manifold", "label": "related_to", "title": "related_to", "to": "Domain Adaptation", "width": 3.0999999999999996}, {"arrows": "to", "color": "#CC7700", "from": "Rolling Maps", "label": "related_to", "title": "related_to", "to": "Domain Adaptation", "width": 2.95}, {"arrows": "to", "color": "#0077CC", "from": "Gopalan", "label": "authored", "title": "authored", "to": "Unsupervised adaptation across domain shifts by generating intermediate data representations", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Li", "label": "co-authored", "title": "co-authored", "to": "Unsupervised adaptation across domain shifts by generating intermediate data representations", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Griffin", "label": "developed", "title": "developed", "to": "Caltech-256 object category dataset", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Holub", "label": "co-developed", "title": "co-developed", "to": "Cal tech-256 object category dataset", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Caseiro", "label": "authored", "title": "authored", "to": "non-parametric riemannian framework", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Henriques", "label": "co-authored", "title": "co-authored", "to": "non-parametric riemannian framework", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Martins", "label": "co-authored", "title": "co-authored", "to": "non-parametric riemannian framework", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Batista", "label": "co-authored", "title": "co-authored", "to": "non-parametric riemannian framework", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Hays", "label": "authored", "title": "authored", "to": "Im2gps", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Efroos", "label": "co-authored", "title": "co-authored", "to": "Im2gps", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Pan", "label": "authored", "title": "authored", "to": "A survey on transfer learning", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Jo\u00e3o F. Henriques", "label": "affiliated_with", "title": "affiliated_with", "to": "Institute of Systems and Robotics - University of Coimbra", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Sparse Composite Quantization", "label": "is_publication", "title": "is_publication", "to": "CVPR paper", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Sparse Composite Quantization", "label": "published_in", "title": "published_in", "to": "2015", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "Sparse Composite Quantization", "label": "addresses", "title": "addresses", "to": "quantization techniques", "width": 3.25}, {"arrows": "to", "color": "#0077CC", "from": "Zhang_Sparse_Composite_Quantization_2015_CVPR_paper", "label": "is_file_name", "title": "is_file_name", "to": "PDF document", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "sparse composite quantization", "label": "is_approach_for", "title": "is_approach_for", "to": "approximate nearest neighbor search", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "sparse composite quantization", "label": "achieves", "title": "achieves", "to": "competitive search accuracy", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "sparse composite quantization", "label": "builds_on", "title": "builds_on", "to": "product quantization", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "sparse composite quantization", "label": "builds_on", "title": "builds_on", "to": "Cartesian k-means", "width": 3.64}, {"arrows": "to", "color": "#CC7700", "from": "sparse composite quantization", "label": "builds_on", "title": "builds_on", "to": "composite quantization", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "sparse composite quantization", "label": "constructs", "title": "constructs", "to": "sparse dictionaries", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "sparse composite quantization", "label": "reduces", "title": "reduces", "to": "distance table computation time", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Cartesian k-means", "label": "is_a", "title": "is_a", "to": "variation", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Cartesian k-means", "label": "introduced_in", "title": "introduced_in", "to": "CVPR (2013)", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "distance table computation", "label": "is_bottleneck_in", "title": "is_bottleneck_in", "to": "composite quantization", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "distance table computation", "label": "reduces", "title": "reduces", "to": "time", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "sparse dictionaries", "label": "accelerates", "title": "accelerates", "to": "distance evaluation", "width": 3.61}, {"arrows": "to", "color": "#0077CC", "from": "large-scale datasets", "label": "contains", "title": "contains", "to": "SIFTs", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "SIFTs", "label": "has_scale", "title": "has_scale", "to": "1M", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "SIFTs", "label": "scale", "title": "scale", "to": "1B", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "search times", "label": "are", "title": "are", "to": "faster", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "SIFTS", "label": "has_scale", "title": "has_scale", "to": "1B", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "ANN", "label": "is_method_for", "title": "is_method_for", "to": "nearest neighbor search", "width": 3.7600000000000002}, {"arrows": "to", "color": "#CC7700", "from": "Product Quantization", "label": "is_technique", "title": "is_technique", "to": "for nearest neighbor search", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "Babenko and Lempitsky (2012)", "label": "introduces", "title": "introduces", "to": "inverted multi-index", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Babenko and Lempitsky (2014)", "label": "improves", "title": "improves", "to": "bilayer product quantization", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Babenko and Lempitsky (2014)", "label": "targets", "title": "targets", "to": "billion-scale approximate nearest neighbors", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "bilayer product quantization", "label": "builds_upon", "title": "builds_upon", "to": "product quantization", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "bilayer product quantization", "label": "addresses", "title": "addresses", "to": "large-scale applications", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "High-Dimensional Data", "label": "requires", "title": "requires", "to": "approximate nearest neighbors", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "vocabulary trees", "label": "used_in", "title": "used_in", "to": "scalable recognition", "width": 3.61}, {"arrows": "to", "color": "#0077CC", "from": "Hamming embedding", "label": "utilized_for", "title": "utilized_for", "to": "large scale image search", "width": 3.67}, {"arrows": "to", "color": "#0077CC", "from": "semi-supervised hashing", "label": "applied_to", "title": "applied_to", "to": "large-scale search", "width": 3.73}, {"arrows": "to", "color": "#0077CC", "from": "Re-ranking strategies", "label": "addresses", "title": "addresses", "to": "large-scale search", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Graph-based search methods", "label": "focuses on", "title": "focuses on", "to": "search", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Angular quantization", "label": "introduces", "title": "introduces", "to": "binary codes", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Guo-Jun Qi", "label": "affiliated_with", "title": "affiliated_with", "to": "University of Central Florida", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Jinhui Tang", "label": "affiliated with", "title": "affiliated with", "to": "Unknown", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Jinhui Tang", "label": "affiliated_with", "title": "affiliated_with", "to": "Nanjing University of Science and Technology", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Ting Zhang", "label": "affiliated_with", "title": "affiliated_with", "to": "University of Science and Technology of China", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Jingding Wang", "label": "affiliated_with", "title": "affiliated_with", "to": "Microsoft Research", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Ioannis Gkioulekalas", "label": "author_of", "title": "author_of", "to": "On the Appearance of Translueceny Edges", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "On the Appearence of Translueceny Edges", "label": "presented_at", "title": "presented_at", "to": "CVPR", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Gkiooulekas_On_the_Appearance_2015_CVPR_paper.pdf", "label": "contains", "title": "contains", "to": "On the Appearence of Translueceny Edges", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Edges in images", "label": "differ from", "title": "differ from", "to": "Edges in opaque objects", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Edges", "label": "caused by", "title": "caused by", "to": "Discontinuity in surface orientation", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Authors", "label": "explain", "title": "explain", "to": "Edge patterns", "width": 3.8499999999999996}, {"arrows": "to", "color": "#00CC77", "from": "Edge patterns", "label": "result from", "title": "result from", "to": "Light Transport", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Simulations", "label": "utilize", "title": "utilize", "to": "Scattering parameters", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Visual Inference tasks", "label": "involve", "title": "involve", "to": "Shape estimation", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Visual Inference tasks", "label": "involve", "title": "involve", "to": "Material estimation", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Light Transport", "label": "addressed_in", "title": "addressed_in", "to": "Jensen et al. (2001)", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Wave Propagation", "label": "described_in", "title": "described_in", "to": "Ishimaru (1978)", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Stereo Reconstruction", "label": "relies_on", "title": "relies_on", "to": "Wave Propagation", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Human Perception", "label": "studied_in", "title": "studied_in", "to": "Adelson (2001)", "width": 3.91}, {"arrows": "to", "color": "#CC7700", "from": "Machine Vision", "label": "informed_by", "title": "informed_by", "to": "Human Perception", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Rendering", "label": "requires", "title": "requires", "to": "Light Transport", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Reconstruction", "label": "requires", "title": "requires", "to": "Light Transport", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Translucent Objects", "label": "involved_in", "title": "involved_in", "to": "Shape Estimation", "width": 3.25}, {"arrows": "to", "color": "#0077CC", "from": "Translucent Objects", "label": "involved_in", "title": "involved_in", "to": "Material Estimation", "width": 3.25}, {"arrows": "to", "color": "#CC7700", "from": "Material Metameters", "label": "related_to", "title": "related_to", "to": "Translucent Objects", "width": 3.0999999999999996}, {"arrows": "to", "color": "#0077CC", "from": "light transport", "label": "occurs_within", "title": "occurs_within", "to": "translucient materials", "width": 3.64}, {"arrows": "to", "color": "#CC7700", "from": "rendering", "label": "requires", "title": "requires", "to": "accurate light transport", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "translucent materials", "label": "exhibit", "title": "exhibit", "to": "translucent appearance", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "translucent materials", "label": "has_property", "title": "has_property", "to": "translucency", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "phase functions", "label": "important_for", "title": "important_for", "to": "translucent appearance", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "reconstruction", "label": "requires", "title": "requires", "to": "accurate rendering", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "phase function", "label": "affects", "title": "affects", "to": "translucent appearance", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "photon diffusion", "label": "is_rendering_technique_for", "title": "is_rendering_technique_for", "to": "translucent materials", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Open-surfaces catalog", "label": "is_dataset_of", "title": "is_dataset_of", "to": "surface appearances", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "surface appearances", "label": "used_for", "title": "used_for", "to": "training", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "surface appearances", "label": "used_for", "title": "used_for", "to": "evaluation", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "evaluation", "label": "uses", "title": "uses", "to": "hybrid multi-camera and marker-based capture dataset", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "ACM Transactions on Graphics", "label": "publishes", "title": "publishes", "to": "research on phase function", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "ACM Transactions on Graphics", "label": "publishes", "title": "publishes", "to": "Bala. Open-surfaces", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Journal of Vision", "label": "publishes", "title": "publishes", "to": "research on translucency", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "ACM SIGGRAPH", "label": "publishes", "title": "publishes", "to": "research on rendering techniques", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Bala. Open-surfaces", "label": "provides", "title": "provides", "to": "dataset of surface appearances", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "Gkiouslekas et al.", "label": "examines", "title": "examines", "to": "phase function in translucent appearance", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "materials in context database", "label": "relevant for", "title": "relevant for", "to": "translucent objects", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Mingkui Tan", "label": "author_of", "title": "author_of", "to": "Learning graph structure...", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Mingkui Tan", "label": "affiliated_with", "title": "affiliated_with", "to": "University of Adelaide", "width": 3.9699999999999998}, {"arrows": "to", "color": "#CC7700", "from": "Learning graph structure...", "label": "is_a", "title": "is_a", "to": "CVPR paper", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Learning graph structure...", "label": "file_name", "title": "file_name", "to": "Tan_Learning_Graph_Structure_2015_CVPR_paper.pdf", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Qinfeng Shi", "label": "author_of", "title": "author_of", "to": "Learning graph structure...", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Qinfeng Shi", "label": "affiliated_with", "title": "affiliated_with", "to": "University of Adelaide", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Fuyuan Hu", "label": "author_of", "title": "author_of", "to": "Learning graph structure...", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Zhen Zhang", "label": "author_of", "title": "author_of", "to": "Learning graph structure...", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Multi-label image classification", "label": "improves", "title": "improves", "to": "Classification performance", "width": 3.7600000000000002}, {"arrows": "to", "color": "#CC7700", "from": "Multi-label image classification", "label": "uses", "title": "uses", "to": "Probablistic Graphical Models", "width": 3.25}, {"arrows": "to", "color": "#0077CC", "from": "Probabilistic Graphical Models", "label": "represents", "title": "represents", "to": "Label dependency", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Graphical model structure", "label": "determined by", "title": "determined by", "to": "Heuristic methods", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Graphical model structure", "label": "learned from", "title": "learned from", "to": "Limited information", "width": 3.46}, {"arrows": "to", "color": "#0077CC", "from": "Problem", "label": "formulated into", "title": "formulated into", "to": "Max-margin framework", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Problem", "label": "transformed into", "title": "transformed into", "to": "Convex programming problem", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "Max-margin framework", "label": "used_in", "title": "used_in", "to": "learning", "width": 2.95}, {"arrows": "to", "color": "#0077CC", "from": "Procedure", "label": "activates", "title": "activates", "to": "Set of cliques", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "performance improvement", "label": "over", "title": "over", "to": "methods", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "Graph structure learning", "label": "related_to", "title": "related_to", "to": "Probablistic Graphical Models", "width": 3.0999999999999996}, {"arrows": "to", "color": "#CC7700", "from": "Clique generation", "label": "related_to", "title": "related_to", "to": "Graph structure learning", "width": 3.0999999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Boutell et al. (2004)", "label": "studied", "title": "studied", "to": "scene classification", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Bradley \u0026 Guestrin (2010)", "label": "studied", "title": "studied", "to": "tree conditional random fields", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Bucak et al. (2009)", "label": "studied", "title": "studied", "to": "multi-label ranking", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "R.", "label": "authored", "title": "authored", "to": "Efficient multi-label ranking", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Efficient multi-label ranking", "label": "presented_at", "title": "presented_at", "to": "IEEE Conference on Computer Vision and Pattern Recognition", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Jain, A. K.", "label": "co-authored", "title": "co-authored", "to": "Efficient multi-label ranking", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Cai, X.", "label": "authored", "title": "authored", "to": "Graph structured sparsity model", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Chow, C.", "label": "authored", "title": "authored", "to": "Approximating discrete probability distributions", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Approximating discrete probability distributions", "label": "published_in", "title": "published_in", "to": "IEEE Transactions on Information Theory", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Liu, C.", "label": "co-authored", "title": "co-authored", "to": "Approximating discrete probability distributions", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Dembczy\u0144ski, K.", "label": "authored", "title": "authored", "to": "Label dependence and loss minimization", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Label dependence and loss minimization", "label": "published_in", "title": "published_in", "to": "Machine Learning", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Waegeman, W.", "label": "co-authored", "title": "co-authored", "to": "Label dependence and loss minimization", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Waegeman, W.", "label": "co-authored", "title": "co-authored", "to": "Analysis of chaining", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Cheng, W.", "label": "co-authored", "title": "co-authored", "to": "Label dependence and loss minimization", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "H\u00fcllermeier, E.", "label": "co-authored", "title": "co-authored", "to": "Label dependence and loss minimization", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "H\u00fcllermeier, E.", "label": "co-authored", "title": "co-authored", "to": "Analysis of chaining", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Dembczynski, K.", "label": "authored", "title": "authored", "to": "Analysis of chaining", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Analysis of chaining", "label": "presented_at", "title": "presented_at", "to": "European Conference on Artificial Intelligence", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Dembczynski", "label": "authored", "title": "authored", "to": "analysis of chaining", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Dembczynski", "label": "affilates_with", "title": "affilates_with", "to": "European Conference on Artificial Intelligence", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Everingham", "label": "created", "title": "created", "to": "PASUAL Visual Object Classes Challenge 2012", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Bolei Zhou", "label": "authored", "title": "authored", "to": "ConceptLearner", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Bolei Zhou", "label": "affiliated_with", "title": "affiliated_with", "to": "MIT", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "ConceptLearner", "label": "is_a", "title": "is_a", "to": "CVPR paper", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "ConceptLearner", "label": "proposes", "title": "proposes", "to": "scalable approach", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "ConceptLearner", "label": "discovers", "title": "discovers", "to": "visual concepts", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "ConceptLearner", "label": "demonstrates", "title": "demonstrates", "to": "promising performance", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "ConceptLearner", "label": "compared to", "title": "compared to", "to": "fully supervised methods", "width": 3.46}, {"arrows": "to", "color": "#0077CC", "from": "ConceptLearner", "label": "compared to", "title": "compared to", "to": "weakly supervised methods", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "Vignesh Jagadeesh", "label": "authored", "title": "authored", "to": "ConceptLearner", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Vignesh Jagadeesh", "label": "affiliated_with", "title": "affiliated_with", "to": "eBay Research Labs", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Zhou_ConceptLearner_Discovering_Visual_2015_CVPR_paper.pdf", "label": "is_file_of", "title": "is_file_of", "to": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Charles Sturt University", "label": "has_author", "title": "has_author", "to": "Junbin Gao", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "computer vision recognition systems", "label": "requires", "title": "requires", "to": "visual knowledge", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "fully labeled data", "label": "is", "title": "is", "to": "expensive", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "visual concept detectors", "label": "are learned", "title": "are learned", "to": "automatically", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "visual concept detectors", "label": "applied to", "title": "applied to", "to": "image region-level detection", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "learned concepts", "label": "evaluated_for", "title": "evaluated_for", "to": "scene recognition", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "learned concepts", "label": "evaluated_for", "title": "evaluated_for", "to": "object detection", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "fully supervised methods", "label": "outperformed_by", "title": "outperformed_by", "to": "weakly supervised methods", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "weakly supervised methods", "label": "outperformed_by", "title": "outperformed_by", "to": "domain-specific supervision", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "automatic attribute discovery", "label": "characterized_from", "title": "characterized_from", "to": "noisy web data", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Im2text", "label": "describes", "title": "describes", "to": "images", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "Im2text", "label": "uses", "title": "uses", "to": "captioned photographs", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "object detectors", "label": "adapted_from", "title": "adapted_from", "to": "images", "width": 3.61}, {"arrows": "to", "color": "#0077CC", "from": "object detectors", "label": "adapted_to", "title": "adapted_to", "to": "video", "width": 3.61}, {"arrows": "to", "color": "#0077CC", "from": "Deng et al.", "label": "created", "title": "created", "to": "Imaginet", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Places database", "label": "used_for", "title": "used_for", "to": "scene recognition", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "Zhou et al.", "label": "utilized", "title": "utilized", "to": "Places database", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "Piction", "label": "labels", "title": "labels", "to": "human faces", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Srihari", "label": "developed", "title": "developed", "to": "Piction", "width": 3.73}, {"arrows": "to", "color": "#0077CC", "from": "Divvala et al.", "label": "developed", "title": "developed", "to": "webly-supervised visual concept learning", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Tang et al.", "label": "adapted", "title": "adapted", "to": "object detectors", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "AAAI", "label": "published_by", "title": "published_by", "to": "AAAI Press", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "AAAI", "label": "published_by", "title": "published_by", "to": "The MIT Press", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Divvala, S. K.", "label": "affiliated_with", "title": "affiliated_with", "to": "CVPR", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Robinson Piramuthu", "label": "affiliated_with", "title": "affiliated_with", "to": "eBay Research Labs", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Yumin Suh", "label": "author_of", "title": "author_of", "to": "Subgraph Matching using Compactness Prior", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Yumin Suh", "label": "affiliated_with", "title": "affiliated_with", "to": "Seoul National University", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Subgraph Matching using Compactness Prior", "label": "presented_at", "title": "presented_at", "to": "CVPR", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Kamil Adamczewski", "label": "author_of", "title": "author_of", "to": "Subgraph Matching using Compactness Prior", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Kamil Adamczewski", "label": "affiliated_with", "title": "affiliated_with", "to": "Seoul National University", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Suh_Subgraph_Matching_Using_2015_CVPR_paper.pdf", "label": "contains", "title": "contains", "to": "Subgraph Matching using Compactness Prior", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Feature correspondence", "label": "plays_role_in", "title": "plays_role_in", "to": "computer vision applications", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Feature correspondence", "label": "presented_in", "title": "presented_in", "to": "2009 IEEE 12th International Conference on Computer Vision", "width": 3.79}, {"arrows": "to", "color": "#CC7700", "from": "Graph matching", "label": "is_formulated_as", "title": "is_formulated_as", "to": "problem", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Graph matching algorithms", "label": "rarely_considers", "title": "rarely_considers", "to": "precision", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Solutions", "label": "have", "title": "have", "to": "outliers", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Subgraph matching formulation", "label": "uses", "title": "uses", "to": "compactness prior", "width": 3.82}, {"arrows": "to", "color": "#CC7700", "from": "compactness prior", "label": "prefers", "title": "prefers", "to": "sparsity", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "compactness prior", "label": "eliminates", "title": "eliminates", "to": "outliers", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Meta-algorithm", "label": "based_on", "title": "based_on", "to": "Markov chain Monte Carlo", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Formulation and algorithm", "label": "improve", "title": "improve", "to": "baseline performance", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Cho, M.", "label": "authored", "title": "authored", "to": "Learning graphs to match", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Cho, M.", "label": "authored", "title": "authored", "to": "Feature correspondence", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Cho, M.", "label": "authored", "title": "authored", "to": "Reweighted random walks", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Cho, M.", "label": "author_of", "title": "author_of", "to": "max-pooling strategy", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Reweighted random walks", "label": "presented_in", "title": "presented_in", "to": "Computer Vision\u2013ECCV 2010", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "Progressive graph matching", "label": "presented_in", "title": "presented_in", "to": "Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on", "width": 3.73}, {"arrows": "to", "color": "#0077CC", "from": "Alahari, K.", "label": "co-authored", "title": "co-authored", "to": "Learning graphs to match", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Ponce, J.", "label": "co-authored", "title": "co-authored", "to": "Learning graphs to match", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Lee, Kyoung Mu", "label": "affiliation", "title": "affiliation", "to": "Seoul National University", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Lee, Kyoung Mu", "label": "author_of", "title": "author_of", "to": "progressive graph matching", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Adamczewski, Kamil", "label": "affiliation", "title": "affiliation", "to": "Seoul National University", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Suh, Yumin", "label": "affiliation", "title": "affiliation", "to": "Seoul National University", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Duchenne, O.", "label": "author_of", "title": "author_of", "to": "tensor-based algorithm", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Duchenne, O.", "label": "affiliation", "title": "affiliation", "to": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Gilks, W. R.", "label": "author_of", "title": "author_of", "to": "Markov chain monte carlo", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Cour, T.", "label": "author_of", "title": "author_of", "to": "balanced graph matching", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "progressive graph matching", "label": "presented_at", "title": "presented_at", "to": "IEEE Conference on Computer Vision and Pattern Recognition", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Guancong Zhang", "label": "author_of", "title": "author_of", "to": "Good Features to Track for Visual SLAM", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Guancong Zhang", "label": "contributed_to", "title": "contributed_to", "to": "Zhang_Good_Features_to_2015_CVPR_paper.pdf", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Good Features to Track for Visual SLAM", "label": "is_publication_of", "title": "is_publication_of", "to": "CVPR", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Good Features to Track for Visual SLAM", "label": "published_in_year", "title": "published_in_year", "to": "2015", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "Patricio A. Vela", "label": "author_of", "title": "author_of", "to": "Good Features to Track for Visual SLAM", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Patricio A. Vela", "label": "contributed_to", "title": "contributed_to", "to": "Zhang_Good_Features_to_2015_CVPR_paper.pdf", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Patricio A. Vela", "label": "email", "title": "email", "to": "pvela@gatech.edu", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Zhang_Good_Features_to_2015_CVPR_paper.pdf", "label": "is_file_of", "title": "is_file_of", "to": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "measured features", "label": "contribute_to", "title": "contribute_to", "to": "accurate localization", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "method for selecting features", "label": "is_useful_for", "title": "is_useful_for", "to": "localization", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "method for selecting features", "label": "is_derived_from", "title": "is_derived_from", "to": "observability of SLAM", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "method for selecting features", "label": "integrates_into", "title": "integrates_into", "to": "existing SLAM systems", "width": 3.82}, {"arrows": "to", "color": "#00CC77", "from": "method for selecting features", "label": "improves", "title": "improves", "to": "localization accuracy", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "estimation utility", "label": "is_formulated_with", "title": "is_formulated_with", "to": "observability indices", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "observability indices", "label": "are_computed_using", "title": "are_computed_using", "to": "incremental singular value decomposition (SVD)", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "observability indices", "label": "calculated_using", "title": "calculated_using", "to": "greedy selection", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "observability indices", "label": "described_by", "title": "described_by", "to": "incremental singular value decomposition", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "greedy selection", "label": "is_approximately", "title": "is_approximately", "to": "submodular", "width": 3.7600000000000002}, {"arrows": "to", "color": "#CC7700", "from": "greedy selection", "label": "is", "title": "is", "to": "near-optimal", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "SLAM", "label": "related_to", "title": "related_to", "to": "SfM", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "incremental singular value decomposition", "label": "used_for", "title": "used_for", "to": "temporal observability indices", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "synthetic experiments", "label": "demonstrate", "title": "demonstrate", "to": "improved localization accuracy", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "SLAM experiments", "label": "demonstrate", "title": "demonstrate", "to": "improved data association", "width": 3.79}, {"arrows": "to", "color": "#CC7700", "from": "Visual SLAM", "label": "relies_on", "title": "relies_on", "to": "Data Association", "width": 3.7600000000000002}, {"arrows": "to", "color": "#CC7700", "from": "Data Association", "label": "addressed_by", "title": "addressed_by", "to": "Incremental SVD", "width": 3.64}, {"arrows": "to", "color": "#00CC77", "from": "Observability Analysis", "label": "impacts", "title": "impacts", "to": "map_building", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "MonoSLAM", "label": "implements", "title": "implements", "to": "real-time SLAM", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "LSD-SLAM", "label": "uses", "title": "uses", "to": "direct monocular SLAM", "width": 3.61}, {"arrows": "to", "color": "#CC7700", "from": "LSD-SLAM", "label": "is_a", "title": "is_a", "to": "Slam_Algorithm", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Covariance recovery", "label": "solves", "title": "solves", "to": "data association", "width": 3.79}, {"arrows": "to", "color": "#00CC77", "from": "Feature Selection", "label": "influences", "title": "influences", "to": "localization accuracy", "width": 3.46}, {"arrows": "to", "color": "#0077CC", "from": "Andrade-Cetto and Sanfeliu", "label": "analyzed", "title": "analyzed", "to": "partial observability", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Kaess and Dellaert", "label": "proposed", "title": "proposed", "to": "covariance recovery", "width": 3.73}, {"arrows": "to", "color": "#0077CC", "from": "Davison et al.", "label": "developed", "title": "developed", "to": "MonoSLAM", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Engel et al.", "label": "created", "title": "created", "to": "LSD-SLAM", "width": 3.82}, {"arrows": "to", "color": "#CC7700", "from": "Slam_Algorithm", "label": "is_field_of", "title": "is_field_of", "to": "Machine Intelligence", "width": 3.7600000000000002}, {"arrows": "to", "color": "#CC7700", "from": "iSAM2", "label": "is_algorithm", "title": "is_algorithm", "to": "Bayes Tree", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "iSAM2", "label": "uses", "title": "uses", "to": "Incremental Smoothing", "width": 3.88}, {"arrows": "to", "color": "#CC7700", "from": "iSAM2", "label": "is_variant_of", "title": "is_variant_of", "to": "Slam", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Active search", "label": "is_topic_of", "title": "is_topic_of", "to": "IEEE International Conference on Computer Vision", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "Live dense reconstruction", "label": "uses", "title": "uses", "to": "Single moving camera", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Zheng Ma", "label": "affiliated_with", "title": "affiliated_with", "to": "School of ECR", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Zheng Ma", "label": "email", "title": "email", "to": "zhanggc@gatech.edu", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Zheng Ma", "label": "affiliated_with", "title": "affiliated_with", "to": "City University of Hong Kong", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Lei Yu", "label": "affiliated_with", "title": "affiliated_with", "to": "School of ECR", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Lei Yu", "label": "affiliated_with", "title": "affiliated_with", "to": "City University of Hong Kong", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Antoni B. Chan", "label": "affiliated_with", "title": "affiliated_with", "to": "School of ECR", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Antoni B. Chan", "label": "affiliated_with", "title": "affiliated_with", "to": "City University of Hong Kong", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Georgia Tech", "label": "has_affiliation", "title": "has_affiliation", "to": "School of ECE", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Ma_Small_Instance_Detection_2015_CVPR_paper", "label": "published_in", "title": "published_in", "to": "CVPR", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Ma_Small_Instance_Detection_2015_CVPR_paper", "label": "file_path", "title": "file_path", "to": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers", "width": 3.25}, {"arrows": "to", "color": "#CC7700", "from": "Small Instance Detection", "label": "uses", "title": "uses", "to": "Integer Programming", "width": 3.4000000000000004}, {"arrows": "to", "color": "#CC7700", "from": "Small Instance Detection", "label": "analyzes", "title": "analyzes", "to": "Object Density Maps", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "Small Instance Detection", "label": "is_topic", "title": "is_topic", "to": "Computer Vision", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "Integer Programming", "label": "is_topic", "title": "is_topic", "to": "Computer Vision", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "Object Density Maps", "label": "is_topic", "title": "is_topic", "to": "Computer Vision", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "pedestrians", "label": "is_example_of", "title": "is_example_of", "to": "partially-occluded small instances", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "cells", "label": "is_example_of", "title": "is_example_of", "to": "partially-occluding small instances", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "2D integer programming", "label": "is_used_to", "title": "is_used_to", "to": "recover object instance locations", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "ROI counts", "label": "used_in", "title": "used_in", "to": "2D integer programming", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "density map", "label": "regularizes", "title": "regularizes", "to": "detection performance", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "object instances", "label": "located_using", "title": "located_using", "to": "2D integer programming", "width": 3.79}, {"arrows": "to", "color": "#CC7700", "from": "HOG features", "label": "is_feature_descriptor_for", "title": "is_feature_descriptor_for", "to": "human detection", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "HOG features", "label": "used in", "title": "used in", "to": "object detection", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "Bayesian regression", "label": "is_method_for", "title": "is_method_for", "to": "crowd counting", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Crowd counting", "label": "uses", "title": "uses", "to": "low-level features", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Crowd counting", "label": "uses", "title": "uses", "to": "multiple local features", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "IEEE Conf. Computer Vision and Pattern Recognition", "label": "publishes", "title": "publishes", "to": "HOG features", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "IEEE Trans. on Image Processing", "label": "publishes", "title": "publishes", "to": "Bayesian regression", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Digital Image Computing: Techniques and Applications", "label": "publishes", "title": "publishes", "to": "Crowd counting using multiple local features", "width": 3.64}, {"arrows": "to", "color": "#CC7700", "from": "Human detection", "label": "requires", "title": "requires", "to": "HOG features", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Sridharan (2009)", "label": "addresses", "title": "addresses", "to": "crowd counting", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Sridharan (2009)", "label": "uses", "title": "uses", "to": "multiple local features", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Lowe (2004)", "label": "introduces", "title": "introduces", "to": "SIFT features", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Chan (2008)", "label": "addresses", "title": "addresses", "to": "privacy concerns", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Chan (2008)", "label": "addresses", "title": "addresses", "to": "crowd monitoring", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Zhao (2003)", "label": "focuses on", "title": "focuses on", "to": "Bayesian segmentation", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Bayesian segmentation", "label": "applied to", "title": "applied to", "to": "crowded scenes", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "R. ia", "label": "focuses_on", "title": "focuses_on", "to": "Bayesian segmentation", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Lemtipsky, V.", "label": "works_on", "title": "works_on", "to": "object counting", "width": 3.91}, {"arrows": "to", "color": "#CC7700", "from": "PASCAL VOC challenge", "label": "is_a", "title": "is_a", "to": "benchmark", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "PASCAL VOC challenge", "label": "used_for", "title": "used_for", "to": "object detection", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "City University of Hong Kong", "label": "has_member", "title": "has_member", "to": "Antoni B. Chan", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Mohammad Rastegari", "label": "is_author_of", "title": "is_author_of", "to": "Computationally Bound Retrieval", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Mohammad Rastegari", "label": "author_of", "title": "author_of", "to": "Computationally Bounded Retrieval", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Mohammad Rastegari", "label": "affiliated_with", "title": "affiliated_with", "to": "University of Maryland", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Computationally Bounded Retrieval", "label": "is_paper_from", "title": "is_paper_from", "to": "CVPR", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "Cem Keskin", "label": "author_of", "title": "author_of", "to": "Computationally Bounded Retrieval", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Cem Keskin", "label": "affiliated_with", "title": "affiliated_with", "to": "Microsoft Research", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Shahram Izadi", "label": "author_of", "title": "author_of", "to": "Computationally Bounded Retrieval", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Shahram Izadi", "label": "affiliated_with", "title": "affiliated_with", "to": "Microsoft Research", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Rastegari_Computationally_Bounded_Retrieval_2015_CVPR_paper.pdf", "label": "contains", "title": "contains", "to": "Computationally Bounded Retrieval", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Rastegari_Computationality_Bounded_Retrieval_2015_CVPR_paper", "label": "is_title_of", "title": "is_title_of", "to": "Computationally Bounded Retrieval", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "large image databases", "label": "makes", "title": "makes", "to": "efficient retrieval challenging", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "high dimensional data", "label": "poses", "title": "poses", "to": "retrieval challenge", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "hashing methods", "label": "sacrifices", "title": "sacrifices", "to": "accuracy for speed", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "orthogonality constraint", "label": "reduces", "title": "reduces", "to": "bit correlation", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "iterative scheme", "label": "optimizes", "title": "optimizes", "to": "objective", "width": 3.91}, {"arrows": "to", "color": "#CC7700", "from": "Near-optimal Hasing Algorithms", "label": "solves", "title": "solves", "to": "Approximate Nearest Neighbor", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "Near-optimal Hasing Algorithms", "label": "addresses", "title": "addresses", "to": "High Dimensions", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Datar et al. (2004)", "label": "introduces", "title": "introduces", "to": "Locality-Sensitive Hashing Scheme", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Locality-Sensitive Hashing Scheme", "label": "based_on", "title": "based_on", "to": "P-stable Distributions", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Methods", "label": "demonstrates", "title": "demonstrates", "to": "Speed-up", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Speed-up", "label": "is", "title": "is", "to": "factor of 100", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Gong et al. (2013)", "label": "presented_at", "title": "presented_at", "to": "Computer Vision and Pattern Recognition (CVPR), 2013", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Gong et al. (2013)", "label": "uses", "title": "uses", "to": "bilinear projections", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Gong \u0026 Lazebnik (2011)", "label": "presented_at", "title": "presented_at", "to": "2011 IEEE Conference on Computer Vision and Pattern Recognition", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Gong \u0026 Lazebnik (2011)", "label": "uses", "title": "uses", "to": "iterative quantization", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "J\u00e9goeu et al. (2009)", "label": "developed", "title": "developed", "to": "Searching with quantization", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Searching with quantization", "label": "uses", "title": "uses", "to": "short codes", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "J\u00e9gou et al (2009)", "label": "researches", "title": "researches", "to": "approximate nearest neighbor search", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Krizhevskya et al (2012)", "label": "performs", "title": "performs", "to": "ImageNet classification", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Krizhevskya et al (2012)", "label": "uses", "title": "uses", "to": "deep convolutional neural networks", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Majia et al (2013)", "label": "focuses on", "title": "focuses on", "to": "efficient classification", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Norouzia et al (2011)", "label": "develops", "title": "develops", "to": "minimal loss hashing", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "New Insights into Laplacian Similarity Search", "label": "presented_at", "title": "presented_at", "to": "CVPR", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Wu_New_Insights_Into_2015_CVPR_paper.pdf", "label": "is_document_of", "title": "is_document_of", "to": "New Insights into Laplacian Similarity Search", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Graph-based computer vision applications", "label": "relies_on", "title": "relies_on", "to": "similarity metrics", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "similarity metrics", "label": "computes", "title": "computes", "to": "pairwise similarity", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "pairwise similarity", "label": "is_between", "title": "is_between", "to": "vertices", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "(L + \u03b1\u039b)\u22121", "label": "includes", "title": "includes", "to": "graph Laplacian", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "(L + \u03b1\u039b)\u22121", "label": "includes", "title": "includes", "to": "positive diagonal matrix", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "(L + \u03b1\u039b)\u22121", "label": "respects", "title": "respects", "to": "graph topology", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "positive diagonal matrix", "label": "acts_as", "title": "acts_as", "to": "regularizer", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "regularizer", "label": "impacts", "title": "impacts", "to": "cluster density", "width": 3.64}, {"arrows": "to", "color": "#CC7700", "from": "regularizer", "label": "denotes", "title": "denotes", "to": "\u039b", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "choices", "label": "lead_to", "title": "lead_to", "to": "complementary behaviors", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Paper (1999)", "label": "discusses", "title": "discusses", "to": "Pagerank Citation Ranking", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Paper (1999)", "label": "validates", "title": "validates", "to": "approach", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Pagerank Citation Ranking", "label": "aims_to", "title": "aims_to", "to": "bring order to web", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Chung (1997)", "label": "authored", "title": "authored", "to": "Spectral Graph Theory", "width": 3.9699999999999998}, {"arrows": "to", "color": "#CC7700", "from": "Spectral Graph Theory", "label": "covers", "title": "covers", "to": "Graph Topology", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Andersen et al. (2006)", "label": "uses", "title": "uses", "to": "Pagerank Vectors", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Andersen et al. (2006)", "label": "addresses", "title": "addresses", "to": "Local Graph Partitioning", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Belkin \u0026 Niyogi (2001)", "label": "proposes", "title": "proposes", "to": "Laplacian Eigenmaps", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Shi \u0026 Malik (2000)", "label": "introduces", "title": "introduces", "to": "Normalized Cuts", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "Laplacianfaces", "label": "used_for", "title": "used_for", "to": "face recognition", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "Random walks", "label": "used_for", "title": "used_for", "to": "image segmentation", "width": 3.73}, {"arrows": "to", "color": "#0077CC", "from": "Wu, X.-M.", "label": "affiliated_with", "title": "affiliated_with", "to": "Columbia University", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Wu, X.-M.", "label": "works_in", "title": "works_in", "to": "Electrical Engineering", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Wu, X.-M.", "label": "authored", "title": "authored", "to": "Analyzing the harmonic structure in graph-based learning", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Wu, X.-M.", "label": "works_in", "title": "works_in", "to": "Department of Electrical Engineering", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "Wu, X.-M.", "label": "contributes_to", "title": "contributes_to", "to": "graph-based learning", "width": 3.25}, {"arrows": "to", "color": "#0077CC", "from": "graph-based learning", "label": "analyzes", "title": "analyzes", "to": "harmonic structure", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Chang, S.-F.", "label": "authored", "title": "authored", "to": "Analyzing the harmonic structure in graph-based learning", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Wenguan Wang", "label": "is_author_of", "title": "is_author_of", "to": "Saliency-Aware Geodesic Video Object Segmentation", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Wenguan Wang", "label": "affiliated_with", "title": "affiliated_with", "to": "Beijing Lab of Intelligent Information Technology", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Wenguan Wang", "label": "member_of", "title": "member_of", "to": "School of Computer Science", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Wenguan Wang", "label": "located_in", "title": "located_in", "to": "Beijing Institute of Technology", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Saliency-Aware Geodesic Video Object Segmentation", "label": "published_in", "title": "published_in", "to": "NIPS", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Jianbing Shen", "label": "is_author_of", "title": "is_author_of", "to": "Saliency-Aware Geodesic Video Object Segmentation", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Jianbing Shen", "label": "affiliated_with", "title": "affiliated_with", "to": "Beijing Lab of Intelligent Information Technology", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Jianbing Shen", "label": "located_in", "title": "located_in", "to": "Beijing Institute of Technology", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Fatih Porikli", "label": "is_author_of", "title": "is_author_of", "to": "Saliency-Aware Geodesic Video Object Segmentation", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Fatih Porikli", "label": "affiliated_with", "title": "affiliated_with", "to": "Research School of Engineering", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Fatih Porikli", "label": "affiliated_with", "title": "affiliated_with", "to": "Australian National University", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Fatih Porikli", "label": "member_of", "title": "member_of", "to": "NICTA Australia", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "Saliency-Aware Geosesic Video Object Segmentation", "label": "is_a", "title": "is_a", "to": "CVPR paper", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "unsupervised method", "label": "is_based_on", "title": "is_based_on", "to": "geodesic distance", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "spatial edges", "label": "are_indicators_of", "title": "are_indicators_of", "to": "spatiotemporal salience maps", "width": 3.7600000000000002}, {"arrows": "to", "color": "#CC7700", "from": "Video Object Segmentation", "label": "relies_on", "title": "relies_on", "to": "Energy Minimization", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Video Object Segmentation", "label": "achieves", "title": "achieves", "to": "spatially and temporally coherent object segmentation", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Geos image segmentation", "label": "introduced_in", "title": "introduced_in", "to": "ECCV, 2008", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Geos image segmentation", "label": "uses", "title": "uses", "to": "geodesic image segmentation approach", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "Motion Boundaries", "label": "relevant_to", "title": "relevant_to", "to": "Video Object Segmentation", "width": 3.0999999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Geos", "label": "introduces", "title": "introduces", "to": "geospatial image segmentation approach", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Geos", "label": "is_method_of", "title": "is_method_of", "to": "image segmentation", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Geodesic graph cut", "label": "is_method_of", "title": "is_method_of", "to": "interactive image segmentation", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Grabcut", "label": "is_method_of", "title": "is_method_of", "to": "interactive foreground extraction", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "Grabcut", "label": "uses", "title": "uses", "to": "iterated graph cuts", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Object segmentation", "label": "uses", "title": "uses", "to": "trajectory analysis", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Video object segmentation", "label": "addressed by", "title": "addressed by", "to": "W. Brendel and S. Todorovic", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Geodesic image and video editing", "label": "combines", "title": "combines", "to": "geodesic methods", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "J. Carreira", "label": "authored", "title": "authored", "to": "Constrained parametric min-cuts", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "W. Brendel", "label": "authored", "title": "authored", "to": "Video object segmentation by tracking regions", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "D. Tsai", "label": "authored", "title": "authored", "to": "Motion coherent tracking using multi-label mrf optimization", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Tali Dekel", "label": "author_of", "title": "author_of", "to": "Best-Buddies Similarity", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Tali Dekel", "label": "affiliated_with", "title": "affiliated_with", "to": "MIT CSAIL", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "Best-Buddies Similarity", "label": "is_a", "title": "is_a", "to": "paper", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Best-Buddies Similarity", "label": "published_in", "title": "published_in", "to": "CVPR", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "template matching", "label": "occurs_in", "title": "occurs_in", "to": "unconstrained environments", "width": 3.7600000000000002}, {"arrows": "to", "color": "#CC7700", "from": "Best-Buddies Similarity (BBS)", "label": "is", "title": "is", "to": "similarity measure", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Best-Buddies Similarity (BBS)", "label": "is_based_on", "title": "is_based_on", "to": "counting Best-Buddie Pairs (BBPs)", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Best-Buddies Similarity (BBS)", "label": "is_robust_against", "title": "is_robust_against", "to": "geometric deformations", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "Best-Buddies Similarity (BBS)", "label": "is_robust_against", "title": "is_robust_against", "to": "outliers", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Best-Buddies Similarity (BSS)", "label": "is", "title": "is", "to": "parameter-free", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Best-Buddie Pairs (BBPs)", "label": "are", "title": "are", "to": "pairs of points", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "BBS", "label": "solves", "title": "solves", "to": "non-rigid object tracking", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "BBS", "label": "demonstrates", "title": "demonstrates", "to": "consistent success", "width": 3.4000000000000004}, {"arrows": "to", "color": "#CC7700", "from": "BBS", "label": "uses", "title": "uses", "to": "Similarity Measures", "width": 3.25}, {"arrows": "to", "color": "#CC7700", "from": "BBS", "label": "handles", "title": "handles", "to": "Outlier Robustness", "width": 3.0999999999999996}, {"arrows": "to", "color": "#CC7700", "from": "BBS", "label": "incorporates", "title": "incorporates", "to": "Geometric Deformations", "width": 2.95}, {"arrows": "to", "color": "#0077CC", "from": "Comaniciu, D. et al.", "label": "authored", "title": "authored", "to": "mean shift tracking", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "mean shift tracking", "label": "addresses", "title": "addresses", "to": "non-rigid object tracking", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Rubner, Y. et al.", "label": "introduced", "title": "introduced", "to": "Earth Mover\u0027s Distance", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "Earth Mover\u0027s Distance", "label": "is_a", "title": "is_a", "to": "metric for image retrieval", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Earth Mover\u0027s Distance", "label": "is_metric_for", "title": "is_metric_for", "to": "image comparison", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Earth Mover\u0027s Distance", "label": "introduced_by", "title": "introduced_by", "to": "Rubner et al. (2000)", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Pele et al. (2008)", "label": "addresses", "title": "addresses", "to": "robust pattern matching", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Simaov et al. (2008)", "label": "explores", "title": "explores", "to": "summarizing visual data", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Simaov et al. (2008)", "label": "uses", "title": "uses", "to": "similarity measures", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Hel-Or et al. (2014)", "label": "focuses_on", "title": "focuses_on", "to": "photometric invariant template matching", "width": 3.91}, {"arrows": "to", "color": "#CC7700", "from": "photometric invariant template matching", "label": "is_a", "title": "is_a", "to": "matching technique", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Tian et al. (2012)", "label": "deals_with", "title": "deals_with", "to": "estimating nonrigid image distortions", "width": 3.88}, {"arrows": "to", "color": "#CC7700", "from": "nonrigid image distortions", "label": "impacts", "title": "impacts", "to": "image estimation", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "Tian \u0026 Narasimhan (2012)", "label": "deals_with", "title": "deals_with", "to": "nonrigid image distortions", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Korman et al. (2013)", "label": "presents", "title": "presents", "to": "fast affine template matching algorithm", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Wu et al. (2013)", "label": "provides", "title": "provides", "to": "online object tracking benchmark", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Olson (2002)", "label": "discusses", "title": "discusses", "to": "maximum-likelihood image matching", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Michael Rubinstein", "label": "affiliated_with", "title": "affiliated_with", "to": "Google Research", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Shai Avidan", "label": "affiliated_with", "title": "affiliated_with", "to": "Tel Aviv University", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Shai Avidan", "label": "email", "title": "email", "to": "avidan@eng.tau.ac.il", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Nianyi Li", "label": "author_of", "title": "author_of", "to": "A Weighted Sparse Coding Framework for Saliency Detection", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Nianyi Li", "label": "authored", "title": "authored", "to": "A Weighted Sparse Coding Framework", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Nianyi Li", "label": "affilates_with", "title": "affilates_with", "to": "Tel Aviv University", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Bilin Sun", "label": "author_of", "title": "author_of", "to": "A Weighted Sparse Coding Framework for Saliency Detection", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Bilin Sun", "label": "authored", "title": "authored", "to": "A Weighted Sparse Coding Framework", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Jingyi", "label": "author_of", "title": "author_of", "to": "A Weighted Sparse Coding Framework for Saliency Detection", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "A Weighted Sparse Coding Framework", "label": "addresses", "title": "addresses", "to": "Saliency Detection", "width": 3.7600000000000002}, {"arrows": "to", "color": "#CC7700", "from": "A Weighted Sparse Coding Framework", "label": "is_paper_type", "title": "is_paper_type", "to": "CVPR paper", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Jingyi Yu", "label": "authored", "title": "authored", "to": "A Weighted Sparse Coding Framework", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "avidan@eng.tau.ac.il", "label": "email_associated_with", "title": "email_associated_with", "to": "Nianyi Li", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "billf@mit.edu", "label": "email_associated_with", "title": "email_associated_with", "to": "William T. Freeman", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Li_A_Weighted_Sparse_2015_CVPR_paper", "label": "is_file", "title": "is_file", "to": "pdf", "width": 3.91}, {"arrows": "to", "color": "#CC7700", "from": "salience detection", "label": "utilizes", "title": "utilizes", "to": "high-dimensional datasets", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "dictionaries", "label": "uses", "title": "uses", "to": "data-speci\ufb01c features", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "dictionary", "label": "prunes", "title": "prunes", "to": "outliers", "width": 3.7}, {"arrows": "to", "color": "#00CC77", "from": "dictionary", "label": "refined_by", "title": "refined_by", "to": "superpixels", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "Liu et al. (2011)", "label": "proposes", "title": "proposes", "to": "Salience Detection", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Liu et al. (2011)", "label": "published_in", "title": "published_in", "to": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Achanta et al. (2012)", "label": "compares", "title": "compares", "to": "Superpixels", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Achanta et al. (2012)", "label": "published_in", "title": "published_in", "to": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "Borji \u0026 Itti (2012)", "label": "exploits", "title": "exploits", "to": "Patch Rarities", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Borji, Sihite, \u0026 Itti (2012)", "label": "introduces", "title": "introduces", "to": "Salient Object Detection Benchmark", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Borji, Sihite, \u0026 Itti (2012)", "label": "published_in", "title": "published_in", "to": "ECCV", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Reynolds \u0026 Desimone", "label": "study", "title": "study", "to": "V4", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Reynolds \u0026 Desimone", "label": "investigate", "title": "investigate", "to": "attention", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "Reynolds \u0026 Desimone", "label": "Published in", "title": "Published in", "to": "Neuron", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "Nothdurft", "label": "proposes", "title": "proposes", "to": "additivity across dimensions", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Nothdurft", "label": "studies", "title": "studies", "to": "salience from feature contrast", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Perazzi et al.", "label": "develops", "title": "develops", "to": "salience filters", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Perazzi et al.", "label": "uses", "title": "uses", "to": "contrast based filtering", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Cheng et al.", "label": "develops", "title": "develops", "to": "Global contrast based salient region detection", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Borji et al.", "label": "presents", "title": "presents", "to": "cal and global patch rarities", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "Movahedi \u0026 Elder", "label": "validates", "title": "validates", "to": "performance measures", "width": 3.73}, {"arrows": "to", "color": "#0077CC", "from": "Reynolds \u0026 Desimnone", "label": "studied", "title": "studied", "to": "V4", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Itti \u0026 Koch", "label": "Published in", "title": "Published in", "to": "Nature Reviews Neuroscience", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Reynolds", "label": "Published in", "title": "Published in", "to": "CVPR", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Souvenir", "label": "affiliated with", "title": "affiliated with", "to": "University of Delaware", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Souvenir", "label": "authored", "title": "authored", "to": "Robust Regression on Image Manifolds", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Sun", "label": "email", "title": "email", "to": "sunbilin@eecis.udel.edu", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Yu", "label": "email", "title": "email", "to": "yu@eecis.udel.edu", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "robust regression method", "label": "is", "title": "is", "to": "non-parametric", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "mis-labeled examples", "label": "has", "title": "has", "to": "ordered labels", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "label corruption levels", "label": "reaches", "title": "reaches", "to": "80%", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "image labels", "label": "describe", "title": "describe", "to": "associated images", "width": 3.7600000000000002}, {"arrows": "to", "color": "#CC7700", "from": "Ordered Labels", "label": "relates_to", "title": "relates_to", "to": "Ordinal Data", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Label Denoising", "label": "improves", "title": "improves", "to": "accuracy of image labels", "width": 3.46}, {"arrows": "to", "color": "#0077CC", "from": "Building Rome in a day", "label": "published_in", "title": "published_in", "to": "IEEE International Conference on Computer Visions", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "R. C. Bolles", "label": "author of", "title": "author of", "to": "Random sample consensus", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "C.-C. Chang", "label": "author of", "title": "author of", "to": "LIBSVM", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "C.-J. Lin", "label": "author of", "title": "author of", "to": "LIBSVM", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "USAC", "label": "framework for", "title": "framework for", "to": "Random sample consensus", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "R. Raguram", "label": "author of", "title": "author of", "to": "USAC", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "l-curve", "label": "used for", "title": "used for", "to": "Analysis of discrete ill-posed problems", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "P. C. Hansen", "label": "author of", "title": "author of", "to": "Analysis of discrete ill-posed problems", "width": 3.73}, {"arrows": "to", "color": "#0077CC", "from": "Internet photo collections", "label": "used for", "title": "used for", "to": "Modeling the world", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "N. Snavely", "label": "author of", "title": "author of", "to": "Modeling the world", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "SIAM review", "label": "is_published_in", "title": "is_published_in", "to": "journal", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Kai Han", "label": "is_author_of", "title": "is_author_of", "to": "A Fixed Viewpoint Approach", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Kai Han", "label": "affiliation", "title": "affiliation", "to": "The University of Hokkaido", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "A Fixed Viewpoint Approach", "label": "is_publication", "title": "is_publication", "to": "paper", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Kwan-Yee K. Wong", "label": "is_author_of", "title": "is_author_of", "to": "A Fixed Viewpoint Approach", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Kwan-Yee K. Wong", "label": "affiliation", "title": "affiliation", "to": "The University of Hokkaido", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Miaomiao Liu", "label": "is_author_of", "title": "is_author_of", "to": "A Fixed View Point Approach", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Miaomiao Liu", "label": "affiliation", "title": "affiliation", "to": "NICTA", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Miaomiao Liu", "label": "affiliation", "title": "affiliation", "to": "CECS, ANU", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "light path triangulation", "label": "relates_to", "title": "relates_to", "to": "refractive photo-light-path", "width": 3.46}, {"arrows": "to", "color": "#0077CC", "from": "M. Ben-Ezra and S. K. Nayr", "label": "addresses", "title": "addresses", "to": "transparency analysis", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "G. Eren et al.", "label": "proposes", "title": "proposes", "to": "shape estimation", "width": 3.58}, {"arrows": "to", "color": "#0077CC", "from": "shape estimation", "label": "uses", "title": "uses", "to": "local surface heating", "width": 3.52}, {"arrows": "to", "color": "#0077CC", "from": "local surface heating", "label": "is_method_for", "title": "is_method_for", "to": "shape estimation", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Fischler and Bolles", "label": "Introduces", "title": "Introduces", "to": "RANSAC", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Hata et al.", "label": "Explores", "title": "Explores", "to": "genetic algorithm", "width": 3.91}, {"arrows": "to", "color": "#CC7700", "from": "genetic algorithm", "label": "is_used_for", "title": "is_used_for", "to": "shape extraction", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Ihrke et al. (2005)", "label": "Demonstrates", "title": "Demonstrates", "to": "geometry reconstruction", "width": 3.88}, {"arrows": "to", "color": "#CC7700", "from": "geometry reconstruction", "label": "occurs_in", "title": "occurs_in", "to": "dynamic environments", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Ihrke et al. (2008)", "label": "Provides", "title": "Provides", "to": "overview", "width": 3.82}, {"arrows": "to", "color": "#CC7700", "from": "Transparent objects", "label": "requires", "title": "requires", "to": "shape estimation", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Benjamin Allain", "label": "author_of", "title": "author_of", "to": "An Efficient Volumetric Framework for Shape Tracking", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Benjamin Allain", "label": "affiliated_with", "title": "affiliated_with", "to": "Inria Grenoble Rh\u02c6one-Alpes - LJK", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Benjamin Allain", "label": "affiliation", "title": "affiliation", "to": "Inria Grenoble Rh\u02c6one- Alpes", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Benjamin Allain", "label": "is_affiliated_with", "title": "is_affiliated_with", "to": "LJK", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "An Efficient Volumetric Framework for Shape Tracking", "label": "presented_at", "title": "presented_at", "to": "CVPR", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "An Efficient Volumetric Framework for Shape Tracking", "label": "file_name", "title": "file_name", "to": "Allain_An_Efficient_Volumetric_2015_CVPR_paper.pdf", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Jean-S\u00e9batian Franco", "label": "author_of", "title": "author_of", "to": "An Efficient Volumetric Framework for Shape Tracking", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Edmond Boyer", "label": "author_of", "title": "author_of", "to": "An Efficient Volumetric Framework for Shape Tracking", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Edmond Boyer", "label": "affiliated_with", "title": "affiliated_with", "to": "Inria Grenoble Rh\u02c6one-Alpes - LJK", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "surface-based strategies", "label": "can_fail_when", "title": "can_fail_when", "to": "observations define several feasible surfaces", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "this work", "label": "investigates", "title": "investigates", "to": "volumetric shape parametrization", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "volumetric shape parametrization", "label": "utilizes", "title": "utilizes", "to": "Centroidal Voronoi Tesselations (CVT)", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "volumetric deformation model", "label": "demonstrates", "title": "demonstrates", "to": "improved precision and robustness", "width": 3.73}, {"arrows": "to", "color": "#0077CC", "from": "volumetric deformation model", "label": "compared_to", "title": "compared_to", "to": "state-of-the-art methods", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "volumetric shape tracking", "label": "uses", "title": "uses", "to": "Centroidal Voronoi Tesselations (CVT)", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Volumetric Shape Tracking", "label": "compares_to", "title": "compares_to", "to": "state-of-the-art methods", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "Centroidal Voronoi Tesselations", "label": "is_a", "title": "is_a", "to": "method", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "Dynamic Shape Capture", "label": "is_a", "title": "is_a", "to": "method", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "Motion Estimation", "label": "is_a", "title": "is_a", "to": "method", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "Surface-based Methods", "label": "is_a", "title": "is_a", "to": "approach", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "Volume-based Methods", "label": "is_a", "title": "is_a", "to": "approach", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Alexa et al. (2000)", "label": "authored", "title": "authored", "to": "As-rigid-as-possible shape interpolation", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Allain et al. (2014)", "label": "authored", "title": "authored", "to": "On mean pose and variability of 3d deformable models", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Ballan \u0026 Cortelazzo (2008)", "label": "authored", "title": "authored", "to": "Marker-less motion capture of skinned models", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Bishop (2006)", "label": "authored", "title": "authored", "to": "Pattern Recognition and Machine Learning", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Botsu et al. (2007)", "label": "authored", "title": "authored", "to": "Adaptive space deformations based on rigid cells", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Adaptive space deformations based on rigid cells", "label": "published_in", "title": "published_in", "to": "Comput. Graph. Forum", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Botsu, M.", "label": "authored", "title": "authored", "to": "Adaptive space deformations based on rigid cells", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Cagniart, C.", "label": "authored", "title": "authored", "to": "Free-form mesh tracking: a patch-based approach", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Cagniart, C.", "label": "authored", "title": "authored", "to": "Probabilistic deformable surface tracking from multiple videos", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Free-form mesh tracking: a patch-based approach", "label": "published_in", "title": "published_in", "to": "CVPR", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Probabilistic deformable surface tracking from multiple videos", "label": "published_in", "title": "published_in", "to": "ECCV", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "de Aguiar, E.", "label": "authored", "title": "authored", "to": "Performance capture from sparse multi-view video", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Performance capture from sparse multi-view video", "label": "published_in", "title": "published_in", "to": "ACM Transactions on Graphics", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "de Aguliar, E.", "label": "authored", "title": "authored", "to": "Marker-less deformable mesh tracking for human shape and motion capture", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Benjamin Allaine", "label": "email", "title": "email", "to": "firstname.lastname@inria.fr", "width": 3.9699999999999998}, {"arrows": "to", "color": "#CC7700", "from": "Maximum likelihood", "label": "method_used_in", "title": "method_used_in", "to": "em algorithm", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "em algorithm", "label": "used_for", "title": "used_for", "to": "Maximum likelihood from incomplete data", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Journal of the Royal Statistical Society, series B", "label": "publishes", "title": "publishes", "to": "Maximum likelihood from incomplete data via the em algorithm", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Jean-S\u00e9bastien Franco", "label": "affiliated_with", "title": "affiliated_with", "to": "Inria Grenoble Rh\u02c6one-Alpes - LHK", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Yi-Hsuan Tsai", "label": "author_of", "title": "author_of", "to": "Adaptive Region Pooling for Object Detection", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Yi-Hsuan Tsai", "label": "affiliated_with", "title": "affiliated_with", "to": "UC Merced", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Adaptive Region Pooling for Object Detection", "label": "presented_at", "title": "presented_at", "to": "CVPR", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Adaptive Region Pooling for Object Detection", "label": "is_located_at", "title": "is_located_at", "to": "/mnt/DATA/Glucoma/LLM/Ollama_pdf_handle/cvpr_papers/Tsai_Adaptive_Region_Pooling_2015_CVPR_supplemental.pdf", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Onur C. Hamsici", "label": "author_of", "title": "author_of", "to": "Adaptive Region Pooling for Object Detection", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Onur C. Hamsici", "label": "affiliated_with", "title": "affiliated_with", "to": "Qualcomm Research", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Tsai_Adaptive_Region_Pooling_2015_CVPR_supplemental", "label": "is_file_name", "title": "is_file_name", "to": "Adaptive Region Pooling for Object Detection", "width": 3.25}, {"arrows": "to", "color": "#0077CC", "from": "Adaptive Region Pooling", "label": "is_method_for", "title": "is_method_for", "to": "object detection", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Adaptive Region Pooling", "label": "discovers", "title": "discovers", "to": "discriminative object parts", "width": 3.88}, {"arrows": "to", "color": "#CC7700", "from": "Adaptive Region Pooling", "label": "addresses", "title": "addresses", "to": "challenging conditions", "width": 3.64}, {"arrows": "to", "color": "#CC7700", "from": "Adaptive Region Pooling", "label": "is_topic", "title": "is_topic", "to": "Object Detection", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Adaptive Region Processing", "label": "transfers", "title": "transfers", "to": "keypoints", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "representative parts", "label": "transferred_to", "title": "transferred_to", "to": "detected objects", "width": 3.73}, {"arrows": "to", "color": "#0077CC", "from": "challenging conditions", "label": "includes", "title": "includes", "to": "occlusion", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "challenging conditions", "label": "includes", "title": "includes", "to": "varying lighting", "width": 3.4899999999999998}, {"arrows": "to", "color": "#0077CC", "from": "ARP", "label": "compared_with", "title": "compared_with", "to": "ESVM", "width": 3.61}, {"arrows": "to", "color": "#0077CC", "from": "Ensemble of exemplar-svms", "label": "compared_to", "title": "compared_to", "to": "method", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Ensemble of exemplar-svms", "label": "presented_at", "title": "presented_at", "to": "ICCV", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "Keypoint Transfer", "label": "is_topic", "title": "is_topic", "to": "Object Detection", "width": 3.64}, {"arrows": "to", "color": "#CC7700", "from": "Object Parts Discovery", "label": "is_topic", "title": "is_topic", "to": "Object Detection", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Long-term Recurrent Convolutional Networks", "label": "aims_to", "title": "aims_to", "to": "Visual Recognition", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Long-term Recurrent Convolutional Networks", "label": "aims_to", "title": "aims_to", "to": "Visual Description", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Jeff Donahue", "label": "is_author", "title": "is_author", "to": "Long-term Recurrent Convolutional Networks", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Jeff Donahue", "label": "affiliated_with", "title": "affiliated_with", "to": "UC Berkeley", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Lisa Anne Hendricks", "label": "is_author", "title": "is_author", "to": "Long-term Recurrent Convolutional Networks", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Lisa Anne Hendricks", "label": "affiliated_with", "title": "affiliated_with", "to": "UC Berkeley", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Sergio Guadarrama", "label": "contributed_to", "title": "contributed_to", "to": "Donahue_Long-Term_Recurrent_Convolutional_2015_CVPR_paper", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Sergio Guadarrama", "label": "affiliated_with", "title": "affiliated_with", "to": "UC Berkeley", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Donahue_Long-Term_Recurrent_Convolutional_2015_CVPR_paper", "label": "discusses", "title": "discusses", "to": "Visual Features", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Donahue_Long-Term_Recurrent_Convolutional_2015_CVPR_paper", "label": "generates", "title": "generates", "to": "Predictions", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "Donahue_Long-Term_Recurrent_Convolutional_2015_CVPR_paper", "label": "operates_on", "title": "operates_on", "to": "Visual Input", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "Marcus Rohrbach", "label": "contributed_to", "title": "contributed_to", "to": "Donahue_Long-Term_Recurrent_Convolutional_2015_CVPR_paper", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Subhashini Venugopalan", "label": "contributed_to", "title": "contributed_to", "to": "Donahue_Long-Term_Recurrent_Convolutional_2015_CVPR_paper", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Subhashini Venugopalan", "label": "affiliated_with", "title": "affiliated_with", "to": "UT Austin", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Kate Saenko", "label": "contributed_to", "title": "contributed_to", "to": "Donahue_Long-Term_Recurrent_Convolutional_2015_CVPR_paper", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Kate Saenko", "label": "affiliation", "title": "affiliation", "to": "UMass Lowell", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Trevor Darrell", "label": "contributed_to", "title": "contributed_to", "to": "Donahue_Long-Term_Recurrent_Convolutional_2015_CVPR_paper", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Trevor Darrell", "label": "affiliation", "title": "affiliation", "to": "UC Berkeley", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "Visual Features", "label": "is_aspect_of", "title": "is_aspect_of", "to": "Donahue_Long-Term_Recurrent_Convolutional_2015_CVPR_paper", "width": 3.25}, {"arrows": "to", "color": "#0077CC", "from": "Models", "label": "based_on", "title": "based_on", "to": "deep convolutional networks", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Models", "label": "are", "title": "are", "to": "recurrent", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Models", "label": "are", "title": "are", "to": "temporally deep", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Models", "label": "demonstrate value on", "title": "demonstrate value on", "to": "video recognition tasks", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Models", "label": "is_used_for", "title": "is_used_for", "to": "image description", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "Models", "label": "address", "title": "address", "to": "video narration challenges", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "recurrent convolutional models", "label": "are", "title": "are", "to": "doubly deep", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "recurrent convolutional models", "label": "have_advantage", "title": "have_advantage", "to": "complex target concepts", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "recurrent convolutional models", "label": "have_advantage", "title": "have_advantage", "to": "limited training data", "width": 3.4000000000000004}, {"arrows": "to", "color": "#00CC77", "from": "network state updates", "label": "enable", "title": "enable", "to": "long-term dependencies", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "long-term RNN models", "label": "map", "title": "map", "to": "variable length outputs", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "long-term RNN models", "label": "model", "title": "model", "to": "complex temporal dynamics", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "recurrent long-term models", "label": "are_connected_to", "title": "are_connected_to", "to": "visual convnet models", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Recurrent Convolutional Networks (LRCNs)", "label": "applied_to", "title": "applied_to", "to": "Video Recognition", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "Recurrent Convolutional Networks (LRCNs)", "label": "addresses", "title": "addresses", "to": "Long-Term Dependencies", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Recurrent Convolutional Networks (LRCNs)", "label": "has_advantage_over", "title": "has_advantage_over", "to": "state-of-the-art models", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Recurrent Convolutional Networks (LRCNs)", "label": "improves", "title": "improves", "to": "recognition", "width": 3.25}, {"arrows": "to", "color": "#CC7700", "from": "Video Recognition", "label": "relies_on", "title": "relies_on", "to": "Convolutional Networks", "width": 3.46}, {"arrows": "to", "color": "#CC7700", "from": "Video Recognition", "label": "related_to", "title": "related_to", "to": "Sequence Learning", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "Long-Term Dependencies", "label": "requires", "title": "requires", "to": "Recurrent Convolutional Networks (LRCNs)", "width": 3.4000000000000004}, {"arrows": "to", "color": "#CC7700", "from": "Sequence Learning", "label": "enables", "title": "enables", "to": "Long-Term Dependencies", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "state-of-the-art models", "label": "are", "title": "are", "to": "separately optimized", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Action Classification", "label": "uses", "title": "uses", "to": "Long short-term memory recurrent neural networks", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Action Classification", "label": "occurs_in", "title": "occurs_in", "to": "soccer videos", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "Long short-term memory recurrent neural networks", "label": "addresses", "title": "addresses", "to": "Long-Term Dependencies", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Multimodal neural language models", "label": "combines", "title": "combines", "to": "visual-semantic embeddings", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Unifying visual-semantic embeddings", "label": "achieved_by", "title": "achieved_by", "to": "Multimodal neural language models", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Video in sentences out", "label": "investigates", "title": "investigates", "to": "Video Recognition", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Video in sentences out", "label": "presented_at", "title": "presented_at", "to": "UAI", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "High accuracy optical flow estimation", "label": "presented_at", "title": "presented_at", "to": "ECCV", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "3D convolutional neural networks", "label": "published_in", "title": "published_in", "to": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Generating sequences", "label": "published_as", "title": "published_as", "to": "arXiv preprint arXiv:1308.0850", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Recurrent neural networks", "label": "used_for", "title": "used_for", "to": "generating sequences", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "J. Deng", "label": "affilates_with", "title": "affilates_with", "to": "CVPR", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "W. Dong", "label": "affilates_with", "title": "affilates_with", "to": "CVPR", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "R. Socher", "label": "affilates_with", "title": "affilates_with", "to": "CVPR", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "L.-J. Li", "label": "affilages_with", "title": "affilages_with", "to": "CVPR", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "K. Li", "label": "affiliates_with", "title": "affiliates_with", "to": "CVPR", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "L. Fei-Fei", "label": "affiliates_with", "title": "affiliates_with", "to": "CVPR", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "A. Frome", "label": "affiliates_with", "title": "affiliates_with", "to": "NIPS", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "ubhashini Venugopalan", "label": "affiliation", "title": "affiliation", "to": "UT Austin", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "SOM", "label": "is_a", "title": "is_a", "to": "Semantic Obviousness Metric", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "SOM", "label": "used_for", "title": "used_for", "to": "Image Quality Assessment", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "SOM", "label": "presented_at", "title": "presented_at", "to": "CVPR", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Peng Zhang", "label": "author_of", "title": "author_of", "to": "SOM", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Peng Zhang", "label": "has_email", "title": "has_email", "to": "pzhangoo@mail.ustc.edu.cn", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Wengang Zhou", "label": "author_of", "title": "author_of", "to": "SOM", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Wengang Zhou", "label": "has_email", "title": "has_email", "to": "zhwg@ustc.edu.cn", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Lei Wu", "label": "author_of", "title": "author_of", "to": "SOM", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Lei Wu", "label": "has_email", "title": "has_email", "to": "wuleibig@gmail.com", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Houqiang Li", "label": "author_of", "title": "author_of", "to": "SOM", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Houqiang Li", "label": "has_email", "title": "has_email", "to": "lihq@ustc.edu.cn", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Zhang_SOM_Semantic_Obviousness_2015_CVPR_paper.pdf", "label": "contains", "title": "contains", "to": "SOM", "width": 3.7600000000000002}, {"arrows": "to", "color": "#CCCCCC", "from": "Image quality assessment (IQA)", "label": "aims_to", "title": "aims_to", "to": "objectively estimate human perception", "width": 3.94}, {"arrows": "to", "color": "#CCCCCC", "from": "Image quality assessment (IQA)", "label": "targets", "title": "targets", "to": "problem", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "no-referece (NR) image quality assessment (IQA) framework", "label": "based_on", "title": "based_on", "to": "semantic obviousness", "width": 3.88}, {"arrows": "to", "color": "#00CC77", "from": "semantic-level factors", "label": "affects", "title": "affects", "to": "human perception of image quality", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "comparable results", "label": "related_to", "title": "related_to", "to": "state-of-the-art full-referece IQA (FR-IQA) methods", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "generalization ability", "label": "belongs_to", "title": "belongs_to", "to": "approach", "width": 3.64}, {"arrows": "to", "color": "#CC7700", "from": "NR-IQA algorithms", "label": "are_type_of", "title": "are_type_of", "to": "IQA algorithms", "width": 3.25}, {"arrows": "to", "color": "#0077CC", "from": "NR-IQA algorithms", "label": "compared_to", "title": "compared_to", "to": "existing algorithms", "width": 3.55}, {"arrows": "to", "color": "#CC7700", "from": "full-referece IQA (FR-IQA) methods", "label": "are_type_of", "title": "are_type_of", "to": "IQA methods", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "Image Quality Assessment (IQA)", "label": "is_a", "title": "is_a", "to": "assessment method", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "No-Reference Image Quality Assessment (NR-IQA)", "label": "is_a", "title": "is_a", "to": "IQA method", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "NR-IQA", "label": "achieves", "title": "achieves", "to": "comparable results", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "Damien Teney", "label": "is_author", "title": "is_author", "to": "Learning Similarity Metrics", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Learning Similarity Metrics", "label": "addresses", "title": "addresses", "to": "Dynamic Scene Segmentation", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Matthew Brown", "label": "is_author", "title": "is_author", "to": "Learning Similarity Metrics", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "dynamic textures", "label": "exhibit", "title": "exhibit", "to": "complex patterns", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "metric-learning framework", "label": "optimizes", "title": "optimizes", "to": "representation", "width": 3.79}, {"arrows": "to", "color": "#CC7700", "from": "object and motion segmentation", "label": "is_type_of", "title": "is_type_of", "to": "segmentation", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Dynamic textures", "label": "used_in", "title": "used_in", "to": "video segmentation", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Spatio-temporal filters", "label": "used_in", "title": "used_in", "to": "video segmentation", "width": 3.4000000000000004}, {"arrows": "to", "color": "#0077CC", "from": "Metric learning", "label": "used_in", "title": "used_in", "to": "video segmentation", "width": 3.25}, {"arrows": "to", "color": "#CC7700", "from": "Graph-based segmentation", "label": "is_type_of", "title": "is_type_of", "to": "image segmentation", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Alpert et al. (2007)", "label": "published_in", "title": "published_in", "to": "CVPR", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Brox \u0026 Malik (2010)", "label": "published_in", "title": "published_in", "to": "ECCV", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Chan \u0026 Vasconcelos (2008)", "label": "published_in", "title": "published_in", "to": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Chan \u0026 Vasconcelos (2009)", "label": "published_in", "title": "published_in", "to": "CVPR", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "Variational layered dynamic textures", "label": "is_a", "title": "is_a", "to": "dynamic textures", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "Corso", "label": "delivered", "title": "delivered", "to": "CVPR tutorial on video segmentation", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "Spacetime texture representation and recognition", "label": "uses", "title": "uses", "to": "spatio-temporal orientation analysis", "width": 3.64}, {"arrows": "to", "color": "#CC7700", "from": "Dynamic texture detection", "label": "based_on", "title": "based_on", "to": "motion analysis", "width": 3.7600000000000002}, {"arrows": "to", "color": "#0077CC", "from": "S.", "label": "authored", "title": "authored", "to": "Dynamic textures", "width": 19}, {"arrows": "to", "color": "#0077CC", "from": "Wu, Y. N.", "label": "authored", "title": "authored", "to": "Dynamic textures", "width": 19}, {"arrows": "to", "color": "#0077CC", "from": "Fazekas, S.", "label": "authored", "title": "authored", "to": "Dynamic texture detection", "width": 19}, {"arrows": "to", "color": "#0077CC", "from": "Amiatz, T.", "label": "authored", "title": "authored", "to": "Dynamic texture detection", "width": 19}, {"arrows": "to", "color": "#0077CC", "from": "Chetverikov, D.", "label": "authored", "title": "authored", "to": "Dynamic texture detection", "width": 19}, {"arrows": "to", "color": "#0077CC", "from": "Feichtenhofer, C.", "label": "authored", "title": "authored", "to": "Bags of spacetime energies", "width": 19}, {"arrows": "to", "color": "#0077CC", "from": "Pinz, A.", "label": "authored", "title": "authored", "to": "Bags of spacetime energies", "width": 19}, {"arrows": "to", "color": "#0077CC", "from": "Wilides, R.", "label": "authored", "title": "authored", "to": "Bags of spacetime energies", "width": 19}, {"arrows": "to", "color": "#0077CC", "from": "Teney, Damien", "label": "affiliated_with", "title": "affiliated_with", "to": "Carnegie Mellon University", "width": 19}, {"arrows": "to", "color": "#0077CC", "from": "Brown, Matthew", "label": "affiliated_with", "title": "affiliated_with", "to": "University of Bath", "width": 19}, {"arrows": "to", "color": "#0077CC", "from": "Kit, Dimitry", "label": "affiliated_with", "title": "affiliated_with", "to": "University of Bath", "width": 19}, {"arrows": "to", "color": "#0077CC", "from": "Hall, Peter", "label": "affiliated_with", "title": "affiliated_with", "to": "University of Bath", "width": 19}, {"arrows": "to", "color": "#0077CC", "from": "Li, Yang", "label": "author_of", "title": "author_of", "to": "Reliable Patch Tracers", "width": 19}, {"arrows": "to", "color": "#0077CC", "from": "Zhu, Jianke", "label": "author_of", "title": "author_of", "to": "Reliable Patch Tracers", "width": 19}, {"arrows": "to", "color": "#0077CC", "from": "Hoi, Steven C.H.", "label": "author_of", "title": "author_of", "to": "Reliable Patch Tracers", "width": 19}, {"arrows": "to", "color": "#0077CC", "from": "modern trackers", "label": "use", "title": "use", "to": "bounding box", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "tracking results", "label": "sensitive to", "title": "sensitive to", "to": "initialization", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Reliable Patch Tracers (RPT)", "label": "is", "title": "is", "to": "tracking method", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Reliable Patch Tracers (RPT)", "label": "attempts to", "title": "attempts to", "to": "identify reliable patches", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "reliable patches", "label": "can be", "title": "can be", "to": "tracked effectively", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "reliable patches", "label": "are distributed over", "title": "are distributed over", "to": "image", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "tracking reliability metric", "label": "measures", "title": "measures", "to": "reliability of patch", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "probability model", "label": "estimates", "title": "estimates", "to": "distribution of reliable patches", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "probability model", "label": "uses", "title": "uses", "to": "sequential Monte Carlo framework", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "motion trajectories", "label": "distinguish", "title": "distinguish", "to": "reliable patches from background", "width": 3.7600000000000002}, {"arrows": "to", "color": "#CC7700", "from": "visual object", "label": "is defined as", "title": "is defined as", "to": "cluster", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "source code", "label": "is_available", "title": "is_available", "to": "public", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "Adam, A., Rivlin, E., \u0026 Shimshoni, I. (2006)", "label": "published", "title": "published", "to": "Robust fragments-based tracking", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Robust fragments-based tracking", "label": "uses", "title": "uses", "to": "integral histogram", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "Lucas, B. D., \u0026 Kanade, T. (1981)", "label": "developed", "title": "developed", "to": "iterative image registration technique", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "Poling, B., Lerman, G., \u0026 Szlarm, A. (2014)", "label": "presented", "title": "presented", "to": "Better feature tracking", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "Cai, Z., Wen, L., Yang, J., Lei, Z., \u0026 Li, S. (2012)", "label": "introduced", "title": "introduced", "to": "Structured visual tracking", "width": 3.7600000000000002}, {"arrows": "to", "color": "#CC7700", "from": "Sequential Monte Carlo Methods in Practice", "label": "describes", "title": "describes", "to": "Sequential Monte Carlo Framework", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "Szlarm", "label": "presented_in", "title": "presented_in", "to": "CVPR", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Cai et al.", "label": "presented_in", "title": "presented_in", "to": "ACCV", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Mannning et al.", "label": "authored", "title": "authored", "to": "Introduction to Information Retrieval", "width": 3.9699999999999998}, {"arrows": "to", "color": "#0077CC", "from": "Danelljan et al.", "label": "presented_in", "title": "presented_in", "to": "CVPR", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Everingham et al.", "label": "created", "title": "created", "to": "The pascal visual object classes(voc) challenge", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Grundmann et al.", "label": "presented_in", "title": "presented_in", "to": "CVPR", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Yang Li", "label": "affiliated_with", "title": "affiliated_with", "to": "College of Computer Science", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Yang Li", "label": "affiliated_with", "title": "affiliated_with", "to": "Zhejiang University", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Yang Li", "label": "email", "title": "email", "to": "liyang89@zju.edu.cn", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Jianke Zhu", "label": "affiliated_with", "title": "affiliated_with", "to": "College of Computer Science", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "V.", "label": "affiliated_with", "title": "affiliated_with", "to": "CVPR", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Han, M.", "label": "affiliated_with", "title": "affiliated_with", "to": "CVPR", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Essa, I.", "label": "affiliated_with", "title": "affiliated_with", "to": "CVPR", "width": 3.94}, {"arrows": "to", "color": "#CC7700", "from": "SALICON", "label": "is_effort_to", "title": "is_effort_to", "to": "understand and predict visual attention", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "SALICON", "label": "focuses_on", "title": "focuses_on", "to": "collecting large-scale human data", "width": 3.8499999999999996}, {"arrows": "to", "color": "#CC7700", "from": "SALICON", "label": "offers", "title": "offers", "to": "new possibilities for visual understanding", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "mouse-contingent paradigm", "label": "replaces", "title": "replaces", "to": "eye tracker", "width": 3.7}, {"arrows": "to", "color": "#0077CC", "from": "SALICON dataset", "label": "is_dataset_of", "title": "is_dataset_of", "to": "human \u0027free-viewing\u0027 data", "width": 3.91}, {"arrows": "to", "color": "#0077CC", "from": "SALICON dataset", "label": "contains_data_from", "title": "contains_data_from", "to": "10,000 images", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "SALICON dataset", "label": "is_based_on", "title": "is_based_on", "to": "Microsoft COCO dataset", "width": 3.82}, {"arrows": "to", "color": "#0077CC", "from": "SALICON dataset", "label": "serves_as", "title": "serves_as", "to": "ground truth for evaluating salience algorithms", "width": 3.79}, {"arrows": "to", "color": "#0077CC", "from": "SALICON dataset", "label": "complements", "title": "complements", "to": "existing annotations", "width": 3.64}, {"arrows": "to", "color": "#0077CC", "from": "Ming Jiang", "label": "affiliated_with", "title": "affiliated_with", "to": "National University of Singapore", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Shengshen Huang", "label": "affiliated_with", "title": "affiliated_with", "to": "National University of Singapore", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Juanyong Duan", "label": "affiliated_with", "title": "affiliated_with", "to": "National University of Singapore", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Qi Zhao", "label": "affiliated_with", "title": "affiliated_with", "to": "National University of Singapore", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "Deep LAC", "label": "focuses_on", "title": "focuses_on", "to": "fine-grained recognition", "width": 3.82}, {"arrows": "to", "color": "#CC7700", "from": "Deep LAC", "label": "is_a", "title": "is_a", "to": "Fine-grained Recognition", "width": 3.94}, {"arrows": "to", "color": "#0077CC", "from": "Deep LAC", "label": "is_author_of", "title": "is_author_of", "to": "Xiaoyong Shen", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Deep LAC", "label": "presented_at", "title": "presented_at", "to": "CVPR", "width": 3.7}, {"arrows": "to", "color": "#CC7700", "from": "Deep LAC", "label": "performs", "title": "performs", "to": "Localization", "width": 3.0999999999999996}, {"arrows": "to", "color": "#CC7700", "from": "Deep LAC", "label": "performs", "title": "performs", "to": "Classification", "width": 3.0999999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Di Lin", "label": "author of", "title": "author of", "to": "Deep LAC", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Xiaoyong Shen", "label": "author of", "title": "author of", "to": "Deep LAC", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Cewu Lu", "label": "author of", "title": "author of", "to": "Deep LAC", "width": 3.8499999999999996}, {"arrows": "to", "color": "#0077CC", "from": "Lin_Deep_LAC_Deep_2015_CVPR_paper", "label": "is_publication", "title": "is_publication", "to": "Deep LAC", "width": 3.55}, {"arrows": "to", "color": "#0077CC", "from": "eleqiz@nus.edu.sg", "label": "is_contact_email", "title": "is_contact_email", "to": "National University of Singapore", "width": 3.4000000000000004}, {"arrows": "to", "color": "#CCCCCC", "from": "fine-grained recognition system", "label": "incorporates", "title": "incorporates", "to": "part localization", "width": 3.94}, {"arrows": "to", "color": "#CCCCCC", "from": "fine-grained recognition system", "label": "incorporates", "title": "incorporates", "to": "alignment", "width": 3.94}, {"arrows": "to", "color": "#CCCCCC", "from": "fine-grained recognition system", "label": "incorporates", "title": "incorporates", "to": "classification", "width": 3.94}, {"arrows": "to", "color": "#CCCCCC", "from": "valve linkage function", "label": "enables", "title": "enables", "to": "back-propagation chaining", "width": 3.91}, {"arrows": "to", "color": "#CCCCCC", "from": "valve linkage function", "label": "compromises", "title": "compromises", "to": "classification errors", "width": 3.82}, {"arrows": "to", "color": "#CCCCCC", "from": "valve linkage function", "label": "compromises", "title": "compromises", "to": "alignment errors", "width": 3.82}, {"arrows": "to", "color": "#CCCCCC", "from": "valve linkage function", "label": "helps", "title": "helps", "to": "update localization", "width": 3.7600000000000002}, {"arrows": "to", "color": "#CCCCCC", "from": "deep LAC system", "label": "uses", "title": "uses", "to": "valve linkage function", "width": 3.88}, {"arrows": "to", "color": "#0077CC", "from": "LAC system", "label": "performs well on", "title": "performs well on", "to": "fine-grained object data", "width": 3.79}]);

                  nodeColors = {};
                  allNodes = nodes.get({ returnType: "Object" });
                  for (nodeId in allNodes) {
                    nodeColors[nodeId] = allNodes[nodeId].color;
                  }
                  allEdges = edges.get({ returnType: "Object" });
                  // adding nodes and edges to the graph
                  data = {nodes: nodes, edges: edges};

                  var options = {
    "configure": {
        "enabled": false
    },
    "edges": {
        "color": {
            "inherit": true
        },
        "smooth": {
            "enabled": true,
            "type": "dynamic"
        }
    },
    "interaction": {
        "dragNodes": true,
        "hideEdgesOnDrag": false,
        "hideNodesOnDrag": false
    },
    "physics": {
        "barnesHut": {
            "avoidOverlap": 0,
            "centralGravity": 0.3,
            "damping": 0.09,
            "gravitationalConstant": -80000,
            "springConstant": 0.001,
            "springLength": 200
        },
        "enabled": true,
        "stabilization": {
            "enabled": true,
            "fit": true,
            "iterations": 1000,
            "onlyDynamicEdges": false,
            "updateInterval": 50
        }
    }
};

                  


                  

                  network = new vis.Network(container, data, options);

                  

                  

                  


                  
                      network.on("stabilizationProgress", function(params) {
                          document.getElementById('loadingBar').removeAttribute("style");
                          var maxWidth = 496;
                          var minWidth = 20;
                          var widthFactor = params.iterations/params.total;
                          var width = Math.max(minWidth,maxWidth * widthFactor);
                          document.getElementById('bar').style.width = width + 'px';
                          document.getElementById('text').innerHTML = Math.round(widthFactor*100) + '%';
                      });
                      network.once("stabilizationIterationsDone", function() {
                          document.getElementById('text').innerHTML = '100%';
                          document.getElementById('bar').style.width = '496px';
                          document.getElementById('loadingBar').style.opacity = 0;
                          // really clean the dom element
                          setTimeout(function () {document.getElementById('loadingBar').style.display = 'none';}, 500);
                      });
                  

                  return network;

              }
              drawGraph();
        </script>
    </body>
</html>